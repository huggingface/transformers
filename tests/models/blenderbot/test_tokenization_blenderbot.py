import unittest

from transformers.models.blenderbot.tokenization_blenderbot import BlenderbotTokenizer
from transformers.testing_utils import require_tokenizers

from ...test_tokenization_common import TokenizerTesterMixin


@require_tokenizers
class BlenderbotTokenizationTest(TokenizerTesterMixin, unittest.TestCase):
    from_pretrained_id = ["facebook/blenderbot-3B"]
    tokenizer_class = BlenderbotTokenizer

    integration_expected_tokens = ['Ä This', 'Ä is', 'Ä a', 'Ä test', 'Ä Ã°ÅÄº', 'Ä¬', 'ÄŠ', 'I', 'Ä was', 'Ä born', 'Ä in', 'Ä 9', '2', '000', ',', 'Ä and', 'Ä this', 'Ä is', 'Ä f', 'als', 'ÃƒÂ©', '.', 'ÄŠ', 'Ã§', 'Ä¶', 'Å', 'Ã¦', 'Â´', 'Â»', 'Ã§', 'Ä¼', 'Ä¦', 'Ã§', 'Ä¾', 'Å', 'Ã¨', 'Â°', 'Ä½', 'Ã¦', 'Äº', 'Â¯', 'ÄŠ', 'H', 'i', 'Ä ', 'Ä Hello', 'ÄŠ', 'H', 'i', 'Ä ', 'Ä ', 'Ä Hello', 'ÄŠ', 'ÄŠ', 'Ä ', 'ÄŠ', 'Ä ', 'Ä ', 'ÄŠ', 'Ä Hello', 'ÄŠ', '<s>', 'Ä ', 'ÄŠ', 'hi', '<s>', 'Ä there', 'ÄŠ', 'The', 'Ä following', 'Ä string', 'Ä should', 'Ä be', 'Ä properly', 'Ä enc', 'od', 'ed', ':', 'Ä Hello', '.', 'ÄŠ', 'B', 'ut', 'Ä ', 'ird', 'Ä and', 'Ä ', 'Ã ', 'Â¸', 'Ä½', 'Ã ', 'Â¸', 'Âµ', 'Ä ', 'Ä ', 'Ä ', 'ird', 'Ä ', 'Ä ', 'Ä ', 'Ã ', 'Â¸', 'Ä¶', 'ÄŠ', 'H', 'ey', 'Ä how', 'Ä are', 'Ä you', 'Ä doing']  # fmt: skip
    integration_expected_token_ids = [678, 315, 265, 1689, 3417, 240, 206, 48, 372, 3647, 302, 1207, 25, 1694, 19, 298, 381, 315, 284, 1095, 3952, 21, 206, 171, 250, 261, 170, 120, 127, 171, 256, 234, 171, 258, 261, 172, 116, 257, 170, 254, 115, 206, 47, 80, 228, 6950, 206, 47, 80, 228, 228, 6950, 206, 206, 228, 206, 228, 228, 206, 6950, 206, 1, 228, 206, 7417, 1, 505, 206, 2839, 3504, 7884, 636, 310, 3867, 2525, 621, 296, 33, 6950, 21, 206, 41, 329, 228, 1221, 298, 228, 164, 124, 257, 164, 124, 121, 228, 228, 228, 1221, 228, 228, 228, 164, 124, 250, 206, 47, 3110, 544, 366, 304, 929]  # fmt: skip
    expected_tokens_from_ids = ['Ä This', 'Ä is', 'Ä a', 'Ä test', 'Ä Ã°ÅÄº', 'Ä¬', 'ÄŠ', 'I', 'Ä was', 'Ä born', 'Ä in', 'Ä 9', '2', '000', ',', 'Ä and', 'Ä this', 'Ä is', 'Ä f', 'als', 'ÃƒÂ©', '.', 'ÄŠ', 'Ã§', 'Ä¶', 'Å', 'Ã¦', 'Â´', 'Â»', 'Ã§', 'Ä¼', 'Ä¦', 'Ã§', 'Ä¾', 'Å', 'Ã¨', 'Â°', 'Ä½', 'Ã¦', 'Äº', 'Â¯', 'ÄŠ', 'H', 'i', 'Ä ', 'Ä Hello', 'ÄŠ', 'H', 'i', 'Ä ', 'Ä ', 'Ä Hello', 'ÄŠ', 'ÄŠ', 'Ä ', 'ÄŠ', 'Ä ', 'Ä ', 'ÄŠ', 'Ä Hello', 'ÄŠ', '<s>', 'Ä ', 'ÄŠ', 'hi', '<s>', 'Ä there', 'ÄŠ', 'The', 'Ä following', 'Ä string', 'Ä should', 'Ä be', 'Ä properly', 'Ä enc', 'od', 'ed', ':', 'Ä Hello', '.', 'ÄŠ', 'B', 'ut', 'Ä ', 'ird', 'Ä and', 'Ä ', 'Ã ', 'Â¸', 'Ä½', 'Ã ', 'Â¸', 'Âµ', 'Ä ', 'Ä ', 'Ä ', 'ird', 'Ä ', 'Ä ', 'Ä ', 'Ã ', 'Â¸', 'Ä¶', 'ÄŠ', 'H', 'ey', 'Ä how', 'Ä are', 'Ä you', 'Ä doing']  # fmt: skip
    integration_expected_decoded_text = " This is a test ğŸ˜Š\nI was born in 92000, and this is falsÃ©.\nç”Ÿæ´»çš„çœŸè°›æ˜¯\nHi  Hello\nHi   Hello\n\n \n  \n Hello\n<s> \nhi<s> there\nThe following string should be properly encoded: Hello.\nBut ird and à¸›à¸µ   ird   à¸”\nHey how are you doing"

    def test_pretokenized_inputs(self, *args, **kwargs):
        # It's very difficult to mix/test pretokenization with byte-level tokenizers
        # The issue is that when you have a sequence with leading spaces, splitting it
        # with .split() loses the leading spaces, so the tokenization results differ
        pass
