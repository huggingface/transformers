# Copyright 2024 The Qwen team, Alibaba Group and the HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


import unittest
import tempfile

from transformers import AutoTokenizer, AddedToken, PreTrainedTokenizerFast
from transformers.models.qwen2.tokenization_qwen2_fast import Qwen2TokenizerFast
from transformers.create_fast_tokenizer import SentencePieceExtractor
from transformers.testing_utils import (
    require_sentencepiece,
    require_tokenizers,
)
from tests.test_tokenization_common import TokenizerTesterMixin

input_string = """Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet...) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between Jax, PyTorch and TensorFlow.ğŸ¤— Transformers æä¾›äº†å¯ä»¥è½»æ¾åœ°ä¸‹è½½å¹¶ä¸”è®­ç»ƒå…ˆè¿›çš„é¢„è®­ç»ƒæ¨¡å‹çš„ API å’Œå·¥å…·ã€‚ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹å¯ä»¥å‡å°‘è®¡ç®—æ¶ˆè€—å’Œç¢³æ’æ”¾ï¼Œå¹¶ä¸”èŠ‚çœä»å¤´è®­ç»ƒæ‰€éœ€è¦çš„æ—¶é—´å’Œèµ„æºã€‚```python
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-tokenizer")
tokenizer("ä¸–ç•Œï¼Œä½ å¥½ï¼")```<|im_start|>Hello, world!<|im_end|>
"""

expected_tokens = ['Transform', 'ers', 'Ä (', 'formerly', 'Ä known', 'Ä as', 'Ä py', 'torch', '-transform', 'ers', 'Ä and', 'Ä py', 'torch', '-pre', 'trained', '-b', 'ert', ')', 'Ä provides', 'Ä general', '-purpose', 'Ä architectures', 'Ä (', 'BERT', ',', 'Ä G', 'PT', '-', '2', ',', 'Ä Ro', 'BERT', 'a', ',', 'Ä X', 'LM', ',', 'Ä Dist', 'il', 'B', 'ert', ',', 'Ä XL', 'Net', '...)', 'Ä for', 'Ä Natural', 'Ä Language', 'Ä Understanding', 'Ä (', 'N', 'LU', ')', 'Ä and', 'Ä Natural', 'Ä Language', 'Ä Generation', 'Ä (', 'NL', 'G', ')', 'Ä with', 'Ä over', 'Ä ', '3', '2', '+', 'Ä pretrained', 'Ä models', 'Ä in', 'Ä ', '1', '0', '0', '+', 'Ä languages', 'Ä and', 'Ä deep', 'Ä interoper', 'ability', 'Ä between', 'Ä J', 'ax', ',', 'Ä Py', 'T', 'orch', 'Ä and', 'Ä TensorFlow', '.', 'Ã°ÅÂ¤Ä¹', 'Ä Transformers', 'Ä Ã¦Ä±Ä²', 'Ã¤Â¾Ä½', 'Ã¤ÂºÄ¨', 'Ã¥Ä±Â¯Ã¤Â»Â¥', 'Ã¨Â½Â»Ã¦Ä¿Â¾', 'Ã¥Ä¾Â°', 'Ã¤Â¸Ä­Ã¨Â½Â½', 'Ã¥Â¹Â¶Ã¤Â¸Ä¶', 'Ã¨Â®ÅƒÃ§Â»Ä¥', 'Ã¥Ä§ÄªÃ¨Â¿Ä½Ã§Ä¼Ä¦', 'Ã©Â¢Ä¦', 'Ã¨Â®ÅƒÃ§Â»Ä¥', 'Ã¦Â¨Â¡Ã¥Å€Ä­', 'Ã§Ä¼Ä¦', 'Ä API', 'Ä Ã¥Ä´Ä®', 'Ã¥Â·Â¥Ã¥Ä§Â·', 'Ã£Ä¢Ä¤', 'Ã¤Â½Â¿Ã§Ä¶Â¨', 'Ã©Â¢Ä¦', 'Ã¨Â®ÅƒÃ§Â»Ä¥', 'Ã¦Â¨Â¡Ã¥Å€Ä­', 'Ã¥Ä±Â¯Ã¤Â»Â¥', 'Ã¥Ä©Ä±Ã¥Â°Ä³', 'Ã¨Â®Â¡Ã§Â®Ä¹', 'Ã¦Â¶ÄªÃ¨Ä¢Ä¹', 'Ã¥Ä´Ä®', 'Ã§Â¢Â³', 'Ã¦Ä°Ä´Ã¦Ä¶Â¾', 'Ã¯Â¼Ä®Ã¥Â¹Â¶', 'Ã¤Â¸Ä¶', 'Ã¨Ä¬Ä¤Ã§Ä¾Ä£', 'Ã¤Â»Ä°', 'Ã¥Â¤Â´', 'Ã¨Â®ÅƒÃ§Â»Ä¥', 'Ã¦Ä«Ä¢Ã©Ä¾Ä¢Ã¨Â¦Ä£', 'Ã§Ä¼Ä¦Ã¦Ä¹Â¶Ã©Ä¹Â´', 'Ã¥Ä´Ä®', 'Ã¨ÂµÄ¦Ã¦ÂºÄ²', 'Ã£Ä¢Ä¤', '```', 'python', 'ÄŠ', 'tokenizer', 'Ä =', 'Ä Auto', 'Tokenizer', '.from', '_pre', 'trained', '("', 'Q', 'wen', '/Q', 'wen', '-token', 'izer', '")ÄŠ', 'tokenizer', '("', 'Ã¤Â¸Ä¸Ã§Ä·Ä®', 'Ã¯Â¼Ä®', 'Ã¤Â½Å‚Ã¥Â¥Â½', 'Ã¯Â¼Ä£', '")', '```', '<|im_start|>', 'Hello', ',', 'Ä world', '!', '<|im_end|>', 'ÄŠ']
expected_token_ids = [8963, 388, 320, 69514, 3881, 438, 4510, 27414, 32852, 388, 323, 4510, 27414, 21334, 35722, 1455, 529, 8, 5707, 4586, 58238, 77235, 320, 61437, 11, 479, 2828, 12, 17, 11, 11830, 61437, 64, 11, 1599, 10994, 11, 27604, 321, 33, 529, 11, 29881, 6954, 32574, 369, 18448, 11434, 45451, 320, 45, 23236, 8, 323, 18448, 11434, 23470, 320, 30042, 38, 8, 448, 916, 220, 18, 17, 10, 80669, 4119, 304, 220, 16, 15, 15, 10, 15459, 323, 5538, 94130, 2897, 1948, 619, 706, 11, 5355, 51, 21584, 323, 94986, 13, 144834, 80532, 93685, 83744, 34187, 73670, 104261, 29490, 62189, 103937, 104034, 102830, 98841, 104034, 104949, 9370, 5333, 58143, 102011, 1773, 37029, 98841, 104034, 104949, 73670, 101940, 100768, 104997, 33108, 100912, 105054, 90395, 100136, 106831, 45181, 64355, 104034, 113521, 101975, 33108, 85329, 1773, 73594, 12669, 198, 85593, 284, 8979, 37434, 6387, 10442, 35722, 445, 48, 16948, 45274, 16948, 34841, 3135, 1138, 85593, 445, 99489, 3837, 108386, 6313, 899, 73594, 151644, 9707, 11, 1879, 0, 151645, 198]

@require_tokenizers
class Qwen2TokenizationTest(TokenizerTesterMixin, unittest.TestCase):
    from_pretrained_id = "Qwen/Qwen2.5-VL-7B-Instruct"
    tokenizer_class = Qwen2TokenizerFast
    rust_tokenizer_class = Qwen2TokenizerFast
    test_slow_tokenizer = True
    test_rust_tokenizer = False # we're going to just test the fast one I'll remove this
    space_between_special_tokens = False
    from_pretrained_kwargs = {}
    test_seq2seq = False

    @classmethod
    def setUpClass(cls):
        super().setUpClass()

        from_pretrained_id = "Qwen/Qwen2.5-VL-7B-Instruct"
        
        tok_auto = AutoTokenizer.from_pretrained(from_pretrained_id)
        tok_auto.save_pretrained(cls.tmpdirname)

        cls.tokenizers = [tok_auto]

    def test_integration_expected_tokens(self):
        for tok in self.tokenizers:
            self.assertEqual(tok.tokenize(input_string), expected_tokens)

    def test_integration_expected_token_ids(self):
        for tok in self.tokenizers:
            self.assertEqual(tok.encode(input_string), expected_token_ids)

    def test_save_and_reload(self):
        for tok in self.tokenizers:
            with self.subTest(f"{tok.__class__.__name__}"):
                original_tokens = tok.tokenize(input_string)
                original_ids = tok.encode(input_string)

                # Save tokenizer to temporary directory
                with tempfile.TemporaryDirectory() as tmp_dir:
                    tok.save_pretrained(tmp_dir)

                    # Reload tokenizer from saved directory
                    reloaded_tok = tok.__class__.from_pretrained(tmp_dir)

                    # Test that reloaded tokenizer produces same results
                    reloaded_tokens = reloaded_tok.tokenize(input_string)
                    reloaded_ids = reloaded_tok.encode(input_string)

                    self.assertEqual(original_tokens, reloaded_tokens)
                    self.assertEqual(original_ids, reloaded_ids)
