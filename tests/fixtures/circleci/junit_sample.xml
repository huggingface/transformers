<?xml version="1.0" encoding="utf-8"?><testsuites name="pytest tests"><testsuite name="pytest" errors="0" failures="4" skipped="18" tests="243" time="28.981" timestamp="2025-11-16T22:40:27.820441+00:00" hostname="d89944057df4"><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_chat_template_save_loading" file="tests/test_tokenization_common.py" line="882" time="0.913"><skipped type="pytest.skip" message="tokenizer doesn't accept chat templates at input">/root/project/tests/test_tokenization_common.py:883: tokenizer doesn't accept chat templates at input</skipped></testcase><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_continue_final_message" file="tests/test_tokenization_common.py" line="1278" time="0.007" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_chat_template" file="tests/test_tokenization_common.py" line="821" time="0.950" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_chat_template_batched" file="tests/test_tokenization_common.py" line="924" time="0.003" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_jinja_loopcontrols" file="tests/test_tokenization_common.py" line="958" time="0.966" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_mecab_tokenizer_no_normalize" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="192" time="0.967" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_continue_final_message_with_decoy_earlier_message" file="tests/test_tokenization_common.py" line="1336" time="0.005" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_jinja_strftime" file="tests/test_tokenization_common.py" line="978" time="0.004" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_mecab_tokenizer_unidic" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="153" time="0.009" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_chat_template_dict" file="tests/test_tokenization_common.py" line="1361" time="0.004" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_encode_plus_with_padding_0" file="tests/test_tokenization_common.py" line="1992" time="1.002" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_encode_plus_with_padding_1" file="tests/test_tokenization_common.py" line="1992" time="0.003" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_continue_final_message_with_trim" file="tests/test_tokenization_common.py" line="1306" time="0.003" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_mecab_tokenizer_unidic_lite" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="142" time="0.002" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_chat_template_dict_saving" file="tests/test_tokenization_common.py" line="1377" time="0.008" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_full_tokenizer" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="98" time="0.001" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_mecab_tokenizer_with_option" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="178" time="0.001" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_jumanpp_tokenizer_trim_whitespace" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="335" time="1.058" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_mask_output" file="tests/test_tokenization_common.py" line="779" time="0.003" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_jumanpp_full_tokenizer_with_jumanpp_kwargs_trim_whitespace" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="344" time="0.066" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_chat_template_file_priority" file="tests/test_tokenization_common.py" line="1409" time="0.005" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_get_vocab" file="tests/test_tokenization_common.py" line="2104" time="0.003" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_conversion_reversible" file="tests/test_tokenization_common.py" line="2117" time="0.003"><failure message="AssertionError: 2 != 1">self = &lt;tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest testMethod=test_conversion_reversible&gt;

    def test_conversion_reversible(self):
        tokenizer = self.get_tokenizer(do_lower_case=False)
        vocab = tokenizer.get_vocab()
        for word, ind in vocab.items():
            if word == tokenizer.unk_token:
                continue
&gt;           self.assertEqual(tokenizer.convert_tokens_to_ids(word), ind)
E           AssertionError: 2 != 1

tests/test_tokenization_common.py:2124: AssertionError</failure></testcase><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_model_input_names_signature" file="tests/test_tokenization_common.py" line="511" time="0.003" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_empty_input_string" file="tests/test_tokenization_common.py" line="2462" time="0.003" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_jumanpp_tokenizer" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="316" time="0.027" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_maximum_encoding_length_pair_input" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="92" time="0.001" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_integration" file="tests/test_tokenization_common.py" line="712" time="0.001"><skipped type="pytest.skip" message="No integration expected tokens provided">/root/project/tests/test_tokenization_common.py:713: No integration expected tokens provided</skipped></testcase><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_encode_basic_padding" file="tests/test_tokenization_common.py" line="1860" time="0.003" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_number_of_added_tokens" file="tests/test_tokenization_common.py" line="1423" time="0.004" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_maximum_encoding_length_single_input" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="95" time="0.001" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_jumanpp_tokenizer_ext" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="355" time="0.028" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_integration_from_extractor" file="tests/test_tokenization_common.py" line="734" time="0.001"><skipped type="pytest.skip" message="No integration expected tokens provided">/root/project/tests/test_tokenization_common.py:735: No integration expected tokens provided</skipped></testcase><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_pretokenized_inputs" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="89" time="0.001" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_pad_token_initialization" file="tests/test_tokenization_common.py" line="2490" time="0.003" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_jumanpp_tokenizer_lower" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="323" time="0.026" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_mecab_full_tokenizer_with_mecab_kwargs" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="125" time="0.002" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_right_and_left_truncation" file="tests/test_tokenization_common.py" line="1889" time="0.003" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_jumanpp_tokenizer_no_normalize" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="329" time="0.026" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_mecab_tokenizer_ipadic" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="134" time="0.001" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_internal_consistency" file="tests/test_tokenization_common.py" line="763" time="0.004"><failure message="AssertionError: '[SEP] 、 世界 。 [MASK]ばんは 、 世界 。' != 'こんにちは 、 世界 。 こんばんは 、 世界 。'&#10;- [SEP] 、 世界 。 [MASK]ばんは 、 世界 。&#10;+ こんにちは 、 世界 。 こんばんは 、 世界 。">self = &lt;tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest testMethod=test_internal_consistency&gt;

    def test_internal_consistency(self):
        tokenizer = self.get_tokenizer()
        input_text, output_text = self.get_input_output_texts(tokenizer)
    
        tokens = tokenizer.tokenize(input_text)
        ids = tokenizer.convert_tokens_to_ids(tokens)
        ids_2 = tokenizer.encode(input_text, add_special_tokens=False)
        self.assertListEqual(ids, ids_2)
    
        tokens_2 = tokenizer.convert_ids_to_tokens(ids)
        self.assertNotEqual(len(tokens_2), 0)
        text_2 = tokenizer.decode(ids)
        self.assertIsInstance(text_2, str)
    
&gt;       self.assertEqual(text_2, output_text)
E       AssertionError: '[SEP] 、 世界 。 [MASK]ばんは 、 世界 。' != 'こんにちは 、 世界 。 こんばんは 、 世界 。'
E       - [SEP] 、 世界 。 [MASK]ばんは 、 世界 。
E       + こんにちは 、 世界 。 こんばんは 、 世界 。

tests/test_tokenization_common.py:778: AssertionError</failure></testcase><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_tokenizer_initialization_with_conflicting_key" file="tests/test_tokenization_common.py" line="2455" time="0.006" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_mecab_tokenizer_lower" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="170" time="0.002" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_sudachi_tokenizer_split_mode_A" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="232" time="0.054" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_save_and_load_tokenizer" file="tests/test_tokenization_common.py" line="602" time="0.018" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_tokenizer_store_full_signature" file="tests/test_tokenization_common.py" line="522" time="0.002" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_sudachi_tokenizer_split_mode_B" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="238" time="0.041" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_sudachi_tokenizer_split_mode_C" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="244" time="0.024" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_tokenizers_common_ids_setters" file="tests/test_tokenization_common.py" line="569" time="0.003" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_tokenizers_common_properties" file="tests/test_tokenization_common.py" line="537" time="0.002" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_sudachi_tokenizer_trim_whitespace" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="286" time="0.028" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_token_type_ids" file="tests/test_tokenization_common.py" line="787" time="0.003" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_tokenize_special_tokens" file="tests/test_tokenization_common.py" line="491" time="0.003" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_added_tokens_serialization" file="tests/test_tokenization_common.py" line="2428" time="1.455" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_chat_template_return_assistant_tokens_mask" file="tests/test_tokenization_common.py" line="997" time="0.339"><skipped type="pytest.skip" message="Custom backend tokenizer">/root/project/tests/test_tokenization_common.py:998: Custom backend tokenizer</skipped></testcase><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_added_tokens_serialization" file="tests/test_tokenization_common.py" line="2428" time="0.429" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_batch_encode_dynamic_overflowing" file="tests/test_tokenization_common.py" line="2365" time="0.004" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_batch_encode_plus_batch_sequence_length" file="tests/test_tokenization_common.py" line="2155" time="0.011" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_character_tokenizer" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="447" time="0.420" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_chat_template" file="tests/test_tokenization_common.py" line="821" time="0.013" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_batch_encode_plus_padding" file="tests/test_tokenization_common.py" line="2201" time="0.008" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_chat_template_batched" file="tests/test_tokenization_common.py" line="924" time="0.003" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_bos_token_with_add_bos_token_false" file="tests/test_tokenization_common.py" line="2540" time="0.003" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_chat_template_dict" file="tests/test_tokenization_common.py" line="1361" time="0.004" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_bos_token_with_add_bos_token_true" file="tests/test_tokenization_common.py" line="2523" time="0.003" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_chat_template_return_assistant_tokens_mask_truncated" file="tests/test_tokenization_common.py" line="1175" time="0.394"><skipped type="pytest.skip" message="Custom backend tokenizer">/root/project/tests/test_tokenization_common.py:1176: Custom backend tokenizer</skipped></testcase><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_batch_encode_dynamic_overflowing" file="tests/test_tokenization_common.py" line="2365" time="0.402" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_special_tokens_mask_input_pairs" file="tests/test_tokenization_common.py" line="1803" time="0.003" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_chat_template_dict_saving" file="tests/test_tokenization_common.py" line="1377" time="0.008" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_batch_encode_plus_batch_sequence_length" file="tests/test_tokenization_common.py" line="2155" time="0.007" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_call" file="tests/test_tokenization_common.py" line="2126" time="0.010" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_chat_template_file_priority" file="tests/test_tokenization_common.py" line="1409" time="0.005" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_batch_encode_plus_padding" file="tests/test_tokenization_common.py" line="2201" time="0.007" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_full_tokenizer" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="438" time="0.001" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_bos_token_with_add_bos_token_false" file="tests/test_tokenization_common.py" line="2540" time="0.003" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_jinja_loopcontrols" file="tests/test_tokenization_common.py" line="958" time="0.005" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_sudachi_full_tokenizer_with_sudachi_kwargs_split_mode_B" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="250" time="0.048" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_get_vocab" file="tests/test_tokenization_common.py" line="2104" time="0.003" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_chat_template_return_assistant_tokens_mask" file="tests/test_tokenization_common.py" line="997" time="0.484"><skipped type="pytest.skip" message="Custom backend tokenizer">/root/project/tests/test_tokenization_common.py:998: Custom backend tokenizer</skipped></testcase><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_chat_template_return_assistant_tokens_mask_truncated" file="tests/test_tokenization_common.py" line="1175" time="0.002"><skipped type="pytest.skip" message="Custom backend tokenizer">/root/project/tests/test_tokenization_common.py:1176: Custom backend tokenizer</skipped></testcase><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_jinja_strftime" file="tests/test_tokenization_common.py" line="978" time="0.003" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_bos_token_with_add_bos_token_true" file="tests/test_tokenization_common.py" line="2523" time="0.003" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_sudachi_full_tokenizer_with_sudachi_kwargs_sudachi_projection" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="266" time="0.042" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_integration" file="tests/test_tokenization_common.py" line="712" time="0.001"><skipped type="pytest.skip" message="No integration expected tokens provided">/root/project/tests/test_tokenization_common.py:713: No integration expected tokens provided</skipped></testcase><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_chat_template_save_loading" file="tests/test_tokenization_common.py" line="882" time="0.002"><skipped type="pytest.skip" message="tokenizer doesn't accept chat templates at input">/root/project/tests/test_tokenization_common.py:883: tokenizer doesn't accept chat templates at input</skipped></testcase><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_sudachi_tokenizer_core" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="221" time="0.023" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_mask_output" file="tests/test_tokenization_common.py" line="779" time="0.003" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_call" file="tests/test_tokenization_common.py" line="2126" time="0.007" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_integration_from_extractor" file="tests/test_tokenization_common.py" line="734" time="0.001"><skipped type="pytest.skip" message="No integration expected tokens provided">/root/project/tests/test_tokenization_common.py:735: No integration expected tokens provided</skipped></testcase><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_sudachi_tokenizer_lower" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="274" time="0.024" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_continue_final_message" file="tests/test_tokenization_common.py" line="1278" time="0.006" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_maximum_encoding_length_pair_input" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="432" time="0.001" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_sudachi_tokenizer_no_normalize" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="280" time="0.024" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_continue_final_message_with_decoy_earlier_message" file="tests/test_tokenization_common.py" line="1336" time="0.005" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_maximum_encoding_length_single_input" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="435" time="0.001" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_internal_consistency" file="tests/test_tokenization_common.py" line="763" time="0.003"><failure message="AssertionError: '[SEP] [MASK] に ち は 、 世 界 。 [SEP] [MASK] ば [MASK] は 、 世 界 。' != 'こ ん に ち は 、 世 界 。 こ ん ば ん は 、 世 界 。'&#10;- [SEP] [MASK] に ち は 、 世 界 。 [SEP] [MASK] ば [MASK] は 、 世 界 。&#10;+ こ ん に ち は 、 世 界 。 こ ん ば ん は 、 世 界 。">self = &lt;tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest testMethod=test_internal_consistency&gt;

    def test_internal_consistency(self):
        tokenizer = self.get_tokenizer()
        input_text, output_text = self.get_input_output_texts(tokenizer)
    
        tokens = tokenizer.tokenize(input_text)
        ids = tokenizer.convert_tokens_to_ids(tokens)
        ids_2 = tokenizer.encode(input_text, add_special_tokens=False)
        self.assertListEqual(ids, ids_2)
    
        tokens_2 = tokenizer.convert_ids_to_tokens(ids)
        self.assertNotEqual(len(tokens_2), 0)
        text_2 = tokenizer.decode(ids)
        self.assertIsInstance(text_2, str)
    
&gt;       self.assertEqual(text_2, output_text)
E       AssertionError: '[SEP] [MASK] に ち は 、 世 界 。 [SEP] [MASK] ば [MASK] は 、 世 界 。' != 'こ ん に ち は 、 世 界 。 こ ん ば ん は 、 世 界 。'
E       - [SEP] [MASK] に ち は 、 世 界 。 [SEP] [MASK] ば [MASK] は 、 世 界 。
E       + こ ん に ち は 、 世 界 。 こ ん ば ん は 、 世 界 。

tests/test_tokenization_common.py:778: AssertionError</failure></testcase><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_padding_with_attention_mask" file="tests/test_tokenization_common.py" line="1975" time="0.002" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_continue_final_message_with_trim" file="tests/test_tokenization_common.py" line="1306" time="0.002" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_sudachi_tokenizer_projection" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="258" time="0.024" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_prepare_seq2seq_batch" file="tests/test_tokenization_common.py" line="2323" time="0.011" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_pretokenized_inputs" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="429" time="0.001" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_right_and_left_truncation" file="tests/test_tokenization_common.py" line="1889" time="0.003" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_truncation_side_in_kwargs" file="tests/test_tokenization_common.py" line="1843" time="0.128" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_save_and_load_tokenizer" file="tests/test_tokenization_common.py" line="602" time="0.019" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_truncation_side_in_kwargs" file="tests/test_tokenization_common.py" line="1843" time="0.927" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_wordpiece_tokenizer" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="364" time="0.001" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_model_input_names_signature" file="tests/test_tokenization_common.py" line="511" time="0.402" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_number_of_added_tokens" file="tests/test_tokenization_common.py" line="1423" time="0.003" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_pad_token_initialization" file="tests/test_tokenization_common.py" line="2490" time="0.004" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_tokenize_special_tokens" file="tests/test_tokenization_common.py" line="491" time="0.409" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_tokenizer_initialization_with_conflicting_key" file="tests/test_tokenization_common.py" line="2455" time="0.005" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_tokenizer_store_full_signature" file="tests/test_tokenization_common.py" line="522" time="0.002" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.AutoTokenizerCustomTest" name="test_tokenizer_bert_japanese" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="477" time="0.332" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_tokenizers_common_ids_setters" file="tests/test_tokenization_common.py" line="569" time="0.003" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_padding_side_in_kwargs" file="tests/test_tokenization_common.py" line="1826" time="0.127" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_padding_to_multiple_of" file="tests/test_tokenization_common.py" line="1943" time="0.003" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_tokenizers_common_properties" file="tests/test_tokenization_common.py" line="537" time="0.003" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_sentencepiece_tokenizer" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="380" time="1.477" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_sequence_builders" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="461" time="0.737" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_sequence_ids" file="tests/test_tokenization_common.py" line="798" time="0.004"><skipped type="pytest.skip" message="Tokenizers backend tokenizer">/root/project/tests/test_tokenization_common.py:799: Tokenizers backend tokenizer</skipped></testcase><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_conversion_reversible" file="tests/test_tokenization_common.py" line="2117" time="0.450"><failure message="AssertionError: 2 != 1">self = &lt;tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest testMethod=test_conversion_reversible&gt;

    def test_conversion_reversible(self):
        tokenizer = self.get_tokenizer(do_lower_case=False)
        vocab = tokenizer.get_vocab()
        for word, ind in vocab.items():
            if word == tokenizer.unk_token:
                continue
&gt;           self.assertEqual(tokenizer.convert_tokens_to_ids(word), ind)
E           AssertionError: 2 != 1

tests/test_tokenization_common.py:2124: AssertionError</failure></testcase><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_empty_input_string" file="tests/test_tokenization_common.py" line="2462" time="0.003" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_special_tokens_mask" file="tests/test_tokenization_common.py" line="1786" time="0.005" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_encode_basic_padding" file="tests/test_tokenization_common.py" line="1860" time="0.003" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_special_tokens_mask_input_pairs" file="tests/test_tokenization_common.py" line="1803" time="0.005" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_encode_plus_with_padding_0" file="tests/test_tokenization_common.py" line="1992" time="0.003" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_token_type_ids" file="tests/test_tokenization_common.py" line="787" time="0.004" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseCharacterTokenizationTest" name="test_encode_plus_with_padding_1" file="tests/test_tokenization_common.py" line="1992" time="0.004" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_sequence_builders" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="390" time="0.426" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_sequence_ids" file="tests/test_tokenization_common.py" line="798" time="0.003"><skipped type="pytest.skip" message="Tokenizers backend tokenizer">/root/project/tests/test_tokenization_common.py:799: Tokenizers backend tokenizer</skipped></testcase><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_special_tokens_mask" file="tests/test_tokenization_common.py" line="1786" time="0.003" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_batch_encode_dynamic_overflowing" file="tests/test_tokenization_common.py" line="2365" time="1.410" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_bos_token_with_add_bos_token_true" file="tests/test_tokenization_common.py" line="2523" time="1.050" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_chat_template_dict" file="tests/test_tokenization_common.py" line="1361" time="1.172" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertTokenizerMismatchTest" name="test_tokenizer_mismatch_warning" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="484" time="1.324" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_continue_final_message_with_trim" file="tests/test_tokenization_common.py" line="1306" time="1.128" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_call" file="tests/test_tokenization_common.py" line="2126" time="0.422" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_batch_encode_plus_batch_sequence_length" file="tests/test_tokenization_common.py" line="2155" time="0.444" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_chat_template_return_assistant_tokens_mask_truncated" file="tests/test_tokenization_common.py" line="1175" time="1.207" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_encode_plus_with_padding_0" file="tests/test_tokenization_common.py" line="1992" time="1.082" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_padding_side_in_kwargs" file="tests/test_tokenization_common.py" line="1826" time="4.437" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_padding_to_multiple_of" file="tests/test_tokenization_common.py" line="1943" time="0.003" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_padding_with_attention_mask" file="tests/test_tokenization_common.py" line="1975" time="0.003" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_pickle_jumanpp_tokenizer" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="295" time="0.091" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_pickle_mecab_tokenizer" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="105" time="0.003" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_chat_template_save_loading" file="tests/test_tokenization_common.py" line="882" time="0.445"><skipped type="pytest.skip" message="tokenizer doesn't accept chat templates at input">/root/project/tests/test_tokenization_common.py:883: tokenizer doesn't accept chat templates at input</skipped></testcase><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_pickle_sudachi_tokenizer" file="tests/models/bert_japanese/test_tokenization_bert_japanese.py" line="200" time="0.114" /><testcase classname="tests.models.bert_japanese.test_tokenization_bert_japanese.BertJapaneseTokenizationTest" name="test_prepare_seq2seq_batch" file="tests/test_tokenization_common.py" line="2323" time="0.012" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_encode_plus_with_padding_1" file="tests/test_tokenization_common.py" line="1992" time="0.421" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_chat_template_dict_saving" file="tests/test_tokenization_common.py" line="1377" time="0.940" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_conversion_reversible" file="tests/test_tokenization_common.py" line="2117" time="0.704" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_continue_final_message" file="tests/test_tokenization_common.py" line="1278" time="0.256" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_batch_encode_plus_padding" file="tests/test_tokenization_common.py" line="2201" time="0.768" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_empty_input_string" file="tests/test_tokenization_common.py" line="2462" time="0.236" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_chat_template" file="tests/test_tokenization_common.py" line="821" time="0.966" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_get_vocab" file="tests/test_tokenization_common.py" line="2104" time="0.477" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_bos_token_with_add_bos_token_false" file="tests/test_tokenization_common.py" line="2540" time="0.334" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_chat_template_batched" file="tests/test_tokenization_common.py" line="924" time="0.220" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_continue_final_message_with_decoy_earlier_message" file="tests/test_tokenization_common.py" line="1336" time="0.455" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_pad_token_initialization" file="tests/test_tokenization_common.py" line="2490" time="0.224" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_encode_basic_padding" file="tests/test_tokenization_common.py" line="1860" time="0.433" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_added_tokens_serialization" file="tests/test_tokenization_common.py" line="2428" time="1.619" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_chat_template_file_priority" file="tests/test_tokenization_common.py" line="1409" time="0.798" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_integration" file="tests/test_tokenization_common.py" line="712" time="0.495" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_padding_with_attention_mask" file="tests/test_tokenization_common.py" line="1975" time="0.234" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_right_and_left_truncation" file="tests/test_tokenization_common.py" line="1889" time="0.412" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_jinja_strftime" file="tests/test_tokenization_common.py" line="978" time="1.083" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_special_tokens_mask" file="tests/test_tokenization_common.py" line="1786" time="0.227" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_chat_template_return_assistant_tokens_mask" file="tests/test_tokenization_common.py" line="997" time="0.522" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_special_tokens_mask_input_pairs" file="tests/test_tokenization_common.py" line="1803" time="0.233" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_prepare_seq2seq_batch" file="tests/test_tokenization_common.py" line="2323" time="0.439" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_tokenizer_initialization_with_conflicting_key" file="tests/test_tokenization_common.py" line="2455" time="0.005" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_mask_output" file="tests/test_tokenization_common.py" line="779" time="0.404" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_token_type_ids" file="tests/test_tokenization_common.py" line="787" time="0.266" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_integration_from_extractor" file="tests/test_tokenization_common.py" line="734" time="0.925" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_padding_side_in_kwargs" file="tests/test_tokenization_common.py" line="1826" time="0.989" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_padding_to_multiple_of" file="tests/models/clip/test_tokenization_clip.py" line="46" time="0.001"><skipped type="pytest.skip" message="Skipping padding to multiple of test bc vocab is too small.">/root/project/tests/models/clip/test_tokenization_clip.py:47: Skipping padding to multiple of test bc vocab is too small.</skipped></testcase><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_pretokenized_inputs" file="tests/test_tokenization_common.py" line="2243" time="0.280" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_tokenizer_store_full_signature" file="tests/test_tokenization_common.py" line="522" time="0.434" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_tokenize_special_tokens" file="tests/test_tokenization_common.py" line="491" time="0.416" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_tokenizers_common_ids_setters" file="tests/test_tokenization_common.py" line="569" time="0.466" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_tokenizers_common_properties" file="tests/test_tokenization_common.py" line="537" time="0.232" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_bos_token_with_add_bos_token_true" file="tests/test_tokenization_common.py" line="2523" time="0.966" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_truncation_side_in_kwargs" file="tests/test_tokenization_common.py" line="1843" time="0.825" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_call" file="tests/test_tokenization_common.py" line="2126" time="0.182" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_batch_encode_plus_padding" file="tests/test_tokenization_common.py" line="2201" time="0.927" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_maximum_encoding_length_single_input" file="tests/test_tokenization_common.py" line="1435" time="2.473" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_bos_token_with_add_bos_token_false" file="tests/test_tokenization_common.py" line="2540" time="0.204" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_chat_template_dict" file="tests/test_tokenization_common.py" line="1361" time="0.375" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_model_input_names_signature" file="tests/test_tokenization_common.py" line="511" time="0.314" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_chat_template_return_assistant_tokens_mask" file="tests/test_tokenization_common.py" line="997" time="0.307" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_chat_template" file="tests/test_tokenization_common.py" line="821" time="1.286" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_chat_template_file_priority" file="tests/test_tokenization_common.py" line="1409" time="0.449" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_added_tokens_serialization" file="tests/test_tokenization_common.py" line="2428" time="1.039" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_number_of_added_tokens" file="tests/test_tokenization_common.py" line="1423" time="0.437" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_chat_template_return_assistant_tokens_mask_truncated" file="tests/test_tokenization_common.py" line="1175" time="0.451" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_internal_consistency" file="tests/test_tokenization_common.py" line="763" time="2.269" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_maximum_encoding_length_pair_input" file="tests/test_tokenization_common.py" line="1530" time="2.596" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_chat_template_batched" file="tests/test_tokenization_common.py" line="924" time="0.393" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_continue_final_message_with_decoy_earlier_message" file="tests/test_tokenization_common.py" line="1336" time="0.208" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_chat_template_save_loading" file="tests/test_tokenization_common.py" line="882" time="0.429"><skipped type="pytest.skip" message="tokenizer doesn't accept chat templates at input">/root/project/tests/test_tokenization_common.py:883: tokenizer doesn't accept chat templates at input</skipped></testcase><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_save_and_load_tokenizer" file="tests/test_tokenization_common.py" line="602" time="3.233" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_continue_final_message_with_trim" file="tests/test_tokenization_common.py" line="1306" time="0.199" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_encode_plus_with_padding_0" file="tests/test_tokenization_common.py" line="1992" time="0.201" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_continue_final_message" file="tests/test_tokenization_common.py" line="1278" time="0.568" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_jinja_loopcontrols" file="tests/test_tokenization_common.py" line="958" time="0.468" /><testcase classname="tests.models.clip.test_tokenization_clip.CLIPTokenizationTest" name="test_sequence_ids" file="tests/test_tokenization_common.py" line="798" time="0.217" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_conversion_reversible" file="tests/test_tokenization_common.py" line="2117" time="0.391" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_chat_template_dict_saving" file="tests/test_tokenization_common.py" line="1377" time="0.855" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_get_vocab" file="tests/test_tokenization_common.py" line="2104" time="0.349" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_encode_basic_padding" file="tests/test_tokenization_common.py" line="1860" time="0.380" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_empty_input_string" file="tests/test_tokenization_common.py" line="2462" time="0.190" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_integration" file="tests/test_tokenization_common.py" line="712" time="0.516" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_mask_output" file="tests/test_tokenization_common.py" line="779" time="0.212" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_batch_encode_dynamic_overflowing" file="tests/test_tokenization_common.py" line="2365" time="0.923"><skipped type="pytest.skip" message="This tokenizer has no padding token set, or pad_token_id &lt; 0">/root/project/tests/test_tokenization_common.py:2366: This tokenizer has no padding token set, or pad_token_id &lt; 0</skipped></testcase><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_encode_plus_with_padding_1" file="tests/test_tokenization_common.py" line="1992" time="0.601" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_jinja_loopcontrols" file="tests/test_tokenization_common.py" line="958" time="0.399" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_jinja_strftime" file="tests/test_tokenization_common.py" line="978" time="0.209" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_number_of_added_tokens" file="tests/test_tokenization_common.py" line="1423" time="0.388" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_batch_encode_plus_batch_sequence_length" file="tests/test_tokenization_common.py" line="2155" time="0.205" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_pad_token_initialization" file="tests/test_tokenization_common.py" line="2490" time="0.219" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_padding_with_attention_mask" file="tests/test_tokenization_common.py" line="1975" time="0.207"><skipped type="pytest.skip" message="No padding token.">/root/project/tests/test_tokenization_common.py:1976: No padding token.</skipped></testcase><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_integration_from_extractor" file="tests/test_tokenization_common.py" line="734" time="0.758" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_padding_to_multiple_of" file="tests/test_tokenization_common.py" line="1943" time="0.191"><skipped type="pytest.skip" message="No padding token.">/root/project/tests/test_tokenization_common.py:1944: No padding token.</skipped></testcase><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_model_input_names_signature" file="tests/test_tokenization_common.py" line="511" time="0.188" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_tokenizer_initialization_with_conflicting_key" file="tests/test_tokenization_common.py" line="2455" time="0.002" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_special_tokens_mask" file="tests/test_tokenization_common.py" line="1786" time="0.183" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_pretokenized_inputs" file="tests/test_tokenization_common.py" line="2243" time="0.414" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_sequence_ids" file="tests/test_tokenization_common.py" line="798" time="0.400" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_special_tokens_mask_input_pairs" file="tests/test_tokenization_common.py" line="1803" time="0.207" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_padding_side_in_kwargs" file="tests/test_tokenization_common.py" line="1826" time="0.895" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_tokenize_special_tokens" file="tests/test_tokenization_common.py" line="491" time="0.214" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_tokenizers_common_ids_setters" file="tests/test_tokenization_common.py" line="569" time="0.414" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_tokenizer_store_full_signature" file="tests/test_tokenization_common.py" line="522" time="0.391" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_prepare_seq2seq_batch" file="tests/test_tokenization_common.py" line="2323" time="0.208" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_tokenizers_common_properties" file="tests/test_tokenization_common.py" line="537" time="0.182" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_truncation_side_in_kwargs" file="tests/test_tokenization_common.py" line="1843" time="0.664" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_internal_consistency" file="tests/test_tokenization_common.py" line="763" time="2.333" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_maximum_encoding_length_pair_input" file="tests/test_tokenization_common.py" line="1530" time="2.014" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_right_and_left_truncation" file="tests/test_tokenization_common.py" line="1889" time="0.192" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_save_and_load_tokenizer" file="tests/test_tokenization_common.py" line="602" time="2.921" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_token_type_ids" file="tests/test_tokenization_common.py" line="787" time="0.179" /><testcase classname="tests.models.openai.test_tokenization_openai.OpenAIGPTTokenizationTest" name="test_maximum_encoding_length_single_input" file="tests/test_tokenization_common.py" line="1435" time="1.935" /></testsuite></testsuites>