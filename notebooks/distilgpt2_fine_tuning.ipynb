{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine-Tune DistilGPT2 and Generate Text",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b555ef26a30c432cb0b2812c7776cbd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ed454cbb7757405bb35fcb218a988f04",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6b895b33a80c47b5b09fc401974e3913",
              "IPY_MODEL_9a0e7b1223914848b09fb53eac7ae7b2"
            ]
          }
        },
        "ed454cbb7757405bb35fcb218a988f04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6b895b33a80c47b5b09fc401974e3913": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_59565629659e43819c459062377d92c7",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1042301,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1042301,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d14f007101624210af7ff92a55880026"
          }
        },
        "9a0e7b1223914848b09fb53eac7ae7b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_244b6d6ef7374e0da355b665825c71a2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 1.04M/1.04M [00:00&lt;00:00, 12.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ae08cad8c7424f189b3214a878d477f5"
          }
        },
        "59565629659e43819c459062377d92c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d14f007101624210af7ff92a55880026": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "244b6d6ef7374e0da355b665825c71a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ae08cad8c7424f189b3214a878d477f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d3ff5d92ffa34f33939c213934217915": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_99546d731cec47ae94f1d210643d4f99",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_798d6ab10350496e8c188b18f6a9bfa9",
              "IPY_MODEL_4ff7150b00334536835477025423da51"
            ]
          }
        },
        "99546d731cec47ae94f1d210643d4f99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "798d6ab10350496e8c188b18f6a9bfa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e06459405a8a41488b1038ba8a66335c",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5c4f51e4d49d44a0a23a5ba5614586e9"
          }
        },
        "4ff7150b00334536835477025423da51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9b1740ef35c4454aa43f2a668b3d25ff",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 456k/456k [00:00&lt;00:00, 6.40MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7803f1620adb4e0e8546514ffc5554a1"
          }
        },
        "e06459405a8a41488b1038ba8a66335c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5c4f51e4d49d44a0a23a5ba5614586e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9b1740ef35c4454aa43f2a668b3d25ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7803f1620adb4e0e8546514ffc5554a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C9zObeOoEoN",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Here is a tutorial about generating text using a SOTA inspired language generation model, distilgpt2. This model lighter in weight and faster in language generation than the original OpenAI GPT2. Using this tutorial, you can train a language generation model which can generate text for any subject in English. Here, we will generate movie reviews by fine-tuning distilgpt2 on a sample of IMDB movie reviews.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW8OfkKEhPPu",
        "colab_type": "text"
      },
      "source": [
        "Click on the link below and a file will be downloaded containing IMDB sample dataset of 1000 samples\n",
        "\n",
        "http://files.fast.ai/data/examples/imdb_sample.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1bteQwqhsUf",
        "colab_type": "text"
      },
      "source": [
        "Upload this file in this colab notebook using the upload button on the top left "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCqeIu66WrOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Extract the csv file from the uploaded tgz file\n",
        "\n",
        "import tarfile\n",
        "with tarfile.open('imdb_sample.tgz', 'r:gz') as tar:\n",
        "    tar.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziTUQXO7Yxve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye9GZEhpZRtz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('imdb_sample/texts.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSU8OLAhZada",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "aab63c20-b399-42ab-ced5-41feb492b1f3"
      },
      "source": [
        "### This is how the CSV look like\n",
        "data"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>is_valid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>negative</td>\n",
              "      <td>Un-bleeping-believable! Meg Ryan doesn't even ...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>This is a extremely well-made film. The acting...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negative</td>\n",
              "      <td>Every once in a long while a movie will come a...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>positive</td>\n",
              "      <td>Name just says it all. I watched this movie wi...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "      <td>This movie succeeds at being one of the most u...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>negative</td>\n",
              "      <td>There are many different versions of this one ...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>positive</td>\n",
              "      <td>Once upon a time Hollywood produced live-actio...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>negative</td>\n",
              "      <td>Wenders was great with Million $ Hotel.I don't...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>negative</td>\n",
              "      <td>Although a film with Bruce Willis is always wo...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>positive</td>\n",
              "      <td>A compelling, honest, daring, and unforgettabl...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        label                                               text  is_valid\n",
              "0    negative  Un-bleeping-believable! Meg Ryan doesn't even ...     False\n",
              "1    positive  This is a extremely well-made film. The acting...     False\n",
              "2    negative  Every once in a long while a movie will come a...     False\n",
              "3    positive  Name just says it all. I watched this movie wi...     False\n",
              "4    negative  This movie succeeds at being one of the most u...     False\n",
              "..        ...                                                ...       ...\n",
              "995  negative  There are many different versions of this one ...      True\n",
              "996  positive  Once upon a time Hollywood produced live-actio...      True\n",
              "997  negative  Wenders was great with Million $ Hotel.I don't...      True\n",
              "998  negative  Although a film with Bruce Willis is always wo...      True\n",
              "999  positive  A compelling, honest, daring, and unforgettabl...      True\n",
              "\n",
              "[1000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y55Nj02it0c",
        "colab_type": "text"
      },
      "source": [
        "Let's get the number of samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-85131-nifk5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a53fb651-5c4a-4ff4-c6dc-64c2dc6b9fc8"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRvpRcHai0qa",
        "colab_type": "text"
      },
      "source": [
        "For Finetuning distilgpt2, we just need the text field"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAbMLfxrZbKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts = list(set(data['text']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buC-SCx-Zkty",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0a1e0db9-7062-4dc4-bd47-319110dc88cf"
      },
      "source": [
        "len(texts)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUXGPS5Li-E4",
        "colab_type": "text"
      },
      "source": [
        "Store the reviews in a txt file where each line of txt file is a single review "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LotaG9qgZmHy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_name = 'testing.txt'\n",
        "with open(file_name, 'w') as f:\n",
        "    f.write(\" |EndOfText|\\n\".join(texts))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KwFvrFRjOwo",
        "colab_type": "text"
      },
      "source": [
        "Now, let's come to Transformers by Huggingface, and unleash the Transformers (Autobots... just kidding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkocIBHfaZul",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 776
        },
        "outputId": "05a04f03-8ebe-4548-c5de-30ae1951f289"
      },
      "source": [
        "!pip install transformers\n",
        "!git clone https://github.com/huggingface/transformers.git"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 9.0MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 20.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 33.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=6bae28e8c38d415033060d6c0710d1b9d8c5d208224b60f860e8a37e42f943c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.5.1\n",
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 45, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 21130 (delta 16), reused 24 (delta 5), pack-reused 21085\u001b[K\n",
            "Receiving objects: 100% (21130/21130), 12.73 MiB | 24.82 MiB/s, done.\n",
            "Resolving deltas: 100% (15221/15221), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbIJfTnDjmG6",
        "colab_type": "text"
      },
      "source": [
        "Make 2 directories. \n",
        "\n",
        "1) weights - for storing the weights of distilgpt2\n",
        "\n",
        "2) tokenizer - for storing the tokenizer of distilgpt2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNSxAcX_arFZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "9bfc5936-0657-4ba9-c1e6-196126f0b032"
      },
      "source": [
        "dir_ = \"models/\"\n",
        "!mkdir {dir_}\n",
        "dir_ = \"models/gpt2/\"\n",
        "!mkdir {dir_}\n",
        "weights_dir = \"models/gpt2/weights\"\n",
        "tokenizer_dir = \"models/gpt2/tokenizer\"\n",
        "!mkdir {weights_dir}\n",
        "!mkdir {tokenizer_dir}"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘models/’: File exists\n",
            "mkdir: cannot create directory ‘models/gpt2/’: File exists\n",
            "mkdir: cannot create directory ‘models/gpt2/tokenizer’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYMYRSfZj9Yd",
        "colab_type": "text"
      },
      "source": [
        "Store the tokenizer files in tokenizer_dir"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3INfIY1bMuR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182,
          "referenced_widgets": [
            "b555ef26a30c432cb0b2812c7776cbd6",
            "ed454cbb7757405bb35fcb218a988f04",
            "6b895b33a80c47b5b09fc401974e3913",
            "9a0e7b1223914848b09fb53eac7ae7b2",
            "59565629659e43819c459062377d92c7",
            "d14f007101624210af7ff92a55880026",
            "244b6d6ef7374e0da355b665825c71a2",
            "ae08cad8c7424f189b3214a878d477f5",
            "d3ff5d92ffa34f33939c213934217915",
            "99546d731cec47ae94f1d210643d4f99",
            "798d6ab10350496e8c188b18f6a9bfa9",
            "4ff7150b00334536835477025423da51",
            "e06459405a8a41488b1038ba8a66335c",
            "5c4f51e4d49d44a0a23a5ba5614586e9",
            "9b1740ef35c4454aa43f2a668b3d25ff",
            "7803f1620adb4e0e8546514ffc5554a1"
          ]
        },
        "outputId": "a68b7b10-615c-4a7a-e917-d887de011fa5"
      },
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
        "tokenizer.save_pretrained(tokenizer_dir)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b555ef26a30c432cb0b2812c7776cbd6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=1042301, style=ProgressStyle(description_wi…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d3ff5d92ffa34f33939c213934217915",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=456318, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('models/gpt2/tokenizer/vocab.json',\n",
              " 'models/gpt2/tokenizer/merges.txt',\n",
              " 'models/gpt2/tokenizer/special_tokens_map.json',\n",
              " 'models/gpt2/tokenizer/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1HiZgs-kFLg",
        "colab_type": "text"
      },
      "source": [
        "Now, its time for Training (or fine tuning) distilgpt2 with IMDB reviews\n",
        "Given below is a command containing few parameters to help Transformers finetune distilgpt2. now, let's understand what these parameters mean\n",
        "\n",
        "1) output_dir: It is the weights_dir we made where our finetuned model will be stored in the form of checkpoints\n",
        "\n",
        "2) tokenizer_name: It is the tokenizer_dir we made where tokenizer for distilgpt2 is stored\n",
        "\n",
        "3) line_by_line: It helps in preparation of data where each line of text is treated separately as a single observation\n",
        "\n",
        "4) model_name_or_path: It tells the kind of model we are currently dealing with\n",
        "\n",
        "5) per_gpu_train_batch_size: It tells the batch size for each gpu\n",
        "\n",
        "6) do_train: It tells pytorch to start training mode\n",
        "\n",
        "7) train_data_file: This is where we give the input text data \n",
        "\n",
        "8) num_train_epochs: Number of epochs for finetuning\n",
        "\n",
        "Rest of the parameters are self explanatory. For more information check https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py\n",
        "\n",
        "Now, let the training begin..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj0VUdVYbggE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cmd = '''\n",
        "python transformers/examples/run_language_modeling.py \\\n",
        "--output_dir {0} \\\n",
        "--tokenizer_name {1} \\\n",
        "--line_by_line \\\n",
        "--model_type gpt2 \\\n",
        "--overwrite_cache \\\n",
        "--model_name_or_path distilgpt2 \\\n",
        "--per_gpu_train_batch_size 2 \\\n",
        "--do_train \\\n",
        "--overwrite_output_dir \\\n",
        "--train_data_file testing.txt \\\n",
        "--num_train_epochs 3.0 \\\n",
        "--logging_steps 50 \\\n",
        "--save_steps 100 \\\n",
        "--save_total_limit 2 \\\n",
        "--seed 100\n",
        "'''.format(weights_dir,tokenizer_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQKT9jlOcnjY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3dcbf335-9800-45a8-a3de-8ab9cca3acdb"
      },
      "source": [
        "!{cmd}"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/07/2020 14:50:17 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "03/07/2020 14:50:17 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-config.json from cache at /root/.cache/torch/transformers/eb0f77b3f095880586731f57e2fe19060d71d1036ef8daf727bd97a17fb66a43.a41f80bd12c111d611dcd5546611b7e47c16a0a995f83df2f7b437a20b6849b5\n",
            "03/07/2020 14:50:17 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 1,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "03/07/2020 14:50:17 - INFO - transformers.tokenization_utils -   Model name 'models/gpt2/tokenizer' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'models/gpt2/tokenizer' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "03/07/2020 14:50:17 - INFO - transformers.tokenization_utils -   Didn't find file models/gpt2/tokenizer/added_tokens.json. We won't load it.\n",
            "03/07/2020 14:50:17 - INFO - transformers.tokenization_utils -   loading file models/gpt2/tokenizer/vocab.json\n",
            "03/07/2020 14:50:17 - INFO - transformers.tokenization_utils -   loading file models/gpt2/tokenizer/merges.txt\n",
            "03/07/2020 14:50:17 - INFO - transformers.tokenization_utils -   loading file None\n",
            "03/07/2020 14:50:17 - INFO - transformers.tokenization_utils -   loading file models/gpt2/tokenizer/special_tokens_map.json\n",
            "03/07/2020 14:50:17 - INFO - transformers.tokenization_utils -   loading file models/gpt2/tokenizer/tokenizer_config.json\n",
            "03/07/2020 14:50:17 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-pytorch_model.bin from cache at /root/.cache/torch/transformers/a2212aabe89bddaac8786d5937151284ae0a36a1cf96d2c0b9eb8c3dfa94fff5.ffe4c53a2a410b15148cf4170cc408d2d2f98adeecdde146ef8e71843039ff3c\n",
            "03/07/2020 14:50:26 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=1024, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=False, do_train=True, eval_all_checkpoints=False, eval_data_file=None, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, line_by_line=True, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_name_or_path='distilgpt2', model_type='gpt2', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='models/gpt2/weights', overwrite_cache=True, overwrite_output_dir=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=2, save_steps=100, save_total_limit=2, seed=100, server_ip='', server_port='', should_continue=False, tokenizer_name='models/gpt2/tokenizer', train_data_file='testing.txt', warmup_steps=0, weight_decay=0.0)\n",
            "03/07/2020 14:50:26 - INFO - __main__ -   Creating features from dataset file at testing.txt\n",
            "03/07/2020 14:50:28 - INFO - __main__ -   ***** Running training *****\n",
            "03/07/2020 14:50:28 - INFO - __main__ -     Num examples = 1026\n",
            "03/07/2020 14:50:28 - INFO - __main__ -     Num Epochs = 3\n",
            "03/07/2020 14:50:28 - INFO - __main__ -     Instantaneous batch size per GPU = 2\n",
            "03/07/2020 14:50:28 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "03/07/2020 14:50:28 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "03/07/2020 14:50:28 - INFO - __main__ -     Total optimization steps = 1539\n",
            "Epoch:   0% 0/3 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/513 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/513 [00:00<02:18,  3.71it/s]\u001b[A\n",
            "Iteration:   1% 3/513 [00:00<01:49,  4.68it/s]\u001b[A\n",
            "Iteration:   1% 4/513 [00:00<01:34,  5.36it/s]\u001b[A\n",
            "Iteration:   1% 5/513 [00:00<01:24,  6.03it/s]\u001b[A\n",
            "Iteration:   1% 7/513 [00:00<01:10,  7.16it/s]\u001b[A\n",
            "Iteration:   2% 9/513 [00:01<01:01,  8.15it/s]\u001b[A\n",
            "Iteration:   2% 11/513 [00:01<00:54,  9.25it/s]\u001b[A\n",
            "Iteration:   3% 13/513 [00:01<01:03,  7.83it/s]\u001b[A\n",
            "Iteration:   3% 15/513 [00:01<00:56,  8.77it/s]\u001b[A\n",
            "Iteration:   3% 17/513 [00:01<01:03,  7.77it/s]\u001b[A\n",
            "Iteration:   4% 19/513 [00:02<01:07,  7.28it/s]\u001b[A\n",
            "Iteration:   4% 21/513 [00:02<00:59,  8.33it/s]\u001b[A\n",
            "Iteration:   4% 22/513 [00:02<01:05,  7.50it/s]\u001b[A\n",
            "Iteration:   4% 23/513 [00:02<01:04,  7.54it/s]\u001b[A\n",
            "Iteration:   5% 24/513 [00:02<01:00,  8.04it/s]\u001b[A\n",
            "Iteration:   5% 25/513 [00:02<00:58,  8.27it/s]\u001b[A\n",
            "Iteration:   5% 26/513 [00:03<00:56,  8.60it/s]\u001b[A\n",
            "Iteration:   5% 28/513 [00:03<00:49,  9.76it/s]\u001b[A\n",
            "Iteration:   6% 30/513 [00:03<00:52,  9.17it/s]\u001b[A\n",
            "Iteration:   6% 32/513 [00:03<00:48,  9.93it/s]\u001b[A\n",
            "Iteration:   7% 34/513 [00:03<00:47, 10.03it/s]\u001b[A\n",
            "Iteration:   7% 36/513 [00:03<00:43, 10.97it/s]\u001b[A\n",
            "Iteration:   7% 38/513 [00:04<00:40, 11.65it/s]\u001b[A\n",
            "Iteration:   8% 40/513 [00:04<00:40, 11.58it/s]\u001b[A\n",
            "Iteration:   8% 42/513 [00:04<00:40, 11.73it/s]\u001b[A\n",
            "Iteration:   9% 44/513 [00:04<00:40, 11.48it/s]\u001b[A\n",
            "Iteration:   9% 46/513 [00:04<00:49,  9.43it/s]\u001b[A\n",
            "Iteration:   9% 48/513 [00:05<00:56,  8.27it/s]\u001b[A/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "\n",
            "Iteration:  10% 50/513 [00:05<00:50,  9.23it/s]\u001b[A\n",
            "Iteration:  10% 52/513 [00:05<00:59,  7.80it/s]\u001b[A\n",
            "Iteration:  11% 54/513 [00:05<00:56,  8.11it/s]\u001b[A\n",
            "Iteration:  11% 55/513 [00:06<00:57,  7.98it/s]\u001b[A\n",
            "Iteration:  11% 57/513 [00:06<00:51,  8.94it/s]\u001b[A\n",
            "Iteration:  11% 58/513 [00:06<00:58,  7.80it/s]\u001b[A\n",
            "Iteration:  12% 60/513 [00:06<01:05,  6.89it/s]\u001b[A\n",
            "Iteration:  12% 61/513 [00:06<01:09,  6.52it/s]\u001b[A\n",
            "Iteration:  12% 63/513 [00:07<00:58,  7.64it/s]\u001b[A\n",
            "Iteration:  12% 64/513 [00:07<01:00,  7.43it/s]\u001b[A\n",
            "Iteration:  13% 66/513 [00:07<01:03,  7.09it/s]\u001b[A\n",
            "Iteration:  13% 67/513 [00:07<01:01,  7.30it/s]\u001b[A\n",
            "Iteration:  13% 68/513 [00:07<00:57,  7.73it/s]\u001b[A\n",
            "Iteration:  13% 69/513 [00:08<01:03,  7.04it/s]\u001b[A\n",
            "Iteration:  14% 71/513 [00:08<00:54,  8.09it/s]\u001b[A\n",
            "Iteration:  14% 73/513 [00:08<00:48,  9.05it/s]\u001b[A\n",
            "Iteration:  14% 74/513 [00:08<01:11,  6.15it/s]\u001b[A\n",
            "Iteration:  15% 76/513 [00:08<01:00,  7.25it/s]\u001b[A\n",
            "Iteration:  15% 77/513 [00:08<01:06,  6.51it/s]\u001b[A\n",
            "Iteration:  15% 78/513 [00:09<01:04,  6.79it/s]\u001b[A\n",
            "Iteration:  16% 80/513 [00:09<01:05,  6.58it/s]\u001b[A\n",
            "Iteration:  16% 82/513 [00:09<00:59,  7.21it/s]\u001b[A\n",
            "Iteration:  16% 83/513 [00:09<01:01,  6.96it/s]\u001b[A\n",
            "Iteration:  17% 85/513 [00:09<00:54,  7.90it/s]\u001b[A\n",
            "Iteration:  17% 86/513 [00:10<01:02,  6.85it/s]\u001b[A\n",
            "Iteration:  17% 87/513 [00:10<01:00,  7.05it/s]\u001b[A\n",
            "Iteration:  17% 89/513 [00:10<00:52,  8.05it/s]\u001b[A\n",
            "Iteration:  18% 91/513 [00:10<00:46,  9.07it/s]\u001b[A\n",
            "Iteration:  18% 93/513 [00:10<00:57,  7.35it/s]\u001b[A\n",
            "Iteration:  18% 94/513 [00:11<00:55,  7.58it/s]\u001b[A\n",
            "Iteration:  19% 96/513 [00:11<00:57,  7.30it/s]\u001b[A\n",
            "Iteration:  19% 97/513 [00:11<00:54,  7.64it/s]\u001b[A\n",
            "Iteration:  19% 98/513 [00:11<01:08,  6.09it/s]\u001b[A03/07/2020 14:50:40 - INFO - transformers.configuration_utils -   Configuration saved in models/gpt2/weights/checkpoint-100/config.json\n",
            "03/07/2020 14:50:41 - INFO - transformers.modeling_utils -   Model weights saved in models/gpt2/weights/checkpoint-100/pytorch_model.bin\n",
            "03/07/2020 14:50:41 - INFO - __main__ -   Saving model checkpoint to models/gpt2/weights/checkpoint-100\n",
            "03/07/2020 14:50:44 - INFO - __main__ -   Saving optimizer and scheduler states to models/gpt2/weights/checkpoint-100\n",
            "\n",
            "Iteration:  19% 100/513 [00:15<04:33,  1.51it/s]\u001b[A\n",
            "Iteration:  20% 101/513 [00:15<03:23,  2.02it/s]\u001b[A\n",
            "Iteration:  20% 103/513 [00:15<02:30,  2.72it/s]\u001b[A\n",
            "Iteration:  20% 105/513 [00:15<01:57,  3.47it/s]\u001b[A\n",
            "Iteration:  21% 107/513 [00:16<01:32,  4.37it/s]\u001b[A\n",
            "Iteration:  21% 109/513 [00:16<01:13,  5.50it/s]\u001b[A\n",
            "Iteration:  22% 111/513 [00:16<01:03,  6.29it/s]\u001b[A\n",
            "Iteration:  22% 113/513 [00:16<01:05,  6.12it/s]\u001b[A\n",
            "Iteration:  22% 114/513 [00:17<01:16,  5.24it/s]\u001b[A\n",
            "Iteration:  23% 116/513 [00:17<01:06,  6.00it/s]\u001b[A\n",
            "Iteration:  23% 117/513 [00:17<01:04,  6.15it/s]\u001b[A\n",
            "Iteration:  23% 118/513 [00:17<01:06,  5.91it/s]\u001b[A\n",
            "Iteration:  23% 119/513 [00:17<01:08,  5.76it/s]\u001b[A\n",
            "Iteration:  24% 121/513 [00:17<00:56,  6.97it/s]\u001b[A\n",
            "Iteration:  24% 123/513 [00:18<00:55,  7.03it/s]\u001b[A\n",
            "Iteration:  24% 124/513 [00:18<00:58,  6.60it/s]\u001b[A\n",
            "Iteration:  24% 125/513 [00:18<01:09,  5.57it/s]\u001b[A\n",
            "Iteration:  25% 127/513 [00:18<00:58,  6.62it/s]\u001b[A\n",
            "Iteration:  25% 129/513 [00:18<00:50,  7.62it/s]\u001b[A\n",
            "Iteration:  25% 130/513 [00:19<01:03,  6.04it/s]\u001b[A\n",
            "Iteration:  26% 131/513 [00:19<01:04,  5.96it/s]\u001b[A\n",
            "Iteration:  26% 133/513 [00:19<01:04,  5.90it/s]\u001b[A\n",
            "Iteration:  26% 134/513 [00:19<01:01,  6.17it/s]\u001b[A\n",
            "Iteration:  27% 136/513 [00:20<00:51,  7.28it/s]\u001b[A\n",
            "Iteration:  27% 138/513 [00:20<00:45,  8.32it/s]\u001b[A\n",
            "Iteration:  27% 140/513 [00:20<00:46,  8.07it/s]\u001b[A\n",
            "Iteration:  27% 141/513 [00:20<01:01,  6.07it/s]\u001b[A\n",
            "Iteration:  28% 142/513 [00:20<01:03,  5.87it/s]\u001b[A\n",
            "Iteration:  28% 143/513 [00:20<00:57,  6.49it/s]\u001b[A\n",
            "Iteration:  28% 144/513 [00:21<01:09,  5.34it/s]\u001b[A\n",
            "Iteration:  28% 145/513 [00:21<01:13,  4.99it/s]\u001b[A\n",
            "Iteration:  29% 147/513 [00:21<01:10,  5.20it/s]\u001b[A\n",
            "Iteration:  29% 148/513 [00:21<01:04,  5.69it/s]\u001b[A\n",
            "Iteration:  29% 149/513 [00:22<00:56,  6.48it/s]\u001b[A\n",
            "Iteration:  29% 151/513 [00:22<00:46,  7.80it/s]\u001b[A\n",
            "Iteration:  30% 152/513 [00:22<00:47,  7.63it/s]\u001b[A\n",
            "Iteration:  30% 154/513 [00:22<00:44,  8.13it/s]\u001b[A\n",
            "Iteration:  30% 156/513 [00:22<00:49,  7.14it/s]\u001b[A\n",
            "Iteration:  31% 157/513 [00:23<00:45,  7.78it/s]\u001b[A\n",
            "Iteration:  31% 159/513 [00:23<00:40,  8.74it/s]\u001b[A\n",
            "Iteration:  31% 161/513 [00:23<00:36,  9.72it/s]\u001b[A\n",
            "Iteration:  32% 163/513 [00:23<00:40,  8.68it/s]\u001b[A\n",
            "Iteration:  32% 164/513 [00:23<00:58,  5.93it/s]\u001b[A\n",
            "Iteration:  32% 165/513 [00:24<01:06,  5.21it/s]\u001b[A\n",
            "Iteration:  32% 166/513 [00:24<00:58,  5.93it/s]\u001b[A\n",
            "Iteration:  33% 167/513 [00:24<00:54,  6.33it/s]\u001b[A\n",
            "Iteration:  33% 168/513 [00:24<00:55,  6.26it/s]\u001b[A\n",
            "Iteration:  33% 170/513 [00:24<00:47,  7.25it/s]\u001b[A\n",
            "Iteration:  33% 171/513 [00:24<00:43,  7.78it/s]\u001b[A\n",
            "Iteration:  34% 172/513 [00:25<00:55,  6.11it/s]\u001b[A\n",
            "Iteration:  34% 174/513 [00:25<00:47,  7.18it/s]\u001b[A\n",
            "Iteration:  34% 175/513 [00:25<00:58,  5.80it/s]\u001b[A\n",
            "Iteration:  35% 177/513 [00:25<00:48,  6.94it/s]\u001b[A\n",
            "Iteration:  35% 179/513 [00:25<00:46,  7.14it/s]\u001b[A\n",
            "Iteration:  35% 181/513 [00:26<00:44,  7.49it/s]\u001b[A\n",
            "Iteration:  35% 182/513 [00:26<00:42,  7.73it/s]\u001b[A\n",
            "Iteration:  36% 184/513 [00:26<00:38,  8.48it/s]\u001b[A\n",
            "Iteration:  36% 185/513 [00:26<00:39,  8.39it/s]\u001b[A\n",
            "Iteration:  36% 187/513 [00:26<00:34,  9.35it/s]\u001b[A\n",
            "Iteration:  37% 189/513 [00:27<00:41,  7.72it/s]\u001b[A\n",
            "Iteration:  37% 191/513 [00:27<00:37,  8.62it/s]\u001b[A\n",
            "Iteration:  38% 193/513 [00:27<00:35,  9.09it/s]\u001b[A\n",
            "Iteration:  38% 195/513 [00:27<00:32,  9.75it/s]\u001b[A\n",
            "Iteration:  38% 197/513 [00:27<00:33,  9.49it/s]\u001b[A\n",
            "Iteration:  39% 198/513 [00:28<00:47,  6.66it/s]\u001b[A\n",
            "Iteration:  39% 199/513 [00:28<00:56,  5.52it/s]\u001b[A03/07/2020 14:50:57 - INFO - transformers.configuration_utils -   Configuration saved in models/gpt2/weights/checkpoint-200/config.json\n",
            "03/07/2020 14:50:58 - INFO - transformers.modeling_utils -   Model weights saved in models/gpt2/weights/checkpoint-200/pytorch_model.bin\n",
            "03/07/2020 14:50:58 - INFO - __main__ -   Saving model checkpoint to models/gpt2/weights/checkpoint-200\n",
            "03/07/2020 14:51:00 - INFO - __main__ -   Saving optimizer and scheduler states to models/gpt2/weights/checkpoint-200\n",
            "\n",
            "Iteration:  39% 200/513 [00:31<06:16,  1.20s/it]\u001b[A\n",
            "Iteration:  39% 201/513 [00:32<04:38,  1.12it/s]\u001b[A\n",
            "Iteration:  39% 202/513 [00:32<03:23,  1.53it/s]\u001b[A\n",
            "Iteration:  40% 204/513 [00:32<02:30,  2.06it/s]\u001b[A\n",
            "Iteration:  40% 205/513 [00:32<01:55,  2.66it/s]\u001b[A\n",
            "Iteration:  40% 207/513 [00:32<01:28,  3.46it/s]\u001b[A\n",
            "Iteration:  41% 209/513 [00:32<01:09,  4.36it/s]\u001b[A\n",
            "Iteration:  41% 210/513 [00:33<01:02,  4.88it/s]\u001b[A\n",
            "Iteration:  41% 211/513 [00:33<00:53,  5.62it/s]\u001b[A\n",
            "Iteration:  42% 213/513 [00:33<00:45,  6.57it/s]\u001b[A\n",
            "Iteration:  42% 214/513 [00:33<00:42,  7.10it/s]\u001b[A\n",
            "Iteration:  42% 216/513 [00:33<00:35,  8.26it/s]\u001b[A\n",
            "Iteration:  42% 218/513 [00:34<00:43,  6.71it/s]\u001b[A\n",
            "Iteration:  43% 220/513 [00:34<00:36,  8.04it/s]\u001b[A\n",
            "Iteration:  43% 222/513 [00:34<00:35,  8.26it/s]\u001b[A\n",
            "Iteration:  44% 224/513 [00:34<00:39,  7.29it/s]\u001b[A\n",
            "Iteration:  44% 225/513 [00:34<00:42,  6.72it/s]\u001b[A\n",
            "Iteration:  44% 226/513 [00:35<00:42,  6.75it/s]\u001b[A\n",
            "Iteration:  44% 227/513 [00:35<00:38,  7.42it/s]\u001b[A\n",
            "Iteration:  45% 229/513 [00:35<00:32,  8.75it/s]\u001b[A\n",
            "Iteration:  45% 231/513 [00:35<00:36,  7.72it/s]\u001b[A\n",
            "Iteration:  45% 232/513 [00:35<00:33,  8.29it/s]\u001b[A\n",
            "Iteration:  46% 234/513 [00:35<00:31,  8.97it/s]\u001b[A\n",
            "Iteration:  46% 235/513 [00:36<00:36,  7.69it/s]\u001b[A\n",
            "Iteration:  46% 236/513 [00:36<00:37,  7.34it/s]\u001b[A\n",
            "Iteration:  46% 237/513 [00:36<00:40,  6.74it/s]\u001b[A\n",
            "Iteration:  46% 238/513 [00:36<00:39,  6.91it/s]\u001b[A\n",
            "Iteration:  47% 239/513 [00:36<00:36,  7.51it/s]\u001b[A\n",
            "Iteration:  47% 240/513 [00:36<00:33,  8.04it/s]\u001b[A\n",
            "Iteration:  47% 242/513 [00:36<00:31,  8.54it/s]\u001b[A\n",
            "Iteration:  48% 244/513 [00:37<00:30,  8.77it/s]\u001b[A\n",
            "Iteration:  48% 246/513 [00:37<00:28,  9.41it/s]\u001b[A\n",
            "Iteration:  48% 248/513 [00:37<00:26, 10.18it/s]\u001b[A\n",
            "Iteration:  49% 250/513 [00:37<00:28,  9.08it/s]\u001b[A\n",
            "Iteration:  49% 251/513 [00:37<00:30,  8.50it/s]\u001b[A\n",
            "Iteration:  49% 252/513 [00:38<00:29,  8.82it/s]\u001b[A\n",
            "Iteration:  49% 253/513 [00:38<00:39,  6.53it/s]\u001b[A\n",
            "Iteration:  50% 254/513 [00:38<00:40,  6.38it/s]\u001b[A\n",
            "Iteration:  50% 256/513 [00:38<00:33,  7.58it/s]\u001b[A\n",
            "Iteration:  50% 258/513 [00:38<00:31,  7.99it/s]\u001b[A\n",
            "Iteration:  50% 259/513 [00:38<00:30,  8.24it/s]\u001b[A\n",
            "Iteration:  51% 260/513 [00:39<00:42,  5.95it/s]\u001b[A\n",
            "Iteration:  51% 261/513 [00:39<00:37,  6.68it/s]\u001b[A\n",
            "Iteration:  51% 262/513 [00:39<00:44,  5.62it/s]\u001b[A\n",
            "Iteration:  51% 263/513 [00:39<00:48,  5.17it/s]\u001b[A\n",
            "Iteration:  51% 264/513 [00:39<00:42,  5.87it/s]\u001b[A\n",
            "Iteration:  52% 265/513 [00:39<00:38,  6.42it/s]\u001b[A\n",
            "Iteration:  52% 267/513 [00:40<00:33,  7.44it/s]\u001b[A\n",
            "Iteration:  52% 268/513 [00:40<00:34,  7.20it/s]\u001b[A\n",
            "Iteration:  52% 269/513 [00:40<00:33,  7.31it/s]\u001b[A\n",
            "Iteration:  53% 271/513 [00:40<00:29,  8.33it/s]\u001b[A\n",
            "Iteration:  53% 272/513 [00:40<00:28,  8.42it/s]\u001b[A\n",
            "Iteration:  53% 274/513 [00:41<00:29,  8.04it/s]\u001b[A\n",
            "Iteration:  54% 276/513 [00:41<00:31,  7.43it/s]\u001b[A\n",
            "Iteration:  54% 278/513 [00:41<00:27,  8.62it/s]\u001b[A\n",
            "Iteration:  55% 280/513 [00:41<00:25,  8.97it/s]\u001b[A\n",
            "Iteration:  55% 281/513 [00:41<00:27,  8.43it/s]\u001b[A\n",
            "Iteration:  55% 283/513 [00:41<00:25,  8.98it/s]\u001b[A\n",
            "Iteration:  55% 284/513 [00:42<00:27,  8.31it/s]\u001b[A\n",
            "Iteration:  56% 286/513 [00:42<00:25,  8.95it/s]\u001b[A\n",
            "Iteration:  56% 287/513 [00:42<00:34,  6.59it/s]\u001b[A\n",
            "Iteration:  56% 289/513 [00:42<00:28,  7.87it/s]\u001b[A\n",
            "Iteration:  57% 291/513 [00:42<00:25,  8.55it/s]\u001b[A\n",
            "Iteration:  57% 293/513 [00:43<00:24,  9.05it/s]\u001b[A\n",
            "Iteration:  58% 295/513 [00:43<00:23,  9.45it/s]\u001b[A\n",
            "Iteration:  58% 297/513 [00:43<00:25,  8.55it/s]\u001b[A\n",
            "Iteration:  58% 298/513 [00:43<00:33,  6.41it/s]\u001b[A\n",
            "Iteration:  58% 299/513 [00:44<00:38,  5.51it/s]\u001b[A03/07/2020 14:51:13 - INFO - transformers.configuration_utils -   Configuration saved in models/gpt2/weights/checkpoint-300/config.json\n",
            "03/07/2020 14:51:14 - INFO - transformers.modeling_utils -   Model weights saved in models/gpt2/weights/checkpoint-300/pytorch_model.bin\n",
            "03/07/2020 14:51:14 - INFO - __main__ -   Saving model checkpoint to models/gpt2/weights/checkpoint-300\n",
            "03/07/2020 14:51:14 - INFO - __main__ -   Deleting older checkpoint [models/gpt2/weights/checkpoint-100] due to args.save_total_limit\n",
            "03/07/2020 14:51:29 - INFO - __main__ -   Saving optimizer and scheduler states to models/gpt2/weights/checkpoint-300\n",
            "\n",
            "Iteration:  58% 300/513 [01:00<17:57,  5.06s/it]\u001b[A\n",
            "Iteration:  59% 301/513 [01:00<12:37,  3.57s/it]\u001b[A\n",
            "Iteration:  59% 302/513 [01:00<08:55,  2.54s/it]\u001b[A\n",
            "Iteration:  59% 304/513 [01:00<06:18,  1.81s/it]\u001b[A\n",
            "Iteration:  59% 305/513 [01:01<04:35,  1.32s/it]\u001b[A\n",
            "Iteration:  60% 307/513 [01:01<03:16,  1.05it/s]\u001b[A\n",
            "Iteration:  60% 308/513 [01:01<02:23,  1.43it/s]\u001b[A\n",
            "Iteration:  60% 309/513 [01:01<01:46,  1.92it/s]\u001b[A\n",
            "Iteration:  61% 311/513 [01:01<01:18,  2.56it/s]\u001b[A\n",
            "Iteration:  61% 312/513 [01:01<01:05,  3.05it/s]\u001b[A\n",
            "Iteration:  61% 313/513 [01:02<00:53,  3.72it/s]\u001b[A\n",
            "Iteration:  61% 314/513 [01:02<00:47,  4.16it/s]\u001b[A\n",
            "Iteration:  61% 315/513 [01:02<00:46,  4.25it/s]\u001b[A\n",
            "Iteration:  62% 316/513 [01:02<00:40,  4.91it/s]\u001b[A\n",
            "Iteration:  62% 317/513 [01:02<00:34,  5.66it/s]\u001b[A\n",
            "Iteration:  62% 319/513 [01:02<00:29,  6.67it/s]\u001b[A\n",
            "Iteration:  63% 321/513 [01:02<00:24,  7.99it/s]\u001b[A\n",
            "Iteration:  63% 323/513 [01:03<00:26,  7.25it/s]\u001b[A\n",
            "Iteration:  63% 325/513 [01:03<00:23,  8.12it/s]\u001b[A\n",
            "Iteration:  64% 326/513 [01:03<00:21,  8.51it/s]\u001b[A\n",
            "Iteration:  64% 327/513 [01:03<00:24,  7.74it/s]\u001b[A\n",
            "Iteration:  64% 328/513 [01:03<00:22,  8.07it/s]\u001b[A\n",
            "Iteration:  64% 329/513 [01:03<00:23,  7.86it/s]\u001b[A\n",
            "Iteration:  64% 330/513 [01:04<00:23,  7.82it/s]\u001b[A\n",
            "Iteration:  65% 331/513 [01:04<00:25,  7.18it/s]\u001b[A\n",
            "Iteration:  65% 332/513 [01:04<00:30,  5.85it/s]\u001b[A\n",
            "Iteration:  65% 333/513 [01:04<00:27,  6.47it/s]\u001b[A\n",
            "Iteration:  65% 334/513 [01:04<00:27,  6.51it/s]\u001b[A\n",
            "Iteration:  65% 335/513 [01:04<00:29,  6.09it/s]\u001b[A\n",
            "Iteration:  66% 337/513 [01:05<00:24,  7.28it/s]\u001b[A\n",
            "Iteration:  66% 339/513 [01:05<00:25,  6.94it/s]\u001b[A\n",
            "Iteration:  66% 340/513 [01:05<00:30,  5.75it/s]\u001b[A\n",
            "Iteration:  66% 341/513 [01:05<00:28,  6.06it/s]\u001b[A\n",
            "Iteration:  67% 342/513 [01:05<00:26,  6.48it/s]\u001b[A\n",
            "Iteration:  67% 343/513 [01:06<00:23,  7.20it/s]\u001b[A\n",
            "Iteration:  67% 345/513 [01:06<00:20,  8.38it/s]\u001b[A\n",
            "Iteration:  67% 346/513 [01:06<00:21,  7.72it/s]\u001b[A\n",
            "Iteration:  68% 347/513 [01:06<00:21,  7.72it/s]\u001b[A\n",
            "Iteration:  68% 349/513 [01:06<00:18,  8.78it/s]\u001b[A\n",
            "Iteration:  68% 351/513 [01:06<00:18,  8.53it/s]\u001b[A\n",
            "Iteration:  69% 352/513 [01:07<00:24,  6.56it/s]\u001b[A\n",
            "Iteration:  69% 353/513 [01:07<00:24,  6.41it/s]\u001b[A\n",
            "Iteration:  69% 355/513 [01:07<00:20,  7.73it/s]\u001b[A\n",
            "Iteration:  69% 356/513 [01:07<00:25,  6.12it/s]\u001b[A\n",
            "Iteration:  70% 358/513 [01:07<00:21,  7.26it/s]\u001b[A\n",
            "Iteration:  70% 360/513 [01:08<00:19,  7.66it/s]\u001b[A\n",
            "Iteration:  70% 361/513 [01:08<00:18,  8.15it/s]\u001b[A\n",
            "Iteration:  71% 363/513 [01:08<00:19,  7.80it/s]\u001b[A\n",
            "Iteration:  71% 365/513 [01:08<00:17,  8.33it/s]\u001b[A\n",
            "Iteration:  72% 367/513 [01:08<00:16,  8.70it/s]\u001b[A\n",
            "Iteration:  72% 369/513 [01:09<00:17,  8.23it/s]\u001b[A\n",
            "Iteration:  72% 370/513 [01:09<00:21,  6.77it/s]\u001b[A\n",
            "Iteration:  72% 371/513 [01:09<00:19,  7.29it/s]\u001b[A\n",
            "Iteration:  73% 373/513 [01:09<00:17,  7.90it/s]\u001b[A\n",
            "Iteration:  73% 375/513 [01:09<00:14,  9.30it/s]\u001b[A\n",
            "Iteration:  73% 377/513 [01:09<00:14,  9.38it/s]\u001b[A\n",
            "Iteration:  74% 379/513 [01:10<00:14,  9.05it/s]\u001b[A\n",
            "Iteration:  74% 380/513 [01:10<00:18,  7.11it/s]\u001b[A\n",
            "Iteration:  74% 381/513 [01:10<00:17,  7.51it/s]\u001b[A\n",
            "Iteration:  75% 383/513 [01:10<00:15,  8.58it/s]\u001b[A\n",
            "Iteration:  75% 385/513 [01:10<00:13,  9.34it/s]\u001b[A\n",
            "Iteration:  75% 387/513 [01:11<00:14,  8.74it/s]\u001b[A\n",
            "Iteration:  76% 388/513 [01:11<00:17,  7.34it/s]\u001b[A\n",
            "Iteration:  76% 390/513 [01:11<00:15,  7.89it/s]\u001b[A\n",
            "Iteration:  76% 392/513 [01:11<00:13,  8.86it/s]\u001b[A\n",
            "Iteration:  77% 394/513 [01:11<00:13,  9.12it/s]\u001b[A\n",
            "Iteration:  77% 395/513 [01:12<00:14,  8.32it/s]\u001b[A\n",
            "Iteration:  77% 397/513 [01:12<00:13,  8.44it/s]\u001b[A\n",
            "Iteration:  78% 399/513 [01:12<00:13,  8.74it/s]\u001b[A03/07/2020 14:51:41 - INFO - transformers.configuration_utils -   Configuration saved in models/gpt2/weights/checkpoint-400/config.json\n",
            "03/07/2020 14:51:42 - INFO - transformers.modeling_utils -   Model weights saved in models/gpt2/weights/checkpoint-400/pytorch_model.bin\n",
            "03/07/2020 14:51:42 - INFO - __main__ -   Saving model checkpoint to models/gpt2/weights/checkpoint-400\n",
            "03/07/2020 14:51:42 - INFO - __main__ -   Deleting older checkpoint [models/gpt2/weights/checkpoint-200] due to args.save_total_limit\n",
            "03/07/2020 14:51:49 - INFO - __main__ -   Saving optimizer and scheduler states to models/gpt2/weights/checkpoint-400\n",
            "\n",
            "Iteration:  78% 400/513 [01:20<04:55,  2.61s/it]\u001b[A\n",
            "Iteration:  78% 402/513 [01:21<03:28,  1.88s/it]\u001b[A\n",
            "Iteration:  79% 404/513 [01:21<02:26,  1.35s/it]\u001b[A\n",
            "Iteration:  79% 405/513 [01:21<01:46,  1.02it/s]\u001b[A\n",
            "Iteration:  79% 406/513 [01:21<01:17,  1.38it/s]\u001b[A\n",
            "Iteration:  79% 407/513 [01:21<00:57,  1.83it/s]\u001b[A\n",
            "Iteration:  80% 409/513 [01:22<00:41,  2.48it/s]\u001b[A\n",
            "Iteration:  80% 411/513 [01:22<00:32,  3.17it/s]\u001b[A\n",
            "Iteration:  81% 413/513 [01:22<00:24,  4.09it/s]\u001b[A\n",
            "Iteration:  81% 415/513 [01:22<00:20,  4.70it/s]\u001b[A\n",
            "Iteration:  81% 416/513 [01:22<00:17,  5.55it/s]\u001b[A\n",
            "Iteration:  81% 418/513 [01:22<00:15,  6.29it/s]\u001b[A\n",
            "Iteration:  82% 420/513 [01:23<00:13,  7.04it/s]\u001b[A\n",
            "Iteration:  82% 422/513 [01:23<00:11,  8.06it/s]\u001b[A\n",
            "Iteration:  83% 424/513 [01:23<00:12,  7.12it/s]\u001b[A\n",
            "Iteration:  83% 426/513 [01:24<00:13,  6.65it/s]\u001b[A\n",
            "Iteration:  83% 427/513 [01:24<00:15,  5.60it/s]\u001b[A\n",
            "Iteration:  83% 428/513 [01:24<00:14,  5.97it/s]\u001b[A\n",
            "Iteration:  84% 429/513 [01:24<00:14,  5.81it/s]\u001b[A\n",
            "Iteration:  84% 430/513 [01:24<00:13,  6.34it/s]\u001b[A\n",
            "Iteration:  84% 431/513 [01:24<00:14,  5.60it/s]\u001b[A\n",
            "Iteration:  84% 433/513 [01:25<00:12,  6.42it/s]\u001b[A\n",
            "Iteration:  85% 435/513 [01:25<00:11,  6.82it/s]\u001b[A\n",
            "Iteration:  85% 437/513 [01:25<00:09,  8.15it/s]\u001b[A\n",
            "Iteration:  86% 439/513 [01:25<00:07,  9.56it/s]\u001b[A\n",
            "Iteration:  86% 441/513 [01:25<00:08,  8.56it/s]\u001b[A\n",
            "Iteration:  86% 443/513 [01:26<00:08,  8.14it/s]\u001b[A\n",
            "Iteration:  87% 444/513 [01:26<00:08,  8.29it/s]\u001b[A\n",
            "Iteration:  87% 445/513 [01:26<00:07,  8.67it/s]\u001b[A\n",
            "Iteration:  87% 447/513 [01:26<00:06,  9.89it/s]\u001b[A\n",
            "Iteration:  88% 449/513 [01:26<00:06,  9.27it/s]\u001b[A\n",
            "Iteration:  88% 451/513 [01:27<00:07,  8.75it/s]\u001b[A\n",
            "Iteration:  88% 453/513 [01:27<00:06,  9.50it/s]\u001b[A\n",
            "Iteration:  89% 455/513 [01:27<00:06,  9.29it/s]\u001b[A\n",
            "Iteration:  89% 456/513 [01:27<00:07,  7.33it/s]\u001b[A\n",
            "Iteration:  89% 458/513 [01:27<00:06,  8.10it/s]\u001b[A\n",
            "Iteration:  90% 460/513 [01:28<00:06,  7.67it/s]\u001b[A\n",
            "Iteration:  90% 462/513 [01:28<00:06,  7.99it/s]\u001b[A\n",
            "Iteration:  90% 463/513 [01:28<00:08,  6.22it/s]\u001b[A\n",
            "Iteration:  90% 464/513 [01:28<00:09,  5.37it/s]\u001b[A\n",
            "Iteration:  91% 466/513 [01:29<00:08,  5.57it/s]\u001b[A\n",
            "Iteration:  91% 467/513 [01:29<00:08,  5.37it/s]\u001b[A\n",
            "Iteration:  91% 468/513 [01:29<00:08,  5.43it/s]\u001b[A\n",
            "Iteration:  92% 470/513 [01:29<00:06,  6.48it/s]\u001b[A\n",
            "Iteration:  92% 472/513 [01:30<00:06,  6.25it/s]\u001b[A\n",
            "Iteration:  92% 473/513 [01:30<00:07,  5.37it/s]\u001b[A\n",
            "Iteration:  92% 474/513 [01:30<00:06,  6.22it/s]\u001b[A\n",
            "Iteration:  93% 476/513 [01:30<00:05,  7.03it/s]\u001b[A\n",
            "Iteration:  93% 478/513 [01:30<00:04,  7.19it/s]\u001b[A\n",
            "Iteration:  94% 480/513 [01:31<00:04,  7.90it/s]\u001b[A\n",
            "Iteration:  94% 482/513 [01:31<00:03,  8.32it/s]\u001b[A\n",
            "Iteration:  94% 483/513 [01:31<00:03,  8.27it/s]\u001b[A\n",
            "Iteration:  95% 485/513 [01:31<00:03,  9.21it/s]\u001b[A\n",
            "Iteration:  95% 486/513 [01:31<00:03,  8.25it/s]\u001b[A\n",
            "Iteration:  95% 488/513 [01:31<00:02,  9.09it/s]\u001b[A\n",
            "Iteration:  95% 489/513 [01:32<00:03,  7.59it/s]\u001b[A\n",
            "Iteration:  96% 491/513 [01:32<00:02,  8.43it/s]\u001b[A\n",
            "Iteration:  96% 492/513 [01:32<00:03,  6.92it/s]\u001b[A\n",
            "Iteration:  96% 494/513 [01:32<00:02,  7.41it/s]\u001b[A\n",
            "Iteration:  96% 495/513 [01:32<00:02,  7.20it/s]\u001b[A\n",
            "Iteration:  97% 496/513 [01:33<00:02,  5.86it/s]\u001b[A\n",
            "Iteration:  97% 497/513 [01:33<00:02,  5.66it/s]\u001b[A\n",
            "Iteration:  97% 499/513 [01:33<00:02,  5.76it/s]\u001b[A03/07/2020 14:52:02 - INFO - transformers.configuration_utils -   Configuration saved in models/gpt2/weights/checkpoint-500/config.json\n",
            "03/07/2020 14:52:03 - INFO - transformers.modeling_utils -   Model weights saved in models/gpt2/weights/checkpoint-500/pytorch_model.bin\n",
            "03/07/2020 14:52:03 - INFO - __main__ -   Saving model checkpoint to models/gpt2/weights/checkpoint-500\n",
            "03/07/2020 14:52:03 - INFO - __main__ -   Deleting older checkpoint [models/gpt2/weights/checkpoint-300] due to args.save_total_limit\n",
            "03/07/2020 14:52:16 - INFO - __main__ -   Saving optimizer and scheduler states to models/gpt2/weights/checkpoint-500\n",
            "\n",
            "Iteration:  97% 500/513 [01:47<00:56,  4.36s/it]\u001b[A\n",
            "Iteration:  98% 501/513 [01:47<00:37,  3.09s/it]\u001b[A\n",
            "Iteration:  98% 502/513 [01:47<00:24,  2.19s/it]\u001b[A\n",
            "Iteration:  98% 503/513 [01:48<00:16,  1.61s/it]\u001b[A\n",
            "Iteration:  98% 505/513 [01:48<00:09,  1.15s/it]\u001b[A\n",
            "Iteration:  99% 506/513 [01:48<00:05,  1.19it/s]\u001b[A\n",
            "Iteration:  99% 508/513 [01:48<00:03,  1.62it/s]\u001b[A\n",
            "Iteration:  99% 510/513 [01:48<00:01,  2.17it/s]\u001b[A\n",
            "Iteration: 100% 511/513 [01:49<00:00,  2.79it/s]\u001b[A\n",
            "Iteration: 100% 513/513 [01:49<00:00,  3.50it/s]\u001b[A\n",
            "Epoch:  33% 1/3 [01:49<03:38, 109.27s/it]\n",
            "Iteration:   0% 0/513 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/513 [00:00<02:07,  4.01it/s]\u001b[A\n",
            "Iteration:   0% 2/513 [00:00<01:52,  4.54it/s]\u001b[A\n",
            "Iteration:   1% 4/513 [00:00<01:29,  5.66it/s]\u001b[A\n",
            "Iteration:   1% 5/513 [00:00<01:18,  6.49it/s]\u001b[A\n",
            "Iteration:   1% 6/513 [00:00<01:26,  5.87it/s]\u001b[A\n",
            "Iteration:   1% 7/513 [00:01<01:29,  5.64it/s]\u001b[A\n",
            "Iteration:   2% 9/513 [00:01<01:15,  6.68it/s]\u001b[A\n",
            "Iteration:   2% 11/513 [00:01<01:07,  7.41it/s]\u001b[A\n",
            "Iteration:   2% 12/513 [00:01<01:18,  6.38it/s]\u001b[A\n",
            "Iteration:   3% 13/513 [00:01<01:28,  5.65it/s]\u001b[A\n",
            "Iteration:   3% 15/513 [00:02<01:24,  5.93it/s]\u001b[A\n",
            "Iteration:   3% 17/513 [00:02<01:24,  5.84it/s]\u001b[A\n",
            "Iteration:   4% 19/513 [00:02<01:10,  7.05it/s]\u001b[A\n",
            "Iteration:   4% 21/513 [00:02<01:01,  7.98it/s]\u001b[A\n",
            "Iteration:   4% 23/513 [00:03<01:08,  7.19it/s]\u001b[A\n",
            "Iteration:   5% 24/513 [00:03<01:06,  7.38it/s]\u001b[A\n",
            "Iteration:   5% 25/513 [00:03<01:26,  5.63it/s]\u001b[A\n",
            "Iteration:   5% 26/513 [00:03<01:22,  5.91it/s]\u001b[A\n",
            "Iteration:   5% 28/513 [00:03<01:13,  6.59it/s]\u001b[A\n",
            "Iteration:   6% 29/513 [00:04<01:07,  7.15it/s]\u001b[A\n",
            "Iteration:   6% 31/513 [00:04<01:07,  7.18it/s]\u001b[A\n",
            "Iteration:   6% 33/513 [00:04<00:56,  8.44it/s]\u001b[A\n",
            "Iteration:   7% 34/513 [00:04<01:07,  7.12it/s]\u001b[A\n",
            "Iteration:   7% 35/513 [00:04<01:16,  6.28it/s]\u001b[A\n",
            "Iteration:   7% 37/513 [00:05<01:07,  7.05it/s]\u001b[A\n",
            "Iteration:   8% 39/513 [00:05<00:56,  8.32it/s]\u001b[A\n",
            "Iteration:   8% 41/513 [00:05<00:50,  9.39it/s]\u001b[A\n",
            "Iteration:   8% 43/513 [00:05<00:53,  8.73it/s]\u001b[A\n",
            "Iteration:   9% 44/513 [00:05<00:55,  8.43it/s]\u001b[A\n",
            "Iteration:   9% 46/513 [00:05<00:51,  9.01it/s]\u001b[A\n",
            "Iteration:   9% 47/513 [00:06<00:57,  8.08it/s]\u001b[A\n",
            "Iteration:  10% 49/513 [00:06<00:52,  8.88it/s]\u001b[A\n",
            "Iteration:  10% 51/513 [00:06<00:47,  9.65it/s]\u001b[A\n",
            "Iteration:  10% 53/513 [00:06<00:48,  9.53it/s]\u001b[A\n",
            "Iteration:  11% 55/513 [00:07<00:59,  7.71it/s]\u001b[A\n",
            "Iteration:  11% 57/513 [00:07<00:51,  8.90it/s]\u001b[A\n",
            "Iteration:  12% 59/513 [00:07<00:52,  8.61it/s]\u001b[A\n",
            "Iteration:  12% 60/513 [00:07<01:01,  7.36it/s]\u001b[A\n",
            "Iteration:  12% 61/513 [00:07<01:16,  5.94it/s]\u001b[A\n",
            "Iteration:  12% 63/513 [00:08<01:08,  6.57it/s]\u001b[A\n",
            "Iteration:  12% 64/513 [00:08<01:08,  6.54it/s]\u001b[A\n",
            "Iteration:  13% 66/513 [00:08<00:57,  7.83it/s]\u001b[A\n",
            "Iteration:  13% 67/513 [00:08<01:04,  6.94it/s]\u001b[A\n",
            "Iteration:  13% 68/513 [00:08<00:58,  7.56it/s]\u001b[A\n",
            "Iteration:  14% 70/513 [00:08<00:54,  8.09it/s]\u001b[A\n",
            "Iteration:  14% 71/513 [00:08<00:51,  8.56it/s]\u001b[A\n",
            "Iteration:  14% 72/513 [00:09<00:59,  7.37it/s]\u001b[A\n",
            "Iteration:  14% 74/513 [00:09<01:04,  6.80it/s]\u001b[A\n",
            "Iteration:  15% 75/513 [00:09<00:59,  7.40it/s]\u001b[A\n",
            "Iteration:  15% 76/513 [00:09<01:13,  5.98it/s]\u001b[A\n",
            "Iteration:  15% 77/513 [00:09<01:04,  6.79it/s]\u001b[A\n",
            "Iteration:  15% 79/513 [00:10<00:54,  7.91it/s]\u001b[A\n",
            "Iteration:  16% 80/513 [00:10<00:51,  8.42it/s]\u001b[A\n",
            "Iteration:  16% 82/513 [00:10<00:49,  8.70it/s]\u001b[A\n",
            "Iteration:  16% 83/513 [00:10<00:53,  8.03it/s]\u001b[A\n",
            "Iteration:  16% 84/513 [00:10<00:57,  7.51it/s]\u001b[A\n",
            "Iteration:  17% 85/513 [00:10<00:52,  8.08it/s]\u001b[A\n",
            "Iteration:  17% 86/513 [00:10<00:52,  8.11it/s]\u001b[A03/07/2020 14:52:29 - INFO - transformers.configuration_utils -   Configuration saved in models/gpt2/weights/checkpoint-600/config.json\n",
            "03/07/2020 14:52:30 - INFO - transformers.modeling_utils -   Model weights saved in models/gpt2/weights/checkpoint-600/pytorch_model.bin\n",
            "03/07/2020 14:52:30 - INFO - __main__ -   Saving model checkpoint to models/gpt2/weights/checkpoint-600\n",
            "03/07/2020 14:52:30 - INFO - __main__ -   Deleting older checkpoint [models/gpt2/weights/checkpoint-400] due to args.save_total_limit\n",
            "03/07/2020 14:52:42 - INFO - __main__ -   Saving optimizer and scheduler states to models/gpt2/weights/checkpoint-600\n",
            "\n",
            "Iteration:  17% 87/513 [00:24<29:43,  4.19s/it]\u001b[A\n",
            "Iteration:  17% 88/513 [00:24<21:00,  2.97s/it]\u001b[A\n",
            "Iteration:  17% 89/513 [00:24<15:14,  2.16s/it]\u001b[A\n",
            "Iteration:  18% 91/513 [00:25<10:53,  1.55s/it]\u001b[A\n",
            "Iteration:  18% 93/513 [00:25<07:54,  1.13s/it]\u001b[A\n",
            "Iteration:  18% 94/513 [00:25<05:44,  1.22it/s]\u001b[A\n",
            "Iteration:  19% 96/513 [00:25<04:11,  1.66it/s]\u001b[A\n",
            "Iteration:  19% 98/513 [00:26<03:05,  2.24it/s]\u001b[A\n",
            "Iteration:  19% 100/513 [00:26<02:23,  2.87it/s]\u001b[A\n",
            "Iteration:  20% 101/513 [00:26<01:56,  3.54it/s]\u001b[A\n",
            "Iteration:  20% 102/513 [00:26<01:39,  4.13it/s]\u001b[A\n",
            "Iteration:  20% 103/513 [00:26<01:34,  4.35it/s]\u001b[A\n",
            "Iteration:  20% 105/513 [00:26<01:15,  5.40it/s]\u001b[A\n",
            "Iteration:  21% 106/513 [00:27<01:06,  6.09it/s]\u001b[A\n",
            "Iteration:  21% 108/513 [00:27<01:01,  6.62it/s]\u001b[A\n",
            "Iteration:  21% 110/513 [00:27<00:50,  7.93it/s]\u001b[A\n",
            "Iteration:  22% 112/513 [00:27<00:45,  8.75it/s]\u001b[A\n",
            "Iteration:  22% 114/513 [00:27<00:54,  7.35it/s]\u001b[A\n",
            "Iteration:  22% 115/513 [00:28<01:07,  5.92it/s]\u001b[A\n",
            "Iteration:  23% 116/513 [00:28<01:03,  6.27it/s]\u001b[A\n",
            "Iteration:  23% 118/513 [00:28<01:03,  6.26it/s]\u001b[A\n",
            "Iteration:  23% 120/513 [00:28<00:53,  7.34it/s]\u001b[A\n",
            "Iteration:  24% 121/513 [00:28<00:53,  7.39it/s]\u001b[A\n",
            "Iteration:  24% 123/513 [00:29<00:48,  8.09it/s]\u001b[A\n",
            "Iteration:  24% 125/513 [00:29<00:45,  8.55it/s]\u001b[A\n",
            "Iteration:  25% 127/513 [00:29<00:44,  8.76it/s]\u001b[A\n",
            "Iteration:  25% 128/513 [00:29<00:46,  8.25it/s]\u001b[A\n",
            "Iteration:  25% 130/513 [00:29<00:42,  8.97it/s]\u001b[A\n",
            "Iteration:  26% 131/513 [00:30<01:00,  6.27it/s]\u001b[A\n",
            "Iteration:  26% 132/513 [00:30<01:01,  6.16it/s]\u001b[A\n",
            "Iteration:  26% 133/513 [00:30<01:11,  5.33it/s]\u001b[A\n",
            "Iteration:  26% 135/513 [00:30<01:08,  5.52it/s]\u001b[A\n",
            "Iteration:  27% 137/513 [00:31<00:56,  6.64it/s]\u001b[A\n",
            "Iteration:  27% 138/513 [00:31<01:07,  5.59it/s]\u001b[A\n",
            "Iteration:  27% 139/513 [00:31<01:02,  6.03it/s]\u001b[A\n",
            "Iteration:  27% 140/513 [00:31<01:08,  5.43it/s]\u001b[A\n",
            "Iteration:  27% 141/513 [00:31<01:08,  5.41it/s]\u001b[A\n",
            "Iteration:  28% 142/513 [00:31<00:59,  6.19it/s]\u001b[A\n",
            "Iteration:  28% 143/513 [00:32<00:58,  6.28it/s]\u001b[A\n",
            "Iteration:  28% 145/513 [00:32<00:50,  7.35it/s]\u001b[A\n",
            "Iteration:  28% 146/513 [00:32<00:47,  7.70it/s]\u001b[A\n",
            "Iteration:  29% 147/513 [00:32<01:00,  6.08it/s]\u001b[A\n",
            "Iteration:  29% 149/513 [00:32<00:48,  7.51it/s]\u001b[A\n",
            "Iteration:  29% 151/513 [00:32<00:44,  8.18it/s]\u001b[A\n",
            "Iteration:  30% 153/513 [00:33<00:39,  9.13it/s]\u001b[A\n",
            "Iteration:  30% 155/513 [00:33<00:38,  9.38it/s]\u001b[A\n",
            "Iteration:  31% 157/513 [00:33<00:41,  8.49it/s]\u001b[A\n",
            "Iteration:  31% 159/513 [00:33<00:37,  9.44it/s]\u001b[A\n",
            "Iteration:  31% 161/513 [00:33<00:36,  9.78it/s]\u001b[A\n",
            "Iteration:  32% 163/513 [00:34<00:32, 10.69it/s]\u001b[A\n",
            "Iteration:  32% 165/513 [00:34<00:32, 10.85it/s]\u001b[A\n",
            "Iteration:  33% 167/513 [00:34<00:38,  8.98it/s]\u001b[A\n",
            "Iteration:  33% 169/513 [00:34<00:37,  9.14it/s]\u001b[A\n",
            "Iteration:  33% 170/513 [00:34<00:42,  8.12it/s]\u001b[A\n",
            "Iteration:  33% 171/513 [00:35<00:42,  8.00it/s]\u001b[A\n",
            "Iteration:  34% 173/513 [00:35<00:40,  8.43it/s]\u001b[A\n",
            "Iteration:  34% 174/513 [00:35<00:43,  7.82it/s]\u001b[A\n",
            "Iteration:  34% 176/513 [00:35<00:39,  8.53it/s]\u001b[A\n",
            "Iteration:  35% 178/513 [00:35<00:34,  9.66it/s]\u001b[A\n",
            "Iteration:  35% 180/513 [00:35<00:31, 10.43it/s]\u001b[A\n",
            "Iteration:  35% 182/513 [00:36<00:32, 10.09it/s]\u001b[A\n",
            "Iteration:  36% 184/513 [00:36<00:42,  7.80it/s]\u001b[A\n",
            "Iteration:  36% 185/513 [00:36<00:40,  8.03it/s]\u001b[A03/07/2020 14:52:55 - INFO - transformers.configuration_utils -   Configuration saved in models/gpt2/weights/checkpoint-700/config.json\n",
            "03/07/2020 14:52:56 - INFO - transformers.modeling_utils -   Model weights saved in models/gpt2/weights/checkpoint-700/pytorch_model.bin\n",
            "03/07/2020 14:52:56 - INFO - __main__ -   Saving model checkpoint to models/gpt2/weights/checkpoint-700\n",
            "03/07/2020 14:52:56 - INFO - __main__ -   Deleting older checkpoint [models/gpt2/weights/checkpoint-500] due to args.save_total_limit\n",
            "03/07/2020 14:53:08 - INFO - __main__ -   Saving optimizer and scheduler states to models/gpt2/weights/checkpoint-700\n",
            "\n",
            "Iteration:  36% 187/513 [00:50<11:53,  2.19s/it]\u001b[A\n",
            "Iteration:  37% 188/513 [00:50<08:33,  1.58s/it]\u001b[A\n",
            "Iteration:  37% 189/513 [00:50<06:14,  1.16s/it]\u001b[A\n",
            "Iteration:  37% 191/513 [00:51<04:31,  1.19it/s]\u001b[A\n",
            "Iteration:  38% 193/513 [00:51<03:16,  1.63it/s]\u001b[A\n",
            "Iteration:  38% 195/513 [00:51<02:29,  2.13it/s]\u001b[A\n",
            "Iteration:  38% 196/513 [00:51<01:54,  2.77it/s]\u001b[A\n",
            "Iteration:  38% 197/513 [00:51<01:29,  3.52it/s]\u001b[A\n",
            "Iteration:  39% 198/513 [00:51<01:14,  4.25it/s]\u001b[A\n",
            "Iteration:  39% 199/513 [00:52<01:03,  4.96it/s]\u001b[A\n",
            "Iteration:  39% 201/513 [00:52<00:51,  6.11it/s]\u001b[A\n",
            "Iteration:  39% 202/513 [00:52<00:51,  6.00it/s]\u001b[A\n",
            "Iteration:  40% 204/513 [00:52<00:44,  6.87it/s]\u001b[A\n",
            "Iteration:  40% 205/513 [00:52<00:42,  7.25it/s]\u001b[A\n",
            "Iteration:  40% 207/513 [00:53<00:44,  6.85it/s]\u001b[A\n",
            "Iteration:  41% 209/513 [00:53<00:39,  7.67it/s]\u001b[A\n",
            "Iteration:  41% 211/513 [00:53<00:34,  8.65it/s]\u001b[A\n",
            "Iteration:  41% 212/513 [00:53<00:37,  8.04it/s]\u001b[A\n",
            "Iteration:  42% 213/513 [00:53<00:36,  8.21it/s]\u001b[A\n",
            "Iteration:  42% 215/513 [00:53<00:36,  8.13it/s]\u001b[A\n",
            "Iteration:  42% 216/513 [00:54<00:42,  6.99it/s]\u001b[A\n",
            "Iteration:  42% 217/513 [00:54<00:38,  7.66it/s]\u001b[A\n",
            "Iteration:  43% 219/513 [00:54<00:41,  7.08it/s]\u001b[A\n",
            "Iteration:  43% 221/513 [00:54<00:38,  7.51it/s]\u001b[A\n",
            "Iteration:  43% 223/513 [00:55<00:41,  6.98it/s]\u001b[A\n",
            "Iteration:  44% 224/513 [00:55<00:41,  6.96it/s]\u001b[A\n",
            "Iteration:  44% 225/513 [00:55<00:38,  7.45it/s]\u001b[A\n",
            "Iteration:  44% 226/513 [00:55<00:44,  6.48it/s]\u001b[A\n",
            "Iteration:  44% 228/513 [00:55<00:37,  7.61it/s]\u001b[A\n",
            "Iteration:  45% 229/513 [00:55<00:38,  7.30it/s]\u001b[A\n",
            "Iteration:  45% 231/513 [00:55<00:33,  8.31it/s]\u001b[A\n",
            "Iteration:  45% 233/513 [00:56<00:35,  7.79it/s]\u001b[A\n",
            "Iteration:  46% 235/513 [00:56<00:37,  7.39it/s]\u001b[A\n",
            "Iteration:  46% 237/513 [00:56<00:36,  7.47it/s]\u001b[A\n",
            "Iteration:  47% 239/513 [00:57<00:32,  8.52it/s]\u001b[A\n",
            "Iteration:  47% 241/513 [00:57<00:29,  9.36it/s]\u001b[A\n",
            "Iteration:  47% 243/513 [00:57<00:33,  8.06it/s]\u001b[A\n",
            "Iteration:  48% 244/513 [00:57<00:35,  7.68it/s]\u001b[A\n",
            "Iteration:  48% 245/513 [00:57<00:42,  6.30it/s]\u001b[A\n",
            "Iteration:  48% 246/513 [00:58<00:43,  6.13it/s]\u001b[A\n",
            "Iteration:  48% 248/513 [00:58<00:43,  6.05it/s]\u001b[A\n",
            "Iteration:  49% 250/513 [00:58<00:42,  6.16it/s]\u001b[A\n",
            "Iteration:  49% 252/513 [00:58<00:36,  7.12it/s]\u001b[A\n",
            "Iteration:  50% 254/513 [00:59<00:39,  6.54it/s]\u001b[A\n",
            "Iteration:  50% 256/513 [00:59<00:33,  7.70it/s]\u001b[A\n",
            "Iteration:  50% 258/513 [00:59<00:31,  8.01it/s]\u001b[A\n",
            "Iteration:  51% 260/513 [00:59<00:27,  9.09it/s]\u001b[A\n",
            "Iteration:  51% 262/513 [00:59<00:24, 10.30it/s]\u001b[A\n",
            "Iteration:  51% 264/513 [01:00<00:27,  9.10it/s]\u001b[A\n",
            "Iteration:  52% 266/513 [01:00<00:29,  8.36it/s]\u001b[A\n",
            "Iteration:  52% 267/513 [01:00<00:28,  8.66it/s]\u001b[A\n",
            "Iteration:  52% 269/513 [01:00<00:26,  9.32it/s]\u001b[A\n",
            "Iteration:  53% 271/513 [01:01<00:29,  8.23it/s]\u001b[A\n",
            "Iteration:  53% 273/513 [01:01<00:25,  9.47it/s]\u001b[A\n",
            "Iteration:  54% 275/513 [01:01<00:29,  8.08it/s]\u001b[A\n",
            "Iteration:  54% 276/513 [01:01<00:29,  8.14it/s]\u001b[A\n",
            "Iteration:  54% 277/513 [01:01<00:27,  8.60it/s]\u001b[A\n",
            "Iteration:  54% 279/513 [01:01<00:25,  9.35it/s]\u001b[A\n",
            "Iteration:  55% 281/513 [01:02<00:23, 10.07it/s]\u001b[A\n",
            "Iteration:  55% 283/513 [01:02<00:27,  8.45it/s]\u001b[A\n",
            "Iteration:  55% 284/513 [01:02<00:26,  8.59it/s]\u001b[A\n",
            "Iteration:  56% 285/513 [01:02<00:27,  8.19it/s]\u001b[A03/07/2020 14:53:21 - INFO - transformers.configuration_utils -   Configuration saved in models/gpt2/weights/checkpoint-800/config.json\n",
            "03/07/2020 14:53:22 - INFO - transformers.modeling_utils -   Model weights saved in models/gpt2/weights/checkpoint-800/pytorch_model.bin\n",
            "03/07/2020 14:53:22 - INFO - __main__ -   Saving model checkpoint to models/gpt2/weights/checkpoint-800\n",
            "03/07/2020 14:53:22 - INFO - __main__ -   Deleting older checkpoint [models/gpt2/weights/checkpoint-600] due to args.save_total_limit\n",
            "03/07/2020 14:53:31 - INFO - __main__ -   Saving optimizer and scheduler states to models/gpt2/weights/checkpoint-800\n",
            "\n",
            "Iteration:  56% 287/513 [01:13<06:14,  1.66s/it]\u001b[A\n",
            "Iteration:  56% 288/513 [01:13<04:29,  1.20s/it]\u001b[A\n",
            "Iteration:  57% 290/513 [01:13<03:13,  1.15it/s]\u001b[A\n",
            "Iteration:  57% 291/513 [01:13<02:31,  1.47it/s]\u001b[A\n",
            "Iteration:  57% 292/513 [01:13<02:01,  1.81it/s]\u001b[A\n",
            "Iteration:  57% 293/513 [01:14<01:31,  2.40it/s]\u001b[A\n",
            "Iteration:  58% 295/513 [01:14<01:10,  3.09it/s]\u001b[A\n",
            "Iteration:  58% 297/513 [01:14<00:53,  4.05it/s]\u001b[A\n",
            "Iteration:  58% 298/513 [01:14<00:47,  4.51it/s]\u001b[A\n",
            "Iteration:  58% 299/513 [01:14<00:44,  4.85it/s]\u001b[A\n",
            "Iteration:  58% 300/513 [01:14<00:46,  4.59it/s]\u001b[A\n",
            "Iteration:  59% 301/513 [01:15<00:40,  5.18it/s]\u001b[A\n",
            "Iteration:  59% 303/513 [01:15<00:34,  6.15it/s]\u001b[A\n",
            "Iteration:  59% 304/513 [01:15<00:31,  6.74it/s]\u001b[A\n",
            "Iteration:  59% 305/513 [01:15<00:32,  6.41it/s]\u001b[A\n",
            "Iteration:  60% 306/513 [01:15<00:38,  5.34it/s]\u001b[A\n",
            "Iteration:  60% 307/513 [01:16<00:42,  4.85it/s]\u001b[A\n",
            "Iteration:  60% 309/513 [01:16<00:34,  5.93it/s]\u001b[A\n",
            "Iteration:  61% 311/513 [01:16<00:32,  6.22it/s]\u001b[A\n",
            "Iteration:  61% 313/513 [01:16<00:27,  7.27it/s]\u001b[A\n",
            "Iteration:  61% 314/513 [01:16<00:28,  7.11it/s]\u001b[A\n",
            "Iteration:  61% 315/513 [01:16<00:25,  7.71it/s]\u001b[A\n",
            "Iteration:  62% 316/513 [01:17<00:24,  7.92it/s]\u001b[A\n",
            "Iteration:  62% 318/513 [01:17<00:25,  7.62it/s]\u001b[A\n",
            "Iteration:  62% 320/513 [01:17<00:27,  7.11it/s]\u001b[A\n",
            "Iteration:  63% 321/513 [01:17<00:25,  7.50it/s]\u001b[A\n",
            "Iteration:  63% 323/513 [01:18<00:23,  8.09it/s]\u001b[A\n",
            "Iteration:  63% 324/513 [01:18<00:22,  8.56it/s]\u001b[A\n",
            "Iteration:  63% 325/513 [01:18<00:21,  8.90it/s]\u001b[A\n",
            "Iteration:  64% 327/513 [01:18<00:20,  8.89it/s]\u001b[A\n",
            "Iteration:  64% 328/513 [01:18<00:23,  8.03it/s]\u001b[A\n",
            "Iteration:  64% 330/513 [01:18<00:22,  8.12it/s]\u001b[A\n",
            "Iteration:  65% 332/513 [01:19<00:20,  8.84it/s]\u001b[A\n",
            "Iteration:  65% 334/513 [01:19<00:18,  9.80it/s]\u001b[A\n",
            "Iteration:  65% 336/513 [01:19<00:17, 10.01it/s]\u001b[A\n",
            "Iteration:  66% 338/513 [01:19<00:21,  8.17it/s]\u001b[A\n",
            "Iteration:  66% 339/513 [01:19<00:21,  8.12it/s]\u001b[A\n",
            "Iteration:  66% 341/513 [01:20<00:19,  8.74it/s]\u001b[A\n",
            "Iteration:  67% 342/513 [01:20<00:18,  9.06it/s]\u001b[A\n",
            "Iteration:  67% 343/513 [01:20<00:28,  6.04it/s]\u001b[A\n",
            "Iteration:  67% 345/513 [01:20<00:25,  6.56it/s]\u001b[A\n",
            "Iteration:  68% 347/513 [01:20<00:22,  7.40it/s]\u001b[A\n",
            "Iteration:  68% 348/513 [01:20<00:22,  7.20it/s]\u001b[A\n",
            "Iteration:  68% 350/513 [01:21<00:19,  8.25it/s]\u001b[A\n",
            "Iteration:  68% 351/513 [01:21<00:19,  8.45it/s]\u001b[A\n",
            "Iteration:  69% 352/513 [01:21<00:25,  6.40it/s]\u001b[A\n",
            "Iteration:  69% 354/513 [01:21<00:21,  7.48it/s]\u001b[A\n",
            "Iteration:  69% 356/513 [01:22<00:23,  6.65it/s]\u001b[A\n",
            "Iteration:  70% 358/513 [01:22<00:22,  6.97it/s]\u001b[A\n",
            "Iteration:  70% 360/513 [01:22<00:18,  8.09it/s]\u001b[A\n",
            "Iteration:  71% 362/513 [01:22<00:17,  8.49it/s]\u001b[A\n",
            "Iteration:  71% 364/513 [01:22<00:17,  8.37it/s]\u001b[A\n",
            "Iteration:  71% 365/513 [01:23<00:17,  8.68it/s]\u001b[A\n",
            "Iteration:  72% 367/513 [01:23<00:15,  9.16it/s]\u001b[A\n",
            "Iteration:  72% 369/513 [01:23<00:14,  9.92it/s]\u001b[A\n",
            "Iteration:  72% 371/513 [01:23<00:17,  8.19it/s]\u001b[A\n",
            "Iteration:  73% 373/513 [01:23<00:15,  9.12it/s]\u001b[A\n",
            "Iteration:  73% 375/513 [01:24<00:20,  6.76it/s]\u001b[A\n",
            "Iteration:  73% 377/513 [01:24<00:16,  8.01it/s]\u001b[A\n",
            "Iteration:  74% 379/513 [01:24<00:19,  7.00it/s]\u001b[A\n",
            "Iteration:  74% 380/513 [01:25<00:21,  6.12it/s]\u001b[A\n",
            "Iteration:  74% 382/513 [01:25<00:21,  6.18it/s]\u001b[A\n",
            "Iteration:  75% 383/513 [01:25<00:25,  5.18it/s]\u001b[A\n",
            "Iteration:  75% 385/513 [01:25<00:21,  5.98it/s]\u001b[A03/07/2020 14:53:44 - INFO - transformers.configuration_utils -   Configuration saved in models/gpt2/weights/checkpoint-900/config.json\n",
            "03/07/2020 14:53:45 - INFO - transformers.modeling_utils -   Model weights saved in models/gpt2/weights/checkpoint-900/pytorch_model.bin\n",
            "03/07/2020 14:53:45 - INFO - __main__ -   Saving model checkpoint to models/gpt2/weights/checkpoint-900\n",
            "03/07/2020 14:53:45 - INFO - __main__ -   Deleting older checkpoint [models/gpt2/weights/checkpoint-700] due to args.save_total_limit\n",
            "03/07/2020 14:53:58 - INFO - __main__ -   Saving optimizer and scheduler states to models/gpt2/weights/checkpoint-900\n",
            "\n",
            "Iteration:  75% 387/513 [01:39<04:37,  2.20s/it]\u001b[A\n",
            "Iteration:  76% 388/513 [01:39<03:17,  1.58s/it]\u001b[A\n",
            "Iteration:  76% 389/513 [01:40<02:21,  1.14s/it]\u001b[A\n",
            "Iteration:  76% 391/513 [01:40<01:42,  1.20it/s]\u001b[A\n",
            "Iteration:  77% 393/513 [01:40<01:12,  1.65it/s]\u001b[A\n",
            "Iteration:  77% 394/513 [01:40<00:54,  2.20it/s]\u001b[A\n",
            "Iteration:  77% 396/513 [01:40<00:44,  2.65it/s]\u001b[A\n",
            "Iteration:  78% 398/513 [01:41<00:33,  3.38it/s]\u001b[A\n",
            "Iteration:  78% 400/513 [01:41<00:27,  4.08it/s]\u001b[A\n",
            "Iteration:  78% 402/513 [01:41<00:21,  5.26it/s]\u001b[A\n",
            "Iteration:  79% 404/513 [01:41<00:18,  6.04it/s]\u001b[A\n",
            "Iteration:  79% 406/513 [01:42<00:17,  6.17it/s]\u001b[A\n",
            "Iteration:  80% 408/513 [01:42<00:16,  6.44it/s]\u001b[A\n",
            "Iteration:  80% 410/513 [01:42<00:14,  7.22it/s]\u001b[A\n",
            "Iteration:  80% 411/513 [01:42<00:13,  7.82it/s]\u001b[A\n",
            "Iteration:  80% 412/513 [01:42<00:16,  6.12it/s]\u001b[A\n",
            "Iteration:  81% 414/513 [01:43<00:14,  7.01it/s]\u001b[A\n",
            "Iteration:  81% 416/513 [01:43<00:12,  7.84it/s]\u001b[A\n",
            "Iteration:  81% 417/513 [01:43<00:15,  6.32it/s]\u001b[A\n",
            "Iteration:  81% 418/513 [01:43<00:17,  5.40it/s]\u001b[A\n",
            "Iteration:  82% 419/513 [01:43<00:19,  4.79it/s]\u001b[A\n",
            "Iteration:  82% 420/513 [01:44<00:16,  5.62it/s]\u001b[A\n",
            "Iteration:  82% 422/513 [01:44<00:13,  6.51it/s]\u001b[A\n",
            "Iteration:  83% 424/513 [01:44<00:11,  7.66it/s]\u001b[A\n",
            "Iteration:  83% 425/513 [01:44<00:14,  6.07it/s]\u001b[A\n",
            "Iteration:  83% 426/513 [01:44<00:12,  6.72it/s]\u001b[A\n",
            "Iteration:  83% 428/513 [01:44<00:10,  7.75it/s]\u001b[A\n",
            "Iteration:  84% 430/513 [01:45<00:11,  7.19it/s]\u001b[A\n",
            "Iteration:  84% 431/513 [01:45<00:11,  7.08it/s]\u001b[A\n",
            "Iteration:  84% 432/513 [01:45<00:12,  6.60it/s]\u001b[A\n",
            "Iteration:  84% 433/513 [01:45<00:12,  6.23it/s]\u001b[A\n",
            "Iteration:  85% 435/513 [01:45<00:10,  7.33it/s]\u001b[A\n",
            "Iteration:  85% 436/513 [01:46<00:09,  7.88it/s]\u001b[A\n",
            "Iteration:  85% 438/513 [01:46<00:08,  8.73it/s]\u001b[A\n",
            "Iteration:  86% 440/513 [01:46<00:07,  9.95it/s]\u001b[A\n",
            "Iteration:  86% 442/513 [01:46<00:07,  9.89it/s]\u001b[A\n",
            "Iteration:  87% 444/513 [01:46<00:09,  7.61it/s]\u001b[A\n",
            "Iteration:  87% 446/513 [01:47<00:09,  6.95it/s]\u001b[A\n",
            "Iteration:  87% 447/513 [01:47<00:10,  6.32it/s]\u001b[A\n",
            "Iteration:  88% 449/513 [01:47<00:09,  6.75it/s]\u001b[A\n",
            "Iteration:  88% 450/513 [01:47<00:10,  6.29it/s]\u001b[A\n",
            "Iteration:  88% 451/513 [01:48<00:10,  5.71it/s]\u001b[A\n",
            "Iteration:  88% 452/513 [01:48<00:10,  5.57it/s]\u001b[A\n",
            "Iteration:  88% 454/513 [01:48<00:08,  6.82it/s]\u001b[A\n",
            "Iteration:  89% 456/513 [01:48<00:07,  7.19it/s]\u001b[A\n",
            "Iteration:  89% 457/513 [01:48<00:07,  7.24it/s]\u001b[A\n",
            "Iteration:  89% 459/513 [01:48<00:06,  8.48it/s]\u001b[A\n",
            "Iteration:  90% 461/513 [01:49<00:05,  9.12it/s]\u001b[A\n",
            "Iteration:  90% 463/513 [01:49<00:04, 10.01it/s]\u001b[A\n",
            "Iteration:  91% 465/513 [01:49<00:05,  8.21it/s]\u001b[A\n",
            "Iteration:  91% 467/513 [01:49<00:05,  8.90it/s]\u001b[A\n",
            "Iteration:  91% 469/513 [01:50<00:04,  9.00it/s]\u001b[A\n",
            "Iteration:  92% 470/513 [01:50<00:05,  8.58it/s]\u001b[A\n",
            "Iteration:  92% 471/513 [01:50<00:05,  7.61it/s]\u001b[A\n",
            "Iteration:  92% 472/513 [01:50<00:07,  5.57it/s]\u001b[A\n",
            "Iteration:  92% 474/513 [01:50<00:05,  6.96it/s]\u001b[A\n",
            "Iteration:  93% 475/513 [01:50<00:05,  7.22it/s]\u001b[A\n",
            "Iteration:  93% 476/513 [01:51<00:05,  7.26it/s]\u001b[A\n",
            "Iteration:  93% 477/513 [01:51<00:04,  7.55it/s]\u001b[A\n",
            "Iteration:  93% 478/513 [01:51<00:05,  6.89it/s]\u001b[A\n",
            "Iteration:  94% 480/513 [01:51<00:04,  7.89it/s]\u001b[A\n",
            "Iteration:  94% 482/513 [01:51<00:03,  8.93it/s]\u001b[A\n",
            "Iteration:  94% 484/513 [01:51<00:03,  7.91it/s]\u001b[A\n",
            "Iteration:  95% 486/513 [01:52<00:03,  8.76it/s]\u001b[A03/07/2020 14:54:10 - INFO - transformers.configuration_utils -   Configuration saved in models/gpt2/weights/checkpoint-1000/config.json\n",
            "03/07/2020 14:54:11 - INFO - transformers.modeling_utils -   Model weights saved in models/gpt2/weights/checkpoint-1000/pytorch_model.bin\n",
            "03/07/2020 14:54:11 - INFO - __main__ -   Saving model checkpoint to models/gpt2/weights/checkpoint-1000\n",
            "03/07/2020 14:54:11 - INFO - __main__ -   Deleting older checkpoint [models/gpt2/weights/checkpoint-800] due to args.save_total_limit\n",
            "03/07/2020 14:54:21 - INFO - __main__ -   Saving optimizer and scheduler states to models/gpt2/weights/checkpoint-1000\n",
            "\n",
            "Iteration:  95% 487/513 [02:03<01:30,  3.49s/it]\u001b[A\n",
            "Iteration:  95% 489/513 [02:03<00:59,  2.48s/it]\u001b[A\n",
            "Iteration:  96% 491/513 [02:03<00:38,  1.76s/it]\u001b[A\n",
            "Iteration:  96% 493/513 [02:04<00:25,  1.26s/it]\u001b[A\n",
            "Iteration:  96% 494/513 [02:04<00:17,  1.10it/s]\u001b[A\n",
            "Iteration:  96% 495/513 [02:04<00:12,  1.46it/s]\u001b[A\n",
            "Iteration:  97% 496/513 [02:04<00:09,  1.89it/s]\u001b[A\n",
            "Iteration:  97% 497/513 [02:04<00:06,  2.47it/s]\u001b[A\n",
            "Iteration:  97% 498/513 [02:04<00:05,  2.90it/s]\u001b[A\n",
            "Iteration:  97% 500/513 [02:04<00:03,  3.81it/s]\u001b[A\n",
            "Iteration:  98% 501/513 [02:05<00:02,  4.29it/s]\u001b[A\n",
            "Iteration:  98% 503/513 [02:05<00:01,  5.22it/s]\u001b[A\n",
            "Iteration:  98% 505/513 [02:05<00:01,  6.46it/s]\u001b[A\n",
            "Iteration:  99% 507/513 [02:05<00:00,  7.07it/s]\u001b[A\n",
            "Iteration:  99% 509/513 [02:05<00:00,  8.07it/s]\u001b[A\n",
            "Iteration: 100% 511/513 [02:06<00:00,  8.00it/s]\u001b[A\n",
            "Iteration: 100% 513/513 [02:06<00:00,  8.75it/s]\u001b[A\n",
            "Epoch:  67% 2/3 [03:55<01:54, 114.38s/it]\n",
            "Iteration:   0% 0/513 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/513 [00:00<01:37,  5.27it/s]\u001b[A\n",
            "Iteration:   0% 2/513 [00:00<01:32,  5.50it/s]\u001b[A\n",
            "Iteration:   1% 4/513 [00:00<01:20,  6.33it/s]\u001b[A\n",
            "Iteration:   1% 5/513 [00:00<01:33,  5.42it/s]\u001b[A\n",
            "Iteration:   1% 6/513 [00:00<01:22,  6.15it/s]\u001b[A\n",
            "Iteration:   2% 8/513 [00:01<01:11,  7.02it/s]\u001b[A\n",
            "Iteration:   2% 9/513 [00:01<01:15,  6.70it/s]\u001b[A\n",
            "Iteration:   2% 10/513 [00:01<01:24,  5.97it/s]\u001b[A\n",
            "Iteration:   2% 12/513 [00:01<01:07,  7.38it/s]\u001b[A\n",
            "Iteration:   3% 14/513 [00:01<01:13,  6.82it/s]\u001b[A\n",
            "Iteration:   3% 16/513 [00:02<01:02,  7.95it/s]\u001b[A\n",
            "Iteration:   4% 18/513 [00:02<00:58,  8.51it/s]\u001b[A\n",
            "Iteration:   4% 19/513 [00:02<00:59,  8.27it/s]\u001b[A\n",
            "Iteration:   4% 20/513 [00:02<01:22,  5.99it/s]\u001b[A\n",
            "Iteration:   4% 21/513 [00:02<01:25,  5.73it/s]\u001b[A\n",
            "Iteration:   4% 23/513 [00:03<01:10,  6.93it/s]\u001b[A\n",
            "Iteration:   5% 24/513 [00:03<01:09,  7.06it/s]\u001b[A\n",
            "Iteration:   5% 26/513 [00:03<01:00,  8.03it/s]\u001b[A\n",
            "Iteration:   5% 27/513 [00:03<01:00,  7.98it/s]\u001b[A\n",
            "Iteration:   5% 28/513 [00:03<00:57,  8.45it/s]\u001b[A\n",
            "Iteration:   6% 30/513 [00:03<00:57,  8.35it/s]\u001b[A\n",
            "Iteration:   6% 32/513 [00:03<00:49,  9.65it/s]\u001b[A\n",
            "Iteration:   7% 34/513 [00:04<01:02,  7.67it/s]\u001b[A\n",
            "Iteration:   7% 35/513 [00:04<01:00,  7.92it/s]\u001b[A\n",
            "Iteration:   7% 36/513 [00:04<00:59,  8.08it/s]\u001b[A\n",
            "Iteration:   7% 37/513 [00:04<01:18,  6.10it/s]\u001b[A\n",
            "Iteration:   8% 39/513 [00:05<01:10,  6.73it/s]\u001b[A\n",
            "Iteration:   8% 41/513 [00:05<01:00,  7.76it/s]\u001b[A\n",
            "Iteration:   8% 42/513 [00:05<01:08,  6.84it/s]\u001b[A\n",
            "Iteration:   8% 43/513 [00:05<01:25,  5.52it/s]\u001b[A\n",
            "Iteration:   9% 45/513 [00:05<01:07,  6.93it/s]\u001b[A\n",
            "Iteration:   9% 47/513 [00:05<00:56,  8.28it/s]\u001b[A\n",
            "Iteration:  10% 49/513 [00:06<00:49,  9.33it/s]\u001b[A\n",
            "Iteration:  10% 51/513 [00:06<00:50,  9.21it/s]\u001b[A\n",
            "Iteration:  10% 53/513 [00:06<00:59,  7.77it/s]\u001b[A\n",
            "Iteration:  11% 55/513 [00:06<00:52,  8.74it/s]\u001b[A\n",
            "Iteration:  11% 57/513 [00:06<00:46,  9.71it/s]\u001b[A\n",
            "Iteration:  12% 59/513 [00:07<00:51,  8.89it/s]\u001b[A\n",
            "Iteration:  12% 61/513 [00:07<01:00,  7.46it/s]\u001b[A\n",
            "Iteration:  12% 63/513 [00:07<00:52,  8.61it/s]\u001b[A\n",
            "Iteration:  13% 65/513 [00:07<00:44, 10.05it/s]\u001b[A\n",
            "Iteration:  13% 67/513 [00:08<00:50,  8.85it/s]\u001b[A\n",
            "Iteration:  13% 69/513 [00:08<00:45,  9.71it/s]\u001b[A\n",
            "Iteration:  14% 71/513 [00:08<00:53,  8.24it/s]\u001b[A\n",
            "Iteration:  14% 72/513 [00:08<00:53,  8.28it/s]\u001b[A03/07/2020 14:54:33 - INFO - transformers.configuration_utils -   Configuration saved in models/gpt2/weights/checkpoint-1100/config.json\n",
            "03/07/2020 14:54:34 - INFO - transformers.modeling_utils -   Model weights saved in models/gpt2/weights/checkpoint-1100/pytorch_model.bin\n",
            "03/07/2020 14:54:34 - INFO - __main__ -   Saving model checkpoint to models/gpt2/weights/checkpoint-1100\n",
            "03/07/2020 14:54:34 - INFO - __main__ -   Deleting older checkpoint [models/gpt2/weights/checkpoint-900] due to args.save_total_limit\n",
            "03/07/2020 14:54:49 - INFO - __main__ -   Saving optimizer and scheduler states to models/gpt2/weights/checkpoint-1100\n",
            "\n",
            "Iteration:  14% 74/513 [00:24<18:21,  2.51s/it]\u001b[A\n",
            "Iteration:  15% 75/513 [00:25<13:03,  1.79s/it]\u001b[A\n",
            "Iteration:  15% 77/513 [00:25<09:16,  1.28s/it]\u001b[A\n",
            "Iteration:  15% 78/513 [00:25<06:47,  1.07it/s]\u001b[A\n",
            "Iteration:  16% 80/513 [00:25<05:00,  1.44it/s]\u001b[A\n",
            "Iteration:  16% 82/513 [00:25<03:45,  1.91it/s]\u001b[A\n",
            "Iteration:  16% 83/513 [00:26<03:08,  2.28it/s]\u001b[A\n",
            "Iteration:  17% 85/513 [00:26<02:21,  3.02it/s]\u001b[A\n",
            "Iteration:  17% 86/513 [00:26<01:52,  3.79it/s]\u001b[A\n",
            "Iteration:  17% 87/513 [00:26<01:43,  4.13it/s]\u001b[A\n",
            "Iteration:  17% 89/513 [00:26<01:22,  5.15it/s]\u001b[A\n",
            "Iteration:  18% 90/513 [00:26<01:11,  5.94it/s]\u001b[A\n",
            "Iteration:  18% 91/513 [00:26<01:04,  6.53it/s]\u001b[A\n",
            "Iteration:  18% 92/513 [00:27<01:03,  6.66it/s]\u001b[A\n",
            "Iteration:  18% 93/513 [00:27<01:00,  6.94it/s]\u001b[A\n",
            "Iteration:  18% 94/513 [00:27<01:00,  6.95it/s]\u001b[A\n",
            "Iteration:  19% 95/513 [00:27<01:06,  6.30it/s]\u001b[A\n",
            "Iteration:  19% 96/513 [00:27<01:17,  5.40it/s]\u001b[A\n",
            "Iteration:  19% 97/513 [00:27<01:10,  5.93it/s]\u001b[A\n",
            "Iteration:  19% 99/513 [00:28<01:07,  6.11it/s]\u001b[A\n",
            "Iteration:  20% 101/513 [00:28<01:04,  6.39it/s]\u001b[A\n",
            "Iteration:  20% 103/513 [00:28<00:55,  7.36it/s]\u001b[A\n",
            "Iteration:  20% 104/513 [00:28<01:09,  5.93it/s]\u001b[A\n",
            "Iteration:  20% 105/513 [00:29<01:05,  6.21it/s]\u001b[A\n",
            "Iteration:  21% 107/513 [00:29<00:54,  7.45it/s]\u001b[A\n",
            "Iteration:  21% 109/513 [00:29<00:56,  7.13it/s]\u001b[A\n",
            "Iteration:  22% 111/513 [00:29<00:50,  7.92it/s]\u001b[A\n",
            "Iteration:  22% 113/513 [00:29<00:51,  7.81it/s]\u001b[A\n",
            "Iteration:  22% 115/513 [00:30<00:46,  8.65it/s]\u001b[A\n",
            "Iteration:  23% 116/513 [00:30<00:57,  6.95it/s]\u001b[A\n",
            "Iteration:  23% 117/513 [00:30<00:53,  7.35it/s]\u001b[A\n",
            "Iteration:  23% 119/513 [00:30<00:47,  8.29it/s]\u001b[A\n",
            "Iteration:  24% 121/513 [00:30<00:43,  8.94it/s]\u001b[A\n",
            "Iteration:  24% 123/513 [00:31<00:50,  7.75it/s]\u001b[A\n",
            "Iteration:  24% 124/513 [00:31<00:59,  6.58it/s]\u001b[A\n",
            "Iteration:  25% 126/513 [00:31<00:57,  6.78it/s]\u001b[A\n",
            "Iteration:  25% 127/513 [00:31<00:52,  7.39it/s]\u001b[A\n",
            "Iteration:  25% 129/513 [00:31<00:45,  8.40it/s]\u001b[A\n",
            "Iteration:  25% 130/513 [00:32<00:48,  7.84it/s]\u001b[A\n",
            "Iteration:  26% 132/513 [00:32<00:42,  8.89it/s]\u001b[A\n",
            "Iteration:  26% 134/513 [00:32<00:38,  9.78it/s]\u001b[A\n",
            "Iteration:  27% 136/513 [00:32<00:44,  8.52it/s]\u001b[A\n",
            "Iteration:  27% 138/513 [00:32<00:40,  9.23it/s]\u001b[A\n",
            "Iteration:  27% 140/513 [00:33<00:44,  8.43it/s]\u001b[A\n",
            "Iteration:  27% 141/513 [00:33<00:46,  8.03it/s]\u001b[A\n",
            "Iteration:  28% 143/513 [00:33<00:41,  8.92it/s]\u001b[A\n",
            "Iteration:  28% 145/513 [00:33<00:36, 10.11it/s]\u001b[A\n",
            "Iteration:  29% 147/513 [00:33<00:38,  9.55it/s]\u001b[A\n",
            "Iteration:  29% 149/513 [00:34<00:36,  9.96it/s]\u001b[A\n",
            "Iteration:  29% 151/513 [00:34<00:33, 10.86it/s]\u001b[A\n",
            "Iteration:  30% 153/513 [00:34<00:32, 10.95it/s]\u001b[A\n",
            "Iteration:  30% 155/513 [00:34<00:34, 10.33it/s]\u001b[A\n",
            "Iteration:  31% 157/513 [00:34<00:36,  9.74it/s]\u001b[A\n",
            "Iteration:  31% 159/513 [00:35<00:38,  9.24it/s]\u001b[A\n",
            "Iteration:  31% 161/513 [00:35<00:36,  9.69it/s]\u001b[A\n",
            "Iteration:  32% 162/513 [00:35<00:40,  8.65it/s]\u001b[A\n",
            "Iteration:  32% 164/513 [00:35<00:40,  8.69it/s]\u001b[A\n",
            "Iteration:  32% 166/513 [00:35<00:35,  9.85it/s]\u001b[A\n",
            "Iteration:  33% 168/513 [00:35<00:34, 10.13it/s]\u001b[A\n",
            "Iteration:  33% 170/513 [00:36<00:39,  8.68it/s]\u001b[A\n",
            "Iteration:  33% 171/513 [00:36<00:54,  6.28it/s]\u001b[A\n",
            "Iteration:  34% 173/513 [00:36<00:53,  6.30it/s]\u001b[A03/07/2020 14:55:01 - INFO - transformers.configuration_utils -   Configuration saved in models/gpt2/weights/checkpoint-1200/config.json\n",
            "03/07/2020 14:55:02 - INFO - transformers.modeling_utils -   Model weights saved in models/gpt2/weights/checkpoint-1200/pytorch_model.bin\n",
            "03/07/2020 14:55:02 - INFO - __main__ -   Saving model checkpoint to models/gpt2/weights/checkpoint-1200\n",
            "03/07/2020 14:55:02 - INFO - __main__ -   Deleting older checkpoint [models/gpt2/weights/checkpoint-1000] due to args.save_total_limit\n",
            "03/07/2020 14:55:15 - INFO - __main__ -   Saving optimizer and scheduler states to models/gpt2/weights/checkpoint-1200\n",
            "\n",
            "Iteration:  34% 174/513 [00:51<25:15,  4.47s/it]\u001b[A\n",
            "Iteration:  34% 176/513 [00:51<17:42,  3.15s/it]\u001b[A\n",
            "Iteration:  35% 177/513 [00:51<12:34,  2.25s/it]\u001b[A\n",
            "Iteration:  35% 178/513 [00:51<09:11,  1.65s/it]\u001b[A\n",
            "Iteration:  35% 179/513 [00:51<06:37,  1.19s/it]\u001b[A\n",
            "Iteration:  35% 181/513 [00:52<04:44,  1.17it/s]\u001b[A\n",
            "Iteration:  36% 183/513 [00:52<03:25,  1.60it/s]\u001b[A\n",
            "Iteration:  36% 185/513 [00:52<02:34,  2.13it/s]\u001b[A\n",
            "Iteration:  36% 186/513 [00:52<02:05,  2.62it/s]\u001b[A\n",
            "Iteration:  37% 188/513 [00:52<01:36,  3.36it/s]\u001b[A\n",
            "Iteration:  37% 190/513 [00:53<01:23,  3.88it/s]\u001b[A\n",
            "Iteration:  37% 192/513 [00:53<01:09,  4.60it/s]\u001b[A\n",
            "Iteration:  38% 193/513 [00:53<01:02,  5.09it/s]\u001b[A\n",
            "Iteration:  38% 194/513 [00:53<00:58,  5.43it/s]\u001b[A\n",
            "Iteration:  38% 196/513 [00:54<00:52,  6.09it/s]\u001b[A\n",
            "Iteration:  38% 197/513 [00:54<00:46,  6.82it/s]\u001b[A\n",
            "Iteration:  39% 199/513 [00:54<00:39,  7.86it/s]\u001b[A\n",
            "Iteration:  39% 201/513 [00:54<00:36,  8.47it/s]\u001b[A\n",
            "Iteration:  39% 202/513 [00:54<00:50,  6.15it/s]\u001b[A\n",
            "Iteration:  40% 204/513 [00:54<00:42,  7.21it/s]\u001b[A\n",
            "Iteration:  40% 205/513 [00:55<00:52,  5.84it/s]\u001b[A\n",
            "Iteration:  40% 206/513 [00:55<00:46,  6.58it/s]\u001b[A\n",
            "Iteration:  40% 207/513 [00:55<00:43,  7.04it/s]\u001b[A\n",
            "Iteration:  41% 209/513 [00:55<00:40,  7.53it/s]\u001b[A\n",
            "Iteration:  41% 211/513 [00:55<00:38,  7.78it/s]\u001b[A\n",
            "Iteration:  41% 212/513 [00:55<00:38,  7.88it/s]\u001b[A\n",
            "Iteration:  42% 213/513 [00:56<00:40,  7.49it/s]\u001b[A\n",
            "Iteration:  42% 214/513 [00:56<00:49,  6.00it/s]\u001b[A\n",
            "Iteration:  42% 215/513 [00:56<00:44,  6.74it/s]\u001b[A\n",
            "Iteration:  42% 217/513 [00:56<00:40,  7.22it/s]\u001b[A\n",
            "Iteration:  42% 218/513 [00:56<00:50,  5.80it/s]\u001b[A\n",
            "Iteration:  43% 219/513 [00:57<00:48,  6.11it/s]\u001b[A\n",
            "Iteration:  43% 221/513 [00:57<00:42,  6.80it/s]\u001b[A\n",
            "Iteration:  43% 223/513 [00:57<00:36,  7.92it/s]\u001b[A\n",
            "Iteration:  44% 224/513 [00:57<00:41,  6.90it/s]\u001b[A\n",
            "Iteration:  44% 226/513 [00:57<00:38,  7.48it/s]\u001b[A\n",
            "Iteration:  44% 228/513 [00:58<00:39,  7.16it/s]\u001b[A\n",
            "Iteration:  45% 229/513 [00:58<00:39,  7.19it/s]\u001b[A\n",
            "Iteration:  45% 230/513 [00:58<00:48,  5.88it/s]\u001b[A\n",
            "Iteration:  45% 231/513 [00:58<00:45,  6.26it/s]\u001b[A\n",
            "Iteration:  45% 232/513 [00:58<00:42,  6.63it/s]\u001b[A\n",
            "Iteration:  45% 233/513 [00:59<00:48,  5.74it/s]\u001b[A\n",
            "Iteration:  46% 235/513 [00:59<00:39,  7.00it/s]\u001b[A\n",
            "Iteration:  46% 236/513 [00:59<00:37,  7.38it/s]\u001b[A\n",
            "Iteration:  46% 238/513 [00:59<00:32,  8.45it/s]\u001b[A\n",
            "Iteration:  47% 240/513 [00:59<00:29,  9.36it/s]\u001b[A\n",
            "Iteration:  47% 242/513 [00:59<00:34,  7.78it/s]\u001b[A\n",
            "Iteration:  47% 243/513 [01:00<00:33,  7.95it/s]\u001b[A\n",
            "Iteration:  48% 245/513 [01:00<00:33,  7.92it/s]\u001b[A\n",
            "Iteration:  48% 246/513 [01:00<00:43,  6.19it/s]\u001b[A\n",
            "Iteration:  48% 247/513 [01:00<00:46,  5.70it/s]\u001b[A\n",
            "Iteration:  48% 248/513 [01:01<00:50,  5.28it/s]\u001b[A\n",
            "Iteration:  49% 249/513 [01:01<00:45,  5.84it/s]\u001b[A\n",
            "Iteration:  49% 250/513 [01:01<00:40,  6.43it/s]\u001b[A\n",
            "Iteration:  49% 251/513 [01:01<00:38,  6.79it/s]\u001b[A\n",
            "Iteration:  49% 252/513 [01:01<00:37,  6.89it/s]\u001b[A\n",
            "Iteration:  49% 253/513 [01:01<00:35,  7.40it/s]\u001b[A\n",
            "Iteration:  50% 254/513 [01:01<00:43,  5.94it/s]\u001b[A\n",
            "Iteration:  50% 256/513 [01:02<00:45,  5.68it/s]\u001b[A\n",
            "Iteration:  50% 257/513 [01:02<00:40,  6.30it/s]\u001b[A\n",
            "Iteration:  50% 258/513 [01:02<00:46,  5.44it/s]\u001b[A\n",
            "Iteration:  50% 259/513 [01:02<00:44,  5.74it/s]\u001b[A\n",
            "Iteration:  51% 261/513 [01:02<00:37,  6.81it/s]\u001b[A\n",
            "Iteration:  51% 263/513 [01:03<00:32,  7.73it/s]\u001b[A\n",
            "Iteration:  51% 264/513 [01:03<00:40,  6.10it/s]\u001b[A\n",
            "Iteration:  52% 265/513 [01:03<00:49,  4.98it/s]\u001b[A\n",
            "Iteration:  52% 266/513 [01:03<00:50,  4.90it/s]\u001b[A\n",
            "Iteration:  52% 268/513 [01:04<00:40,  6.03it/s]\u001b[A\n",
            "Iteration:  52% 269/513 [01:04<00:36,  6.63it/s]\u001b[A\n",
            "Iteration:  53% 271/513 [01:04<00:34,  7.05it/s]\u001b[A\n",
            "Iteration:  53% 272/513 [01:04<00:37,  6.45it/s]\u001b[A\n",
            "Iteration:  53% 273/513 [01:04<00:35,  6.80it/s]\u001b[A03/07/2020 14:55:29 - INFO - transformers.configuration_utils -   Configuration saved in models/gpt2/weights/checkpoint-1300/config.json\n",
            "03/07/2020 14:55:30 - INFO - transformers.modeling_utils -   Model weights saved in models/gpt2/weights/checkpoint-1300/pytorch_model.bin\n",
            "03/07/2020 14:55:30 - INFO - __main__ -   Saving model checkpoint to models/gpt2/weights/checkpoint-1300\n",
            "03/07/2020 14:55:30 - INFO - __main__ -   Deleting older checkpoint [models/gpt2/weights/checkpoint-1100] due to args.save_total_limit\n",
            "03/07/2020 14:55:36 - INFO - __main__ -   Saving optimizer and scheduler states to models/gpt2/weights/checkpoint-1300\n",
            "\n",
            "Iteration:  53% 274/513 [01:12<09:19,  2.34s/it]\u001b[A\n",
            "Iteration:  54% 275/513 [01:12<06:49,  1.72s/it]\u001b[A\n",
            "Iteration:  54% 276/513 [01:12<05:05,  1.29s/it]\u001b[A\n",
            "Iteration:  54% 277/513 [01:12<03:46,  1.04it/s]\u001b[A\n",
            "Iteration:  54% 278/513 [01:13<02:49,  1.39it/s]\u001b[A\n",
            "Iteration:  54% 279/513 [01:13<02:06,  1.85it/s]\u001b[A\n",
            "Iteration:  55% 280/513 [01:13<01:37,  2.40it/s]\u001b[A\n",
            "Iteration:  55% 281/513 [01:13<01:14,  3.09it/s]\u001b[A\n",
            "Iteration:  55% 283/513 [01:13<00:57,  4.01it/s]\u001b[A\n",
            "Iteration:  55% 284/513 [01:13<00:54,  4.20it/s]\u001b[A\n",
            "Iteration:  56% 285/513 [01:13<00:49,  4.62it/s]\u001b[A\n",
            "Iteration:  56% 286/513 [01:14<00:41,  5.41it/s]\u001b[A\n",
            "Iteration:  56% 288/513 [01:14<00:38,  5.89it/s]\u001b[A\n",
            "Iteration:  56% 289/513 [01:14<00:41,  5.40it/s]\u001b[A\n",
            "Iteration:  57% 290/513 [01:14<00:45,  4.93it/s]\u001b[A\n",
            "Iteration:  57% 292/513 [01:14<00:36,  6.08it/s]\u001b[A\n",
            "Iteration:  57% 293/513 [01:15<00:39,  5.54it/s]\u001b[A\n",
            "Iteration:  57% 294/513 [01:15<00:35,  6.17it/s]\u001b[A\n",
            "Iteration:  58% 295/513 [01:15<00:34,  6.32it/s]\u001b[A\n",
            "Iteration:  58% 296/513 [01:15<00:32,  6.66it/s]\u001b[A\n",
            "Iteration:  58% 298/513 [01:15<00:28,  7.49it/s]\u001b[A\n",
            "Iteration:  58% 299/513 [01:16<00:37,  5.78it/s]\u001b[A\n",
            "Iteration:  59% 301/513 [01:16<00:30,  7.04it/s]\u001b[A\n",
            "Iteration:  59% 302/513 [01:16<00:32,  6.55it/s]\u001b[A\n",
            "Iteration:  59% 304/513 [01:16<00:31,  6.59it/s]\u001b[A\n",
            "Iteration:  59% 305/513 [01:16<00:33,  6.29it/s]\u001b[A\n",
            "Iteration:  60% 306/513 [01:17<00:38,  5.41it/s]\u001b[A\n",
            "Iteration:  60% 307/513 [01:17<00:35,  5.80it/s]\u001b[A\n",
            "Iteration:  60% 308/513 [01:17<00:30,  6.63it/s]\u001b[A\n",
            "Iteration:  60% 309/513 [01:17<00:36,  5.64it/s]\u001b[A\n",
            "Iteration:  61% 311/513 [01:17<00:30,  6.59it/s]\u001b[A\n",
            "Iteration:  61% 312/513 [01:17<00:35,  5.73it/s]\u001b[A\n",
            "Iteration:  61% 314/513 [01:18<00:31,  6.31it/s]\u001b[A\n",
            "Iteration:  62% 316/513 [01:18<00:26,  7.42it/s]\u001b[A\n",
            "Iteration:  62% 318/513 [01:18<00:22,  8.50it/s]\u001b[A\n",
            "Iteration:  62% 320/513 [01:18<00:22,  8.53it/s]\u001b[A\n",
            "Iteration:  63% 321/513 [01:18<00:26,  7.28it/s]\u001b[A\n",
            "Iteration:  63% 322/513 [01:19<00:25,  7.55it/s]\u001b[A\n",
            "Iteration:  63% 324/513 [01:19<00:25,  7.39it/s]\u001b[A\n",
            "Iteration:  63% 325/513 [01:19<00:24,  7.64it/s]\u001b[A\n",
            "Iteration:  64% 326/513 [01:19<00:24,  7.57it/s]\u001b[A\n",
            "Iteration:  64% 328/513 [01:19<00:21,  8.42it/s]\u001b[A\n",
            "Iteration:  64% 329/513 [01:20<00:31,  5.80it/s]\u001b[A\n",
            "Iteration:  65% 331/513 [01:20<00:27,  6.60it/s]\u001b[A\n",
            "Iteration:  65% 332/513 [01:20<00:24,  7.27it/s]\u001b[A\n",
            "Iteration:  65% 333/513 [01:20<00:30,  5.89it/s]\u001b[A\n",
            "Iteration:  65% 334/513 [01:20<00:28,  6.25it/s]\u001b[A\n",
            "Iteration:  65% 336/513 [01:20<00:25,  7.01it/s]\u001b[A\n",
            "Iteration:  66% 338/513 [01:21<00:22,  7.82it/s]\u001b[A\n",
            "Iteration:  66% 340/513 [01:21<00:19,  8.69it/s]\u001b[A\n",
            "Iteration:  66% 341/513 [01:21<00:19,  9.01it/s]\u001b[A\n",
            "Iteration:  67% 342/513 [01:21<00:24,  7.11it/s]\u001b[A\n",
            "Iteration:  67% 344/513 [01:21<00:21,  7.97it/s]\u001b[A\n",
            "Iteration:  67% 346/513 [01:22<00:20,  7.95it/s]\u001b[A\n",
            "Iteration:  68% 348/513 [01:22<00:18,  8.96it/s]\u001b[A\n",
            "Iteration:  68% 350/513 [01:22<00:16,  9.82it/s]\u001b[A\n",
            "Iteration:  69% 352/513 [01:22<00:15, 10.63it/s]\u001b[A\n",
            "Iteration:  69% 354/513 [01:22<00:16,  9.54it/s]\u001b[A\n",
            "Iteration:  69% 356/513 [01:22<00:15, 10.35it/s]\u001b[A\n",
            "Iteration:  70% 358/513 [01:23<00:14, 10.62it/s]\u001b[A\n",
            "Iteration:  70% 360/513 [01:23<00:13, 11.26it/s]\u001b[A\n",
            "Iteration:  71% 362/513 [01:23<00:14, 10.18it/s]\u001b[A\n",
            "Iteration:  71% 364/513 [01:23<00:13, 10.85it/s]\u001b[A\n",
            "Iteration:  71% 366/513 [01:24<00:17,  8.64it/s]\u001b[A\n",
            "Iteration:  72% 368/513 [01:24<00:15,  9.52it/s]\u001b[A\n",
            "Iteration:  72% 370/513 [01:24<00:16,  8.73it/s]\u001b[A\n",
            "Iteration:  73% 372/513 [01:24<00:14,  9.67it/s]\u001b[A03/07/2020 14:55:49 - INFO - transformers.configuration_utils -   Configuration saved in models/gpt2/weights/checkpoint-1400/config.json\n",
            "03/07/2020 14:55:50 - INFO - transformers.modeling_utils -   Model weights saved in models/gpt2/weights/checkpoint-1400/pytorch_model.bin\n",
            "03/07/2020 14:55:50 - INFO - __main__ -   Saving model checkpoint to models/gpt2/weights/checkpoint-1400\n",
            "03/07/2020 14:55:50 - INFO - __main__ -   Deleting older checkpoint [models/gpt2/weights/checkpoint-1200] due to args.save_total_limit\n",
            "03/07/2020 14:56:02 - INFO - __main__ -   Saving optimizer and scheduler states to models/gpt2/weights/checkpoint-1400\n",
            "\n",
            "Iteration:  73% 374/513 [01:37<04:38,  2.00s/it]\u001b[A\n",
            "Iteration:  73% 375/513 [01:37<03:21,  1.46s/it]\u001b[A\n",
            "Iteration:  73% 376/513 [01:37<02:26,  1.07s/it]\u001b[A\n",
            "Iteration:  73% 377/513 [01:37<01:45,  1.28it/s]\u001b[A\n",
            "Iteration:  74% 378/513 [01:38<01:23,  1.61it/s]\u001b[A\n",
            "Iteration:  74% 379/513 [01:38<01:02,  2.13it/s]\u001b[A\n",
            "Iteration:  74% 380/513 [01:38<00:50,  2.64it/s]\u001b[A\n",
            "Iteration:  74% 381/513 [01:38<00:40,  3.28it/s]\u001b[A\n",
            "Iteration:  74% 382/513 [01:38<00:32,  4.00it/s]\u001b[A\n",
            "Iteration:  75% 383/513 [01:38<00:28,  4.55it/s]\u001b[A\n",
            "Iteration:  75% 384/513 [01:39<00:29,  4.33it/s]\u001b[A\n",
            "Iteration:  75% 385/513 [01:39<00:25,  5.04it/s]\u001b[A\n",
            "Iteration:  75% 386/513 [01:39<00:22,  5.59it/s]\u001b[A\n",
            "Iteration:  75% 387/513 [01:39<00:20,  6.09it/s]\u001b[A\n",
            "Iteration:  76% 388/513 [01:39<00:23,  5.31it/s]\u001b[A\n",
            "Iteration:  76% 390/513 [01:39<00:18,  6.49it/s]\u001b[A\n",
            "Iteration:  76% 392/513 [01:40<00:15,  7.75it/s]\u001b[A\n",
            "Iteration:  77% 394/513 [01:40<00:14,  8.19it/s]\u001b[A\n",
            "Iteration:  77% 395/513 [01:40<00:14,  8.39it/s]\u001b[A\n",
            "Iteration:  77% 397/513 [01:40<00:12,  9.28it/s]\u001b[A\n",
            "Iteration:  78% 399/513 [01:40<00:12,  9.26it/s]\u001b[A\n",
            "Iteration:  78% 401/513 [01:41<00:14,  7.97it/s]\u001b[A\n",
            "Iteration:  79% 403/513 [01:41<00:11,  9.30it/s]\u001b[A\n",
            "Iteration:  79% 405/513 [01:41<00:11,  9.06it/s]\u001b[A\n",
            "Iteration:  79% 407/513 [01:41<00:11,  9.51it/s]\u001b[A\n",
            "Iteration:  80% 409/513 [01:41<00:10, 10.09it/s]\u001b[A\n",
            "Iteration:  80% 411/513 [01:41<00:09, 10.33it/s]\u001b[A\n",
            "Iteration:  81% 413/513 [01:42<00:09, 10.82it/s]\u001b[A\n",
            "Iteration:  81% 415/513 [01:42<00:09, 10.18it/s]\u001b[A\n",
            "Iteration:  81% 417/513 [01:42<00:09, 10.04it/s]\u001b[A\n",
            "Iteration:  82% 419/513 [01:42<00:09,  9.98it/s]\u001b[A\n",
            "Iteration:  82% 421/513 [01:42<00:08, 10.75it/s]\u001b[A\n",
            "Iteration:  82% 423/513 [01:43<00:12,  7.27it/s]\u001b[A\n",
            "Iteration:  83% 425/513 [01:43<00:10,  8.11it/s]\u001b[A\n",
            "Iteration:  83% 427/513 [01:43<00:09,  9.25it/s]\u001b[A\n",
            "Iteration:  84% 429/513 [01:43<00:09,  8.92it/s]\u001b[A\n",
            "Iteration:  84% 431/513 [01:44<00:08,  9.92it/s]\u001b[A\n",
            "Iteration:  84% 433/513 [01:44<00:09,  8.86it/s]\u001b[A\n",
            "Iteration:  85% 435/513 [01:44<00:08,  9.73it/s]\u001b[A\n",
            "Iteration:  85% 437/513 [01:44<00:08,  9.43it/s]\u001b[A\n",
            "Iteration:  86% 439/513 [01:45<00:07,  9.46it/s]\u001b[A\n",
            "Iteration:  86% 440/513 [01:45<00:07,  9.18it/s]\u001b[A\n",
            "Iteration:  86% 442/513 [01:45<00:07,  9.66it/s]\u001b[A\n",
            "Iteration:  86% 443/513 [01:45<00:07,  9.61it/s]\u001b[A\n",
            "Iteration:  87% 445/513 [01:45<00:06, 10.26it/s]\u001b[A\n",
            "Iteration:  87% 447/513 [01:45<00:06,  9.47it/s]\u001b[A\n",
            "Iteration:  88% 449/513 [01:46<00:06,  9.21it/s]\u001b[A\n",
            "Iteration:  88% 451/513 [01:46<00:06,  9.76it/s]\u001b[A\n",
            "Iteration:  88% 453/513 [01:46<00:07,  7.94it/s]\u001b[A\n",
            "Iteration:  89% 455/513 [01:46<00:06,  8.61it/s]\u001b[A\n",
            "Iteration:  89% 457/513 [01:46<00:05,  9.71it/s]\u001b[A\n",
            "Iteration:  89% 459/513 [01:47<00:05,  9.31it/s]\u001b[A\n",
            "Iteration:  90% 461/513 [01:47<00:06,  7.67it/s]\u001b[A\n",
            "Iteration:  90% 463/513 [01:47<00:06,  7.88it/s]\u001b[A\n",
            "Iteration:  90% 464/513 [01:47<00:05,  8.19it/s]\u001b[A\n",
            "Iteration:  91% 465/513 [01:48<00:08,  5.94it/s]\u001b[A\n",
            "Iteration:  91% 466/513 [01:48<00:07,  6.61it/s]\u001b[A\n",
            "Iteration:  91% 467/513 [01:48<00:07,  6.18it/s]\u001b[A\n",
            "Iteration:  91% 468/513 [01:48<00:06,  6.58it/s]\u001b[A\n",
            "Iteration:  91% 469/513 [01:48<00:07,  5.55it/s]\u001b[A\n",
            "Iteration:  92% 471/513 [01:48<00:06,  6.62it/s]\u001b[A\n",
            "Iteration:  92% 473/513 [01:49<00:05,  7.68it/s]\u001b[A03/07/2020 14:56:13 - INFO - transformers.configuration_utils -   Configuration saved in models/gpt2/weights/checkpoint-1500/config.json\n",
            "03/07/2020 14:56:14 - INFO - transformers.modeling_utils -   Model weights saved in models/gpt2/weights/checkpoint-1500/pytorch_model.bin\n",
            "03/07/2020 14:56:14 - INFO - __main__ -   Saving model checkpoint to models/gpt2/weights/checkpoint-1500\n",
            "03/07/2020 14:56:14 - INFO - __main__ -   Deleting older checkpoint [models/gpt2/weights/checkpoint-1300] due to args.save_total_limit\n",
            "03/07/2020 14:56:28 - INFO - __main__ -   Saving optimizer and scheduler states to models/gpt2/weights/checkpoint-1500\n",
            "\n",
            "Iteration:  92% 474/513 [02:03<02:56,  4.53s/it]\u001b[A\n",
            "Iteration:  93% 475/513 [02:04<02:01,  3.20s/it]\u001b[A\n",
            "Iteration:  93% 477/513 [02:04<01:21,  2.27s/it]\u001b[A\n",
            "Iteration:  93% 479/513 [02:04<00:54,  1.61s/it]\u001b[A\n",
            "Iteration:  94% 481/513 [02:04<00:36,  1.16s/it]\u001b[A\n",
            "Iteration:  94% 482/513 [02:04<00:26,  1.17it/s]\u001b[A\n",
            "Iteration:  94% 483/513 [02:04<00:18,  1.58it/s]\u001b[A\n",
            "Iteration:  94% 484/513 [02:05<00:14,  1.95it/s]\u001b[A\n",
            "Iteration:  95% 486/513 [02:05<00:10,  2.63it/s]\u001b[A\n",
            "Iteration:  95% 488/513 [02:05<00:07,  3.46it/s]\u001b[A\n",
            "Iteration:  96% 490/513 [02:05<00:05,  4.51it/s]\u001b[A\n",
            "Iteration:  96% 492/513 [02:05<00:03,  5.51it/s]\u001b[A\n",
            "Iteration:  96% 494/513 [02:05<00:03,  6.26it/s]\u001b[A\n",
            "Iteration:  97% 496/513 [02:06<00:02,  6.63it/s]\u001b[A\n",
            "Iteration:  97% 497/513 [02:06<00:02,  6.61it/s]\u001b[A\n",
            "Iteration:  97% 499/513 [02:06<00:02,  6.60it/s]\u001b[A\n",
            "Iteration:  98% 501/513 [02:06<00:01,  7.39it/s]\u001b[A\n",
            "Iteration:  98% 502/513 [02:06<00:01,  7.86it/s]\u001b[A\n",
            "Iteration:  98% 504/513 [02:07<00:01,  8.35it/s]\u001b[A\n",
            "Iteration:  98% 505/513 [02:07<00:01,  7.29it/s]\u001b[A\n",
            "Iteration:  99% 507/513 [02:07<00:00,  7.79it/s]\u001b[A\n",
            "Iteration:  99% 508/513 [02:07<00:00,  8.32it/s]\u001b[A\n",
            "Iteration:  99% 509/513 [02:07<00:00,  8.44it/s]\u001b[A\n",
            "Iteration: 100% 511/513 [02:07<00:00,  8.58it/s]\u001b[A\n",
            "Iteration: 100% 512/513 [02:08<00:00,  8.51it/s]\u001b[A\n",
            "Epoch: 100% 3/3 [06:03<00:00, 118.51s/it]\n",
            "03/07/2020 14:56:32 - INFO - __main__ -    global_step = 1539, average loss = 2.8877120691123452\n",
            "03/07/2020 14:56:32 - INFO - __main__ -   Saving model checkpoint to models/gpt2/weights\n",
            "03/07/2020 14:56:32 - INFO - transformers.configuration_utils -   Configuration saved in models/gpt2/weights/config.json\n",
            "03/07/2020 14:56:35 - INFO - transformers.modeling_utils -   Model weights saved in models/gpt2/weights/pytorch_model.bin\n",
            "03/07/2020 14:56:35 - INFO - transformers.configuration_utils -   loading configuration file models/gpt2/weights/config.json\n",
            "03/07/2020 14:56:35 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 1,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "03/07/2020 14:56:35 - INFO - transformers.modeling_utils -   loading weights file models/gpt2/weights/pytorch_model.bin\n",
            "03/07/2020 14:56:39 - INFO - transformers.tokenization_utils -   Model name 'models/gpt2/weights' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'models/gpt2/weights' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "03/07/2020 14:56:39 - INFO - transformers.tokenization_utils -   Didn't find file models/gpt2/weights/added_tokens.json. We won't load it.\n",
            "03/07/2020 14:56:39 - INFO - transformers.tokenization_utils -   loading file models/gpt2/weights/vocab.json\n",
            "03/07/2020 14:56:39 - INFO - transformers.tokenization_utils -   loading file models/gpt2/weights/merges.txt\n",
            "03/07/2020 14:56:39 - INFO - transformers.tokenization_utils -   loading file None\n",
            "03/07/2020 14:56:39 - INFO - transformers.tokenization_utils -   loading file models/gpt2/weights/special_tokens_map.json\n",
            "03/07/2020 14:56:39 - INFO - transformers.tokenization_utils -   loading file models/gpt2/weights/tokenizer_config.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKl2zr2Gm2Db",
        "colab_type": "text"
      },
      "source": [
        "Although, Huggingface provides a run_generation.py file for language generation. Running it from a command (as it takes the input), makes it load the model and the tokenizer everytime you run the file which slows downs generation. To reduce the I/O overhead, I have restructured the run_generation.py file in the following code which only loads the model and tokenizer once in a model and a tokenizer object and we can use these objects to generate text over and over again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8bXiQaOcvcO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "def get_model_tokenizer(weights_dir, device = 'cuda'):\n",
        "    print(\"Loading Model ...\")\n",
        "    model = GPT2LMHeadModel.from_pretrained(weights_dir)\n",
        "    model.to('cuda')\n",
        "    print(\"Model Loaded ...\")\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(weights_dir)\n",
        "    return model, tokenizer\n",
        "\n",
        "def generate_messages(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt_text,\n",
        "    stop_token,\n",
        "    length,\n",
        "    num_return_sequences,\n",
        "    temperature = 0.7,\n",
        "    k=20,\n",
        "    p=0.9,\n",
        "    repetition_penalty = 1.0,\n",
        "    device = 'cuda'\n",
        "):\n",
        "\n",
        "    MAX_LENGTH = int(10000)\n",
        "    def adjust_length_to_model(length, max_sequence_length):\n",
        "        if length < 0 and max_sequence_length > 0:\n",
        "            length = max_sequence_length\n",
        "        elif 0 < max_sequence_length < length:\n",
        "            length = max_sequence_length  # No generation bigger than model size\n",
        "        elif length < 0:\n",
        "            length = MAX_LENGTH  # avoid infinite loop\n",
        "        return length\n",
        "        \n",
        "    length = adjust_length_to_model(length=length, max_sequence_length=model.config.max_position_embeddings)\n",
        "\n",
        "    encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n",
        "\n",
        "    encoded_prompt = encoded_prompt.to(device)\n",
        "\n",
        "    output_sequences = model.generate(\n",
        "            input_ids=encoded_prompt,\n",
        "            max_length=length + len(encoded_prompt[0]),\n",
        "            temperature=temperature,\n",
        "            top_k=k,\n",
        "            top_p=p,\n",
        "            repetition_penalty=repetition_penalty,\n",
        "            do_sample=True,\n",
        "            num_return_sequences=num_return_sequences,\n",
        "        )\n",
        "\n",
        "    if len(output_sequences.shape) > 2:\n",
        "        output_sequences.squeeze_()\n",
        "\n",
        "    generated_sequences = []\n",
        "\n",
        "    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "        #print(\"=== GENERATED SEQUENCE {} ===\".format(generated_sequence_idx + 1))\n",
        "        generated_sequence = generated_sequence.tolist()\n",
        "\n",
        "        # Decode text\n",
        "        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "        # Remove all text after the stop token\n",
        "        text = text[: text.find(stop_token) if stop_token else None]\n",
        "\n",
        "        # Add the prompt at the beginning of the sequence. Remove the excess text that was used for pre-processing\n",
        "        total_sequence = (\n",
        "            prompt_text + text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]\n",
        "        )\n",
        "\n",
        "        generated_sequences.append(total_sequence)\n",
        "    return generated_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWTddU9mdkKY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "ae53955c-8e67-440e-caa5-778cb29fce65"
      },
      "source": [
        "model, tokenizer = get_model_tokenizer(weights_dir, device = 'cuda')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Model ...\n",
            "Model Loaded ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQlGY-XWpTdv",
        "colab_type": "text"
      },
      "source": [
        "These are the hyper-parameters for sampling of tokens(one token at a time)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqrZXJtcdnCj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temperature = 1.0\n",
        "k=400\n",
        "p=0.9\n",
        "repetition_penalty = 1.0\n",
        "num_return_sequences = 5\n",
        "length = 1000\n",
        "stop_token = '|EndOfText|'\n",
        "prompt_text = \"this is\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOP3rozwe9gv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "958b402e-a59a-41f0-9a62-1ac8befaf357"
      },
      "source": [
        "%%time\n",
        "generate_messages(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt_text,\n",
        "    stop_token,\n",
        "    length,\n",
        "    num_return_sequences,\n",
        "    temperature = temperature,\n",
        "    k=k,\n",
        "    p=p,\n",
        "    repetition_penalty = repetition_penalty\n",
        ")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 12.7 s, sys: 2.03 s, total: 14.7 s\n",
            "Wall time: 14.7 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"this is an amazing film, but i thought they had this film. It would be perfect if the cast was more independent. The only reason i was scared is because if they didn't have a director, they would've done the same. I'm not a big fan of any actors, but this movie is one of the better ones. The acting is very good, and the dialogue is nice. The script is always very touching and it makes my heart feel fresh. Overall it was a good movie. \",\n",
              " 'this is still a problem. I read this from the review and found it out.<br /><br />I am a woman who is tired of seeing this horror flick. Although its a very bad movie i do think it is really bad and this movie is definitely worth watching. ',\n",
              " \"this is, of course, bad. I also like watching a movie with an audience. The actors were all wasted, but if I had a screenwriting opportunity, I would have liked to see more like this, but the director was stupid. He was an idiot, but that being said, I really don't care. And I get the feeling from watching this. \",\n",
              " \"this is not good for this film. It shows the constant desire for beauty, the horror of the past, and the terror of the future. There is nothing sexy about this film. It feels weird to watch one of my favorite movies of all time and try not to be nauseating in the way this film is. It is frightening. It even resembles a film from the nineties, where I was in college. I watched the first twenty-one minutes, which was all better than the twenty-one. I was terrified that I would get bored of the film if it didn't do anything about it, even if they were good. There are no sexual features. There are no sex scenes whatsoever.<br /><br />It's not a good movie. It's a depressing film, a wretched film. I can't imagine if this would be a good film, or worse, a pathetic movie. It seems as though the director should have made better movies, like this one. This is just stupid. Its silly. The plot is all on the basis of people pretending to be bad. The plot is simply stupid. The end is like a high school football game with no special place on the field. It is so absurd. We will never see a film like this again. I watched it for twenty years. I have seen it all but four times.<br /><br />I was in high school with the third grade when it was my first film. My teacher watched it once. I feel it is that pointless. It is stupid. I don't think I have ever seen it before. I don't know if it's a substitute for film classes.<br /><br />No, I was scared. I was scared because I wasn't allowed to get off the football field. It felt like I was being watched a bunch of random males.<br /><br />I didn't really feel that I was allowed to do anything. It just felt like I was being shown a movie, but I couldn't figure out anything about it. The acting was terrible. It seemed as though the young people in this film were all in a similar fashion as they were watching it. \",\n",
              " \"this is the worst film I have ever seen - it looks like someone could have made an action flick - because it has always been an action flick, which would have ended there. By the time I saw this one, I knew I was not alone in my dislike of this film. I had never seen the first time I saw this and I was not alone when it came to this. I usually don't see this sort of movie - in the first few months of the film (either in the film itself or in a few other scenes). If you want to watch it, you'll need to understand why this was supposed to be such a bad movie. But here's what I did after watching the first one: A violent sub-plot and ending that was neither good nor stupid. \"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpXtDNZ4dunQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
