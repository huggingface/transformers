#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/vibevoice_semantic_tokenizer/modular_vibevoice_semantic_tokenizer.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_vibevoice_semantic_tokenizer.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# coding=utf-8
# Copyright 2025 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import math
from dataclasses import dataclass
from typing import Optional

import torch
import torch.nn as nn
import torch.nn.functional as F

from ...activations import ACT2FN
from ...modeling_utils import PreTrainedModel
from .configuration_vibevoice_semantic_tokenizer import VibeVoiceSemanticTokenizerConfig
from ...utils import ModelOutput, auto_docstring


@dataclass
@auto_docstring
class VibeVoiceSemanticTokenizerOutput(ModelOutput):
    """
    latents (`torch.Tensor` of shape `(batch_size, dimension, time_steps)`):
        Projected latents (continuous representations for semantic tokens).
    """

    latents: Optional[torch.FloatTensor] = None


# TODO use modular from `LlamaRMSNorm`
class RMSNorm(nn.Module):
    def __init__(self, hidden_size: int, eps: float = 1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states):
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states.to(input_dtype)

    def extra_repr(self):
        return f"{tuple(self.weight.shape)}, eps={self.variance_epsilon}"


class VibeVoiceEncoderFeedForward(nn.Module):
    def __init__(self, config, hidden_size):
        super().__init__()
        self.linear1 = nn.Linear(hidden_size, config.ffn_expansion * hidden_size, bias=config.bias)
        self.activation = ACT2FN[config.hidden_act]
        self.linear2 = nn.Linear(config.ffn_expansion * hidden_size, hidden_size, bias=config.bias)

    def forward(self, x):
        return self.linear2(self.activation(self.linear1(x)))


class VibeVoiceTokenizerStreamingCache:
    """Cache for streaming convolution, similar to KV cache in attention"""

    def __init__(self):
        self.cache = {}  # Dict mapping (layer_id, sample_idx) to state tensor

    def get(self, layer_id: str, sample_indices: torch.Tensor) -> Optional[torch.Tensor]:
        """Get cached states for given layer and sample indices"""
        states = []
        max_length = 0

        # First pass: collect states and find max length
        for idx in sample_indices.tolist():
            key = (layer_id, idx)
            if key not in self.cache:
                return None  # If any sample is missing, return None
            state = self.cache[key]
            states.append(state)
            max_length = max(max_length, state.shape[-1])

        # Second pass: pad states to max length if needed
        if len(states) > 0 and states[0].dim() >= 2:
            padded_states = []
            for state in states:
                if state.shape[-1] < max_length:
                    # Pad on the time dimension (last dimension)
                    pad_size = max_length - state.shape[-1]
                    # Pad with zeros on the LEFT to align the most recent samples
                    padded_state = F.pad(state, (pad_size, 0), mode="constant", value=0)
                    padded_states.append(padded_state)
                else:
                    padded_states.append(state)
            return torch.stack(padded_states, dim=0)
        else:
            return torch.stack(states, dim=0)

    def set(self, layer_id: str, sample_indices: torch.Tensor, states: torch.Tensor):
        """Set cached states for given layer and sample indices"""
        for i, idx in enumerate(sample_indices.tolist()):
            key = (layer_id, idx)
            self.cache[key] = states[i].detach()

    def set_to_zero(self, sample_indices: torch.Tensor):
        """Set all cached states to zero for given sample indices"""
        for key in list(self.cache.keys()):
            layer_id, sample_idx = key
            if sample_idx in sample_indices.tolist():
                # Create zero tensor with same shape and dtype as cached tensor
                cached_tensor = self.cache[key]
                self.cache[key] = torch.zeros_like(cached_tensor)

    def clear(self, layer_id: Optional[str] = None, sample_indices: Optional[torch.Tensor] = None):
        """Clear cache for specific layer/samples or everything"""
        if layer_id is None and sample_indices is None:
            self.cache.clear()
        elif layer_id is not None and sample_indices is None:
            # Clear all samples for a specific layer
            keys_to_remove = [k for k in self.cache.keys() if k[0] == layer_id]
            for k in keys_to_remove:
                del self.cache[k]
        elif layer_id is not None and sample_indices is not None:
            # Clear specific samples for a specific layer
            for idx in sample_indices.tolist():
                key = (layer_id, idx)
                self.cache.pop(key, None)


class StreamingConv1d(nn.Module):
    """Conv1d with built-in handling of streaming, causal padding, and normalization."""

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int = 1,
        dilation: int = 1,
        groups: int = 1,
        bias: bool = True,
    ):
        super().__init__()
        self.conv = nn.Conv1d(
            in_channels, out_channels, kernel_size, stride, dilation=dilation, groups=groups, bias=bias
        )

        # Store configuration
        self.kernel_size = kernel_size
        self.stride = stride

        # For streaming mode (causal convolution), need to maintain kernel_size - 1 samples as context
        # to check use which context_size is more suitable
        self.context_size = (kernel_size - 1) * dilation - (stride - 1)

        # For non-streaming mode, calculate padding
        self.padding_total = (kernel_size - 1) * dilation - (stride - 1)

        # Create a unique layer ID for cache management
        self._layer_id = None

    @property
    def layer_id(self):
        if self._layer_id is None:
            self._layer_id = f"sconv1d_{id(self)}"
        return self._layer_id

    def forward(
        self,
        x: torch.Tensor,
        cache: Optional[VibeVoiceTokenizerStreamingCache] = None,
        sample_indices: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        """
        Forward pass with optional streaming support via cache.

        Args:
            x: Input tensor [batch_size, channels, time]
            cache: VibeVoiceTokenizerStreamingCache object for maintaining states
            sample_indices: Indices identifying each sample for cache management

        Returns:
            Output tensor
        """
        batch_size, channels, _ = x.shape

        # Non-streaming mode
        if cache is None:
            # Compute extra padding for stride alignment
            n_frames = (x.shape[-1] - self.kernel_size + self.padding_total) / self.stride + 1
            ideal_length = (math.ceil(n_frames) - 1) * self.stride + (self.kernel_size - self.padding_total)
            extra_padding = ideal_length - x.shape[-1]
            x = F.pad(x, (self.padding_total, extra_padding), mode="constant", value=0)
            return self.conv(x)

        # Streaming mode (TODO simplify this code path)
        if sample_indices is None:
            raise ValueError("sample_indices must be provided for streaming mode")
        if len(sample_indices) != batch_size:
            raise ValueError("sample_indices must match batch size")

        # Cache operations (not compiled)
        cached_states = cache.get(self.layer_id, sample_indices)

        if cached_states is None:
            # First chunk - initialize with zeros for context
            if self.context_size > 0:
                cached_states = torch.zeros(batch_size, channels, self.context_size, device=x.device, dtype=x.dtype)
            else:
                cached_states = torch.zeros(batch_size, channels, 0, device=x.device, dtype=x.dtype)

        # Concatenate cached states with input
        if cached_states.shape[2] > 0:
            input_with_context = torch.cat([cached_states, x], dim=2)
        else:
            input_with_context = x

        # Apply convolution directly - no extra padding in streaming mode
        # The conv layer will handle its own padding internally
        output = self.conv(input_with_context)

        # Update cache for next chunk
        if self.context_size > 0:
            # Calculate how many samples to keep
            total_input_length = input_with_context.shape[2]

            # Keep the last context_size samples
            if total_input_length >= self.context_size:
                new_cache_start = total_input_length - self.context_size
                new_cache = input_with_context[:, :, new_cache_start:]
            else:
                # If we have less than context_size samples, keep everything
                new_cache = input_with_context

            cache.set(self.layer_id, sample_indices, new_cache)

        return output


class ConvNext1dLayer(nn.Module):
    """
    ConvNeXt-like block adapted for 1D convolutions, used in VibeVoice tokenizer encoder.

    For reference, original 2D `ConvNextLayer`:
    https://github.com/huggingface/transformers/blob/e20df45bf676d80bdddb9757eeeafe6c0c81ecfa/src/transformers/models/convnext/modeling_convnext.py#L120
    """
    def __init__(self, config, hidden_size, drop_path=0.0):
        super().__init__()

        self.norm = RMSNorm(hidden_size, eps=config.rms_norm_eps)
        self.ffn_norm = RMSNorm(hidden_size, eps=config.rms_norm_eps)
        self.mixer = StreamingConv1d(
            in_channels=hidden_size,
            out_channels=hidden_size,
            kernel_size=config.kernel_size,
            groups=hidden_size,
            bias=config.bias,
        )
        self.ffn = VibeVoiceEncoderFeedForward(config, hidden_size)
        if config.layer_scale_init_value > 0:
            self.gamma = nn.Parameter(config.layer_scale_init_value * torch.ones((hidden_size)), requires_grad=True)
            self.ffn_gamma = nn.Parameter(config.layer_scale_init_value * torch.ones((hidden_size)), requires_grad=True)
        else:
            self.gamma = None
            self.ffn_gamma = None

        # TODO (ebezzam) original code has option for DropPath but is never actually used (and `nn.modules.DropPath` does not exist): https://github.com/pengzhiliang/transformers/blob/6e6e60fb95ca908feb0b039483adcc009809f579/src/transformers/models/vibevoice/modular_vibevoice_tokenizer.py#L637
        # however, could be interesting feature for future versions of `ConvNext1dLayer` as the 2D version has it: https://github.com/huggingface/transformers/blob/e20df45bf676d80bdddb9757eeeafe6c0c81ecfa/src/transformers/models/convnext/modeling_convnext.py#L146
        if drop_path > 0.0:
            # possible implementation (that may needed to be adapted for 1D):
            # https://github.com/huggingface/transformers/blob/e20df45bf676d80bdddb9757eeeafe6c0c81ecfa/src/transformers/models/convnext/modeling_convnext.py#L40
            raise NotImplementedError("DropPath is not implemented.")
        self.drop_path = nn.Identity()

    def forward(self, x):
        # mixer
        residual = x
        x = self.norm(x.transpose(1, 2)).transpose(1, 2)
        x = self.mixer(x)
        if self.gamma is not None:
            x = x * self.gamma.unsqueeze(-1)
        # (ebezzam) original code (https://github.com/pengzhiliang/transformers/blob/6e6e60fb95ca908feb0b039483adcc009809f579/src/transformers/models/vibevoice/modular_vibevoice_tokenizer.py#L653)
        # as mentioned above, drop_path is not used and the VibeVoice authors don't use the `forward` method but a custom
        # call which does `residual + x` directly (see link below), which is same as using identity
        # https://github.com/pengzhiliang/transformers/blob/6e6e60fb95ca908feb0b039483adcc009809f579/src/transformers/models/vibevoice/modular_vibevoice_tokenizer.py#L768
        x = residual + self.drop_path(x)

        # ffn
        residual = x
        x = self.ffn_norm(x.transpose(1, 2)).transpose(1, 2)
        x = x.permute(0, 2, 1)
        x = self.ffn(x)
        x = x.permute(0, 2, 1)
        if self.ffn_gamma is not None:
            x = x * self.ffn_gamma.unsqueeze(-1)
        # (ebezzam) see comment above
        x = residual + self.drop_path(x)
        return x


class VibeVoiceSemanticTokenizerEncoder(nn.Module):
    """
    Encoder component for the VibeVoice tokenizer that converts audio to latent representations.

    Paper (https://arxiv.org/pdf/2508.19205) says:
    "7 stages of modified Transformer blocks (using 1D depth-wise causal convolutions instead of self-attention module)
    for efficient streaming processing. Six downsampling layers achieve a cumulative 3200X downsampling rate from a
    24kHz input, yielding 7.5 tokens/frames per second."

    But each block is more like a ConvNeXt block (but 1D): https://arxiv.org/abs/2201.03545
    Hence the name `ConvNext1dLayer` in this code for the blocks.

    Args:
        config: Configuration object with model parameters
    """

    def __init__(self, config):
        super().__init__()

        # stem and intermediate downsampling conv layers
        self.downsample_layers = nn.ModuleList()
        self.downsample_layers.append(
            StreamingConv1d(
                in_channels=config.channels,
                out_channels=config.n_filters,
                kernel_size=config.kernel_size,
                bias=config.bias,
            )
        )
        for i in range(len(config.downsampling_ratios)):
            downsample_layer = StreamingConv1d(
                in_channels=config.n_filters * (2**i),
                out_channels=config.n_filters * (2 ** (i + 1)),
                kernel_size=config.downsampling_ratios[i] * 2,
                stride=config.downsampling_ratios[i],
                bias=config.bias,
            )
            self.downsample_layers.append(downsample_layer)

        # configure ConvNext1D blocks
        self.stages = nn.ModuleList()
        for i in range(len(config.depths)):
            in_ch = config.n_filters * (2**i)
            stage = nn.ModuleList([
                ConvNext1dLayer(config, hidden_size=in_ch)
                for _ in range(config.depths[i])
            ])
            self.stages.append(stage)

        self.head = StreamingConv1d(
            in_channels=in_ch,
            out_channels=config.hidden_size,
            kernel_size=config.kernel_size,
            bias=config.bias,
        )

    def forward(self, x, cache=None, sample_indices=None):
        for i, downsample_layer in enumerate(self.downsample_layers):
            x = downsample_layer(x, cache=cache, sample_indices=sample_indices)
            for block in self.stages[i]:
                x = block(x)
        x = self.head(x, cache=cache, sample_indices=sample_indices)
        return x.permute(0, 2, 1)


@auto_docstring
class VibeVoiceSemanticTokenizerPreTrainedModel(PreTrainedModel):
    config = VibeVoiceSemanticTokenizerConfig
    base_model_prefix = "vibevoice_semantic_tokenizer"
    main_input_name = "audio"
    _supports_flash_attn_2 = True
    _supports_sdpa = True
    _no_split_modules = ["VibeVoiceSemanticTokenizerEncoder"]

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.normal_(module.weight, std=self.config.weight_init_value)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Conv1d):
            nn.init.normal_(module.weight, std=self.config.weight_init_value)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, RMSNorm):
            nn.init.ones_(module.weight)


@auto_docstring
class VibeVoiceSemanticTokenizerModel(VibeVoiceSemanticTokenizerPreTrainedModel):
    """Encoder-only VibeVoice tokenizer model for semantic tokens."""

    def __init__(self, config):
        super().__init__(config)

        # Initialize encoder
        self.encoder = VibeVoiceSemanticTokenizerEncoder(config)

        # Initialize weights and apply final processing
        self.post_init()

    @auto_docstring
    def encode(self, audio, cache=None, sample_indices=None):
        r"""
        audio (`torch.FloatTensor` of shape `(batch_size, channels, sequence_length)`):
            Input audio waveform to be encoded into latent representations.
        cache (`VibeVoiceTokenizerStreamingCache`, *optional*):
            Cache object for streaming mode to maintain convolution states.
        sample_indices (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
            Indices identifying each sample in the batch for cache management.
        """
        latents = self.encoder(audio, cache=cache, sample_indices=sample_indices)
        return VibeVoiceSemanticTokenizerOutput(latents=latents)

    @auto_docstring
    def forward(self, audio, cache=None, sample_indices=None):
        r"""
        audio (`torch.FloatTensor` of shape `(batch_size, channels, sequence_length)`):
            Input audio waveform to be encoded into latent representations.
        cache (`VibeVoiceTokenizerStreamingCache`, *optional*):
            Cache object for streaming mode to maintain convolution states.
        sample_indices (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
            Indices identifying each sample in the batch for cache management.     
        """
        return self.encode(audio, cache=cache, sample_indices=sample_indices)


__all__ = ["VibeVoiceTokenizerStreamingCache", "VibeVoiceSemanticTokenizerModel"]
