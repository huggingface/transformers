#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/audioflamingo3/modular_audioflamingo3.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_audioflamingo3.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# coding=utf-8
# Copyright 2025 NVIDIA CORPORATION and The HuggingFace Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import copy
import math
from typing import Any, Callable, Optional, Union

import torch
from torch import nn

from ...activations import ACT2FN
from ...cache_utils import Cache, EncoderDecoderCache
from ...generation import GenerationConfig, GenerationMixin
from ...modeling_flash_attention_utils import FlashAttentionKwargs
from ...modeling_layers import GradientCheckpointingLayer
from ...modeling_outputs import BaseModelOutput
from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
from ...processing_utils import Unpack
from ...utils import auto_docstring, logging
from ...utils.deprecation import deprecate_kwarg
from ..auto import AutoModel, AutoModelForCausalLM
from .configuration_audioflamingo3 import AudioFlamingo3Config, AudioFlamingo3EncoderConfig


logger = logging.get_logger(__name__)


def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    scaling: Optional[float] = None,
    dropout: float = 0.0,
    head_mask: Optional[torch.Tensor] = None,
    **kwargs,
):
    if scaling is None:
        scaling = query.size(-1) ** -0.5

    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling
    if attention_mask is not None and attention_mask.ndim == 4:
        attn_weights = attn_weights + attention_mask[:, :, :, : key.shape[-2]]

    attn_weights = nn.functional.softmax(attn_weights, dim=-1)

    if head_mask is not None:
        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)

    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
    attn_output = torch.matmul(attn_weights, value)
    attn_output = attn_output.transpose(1, 2).contiguous()

    return attn_output, attn_weights


class AudioFlamingo3Attention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(
        self,
        embed_dim: int,
        num_heads: int,
        dropout: float = 0.0,
        is_decoder: bool = False,
        bias: bool = True,
        is_causal: bool = False,
        layer_idx: Optional[int] = None,
        config: Optional[AudioFlamingo3Config] = None,
    ):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.dropout = dropout
        self.head_dim = embed_dim // num_heads
        self.config = config

        if (self.head_dim * num_heads) != self.embed_dim:
            raise ValueError(
                f"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}"
                f" and `num_heads`: {num_heads})."
            )
        self.scaling = self.head_dim**-0.5
        self.is_decoder = is_decoder
        self.is_causal = is_causal

        if layer_idx is None and is_decoder:
            logger.warning_once(
                f"Instantiating a decoder {self.__class__.__name__} without passing `layer_idx` is not recommended and "
                "will to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` "
                "when creating this class."
            )
        self.layer_idx = layer_idx

        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False)
        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)
        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)
        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)

    @deprecate_kwarg("past_key_value", new_name="past_key_values", version="4.58")
    def forward(
        self,
        hidden_states: torch.Tensor,
        key_value_states: Optional[torch.Tensor] = None,
        past_key_values: Optional[Cache] = None,
        attention_mask: Optional[torch.Tensor] = None,
        layer_head_mask: Optional[torch.Tensor] = None,
        output_attentions: bool = False,
        cache_position: Optional[torch.Tensor] = None,
        # TODO: we need a refactor so that the different attention modules can get their specific kwargs
        # ATM, we have mixed things encoder, decoder, and encoder-decoder attn
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:
        """Input shape: Batch x Time x Channel"""

        # if key_value_states are provided this layer is used as a cross-attention layer
        # for the decoder
        is_cross_attention = key_value_states is not None

        # determine input shapes
        bsz, tgt_len = hidden_states.shape[:-1]
        q_input_shape = (bsz, tgt_len, -1, self.head_dim)

        # Scaling is susceptible to floating point arithmetics' inprecisions
        # which can lead to different results (this is dependent from model
        # to model, e.g. audioflamingo3 is one such case). We therefore keep the
        # original order of scaling to follow the original implementation
        # and enforce no scaling (1.0) in the attention call below.
        query_states = self.q_proj(hidden_states) * self.scaling
        query_states = query_states.view(*q_input_shape)
        query_states = query_states.transpose(1, 2).contiguous()

        # Check is encoder-decoder model is being used. Otherwise we'll get `DynamicCache`
        if past_key_values is not None and isinstance(past_key_values, EncoderDecoderCache):
            is_updated = past_key_values.is_updated.get(self.layer_idx)
            if is_cross_attention:
                # after the first generated id, we can subsequently re-use all key/value_states from cache
                past_key_values.is_updated[self.layer_idx] = True
                past_key_values = past_key_values.cross_attention_cache
            else:
                past_key_values = past_key_values.self_attention_cache

        # use key_value_states if cross attention
        current_states = key_value_states if key_value_states is not None else hidden_states
        if is_cross_attention and past_key_values and is_updated:
            # reuse k,v, cross_attentions
            key_states = past_key_values.layers[self.layer_idx].keys
            value_states = past_key_values.layers[self.layer_idx].values
        else:
            key_states = self.k_proj(current_states).view(bsz, -1, self.num_heads, self.head_dim)
            value_states = self.v_proj(current_states).view(bsz, -1, self.num_heads, self.head_dim)
            key_states = key_states.transpose(1, 2).contiguous()
            value_states = value_states.transpose(1, 2).contiguous()
            if past_key_values is not None:
                # save all key/value_states to cache to be re-used for fast auto-regressive generation
                cache_position = cache_position if not is_cross_attention else None
                key_states, value_states = past_key_values.update(
                    key_states, value_states, self.layer_idx, {"cache_position": cache_position}
                )

        attention_interface: Callable = eager_attention_forward
        if self.config._attn_implementation != "eager":
            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

        attn_output, attn_weights = attention_interface(
            self,
            query_states,
            key_states,
            value_states,
            attention_mask,
            dropout=0.0 if not self.training else self.dropout,
            scaling=1.0,
            output_attentions=output_attentions,
            head_mask=layer_head_mask,
            **kwargs,
        )

        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()
        attn_output = self.out_proj(attn_output)

        return attn_output, attn_weights


class AudioFlamingo3EncoderLayer(GradientCheckpointingLayer):
    def __init__(self, config: AudioFlamingo3Config):
        super().__init__()
        self.embed_dim = config.d_model

        self.self_attn = AudioFlamingo3Attention(
            embed_dim=self.embed_dim,
            num_heads=config.encoder_attention_heads,
            dropout=config.attention_dropout,
            config=config,
        )
        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)
        self.dropout = config.dropout
        self.activation_fn = ACT2FN[config.activation_function]
        self.activation_dropout = config.activation_dropout
        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)
        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)
        self.final_layer_norm = nn.LayerNorm(self.embed_dim)

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: torch.Tensor,
        layer_head_mask: torch.Tensor,
        output_attentions: bool = False,
    ) -> torch.Tensor:
        """
        Args:
            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`
            attention_mask (`torch.FloatTensor`): attention mask of size
                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size
                `(encoder_attention_heads,)`.
            output_attentions (`bool`, *optional*):
                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
                returned tensors for more detail.
        """
        residual = hidden_states
        hidden_states = self.self_attn_layer_norm(hidden_states)
        hidden_states, attn_weights = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            layer_head_mask=layer_head_mask,
            output_attentions=output_attentions,
        )
        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)
        hidden_states = residual + hidden_states

        residual = hidden_states
        hidden_states = self.final_layer_norm(hidden_states)
        hidden_states = self.activation_fn(self.fc1(hidden_states))
        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)
        hidden_states = self.fc2(hidden_states)
        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)
        hidden_states = residual + hidden_states

        if hidden_states.dtype == torch.float16:
            clamp_value = torch.finfo(hidden_states.dtype).max - 1000
            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)

        return hidden_states, attn_weights


@auto_docstring
class AudioFlamingo3PreTrainedModel(PreTrainedModel):
    """
    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained
    models.
    """

    config_class = AudioFlamingo3Config
    base_model_prefix = "model"
    supports_gradient_checkpointing = True
    _no_split_modules = ["AudioFlamingo3Attention"]
    _skip_keys_device_placement = "past_key_values"
    _supports_flash_attn = True
    _supports_sdpa = True

    def _init_weights(self, module: nn.Module) -> None:
        std = (
            self.config.initializer_range
            if hasattr(self.config, "initializer_range")
            else self.config.audio_config.initializer_range
        )

        if isinstance(module, (nn.Linear, nn.Conv1d)):
            module.weight.data.normal_(mean=0.0, std=std)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.LayerNorm):
            module.weight.data.fill_(1.0)
            module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=std)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()


class AudioFlamingo3Encoder(AudioFlamingo3PreTrainedModel):
    """
    Whisper encoder + one extra AvgPool downsample (time/2) then LayerNorm.
    Also accepts a (B, T_mel) frame-validity mask and converts it to a square mask.
    """

    config: AudioFlamingo3EncoderConfig

    def __init__(self, config: AudioFlamingo3EncoderConfig):
        super().__init__(config)
        self.dropout = config.dropout
        self.layerdrop = config.encoder_layerdrop

        embed_dim = config.d_model
        self.num_mel_bins = config.num_mel_bins
        self.padding_idx = config.pad_token_id
        self.max_source_positions = config.max_source_positions
        self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0

        self.conv1 = nn.Conv1d(self.num_mel_bins, embed_dim, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(embed_dim, embed_dim, kernel_size=3, stride=2, padding=1)

        self.embed_positions = nn.Embedding(self.max_source_positions, embed_dim)
        self.embed_positions.requires_grad_(False)

        self.layers = nn.ModuleList([AudioFlamingo3EncoderLayer(config) for _ in range(config.encoder_layers)])
        self.layer_norm = nn.LayerNorm(config.d_model)

        self.gradient_checkpointing = False
        # Additional downsampling after transformer
        self.avg_pooler = nn.AvgPool1d(config.avg_pool_kernel_size, stride=config.avg_pool_stride)
        # Initialize weights and apply final processing
        self.post_init()

    def _freeze_parameters(self):
        for param in self.parameters():
            param.requires_grad = False
        self._requires_grad = False

    def get_input_embeddings(self) -> nn.Module:
        return self.conv1

    def set_input_embeddings(self, value: nn.Module):
        self.conv1 = value

    def forward(
        self,
        input_features: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        head_mask: Optional[torch.Tensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[BaseModelOutput, tuple]:
        r"""
        Args:
            input_features (`torch.LongTensor` of shape `(batch_size, feature_size, sequence_length)`):
                Float values of mel features extracted from the raw speech waveform. Raw speech waveform can be
                obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a
                `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library (`pip install torchcodec`) or
                the soundfile library (`pip install soundfile`). To prepare the array into
                `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the mel features, padding
                and conversion into a tensor of type `torch.FloatTensor`. See [`~AudioFlamingo3FeatureExtractor.__call__`]
            attention_mask (`torch.Tensor`)`, *optional*):
                AudioFlamingo3 does not support masking of the `input_features`, this argument is preserved for compatibility,
                but it is not used. By default the silence in the input log mel spectrogram are ignored.
            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):
                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:

                - 1 indicates the head is **not masked**,
                - 0 indicates the head is **masked**.
            output_attentions (`bool`, *optional*):
                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
                returned tensors for more detail.
            output_hidden_states (`bool`, *optional*):
                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors
                for more detail.
            return_dict (`bool`, *optional*):
                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
        """
        # Convert 2D frame mask to Whisper's 4D square mask
        if attention_mask is not None and attention_mask.dim() == 2:
            attention_mask = self._build_square_attn_mask(attention_mask, input_features.shape[-1])

        output_attentions = self.config.output_attentions if output_attentions is None else output_attentions
        output_hidden_states = (
            self.config.output_hidden_states if output_hidden_states is None else output_hidden_states
        )
        return_dict = self.config.use_return_dict if return_dict is None else return_dict

        # dtype/device normalization
        x = input_features.to(dtype=self.conv1.weight.dtype, device=self.conv1.weight.device)  # (B, M, T)

        # Frontend convolutions
        x = nn.functional.gelu(self.conv1(x))
        x = nn.functional.gelu(self.conv2(x))  # (B, d_model, T/2)

        # Time-major for transformer: (B, S, C)
        x = x.permute(0, 2, 1)
        embed_pos = self.embed_positions.weight  # (max_source_positions, C)
        if embed_pos.shape[0] < x.shape[1]:
            raise ValueError(f"embed_positions shorter than sequence length: {embed_pos.shape[0]} < {x.shape[1]}")
        x = x + embed_pos[: x.shape[1]]
        x = nn.functional.dropout(x, p=self.dropout, training=self.training)

        encoder_states = () if output_hidden_states else None
        all_attns = () if output_attentions else None

        if head_mask is not None:
            if head_mask.size(0) != len(self.layers):
                raise ValueError(f"head_mask should have {len(self.layers)} layers, but has {head_mask.size(0)}.")

        hidden_states = x
        for idx, layer in enumerate(self.layers):
            if output_hidden_states:
                encoder_states = encoder_states + (hidden_states,)

            to_drop = self.training and (torch.rand([]) < self.layerdrop)
            if to_drop:
                layer_outputs = (hidden_states, None)
            else:
                layer_outputs = layer(
                    hidden_states,
                    attention_mask,
                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),
                    output_attentions=output_attentions,
                )
                hidden_states = layer_outputs[0]

            if output_attentions:
                all_attns = all_attns + (layer_outputs[1],)

        # Post transformer: extra pooling + final LN
        hs = hidden_states.permute(0, 2, 1)  # (B, C, S)
        hs = self.avg_pooler(hs).permute(0, 2, 1)  # (B, S', C)
        hs = self.layer_norm(hs)

        if output_hidden_states:
            encoder_states = encoder_states + (hs,)

        if not return_dict:
            return tuple(v for v in (hs, encoder_states, all_attns) if v is not None)

        return BaseModelOutput(last_hidden_state=hs, hidden_states=encoder_states, attentions=all_attns)

    def _build_square_attn_mask(self, mask_1d: torch.Tensor, max_mel_seq_len: int) -> torch.Tensor:
        """
        Convert a (B, T_mel) frame-validity mask to Whisper's 4D square mask (B, 1, S, S)
        with -inf on padded positions. Here S = (T_mel - 2) // 2 + 1 (after frontend).
        """
        if mask_1d.dim() != 2:
            raise ValueError(f"mask_1d must be (B, T_mel), got {tuple(mask_1d.shape)}")
        if mask_1d.shape[1] != max_mel_seq_len:
            raise ValueError(f"T_mel mismatch: mask {mask_1d.shape[1]} vs features {max_mel_seq_len}")

        # length after first stride-2 conv
        audio_feat_lengths = ((mask_1d.sum(-1).to(torch.long) - 1) // 2) + 1
        B = mask_1d.shape[0]
        S = (max_mel_seq_len - 2) // 2 + 1

        seq = torch.arange(S, device=mask_1d.device).unsqueeze(0).expand(B, S)
        padding = seq >= audio_feat_lengths.unsqueeze(1)  # True => pad

        square = padding.view(B, 1, 1, S).expand(B, 1, S, S)
        attn = square.to(dtype=self.conv1.weight.dtype, device=self.conv1.weight.device)
        attn[square] = float("-inf")
        return attn


@auto_docstring(
    custom_intro="""
    The AudioFlamingo3 model which consists of an audio backbone and a language model.
    """
)
class AudioFlamingo3ForConditionalGeneration(AudioFlamingo3PreTrainedModel, GenerationMixin):
    """
    AudioFlamingo3 model for conditional generation tasks. This model inherits from [`PreTrainedModel`]. Check the
    superclass documentation for the generic methods the library implements for all its model (such as downloading
    or saving, resizing the input embeddings, pruning heads etc.)

    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
    and behavior.

    Args:
        config (AudioFlamingo3Config): Model configuration class with all the parameters of the model.
            Initializing with a config file does not load the weights associated with the model, only the
            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
    """

    config_class = AudioFlamingo3Config

    def __init__(self, config: Optional[AudioFlamingo3Config] = None, *args: Any, **kwargs: Any) -> None:
        super().__init__(config)

        self.llm = AutoModelForCausalLM.from_config(config.text_config)
        self.sound_tower = AutoModel.from_config(config.encoder_config)
        self.sound_mm_projector = AudioFlamingo3MultiModalProjector(config)

        self.media_tokens = config.media_tokens
        self.padding_side = config.padding_side
        self.pad_token_id = config.pad_token_id
        self.model_max_length = config.model_max_length
        self.eos_token_id = config.eos_token_id
        self.bos_token_id = config.bos_token_id
        self.sound_token_id = config.sound_token_id
        self.end_newline_token_id = config.end_newline_token_id

    @property
    def end_emb_token(self) -> torch.Tensor:
        """
        Returns the (1, D) embedding row for the end-newline token.
        Uses the live embedding weight so dtype/device are always correct.
        """
        w = self.llm.model.embed_tokens.weight
        return w[self.end_newline_token_id : self.end_newline_token_id + 1]  # (1, D)

    def _sound_features(
        self,
        sounds: torch.Tensor,  # (N, M, T_mel)
        masks: Optional[torch.Tensor],  # (N, T_mel) or None
    ) -> torch.Tensor:
        """
        Runs the audio tower + projector and appends a newline embedding per window.

        Returns:
            Tensor of shape (N, S_feat + 1, D).
        """
        device = self.llm.device
        feats = self.get_audio_features(
            sounds.to(device),
            masks.to(device) if masks is not None else None,
        )  # (N, S_feat, D)

        # Append one end token per sample
        end = self.end_emb_token.to(feats.dtype).unsqueeze(0).expand(feats.size(0), 1, -1)  # (N, 1, D)
        return torch.cat([feats, end], dim=1)  # (N, S_feat + 1, D)

    def get_audio_features(self, sounds: torch.Tensor, masks: Optional[torch.Tensor] = None) -> torch.Tensor:
        device = self.llm.device
        proj_dtype = next(self.sound_mm_projector.parameters()).dtype
        sounds = sounds.to(device=device, dtype=proj_dtype)
        masks = masks.to(device) if masks is not None else None

        # Forward pass through encoder with mask up-conversion
        enc_out = self.sound_tower(
            input_features=sounds,
            attention_mask=masks if masks is not None else None,
        )
        feats = enc_out.last_hidden_state.to(dtype=proj_dtype)
        return self.sound_mm_projector(feats)

    def _compute_audio_embeds(
        self,
        input_features: torch.Tensor,  # (N, M, T_mel)
        feature_attention_mask: torch.Tensor | None = None,  # (N, T_mel) or None
    ) -> torch.Tensor:
        """
        Runs the audio tower + projector and appends one end token per window.
        Returns: (N, S_audio + 1, D)
        """
        device = self.llm.device
        proj_dtype = next(self.sound_mm_projector.parameters()).dtype

        # Encoder forward (accepts a 2D mel-frame mask; encoder will up-convert it internally)
        enc = self.sound_tower(
            input_features=input_features.to(device=device, dtype=proj_dtype),
            attention_mask=(feature_attention_mask.to(device) if feature_attention_mask is not None else None),
        )
        feats = enc.last_hidden_state.to(dtype=proj_dtype)  # (N, S_audio, H_enc)
        feats = self.sound_mm_projector(feats)  # (N, S_audio, D)

        # Append one end-newline embedding per audio window
        end = self.end_emb_token.to(feats.dtype).unsqueeze(0).expand(feats.size(0), 1, -1)  # (N, 1, D)
        return torch.cat([feats, end], dim=1)  # (N, S_audio+1, D)

    def _merge_input_ids_with_audio_features(
        self,
        input_ids: torch.Tensor,
        labels: Optional[torch.Tensor],
        attention_mask: Optional[torch.Tensor],
        *,
        audio_embeds: Optional[torch.Tensor] = None,  # (N, S_audio+1, D) precomputed
        audio_features_mask: Optional[torch.Tensor] = None,  # (N, S_audio+1) boolean; end slot should be False
    ) -> tuple[torch.Tensor, Optional[torch.Tensor], torch.Tensor]:
        # Set default values
        labels = labels if labels is not None else torch.full_like(input_ids, self.config.ignore_index)
        attention_mask = attention_mask if attention_mask is not None else torch.ones_like(input_ids, dtype=torch.bool)

        # text embeddings
        text_embeds = self.llm.model.embed_tokens(input_ids)  # (B, L, D)
        target_device = text_embeds.device
        batch_size = labels.shape[0]

        # Validate precomputed audio embeddings and mask
        if audio_embeds is None or audio_features_mask is None:
            raise ValueError(
                "`audio_embeds` and `audio_features_mask` must be provided to `_merge_input_ids_with_audio_features`."
            )

        audio_embeds = audio_embeds.to(target_device)
        audio_features_mask = audio_features_mask.to(target_device).to(torch.bool)  # (N, S_audio+1)

        num_audios, max_audio_tokens, embed_dim = audio_embeds.shape

        # Flatten variable-length audio to (sum_used, D) with end-token excluded by mask
        masked_audio_features = audio_embeds[audio_features_mask].view(-1, embed_dim)
        num_audio_tokens = audio_features_mask.sum(-1)  # (N,)

        batch_size, sequence_length = input_ids.shape
        _left_padding = torch.any(attention_mask[:, 0] == 0)
        _right_padding = torch.any(attention_mask[:, -1] == 0)

        left_padding = True
        if batch_size > 1:
            if _left_padding and not _right_padding:
                left_padding = True
            elif not _left_padding and _right_padding:
                left_padding = False
            elif not _left_padding and not _right_padding:
                left_padding = self.padding_side == "left"
            else:
                raise ValueError(f"both side of attention_mask has zero, invalid. {attention_mask}")

        # Locate <sound> placeholders
        special_audio_token_mask = input_ids == self.sound_token_id
        attention_mask = attention_mask.to(target_device)
        input_ids = input_ids.to(target_device)

        batch_indices, non_audio_indices = torch.where((input_ids != self.sound_token_id) & (attention_mask == 1))

        # Compute positions for text after expanding placeholders
        token_placeholder_num = torch.zeros_like(input_ids)
        token_placeholder_num[special_audio_token_mask] = num_audio_tokens.long() - 1
        token_placeholder_num = token_placeholder_num + 1
        new_token_positions = torch.cumsum(token_placeholder_num, -1) - 1
        max_token_num = token_placeholder_num.sum(-1).max()
        nb_audio_pad = max_token_num - 1 - new_token_positions[:, -1]
        if left_padding:
            new_token_positions += nb_audio_pad[:, None]
        text_to_overwrite = new_token_positions[batch_indices, non_audio_indices]
        batch_indices, non_audio_indices, text_to_overwrite = (
            batch_indices.to(target_device),
            non_audio_indices.to(target_device),
            text_to_overwrite.to(target_device),
        )

        # Allocate final tensors
        final_embedding = torch.zeros(
            batch_size, max_token_num, embed_dim, dtype=text_embeds.dtype, device=target_device
        )
        final_attention_mask = torch.zeros(batch_size, max_token_num, dtype=attention_mask.dtype, device=target_device)
        final_input_ids = torch.full(
            (batch_size, max_token_num), self.pad_token_id, dtype=input_ids.dtype, device=target_device
        )

        # Scatter text embeddings
        final_embedding[batch_indices, text_to_overwrite] = text_embeds[batch_indices, non_audio_indices]
        final_attention_mask[batch_indices, text_to_overwrite] = attention_mask[batch_indices, non_audio_indices]
        final_input_ids[batch_indices, text_to_overwrite] = input_ids[batch_indices, non_audio_indices]

        final_labels = None
        if labels is not None:
            labels = labels.to(target_device)
            final_labels = torch.full_like(final_attention_mask, self.config.ignore_index, dtype=torch.long)
            final_labels[batch_indices, text_to_overwrite] = labels[batch_indices, non_audio_indices]

        # Scatter audio embeddings
        audio_to_overwrite = torch.full((batch_size, max_token_num), True, dtype=torch.bool, device=target_device)
        audio_to_overwrite[batch_indices, text_to_overwrite] = False

        seq_indices = torch.arange(max_token_num, device=target_device).unsqueeze(0).expand(batch_size, max_token_num)
        if left_padding:
            val = (max_token_num - seq_indices) <= (
                token_placeholder_num.sum(-1) - (attention_mask == 0).long().sum(-1)
            )[:, None]
        else:
            val = seq_indices < (token_placeholder_num.sum(-1) - (attention_mask == 0).long().sum(-1))[:, None]
        audio_to_overwrite &= val

        if audio_to_overwrite.sum() != audio_features_mask.sum():
            raise ValueError("Mismatch between audio feature tokens and <sound> placeholders; indexing would break.")

        final_embedding[audio_to_overwrite] = (
            masked_audio_features.contiguous().reshape(-1, embed_dim).to(target_device)
        )
        final_attention_mask |= audio_to_overwrite

        # Batch padding functionality
        final_embedding = list(final_embedding)
        final_labels = list(final_labels) if final_labels is not None else None

        batch_size = len(final_embedding)
        device = final_embedding[0].device
        hidden_size = final_embedding[0].shape[1]
        max_length = max(final_embedding[k].shape[0] for k in range(batch_size))
        attention_mask = torch.ones((batch_size, max_length), dtype=torch.bool, device=device)

        inputs_batched, labels_batched = [], []
        for k in range(batch_size):
            size_pk = max_length - final_embedding[k].shape[0]
            inputs_pk = torch.zeros((size_pk, hidden_size), dtype=final_embedding[k].dtype, device=device)
            labels_pk = (
                torch.full((size_pk,), self.config.ignore_index, dtype=final_labels[k].dtype, device=device)
                if final_labels is not None
                else None
            )

            if self.padding_side == "right":
                attention_mask[k, final_embedding[k].shape[0] :] = False
                inputs_pk = torch.cat([final_embedding[k], inputs_pk], dim=0)
                if labels_pk is not None:
                    labels_pk = torch.cat([final_labels[k], labels_pk], dim=0)
            else:
                attention_mask[k, : -final_embedding[k].shape[0]] = False
                inputs_pk = torch.cat([inputs_pk, final_embedding[k]], dim=0)
                if labels_pk is not None:
                    labels_pk = torch.cat([labels_pk, final_labels[k]], dim=0)

            inputs_batched.append(inputs_pk)
            if labels_pk is not None:
                labels_batched.append(labels_pk)

        inputs_embeds = torch.stack(inputs_batched, dim=0)
        labels = torch.stack(labels_batched, dim=0) if labels_batched else None
        return inputs_embeds, labels, attention_mask

    @torch.inference_mode()
    def generate(
        self,
        input_ids: Optional[torch.FloatTensor] = None,
        audio_features: Optional[torch.Tensor] = None,  # (N, M, T_mel)
        attention_mask: Optional[torch.LongTensor] = None,
        audio_feature_masks: Optional[torch.Tensor] = None,  # (N, T_mel)
        audio_features_mask: Optional[torch.Tensor] = None,  # (N, S_audio+1)
        **generation_kwargs,
    ) -> torch.LongTensor:
        # Compute audio embeddings upfront
        audio_embeds = None
        if audio_features is not None:
            audio_embeds = self._compute_audio_embeds(audio_features, audio_feature_masks)  # (N, S_audio+1, D)

        # Merge text and audio embeddings
        inputs_embeds, _, attn = self._merge_input_ids_with_audio_features(
            input_ids=input_ids,
            labels=None,
            attention_mask=attention_mask,
            audio_embeds=audio_embeds,
            audio_features_mask=audio_features_mask,
        )
        return self.llm.generate(inputs_embeds=inputs_embeds, attention_mask=attn, **generation_kwargs)

    @property
    def default_generation_config(self) -> GenerationConfig:
        generation_config = copy.deepcopy(self.generation_config or GenerationConfig())
        if self.eos_token_id is None:
            raise ValueError("Tokenizer must have an EOS token")
        if generation_config.max_length == GenerationConfig().max_length:
            generation_config.max_length = self.model_max_length
        if generation_config.pad_token_id is None:
            generation_config.pad_token_id = self.pad_token_id or self.eos_token_id
        if generation_config.bos_token_id is None:
            generation_config.bos_token_id = self.bos_token_id or self.eos_token_id
        if generation_config.eos_token_id is None:
            generation_config.eos_token_id = [self.eos_token_id]
        generation_config.do_sample = False
        generation_config.max_new_tokens = 2048
        return generation_config


class AudioFlamingo3MultiModalProjector(nn.Module):
    """
    Audio adaptor (a small MLP) that projects AF-Whisper features to the LLM
    embedding space so they can replace `<sound>` tokens.

    Input:  (..., S, d_audio) â†’ Output: (..., S, d_text)
    """

    def __init__(self, config: AudioFlamingo3Config) -> None:
        super().__init__()
        self.layers = nn.ModuleList(
            [
                nn.Linear(config.sound_hidden_size, config.hidden_size),
                nn.GELU(),
                nn.Linear(config.hidden_size, config.hidden_size),
            ]
        )

    def forward(self, x: torch.Tensor, *args: Any, **kwargs: Any) -> torch.Tensor:
        for layer in self.layers:
            x = layer(x)
        return x


__all__ = ["AudioFlamingo3ForConditionalGeneration", "AudioFlamingo3PreTrainedModel", "AudioFlamingo3Encoder"]
