#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/deepseek_v32/modular_deepseek_v32.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_deepseek_v32.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# coding=utf-8
# Copyright 2025 DeepSeek AI and The HuggingFace Inc. team. All rights reserved.
#
# This code is based on the DeepSeek-V3.2-Exp implementation from DeepSeek AI.
# Reference: https://github.com/deepseek-ai/DeepSeek-V3.2-Exp
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import math
from collections.abc import Callable
from dataclasses import dataclass
from typing import Optional, Union

import torch
import torch.nn.functional as F
from torch import nn

from ... import initialization as init
from ...activations import ACT2FN
from ...cache_utils import Cache, DynamicCache
from ...generation import GenerationMixin
from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub
from ...masking_utils import create_causal_mask
from ...modeling_flash_attention_utils import FlashAttentionKwargs
from ...modeling_layers import (
    GenericForSequenceClassification,
    GenericForTokenClassification,
    GradientCheckpointingLayer,
)
from ...modeling_outputs import ModelOutput
from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update
from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
from ...processing_utils import Unpack
from ...utils import TransformersKwargs, auto_docstring
from ...utils.import_utils import is_hadamard_available
from .configuration_deepseek_v32 import DeepseekV32Config


# Import fast_hadamard_transform if available, otherwise use fallback
if is_hadamard_available():
    from fast_hadamard_transform import hadamard_transform
else:
    hadamard_transform = None


@dataclass
class CausalLMOutputWithIndexer(ModelOutput):
    """
    Causal language model outputs with indexer KL loss for DeepSeek V3.2.

    Args:
        loss (`torch.FloatTensor` of shape `(1,)`, *optional*):
            Combined loss (lm_loss + indexer_kl_coef * indexer_kl_loss when indexer_kl_coef > 0).
        lm_loss (`torch.FloatTensor` of shape `(1,)`, *optional*):
            Pure language modeling loss (for next-token prediction).
        indexer_kl_loss (`torch.FloatTensor` of shape `(1,)`, *optional*):
            KL divergence loss between indexer predictions and attention distribution.
            Used for training the Lightning Indexer in Stage 2.
        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):
            Prediction scores of the language modeling head.
        past_key_values (`Cache`, *optional*):
            Pre-computed key/value states for fast decoding.
        hidden_states (`tuple(torch.FloatTensor)`, *optional*):
            Hidden states at each layer output.
        attentions (`tuple(torch.FloatTensor)`, *optional*):
            Attention weights after softmax.
    """

    loss: Optional[torch.FloatTensor] = None
    lm_loss: Optional[torch.FloatTensor] = None
    indexer_kl_loss: Optional[torch.FloatTensor] = None
    logits: Optional[torch.FloatTensor] = None
    past_key_values: Optional[Cache] = None
    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None
    attentions: Optional[tuple[torch.FloatTensor, ...]] = None


@dataclass
class BaseModelOutputWithIndexer(ModelOutput):
    """
    Base model outputs with indexer scores for DeepSeek V3.2.

    Args:
        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):
            Hidden states at the output of the last layer.
        past_key_values (`Cache`, *optional*):
            Pre-computed key/value states for fast decoding.
        hidden_states (`tuple(torch.FloatTensor)`, *optional*):
            Hidden states at each layer output.
        attentions (`tuple(torch.FloatTensor)`, *optional*):
            Attention weights after softmax.
        indexer_scores (`tuple(torch.FloatTensor)`, *optional*):
            Raw indexer scores I_{t,s} from each layer.
        indexer_kl_targets (`tuple(torch.FloatTensor)`, *optional*):
            KL target distributions p_{t,:} from each layer.
    """

    last_hidden_state: Optional[torch.FloatTensor] = None
    past_key_values: Optional[Cache] = None
    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None
    attentions: Optional[tuple[torch.FloatTensor, ...]] = None
    indexer_scores: Optional[tuple[torch.FloatTensor, ...]] = None
    indexer_kl_targets: Optional[tuple[torch.FloatTensor, ...]] = None


@use_kernel_forward_from_hub("RMSNorm")
class DeepseekV32RMSNorm(nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        """
        DeepseekV32RMSNorm is equivalent to T5LayerNorm
        """
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states):
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states.to(input_dtype)

    def extra_repr(self):
        return f"{tuple(self.weight.shape)}, eps={self.variance_epsilon}"


class DeepseekV32RotaryEmbedding(nn.Module):
    inv_freq: torch.Tensor  # fix linting for `register_buffer`

    def __init__(self, config: DeepseekV32Config, device=None):
        super().__init__()
        self.max_seq_len_cached = config.max_position_embeddings
        self.original_max_seq_len = config.max_position_embeddings

        self.config = config

        self.rope_type = self.config.rope_parameters["rope_type"]
        rope_init_fn: Callable = self.compute_default_rope_parameters
        if self.rope_type != "default":
            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]
        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)

        self.register_buffer("inv_freq", inv_freq, persistent=False)
        self.original_inv_freq = inv_freq

    @staticmethod
    def compute_default_rope_parameters(
        config: Optional[DeepseekV32Config] = None,
        device: Optional["torch.device"] = None,
        seq_len: Optional[int] = None,
    ) -> tuple["torch.Tensor", float]:
        """
        Computes the inverse frequencies according to the original RoPE implementation
        Args:
            config ([`~transformers.PreTrainedConfig`]):
                The model configuration.
            device (`torch.device`):
                The device to use for initialization of the inverse frequencies.
            seq_len (`int`, *optional*):
                The current sequence length. Unused for this type of RoPE.
        Returns:
            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the
            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).
        """
        base = config.rope_parameters["rope_theta"]
        dim = getattr(config, "head_dim", None) or config.hidden_size // config.num_attention_heads

        attention_factor = 1.0  # Unused in this type of RoPE

        # Compute the inverse frequencies
        inv_freq = 1.0 / (
            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)
        )
        return inv_freq, attention_factor

    @torch.no_grad()
    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)
    def forward(self, x, position_ids):
        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        position_ids_expanded = position_ids[:, None, :].float()

        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != "mps" else "cpu"
        with torch.autocast(device_type=device_type, enabled=False):  # Force float32
            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
            emb = torch.cat((freqs, freqs), dim=-1)
            cos = emb.cos() * self.attention_scaling
            sin = emb.sin() * self.attention_scaling

        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)


class DeepseekV32MLP(nn.Module):
    def __init__(self, config, intermediate_size=None):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size if intermediate_size is None else intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, x):
        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        return down_proj


class DeepseekV32TopkRouter(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.n_routed_experts = config.n_routed_experts

        self.weight = nn.Parameter(torch.empty((self.n_routed_experts, config.hidden_size)))
        self.register_buffer("e_score_correction_bias", torch.zeros(self.n_routed_experts))

    def forward(self, hidden_states):
        hidden_states = hidden_states.view(-1, self.config.hidden_size)
        router_logits = F.linear(hidden_states.type(torch.float32), self.weight.type(torch.float32))
        return router_logits


class DeepseekV32NaiveMoe(nn.Module):
    """Collection of expert weights stored as 3D tensors."""

    def __init__(self, config):
        super().__init__()
        self.num_experts = config.num_local_experts
        self.hidden_dim = config.hidden_size
        self.intermediate_dim = config.intermediate_size
        self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, 2 * self.intermediate_dim, self.hidden_dim))
        self.down_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_dim, self.intermediate_dim))
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(
        self,
        hidden_states: torch.Tensor,
        top_k_index: torch.Tensor,
        top_k_weights: torch.Tensor,
    ) -> torch.Tensor:
        final_hidden_states = torch.zeros_like(hidden_states)
        with torch.no_grad():
            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts)
            expert_mask = expert_mask.permute(2, 1, 0)
            expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()

        for expert_idx in expert_hit:
            expert_idx = expert_idx[0]
            if expert_idx == self.num_experts:
                continue
            top_k_pos, token_idx = torch.where(expert_mask[expert_idx])
            current_state = hidden_states[token_idx]
            # Ensure weights are on the same device as hidden_states (for multi-GPU dispatch)
            gate_up_weight = self.gate_up_proj[expert_idx].to(current_state.device)
            down_weight = self.down_proj[expert_idx].to(current_state.device)
            gate, up = nn.functional.linear(current_state, gate_up_weight).chunk(2, dim=-1)
            current_hidden_states = self.act_fn(gate) * up
            current_hidden_states = nn.functional.linear(current_hidden_states, down_weight)
            current_hidden_states = current_hidden_states * top_k_weights[token_idx, top_k_pos, None]
            final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))

        return final_hidden_states


class DeepseekV32MoE(nn.Module):
    """
    A mixed expert module containing shared experts.
    """

    def __init__(self, config):
        super().__init__()
        self.config = config
        self.experts = DeepseekV32NaiveMoe(config)
        self.gate = DeepseekV32TopkRouter(config)
        self.shared_experts = DeepseekV32MLP(
            config=config, intermediate_size=config.moe_intermediate_size * config.n_shared_experts
        )
        self.n_routed_experts = config.n_routed_experts
        self.n_group = config.n_group
        self.topk_group = config.topk_group
        self.norm_topk_prob = config.norm_topk_prob
        self.routed_scaling_factor = config.routed_scaling_factor
        self.top_k = config.num_experts_per_tok

    def route_tokens_to_experts(self, router_logits):
        router_logits = router_logits.sigmoid()
        router_logits_for_choice = router_logits + self.gate.e_score_correction_bias
        group_scores = (
            router_logits_for_choice.view(-1, self.n_group, self.n_routed_experts // self.n_group)
            .topk(2, dim=-1)[0]
            .sum(dim=-1)
        )
        group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]
        group_mask = torch.zeros_like(group_scores)
        group_mask.scatter_(1, group_idx, 1)
        score_mask = (
            group_mask.unsqueeze(-1)
            .expand(-1, self.n_group, self.n_routed_experts // self.n_group)
            .reshape(-1, self.n_routed_experts)
        )
        scores_for_choice = router_logits_for_choice.masked_fill(~score_mask.bool(), 0.0)
        topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]
        topk_weights = router_logits.gather(1, topk_indices)
        if self.norm_topk_prob:
            denominator = topk_weights.sum(dim=-1, keepdim=True) + 1e-20
            topk_weights /= denominator
        topk_weights = topk_weights * self.routed_scaling_factor
        return topk_indices, topk_weights

    def forward(self, hidden_states):
        residuals = hidden_states
        orig_shape = hidden_states.shape
        router_logits = self.gate(hidden_states)
        topk_indices, topk_weights = self.route_tokens_to_experts(router_logits)
        hidden_states = hidden_states.view(-1, hidden_states.shape[-1])
        hidden_states = self.experts(hidden_states, topk_indices, topk_weights).view(*orig_shape)
        hidden_states = hidden_states + self.shared_experts(residuals)
        return hidden_states


def hadamard_transform_fallback(x: torch.Tensor, scale: float = 1.0) -> torch.Tensor:
    """
    Pure PyTorch Fast Walsh-Hadamard Transform fallback.

    This is significantly slower than the CUDA version but works on CPU and
    doesn't require the fast-hadamard-transform package.

    Args:
        x: Input tensor with shape (..., dim) where dim should be a power of 2
        scale: Multiplier for the output

    Returns:
        Transformed tensor with same shape as input
    """
    orig_dtype = x.dtype
    x = x.float()
    dim = x.shape[-1]

    # Pad to power of 2 if needed
    if dim & (dim - 1) != 0:
        next_pow2 = 1 << (dim - 1).bit_length()
        x = F.pad(x, (0, next_pow2 - dim))
        dim = next_pow2

    # Fast Walsh-Hadamard Transform using butterfly operations
    h = 1
    while h < dim:
        # Reshape for butterfly operation
        x = x.view(*x.shape[:-1], dim // (2 * h), 2, h)
        # Butterfly: [a, b] -> [a + b, a - b]
        a = x[..., 0, :]
        b = x[..., 1, :]
        x = torch.stack([a + b, a - b], dim=-2)
        x = x.view(*x.shape[:-3], dim)
        h *= 2

    return (x * scale).to(orig_dtype)


def rotate_activation(x: torch.Tensor) -> torch.Tensor:
    """
    Apply Hadamard transform for activation rotation in the indexer.

    This is used in the Lightning Indexer to rotate Q and K activations
    before computing index scores.

    Args:
        x: Input tensor with shape (..., hidden_size)

    Returns:
        Rotated tensor with same shape
    """
    hidden_size = x.size(-1)
    scale = hidden_size**-0.5

    if is_hadamard_available():
        # fast-hadamard-transform requires contiguous bfloat16 input
        return hadamard_transform(x.contiguous(), scale=scale)
    else:
        return hadamard_transform_fallback(x, scale=scale)


def apply_rotary_pos_emb_non_interleave(
    q: torch.Tensor,
    k: torch.Tensor,
    cos: torch.Tensor,
    sin: torch.Tensor,
    unsqueeze_dim: int = 2,
) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Applies Rotary Position Embedding with NON-INTERLEAVED layout.

    This is specifically for the Indexer, which requires non-interleaved RoPE
    (different from the MLA which uses interleaved RoPE).

    The difference is in how dimensions are paired:
    - Interleaved: pairs (0,1), (2,3), (4,5), ...
    - Non-interleaved: pairs (0, dim/2), (1, dim/2+1), ...

    For non-interleaved RoPE, the rotation is applied to pairs of elements at positions
    (i, i+dim/2) rather than adjacent pairs. This means cos/sin should have dimension
    dim/2 to match the split halves of q and k.

    Args:
        q: Query tensor of shape (batch, seq_len, heads, head_dim)
        k: Key tensor of shape (batch, seq_len, heads, head_dim) or (batch, seq_len, 1, head_dim)
        cos: Cosine of rotary angles, shape (batch, seq_len, rope_dim)
        sin: Sine of rotary angles, shape (batch, seq_len, rope_dim)
        unsqueeze_dim: Dimension to unsqueeze cos/sin for broadcasting (default 2 for heads dim)

    Returns:
        Tuple of rotated (query, key) tensors
    """
    # Non-interleaved: split in half and rotate
    # q = [q1, q2] where q1 is first half, q2 is second half
    # rotated = [q1 * cos - q2 * sin, q1 * sin + q2 * cos]
    q1, q2 = q.chunk(2, dim=-1)
    k1, k2 = k.chunk(2, dim=-1)

    # For non-interleaved RoPE, cos/sin should match the dimension of q1/q2
    # If cos/sin have full dimension (from standard RoPE), slice to half
    half_dim = q1.shape[-1]
    if cos.shape[-1] != half_dim:
        cos = cos[..., :half_dim]
        sin = sin[..., :half_dim]

    # cos/sin shape: (batch, seq, half_dim)
    # For q/k with shape (batch, seq, heads, half_dim), unsqueeze at dim 2
    # Result: (batch, seq, 1, half_dim) which broadcasts with (batch, seq, heads, half_dim)
    cos = cos.unsqueeze(unsqueeze_dim)
    sin = sin.unsqueeze(unsqueeze_dim)

    q_embed = torch.cat([q1 * cos - q2 * sin, q1 * sin + q2 * cos], dim=-1)
    k_embed = torch.cat([k1 * cos - k2 * sin, k1 * sin + k2 * cos], dim=-1)

    return q_embed, k_embed


class DeepseekV32Indexer(nn.Module):
    """
    Lightning Indexer for DeepSeek Sparse Attention (DSA).

    The indexer computes index scores to select which tokens each query should
    attend to, reducing attention complexity from O(L^2) to O(L*k).

    The index score formula is:
        I_{t,s} = sum_j w^I_{t,j} * ReLU(q^I_{t,j} * k^I_s)

    Key implementation details:
    1. Uses Hadamard transform on Q and K before scoring
    2. Uses NON-INTERLEAVED RoPE (different from MLA which uses interleaved)
    3. Uses LayerNorm on K (not RMSNorm)

    Args:
        config: DeepseekV32Config
        layer_idx: Index of the layer this indexer belongs to
    """

    def __init__(self, config: DeepseekV32Config, layer_idx: int):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx

        self.hidden_size = config.hidden_size
        self.num_heads = config.index_n_heads
        self.head_dim = config.index_head_dim
        self.qk_rope_head_dim = config.qk_rope_head_dim
        self.index_topk = config.index_topk
        self.q_lora_rank = config.q_lora_rank

        # Query projection from compressed representation
        # Named to match official weights: wq_b
        self.wq_b = nn.Linear(self.q_lora_rank, self.num_heads * self.head_dim, bias=False)

        # Key projection (single head, broadcast to all heads)
        # Named to match official weights: wk
        self.wk = nn.Linear(self.hidden_size, self.head_dim, bias=False)

        # LayerNorm for keys (not RMSNorm, following reference)
        # Named to match official weights: k_norm
        self.k_norm = nn.LayerNorm(self.head_dim)

        # Per-head weight projection
        # Named to match official weights: weights_proj
        self.weights_proj = nn.Linear(self.hidden_size, self.num_heads, bias=False)

        self.softmax_scale = self.head_dim**-0.5

    def forward(
        self,
        hidden_states: torch.Tensor,
        q_compressed: torch.Tensor,
        position_embeddings: tuple[torch.Tensor, torch.Tensor],
        attention_mask: Optional[torch.Tensor] = None,
        output_scores: bool = False,
    ) -> Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]]:
        """
        Compute top-k token indices for sparse attention.

        Args:
            hidden_states: Input hidden states [batch, seq_len, hidden_size]
            q_compressed: Compressed query representation [batch, seq_len, q_lora_rank]
            position_embeddings: Tuple of (cos, sin) for RoPE
            attention_mask: Optional attention mask
            output_scores: If True, also return the raw index scores

        Returns:
            topk_indices: Indices of selected tokens [batch, seq_len, topk]
            index_scores: (optional) Raw index scores [batch, seq_len, seq_len] if output_scores=True
        """
        batch_size, seq_len, _ = hidden_states.shape
        cos, sin = position_embeddings

        # Query path
        q = self.wq_b(q_compressed)  # [B, S, num_heads * head_dim]
        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim)

        # Split into RoPE and non-RoPE parts
        q_rope, q_nope = torch.split(q, [self.qk_rope_head_dim, self.head_dim - self.qk_rope_head_dim], dim=-1)

        # Key path
        k = self.wk(hidden_states)  # [B, S, head_dim]
        k = self.k_norm(k)

        k_rope, k_nope = torch.split(k, [self.qk_rope_head_dim, self.head_dim - self.qk_rope_head_dim], dim=-1)

        # Apply NON-INTERLEAVED RoPE (critical difference from MLA!)
        k_rope = k_rope.unsqueeze(2)  # [B, S, 1, rope_dim]
        q_rope, k_rope = apply_rotary_pos_emb_non_interleave(q_rope, k_rope, cos, sin)
        k_rope = k_rope.squeeze(2)  # [B, S, rope_dim]

        # Concatenate back
        q = torch.cat([q_rope, q_nope], dim=-1)  # [B, S, H, D]
        k = torch.cat([k_rope, k_nope], dim=-1)  # [B, S, D]

        # Apply Hadamard transform for activation rotation
        q = rotate_activation(q)
        k = rotate_activation(k)

        # Compute index scores: I_{t,s} = sum_j w_{t,j} * ReLU(q_{t,j} * k_s)
        # q: [B, S, H, D], k: [B, S, D]
        # First compute q * k for all pairs: [B, S_q, H, S_k]
        q = q.transpose(1, 2)  # [B, H, S_q, D]
        k = k.unsqueeze(1)  # [B, 1, S_k, D]

        # Compute attention-like scores
        scores = torch.matmul(q, k.transpose(-1, -2))  # [B, H, S_q, S_k]

        # Apply ReLU
        scores = F.relu(scores)

        # Get per-head weights
        weights = self.weights_proj(hidden_states)  # [B, S, H]
        weights = weights * (self.num_heads**-0.5) * self.softmax_scale
        weights = weights.transpose(1, 2).unsqueeze(-1)  # [B, H, S, 1]

        # Weighted sum over heads: [B, S_q, S_k]
        index_scores = (scores * weights).sum(dim=1)  # [B, S_q, S_k]

        # Apply attention mask if provided
        if attention_mask is not None:
            # attention_mask is typically [B, 1, S_q, S_k] or [B, S_q, S_k]
            if attention_mask.dim() == 4:
                attention_mask = attention_mask.squeeze(1)
            index_scores = index_scores + attention_mask

        # Select top-k tokens
        k_select = min(self.index_topk, seq_len)
        topk_indices = index_scores.topk(k_select, dim=-1).indices  # [B, S, topk]

        if output_scores:
            return topk_indices, index_scores
        return topk_indices


def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)


@use_kernel_func_from_hub("rotary_pos_emb")
def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    """Applies Rotary Position Embedding to the query and key tensors.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`, *optional*):
            Deprecated and unused.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """
    cos = cos.unsqueeze(unsqueeze_dim)
    sin = sin.unsqueeze(unsqueeze_dim)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)


def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    scaling: float,
    dropout: float = 0.0,
    **kwargs: Unpack[TransformersKwargs],
):
    key_states = repeat_kv(key, module.num_key_value_groups)
    value_states = repeat_kv(value, module.num_key_value_groups)

    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling
    if attention_mask is not None:
        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
        attn_weights = attn_weights + causal_mask

    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
    attn_output = torch.matmul(attn_weights, value_states)
    attn_output = attn_output.transpose(1, 2).contiguous()

    return attn_output, attn_weights


def apply_rotary_pos_emb_interleave(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    r"""
    TODO let's just use the original freqcis computation to not have the view
    transpose + reshape! This is not optimized!
    Applies Rotary Position Embedding to the query and key tensors.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`):
            The position indices of the tokens corresponding to the query and key tensors. For example, this can be
            used to pass offsetted position ids when working with a KV-cache.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """
    cos = cos.unsqueeze(unsqueeze_dim)
    sin = sin.unsqueeze(unsqueeze_dim)

    b, h, s, d = q.shape
    q = q.view(b, h, s, d // 2, 2).transpose(4, 3).reshape(b, h, s, d)

    b, h, s, d = k.shape
    k = k.view(b, h, s, d // 2, 2).transpose(4, 3).reshape(b, h, s, d)

    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


def yarn_get_mscale(scale=1, mscale=1):
    if scale <= 1:
        return 1.0
    return 0.1 * mscale * math.log(scale) + 1.0


class DeepseekV32Attention(nn.Module):
    """
    DeepSeek V3.2 Attention with Lightning Indexer for sparse attention.

    Extends DeepseekV3Attention by adding the Lightning Indexer which selects
    top-k tokens for each query position, enabling sparse attention.

    Key differences from V3:
    - Adds Lightning Indexer for token selection
    - Supports sparse attention mask during prefill
    - Falls back to dense attention (V3 behavior) when use_sparse_attention=False
      or during decode (seq_len=1)

    Note: Sparse attention uses eager computation (matching official DeepSeek code).
    When sparse attention is disabled, flash attention and other backends are supported.
    """

    def __init__(self, config: DeepseekV32Config, layer_idx: int):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads
        self.attention_dropout = config.attention_dropout
        self.num_heads = config.num_attention_heads

        self.q_lora_rank = config.q_lora_rank
        self.qk_rope_head_dim = config.qk_rope_head_dim
        self.kv_lora_rank = config.kv_lora_rank
        self.v_head_dim = config.v_head_dim
        self.qk_nope_head_dim = config.qk_nope_head_dim
        self.qk_head_dim = config.qk_head_dim

        self.is_causal = True
        if self.q_lora_rank is None:
            self.q_proj = nn.Linear(config.hidden_size, self.num_heads * self.qk_head_dim, bias=False)
        else:
            self.q_a_proj = nn.Linear(config.hidden_size, config.q_lora_rank, bias=config.attention_bias)
            self.q_a_layernorm = DeepseekV32RMSNorm(config.q_lora_rank)
            self.q_b_proj = nn.Linear(config.q_lora_rank, self.num_heads * self.qk_head_dim, bias=False)

        self.kv_a_proj_with_mqa = nn.Linear(
            config.hidden_size,
            self.kv_lora_rank + self.qk_rope_head_dim,
            bias=config.attention_bias,
        )
        self.kv_a_layernorm = DeepseekV32RMSNorm(self.kv_lora_rank)
        self.kv_b_proj = nn.Linear(
            self.kv_lora_rank,
            self.num_heads * (self.qk_nope_head_dim + self.v_head_dim),
            bias=False,
        )

        self.o_proj = nn.Linear(
            self.num_heads * self.v_head_dim,
            config.hidden_size,
            bias=config.attention_bias,
        )

        self.scaling = self.qk_head_dim ** (-0.5)
        if self.config.rope_parameters.get("rope_type", "default") != "default":
            mscale_all_dim = self.config.rope_parameters.get("mscale_all_dim", 0)
            scaling_factor = self.config.rope_parameters["factor"]
            if mscale_all_dim:
                mscale = yarn_get_mscale(scaling_factor, mscale_all_dim)
                self.scaling = self.scaling * mscale * mscale
        # Add the Lightning Indexer (only new component vs V3)
        self.indexer = DeepseekV32Indexer(config, layer_idx)

    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: tuple[torch.Tensor, torch.Tensor],
        attention_mask: Optional[torch.Tensor],
        past_key_values: Optional[Cache] = None,
        cache_position: Optional[torch.LongTensor] = None,
        output_indexer_scores: bool = False,
        output_indexer_kl_target: bool = False,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:
        """
        Forward pass with sparse attention via Lightning Indexer.

        The sparse attention is only applied during prefill (seq_len > 1) when
        use_sparse_attention=True. During decode or when sparse attention is
        disabled, this behaves identically to DeepseekV3Attention.

        Args:
            hidden_states: Input tensor [batch, seq_len, hidden_size]
            position_embeddings: Tuple of (cos, sin) for RoPE
            attention_mask: Optional causal/padding mask
            past_key_values: Optional KV cache
            cache_position: Optional cache position indices
            output_indexer_scores: If True, return raw indexer scores I_{t,s}
            output_indexer_kl_target: If True, return KL target distribution p_{t,:}

        Returns:
            attn_output: Attention output [batch, seq_len, hidden_size]
            attn_weights: Attention weights (optional)
            indexer_scores: Raw indexer scores [batch, seq, seq] if output_indexer_scores=True
            indexer_kl_target: KL target distribution [batch, seq, seq] if output_indexer_kl_target=True
        """
        batch_size, seq_length = hidden_states.shape[:-1]
        indexer_scores = None
        indexer_kl_target = None

        # Determine if we should use sparse attention
        # Sparse attention only applies during prefill, not decode
        use_sparse = (
            self.config.use_sparse_attention
            and self.q_lora_rank is not None  # Need compressed queries for indexer
            and seq_length > 1  # Only for prefill, not decode
        )

        # If not using sparse attention, use dense attention (V3 path)
        # This handles decode (seq_len=1) and when use_sparse_attention=False
        if not use_sparse:
            # Dense attention path - duplicated from DeepseekV3Attention.forward()
            # to avoid super() call issues with modular converter
            query_shape_dense = (batch_size, seq_length, -1, self.qk_head_dim)
            key_shape_dense = (batch_size, seq_length, -1, self.qk_nope_head_dim + self.v_head_dim)

            if self.q_lora_rank is None:
                q_states_dense = self.q_proj(hidden_states)
            else:
                q_states_dense = self.q_b_proj(self.q_a_layernorm(self.q_a_proj(hidden_states)))
            q_states_dense = q_states_dense.view(query_shape_dense).transpose(1, 2)
            q_pass_dense, q_rot_dense = torch.split(
                q_states_dense, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1
            )

            compressed_kv_dense = self.kv_a_proj_with_mqa(hidden_states)
            k_pass_dense, k_rot_dense = torch.split(
                compressed_kv_dense, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1
            )

            k_pass_dense = self.kv_b_proj(self.kv_a_layernorm(k_pass_dense)).view(key_shape_dense).transpose(1, 2)
            k_pass_dense, value_states_dense = torch.split(
                k_pass_dense, [self.qk_nope_head_dim, self.v_head_dim], dim=-1
            )

            k_rot_dense = k_rot_dense.view(batch_size, 1, seq_length, self.qk_rope_head_dim)

            cos, sin = position_embeddings
            if self.config.rope_interleave:
                q_rot_dense, k_rot_dense = apply_rotary_pos_emb_interleave(q_rot_dense, k_rot_dense, cos, sin)
            else:
                from ..llama.modeling_llama import apply_rotary_pos_emb

                q_rot_dense, k_rot_dense = apply_rotary_pos_emb(q_rot_dense, k_rot_dense, cos, sin)
            k_rot_dense = k_rot_dense.expand(*k_pass_dense.shape[:-1], -1)

            query_states_dense = torch.cat((q_pass_dense, q_rot_dense), dim=-1)
            key_states_dense = torch.cat((k_pass_dense, k_rot_dense), dim=-1)

            if past_key_values is not None:
                cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}
                key_states_dense, value_states_dense = past_key_values.update(
                    key_states_dense, value_states_dense, self.layer_idx, cache_kwargs
                )

            if self.config._attn_implementation == "flash_attention_2" and self.qk_head_dim != self.v_head_dim:
                value_states_dense = F.pad(value_states_dense, [0, self.qk_head_dim - self.v_head_dim])

            attention_interface: Callable = eager_attention_forward
            if self.config._attn_implementation != "eager":
                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

            attn_output_dense, attn_weights_dense = attention_interface(
                self,
                query_states_dense,
                key_states_dense,
                value_states_dense,
                attention_mask,
                dropout=0.0 if not self.training else self.attention_dropout,
                scaling=self.scaling,
                **kwargs,
            )

            if self.config._attn_implementation == "flash_attention_2" and self.qk_head_dim != self.v_head_dim:
                attn_output_dense = attn_output_dense[:, :, :, : self.v_head_dim]

            attn_output_dense = attn_output_dense.reshape(batch_size, seq_length, -1).contiguous()
            attn_output_dense = self.o_proj(attn_output_dense)

            # Return with None for indexer outputs
            return attn_output_dense, attn_weights_dense, None, None

        # Sparse attention path (eager computation, matching official DeepSeek code)
        query_shape = (batch_size, seq_length, -1, self.qk_head_dim)
        key_shape = (batch_size, seq_length, -1, self.qk_nope_head_dim + self.v_head_dim)

        # Query path with LoRA compression
        q_compressed = self.q_a_layernorm(self.q_a_proj(hidden_states))
        q_states = self.q_b_proj(q_compressed)

        # Optionally detach for separate indexer optimization (Stage 2 training)
        if self.config.detach_indexer_input:
            q_compressed_for_indexer = q_compressed.detach()
        else:
            q_compressed_for_indexer = q_compressed

        q_states = q_states.view(query_shape).transpose(1, 2)
        q_pass, q_rot = torch.split(q_states, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)

        # KV path with compression
        compressed_kv = self.kv_a_proj_with_mqa(hidden_states)
        k_pass, k_rot = torch.split(compressed_kv, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)

        k_pass = self.kv_b_proj(self.kv_a_layernorm(k_pass)).view(key_shape).transpose(1, 2)
        k_pass, value_states = torch.split(k_pass, [self.qk_nope_head_dim, self.v_head_dim], dim=-1)

        k_rot = k_rot.view(batch_size, 1, seq_length, self.qk_rope_head_dim)

        # Apply RoPE (INTERLEAVED for MLA)
        cos, sin = position_embeddings
        if self.config.rope_interleave:
            q_rot, k_rot = apply_rotary_pos_emb_interleave(q_rot, k_rot, cos, sin)
        else:
            from ..llama.modeling_llama import apply_rotary_pos_emb

            q_rot, k_rot = apply_rotary_pos_emb(q_rot, k_rot, cos, sin)

        k_rot = k_rot.expand(*k_pass.shape[:-1], -1)

        query_states = torch.cat((q_pass, q_rot), dim=-1)
        key_states = torch.cat((k_pass, k_rot), dim=-1)

        # Update cache if provided
        if past_key_values is not None:
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}
            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)

        # Get top-k indices from the indexer (optionally with scores)
        need_scores = output_indexer_scores or output_indexer_kl_target
        indexer_result = self.indexer(
            hidden_states,
            q_compressed_for_indexer,
            position_embeddings,
            attention_mask,
            output_scores=need_scores,
        )

        if need_scores:
            topk_indices, indexer_scores = indexer_result
        else:
            topk_indices = indexer_result

        # Create sparse attention mask (matching official DeepSeek implementation)
        # topk_indices: [B, S, topk] -> sparse_mask: [B, S, kv_seq_len]
        kv_seq_len = key_states.shape[2]
        sparse_mask = torch.full(
            (batch_size, seq_length, kv_seq_len),
            float("-inf"),
            device=query_states.device,
            dtype=query_states.dtype,
        )
        # Scatter 0s at the selected positions
        sparse_mask.scatter_(-1, topk_indices, 0.0)

        # Combine with causal mask if provided
        if attention_mask is not None:
            if attention_mask.dim() == 4:
                # [B, 1, S, S] -> [B, S, S]
                attention_mask = attention_mask.squeeze(1)
            sparse_mask = sparse_mask + attention_mask

        # Expand for heads: [B, H, S, S]
        sparse_mask = sparse_mask.unsqueeze(1).expand(-1, self.num_heads, -1, -1)

        # Eager attention computation (matching official DeepSeek code - no flash attention)
        attn_weights = torch.matmul(query_states, key_states.transpose(-1, -2)) * self.scaling
        attn_weights = attn_weights + sparse_mask
        attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)

        # Compute KL target if requested (before dropout!)
        # Per tech report: "sum across all attention heads, then L1-normalize"
        if output_indexer_kl_target:
            # attn_weights: [B, H, S_q, S_k] - post-softmax attention
            # Sum across heads: [B, S_q, S_k]
            attn_sum = attn_weights.sum(dim=1)
            # L1 normalize along key dimension to get target distribution p_{t,:}
            indexer_kl_target = attn_sum / (attn_sum.sum(dim=-1, keepdim=True) + 1e-10)
            # Detach - target should not receive gradients
            indexer_kl_target = indexer_kl_target.detach()

        attn_weights = F.dropout(attn_weights, p=self.attention_dropout, training=self.training)
        attn_output = torch.matmul(attn_weights, value_states)

        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.reshape(batch_size, seq_length, -1).contiguous()
        attn_output = self.o_proj(attn_output)

        return attn_output, attn_weights, indexer_scores, indexer_kl_target


class DeepseekV32DecoderLayer(GradientCheckpointingLayer):
    """DeepSeek V3.2 decoder layer with sparse attention."""

    def __init__(self, config: DeepseekV32Config, layer_idx: int):
        # Call grandparent init to avoid V3 attention
        super().__init__()
        self.hidden_size = config.hidden_size

        # Use V3.2 attention with indexer
        self.self_attn = DeepseekV32Attention(config=config, layer_idx=layer_idx)

        # MLP: dense for first k layers, MoE for rest
        if layer_idx >= config.first_k_dense_replace:
            self.mlp = DeepseekV32MoE(config)
        else:
            self.mlp = DeepseekV32MLP(config)

        self.input_layernorm = DeepseekV32RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = DeepseekV32RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        use_cache: Optional[bool] = False,
        cache_position: Optional[torch.LongTensor] = None,
        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,
        output_indexer_scores: bool = False,
        output_indexer_kl_target: bool = False,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:
        """
        Forward pass for the decoder layer.

        Returns:
            Tuple of (hidden_states, attn_weights, indexer_scores, indexer_kl_target)
            - hidden_states: Output hidden states
            - attn_weights: Attention weights (optional, for output_attentions)
            - indexer_scores: Raw indexer scores I_{t,s} (optional, for KL loss)
            - indexer_kl_target: KL target distribution p_{t,:} (optional, for KL loss)
        """
        residual = hidden_states
        hidden_states = self.input_layernorm(hidden_states)

        # Self Attention - V3.2 attention returns 4 values
        hidden_states, attn_weights, indexer_scores, indexer_kl_target = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            use_cache=use_cache,
            cache_position=cache_position,
            position_embeddings=position_embeddings,
            output_indexer_scores=output_indexer_scores,
            output_indexer_kl_target=output_indexer_kl_target,
            **kwargs,
        )
        # Ensure residual is on the same device as hidden_states (for multi-GPU)
        hidden_states = residual.to(hidden_states.device) + hidden_states

        # Fully Connected
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        # Ensure residual is on the same device as hidden_states (for multi-GPU MoE sharding)
        hidden_states = residual.to(hidden_states.device) + hidden_states

        return hidden_states, attn_weights, indexer_scores, indexer_kl_target


@auto_docstring
class DeepseekV32PreTrainedModel(PreTrainedModel):
    """Base class for DeepSeek V3.2 models."""

    config: DeepseekV32Config
    base_model_prefix = "model"
    supports_gradient_checkpointing = True
    _no_split_modules = ["DeepseekV32DecoderLayer"]
    _skip_keys_device_placement = ["past_key_values"]
    _supports_flash_attn = True
    _supports_sdpa = True
    _supports_flex_attn = True
    _can_compile_fullgraph = False
    _supports_attention_backend = True
    _can_record_outputs = {
        "hidden_states": DeepseekV32DecoderLayer,
        "attentions": DeepseekV32Attention,
    }

    config_class = DeepseekV32Config

    @torch.no_grad()
    def _init_weights(self, module):
        super()._init_weights(module)
        if isinstance(module, DeepseekV32TopkRouter):
            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)
        elif isinstance(module, DeepseekV32NaiveMoe):
            init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)
            init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)


# Custom argument documentation for the indexer-specific parameters
DEEPSEEK_V32_INDEXER_ARGS = r"""
        output_indexer_scores (`bool`, *optional*):
            Whether to return raw indexer scores I_{t,s} from each layer. These are used
            for computing the KL divergence loss during indexer training. Auto-enabled
            when `config.indexer_kl_coef > 0`.
        output_indexer_kl_target (`bool`, *optional*):
            Whether to return KL target distributions p_{t,:} from each layer. These are
            the L1-normalized attention distributions used as targets for KL loss.
            Auto-enabled when `config.indexer_kl_coef > 0`.
"""


@auto_docstring
class DeepseekV32Model(DeepseekV32PreTrainedModel):
    """DeepSeek V3.2 Model with sparse attention."""

    _keys_to_ignore_on_load_unexpected = [r"model\.layers\.61.*"]

    config_class = DeepseekV32Config

    def __init__(self, config: DeepseekV32Config):
        super().__init__(config)
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
        self.layers = nn.ModuleList(
            [DeepseekV32DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
        )
        self.norm = DeepseekV32RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.rotary_emb = DeepseekV32RotaryEmbedding(config=config)

        self.gradient_checkpointing = False

        # Initialize weights and apply final processing
        self.post_init()

    @auto_docstring(custom_args=DEEPSEEK_V32_INDEXER_ARGS)
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        cache_position: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        output_indexer_scores: Optional[bool] = None,
        output_indexer_kl_target: Optional[bool] = None,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> BaseModelOutputWithIndexer:
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )

        # Auto-enable indexer outputs if KL loss is configured
        if output_indexer_scores is None:
            output_indexer_scores = self.config.indexer_kl_coef > 0
        if output_indexer_kl_target is None:
            output_indexer_kl_target = self.config.indexer_kl_coef > 0

        if (input_ids is None) ^ (inputs_embeds is not None):
            raise ValueError("You must specify exactly one of input_ids or inputs_embeds")

        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)

        if use_cache and past_key_values is None:
            past_key_values = DynamicCache(config=self.config)

        if cache_position is None:
            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
            cache_position = torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device) + past_seen_tokens

        if position_ids is None:
            position_ids = cache_position.unsqueeze(0)

        causal_mask = create_causal_mask(
            config=self.config,
            input_embeds=inputs_embeds,
            attention_mask=attention_mask,
            cache_position=cache_position,
            past_key_values=past_key_values,
            position_ids=position_ids,
        )

        hidden_states = inputs_embeds
        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)

        # Accumulate outputs
        all_hidden_states = () if output_hidden_states else None
        all_attentions = () if output_attentions else None
        all_indexer_scores = () if output_indexer_scores else None
        all_indexer_kl_targets = () if output_indexer_kl_target else None

        for decoder_layer in self.layers[: self.config.num_hidden_layers]:
            if output_hidden_states:
                all_hidden_states = all_hidden_states + (hidden_states,)

            # V3.2 decoder layer returns 4 values
            hidden_states, attn_weights, indexer_scores, indexer_kl_target = decoder_layer(
                hidden_states,
                attention_mask=causal_mask,
                position_embeddings=position_embeddings,
                position_ids=position_ids,
                past_key_values=past_key_values,
                use_cache=use_cache,
                cache_position=cache_position,
                output_indexer_scores=output_indexer_scores,
                output_indexer_kl_target=output_indexer_kl_target,
                **kwargs,
            )

            if output_attentions and attn_weights is not None:
                all_attentions = all_attentions + (attn_weights,)

            if output_indexer_scores and indexer_scores is not None:
                all_indexer_scores = all_indexer_scores + (indexer_scores,)

            if output_indexer_kl_target and indexer_kl_target is not None:
                all_indexer_kl_targets = all_indexer_kl_targets + (indexer_kl_target,)

        hidden_states = self.norm(hidden_states)

        # Add last hidden state
        if output_hidden_states:
            all_hidden_states = all_hidden_states + (hidden_states,)

        return BaseModelOutputWithIndexer(
            last_hidden_state=hidden_states,
            past_key_values=past_key_values,
            hidden_states=all_hidden_states,
            attentions=all_attentions,
            indexer_scores=all_indexer_scores,
            indexer_kl_targets=all_indexer_kl_targets,
        )


def compute_indexer_kl_loss(
    indexer_scores: tuple[torch.Tensor, ...],
    indexer_kl_targets: tuple[torch.Tensor, ...],
) -> torch.Tensor:
    """
    Compute KL-divergence loss between indexer predictions and attention distribution.

    This implements Equation 3 from the DeepSeek V3.2 technical report:
        L_I = sum_t D_KL(p_{t,:} || Softmax(I_{t,:}))

    where:
    - I_{t,s} = raw indexer output scores (from output_indexer_scores=True)
    - p_{t,:} = target distribution (from output_indexer_kl_target=True)

    Args:
        indexer_scores: Tuple of [batch, seq, seq] tensors per layer (raw indexer I_{t,s} scores)
        indexer_kl_targets: Tuple of [batch, seq, seq] tensors per layer (target distribution p_{t,:})

    Returns:
        Scalar tensor with averaged KL loss across layers
    """
    num_layers = len(indexer_scores)
    # Initialize total_loss on same device as scores to avoid device mismatch
    total_loss = torch.tensor(0.0, device=indexer_scores[0].device, dtype=indexer_scores[0].dtype)

    for scores, targets in zip(indexer_scores, indexer_kl_targets):
        # scores: [B, S, S] - raw indexer scores I_{t,s}
        # targets: [B, S, S] - target distribution p_{t,:} (already L1-normalized)

        # Convert indexer scores to log-probabilities
        log_probs = F.log_softmax(scores, dim=-1)

        # KL divergence: D_KL(p || q) = sum(p * log(p/q)) = sum(p * log(p) - p * log(q))
        # Since we have log(q), we compute: sum(p * log(p)) - sum(p * log(q))
        # The first term is negative entropy of p, second is cross-entropy
        # KL = -H(p) - (-CE(p,q)) = CE(p,q) - H(p) = sum(p * (log(p) - log(q)))

        # Compute KL divergence per position
        # targets is p_{t,:}, log_probs is log(softmax(I_{t,:}))
        # We want sum over s: p_{t,s} * (log(p_{t,s}) - log(q_{t,s}))
        # = sum(p * log(p)) - sum(p * log(q))

        # Add small epsilon to avoid log(0)
        eps = 1e-10
        targets_safe = targets + eps

        # KL divergence: sum(p * log(p/q))
        kl_per_position = targets * (torch.log(targets_safe) - log_probs)  # [B, S, S]
        kl_per_query = kl_per_position.sum(dim=-1)  # [B, S] - sum over keys
        kl_loss = kl_per_query.mean()  # Average over batch and query positions

        # Move kl_loss to same device as total_loss for multi-GPU compatibility
        total_loss = total_loss + kl_loss.to(total_loss.device)

    return total_loss / num_layers


@auto_docstring
class DeepseekV32ForCausalLM(DeepseekV32PreTrainedModel, GenerationMixin):
    """DeepSeek V3.2 for causal language modeling with indexer KL loss support."""

    _tied_weights_keys = {"lm_head.weight": "model.embed_tokens.weight"}
    _tp_plan = {"lm_head": "colwise_rep"}
    _pp_plan = {"lm_head": (["hidden_states"], ["logits"])}

    config_class = DeepseekV32Config

    def __init__(self, config: DeepseekV32Config):
        super().__init__(config)
        self.model = DeepseekV32Model(config)
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

        # Initialize weights and apply final processing
        self.post_init()

    @auto_docstring(custom_args=DEEPSEEK_V32_INDEXER_ARGS)
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        logits_to_keep: Union[int, torch.Tensor] = 0,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        output_indexer_scores: Optional[bool] = None,
        output_indexer_kl_target: Optional[bool] = None,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> CausalLMOutputWithIndexer:
        r"""
        Example:

        ```python
        >>> from transformers import AutoTokenizer, DeepseekV32ForCausalLM

        >>> model = DeepseekV32ForCausalLM.from_pretrained("meta-deepseek_v32/DeepseekV32-2-7b-hf")
        >>> tokenizer = AutoTokenizer.from_pretrained("meta-deepseek_v32/DeepseekV32-2-7b-hf")

        >>> prompt = "Hey, are you conscious? Can you talk to me?"
        >>> inputs = tokenizer(prompt, return_tensors="pt")

        >>> # Generate
        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        "Hey, are you conscious? Can you talk to me?\nI'm not conscious, but I can talk to you."
        ```"""
        # Auto-enable indexer outputs if KL loss is configured
        compute_kl_loss = self.config.indexer_kl_coef > 0
        if output_indexer_scores is None:
            output_indexer_scores = compute_kl_loss
        if output_indexer_kl_target is None:
            output_indexer_kl_target = compute_kl_loss

        outputs: BaseModelOutputWithIndexer = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            cache_position=cache_position,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            output_indexer_scores=output_indexer_scores,
            output_indexer_kl_target=output_indexer_kl_target,
            **kwargs,
        )

        hidden_states = outputs.last_hidden_state
        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss
        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
        logits = self.lm_head(hidden_states[:, slice_indices, :])

        # Compute language modeling loss
        lm_loss = None
        if labels is not None:
            lm_loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)

        # Compute indexer KL loss if we have the required outputs
        indexer_kl_loss = None
        if (
            outputs.indexer_scores is not None
            and outputs.indexer_kl_targets is not None
            and len(outputs.indexer_scores) > 0
            and len(outputs.indexer_kl_targets) > 0
        ):
            indexer_kl_loss = compute_indexer_kl_loss(
                outputs.indexer_scores,
                outputs.indexer_kl_targets,
            )

        # Compute combined loss
        loss = None
        if lm_loss is not None:
            loss = lm_loss
            if indexer_kl_loss is not None and self.config.indexer_kl_coef > 0:
                # Move indexer_kl_loss to same device as loss for multi-GPU compatibility
                loss = loss + self.config.indexer_kl_coef * indexer_kl_loss.to(loss.device)

        return CausalLMOutputWithIndexer(
            loss=loss,
            lm_loss=lm_loss,
            indexer_kl_loss=indexer_kl_loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )


class DeepseekV32ForSequenceClassification(GenericForSequenceClassification, DeepseekV32PreTrainedModel):
    """DeepSeek V3.2 for sequence classification."""

    config_class = DeepseekV32Config


class DeepseekV32ForTokenClassification(GenericForTokenClassification, DeepseekV32PreTrainedModel):
    """DeepSeek V3.2 for token classification."""

    config_class = DeepseekV32Config


__all__ = [
    "DeepseekV32PreTrainedModel",
    "DeepseekV32Model",
    "DeepseekV32ForCausalLM",
    "DeepseekV32ForSequenceClassification",
    "DeepseekV32ForTokenClassification",
]
