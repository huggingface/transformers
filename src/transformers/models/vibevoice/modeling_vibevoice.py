#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/vibevoice/modular_vibevoice.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_vibevoice.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# Copyright 2025 Microsoft and The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import math
from dataclasses import dataclass

import torch
import torch.nn as nn
import torch.nn.functional as F

from ... import initialization as init
from ...cache_utils import Cache
from ...generation import GenerationMixin
from ...modeling_outputs import ModelOutput
from ...modeling_utils import PreTrainedModel
from ...utils import auto_docstring, can_return_tuple
from ..auto import CONFIG_MAPPING, AutoModel
from .configuration_vibevoice import (
    VibeVoiceAcousticCodecConfig,
    VibeVoiceConfig,
    VibeVoiceDiffusionHeadConfig,
    VibeVoiceSemanticEncoderConfig,
    VibeVoiceStreamingConfig,
)


# =============================================================================
# Output Classes
# =============================================================================


@dataclass
@auto_docstring(
    custom_intro="""
    Base class for VibeVoice model outputs.
    """
)
class VibeVoiceOutput(ModelOutput):
    """
    Output type of VibeVoice models.

    Args:
        loss (`torch.FloatTensor` of shape `(1,)`, *optional*):
            Language modeling loss.
        audio_values (`torch.FloatTensor` of shape `(batch_size, audio_length)`):
            Generated audio waveforms.
        hidden_states (`tuple(torch.FloatTensor)`, *optional*):
            Hidden states of the model at the output of each layer.
        attentions (`tuple(torch.FloatTensor)`, *optional*):
            Attention weights.
    """

    loss: torch.FloatTensor | None = None
    audio_values: torch.FloatTensor | None = None
    hidden_states: tuple[torch.FloatTensor, ...] | None = None
    attentions: tuple[torch.FloatTensor, ...] | None = None


@dataclass
class VibeVoiceCausalLMOutputWithPast(ModelOutput):
    """
    Output type for VibeVoice causal language model with past key values.

    Args:
        loss (`torch.FloatTensor` of shape `(1,)`, *optional*):
            Language modeling loss.
        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, vocab_size)`):
            Prediction logits.
        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*):
            Pre-computed hidden-states for fast decoding.
        hidden_states (`tuple(torch.FloatTensor)`, *optional*):
            Hidden-states of the model at each layer.
        attentions (`tuple(torch.FloatTensor)`, *optional*):
            Attention weights at each layer.
        acoustic_features (`torch.FloatTensor`, *optional*):
            Predicted acoustic features from diffusion head.
    """

    loss: torch.FloatTensor | None = None
    logits: torch.FloatTensor | None = None
    past_key_values: tuple[tuple[torch.FloatTensor]] | None = None
    hidden_states: tuple[torch.FloatTensor, ...] | None = None
    attentions: tuple[torch.FloatTensor, ...] | None = None
    acoustic_features: torch.FloatTensor | None = None


# =============================================================================
# Normalization Modules
# =============================================================================


class VibeVoiceRMSNorm(nn.Module):
    """RMS Normalization layer used in VibeVoice."""

    def __init__(self, dim: int, eps: float = 1e-5, elementwise_affine: bool = True):
        super().__init__()
        self.eps = eps
        self.elementwise_affine = elementwise_affine
        if elementwise_affine:
            self.weight = nn.Parameter(torch.ones(dim))
        else:
            self.register_parameter("weight", None)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        rms = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        x = x * rms
        if self.weight is not None:
            x = x * self.weight
        return x


class VibeVoiceConvRMSNorm(nn.Module):
    """RMS Normalization for convolutional features (channel-last normalization)."""

    def __init__(self, dim: int, eps: float = 1e-5, elementwise_affine: bool = True):
        super().__init__()
        self.norm = VibeVoiceRMSNorm(dim, eps=eps, elementwise_affine=elementwise_affine)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (batch, channels, time) -> (batch, time, channels)
        x = x.transpose(1, 2)
        x = self.norm(x)
        # (batch, time, channels) -> (batch, channels, time)
        x = x.transpose(1, 2)
        return x


# =============================================================================
# Convolution Modules
# =============================================================================


class VibeVoiceCausalConv1d(nn.Module):
    """Causal 1D convolution with proper padding for streaming."""

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int = 1,
        dilation: int = 1,
        groups: int = 1,
        bias: bool = True,
        pad_mode: str = "constant",
    ):
        super().__init__()
        self.pad_mode = pad_mode
        self.causal_padding = (kernel_size - 1) * dilation

        self.conv = nn.Conv1d(
            in_channels,
            out_channels,
            kernel_size,
            stride=stride,
            dilation=dilation,
            groups=groups,
            bias=bias,
            padding=0,
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if self.causal_padding > 0:
            x = F.pad(x, (self.causal_padding, 0), mode=self.pad_mode)
        return self.conv(x)


class VibeVoiceCausalConvTranspose1d(nn.Module):
    """Causal transposed 1D convolution for upsampling.

    Uses 'convtr' as attribute name to match pretrained weight naming.
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int = 1,
        groups: int = 1,
        bias: bool = True,
    ):
        super().__init__()
        self.stride = stride
        self.trim = kernel_size - stride

        # Named 'convtr' to match pretrained: upsample_layers.{i}.0.convtr.convtr.weight
        self.convtr = nn.ConvTranspose1d(
            in_channels,
            out_channels,
            kernel_size,
            stride=stride,
            groups=groups,
            bias=bias,
            padding=0,
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.convtr(x)
        if self.trim > 0:
            x = x[:, :, : -self.trim]
        return x


# =============================================================================
# Convolution Wrapper (for weight name matching)
# =============================================================================


class VibeVoiceConvWrapper(nn.Module):
    """Wrapper to match weight naming: conv.conv.bias/weight"""

    def __init__(self, conv_module):
        super().__init__()
        self.conv = conv_module

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.conv(x)


# =============================================================================
# Transformer Blocks for Tokenizers
# =============================================================================


class VibeVoiceTokenizerFFN(nn.Module):
    """Feed-forward network for tokenizer transformer blocks.

    Weight names aligned with pretrained model: ffn.linear1, ffn.linear2
    """

    def __init__(self, dim: int, expansion_ratio: float = 4.0, dropout: float = 0.0):
        super().__init__()
        hidden_dim = int(dim * expansion_ratio)
        self.linear1 = nn.Linear(dim, hidden_dim)
        self.act = nn.GELU()
        self.linear2 = nn.Linear(hidden_dim, dim)
        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.linear1(x)
        x = self.act(x)
        x = self.dropout(x)
        x = self.linear2(x)
        x = self.dropout(x)
        return x


class VibeVoiceTokenizerMixer(nn.Module):
    """Mixer layer using depthwise convolution for local context.

    Weight names aligned with pretrained model: mixer.conv.conv.conv
    """

    def __init__(self, dim: int, kernel_size: int = 7, causal: bool = True):
        super().__init__()
        self.causal = causal
        self.kernel_size = kernel_size

        if causal:
            inner_conv = VibeVoiceCausalConv1d(dim, dim, kernel_size, groups=dim, bias=True)
        else:
            padding = kernel_size // 2
            inner_conv = nn.Conv1d(dim, dim, kernel_size, padding=padding, groups=dim, bias=True)

        # Structure to match: mixer.conv.conv.conv.weight
        # ModuleDict["conv"] -> VibeVoiceCausalConv1d.conv -> nn.Conv1d
        self.conv = nn.ModuleDict({"conv": inner_conv})

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (batch, time, dim) -> (batch, dim, time)
        x = x.transpose(1, 2)
        x = self.conv["conv"](x)
        # (batch, dim, time) -> (batch, time, dim)
        x = x.transpose(1, 2)
        return x


class VibeVoiceTokenizerBlock(nn.Module):
    """Transformer block for the tokenizer encoder/decoder.

    Weight names aligned with pretrained model:
    - norm (not norm1)
    - ffn_norm (not norm2)
    - gamma (not layer_scale1)
    - ffn_gamma (not layer_scale2)
    - ffn.linear1, ffn.linear2
    - mixer.conv.conv.conv
    """

    def __init__(
        self,
        dim: int,
        layernorm: str = "RMSNorm",
        layernorm_eps: float = 1e-5,
        layernorm_elementwise_affine: bool = True,
        mixer_layer: str = "depthwise_conv",
        causal: bool = True,
        layer_scale_init_value: float = 1e-6,
    ):
        super().__init__()

        # Normalization - aligned naming
        if layernorm == "RMSNorm":
            self.norm = VibeVoiceRMSNorm(dim, eps=layernorm_eps, elementwise_affine=layernorm_elementwise_affine)
            self.ffn_norm = VibeVoiceRMSNorm(dim, eps=layernorm_eps, elementwise_affine=layernorm_elementwise_affine)
        else:
            self.norm = nn.LayerNorm(dim, eps=layernorm_eps, elementwise_affine=layernorm_elementwise_affine)
            self.ffn_norm = nn.LayerNorm(dim, eps=layernorm_eps, elementwise_affine=layernorm_elementwise_affine)

        # Mixer
        if mixer_layer == "depthwise_conv":
            self.mixer = VibeVoiceTokenizerMixer(dim, causal=causal)
        else:
            self.mixer = nn.Identity()

        # FFN
        self.ffn = VibeVoiceTokenizerFFN(dim)

        # Layer scale - aligned naming: gamma, ffn_gamma
        if layer_scale_init_value > 0:
            self.gamma = nn.Parameter(torch.ones(dim) * layer_scale_init_value)
            self.ffn_gamma = nn.Parameter(torch.ones(dim) * layer_scale_init_value)
        else:
            self.register_parameter("gamma", None)
            self.register_parameter("ffn_gamma", None)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Mixer block
        residual = x
        x = self.norm(x)
        x = self.mixer(x)
        if self.gamma is not None:
            x = x * self.gamma
        x = residual + x

        # FFN block
        residual = x
        x = self.ffn_norm(x)
        x = self.ffn(x)
        if self.ffn_gamma is not None:
            x = x * self.ffn_gamma
        x = residual + x

        return x


# =============================================================================
# Downsampling/Upsampling Layers
# =============================================================================


class VibeVoiceDownsampleLayer(nn.ModuleList):
    """Downsample layer wrapper to match weight naming: downsample_layers.{i}.0.conv.conv"""

    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int, pad_mode: str = "constant"):
        super().__init__()
        # Structure: downsample_layers[i][0].conv.conv.weight
        # This class IS a ModuleList, so [i] accesses it, [0] accesses first element
        inner_conv = VibeVoiceCausalConv1d(in_channels, out_channels, kernel_size, stride=stride, pad_mode=pad_mode)
        self.append(nn.ModuleDict({"conv": inner_conv}))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self[0]["conv"](x)


class VibeVoiceUpsampleInputLayer(nn.ModuleList):
    """Input projection layer for decoder: upsample_layers.0.0.conv.conv (regular conv)"""

    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 7, pad_mode: str = "constant"):
        super().__init__()
        # Structure: upsample_layers[0][0].conv.conv.weight (regular conv for input projection)
        inner_conv = VibeVoiceCausalConv1d(in_channels, out_channels, kernel_size, stride=1, pad_mode=pad_mode)
        self.append(nn.ModuleDict({"conv": inner_conv}))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self[0]["conv"](x)


class VibeVoiceUpsampleLayer(nn.ModuleList):
    """Upsample layer wrapper to match weight naming: upsample_layers.{i}.0.convtr.convtr"""

    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int):
        super().__init__()
        # Structure: upsample_layers[i][0].convtr.convtr.weight (transposed conv for upsampling)
        inner_conv = VibeVoiceCausalConvTranspose1d(in_channels, out_channels, kernel_size, stride=stride)
        self.append(nn.ModuleDict({"convtr": inner_conv}))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self[0]["convtr"](x)


# =============================================================================
# Acoustic Tokenizer
# =============================================================================


class VibeVoiceAcousticEncoder(nn.Module):
    """Encoder for the acoustic tokenizer (Ïƒ-VAE).

    Weight names aligned with pretrained model:
    - stages[stage_idx][block_idx] for transformer blocks
    - downsample_layers[i][0].conv.conv for downsampling
    - head.conv.conv for output projection

    Architecture: 7 stages with 7 downsamples
    - downsample_layers[0] = stem (audio -> n_filters, kernel=7)
    - downsample_layers[1-6] = based on reversed ratios
    - stages[0-6] = transformer blocks at each resolution
    """

    def __init__(self, config: VibeVoiceAcousticCodecConfig):
        super().__init__()
        self.config = config

        # Parse encoder depths (7 values for 7 stages)
        depths = [int(d) for d in config.encoder_depths.split("-")]
        ratios = config.encoder_ratios  # 6 ratios: [8, 5, 5, 4, 2, 2]
        n_filters = config.encoder_n_filters

        # Reverse ratios for downsampling order (pretrained uses [2, 2, 4, 5, 5, 8])
        reversed_ratios = ratios[::-1]

        # Stages and downsampling layers
        self.stages = nn.ModuleList()
        self.downsample_layers = nn.ModuleList()

        # Stem: downsample_layers[0] - audio to n_filters with fixed kernel_size=7
        self.downsample_layers.append(
            VibeVoiceDownsampleLayer(config.channels, n_filters, kernel_size=7, stride=1, pad_mode=config.pad_mode)
        )

        # First stage at n_filters
        self.stages.append(
            nn.ModuleList(
                [
                    VibeVoiceTokenizerBlock(
                        dim=n_filters,
                        layernorm=config.layernorm,
                        layernorm_eps=config.layernorm_eps,
                        layernorm_elementwise_affine=config.layernorm_elementwise_affine,
                        mixer_layer=config.mixer_layer,
                        causal=config.causal,
                        layer_scale_init_value=config.layer_scale_init_value,
                    )
                    for _ in range(depths[0])
                ]
            )
        )

        current_channels = n_filters

        # Remaining stages: downsample + transformer blocks
        for i, (ratio, depth) in enumerate(zip(reversed_ratios, depths[1:])):
            out_channels = current_channels * 2

            # Downsample layer
            self.downsample_layers.append(
                VibeVoiceDownsampleLayer(
                    current_channels, out_channels, kernel_size=ratio * 2, stride=ratio, pad_mode=config.pad_mode
                )
            )

            # Transformer blocks for this stage
            self.stages.append(
                nn.ModuleList(
                    [
                        VibeVoiceTokenizerBlock(
                            dim=out_channels,
                            layernorm=config.layernorm,
                            layernorm_eps=config.layernorm_eps,
                            layernorm_elementwise_affine=config.layernorm_elementwise_affine,
                            mixer_layer=config.mixer_layer,
                            causal=config.causal,
                            layer_scale_init_value=config.layer_scale_init_value,
                        )
                        for _ in range(depth)
                    ]
                )
            )

            current_channels = out_channels

        # Output projection as head.conv.conv
        # For gaussian VAE: if fix_std > 0, only output mean (fixed variance)
        # Otherwise output both mean and log_var
        if config.std_dist_type == "gaussian" and config.fix_std <= 0:
            out_dim = config.vae_dim * 2  # mean + log_var
        else:
            out_dim = config.vae_dim  # mean only (fixed or no variance)

        # Output projection: head.conv.conv.weight (kernel_size=7 to match pretrained)
        head_conv = VibeVoiceCausalConv1d(current_channels, out_dim, kernel_size=7, pad_mode=config.pad_mode)
        self.head = nn.ModuleDict({"conv": head_conv})

    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor | None]:
        """
        Args:
            x: Audio waveform (batch, channels, time)

        Returns:
            mean: Mean of the VAE latent distribution
            log_var: Log variance (None if std_dist_type is not "gaussian")
        """
        # Process through downsample + stage pairs
        for i, (downsample, stage) in enumerate(zip(self.downsample_layers, self.stages)):
            x = downsample(x)
            # (batch, channels, time) -> (batch, time, channels)
            x = x.transpose(1, 2)
            for block in stage:
                x = block(x)
            # (batch, time, channels) -> (batch, channels, time)
            x = x.transpose(1, 2)

        # Project to latent space via head
        x = self.head["conv"](x)

        # (batch, channels, time) -> (batch, time, channels)
        x = x.transpose(1, 2)

        if self.config.std_dist_type == "gaussian" and self.config.fix_std <= 0:
            # Learned variance: output is mean + log_var
            mean, log_var = x.chunk(2, dim=-1)
            return mean, log_var
        else:
            # Fixed variance or no VAE: output is just mean
            return x, None


class VibeVoiceAcousticDecoder(nn.Module):
    """Decoder for the acoustic tokenizer (Ïƒ-VAE).

    Weight names aligned with pretrained model:
    - upsample_layers[0].0.conv.conv for input projection (regular conv from vae_dim)
    - stages[stage_idx][block_idx] for transformer blocks
    - upsample_layers[i].0.convtr.convtr for upsampling (i > 0, transposed conv)
    - head.conv.conv for final audio output
    """

    def __init__(self, config: VibeVoiceAcousticCodecConfig):
        super().__init__()
        self.config = config

        # Parse decoder depths (mirror of encoder if not specified)
        if config.decoder_depths is not None:
            depths = [int(d) for d in config.decoder_depths.split("-")]
        else:
            depths = [int(d) for d in config.encoder_depths.split("-")][::-1]

        ratios = config.decoder_ratios  # Use ratios in original order (encoder reverses them)
        n_filters = config.decoder_n_filters

        # Calculate initial channels (should match encoder output channels)
        num_stages = len(ratios)
        initial_channels = n_filters * (2**num_stages)

        # Stages and upsample layers
        self.stages = nn.ModuleList()
        self.upsample_layers = nn.ModuleList()

        current_channels = initial_channels

        # First upsample layer is the input projection (regular conv from vae_dim to initial_channels)
        # Structure: upsample_layers.0.0.conv.conv.weight
        self.upsample_layers.append(
            VibeVoiceUpsampleInputLayer(config.vae_dim, initial_channels, kernel_size=7, pad_mode=config.pad_mode)
        )

        # Initial stage transformer blocks (at initial_channels)
        initial_stage_blocks = nn.ModuleList(
            [
                VibeVoiceTokenizerBlock(
                    dim=current_channels,
                    layernorm=config.layernorm,
                    layernorm_eps=config.layernorm_eps,
                    layernorm_elementwise_affine=config.layernorm_elementwise_affine,
                    mixer_layer=config.mixer_layer,
                    causal=config.causal,
                    layer_scale_init_value=config.layer_scale_init_value,
                )
                for _ in range(depths[0])
            ]
        )
        self.stages.append(initial_stage_blocks)

        # Upsampling stages with transposed convolutions
        for i, (ratio, depth) in enumerate(zip(ratios, depths[1:])):
            out_channels = current_channels // 2

            # Upsample layer (transposed conv)
            # Structure: upsample_layers.{i+1}.0.convtr.convtr.weight
            self.upsample_layers.append(
                VibeVoiceUpsampleLayer(current_channels, out_channels, kernel_size=ratio * 2, stride=ratio)
            )

            # Transformer blocks for this stage
            stage_blocks = nn.ModuleList(
                [
                    VibeVoiceTokenizerBlock(
                        dim=out_channels,
                        layernorm=config.layernorm,
                        layernorm_eps=config.layernorm_eps,
                        layernorm_elementwise_affine=config.layernorm_elementwise_affine,
                        mixer_layer=config.mixer_layer,
                        causal=config.causal,
                        layer_scale_init_value=config.layer_scale_init_value,
                    )
                    for _ in range(depth)
                ]
            )
            self.stages.append(stage_blocks)

            current_channels = out_channels

        # Final convolution to audio: head.conv.conv.weight
        head_conv = VibeVoiceCausalConv1d(current_channels, config.channels, kernel_size=7, pad_mode=config.pad_mode)
        self.head = nn.ModuleDict({"conv": head_conv})

    def forward(self, z: torch.Tensor) -> torch.Tensor:
        """
        Args:
            z: Latent representation (batch, time, vae_dim)

        Returns:
            Audio waveform (batch, channels, time)
        """
        # (batch, time, vae_dim) -> (batch, vae_dim, time)
        x = z.transpose(1, 2)

        # Input projection: upsample_layers[0] (regular conv from vae_dim to initial_channels)
        x = self.upsample_layers[0](x)

        # (batch, channels, time) -> (batch, time, channels)
        x = x.transpose(1, 2)

        # Initial transformer blocks (stages[0])
        for block in self.stages[0]:
            x = block(x)

        # (batch, time, channels) -> (batch, channels, time)
        x = x.transpose(1, 2)

        # Upsampling with transposed convs (upsample_layers[1:]) and stage blocks (stages[1:])
        for i, upsample in enumerate(self.upsample_layers[1:]):
            x = upsample(x)
            # (batch, channels, time) -> (batch, time, channels)
            x = x.transpose(1, 2)
            for block in self.stages[i + 1]:
                x = block(x)
            # (batch, time, channels) -> (batch, channels, time)
            x = x.transpose(1, 2)

        # Final convolution to audio
        x = self.head["conv"](x)

        return x


class VibeVoiceAcousticCodec(PreTrainedModel):
    """
    VibeVoice Acoustic Tokenizer (Ïƒ-VAE).

    Converts audio waveforms to continuous latent representations and back.
    Uses a mirror-symmetric encoder-decoder architecture with transformer blocks.
    """

    config_class = VibeVoiceAcousticCodecConfig
    main_input_name = "audio_values"

    def __init__(self, config: VibeVoiceAcousticCodecConfig):
        super().__init__(config)
        self.encoder = VibeVoiceAcousticEncoder(config)
        self.decoder = VibeVoiceAcousticDecoder(config)
        self.fix_std = config.fix_std

        self.post_init()

    def encode(self, audio_values: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor | None]:
        """Encode audio to latent space."""
        return self.encoder(audio_values)

    def decode(self, z: torch.Tensor) -> torch.Tensor:
        """Decode latent to audio."""
        return self.decoder(z)

    def reparameterize(self, mean: torch.Tensor, log_var: torch.Tensor | None) -> torch.Tensor:
        """VAE reparameterization trick."""
        if log_var is None or self.fix_std == 0:
            return mean

        if self.fix_std > 0:
            std = self.fix_std
        else:
            std = torch.exp(0.5 * log_var)

        eps = torch.randn_like(mean)
        return mean + eps * std

    def forward(
        self,
        audio_values: torch.Tensor,
        return_dict: bool = True,
        **kwargs,
    ) -> tuple | ModelOutput:
        """
        Args:
            audio_values: Audio waveform (batch, channels, time)

        Returns:
            Reconstructed audio and latent representations
        """
        # Encode
        mean, log_var = self.encode(audio_values)

        # Sample latent
        z = self.reparameterize(mean, log_var)

        # Decode
        reconstructed = self.decode(z)

        if not return_dict:
            return (reconstructed, mean, log_var, z)

        return ModelOutput(
            reconstructed_audio=reconstructed,
            mean=mean,
            log_var=log_var,
            latent=z,
        )


# =============================================================================
# Semantic Encoder
# =============================================================================


class VibeVoiceSemanticEncoder(PreTrainedModel):
    """
    VibeVoice Semantic Tokenizer.

    Extracts semantic representations from audio, trained with an ASR proxy task.
    Only used in the 1.5B model variant (not in streaming mode).
    """

    config_class = VibeVoiceSemanticEncoderConfig
    main_input_name = "audio_values"

    def __init__(self, config: VibeVoiceSemanticEncoderConfig):
        super().__init__(config)
        self.encoder = VibeVoiceAcousticEncoder(config)

        self.post_init()

    def forward(
        self,
        audio_values: torch.Tensor,
        return_dict: bool = True,
        **kwargs,
    ) -> tuple | ModelOutput:
        """
        Args:
            audio_values: Audio waveform (batch, channels, time)

        Returns:
            Semantic features
        """
        # Encode (no VAE sampling for semantic tokenizer)
        features, _ = self.encoder(audio_values)

        if not return_dict:
            return (features,)

        return ModelOutput(semantic_features=features)


# =============================================================================
# Diffusion Head (Prediction Head)
# =============================================================================


class VibeVoiceTimestepEmbedder(nn.Module):
    """Embeds diffusion timesteps into vector representations.

    Weight names aligned: t_embedder.mlp.0, t_embedder.mlp.2
    """

    def __init__(self, hidden_size: int, frequency_embedding_size: int = 256):
        super().__init__()
        # No bias in MLPs to match pretrained
        self.mlp = nn.Sequential(
            nn.Linear(frequency_embedding_size, hidden_size, bias=False),
            nn.SiLU(),
            nn.Linear(hidden_size, hidden_size, bias=False),
        )
        self.frequency_embedding_size = frequency_embedding_size

    @staticmethod
    def timestep_embedding(t: torch.Tensor, dim: int, max_period: int = 10000) -> torch.Tensor:
        """Create sinusoidal timestep embeddings."""
        half = dim // 2
        freqs = torch.exp(-math.log(max_period) * torch.arange(half, device=t.device, dtype=torch.float32) / half)
        args = t[:, None].float() * freqs[None]
        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)
        if dim % 2:
            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)
        return embedding

    def forward(self, t: torch.Tensor) -> torch.Tensor:
        t_freq = self.timestep_embedding(t, self.frequency_embedding_size)
        return self.mlp(t_freq)


class VibeVoiceDiffusionFFN(nn.Module):
    """Feed-forward network with gating for diffusion head."""

    def __init__(self, hidden_size: int, ffn_ratio: float = 3.0):
        super().__init__()
        hidden_dim = int(hidden_size * ffn_ratio)
        self.gate_proj = nn.Linear(hidden_size, hidden_dim, bias=False)
        self.up_proj = nn.Linear(hidden_size, hidden_dim, bias=False)
        self.down_proj = nn.Linear(hidden_dim, hidden_size, bias=False)
        self.act_fn = nn.SiLU()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        gate = self.act_fn(self.gate_proj(x))
        up = self.up_proj(x)
        return self.down_proj(gate * up)


class VibeVoiceDiffusionLayer(nn.Module):
    """Transformer layer for diffusion head with adaptive layer normalization.

    Weight names aligned with pretrained:
    - layers[i].norm (single norm)
    - layers[i].ffn.{gate_proj, up_proj, down_proj}
    - layers[i].adaLN_modulation.1
    """

    def __init__(self, config: VibeVoiceDiffusionHeadConfig):
        super().__init__()
        self.hidden_size = config.hidden_size

        # Single norm layer (aligned with pretrained)
        self.norm = VibeVoiceRMSNorm(config.hidden_size, eps=config.rms_norm_eps)

        self.ffn = VibeVoiceDiffusionFFN(config.hidden_size, config.head_ffn_ratio)

        # AdaLN modulation - aligned naming: adaLN_modulation.1 (index 1 is Linear after SiLU)
        # Pretrained uses 3 outputs: shift, scale, gate; no bias
        self.adaLN_modulation = nn.Sequential(
            nn.SiLU(),
            nn.Linear(config.hidden_size, 3 * config.hidden_size, bias=False),
        )

        # Initialize modulation to zero
        nn.init.zeros_(self.adaLN_modulation[1].weight)

    def forward(self, x: torch.Tensor, c: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: Input features (batch, seq_len, hidden_size)
            c: Conditioning (batch, hidden_size)
        """
        # AdaLN modulation parameters (shift, scale, gate)
        modulation = self.adaLN_modulation(c)
        shift, scale, gate = modulation.chunk(3, dim=-1)

        # Norm + modulation + FFN
        h = self.norm(x)
        h = h * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)
        h = self.ffn(h)
        x = x + gate.unsqueeze(1) * h

        return x


class VibeVoiceDiffusionFinalLayer(nn.Module):
    """Final layer of diffusion head that projects to latent space.

    Weight names aligned: final_layer.linear, final_layer.adaLN_modulation.1
    Pretrained doesn't have separate norm - just shift/scale modulation.
    """

    def __init__(self, config: VibeVoiceDiffusionHeadConfig):
        super().__init__()
        self.linear = nn.Linear(config.hidden_size, config.latent_size, bias=False)

        # AdaLN modulation (no bias, no separate norm to match pretrained)
        self.adaLN_modulation = nn.Sequential(
            nn.SiLU(),
            nn.Linear(config.hidden_size, 2 * config.hidden_size, bias=False),
        )

        # Initialize to zero
        nn.init.zeros_(self.adaLN_modulation[1].weight)
        nn.init.zeros_(self.linear.weight)

    def forward(self, x: torch.Tensor, c: torch.Tensor) -> torch.Tensor:
        shift, scale = self.adaLN_modulation(c).chunk(2, dim=-1)
        # Apply shift/scale modulation directly (no norm)
        x = x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)
        x = self.linear(x)
        return x


class VibeVoiceDiffusionHead(PreTrainedModel):
    """
    VibeVoice Prediction Head (Diffusion Head).

    Predicts acoustic VAE features conditioned on LLM hidden states
    using Denoising Diffusion Probabilistic Models (DDPM).

    Weight names aligned with pretrained model: prediction_head.*
    - noisy_images_proj (not x_proj)
    - cond_proj for conditioning
    - t_embedder.mlp.{0,2}
    - layers[i].*
    - final_layer.*
    """

    config_class = VibeVoiceDiffusionHeadConfig
    main_input_name = "hidden_states"

    def __init__(self, config: VibeVoiceDiffusionHeadConfig):
        super().__init__(config)

        # Input projections - aligned naming
        self.noisy_images_proj = nn.Linear(config.latent_size, config.hidden_size, bias=False)
        self.cond_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=False)
        self.t_embedder = VibeVoiceTimestepEmbedder(config.hidden_size)

        # Transformer layers
        self.layers = nn.ModuleList([VibeVoiceDiffusionLayer(config) for _ in range(config.head_layers)])

        # Final layer
        self.final_layer = VibeVoiceDiffusionFinalLayer(config)

        self.post_init()

    def forward(
        self,
        noisy_latents: torch.Tensor,
        timesteps: torch.Tensor,
        conditioning: torch.Tensor,
        **kwargs,
    ) -> torch.Tensor:
        """
        Args:
            noisy_latents: Noisy latent features (batch, seq_len, latent_size)
            timesteps: Diffusion timesteps (batch,)
            conditioning: LLM hidden states (batch, seq_len, hidden_size)

        Returns:
            Predicted noise or v-prediction (batch, seq_len, latent_size)
        """
        # Project noisy latents
        x = self.noisy_images_proj(noisy_latents)

        # Project and add conditioning
        cond = self.cond_proj(conditioning)
        x = x + cond

        # Timestep embedding
        t_emb = self.t_embedder(timesteps)

        # Process through layers
        for layer in self.layers:
            x = layer(x, t_emb)

        # Final projection
        output = self.final_layer(x, t_emb)

        return output


# =============================================================================
# Connectors
# =============================================================================


class VibeVoiceConnector(nn.Module):
    """Connects features to the language model dimension.

    Weight names aligned: fc1, norm, fc2
    """

    def __init__(self, input_dim: int, hidden_size: int):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_size)
        self.norm = VibeVoiceRMSNorm(hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.fc1(x)
        x = self.norm(x)
        x = self.fc2(x)
        return x


# =============================================================================
# Main VibeVoice Model
# =============================================================================


class VibeVoicePreTrainedModel(PreTrainedModel):
    """Base class for VibeVoice models."""

    config_class = VibeVoiceConfig
    base_model_prefix = "model"
    supports_gradient_checkpointing = True
    _no_split_modules = ["Qwen2DecoderLayer", "VibeVoiceDiffusionLayer"]
    _skip_keys_device_placement = ["past_key_values"]
    _supports_flash_attn = True
    _supports_sdpa = True
    _supports_cache_class = True

    def _init_weights(self, module):
        std = self.config.decoder_config.initializer_range if hasattr(self.config, "decoder_config") else 0.02
        if isinstance(module, nn.Linear):
            init.normal_(module.weight, mean=0.0, std=std)
            if module.bias is not None:
                init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            init.normal_(module.weight, mean=0.0, std=std)


@auto_docstring(
    custom_intro="""
    The VibeVoice Model for text-to-speech generation.

    This model consists of a Qwen2 language model backbone, acoustic/semantic tokenizers,
    and a diffusion head for generating high-fidelity speech.
    """
)
class VibeVoiceModel(VibeVoicePreTrainedModel):
    def __init__(self, config: VibeVoiceConfig):
        super().__init__(config)

        # Language model backbone - aligned naming
        self.language_model = AutoModel.from_config(config.decoder_config)

        # Acoustic tokenizer
        self.acoustic_tokenizer = VibeVoiceAcousticCodec(config.acoustic_tokenizer_config)

        # Semantic tokenizer (only for 1.5B model)
        if config.has_semantic_tokenizer:
            self.semantic_tokenizer = VibeVoiceSemanticEncoder(config.semantic_tokenizer_config)
        else:
            self.semantic_tokenizer = None

        # Prediction head (diffusion head) - aligned naming
        self.prediction_head = VibeVoiceDiffusionHead(config.diffusion_head_config)

        # Connectors - aligned naming: acoustic_connector, semantic_connector
        self.acoustic_connector = VibeVoiceConnector(config.acoustic_vae_dim, config.decoder_config.hidden_size)

        if config.has_semantic_tokenizer:
            self.semantic_connector = VibeVoiceConnector(config.semantic_vae_dim, config.decoder_config.hidden_size)
        else:
            self.semantic_connector = None

        # Speech scaling factors - aligned naming (scalars, not 1D tensors)
        self.speech_scaling_factor = nn.Parameter(torch.tensor(1.0))
        self.speech_bias_factor = nn.Parameter(torch.tensor(0.0))

        self.post_init()

    def get_input_embeddings(self):
        return self.language_model.get_input_embeddings()

    def set_input_embeddings(self, value):
        self.language_model.set_input_embeddings(value)

    @can_return_tuple
    def forward(
        self,
        input_ids: torch.LongTensor | None = None,
        attention_mask: torch.Tensor | None = None,
        position_ids: torch.LongTensor | None = None,
        past_key_values: Cache | None = None,
        inputs_embeds: torch.FloatTensor | None = None,
        use_cache: bool | None = None,
        output_attentions: bool | None = None,
        output_hidden_states: bool | None = None,
        return_dict: bool | None = None,
        cache_position: torch.LongTensor | None = None,
        **kwargs,
    ):
        """
        Forward pass through the VibeVoice model.
        """
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        # Get embeddings
        if inputs_embeds is None:
            inputs_embeds = self.get_input_embeddings()(input_ids)

        # Forward through language model
        outputs = self.language_model(
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=True,
            cache_position=cache_position,
            **kwargs,
        )

        return outputs


class VibeVoiceForConditionalGeneration(VibeVoicePreTrainedModel, GenerationMixin):
    """
    VibeVoice Model for conditional text-to-speech generation.

    Supports both the 1.5B long-form model with semantic tokenizer and
    the streaming variant without semantic tokenizer.
    """

    _tied_weights_keys = {"lm_head.weight": "model.language_model.embed_tokens.weight"}

    def __init__(self, config: VibeVoiceConfig):
        super().__init__(config)

        self.model = VibeVoiceModel(config)
        self.lm_head = nn.Linear(config.decoder_config.hidden_size, config.decoder_config.vocab_size, bias=False)

        self.post_init()

    def get_input_embeddings(self):
        return self.model.get_input_embeddings()

    def set_input_embeddings(self, value):
        self.model.set_input_embeddings(value)

    def get_output_embeddings(self):
        return self.lm_head

    def set_output_embeddings(self, value):
        self.lm_head = value

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: torch.LongTensor | None = None,
        attention_mask: torch.Tensor | None = None,
        position_ids: torch.LongTensor | None = None,
        past_key_values: Cache | None = None,
        inputs_embeds: torch.FloatTensor | None = None,
        labels: torch.LongTensor | None = None,
        use_cache: bool | None = None,
        output_attentions: bool | None = None,
        output_hidden_states: bool | None = None,
        return_dict: bool | None = None,
        cache_position: torch.LongTensor | None = None,
        **kwargs,
    ) -> VibeVoiceCausalLMOutputWithPast:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss.
        """
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=True,
            cache_position=cache_position,
            **kwargs,
        )

        hidden_states = outputs.last_hidden_state
        logits = self.lm_head(hidden_states)

        loss = None
        if labels is not None:
            loss = self.loss_function(
                logits=logits,
                labels=labels,
                vocab_size=self.config.decoder_config.vocab_size,
                **kwargs,
            )

        return VibeVoiceCausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )

    def prepare_inputs_for_generation(
        self,
        input_ids,
        past_key_values=None,
        attention_mask=None,
        inputs_embeds=None,
        cache_position=None,
        **kwargs,
    ):
        model_inputs = super().prepare_inputs_for_generation(
            input_ids,
            past_key_values=past_key_values,
            attention_mask=attention_mask,
            inputs_embeds=inputs_embeds,
            cache_position=cache_position,
            **kwargs,
        )
        return model_inputs

    def decode_audio(self, acoustic_features: torch.Tensor) -> torch.Tensor:
        """Decode acoustic features to audio waveform."""
        return self.model.acoustic_tokenizer.decode(acoustic_features)


class VibeVoiceEosClassifier(nn.Module):
    """End-of-speech classifier for streaming model."""

    def __init__(self, hidden_size: int):
        super().__init__()
        self.fc1 = nn.Linear(hidden_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, 1)

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        x = self.fc1(hidden_states)
        x = F.silu(x)
        x = self.fc2(x)
        return x


class VibeVoiceStreamingModel(VibeVoicePreTrainedModel):
    """
    VibeVoice Streaming Model backbone.

    This variant splits the language model into two parts:
    - language_model: text backbone with `text_backbone_num_hidden_layers` layers
    - tts_language_model: TTS backbone with `tts_backbone_num_hidden_layers` layers

    The acoustic tokenizer only has decoder (no encoder) for streaming.
    """

    def __init__(self, config: VibeVoiceStreamingConfig):
        super().__init__(config)

        # Create config for text backbone (first N layers)
        text_lm_config_dict = config.decoder_config.to_dict()
        text_lm_config_dict["num_hidden_layers"] = config.text_backbone_num_hidden_layers
        # Truncate layer_types if present to match num_hidden_layers
        if "layer_types" in text_lm_config_dict and text_lm_config_dict["layer_types"]:
            text_lm_config_dict["layer_types"] = text_lm_config_dict["layer_types"][
                : config.text_backbone_num_hidden_layers
            ]
        text_lm_config = CONFIG_MAPPING[config.decoder_config.model_type](**text_lm_config_dict)

        # Create config for TTS backbone (remaining layers)
        tts_lm_config_dict = config.decoder_config.to_dict()
        tts_lm_config_dict["num_hidden_layers"] = config.tts_backbone_num_hidden_layers
        # Truncate layer_types if present to match num_hidden_layers
        if "layer_types" in tts_lm_config_dict and tts_lm_config_dict["layer_types"]:
            tts_lm_config_dict["layer_types"] = tts_lm_config_dict["layer_types"][
                : config.tts_backbone_num_hidden_layers
            ]
        tts_lm_config = CONFIG_MAPPING[config.decoder_config.model_type](**tts_lm_config_dict)

        # Language models - text backbone has no final norm
        self.language_model = AutoModel.from_config(text_lm_config)
        self.tts_language_model = AutoModel.from_config(tts_lm_config)

        # TTS input types embedding
        hidden_size = config.decoder_config.hidden_size
        self.tts_input_types = nn.Embedding(config.num_tts_input_types, hidden_size)

        # Acoustic tokenizer (decoder only for streaming)
        self.acoustic_tokenizer = VibeVoiceAcousticCodec(config.acoustic_tokenizer_config)
        # Remove encoder for streaming - it won't have weights
        self.acoustic_tokenizer.encoder = None

        # Prediction head (diffusion head)
        self.prediction_head = VibeVoiceDiffusionHead(config.diffusion_head_config)

        # Connector
        self.acoustic_connector = VibeVoiceConnector(config.acoustic_vae_dim, hidden_size)

        # Speech scaling factors (scalars)
        self.speech_scaling_factor = nn.Parameter(torch.tensor(1.0))
        self.speech_bias_factor = nn.Parameter(torch.tensor(0.0))

        # No semantic tokenizer/connector in streaming mode (for API consistency)
        self.semantic_tokenizer = None
        self.semantic_connector = None

        self.post_init()

    def get_input_embeddings(self):
        return self.language_model.get_input_embeddings()

    def set_input_embeddings(self, value):
        self.language_model.set_input_embeddings(value)

    @can_return_tuple
    def forward(
        self,
        input_ids: torch.LongTensor | None = None,
        attention_mask: torch.Tensor | None = None,
        position_ids: torch.LongTensor | None = None,
        past_key_values: Cache | None = None,
        inputs_embeds: torch.FloatTensor | None = None,
        use_cache: bool | None = None,
        output_attentions: bool | None = None,
        output_hidden_states: bool | None = None,
        return_dict: bool | None = None,
        cache_position: torch.LongTensor | None = None,
        **kwargs,
    ):
        """Forward pass through the VibeVoice streaming model."""
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        # Get embeddings
        if inputs_embeds is None:
            inputs_embeds = self.get_input_embeddings()(input_ids)

        # Forward through text backbone (language_model)
        outputs = self.language_model(
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=True,
            cache_position=cache_position,
            **kwargs,
        )

        return outputs


class VibeVoiceStreamingForConditionalGeneration(VibeVoicePreTrainedModel, GenerationMixin):
    """
    VibeVoice Streaming Model for real-time text-to-speech generation.

    This variant uses interleaved windowed design for streaming with ~300ms latency.
    It does not use semantic tokenizer and supports single speaker only.

    Architecture:
    - model.language_model: text backbone (4 layers)
    - model.tts_language_model: TTS backbone (20 layers)
    - model.tts_input_types: input type embeddings
    - model.acoustic_tokenizer: decoder only (no encoder)
    - tts_eos_classifier: end-of-speech classifier (at module level)
    """

    config_class = VibeVoiceStreamingConfig
    _tied_weights_keys = {"lm_head.weight": "model.language_model.embed_tokens.weight"}

    def __init__(self, config: VibeVoiceStreamingConfig):
        super().__init__(config)

        self.model = VibeVoiceStreamingModel(config)
        self.lm_head = nn.Linear(config.decoder_config.hidden_size, config.decoder_config.vocab_size, bias=False)

        # EOS classifier at module level (not under model)
        self.tts_eos_classifier = VibeVoiceEosClassifier(config.decoder_config.hidden_size)

        # Store config values
        self.tts_backbone_num_hidden_layers = config.tts_backbone_num_hidden_layers
        self.text_backbone_num_hidden_layers = config.text_backbone_num_hidden_layers

        self.post_init()

    def get_input_embeddings(self):
        return self.model.get_input_embeddings()

    def set_input_embeddings(self, value):
        self.model.set_input_embeddings(value)

    def get_output_embeddings(self):
        return self.lm_head

    def set_output_embeddings(self, value):
        self.lm_head = value

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: torch.LongTensor | None = None,
        attention_mask: torch.Tensor | None = None,
        position_ids: torch.LongTensor | None = None,
        past_key_values: Cache | None = None,
        inputs_embeds: torch.FloatTensor | None = None,
        labels: torch.LongTensor | None = None,
        use_cache: bool | None = None,
        output_attentions: bool | None = None,
        output_hidden_states: bool | None = None,
        return_dict: bool | None = None,
        cache_position: torch.LongTensor | None = None,
        **kwargs,
    ) -> VibeVoiceCausalLMOutputWithPast:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss.
        """
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=True,
            cache_position=cache_position,
            **kwargs,
        )

        hidden_states = outputs.last_hidden_state
        logits = self.lm_head(hidden_states)

        loss = None
        if labels is not None:
            loss = self.loss_function(
                logits=logits,
                labels=labels,
                vocab_size=self.config.decoder_config.vocab_size,
                **kwargs,
            )

        return VibeVoiceCausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )

    def prepare_inputs_for_generation(
        self,
        input_ids,
        past_key_values=None,
        attention_mask=None,
        inputs_embeds=None,
        cache_position=None,
        **kwargs,
    ):
        model_inputs = super().prepare_inputs_for_generation(
            input_ids,
            past_key_values=past_key_values,
            attention_mask=attention_mask,
            inputs_embeds=inputs_embeds,
            cache_position=cache_position,
            **kwargs,
        )
        return model_inputs

    def decode_audio(self, acoustic_features: torch.Tensor) -> torch.Tensor:
        """Decode acoustic features to audio waveform."""
        return self.model.acoustic_tokenizer.decode(acoustic_features)


__all__ = [
    "VibeVoicePreTrainedModel",
    "VibeVoiceModel",
    "VibeVoiceStreamingModel",
    "VibeVoiceForConditionalGeneration",
    "VibeVoiceStreamingForConditionalGeneration",
    "VibeVoiceAcousticCodec",
    "VibeVoiceSemanticEncoder",
    "VibeVoiceDiffusionHead",
]
