#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/vibevoice/modular_vibevoice.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_vibevoice.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# Copyright 2025 Microsoft and The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from ...configuration_utils import PreTrainedConfig
from ...utils import logging
from ..auto import CONFIG_MAPPING, AutoConfig


logger = logging.get_logger(__name__)


# =============================================================================
# Configuration Classes
# =============================================================================


class VibeVoiceAcousticCodecConfig(PreTrainedConfig):
    r"""
    Configuration class for the VibeVoice Acoustic Tokenizer (Ïƒ-VAE).

    This tokenizer converts audio waveforms to continuous latent representations and back.
    It uses a mirror-symmetric encoder-decoder architecture with transformer blocks.

    Args:
        channels (`int`, *optional*, defaults to 1):
            Number of input audio channels.
        vae_dim (`int`, *optional*, defaults to 64):
            Dimensionality of the VAE latent space.
        encoder_n_filters (`int`, *optional*, defaults to 32):
            Base number of filters in the encoder.
        decoder_n_filters (`int`, *optional*, defaults to 32):
            Base number of filters in the decoder.
        encoder_ratios (`list[int]`, *optional*, defaults to `[8, 5, 5, 4, 2, 2]`):
            Downsampling ratios for each encoder stage.
        decoder_ratios (`list[int]`, *optional*, defaults to `[8, 5, 5, 4, 2, 2]`):
            Upsampling ratios for each decoder stage.
        encoder_depths (`str`, *optional*, defaults to `"3-3-3-3-3-3-8"`):
            Number of transformer blocks at each encoder stage.
        decoder_depths (`str`, *optional*):
            Number of transformer blocks at each decoder stage. If None, mirrors encoder.
        causal (`bool`, *optional*, defaults to `True`):
            Whether to use causal convolutions for streaming.
        conv_bias (`bool`, *optional*, defaults to `True`):
            Whether to use bias in convolution layers.
        conv_norm (`str`, *optional*, defaults to `"none"`):
            Type of normalization in convolutions.
        pad_mode (`str`, *optional*, defaults to `"constant"`):
            Padding mode for convolutions.
        layernorm (`str`, *optional*, defaults to `"RMSNorm"`):
            Type of layer normalization.
        layernorm_eps (`float`, *optional*, defaults to 1e-5):
            Epsilon for layer normalization.
        layernorm_elementwise_affine (`bool`, *optional*, defaults to `True`):
            Whether to use elementwise affine in layer norm.
        mixer_layer (`str`, *optional*, defaults to `"depthwise_conv"`):
            Type of mixer layer in transformer blocks.
        layer_scale_init_value (`float`, *optional*, defaults to 1e-6):
            Initial value for layer scale.
        weight_init_value (`float`, *optional*, defaults to 0.01):
            Initial value for weights.
        fix_std (`float`, *optional*, defaults to 0.5):
            Fixed standard deviation for VAE sampling.
        std_dist_type (`str`, *optional*, defaults to `"gaussian"`):
            Type of distribution for VAE ("gaussian" or "none").
        corpus_normalize (`float`, *optional*, defaults to 0.0):
            Corpus normalization factor.
        disable_last_norm (`bool`, *optional*, defaults to `True`):
            Whether to disable the last normalization layer.
    """

    model_type = "vibevoice_acoustic_codec"
    base_config_key = "acoustic_tokenizer_config"

    def __init__(
        self,
        channels: int = 1,
        vae_dim: int = 64,
        encoder_n_filters: int = 32,
        decoder_n_filters: int = 32,
        encoder_ratios: list[int] = [8, 5, 5, 4, 2, 2],
        decoder_ratios: list[int] = [8, 5, 5, 4, 2, 2],
        encoder_depths: str = "3-3-3-3-3-3-8",
        decoder_depths: str | None = None,
        causal: bool = True,
        conv_bias: bool = True,
        conv_norm: str = "none",
        pad_mode: str = "constant",
        layernorm: str = "RMSNorm",
        layernorm_eps: float = 1e-5,
        layernorm_elementwise_affine: bool = True,
        mixer_layer: str = "depthwise_conv",
        layer_scale_init_value: float = 1e-6,
        weight_init_value: float = 0.01,
        fix_std: float = 0.5,
        std_dist_type: str = "gaussian",
        corpus_normalize: float = 0.0,
        disable_last_norm: bool = True,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.channels = channels
        self.vae_dim = vae_dim
        self.encoder_n_filters = encoder_n_filters
        self.decoder_n_filters = decoder_n_filters
        self.encoder_ratios = encoder_ratios
        self.decoder_ratios = decoder_ratios
        self.encoder_depths = encoder_depths
        self.decoder_depths = decoder_depths
        self.causal = causal
        self.conv_bias = conv_bias
        self.conv_norm = conv_norm
        self.pad_mode = pad_mode
        self.layernorm = layernorm
        self.layernorm_eps = layernorm_eps
        self.layernorm_elementwise_affine = layernorm_elementwise_affine
        self.mixer_layer = mixer_layer
        self.layer_scale_init_value = layer_scale_init_value
        self.weight_init_value = weight_init_value
        self.fix_std = fix_std
        self.std_dist_type = std_dist_type
        self.corpus_normalize = corpus_normalize
        self.disable_last_norm = disable_last_norm


class VibeVoiceSemanticEncoderConfig(PreTrainedConfig):
    r"""
    Configuration class for the VibeVoice Semantic Tokenizer.

    This tokenizer extracts semantic representations from audio, trained with an ASR proxy task.
    It mirrors the acoustic tokenizer encoder architecture but without VAE components.
    Only used in the 1.5B model variant.

    Args:
        channels (`int`, *optional*, defaults to 1):
            Number of input audio channels.
        vae_dim (`int`, *optional*, defaults to 128):
            Dimensionality of the output semantic space.
        encoder_n_filters (`int`, *optional*, defaults to 32):
            Base number of filters in the encoder.
        encoder_ratios (`list[int]`, *optional*, defaults to `[8, 5, 5, 4, 2, 2]`):
            Downsampling ratios for each encoder stage.
        encoder_depths (`str`, *optional*, defaults to `"3-3-3-3-3-3-8"`):
            Number of transformer blocks at each encoder stage.
        causal (`bool`, *optional*, defaults to `True`):
            Whether to use causal convolutions.
        conv_bias (`bool`, *optional*, defaults to `True`):
            Whether to use bias in convolution layers.
        conv_norm (`str`, *optional*, defaults to `"none"`):
            Type of normalization in convolutions.
        pad_mode (`str`, *optional*, defaults to `"constant"`):
            Padding mode for convolutions.
        layernorm (`str`, *optional*, defaults to `"RMSNorm"`):
            Type of layer normalization.
        layernorm_eps (`float`, *optional*, defaults to 1e-5):
            Epsilon for layer normalization.
        layernorm_elementwise_affine (`bool`, *optional*, defaults to `True`):
            Whether to use elementwise affine in layer norm.
        mixer_layer (`str`, *optional*, defaults to `"depthwise_conv"`):
            Type of mixer layer in transformer blocks.
        layer_scale_init_value (`float`, *optional*, defaults to 1e-6):
            Initial value for layer scale.
        weight_init_value (`float`, *optional*, defaults to 0.01):
            Initial value for weights.
        fix_std (`float`, *optional*, defaults to 0):
            Fixed standard deviation (0 means no VAE sampling).
        std_dist_type (`str`, *optional*, defaults to `"none"`):
            Type of distribution ("none" for semantic tokenizer).
        corpus_normalize (`float`, *optional*, defaults to 0.0):
            Corpus normalization factor.
        disable_last_norm (`bool`, *optional*, defaults to `True`):
            Whether to disable the last normalization layer.
    """

    model_type = "vibevoice_semantic_encoder"
    base_config_key = "semantic_tokenizer_config"

    def __init__(
        self,
        channels: int = 1,
        vae_dim: int = 128,
        encoder_n_filters: int = 32,
        encoder_ratios: list[int] = [8, 5, 5, 4, 2, 2],
        encoder_depths: str = "3-3-3-3-3-3-8",
        causal: bool = True,
        conv_bias: bool = True,
        conv_norm: str = "none",
        pad_mode: str = "constant",
        layernorm: str = "RMSNorm",
        layernorm_eps: float = 1e-5,
        layernorm_elementwise_affine: bool = True,
        mixer_layer: str = "depthwise_conv",
        layer_scale_init_value: float = 1e-6,
        weight_init_value: float = 0.01,
        fix_std: float = 0,
        std_dist_type: str = "none",
        corpus_normalize: float = 0.0,
        disable_last_norm: bool = True,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.channels = channels
        self.vae_dim = vae_dim
        self.encoder_n_filters = encoder_n_filters
        self.encoder_ratios = encoder_ratios
        self.encoder_depths = encoder_depths
        self.causal = causal
        self.conv_bias = conv_bias
        self.conv_norm = conv_norm
        self.pad_mode = pad_mode
        self.layernorm = layernorm
        self.layernorm_eps = layernorm_eps
        self.layernorm_elementwise_affine = layernorm_elementwise_affine
        self.mixer_layer = mixer_layer
        self.layer_scale_init_value = layer_scale_init_value
        self.weight_init_value = weight_init_value
        self.fix_std = fix_std
        self.std_dist_type = std_dist_type
        self.corpus_normalize = corpus_normalize
        self.disable_last_norm = disable_last_norm


class VibeVoiceDiffusionHeadConfig(PreTrainedConfig):
    r"""
    Configuration class for the VibeVoice Diffusion Head (Prediction Head).

    This module predicts acoustic VAE features conditioned on LLM hidden states
    using Denoising Diffusion Probabilistic Models (DDPM).

    Args:
        hidden_size (`int`, *optional*, defaults to 1536):
            Hidden size of the transformer layers (should match LLM hidden size).
        latent_size (`int`, *optional*, defaults to 64):
            Size of the acoustic latent space.
        speech_vae_dim (`int`, *optional*, defaults to 64):
            Dimensionality of the speech VAE (should match acoustic tokenizer vae_dim).
        head_layers (`int`, *optional*, defaults to 4):
            Number of transformer layers in the diffusion head.
        head_ffn_ratio (`float`, *optional*, defaults to 3.0):
            Ratio of FFN hidden size to hidden size.
        rms_norm_eps (`float`, *optional*, defaults to 1e-5):
            Epsilon for RMS normalization.
        diffusion_type (`str`, *optional*, defaults to `"ddpm"`):
            Type of diffusion process.
        ddpm_num_steps (`int`, *optional*, defaults to 1000):
            Number of diffusion steps during training.
        ddpm_num_inference_steps (`int`, *optional*, defaults to 20):
            Number of diffusion steps during inference.
        ddpm_beta_schedule (`str`, *optional*, defaults to `"cosine"`):
            Beta schedule for DDPM.
        ddpm_batch_mul (`int`, *optional*, defaults to 4):
            Batch multiplier for DDPM training.
        prediction_type (`str`, *optional*, defaults to `"v_prediction"`):
            Type of prediction ("v_prediction" or "epsilon").
    """

    model_type = "vibevoice_diffusion_head"
    base_config_key = "diffusion_head_config"

    def __init__(
        self,
        hidden_size: int = 1536,
        latent_size: int = 64,
        speech_vae_dim: int = 64,
        head_layers: int = 4,
        head_ffn_ratio: float = 3.0,
        rms_norm_eps: float = 1e-5,
        diffusion_type: str = "ddpm",
        ddpm_num_steps: int = 1000,
        ddpm_num_inference_steps: int = 20,
        ddpm_beta_schedule: str = "cosine",
        ddpm_batch_mul: int = 4,
        prediction_type: str = "v_prediction",
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.hidden_size = hidden_size
        self.latent_size = latent_size
        self.speech_vae_dim = speech_vae_dim
        self.head_layers = head_layers
        self.head_ffn_ratio = head_ffn_ratio
        self.rms_norm_eps = rms_norm_eps
        self.diffusion_type = diffusion_type
        self.ddpm_num_steps = ddpm_num_steps
        self.ddpm_num_inference_steps = ddpm_num_inference_steps
        self.ddpm_beta_schedule = ddpm_beta_schedule
        self.ddpm_batch_mul = ddpm_batch_mul
        self.prediction_type = prediction_type


class VibeVoiceConfig(PreTrainedConfig):
    r"""
    Configuration class for VibeVoice models.

    This is the main configuration class that combines all sub-configurations for the
    VibeVoice text-to-speech model, e.g. [microsoft/VibeVoice-1.5B](https://huggingface.co/microsoft/VibeVoice-1.5B).
    It supports both the 1.5B long-form model and the 0.5B real-time streaming variant.

    Args:
        decoder_config (`dict` or `PreTrainedConfig`, *optional*):
            Configuration for the decoder LLM (Qwen2/Qwen2.5).
        acoustic_tokenizer_config (`dict` or `VibeVoiceAcousticCodecConfig`, *optional*):
            Configuration for the acoustic tokenizer.
        semantic_tokenizer_config (`dict` or `VibeVoiceSemanticEncoderConfig`, *optional*):
            Configuration for the semantic tokenizer. If None, the model operates in
            streaming/real-time mode without semantic tokens.
        diffusion_head_config (`dict` or `VibeVoiceDiffusionHeadConfig`, *optional*):
            Configuration for the diffusion head.
        acoustic_vae_dim (`int`, *optional*, defaults to 64):
            Dimensionality of the acoustic VAE latent space.
        semantic_vae_dim (`int`, *optional*, defaults to 128):
            Dimensionality of the semantic VAE latent space.
        tts_backbone_num_hidden_layers (`int`, *optional*):
            Number of hidden layers to use for TTS backbone in streaming mode.
            If None, uses all layers from decoder_config.
        sampling_rate (`int`, *optional*, defaults to 24000):
            Audio sampling rate in Hz.
        num_speakers (`int`, *optional*, defaults to 4):
            Maximum number of speakers supported.

    Example:
        ```python
        >>> from transformers import VibeVoiceConfig, VibeVoiceForConditionalGeneration

        >>> # Initialize a VibeVoice configuration
        >>> configuration = VibeVoiceConfig()

        >>> # Initialize a model from the configuration
        >>> model = VibeVoiceForConditionalGeneration(configuration)

        >>> # Accessing the model configuration
        >>> configuration = model.config
        ```
    """

    model_type = "vibevoice"
    sub_configs = {
        "decoder_config": AutoConfig,
        "acoustic_tokenizer_config": VibeVoiceAcousticCodecConfig,
        "semantic_tokenizer_config": VibeVoiceSemanticEncoderConfig,
        "diffusion_head_config": VibeVoiceDiffusionHeadConfig,
    }

    def __init__(
        self,
        decoder_config: dict | None = None,
        acoustic_tokenizer_config: dict | None = None,
        semantic_tokenizer_config: dict | None = None,
        diffusion_head_config: dict | None = None,
        acoustic_vae_dim: int = 64,
        semantic_vae_dim: int = 128,
        tts_backbone_num_hidden_layers: int | None = None,
        sampling_rate: int = 24000,
        num_speakers: int = 4,
        **kwargs,
    ):
        super().__init__(**kwargs)

        # Decoder config (Qwen2)
        if decoder_config is None:
            logger.info("`decoder_config` is None. Initializing with default Qwen2 values.")
            self.decoder_config = CONFIG_MAPPING["qwen2"]()
        elif isinstance(decoder_config, dict):
            model_type = decoder_config.get("model_type", "qwen2")
            self.decoder_config = CONFIG_MAPPING[model_type](**decoder_config)
        elif isinstance(decoder_config, PreTrainedConfig):
            self.decoder_config = decoder_config
        else:
            raise ValueError(
                f"Invalid type for `decoder_config`. Must be either `dict` or `PreTrainedConfig`. "
                f"Type found: {type(decoder_config)}"
            )

        # Acoustic tokenizer config
        if acoustic_tokenizer_config is None:
            logger.info("`acoustic_tokenizer_config` is None. Initializing with default values.")
            self.acoustic_tokenizer_config = VibeVoiceAcousticCodecConfig()
        elif isinstance(acoustic_tokenizer_config, dict):
            self.acoustic_tokenizer_config = VibeVoiceAcousticCodecConfig(**acoustic_tokenizer_config)
        elif isinstance(acoustic_tokenizer_config, VibeVoiceAcousticCodecConfig):
            self.acoustic_tokenizer_config = acoustic_tokenizer_config
        else:
            raise ValueError(
                f"Invalid type for `acoustic_tokenizer_config`. Must be either `dict` or "
                f"`VibeVoiceAcousticCodecConfig`. Type found: {type(acoustic_tokenizer_config)}"
            )

        # Semantic tokenizer config (optional - None for streaming mode)
        if semantic_tokenizer_config is None:
            self.semantic_tokenizer_config = None
        elif isinstance(semantic_tokenizer_config, dict):
            self.semantic_tokenizer_config = VibeVoiceSemanticEncoderConfig(**semantic_tokenizer_config)
        elif isinstance(semantic_tokenizer_config, VibeVoiceSemanticEncoderConfig):
            self.semantic_tokenizer_config = semantic_tokenizer_config
        else:
            raise ValueError(
                f"Invalid type for `semantic_tokenizer_config`. Must be either `dict`, "
                f"`VibeVoiceSemanticEncoderConfig`, or `None`. Type found: {type(semantic_tokenizer_config)}"
            )

        # Diffusion head config
        if diffusion_head_config is None:
            logger.info("`diffusion_head_config` is None. Initializing with default values.")
            self.diffusion_head_config = VibeVoiceDiffusionHeadConfig(
                hidden_size=self.decoder_config.hidden_size,
                speech_vae_dim=acoustic_vae_dim,
            )
        elif isinstance(diffusion_head_config, dict):
            self.diffusion_head_config = VibeVoiceDiffusionHeadConfig(**diffusion_head_config)
        elif isinstance(diffusion_head_config, VibeVoiceDiffusionHeadConfig):
            self.diffusion_head_config = diffusion_head_config
        else:
            raise ValueError(
                f"Invalid type for `diffusion_head_config`. Must be either `dict` or "
                f"`VibeVoiceDiffusionHeadConfig`. Type found: {type(diffusion_head_config)}"
            )

        self.acoustic_vae_dim = acoustic_vae_dim
        self.semantic_vae_dim = semantic_vae_dim
        self.tts_backbone_num_hidden_layers = tts_backbone_num_hidden_layers
        self.sampling_rate = sampling_rate
        self.num_speakers = num_speakers

    @property
    def has_semantic_tokenizer(self) -> bool:
        """Whether this model has a semantic tokenizer (True for 1.5B, False for streaming)."""
        return self.semantic_tokenizer_config is not None

    @property
    def is_streaming(self) -> bool:
        """Whether this model is a streaming/real-time variant."""
        return not self.has_semantic_tokenizer


class VibeVoiceStreamingConfig(VibeVoiceConfig):
    """Configuration class for VibeVoice Streaming (real-time) model variant,
    e.g. [microsoft/VibeVoice-Realtime-0.5B](https://huggingface.co/microsoft/VibeVoice-Realtime-0.5B).

    The streaming model splits the decoder into two parts:
    - text backbone (language_model): first `text_backbone_num_hidden_layers` layers
    - TTS backbone (tts_language_model): remaining `tts_backbone_num_hidden_layers` layers

    It also includes a tts_eos_classifier and tts_input_types embedding.
    The acoustic tokenizer only has decoder (no encoder) for streaming.
    """

    model_type = "vibevoice_streaming"

    def __init__(
        self,
        decoder_config: dict | None = None,
        acoustic_tokenizer_config: dict | None = None,
        diffusion_head_config: dict | None = None,
        acoustic_vae_dim: int = 64,
        tts_backbone_num_hidden_layers: int = 20,
        text_backbone_num_hidden_layers: int | None = None,
        num_tts_input_types: int = 2,
        sampling_rate: int = 24000,
        **kwargs,
    ):
        # Pop values that we explicitly set to avoid duplicate keyword argument errors
        # when loading from a saved config
        kwargs.pop("semantic_tokenizer_config", None)
        kwargs.pop("semantic_vae_dim", None)
        kwargs.pop("num_speakers", None)

        # Streaming model has no semantic tokenizer
        super().__init__(
            decoder_config=decoder_config,
            acoustic_tokenizer_config=acoustic_tokenizer_config,
            semantic_tokenizer_config=None,  # No semantic tokenizer in streaming mode
            diffusion_head_config=diffusion_head_config,
            acoustic_vae_dim=acoustic_vae_dim,
            semantic_vae_dim=0,
            tts_backbone_num_hidden_layers=tts_backbone_num_hidden_layers,
            sampling_rate=sampling_rate,
            num_speakers=1,  # Streaming mode supports single speaker only
            **kwargs,
        )

        # Compute text_backbone_num_hidden_layers if not provided
        if text_backbone_num_hidden_layers is None:
            total_layers = self.decoder_config.num_hidden_layers
            text_backbone_num_hidden_layers = total_layers - tts_backbone_num_hidden_layers
        self.text_backbone_num_hidden_layers = text_backbone_num_hidden_layers
        self.num_tts_input_types = num_tts_input_types


__all__ = [
    "VibeVoiceConfig",
    "VibeVoiceStreamingConfig",
    "VibeVoiceAcousticCodecConfig",
    "VibeVoiceSemanticEncoderConfig",
    "VibeVoiceDiffusionHeadConfig",
]
