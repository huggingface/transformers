#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/hrm/modular_hrm.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_hrm.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# coding=utf-8
# Copyright 2025 Sapient Inc. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import math
from dataclasses import dataclass
from typing import Optional

import torch
import torch.nn.functional as F
from torch import nn

from ...generation import GenerationMixin
from ...modeling_flash_attention_utils import _flash_attention_forward
from ...modeling_outputs import ModelOutput
from ...modeling_utils import PreTrainedModel
from ...utils import auto_docstring
from .configuration_hrm import HrmConfig


@dataclass
class HrmModelOutput(ModelOutput):
    """
    Output type of [`HrmModel`].

    Args:
        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, vocab_size)`):
            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).
        q_halt_logits (`torch.FloatTensor` of shape `(batch_size,)`):
            Q-values for halting in the Adaptive Computation Time mechanism.
        q_continue_logits (`torch.FloatTensor` of shape `(batch_size,)`):
            Q-values for continuing in the Adaptive Computation Time mechanism.
        carry (`HrmState`, *optional*):
            Model state for recurrent computation, containing hidden states and halting information.
        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or
            when `config.output_hidden_states=True`):
            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of
            shape `(batch_size, sequence_length, hidden_size)`.
        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or
            when `config.output_attentions=True`):
            Tuple of `torch.FloatTensor` (one for each layer) of shape
            `(batch_size, num_heads, sequence_length, sequence_length)`.
    """

    logits: torch.FloatTensor = None
    q_halt_logits: torch.FloatTensor = None
    q_continue_logits: torch.FloatTensor = None
    carry: Optional["HrmState"] = None
    hidden_states: Optional[tuple[torch.FloatTensor]] = None
    attentions: Optional[tuple[torch.FloatTensor]] = None


@dataclass
class HrmCausalLMOutput(ModelOutput):
    """
    Output type of [`HrmForCausalLM`].

    Args:
        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):
            Language modeling loss (for next-token prediction).
        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, vocab_size)`):
            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).
        q_halt_logits (`torch.FloatTensor` of shape `(batch_size,)`):
            Q-values for halting in the Adaptive Computation Time mechanism.
        q_continue_logits (`torch.FloatTensor` of shape `(batch_size,)`):
            Q-values for continuing in the Adaptive Computation Time mechanism.
        carry (`HrmState`, *optional*):
            Model state for recurrent computation, containing hidden states and halting information.
        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or
            when `config.output_hidden_states=True`):
            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of
            shape `(batch_size, sequence_length, hidden_size)`.
        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or
            when `config.output_attentions=True`):
            Tuple of `torch.FloatTensor` (one for each layer) of shape
            `(batch_size, num_heads, sequence_length, sequence_length)`.
    """

    loss: Optional[torch.FloatTensor] = None
    logits: torch.FloatTensor = None
    q_halt_logits: torch.FloatTensor = None
    q_continue_logits: torch.FloatTensor = None
    carry: Optional["HrmState"] = None
    hidden_states: Optional[tuple[torch.FloatTensor]] = None
    attentions: Optional[tuple[torch.FloatTensor]] = None


def truncated_normal_init_(
    tensor: torch.Tensor, std: float = 1.0, lower: float = -2.0, upper: float = 2.0
) -> torch.Tensor:
    """Initialize tensor with truncated normal distribution (JAX-style)."""
    with torch.no_grad():
        if std == 0:
            tensor.zero_()
        else:
            sqrt2 = math.sqrt(2)
            a = math.erf(lower / sqrt2)
            b = math.erf(upper / sqrt2)
            z = (b - a) / 2
            c = (2 * math.pi) ** -0.5
            pdf_u = c * math.exp(-0.5 * lower**2)
            pdf_l = c * math.exp(-0.5 * upper**2)
            comp_std = std / math.sqrt(1 - (upper * pdf_u - lower * pdf_l) / z - ((pdf_u - pdf_l) / z) ** 2)
            tensor.uniform_(a, b)
            tensor.erfinv_()
            tensor.mul_(sqrt2 * comp_std)
            tensor.clip_(lower * comp_std, upper * comp_std)
    return tensor


class HrmLinear(nn.Module):
    """
    Linear layer with automatic type casting for mixed-precision training.

    Note: Cannot inherit from nn.Linear due to custom truncated normal initialization
    that is critical to HRM's training dynamics. This custom initialization differs from
    standard PyTorch initialization and is applied during __init__.
    """

    def __init__(self, in_features: int, out_features: int, bias: bool = False):
        super().__init__()
        self.weight = nn.Parameter(
            truncated_normal_init_(torch.empty((out_features, in_features)), std=1.0 / (in_features**0.5))
        )
        self.bias = None
        if bias:
            self.bias = nn.Parameter(torch.zeros((out_features,)))

    def forward(self, input: torch.Tensor) -> torch.Tensor:
        return F.linear(
            input,
            self.weight.to(input.dtype),
            bias=self.bias.to(input.dtype) if self.bias is not None else None,
        )


class HrmEmbedding(nn.Embedding):
    """Embedding layer with type casting for mixed-precision training."""

    def __init__(self, num_embeddings: int, embedding_dim: int, init_std: float, cast_to: torch.dtype):
        # Initialize parent without calling its __init__ to avoid double initialization
        nn.Module.__init__(self)
        self.num_embeddings = num_embeddings
        self.embedding_dim = embedding_dim
        self.padding_idx = None
        self.max_norm = None
        self.norm_type = 2.0
        self.scale_grad_by_freq = False
        self.sparse = False
        self.cast_to = cast_to
        self.weight = nn.Parameter(truncated_normal_init_(torch.empty((num_embeddings, embedding_dim)), std=init_std))

    def forward(self, input: torch.Tensor) -> torch.Tensor:
        return F.embedding(input, self.weight.to(self.cast_to))


class HrmRotaryEmbedding(nn.Module):
    """
    Rotary Position Embedding (RoPE) precomputation.

    Note: Does not inherit from LlamaRotaryEmbedding because HRM uses a simpler approach.
    HRM precomputes cos/sin for all positions at initialization and doesn't need:
    - Dynamic rope scaling (rope_scaling config)
    - Position-dependent forward passes (position_ids parameter)
    - Rope type variations (default/linear/dynamic/yarn/longrope)
    This simpler design is sufficient for HRM's fixed-length reasoning tasks.
    """

    def __init__(self, dim=None, max_position_embeddings=None, base=10000.0, device=None, config=None):
        super().__init__()
        # Support both direct parameters and config-based initialization
        if config is not None:
            dim = config.hidden_size // config.num_attention_heads
            max_position_embeddings = config.max_position_embeddings
            base = config.rope_theta

        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.float32, device=device) / dim))
        t = torch.arange(max_position_embeddings, dtype=torch.float32, device=device)
        freqs = torch.outer(t, inv_freq)
        emb = torch.cat((freqs, freqs), dim=-1)
        self.register_buffer("cos_cached", emb.cos(), persistent=False)
        self.register_buffer("sin_cached", emb.sin(), persistent=False)

    def forward(self, position_ids=None) -> tuple[torch.Tensor, torch.Tensor]:
        """Forward pass - position_ids parameter is accepted for compatibility but not used."""
        return self.cos_cached, self.sin_cached


def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    """Applies Rotary Position Embedding to the query and key tensors.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`, *optional*):
            Deprecated and unused.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """
    cos = cos.unsqueeze(unsqueeze_dim)
    sin = sin.unsqueeze(unsqueeze_dim)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


class HrmAttention(nn.Module):
    """
    Multi-head self-attention with FlashAttention and eager fallback.

    Note: Does not inherit from LlamaAttention due to architectural differences:
    - Uses fused QKV projection (single linear layer) instead of separate Q, K, V projections
    - Custom truncated normal initialization via HrmLinear
    - Simpler interface without cache_position, past_key_values, or output_attentions
    These differences make inheritance from LlamaAttention impractical while still reusing
    the core rope functions (apply_rotary_pos_emb, rotate_half).
    """

    def __init__(self, config: HrmConfig, causal: bool = False):
        super().__init__()
        self.hidden_size = config.hidden_size
        self.head_dim = config.hidden_size // config.num_attention_heads
        self.output_size = self.head_dim * config.num_attention_heads
        self.num_heads = config.num_attention_heads
        self.num_key_value_heads = config.num_attention_heads
        self.causal = causal

        self.qkv_proj = HrmLinear(
            self.hidden_size,
            (self.num_heads + 2 * self.num_key_value_heads) * self.head_dim,
            bias=False,
        )
        self.o_proj = HrmLinear(self.output_size, self.hidden_size, bias=False)

    def _eager_attention(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -> torch.Tensor:
        """Pure PyTorch fallback attention when FlashAttention is not available."""
        batch, seq_len, num_heads, head_dim = query.shape
        # Transpose to (batch, num_heads, seq_len, head_dim)
        query = query.transpose(1, 2)
        key = key.transpose(1, 2)
        value = value.transpose(1, 2)

        # Compute attention scores
        scores = torch.matmul(query, key.transpose(-2, -1)) / (head_dim**0.5)

        # Apply causal mask if needed
        if self.causal:
            mask = torch.triu(torch.ones(seq_len, seq_len, device=query.device, dtype=torch.bool), diagonal=1)
            scores = scores.masked_fill(mask, float("-inf"))

        # Softmax and apply to values
        attn = torch.softmax(scores, dim=-1)
        output = torch.matmul(attn, value)

        # Transpose back to (batch, seq_len, num_heads, head_dim)
        return output.transpose(1, 2).contiguous()

    def forward(
        self, hidden_states: torch.Tensor, cos_sin: Optional[tuple[torch.Tensor, torch.Tensor]] = None
    ) -> torch.Tensor:
        batch_size, seq_len, _ = hidden_states.shape
        qkv = self.qkv_proj(hidden_states)
        qkv = qkv.view(batch_size, seq_len, self.num_heads + 2 * self.num_key_value_heads, self.head_dim)
        query = qkv[:, :, : self.num_heads]
        key = qkv[:, :, self.num_heads : self.num_heads + self.num_key_value_heads]
        value = qkv[:, :, self.num_heads + self.num_key_value_heads :]

        if cos_sin is not None:
            cos, sin = cos_sin
            # Slice RoPE embeddings to match actual sequence length
            cos = cos[:seq_len, :].unsqueeze(0)  # Add batch dimension: [1, seq_len, head_dim]
            sin = sin[:seq_len, :].unsqueeze(0)  # Add batch dimension: [1, seq_len, head_dim]
            # HRM uses (batch, seq_len, heads, head_dim) so unsqueeze_dim=2
            query, key = apply_rotary_pos_emb(query, key, cos, sin, unsqueeze_dim=2)

        # Try FlashAttention first, fall back to eager if not available
        try:
            attn_output = _flash_attention_forward(
                query_states=query,
                key_states=key,
                value_states=value,
                attention_mask=None,  # HRM doesn't use attention masks
                query_length=seq_len,
                is_causal=self.causal,
                dropout=0.0,  # HRM doesn't use attention dropout
            )
        except (ValueError, ImportError):
            # Fall back to eager attention if Flash Attention is not available
            attn_output = self._eager_attention(query, key, value)

        attn_output = attn_output.view(batch_size, seq_len, self.output_size)
        return self.o_proj(attn_output)


class HrmSwiGLU(nn.Module):
    """SwiGLU feed-forward network layer."""

    def __init__(self, config: HrmConfig):
        super().__init__()
        # Use intermediate_size if provided, otherwise calculate from expansion
        if config.intermediate_size is not None:
            inter = config.intermediate_size
        else:
            inter = int(config.expansion * config.hidden_size * 2 / 3)
            # Round up to multiple of 256 for efficiency
            inter = ((inter + 255) // 256) * 256
        self.gate_up_proj = HrmLinear(config.hidden_size, inter * 2, bias=False)
        self.down_proj = HrmLinear(inter, config.hidden_size, bias=False)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        gate, up = self.gate_up_proj(x).chunk(2, dim=-1)
        return self.down_proj(F.silu(gate) * up)


def rms_forward(hidden_states, variance_epsilon=1e-6):
    """
    Calculates simple RMSNorm with no learnable weights. `MambaRMSNorm` will
    leverage this in order to multiply the final result with the RMSNorm weight

    Args:
        hidden_states (`torch.Tensor`):
            Hidden states to normalize
        variance_epsilon (`float`):
            The eps value to add in the square root scaling factor
    """
    input_dtype = hidden_states.dtype
    hidden_states = hidden_states.to(torch.float32)

    variance = hidden_states.pow(2).mean(-1, keepdim=True)
    hidden_states = hidden_states * torch.rsqrt(variance + variance_epsilon)
    return hidden_states.to(input_dtype)


class HrmBlock(nn.Module):
    """Single transformer block with post-normalization."""

    def __init__(self, config: HrmConfig):
        super().__init__()
        self.self_attn = HrmAttention(config, causal=False)
        self.mlp = HrmSwiGLU(config)
        self.norm_eps = config.rms_norm_eps

    def forward(
        self, hidden_states: torch.Tensor, cos_sin: Optional[tuple[torch.Tensor, torch.Tensor]] = None
    ) -> torch.Tensor:
        # Post-normalization
        hidden_states = rms_forward(
            hidden_states + self.self_attn(hidden_states, cos_sin=cos_sin), variance_epsilon=self.norm_eps
        )
        hidden_states = rms_forward(hidden_states + self.mlp(hidden_states), variance_epsilon=self.norm_eps)
        return hidden_states


class HrmReasoningModule(nn.Module):
    """Reasoning module for H-level or L-level processing."""

    def __init__(self, layers):
        super().__init__()
        self.layers = nn.ModuleList(layers)

    def forward(
        self,
        hidden_states: torch.Tensor,
        input_injection: torch.Tensor,
        cos_sin: Optional[tuple[torch.Tensor, torch.Tensor]] = None,
    ) -> torch.Tensor:
        # Input injection (add)
        hidden_states = hidden_states + input_injection
        # Layers
        for layer in self.layers:
            hidden_states = layer(hidden_states, cos_sin=cos_sin)
        return hidden_states


@dataclass
class HrmInnerState:
    """Internal state for the hierarchical reasoning modules."""

    high_level_state: torch.Tensor  # High-level hidden state for abstract reasoning
    low_level_state: torch.Tensor  # Low-level hidden state for detailed computations


@dataclass
class HrmState:
    """Complete state for HRM with ACT mechanism."""

    inner_state: HrmInnerState
    steps: torch.Tensor
    halted: torch.Tensor
    current_data: dict


class HrmInner(nn.Module):
    """Core inner model implementing hierarchical reasoning with dual recurrence."""

    def __init__(self, config: HrmConfig):
        super().__init__()
        self.config = config
        self.forward_dtype = (
            getattr(torch, config.torch_dtype) if isinstance(config.torch_dtype, str) else config.torch_dtype
        )

        # Embeddings
        self.embed_scale = math.sqrt(config.hidden_size)
        embed_init_std = 1.0 / self.embed_scale
        self.embed_tokens = HrmEmbedding(config.vocab_size, config.hidden_size, embed_init_std, self.forward_dtype)

        # Output heads
        self.lm_head = HrmLinear(config.hidden_size, config.vocab_size, bias=False)
        self.q_head = HrmLinear(config.hidden_size, 2, bias=True)

        # Puzzle embeddings (sparse, optional)
        self.puzzle_emb_len = -(config.puzzle_emb_ndim // -config.hidden_size)  # ceil div
        if config.puzzle_emb_ndim > 0:
            # Simplified version without CastedSparseEmbedding
            self.puzzle_emb = nn.Embedding(config.num_puzzle_identifiers, config.puzzle_emb_ndim)
            nn.init.zeros_(self.puzzle_emb.weight)

        # Positional encodings - always initialize appropriate type based on config
        self.pos_encodings = config.pos_encodings
        if self.pos_encodings == "rope":
            self.rotary_emb = HrmRotaryEmbedding(
                dim=config.hidden_size // config.num_attention_heads,
                max_position_embeddings=config.max_position_embeddings + self.puzzle_emb_len,
                base=config.rope_theta,
            )
        elif self.pos_encodings == "learned":
            self.embed_pos = HrmEmbedding(
                config.max_position_embeddings + self.puzzle_emb_len,
                config.hidden_size,
                embed_init_std,
                self.forward_dtype,
            )
        else:
            raise ValueError(f"Unknown pos_encodings: {config.pos_encodings}")

        # Reasoning modules
        self.high_level_module = HrmReasoningModule(layers=[HrmBlock(config) for _ in range(config.h_layers)])
        self.low_level_module = HrmReasoningModule(layers=[HrmBlock(config) for _ in range(config.l_layers)])

        # Initial states
        self.register_buffer(
            "high_level_init_state",
            truncated_normal_init_(torch.empty(config.hidden_size, dtype=self.forward_dtype), std=1),
            persistent=True,
        )
        self.register_buffer(
            "low_level_init_state",
            truncated_normal_init_(torch.empty(config.hidden_size, dtype=self.forward_dtype), std=1),
            persistent=True,
        )

    def _get_rotary_embeddings(self) -> Optional[tuple[torch.Tensor, torch.Tensor]]:
        """Get rotary embeddings if using RoPE, otherwise return None."""
        if self.pos_encodings == "rope":
            return self.rotary_emb()
        return None

    def _input_embeddings(
        self, input: torch.Tensor, puzzle_identifiers: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """Compute input embeddings with token, puzzle, and position encoding."""
        embedding = self.embed_tokens(input.to(torch.int32))

        # Puzzle embeddings
        if self.config.puzzle_emb_ndim > 0 and puzzle_identifiers is not None:
            puzzle_embedding = self.puzzle_emb(puzzle_identifiers)
            pad_count = self.puzzle_emb_len * self.config.hidden_size - puzzle_embedding.shape[-1]
            if pad_count > 0:
                puzzle_embedding = F.pad(puzzle_embedding, (0, pad_count))
            embedding = torch.cat(
                (puzzle_embedding.view(-1, self.puzzle_emb_len, self.config.hidden_size), embedding), dim=-2
            )

        # Position embeddings
        if self.config.pos_encodings == "learned":
            # Scale factor is 1/sqrt(2) to maintain variance after adding two embeddings
            embedding = math.sqrt(0.5) * (embedding + self.embed_pos.weight.to(self.forward_dtype))

        return self.embed_scale * embedding

    def empty_state(self, batch_size: int, sequence_length: int, device: torch.device) -> HrmInnerState:
        """Create uninitialized state tensors.

        Args:
            batch_size (int): Batch size
            sequence_length (int): Sequence length (without puzzle embedding positions)
            device (torch.device): Device to create tensors on

        Returns:
            HrmInnerState: Uninitialized inner state
        """
        return HrmInnerState(
            high_level_state=torch.empty(
                batch_size,
                sequence_length + self.puzzle_emb_len,
                self.config.hidden_size,
                dtype=self.forward_dtype,
                device=device,
            ),
            low_level_state=torch.empty(
                batch_size,
                sequence_length + self.puzzle_emb_len,
                self.config.hidden_size,
                dtype=self.forward_dtype,
                device=device,
            ),
        )

    def reset_state(self, reset_flag: torch.Tensor, state: HrmInnerState) -> HrmInnerState:
        """Reset state to learned initial values based on flags."""
        device = state.high_level_state.device
        high_init = self.high_level_init_state.to(device)
        low_init = self.low_level_init_state.to(device)
        return HrmInnerState(
            high_level_state=torch.where(reset_flag.view(-1, 1, 1), high_init, state.high_level_state),
            low_level_state=torch.where(reset_flag.view(-1, 1, 1), low_init, state.low_level_state),
        )

    def forward(
        self, state: HrmInnerState, batch: dict
    ) -> tuple[HrmInnerState, torch.Tensor, tuple[torch.Tensor, torch.Tensor]]:
        """Execute hierarchical reasoning forward pass with 1-step gradient."""
        # Get rotary embeddings if using RoPE (None for learned positional embeddings)
        cos_sin = self._get_rotary_embeddings()

        # Input encoding
        input_embeddings = self._input_embeddings(batch["input_ids"], batch.get("puzzle_identifiers"))

        # Forward iterations without gradients (for computational efficiency)
        with torch.no_grad():
            high = state.high_level_state
            low = state.low_level_state

            for high_cycle_idx in range(self.config.h_cycles):
                for low_cycle_idx in range(self.config.l_cycles):
                    # Skip the last L-level update (will be done with gradients)
                    is_last_cycle = (high_cycle_idx == self.config.h_cycles - 1) and (
                        low_cycle_idx == self.config.l_cycles - 1
                    )
                    if not is_last_cycle:
                        low = self.low_level_module(low, high + input_embeddings, cos_sin=cos_sin)
                # Skip the last H-level update (will be done with gradients)
                if high_cycle_idx != self.config.h_cycles - 1:
                    high = self.high_level_module(high, low, cos_sin=cos_sin)

        # Final iteration with 1-step gradient for backpropagation
        low = self.low_level_module(low, high + input_embeddings, cos_sin=cos_sin)
        high = self.high_level_module(high, low, cos_sin=cos_sin)

        # Prepare outputs
        new_state = HrmInnerState(high_level_state=high.detach(), low_level_state=low.detach())
        output = self.lm_head(high)[:, self.puzzle_emb_len :]
        q_logits = self.q_head(high[:, 0]).to(torch.float32)

        return new_state, output, (q_logits[..., 0], q_logits[..., 1])


class HrmPreTrainedModel(PreTrainedModel):
    """
    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained
    models.
    """

    config_class = HrmConfig
    base_model_prefix = "hrm"
    supports_gradient_checkpointing = False
    _no_split_modules = ["HrmBlock"]
    _skip_keys_device_placement = ["high_level_init_state", "low_level_init_state"]

    def _init_weights(self, module):
        """Initialize the weights using truncated normal initialization."""
        std = self.config.initializer_range
        if isinstance(module, HrmLinear):
            # Special initialization for Q head (ACT halting mechanism)
            if hasattr(module, "weight") and module.weight.shape == (2, self.config.hidden_size):
                # This is the q_head: zero weights and strong negative bias for ACT
                module.weight.data.zero_()
                if module.bias is not None:
                    module.bias.data.fill_(-5.0)
            else:
                # Standard HrmLinear initialization
                truncated_normal_init_(module.weight, std=std / (module.weight.shape[1] ** 0.5))
                if module.bias is not None:
                    module.bias.data.zero_()
        elif isinstance(module, HrmEmbedding):
            # Reinitialize HrmEmbedding weights
            truncated_normal_init_(module.weight, std=std)
        elif isinstance(module, nn.Embedding):
            # For puzzle embeddings
            module.weight.data.normal_(mean=0.0, std=std)
        elif isinstance(module, nn.Linear):
            # For any standard linear layers
            module.weight.data.normal_(mean=0.0, std=std)
            if module.bias is not None:
                module.bias.data.zero_()


class HrmModel(HrmPreTrainedModel):
    """Hierarchical Reasoning Model with Adaptive Computation Time."""

    def __init__(self, config: HrmConfig):
        super().__init__(config)
        self.config = config
        self.inner = HrmInner(config)
        self.post_init()

    def get_input_embeddings(self):
        return self.inner.embed_tokens

    def set_input_embeddings(self, value):
        self.inner.embed_tokens = value

    def initial_state(self, batch: dict) -> HrmState:
        """Initialize state for a new batch."""
        batch_size = batch["input_ids"].shape[0]
        sequence_length = batch["input_ids"].shape[1]
        device = batch["input_ids"].device
        return HrmState(
            inner_state=self.inner.empty_state(batch_size, sequence_length, device),
            steps=torch.zeros((batch_size,), dtype=torch.int32, device=device),
            halted=torch.ones((batch_size,), dtype=torch.bool, device=device),
            current_data={k: torch.empty_like(v) for k, v in batch.items()},
        )

    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        puzzle_identifiers: Optional[torch.LongTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        state: Optional[HrmState] = None,
        output_hidden_states: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        **kwargs,
    ):
        """Execute one ACT step with halting logic."""
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions

        batch = {"input_ids": input_ids}
        if puzzle_identifiers is not None:
            batch["puzzle_identifiers"] = puzzle_identifiers
        if labels is not None:
            batch["labels"] = labels

        # Initialize state if not provided
        if state is None:
            state = self.initial_state(batch)

        # Update data and state
        new_inner_state = self.inner.reset_state(state.halted, state.inner_state)
        new_steps = torch.where(state.halted, torch.zeros_like(state.steps), state.steps)

        # Handle size mismatch during generation - update state data with batch data
        # Use scatter operations for efficient batch-wise updates
        new_current_data = {}

        # Separate keys into common and state-only for batch processing
        common_keys = [k for k in state.current_data.keys() if k in batch]
        state_only_keys = [k for k in state.current_data.keys() if k not in batch]

        # Get halted indices once for reuse across all keys
        halted_indices = state.halted.nonzero(as_tuple=False).squeeze(-1)

        # Process common keys using scatter operations
        for k in common_keys:
            batch_val = batch[k]
            state_val = state.current_data[k]

            # During generation, input_ids may be a single token while state has full sequence
            # Pad batch to match state size in this expected case
            if batch_val.shape[1] < state_val.shape[1]:
                pad_size = state_val.shape[1] - batch_val.shape[1]
                batch_val = F.pad(batch_val, (0, pad_size), value=0)

            # Use scatter operation to update halted sequences
            # Clone state and scatter batch values at halted positions (dim 0 is batch dimension)
            result = state_val.clone()
            if halted_indices.numel() > 0:
                result.index_copy_(0, halted_indices, batch_val.index_select(0, halted_indices))

            new_current_data[k] = result

        # Keep existing state data for keys not in current batch
        for k in state_only_keys:
            new_current_data[k] = state.current_data[k]

        # Forward inner model
        new_inner_state, logits, (q_halt_logits, q_continue_logits) = self.inner(new_inner_state, new_current_data)

        # Halting logic
        with torch.no_grad():
            new_steps = new_steps + 1
            is_last_step = new_steps >= self.config.halt_max_steps
            halted = is_last_step

            if self.training and self.config.halt_max_steps > 1:
                halted = halted | (q_halt_logits > q_continue_logits)
                # Exploration
                min_halt_steps = (
                    torch.rand_like(q_halt_logits.float()) < self.config.halt_exploration_prob
                ) * torch.randint_like(new_steps, low=2, high=self.config.halt_max_steps + 1)
                halted = halted & (new_steps >= min_halt_steps)

        new_state = HrmState(new_inner_state, new_steps, halted, new_current_data)

        # Prepare hidden states and attentions if requested
        all_hidden_states = None
        all_attentions = None

        if output_hidden_states:
            # Return embeddings + H-level + L-level states to match transformers convention
            # Standard format: (embeddings, *layer_outputs)
            # For HRM: embeddings would be the input embeddings, then H and L level outputs
            # Note: We strip puzzle embedding positions to match input sequence length
            # During generation, only return hidden states for the input tokens (not the full carry)
            embeddings = self.inner._input_embeddings(
                new_current_data["input_ids"], new_current_data.get("puzzle_identifiers")
            )
            puzzle_len = self.inner.puzzle_emb_len

            # Get the sequence length from input_ids (which may be just 1 token during generation)
            seq_len = input_ids.shape[1]
            all_hidden_states = (
                embeddings[:, puzzle_len : puzzle_len + seq_len] if puzzle_len > 0 else embeddings[:, :seq_len],
                new_inner_state.high_level_state[:, puzzle_len : puzzle_len + seq_len]
                if puzzle_len > 0
                else new_inner_state.high_level_state[:, :seq_len],
                new_inner_state.low_level_state[:, puzzle_len : puzzle_len + seq_len]
                if puzzle_len > 0
                else new_inner_state.low_level_state[:, :seq_len],
            )

        # Note: HRM doesn't explicitly compute attention weights in the current implementation
        # so attentions will be None for now
        if output_attentions:
            all_attentions = ()  # Empty tuple as HRM doesn't expose attention weights

        if not return_dict:
            return (logits, q_halt_logits, q_continue_logits, new_state, all_hidden_states, all_attentions)

        return HrmModelOutput(
            logits=logits,
            q_halt_logits=q_halt_logits,
            q_continue_logits=q_continue_logits,
            carry=new_state,
            hidden_states=all_hidden_states,
            attentions=all_attentions,
        )


class HrmForCausalLM(HrmPreTrainedModel, GenerationMixin):
    def __init__(self, config: HrmConfig):
        super().__init__(config)
        self.model = HrmModel(config)
        self.post_init()

    def get_input_embeddings(self):
        return self.model.get_input_embeddings()

    def set_input_embeddings(self, value):
        self.model.set_input_embeddings(value)

    def get_output_embeddings(self):
        return None  # HRM doesn't tie output embeddings

    def set_output_embeddings(self, new_embeddings):
        pass  # HRM doesn't tie output embeddings

    def prepare_inputs_for_generation(self, input_ids, state=None, **kwargs):
        """Prepare inputs for generation, handling dynamic sequence lengths."""
        # During generation, only pass the last token if state exists
        if state is not None:
            input_ids = input_ids[:, -1:]

        return {"input_ids": input_ids, "state": state, **kwargs}

    def _update_model_kwargs_for_generation(self, outputs, model_kwargs, **kwargs):
        """Update model kwargs for next generation step."""
        model_kwargs["state"] = outputs.carry
        return model_kwargs

    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        puzzle_identifiers: Optional[torch.LongTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        state: Optional[HrmState] = None,
        output_hidden_states: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        **kwargs,
    ):
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        outputs = self.model(
            input_ids=input_ids,
            puzzle_identifiers=puzzle_identifiers,
            labels=labels,
            state=state,
            output_hidden_states=output_hidden_states,
            output_attentions=output_attentions,
            return_dict=return_dict,
            **kwargs,
        )

        # Extract outputs
        if return_dict:
            logits = outputs.logits
            q_halt_logits = outputs.q_halt_logits
            q_continue_logits = outputs.q_continue_logits
            state_out = outputs.carry
            hidden_states = outputs.hidden_states
            attentions = outputs.attentions
        else:
            logits, q_halt_logits, q_continue_logits, state_out, hidden_states, attentions = outputs

        loss = None
        if labels is not None:
            # Standard cross entropy loss
            loss_fct = nn.CrossEntropyLoss()
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            loss = loss_fct(shift_logits.view(-1, self.config.vocab_size), shift_labels.view(-1))

        if not return_dict:
            return (loss, logits, q_halt_logits, q_continue_logits, state_out, hidden_states, attentions)

        return HrmCausalLMOutput(
            loss=loss,
            logits=logits,
            q_halt_logits=q_halt_logits,
            q_continue_logits=q_continue_logits,
            carry=state_out,
            hidden_states=hidden_states,
            attentions=attentions,
        )


__all__ = ["HrmModel", "HrmForCausalLM", "HrmPreTrainedModel"]
