#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/fl_clap/modular_fl_clap.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_fl_clap.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
import math
from dataclasses import asdict, dataclass
from typing import Optional

import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange

from ...modeling_utils import PreTrainedModel

# from ..modernbert.modeling_modernbert import ModernBertModel
from ..modernbert import ModernBertModel
from .configuration_fl_clap import DACVAEConfig, FLCLAPConfig, FlClapModernBertConfig


## Patcher


def pad1d(
    x: torch.Tensor,
    paddings: tuple[int, int],
    mode: str = "constant",
    value: float = 0.0,
):
    """Tiny wrapper around F.pad, just to allow for reflect padding on small input.
    If this is the case, we insert extra 0 padding to the right before the reflection happen.
    """
    length = x.shape[-1]
    padding_left, padding_right = paddings
    assert padding_left >= 0 and padding_right >= 0, (padding_left, padding_right)
    if mode == "reflect":
        max_pad = max(padding_left, padding_right)
        extra_pad = 0
        if length <= max_pad:
            extra_pad = max_pad - length + 1
            x = F.pad(x, (0, extra_pad))
        padded = F.pad(x, paddings, mode, value)
        end = padded.shape[-1] - extra_pad
        return padded[..., :end]
    else:
        return F.pad(x, paddings, mode, value)


def get_extra_padding_for_conv1d(x: torch.Tensor, kernel_size: int, stride: int, padding_total: int = 0) -> int:
    """See `pad_for_conv1d`."""
    length = x.shape[-1]
    n_frames = (length - kernel_size + padding_total) / stride + 1
    ideal_length = (math.ceil(n_frames) - 1) * stride + (kernel_size - padding_total)
    return ideal_length - length


class Conv1d(torch.nn.Conv1d):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        kernel_size = self.kernel_size[0]
        stride = self.stride[0]
        dilation = self.dilation[0]
        kernel_size = (kernel_size - 1) * dilation + 1  # effective kernel size with dilations
        padding_total = kernel_size - stride
        extra_padding = get_extra_padding_for_conv1d(x, kernel_size, stride, padding_total)
        # Asymmetric padding required for odd strides
        padding_right = padding_total // 2
        padding_left = padding_total - padding_right
        x = pad1d(x, (padding_left, padding_right + extra_padding))
        return super().forward(x)


## Audio Codec


class VAEBottleneck(torch.nn.Module):
    def __init__(
        self,
        input_dim: int = 512,
        bottleneck_dim: int = 512,
    ):
        super().__init__()
        self.in_proj = torch.nn.Conv1d(input_dim, bottleneck_dim * 2, kernel_size=1)
        self.out_proj = torch.nn.Conv1d(bottleneck_dim, input_dim, kernel_size=1)

    def forward(self, z: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        mean, scale = self.in_proj(z).chunk(2, dim=1)
        stdev = torch.nn.functional.softplus(scale) + 1e-4
        var = stdev * stdev
        logvar = torch.log(var)
        latents = torch.randn_like(mean) * stdev + mean
        kl = (mean * mean + var - logvar - 1).sum(1).mean()
        return latents, kl


class Snake1d(nn.Module):
    """
    A 1-dimensional Snake activation function module.
    """

    def __init__(self, hidden_dim):
        super().__init__()
        self.alpha = nn.Parameter(torch.ones(1, hidden_dim, 1))

    def forward(self, hidden_states):
        shape = hidden_states.shape
        hidden_states = hidden_states.reshape(shape[0], shape[1], -1)
        hidden_states = hidden_states + (self.alpha + 1e-9).reciprocal() * torch.sin(self.alpha * hidden_states).pow(2)
        hidden_states = hidden_states.reshape(shape)
        return hidden_states


class DACVAEResidualUnit(nn.Module):
    """
    A residual unit composed of Snake1d and weight-normalized Conv1d layers with dilations.
    """

    def __init__(self, dimension: int = 16, dilation: int = 1):
        super().__init__()
        pad = ((7 - 1) * dilation) // 2

        self.snake1 = Snake1d(dimension)
        self.conv1 = nn.Conv1d(dimension, dimension, kernel_size=7, dilation=dilation, padding=pad)
        self.snake2 = Snake1d(dimension)
        self.conv2 = nn.Conv1d(dimension, dimension, kernel_size=1)

    def forward(self, hidden_state):
        """
        Forward pass through the residual unit.

        Args:
            hidden_state (`torch.Tensor` of shape `(batch_size, channels, time_steps)`):
                Input tensor .

        Returns:
            output_tensor (`torch.Tensor` of shape `(batch_size, channels, time_steps)`):
                Input tensor after passing through the residual unit.
        """
        output_tensor = hidden_state
        output_tensor = self.conv1(self.snake1(output_tensor))
        output_tensor = self.conv2(self.snake2(output_tensor))

        padding = (hidden_state.shape[-1] - output_tensor.shape[-1]) // 2
        if padding > 0:
            hidden_state = hidden_state[..., padding:-padding]
        output_tensor = hidden_state + output_tensor
        return output_tensor


class DACVAEEncoderBlock(nn.Module):
    """Encoder block used in D_A_C_V_A_E encoder."""

    def __init__(self, config: DACVAEConfig, stride: int = 1, stride_index: int = 1):
        super().__init__()

        dimension = config.encoder_hidden_size * 2**stride_index
        self.res_unit1 = DACVAEResidualUnit(dimension // 2, dilation=1)
        self.res_unit2 = DACVAEResidualUnit(dimension // 2, dilation=3)
        self.res_unit3 = DACVAEResidualUnit(dimension // 2, dilation=9)
        self.snake1 = Snake1d(dimension // 2)
        self.conv1 = nn.Conv1d(
            dimension // 2, dimension, kernel_size=2 * stride, stride=stride, padding=math.ceil(stride / 2)
        )

    def forward(self, hidden_state):
        hidden_state = self.res_unit1(hidden_state)
        hidden_state = self.res_unit2(hidden_state)
        hidden_state = self.snake1(self.res_unit3(hidden_state))
        hidden_state = self.conv1(hidden_state)

        return hidden_state


class DACVAEEncoder(nn.Module):
    """D_A_C_V_A_E Encoder"""

    def __init__(self, config: DACVAEConfig):
        super().__init__()

        strides = config.downsampling_ratios
        # Create first convolution
        self.conv1 = nn.Conv1d(1, config.encoder_hidden_size, kernel_size=7, padding=3)

        self.block = []
        # Create EncoderBlocks that double channels as they downsample by `stride`
        for stride_index, stride in enumerate(strides):
            stride_index = stride_index + 1
            self.block += [DACVAEEncoderBlock(config, stride=stride, stride_index=stride_index)]

        self.block = nn.ModuleList(self.block)
        d_model = config.encoder_hidden_size * 2**stride_index
        self.snake1 = Snake1d(d_model)
        self.conv2 = nn.Conv1d(d_model, config.hidden_size, kernel_size=3, padding=1)

    def forward(self, hidden_state):
        hidden_state = self.conv1(hidden_state)

        for module in self.block:
            hidden_state = module(hidden_state)

        hidden_state = self.snake1(hidden_state)
        hidden_state = self.conv2(hidden_state)

        return hidden_state


class FlClapDACVAE(torch.nn.Module):
    def __init__(self, config: DACVAEConfig) -> None:
        super().__init__()
        self.encoder = DACVAEEncoder(config)
        self.bottleneck = VAEBottleneck(config.codebook_size, config.codebook_dim)
        self.hop_length = config.hop_length
        self.mean = 0.0
        self.std = 1.0
        self.sampling_rate = config.sampling_rate

    def forward(self, waveform: torch.Tensor) -> torch.Tensor:
        with torch.no_grad(), torch.backends.cudnn.flags(enabled=False):
            z = self.encoder(self._pad(waveform))
            encoded_frames, _ = self.bottleneck(z)
            encoded_frames = (encoded_frames - self.mean) / self.std
        return encoded_frames

    def _pad(self, wavs):
        length = wavs.size(-1)
        if length % self.hop_length:
            p1d = (0, self.hop_length - (length % self.hop_length))
            return torch.nn.functional.pad(wavs, p1d, "reflect")
        else:
            return wavs

    def feature_idx_to_wav_idx(self, feature_idx, sampling_rate=None):
        if sampling_rate is None:
            sampling_rate = self.sampling_rate
        orig_freq = sampling_rate
        new_freq = self.sampling_rate
        wav_idx = feature_idx * self.hop_length * (orig_freq / new_freq)
        return wav_idx.int()

    def wav_idx_to_feature_idx(self, wav_idx, sampling_rate=None):
        if sampling_rate is None:
            sampling_rate = self.sampling_rate
        orig_freq = sampling_rate
        new_freq = self.sampling_rate
        target_length = torch.ceil(new_freq * wav_idx / orig_freq)
        feature_idx = torch.ceil(target_length / self.hop_length)
        return feature_idx.int()


class ConvBlock1d(torch.nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        *,
        kernel_size: int = 3,
        stride: int = 1,
        dilation: int = 1,
        num_groups: int = 8,
    ) -> None:
        super().__init__()

        self.groupnorm = torch.nn.GroupNorm(num_groups=num_groups, num_channels=in_channels)
        self.activation = torch.nn.SiLU()
        self.project = Conv1d(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=kernel_size,
            stride=stride,
            dilation=dilation,
        )

    def forward(
        self,
        x: torch.Tensor,
    ) -> torch.Tensor:
        x = self.groupnorm(x)
        x = self.activation(x)
        return self.project(x)


class ResnetBlock1d(torch.nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        *,
        kernel_size: int = 3,
        stride: int = 1,
        dilation: int = 1,
        num_groups: int = 8,
    ) -> None:
        super().__init__()

        self.block1 = ConvBlock1d(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=kernel_size,
            stride=stride,
            dilation=dilation,
            num_groups=num_groups,
        )

        self.block2 = ConvBlock1d(
            in_channels=out_channels,
            out_channels=out_channels,
            num_groups=num_groups,
        )

        self.to_out = (
            Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=1)
            if in_channels != out_channels
            else torch.nn.Identity()
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        h = self.block1(x)
        h = self.block2(h)
        return h + self.to_out(x)


class Patcher(torch.nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        patch_size: int,
    ):
        super().__init__()
        assert_message = f"out_channels must be divisible by patch_size ({patch_size})"
        assert out_channels % patch_size == 0, assert_message
        self.patch_size = patch_size
        self.block = ResnetBlock1d(
            in_channels=in_channels,
            out_channels=out_channels // patch_size,
            num_groups=1,
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.block(x)
        x = rearrange(x, "b c (l p) -> b (c p) l", p=self.patch_size)
        return x


class RotaryEmbedding(torch.nn.Module):
    """
    RotaryEmbedding Module
    """

    def __init__(
        self,
        theta: float,
        head_dim: int,
        max_seqlen: int = 1024,
        scale_factor: int = 1,
        low_freq_factor: int = 1,
        high_freq_factor: int = 32,
        old_context_len: int = 8192,
    ):
        super().__init__()

        self.theta = theta
        self.head_dim = head_dim
        self.max_seqlen = max_seqlen
        self.scale_factor = scale_factor
        self.low_freq_factor = low_freq_factor
        self.high_freq_factor = high_freq_factor
        self.old_context_len = old_context_len
        if scale_factor != 1:
            self.low_freq_wavelen = old_context_len / low_freq_factor
            self.high_freq_wavelen = old_context_len / high_freq_factor
            assert self.low_freq_wavelen >= self.high_freq_wavelen

    def reset_parameters(self):
        freqs_cis = self.precompute_freqs_cis(dim=self.head_dim, end=self.max_seqlen, theta=self.theta)
        S, D, _, _ = freqs_cis.shape
        # S D 2 2 -> 1 S 1 D 2 2
        freqs_cis = freqs_cis.view(1, S, 1, D, 2, 2)
        self.register_buffer(
            "freqs_cis",
            freqs_cis,
            persistent=False,
        )

    def apply_scaling(self, freqs):
        if self.scale_factor == 1:
            return freqs
        new_freqs = []
        for freq in freqs:
            wavelen = 2 * math.pi / freq
            if wavelen < self.high_freq_wavelen:
                new_freqs.append(freq)
            elif wavelen > self.low_freq_wavelen:
                new_freqs.append(freq / self.scale_factor)
            else:
                assert self.low_freq_wavelen != self.high_freq_wavelen
                smooth = (self.old_context_len / wavelen - self.low_freq_factor) / (
                    self.high_freq_factor - self.low_freq_factor
                )
                new_freqs.append((1 - smooth) * freq / self.scale_factor + smooth * freq)
        return torch.tensor(new_freqs, dtype=freqs.dtype, device=freqs.device)

    def precompute_freqs_cis(
        self,
        dim: int,
        end: int,
        theta: float = 10000.0,
    ):
        """
        Precompute the frequency tensor for complex exponentials (cis) with given dimensions.

        This function calculates a frequency tensor with complex exponentials using the given dimension 'dim'
        and the end index 'end'. The 'theta' parameter scales the frequencies.
        The returned tensor contains complex values in complex64 data type.

        Args:
            dim (int): Dimension of the frequency tensor.
            end (int): End index for precomputing frequencies.
            theta (float, optional): Scaling factor for frequency computation. Defaults to 10000.0.

        Returns:
            torch.Tensor: Precomputed frequency tensor with complex exponentials.
        """
        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))
        freqs = self.apply_scaling(freqs)

        t = torch.arange(end, device=freqs.device)
        freqs = torch.outer(t, freqs).float()

        cos, sin = freqs.cos(), freqs.sin()

        return torch.stack((cos, -sin, sin, cos), dim=-1).view(*freqs.size(), 2, 2)

    def forward(self, x: torch.Tensor, bhle: bool = False, **kwargs):
        if bhle:
            x = x.transpose(1, 2)  # (B H L E) -> (B L H E)
        seqlen = x.size(1)
        x_ = x.reshape(*x.shape[:-1], -1, 1, 2)  # B L H E -> B L H E/2 1 2
        x_out = (x_ * self.freqs_cis[:, :seqlen]).sum(5).flatten(3)
        if bhle:
            x_out = x_out.transpose(1, 2)
        return x_out.type_as(x)


class RMSNorm(torch.nn.Module):
    def __init__(self, dim: int, eps: float = 1e-5):
        super().__init__()
        self.eps = eps
        self.weight = torch.nn.Parameter(torch.ones(dim))

    def _norm(self, x):
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        output = self._norm(x.float())
        return (output * self.weight).type_as(x)


class CLSToken(torch.nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.weight = torch.nn.Parameter(torch.randn(1, 1, d_model))

    def forward(self, input_data: torch.Tensor) -> torch.Tensor:
        """Expands CLS token to match input batch size.

        Args:
            input_data: Input tensor of shape (batch_size, seq_len, d_model)

        Returns:
            Expanded CLS tokens of shape (batch_size, 1, d_model)
        """
        return self.weight.expand(input_data.size(0), -1, -1)


class Attention(torch.nn.Module):
    def __init__(
        self,
        dim: int,
        head_dim: int,
        n_heads: int,
        n_kv_heads: int,
        norm_eps: float = 1e-5,
        use_qk_norm: bool = True,
    ):
        super().__init__()
        assert n_heads % n_kv_heads == 0

        self.head_dim = head_dim
        self.n_heads = n_heads
        self.n_kv_heads = n_kv_heads
        self.use_qk_norm = use_qk_norm

        # local heads
        assert self.n_heads % self.n_kv_heads == 0

        self.wq = torch.nn.Linear(dim, n_heads * head_dim, bias=False)
        self.wk, self.wv = [torch.nn.Linear(dim, n_kv_heads * head_dim, bias=False) for _ in range(2)]
        self.wo = torch.nn.Linear(n_heads * head_dim, dim, bias=False)

        self.q_norm, self.k_norm = [RMSNorm(head_dim, norm_eps) for _ in range(2)]

    def reshape_heads(self, x: torch.Tensor, heads: int) -> torch.Tensor:
        B, T, C = x.shape
        # B x T x C -> B x T x C/H x H
        x = x.reshape(B, T, C // heads, heads)
        # B x T x C/H x H -> B x H x T x C/H
        return x.permute(0, 3, 1, 2)

    def forward(
        self,
        x: torch.Tensor,
        cross_x: Optional[torch.Tensor] = None,
        key_padding_mask: Optional[torch.Tensor] = None,
        rope_embeddings: Optional[RotaryEmbedding] = None,
    ):
        # x: B, T, E
        xq = self.wq(x)
        if cross_x is not None:
            xk, xv = self.wk(cross_x), self.wv(cross_x)
        else:
            xk, xv = self.wk(x), self.wv(x)

        xk = self.reshape_heads(xk, self.n_kv_heads)
        xv = self.reshape_heads(xv, self.n_kv_heads)
        xq = self.reshape_heads(xq, self.n_heads)
        if self.use_qk_norm:
            xq = self.q_norm(xq)
            xk = self.k_norm(xk)

        if rope_embeddings is not None:
            xq = rope_embeddings(xq, bhle=True)
            xk = rope_embeddings(xk, bhle=True)

        attn_mask = None
        if key_padding_mask is not None:
            attn_mask = key_padding_mask[:, None, None, :]

        output = F.scaled_dot_product_attention(xq, xk, xv, attn_mask=attn_mask)

        output = rearrange(output, "b h n d -> b n (h d)")
        return self.wo(output)


class FeedForward(torch.nn.Module):
    def __init__(
        self,
        dim: int,
        hidden_dim: int,
        ffn_dim_multiplier: float,
        multiple_of: int,
        dropout: float,
        non_linearity: str,
    ):
        super().__init__()
        self.swiglu = non_linearity == "swiglu"
        self.dropout = dropout
        # swiglu hidden dim factor multiplier (same #params as relu / gelu)
        hidden_dim = int(2 * hidden_dim / 3)

        # custom dim factor multiplier
        hidden_dim = int(ffn_dim_multiplier * hidden_dim)
        # round hidden dimension to `multiple_of`
        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)
        # layers
        self.w1 = torch.nn.Linear(
            dim,
            hidden_dim,
            bias=False,
        )
        self.w2 = torch.nn.Linear(
            hidden_dim,
            dim,
            bias=False,
        )
        if self.swiglu:
            self.w3 = torch.nn.Linear(
                dim,
                hidden_dim,
                bias=False,
            )

        # non-linearity
        self.non_linearity = {
            "relu": F.relu,
            "gelu": F.gelu,
            "swiglu": None,
            "srelu": lambda x: F.relu(x) ** 2,
            "silu": F.silu,
        }[non_linearity]

    def forward(
        self,
        x,
    ):
        hidden1 = self.w1(x)
        if self.swiglu:
            hidden3 = self.w3(x)
            hidden = F.silu(hidden1) * hidden3
        else:
            hidden = self.non_linearity(hidden1)
        hidden = F.dropout(hidden, p=self.dropout, training=self.training)
        return self.w2(hidden)


class TransformerBlock(torch.nn.Module):
    def __init__(
        self,
        dim: int,
        n_heads: int,
        n_kv_heads: Optional[int] = None,
        dropout: float = 0.0,
        pre_norm: bool = True,
        norm_eps: float = 1e-5,
        qk_norm: bool = True,
        ffn_exp: int = 4,
        ffn_dim_multiplier: int = 1,
        multiple_of: int = 64,
        non_linearity: str = "swiglu",
    ):
        super().__init__()
        assert dim % n_heads == 0
        self.n_heads = n_heads
        self.n_kv_heads = n_heads if n_kv_heads is None else n_kv_heads
        self.dim = dim
        self.dropout = dropout
        self.head_dim = dim // n_heads
        self.pre_norm = pre_norm
        assert self.n_heads % self.n_kv_heads == 0

        self.attention = Attention(
            dim=dim,
            head_dim=self.head_dim,
            n_heads=self.n_heads,
            n_kv_heads=self.n_kv_heads,
            norm_eps=norm_eps,
            use_qk_norm=qk_norm,
        )
        self.feed_forward = FeedForward(
            dim=dim,
            hidden_dim=int(ffn_exp * dim),
            ffn_dim_multiplier=ffn_dim_multiplier,
            multiple_of=multiple_of,
            dropout=dropout,
            non_linearity=non_linearity,
        )

        self.attention_norm, self.ffn_norm = [RMSNorm(dim, norm_eps) for _ in range(2)]

    def forward(
        self,
        x: torch.Tensor,
        padding_mask: Optional[torch.Tensor],
        rope_embeddings: Optional[RotaryEmbedding] = None,
    ):
        assert self.attention is not None and self.attention_norm is not None
        h_attn = self.attention(
            self.attention_norm(x),
            key_padding_mask=padding_mask,
            rope_embeddings=rope_embeddings,
        )
        h = x + h_attn
        h_ff = self.feed_forward(self.ffn_norm(h))
        out = h + h_ff
        return out


class FlClapTransformer(torch.nn.Module):
    def __init__(
        self,
        in_channels: int,
        dim: int,
        n_heads: int,
        n_layers: int,
        out_channels: int,
        dropout: float = 0.0,
        pre_norm: bool = True,
        norm_eps: float = 1e-5,
        qk_norm: bool = True,
        fc_bias: bool = False,
        ffn_exp: int = 4,
        ffn_dim_multiplier: int = 1,
        multiple_of: int = 64,
        non_linearity: str = "swiglu",
        use_rope: bool = True,
        max_positions: int = 10000,
        patch_size: int = 1,
    ):
        self.patch_size = patch_size
        self.out_channels = out_channels
        self.output_size = (self.patch_size**2) * self.out_channels

        super().__init__()
        self.n_layers = n_layers
        self.dim = dim
        self.head_dim = dim // n_heads
        self.dropout = dropout
        self.use_rope = use_rope
        self.non_linearity = non_linearity
        self.n_kv_heads = n_heads

        self.rope_embeddings = None
        if self.use_rope:
            self.rope_embeddings = RotaryEmbedding(
                theta=max(10000, 2 * max_positions),
                head_dim=dim // n_heads,
                max_seqlen=max_positions,
            )
            self.rope_embeddings.reset_parameters()

        # fl_clap_transformer blocks
        self.layers = torch.nn.ModuleList()
        for _ in range(n_layers):
            layer = TransformerBlock(
                dim=dim,
                n_heads=n_heads,
                n_kv_heads=self.n_kv_heads,
                dropout=dropout,
                pre_norm=pre_norm,
                norm_eps=norm_eps,
                qk_norm=qk_norm,
                ffn_exp=ffn_exp,
                ffn_dim_multiplier=ffn_dim_multiplier,
                multiple_of=multiple_of,
                non_linearity=non_linearity,
            )
            self.layers.append(layer)

        self.norm = None
        if pre_norm:
            self.norm = RMSNorm(dim, norm_eps)

        # output layer
        self.output = torch.nn.Linear(dim, out_channels, bias=fc_bias)

        self.x_embedder = Patcher(
            in_channels=dim,
            out_channels=dim,
            patch_size=self.patch_size,
        )

        self.data_proj = torch.nn.Linear(in_channels, dim)
        self.cls_token = CLSToken(dim)

    def unpatchify(self, x):
        return x

    def forward(
        self,
        x: torch.Tensor,
        *,
        padding_mask: Optional[torch.Tensor] = None,
    ):
        x = self.data_proj(x)

        # Prepend the cls token
        x = torch.cat([self.cls_token(x), x], dim=1)
        if padding_mask is not None:
            padding_mask = torch.cat([padding_mask[:, [0]], padding_mask], dim=1)

        x = rearrange(x, "b l c-> b c l")
        h = self.x_embedder(x)
        h = rearrange(h, "b c l -> b l c")
        original_N = h.shape[1]
        N = h.shape[1]
        h = F.dropout(h, p=self.dropout, training=self.training)

        for i, layer in enumerate(self.layers):
            h = layer(
                x=h,
                padding_mask=padding_mask,
                rope_embeddings=self.rope_embeddings,
            )

        # output layer
        if self.norm is not None:
            h = self.norm(h)
        h = F.dropout(h, p=self.dropout, training=self.training)
        output = self.output(h)
        N = output.shape[1]
        if original_N != N:
            output = output[:, -original_N:]
        output = self.unpatchify(output)
        return output[:, 1:], output[:, 0]


# class FlClapModernBertModel(ModernBertModel): ...


## Text Encoder
class FlClapModernBertModel(torch.nn.Module):
    def __init__(self, config: FlClapModernBertConfig):
        super().__init__()
        self.nth_layer = config.nth_layer
        self.model = ModernBertModel(config)

    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None):
        output = self.model(
            input_ids,
            attention_mask=attention_mask,
            output_hidden_states=self.nth_layer is not None,
        )
        if self.nth_layer is None:
            # Note that `hidden_state[-1]` is not necessarily equivalent to `last_hidden_state`
            # https://huggingface.co/docs/transformers/en/main_classes/output#model-outputs
            return output.last_hidden_state[:, 0]
        return output.hidden_states[self.nth_layer][:, 0]


@dataclass
class JudgeOutput:
    overall: torch.Tensor
    recall: torch.Tensor
    precision: torch.Tensor
    faithfulness: torch.Tensor


class ContrastiveHead(torch.nn.Module):
    def __init__(self, in_dim: int, out_dim: int) -> None:
        super().__init__()
        self.layer_norm = torch.nn.LayerNorm(normalized_shape=in_dim, eps=1e-6)
        self.proj = torch.nn.Linear(in_dim, out_dim, bias=False)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.proj(self.layer_norm(x))


@dataclass
class FLCLAPOutput:
    text_embeddings: Optional[torch.Tensor] = None
    audio_embeddings: Optional[torch.Tensor] = None


class FLCLAPModel(PreTrainedModel):
    config: FLCLAPConfig

    def __init__(self, cfg: FLCLAPConfig):
        super().__init__(cfg)
        self.cfg = cfg
        self.text_encoder = FlClapModernBertModel(cfg.text_encoder)
        self.audio_encoder = FlClapTransformer(**asdict(cfg.audio_encoder))
        self.audio_codec = FlClapDACVAE(cfg.audio_codec)

        self.audio_head = ContrastiveHead(cfg.audio_encoder.dim, cfg.out_dim)
        self.text_head = ContrastiveHead(cfg.text_encoder.hidden_size, cfg.out_dim)

    @property
    def sample_rate(self):
        return self.cfg.audio_codec.sample_rate

    def encode_text(self, text: torch.Tensor, attention_mask: Optional[torch.Tensor] = None):
        return self.text_head(self.text_encoder(text, attention_mask=attention_mask))

    def encode_audio(self, audio: torch.Tensor):
        features = self.audio_codec(audio)
        emb, _ = self.audio_encoder(features.transpose(1, 2))
        return self.audio_head(emb)

    def forward(
        self,
        audio: Optional[torch.Tensor] = None,
        input_ids: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
    ):
        audio_emb = text_emb = None
        if audio is not None:
            audio_emb = self.encode_audio(audio)
        if input_ids is not None:
            text_emb = self.encode_text(input_ids, attention_mask=attention_mask)
        return FLCLAPOutput(text_embeddings=text_emb, audio_embeddings=audio_emb)


__all__ = ["FLCLAPModel", "JudgeOutput"]
