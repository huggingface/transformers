#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/pp_lcnet/modular_pp_lcnet.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_pp_lcnet.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
from dataclasses import dataclass
from typing import Optional, Union

import torch
import torch.nn as nn

from ...modeling_outputs import BaseModelOutputWithNoAttention
from ...modeling_utils import PreTrainedModel
from ...utils import ModelOutput
from .configuration_pp_lcnet import PPLCNetConfig


def _create_act(act):
    """
    Create the corresponding PyTorch activation module based on activation name.

    Args:
        act (str): Name of activation function. Supported values: "hardswish", "relu", "relu6".

    Returns:
        nn.Module: Instantiated PyTorch activation module.

    Raises:
        RuntimeError: If the input activation name is not supported.
    """
    if act == "hardswish":
        return nn.Hardswish()
    elif act == "relu":
        return nn.ReLU()
    elif act == "relu6":
        return nn.ReLU6()
    else:
        raise RuntimeError(f"The activation function is not supported: {act}")


class ConvBNLayer(nn.Module):
    """
    Combined layer: Conv2d -> BatchNorm2d -> Activation
    A common basic component in lightweight models to reduce redundant code.
    """

    def __init__(
        self,
        num_channels,
        filter_size,
        num_filters,
        stride,
        num_groups=1,
        act="hardswish",
    ):
        """
        Initialize the ConvBNLayer module.

        Args:
            num_channels (int): Number of channels of the input feature map.
            filter_size (int): Size of the convolutional kernel (square kernel).
            num_filters (int): Number of channels of the output feature map.
            stride (int): Stride of the convolution operation.
            num_groups (int, optional): Number of groups for grouped convolution. Defaults to 1 (standard convolution).
            act (str, optional): Name of activation function. Defaults to "hardswish".
        """
        super().__init__()

        self.conv = nn.Conv2d(
            in_channels=num_channels,
            out_channels=num_filters,
            kernel_size=filter_size,
            stride=stride,
            padding=(filter_size - 1) // 2,
            groups=num_groups,
            bias=False,
        )

        nn.init.kaiming_normal_(self.conv.weight)
        self.bn = nn.BatchNorm2d(
            num_filters,
            momentum=0.9,
        )
        self.act = _create_act(act)

    def forward(self, x):
        """
        Forward propagation logic.

        Args:
            x (FloatTensor): Input feature map with shape [B, C, H, W].

        Returns:
            FloatTensor: Output feature map with shape [B, num_filters, H', W'].
        """
        x = self.conv(x)
        x = self.bn(x)
        x = self.act(x)
        return x


class DepthwiseSeparable(nn.Module):
    """
    Depthwise Separable Convolution Layer: Depthwise Conv -> SE Module (optional) -> Pointwise Conv
    Core component of lightweight models (e.g., MobileNet, PP-LCNet) that significantly reduces
    the number of parameters and computational cost.
    """

    def __init__(
        self,
        num_channels,
        num_filters,
        stride,
        reduction: int,
        dw_size=3,
        use_se=False,
        act="hardswish",
    ):
        """
        Initialize the DepthwiseSeparable module.

        Args:
            num_channels (int): Number of channels of the input feature map.
            num_filters (int): Number of channels of the output feature map.
            stride (int): Stride of the depthwise convolution.
            reduction (int): Reduction ratio for SE module.
            dw_size (int, optional): Kernel size of depthwise convolution. Defaults to 3.
            use_se (bool, optional): Whether to use SE module. Defaults to False.
            act (str, optional): Name of activation function. Defaults to "hardswish".
        """
        super().__init__()
        self.use_se = use_se
        self.dw_conv = ConvBNLayer(
            num_channels=num_channels,
            num_filters=num_channels,
            filter_size=dw_size,
            stride=stride,
            num_groups=num_channels,
            act=act,
        )
        self.se = SEModule(num_channels, reduction) if use_se else nn.Identity()
        self.pw_conv = ConvBNLayer(
            num_channels=num_channels,
            filter_size=1,
            num_filters=num_filters,
            stride=1,
            act=act,
        )

    def forward(self, x):
        """
        Forward propagation logic.

        Args:
            x (FloatTensor): Input feature map with shape [B, C, H, W].

        Returns:
            FloatTensor: Output feature map with shape [B, num_filters, H', W'].
        """
        x = self.dw_conv(x)
        x = self.se(x)
        x = self.pw_conv(x)
        return x


class SEModule(nn.Module):
    """
    Squeeze-and-Excitation (SE) Module: Adaptive feature recalibration
    Enhances the model's ability to focus on important channels by learning channel-wise attention weights.
    """

    def __init__(self, channel, reduction=4):
        """
        Initialize the SEModule module.

        Args:
            channel (int): Number of channels of the input feature map.
            reduction (int, optional): Reduction ratio for bottleneck layer. Defaults to 4.
        """
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        conv_kwargs = {"kernel_size": 1, "stride": 1, "padding": 0, "bias": True}
        self.conv1 = nn.Conv2d(in_channels=channel, out_channels=channel // reduction, **conv_kwargs)
        self.conv2 = nn.Conv2d(in_channels=channel // reduction, out_channels=channel, **conv_kwargs)
        self.relu = nn.ReLU()

        self.hardsigmoid = nn.Hardsigmoid()

    def forward(self, x):
        """
        Forward propagation logic.

        Args:
            x (FloatTensor): Input feature map with shape [B, C, H, W].

        Returns:
            FloatTensor: Attention-weighted feature map with shape [B, C, H, W].
        """
        identity = x
        x = self.avg_pool(x)
        x = self.conv1(x)
        x = self.relu(x)
        x = self.conv2(x)
        x = self.hardsigmoid(x)
        x = identity * x
        return x


@dataclass
class PPLCNetModelOutput(ModelOutput):
    """
    Output class for PP-LCNet base model.

    Args:
        logits (Optional[FloatTensor]): Classification logits with shape [B, num_classes]. Defaults to None.
        last_hidden_state (Optional[FloatTensor]): Final hidden state from backbone with shape [B, C, H, W]. Defaults to None.
        hidden_states (Optional[Tuple[FloatTensor, ...]]): Tuple of hidden states at each layer. Defaults to None.
    """

    logits: Optional[torch.FloatTensor] = None
    last_hidden_state: Optional[torch.FloatTensor] = None
    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None


class PPLCNetPreTrainedModel(PreTrainedModel):
    """
    An abstract base class for PP-LCNet models that inherits from Hugging Face PreTrainedModel.
    Provides common functionality for weight initialization and loading.
    """

    config: PPLCNetConfig
    base_model_prefix = "pp_lcnet"
    main_input_name = "pixel_values"
    input_modalities = ("image",)


def make_divisible(v, divisor=8, min_value=None):
    """
    Ensure that the number of channels is divisible by the given divisor (for hardware optimization).

    Args:
        v (float): Original number of channels.
        divisor (int, optional): Divisor for channel adjustment. Defaults to 8.
        min_value (Optional[int]): Minimum number of channels after adjustment. Defaults to None.

    Returns:
        int: Adjusted number of channels that is divisible by the divisor.
    """
    if min_value is None:
        min_value = divisor
    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)
    if new_v < 0.9 * v:
        new_v += divisor
    return new_v


class PPLCNetModel(PPLCNetPreTrainedModel):
    """
    PP-LCNet base model: lightweight CNN backbone for image classification tasks.
    """

    def __init__(self, config: PPLCNetConfig):
        """
        Initialize the PPLCNetModel with given configuration.

        Args:
            config (PPLCNetConfig): Configuration class for PP-LCNet.
        """
        super().__init__(config)

        assert isinstance(config.stride_list, (list, tuple)), (
            f"stride_list should be in (list, tuple) but got {type(config.stride_list)}"
        )
        assert len(config.stride_list) == 5, f"stride_list length should be 5 but got {len(config.stride_list)}"

        self.dropout_prob = config.dropout_prob

        for i, stride in enumerate(config.stride_list[1:]):
            config.backbone_config[f"blocks{i + 3}"][0][3] = stride
        self.conv1 = ConvBNLayer(
            num_channels=3,
            filter_size=3,
            num_filters=make_divisible(16 * config.scale),
            stride=config.stride_list[0],
            act=config.act,
        )

        def _build_block(block_name):
            return nn.Sequential(
                *[
                    DepthwiseSeparable(
                        num_channels=make_divisible(in_c * config.scale),
                        num_filters=make_divisible(out_c * config.scale),
                        dw_size=k,
                        stride=s,
                        reduction=config.reduction,
                        use_se=se,
                        act=config.act,
                    )
                    for i, (k, in_c, out_c, s, se) in enumerate(config.backbone_config[block_name])
                ]
            )

        self.blocks2 = _build_block("blocks2")
        self.blocks3 = _build_block("blocks3")
        self.blocks4 = _build_block("blocks4")
        self.blocks5 = _build_block("blocks5")
        self.blocks6 = _build_block("blocks6")

        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        if config.use_last_conv:
            self.last_conv = nn.Conv2d(
                in_channels=make_divisible(config.backbone_config["blocks6"][-1][2] * config.scale),
                out_channels=config.class_expand,
                kernel_size=1,
                stride=1,
                padding=0,
                bias=False,
            )
            self.act = _create_act(config.act)
        else:
            self.last_conv = None
        self.flatten = nn.Flatten(start_dim=1, end_dim=-1)

        if config.use_last_conv:
            fc_in_channels = config.class_expand
        else:
            fc_in_channels = make_divisible(config.backbone_config["blocks6"][-1][2] * config.scale)
        self.fc = nn.Linear(fc_in_channels, config.class_num)
        self.out_act = nn.Softmax(dim=-1)

        self.post_init()

    def forward(
        self,
        hidden_state: torch.FloatTensor,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        **kwargs,
    ) -> Union[tuple[torch.FloatTensor], PPLCNetModelOutput]:
        """
        Forward propagation of PP-LCNet base model.

        Args:
            hidden_state (FloatTensor): Input image tensor with shape [B, 3, H, W].
            output_hidden_states (Optional[bool]): Whether to return hidden states at each layer. Defaults to None.
            return_dict (Optional[bool]): Whether to return output as a dataclass. Defaults to None.

        Returns:
            Union[tuple[FloatTensor], PPLCNetModelOutput]: Model outputs.
        """
        hidden_states = () if output_hidden_states else None

        if output_hidden_states:
            hidden_states = hidden_states + (hidden_state,)
        hidden_state = self.conv1(hidden_state)
        if output_hidden_states:
            hidden_states = hidden_states + (hidden_state,)
        hidden_state = self.blocks2(hidden_state)
        if output_hidden_states:
            hidden_states = hidden_states + (hidden_state,)
        hidden_state = self.blocks3(hidden_state)
        if output_hidden_states:
            hidden_states = hidden_states + (hidden_state,)
        hidden_state = self.blocks4(hidden_state)
        if output_hidden_states:
            hidden_states = hidden_states + (hidden_state,)
        hidden_state = self.blocks5(hidden_state)
        if output_hidden_states:
            hidden_states = hidden_states + (hidden_state,)
        hidden_state = self.blocks6(hidden_state)
        if output_hidden_states:
            hidden_states = hidden_states + (hidden_state,)
        last_hidden_state = hidden_states

        hidden_state = self.avg_pool(hidden_state)
        if self.last_conv is not None:
            hidden_state = self.last_conv(hidden_state)
            hidden_state = self.act(hidden_state)
            hidden_state = hidden_state * (1 - self.dropout_prob)  # dropout
        hidden_state = self.flatten(hidden_state)
        hidden_state = self.fc(hidden_state)

        hidden_state = self.out_act(hidden_state)

        if not return_dict:
            output = (last_hidden_state,)
            if output_hidden_states:
                output += (hidden_states,)
            output += (hidden_state,)
            return output

        return PPLCNetModelOutput(
            logits=hidden_state,
            last_hidden_state=last_hidden_state,
            hidden_states=hidden_states if output_hidden_states else None,
        )


@dataclass
class PPLCNetForImageClassificationOutput(BaseModelOutputWithNoAttention):
    """
    Output class for PP-LCNet image classification model.

    Args:
        logits (Optional[FloatTensor]): Classification logits with shape [B, num_classes]. Defaults to None.
        last_hidden_state (Optional[FloatTensor]): Final hidden state from backbone with shape [B, C, H, W]. Defaults to None.
        hidden_states (Optional[Tuple[FloatTensor, ...]]): Tuple of hidden states at each layer. Defaults to None.
    """

    logits: Optional[torch.FloatTensor] = None
    shape: Optional[torch.FloatTensor] = None


class PPLCNetForImageClassification(PPLCNetPreTrainedModel):
    """
    PP-LCNet model for image classification tasks.
    Wraps the base model with a classification head (compatible with Transformers pipeline).
    """

    _keys_to_ignore_on_load_missing = ["num_batches_tracked"]

    def __init__(self, config: PPLCNetConfig):
        """
        Initialize the PPLCNetForImageClassification model.

        Args:
            config (PPLCNetConfig): Configuration class for PP-LCNet.
        """
        super().__init__(config)
        self.model = PPLCNetModel(config)
        self.post_init()

    def forward(
        self,
        pixel_values: torch.FloatTensor,
        labels: Optional[list[dict]] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        **kwargs,
    ) -> Union[tuple[torch.FloatTensor], PPLCNetForImageClassificationOutput]:
        """
        Forward propagation of PP-LCNet image classification model.

        Args:
            pixel_values (FloatTensor): Input image tensor with shape [B, 3, H, W].
            labels (Optional[List[Dict[str, Any]]]): Ground truth labels for loss calculation. Defaults to None.
            output_hidden_states (Optional[bool]): Whether to return hidden states at each layer. Defaults to None.
            return_dict (Optional[bool]): Whether to return output as a dataclass. Defaults to None.

        Returns:
            Union[tuple[FloatTensor], PPLCNetForImageClassificationOutput]: Classification outputs.
        """

        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        outputs = self.model(pixel_values, output_hidden_states=output_hidden_states, return_dict=return_dict)

        if not return_dict:
            output = (outputs[0],)
            if output_hidden_states:
                output += (outputs[1], outputs[2])
            else:
                output += (outputs[1],)

            return output
        return PPLCNetForImageClassificationOutput(
            logits=outputs.logits,
            last_hidden_state=outputs.last_hidden_state,
            hidden_states=outputs.hidden_states if output_hidden_states else None,
        )


__all__ = ["PPLCNetForImageClassification", "PPLCNetModel", "PPLCNetPreTrainedModel"]
