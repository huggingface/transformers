#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/vibevoice_acoustic_tokenizer/modular_vibevoice_acoustic_tokenizer.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_vibevoice_acoustic_tokenizer.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# coding=utf-8
# Copyright 2025 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from dataclasses import dataclass
from typing import Optional

import torch
import torch.nn as nn

from ...activations import ACT2FN
from ...integrations import use_kernel_forward_from_hub
from ...modeling_utils import PreTrainedModel
from ...utils import ModelOutput, auto_docstring, can_return_tuple
from .configuration_vibevoice_acoustic_tokenizer import VibeVoiceAcousticTokenizerConfig


@dataclass
@auto_docstring
class VibeVoiceAcousticTokenizerOutput(ModelOutput):
    """
    audio (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):
        Projected latents (continuous representations for acoustic tokens) at the output of the encoder.
    latents (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):
        Projected latents (continuous representations for acoustic tokens) at the output of the encoder.
    padding_cache (`VibeVoiceConv1dCache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
        A [`VibeVoiceConv1dCache`] instance containing cached convolution states for each layer that
        can be passed to subsequent forward calls.
    """

    audio: Optional[torch.FloatTensor] = None
    latents: Optional[torch.FloatTensor] = None
    padding_cache: Optional["VibeVoiceConv1dCache"] = None


@dataclass
@auto_docstring
class VibeVoiceAcousticTokenizerEncoderOutput(ModelOutput):
    """
    latents (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):
        Projected latents (continuous representations for semantic tokens) at the output of the encoder.
    padding_cache (`VibeVoiceConv1dCache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
        A [`VibeVoiceConv1dCache`] instance containing cached convolution states for each layer that
        can be passed to subsequent forward calls.
    """

    latents: Optional[torch.FloatTensor] = None
    padding_cache: Optional["VibeVoiceConv1dCache"] = None


@dataclass
@auto_docstring
class VibeVoiceAcousticTokenizerDecoderOutput(ModelOutput):
    """
    audio (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):
        Projected latents (continuous representations for acoustic tokens) at the output of the encoder.
    padding_cache (`VibeVoiceConv1dCache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
        A [`VibeVoiceConv1dCache`] instance containing cached convolution states for each layer that
        can be passed to subsequent forward calls.
    """

    audio: Optional[torch.FloatTensor] = None
    padding_cache: Optional["VibeVoiceConv1dCache"] = None


class VibeVoiceConv1dCache:
    """
    Similar to Mimi's Cache: https://github.com/huggingface/transformers/blob/cad7eeeb5e8a173f8d7d746ccdb6ef670ffe6be4/src/transformers/models/mimi/modeling_mimi.py#L76
    But with:
    - `batch_mask` support for selective cache updates
    - different logic for Conv1d and ConvTranspose1d layers as the Decoder mixes both types

    Original (uses unique key per layer and sample): https://github.com/pengzhiliang/transformers/blob/6e6e60fb95ca908feb0b039483adcc009809f579/src/transformers/models/vibevoice/modular_vibevoice_tokenizer.py#L174
    """

    def __init__(self, num_layers: int):
        self.cache = [None] * num_layers

    def is_empty(self, layer_idx: int) -> bool:
        return self.cache[layer_idx] is None

    def update(
        self,
        layer_idx: int,
        context: int,
        hidden_states: torch.Tensor,
        batch_mask: Optional[torch.Tensor] = None,
        is_transpose: bool = False,
    ):
        """
        Update the cache for the given layer.

        Parameters:
            layer_idx (`int`): Index of the layer.
            hidden_states (`torch.Tensor`): Shape (batch_size, channels, length)
            batch_mask (`torch.LongTensor`, optional): Indices of samples to update.
            context (`int`): Amount of context to maintain.
            is_transpose (`bool`): Use ConvTranspose1d logic if True, else Conv1d logic.

        Returns:
            `torch.Tensor`: The current cached states for the specified samples.
        """
        batch_size, channels, _ = hidden_states.shape

        if batch_mask is None:
            batch_mask = torch.arange(batch_size, device=hidden_states.device)
        else:
            if len(batch_mask) != batch_size:
                raise ValueError("batch_mask length must match batch size")

        # Get existing cache or initialize if None
        existing_cache = self.cache[layer_idx]
        if existing_cache is None:
            if is_transpose:
                # https://github.com/pengzhiliang/transformers/blob/6e6e60fb95ca908feb0b039483adcc009809f579/src/transformers/models/vibevoice/modular_vibevoice_tokenizer.py#L471
                current_cache = torch.zeros(
                    batch_size, channels, 0, device=hidden_states.device, dtype=hidden_states.dtype
                )
            else:
                # https://github.com/pengzhiliang/transformers/blob/6e6e60fb95ca908feb0b039483adcc009809f579/src/transformers/models/vibevoice/modular_vibevoice_tokenizer.py#L319
                current_cache = torch.zeros(
                    batch_size, channels, max(context, 0), device=hidden_states.device, dtype=hidden_states.dtype
                )
        else:
            current_cache = existing_cache[batch_mask]

        # Update cache
        input_with_context = torch.cat([current_cache, hidden_states], dim=-1)
        if is_transpose:
            # https://github.com/pengzhiliang/transformers/blob/6e6e60fb95ca908feb0b039483adcc009809f579/src/transformers/models/vibevoice/modular_vibevoice_tokenizer.py#L519
            if input_with_context.shape[2] > context:
                new_cache = input_with_context[..., -context:]
            else:
                new_cache = input_with_context
        else:
            # https://github.com/pengzhiliang/transformers/blob/6e6e60fb95ca908feb0b039483adcc009809f579/src/transformers/models/vibevoice/modular_vibevoice_tokenizer.py#L345
            if context > 0:
                new_cache = input_with_context[..., -context:]
            else:
                new_cache = torch.empty(
                    batch_size, channels, 0, device=hidden_states.device, dtype=hidden_states.dtype
                )

        if existing_cache is None:
            self.cache[layer_idx] = new_cache
        else:
            if new_cache.shape[-1] != self.cache[layer_idx].shape[-1]:
                self.cache[layer_idx] = new_cache
            else:
                self.cache[layer_idx][batch_mask] = new_cache
        return current_cache


@use_kernel_forward_from_hub("RMSNorm")
class VibeVoiceAcousticTokenizerRMSNorm(nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        """
        VibeVoiceAcousticTokenizerRMSNorm is equivalent to T5LayerNorm
        """
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states):
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states.to(input_dtype)

    def extra_repr(self):
        return f"{tuple(self.weight.shape)}, eps={self.variance_epsilon}"


class VibeVoiceCausalConvTranspose1d(nn.Module):
    """ConvTranspose1d with built-in causal padding and optional streaming support through a cache."""

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int = 1,
        bias: bool = True,
        layer_idx: Optional[int] = None,
    ):
        super().__init__()
        self.convtr = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride, bias=bias)

        # For streaming, we need to keep track of input history
        # Transposed conv needs to see multiple input samples to produce correct output
        self.context_size = kernel_size - stride
        self.stride = stride
        self.layer_idx = layer_idx

    def forward(
        self,
        hidden_states: torch.Tensor,
        padding_cache: Optional[VibeVoiceConv1dCache] = None,
        batch_mask: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        """
        Forward pass with optional streaming support via cache.
        """
        time_dim = hidden_states.shape[-1]

        # Get input for convolution (with context for streaming)
        if padding_cache is not None:
            first_chunk = padding_cache.is_empty(self.layer_idx)
            layer_padding = padding_cache.update(
                self.layer_idx, self.context_size, hidden_states, batch_mask, is_transpose=True
            )
            hidden_states = torch.cat([layer_padding, hidden_states], dim=-1)

        # Apply transposed convolution
        hidden_states = self.convtr(hidden_states)

        # Truncate for both streaming and non-streaming
        # - streaming: https://github.com/pengzhiliang/transformers/blob/6e6e60fb95ca908feb0b039483adcc009809f579/src/transformers/models/vibevoice/modular_vibevoice_tokenizer.py#L497
        # - non-streaming: https://github.com/pengzhiliang/transformers/blob/6e6e60fb95ca908feb0b039483adcc009809f579/src/transformers/models/vibevoice/modular_vibevoice_tokenizer.py#L551
        if self.context_size > 0:
            hidden_states = hidden_states[..., : -self.context_size]

        if padding_cache is not None and not first_chunk:
            # return only new audio: https://github.com/pengzhiliang/transformers/blob/6e6e60fb95ca908feb0b039483adcc009809f579/src/transformers/models/vibevoice/modular_vibevoice_tokenizer.py#L507
            expected_new_output = time_dim * self.stride
            if hidden_states.shape[2] >= expected_new_output:
                hidden_states = hidden_states[..., -expected_new_output:]

        return hidden_states


class VibeVoiceAcousticTokenizerDecoder(nn.Module):
    """
    Decoder component for the VibeVoice tokenizer that converts latent representations back to audio.

    Args:
        config: Configuration object with model parameters
    """

    def __init__(self, config):
        super().__init__()

        # stem and upsampling layers
        self.upsample_layers = nn.ModuleList()
        self.upsample_layers.append(
            VibeVoiceCausalConv1d(
                in_channels=config.hidden_size,
                out_channels=config.n_filters * 2 ** (len(config.decoder_depths) - 1),
                kernel_size=config.kernel_size,
                bias=config.bias,
                layer_idx=0,
            )
        )
        for stage_idx in range(len(config.upsampling_ratios)):
            input_channels = config.n_filters * (2 ** (len(config.decoder_depths) - 1 - stage_idx))
            output_channels = config.n_filters * (2 ** (len(config.decoder_depths) - 1 - stage_idx - 1))
            upsample_layer = VibeVoiceCausalConvTranspose1d(
                input_channels,
                output_channels,
                kernel_size=config.upsampling_ratios[stage_idx] * 2,
                stride=config.upsampling_ratios[stage_idx],
                bias=config.bias,
                layer_idx=stage_idx + 1,
            )
            self.upsample_layers.append(upsample_layer)

        # configure ConvNext1D blocks
        self.stages = nn.ModuleList()
        for stage_idx in range(len(config.decoder_depths)):
            input_channels = config.n_filters * (2 ** (len(config.decoder_depths) - 1 - stage_idx))
            stage = nn.ModuleList(
                [
                    VibeVoiceConvNext1dLayer(config, hidden_size=input_channels)
                    for _ in range(config.decoder_depths[stage_idx])
                ]
            )
            self.stages.append(stage)

        self.head = VibeVoiceCausalConv1d(
            in_channels=input_channels,
            out_channels=config.channels,
            kernel_size=config.kernel_size,
            bias=config.bias,
            layer_idx=len(config.upsampling_ratios) + 1,
        )

    def forward(self, hidden_states, padding_cache=None, batch_mask=None):
        for layer_idx, upsample_layer in enumerate(self.upsample_layers):
            hidden_states = upsample_layer(hidden_states, padding_cache=padding_cache, batch_mask=batch_mask)
            for block in self.stages[layer_idx]:
                hidden_states = block(hidden_states)
        hidden_states = self.head(hidden_states, padding_cache=padding_cache, batch_mask=batch_mask)
        return hidden_states


@auto_docstring
class VibeVoiceAcousticTokenizerPreTrainedModel(PreTrainedModel):
    config: VibeVoiceAcousticTokenizerConfig
    base_model_prefix = "vibevoice_acoustic_tokenizer"
    main_input_name = "audio"
    _supports_flash_attn_2 = True
    _supports_sdpa = True
    _no_split_modules = ["VibeVoiceAcousticTokenizerEncoder", "VibeVoiceAcousticTokenizerDecoder"]

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.normal_(module.weight, std=self.config.weight_init_value)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Conv1d):
            nn.init.normal_(module.weight, std=self.config.weight_init_value)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, nn.ConvTranspose1d):
            nn.init.normal_(module.weight, std=self.config.weight_init_value)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, VibeVoiceAcousticTokenizerRMSNorm):
            nn.init.ones_(module.weight)


@use_kernel_forward_from_hub("RMSNorm")
class VibeVoiceRMSNorm(nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        """
        VibeVoiceRMSNorm is equivalent to T5LayerNorm
        """
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states):
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states.to(input_dtype)

    def extra_repr(self):
        return f"{tuple(self.weight.shape)}, eps={self.variance_epsilon}"


class VibeVoiceEncoderFeedForward(nn.Module):
    def __init__(self, config, hidden_size):
        super().__init__()
        self.linear1 = nn.Linear(hidden_size, config.ffn_expansion * hidden_size, bias=config.bias)
        self.activation = ACT2FN[config.hidden_act]
        self.linear2 = nn.Linear(config.ffn_expansion * hidden_size, hidden_size, bias=config.bias)

    def forward(self, hidden_states):
        return self.linear2(self.activation(self.linear1(hidden_states)))


class VibeVoiceCausalConv1d(nn.Module):
    """Conv1d with built-in causal padding and optional streaming support through a cache."""

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int = 1,
        dilation: int = 1,
        groups: int = 1,
        bias: bool = True,
        layer_idx: Optional[int] = None,
    ):
        super().__init__()
        self.conv = nn.Conv1d(
            in_channels, out_channels, kernel_size, stride, dilation=dilation, groups=groups, bias=bias
        )
        # Padding for causality: https://github.com/pengzhiliang/transformers/blob/6e6e60fb95ca908feb0b039483adcc009809f579/src/transformers/models/vibevoice/modular_vibevoice_tokenizer.py#L263C28-L263C72
        self.causal_padding = (kernel_size - 1) * dilation - (stride - 1)
        if self.causal_padding < 0:
            raise ValueError(
                f"Invalid causal padding {self.causal_padding} for kernel_size={kernel_size}, "
                f"dilation={dilation}, stride={stride}."
            )
        self.layer_idx = layer_idx

    def forward(
        self,
        hidden_states: torch.Tensor,
        padding_cache: Optional[VibeVoiceConv1dCache] = None,
        batch_mask: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        """
        Forward pass with optional streaming support via cache.
        Original code: https://github.com/vibevoice-community/VibeVoice/blob/63a21e2b45e908be63765bf312a9ecfb3a588315/vibevoice/modular/modular_vibevoice_tokenizer.py#L296
        """

        if padding_cache is not None:
            layer_padding = padding_cache.update(self.layer_idx, self.causal_padding, hidden_states, batch_mask)
        else:
            # non-streaming mode: https://github.com/pengzhiliang/transformers/blob/6e6e60fb95ca908feb0b039483adcc009809f579/src/transformers/models/vibevoice/modular_vibevoice_tokenizer.py#L365
            layer_padding = torch.zeros(
                hidden_states.shape[0],
                hidden_states.shape[1],
                self.causal_padding,
                device=hidden_states.device,
                dtype=hidden_states.dtype,
            )
        hidden_states = torch.cat([layer_padding, hidden_states], dim=-1)

        return self.conv(hidden_states)


class VibeVoiceConvNext1dLayer(nn.Module):
    """
    ConvNeXt-like block adapted for 1D convolutions, used in VibeVoice tokenizer encoder.

    For reference, original 2D `ConvNextLayer`:
    https://github.com/huggingface/transformers/blob/e20df45bf676d80bdddb9757eeeafe6c0c81ecfa/src/transformers/models/convnext/modeling_convnext.py#L120
    """

    def __init__(self, config, hidden_size, drop_path=0.0, dilation=1, stride=1):
        super().__init__()

        self.norm = VibeVoiceRMSNorm(hidden_size, eps=config.rms_norm_eps)
        self.ffn_norm = VibeVoiceRMSNorm(hidden_size, eps=config.rms_norm_eps)
        self.ffn = VibeVoiceEncoderFeedForward(config, hidden_size)
        self.gamma = nn.Parameter(config.layer_scale_init_value * torch.ones(hidden_size), requires_grad=True)
        self.ffn_gamma = nn.Parameter(config.layer_scale_init_value * torch.ones(hidden_size), requires_grad=True)

        # TODO (ebezzam) original code has option for DropPath but is never actually used (and `nn.modules.DropPath` does not exist):
        # https://github.com/pengzhiliang/transformers/blob/6e6e60fb95ca908feb0b039483adcc009809f579/src/transformers/models/vibevoice/modular_vibevoice_tokenizer.py#L637
        # however, could be interesting feature for future versions of `ConvNext1dLayer` as the 2D version has it:
        # https://github.com/huggingface/transformers/blob/e20df45bf676d80bdddb9757eeeafe6c0c81ecfa/src/transformers/models/convnext/modeling_convnext.py#L146
        if drop_path > 0.0:
            # possible implementation (that may needed to be adapted for 1D):
            # https://github.com/huggingface/transformers/blob/e20df45bf676d80bdddb9757eeeafe6c0c81ecfa/src/transformers/models/convnext/modeling_convnext.py#L40
            raise NotImplementedError("DropPath is not implemented.")
        self.drop_path = nn.Identity()

        self.mixer = VibeVoiceCausalConv1d(
            in_channels=hidden_size,
            out_channels=hidden_size,
            kernel_size=config.kernel_size,
            groups=hidden_size,
            bias=config.bias,
            dilation=dilation,
            stride=stride,
        )

    def forward(self, hidden_states):
        residual = hidden_states
        hidden_states = self.norm(hidden_states.transpose(1, 2)).transpose(1, 2)
        hidden_states = self.mixer(hidden_states)
        hidden_states = hidden_states * self.gamma.unsqueeze(-1)
        # (ebezzam) original code (https://github.com/pengzhiliang/transformers/blob/6e6e60fb95ca908feb0b039483adcc009809f579/src/transformers/models/vibevoice/modular_vibevoice_tokenizer.py#L653)
        # as mentioned above, drop_path is not used and the VibeVoice authors don't use the `forward` method but a custom
        # call which does `residual + hidden_states` directly (see link below), which is same as using identity
        # https://github.com/pengzhiliang/transformers/blob/6e6e60fb95ca908feb0b039483adcc009809f579/src/transformers/models/vibevoice/modular_vibevoice_tokenizer.py#L768
        hidden_states = residual + self.drop_path(hidden_states)

        # ffn
        residual = hidden_states
        hidden_states = self.ffn_norm(hidden_states.transpose(1, 2))  # [B, T, C]
        hidden_states = self.ffn(hidden_states)  # FFN expects [B, T, C]
        hidden_states = hidden_states.transpose(1, 2)  # Back to [B, C, T]
        hidden_states = hidden_states * self.ffn_gamma.unsqueeze(-1)
        # (ebezzam) see comment above
        hidden_states = residual + self.drop_path(hidden_states)
        return hidden_states


class VibeVoiceAcousticTokenizerEncoder(nn.Module):
    """
    Encoder component for the VibeVoice tokenizer that converts audio to latent representations.

    Paper (https://arxiv.org/pdf/2508.19205) says:
    "7 stages of modified Transformer blocks (using 1D depth-wise causal convolutions instead of self-attention module)
    for efficient streaming processing. Six downsampling layers achieve a cumulative 3200X downsampling rate from a
    24kHz input, yielding 7.5 tokens/frames per second."

    But each block is more like a ConvNeXt block (but 1D): https://arxiv.org/abs/2201.03545
    Hence the name `ConvNext1dLayer` in this code for the blocks.

    Args:
        config: Configuration object with model parameters
    """

    def __init__(self, config):
        super().__init__()

        # stem and intermediate downsampling conv layers
        self.downsample_layers = nn.ModuleList()
        self.downsample_layers.append(
            VibeVoiceCausalConv1d(
                in_channels=config.channels,
                out_channels=config.n_filters,
                kernel_size=config.kernel_size,
                bias=config.bias,
                layer_idx=0,
            )
        )
        for stage_idx in range(len(config.downsampling_ratios)):
            downsample_layer = VibeVoiceCausalConv1d(
                in_channels=config.n_filters * (2**stage_idx),
                out_channels=config.n_filters * (2 ** (stage_idx + 1)),
                kernel_size=config.downsampling_ratios[stage_idx] * 2,
                stride=config.downsampling_ratios[stage_idx],
                bias=config.bias,
                layer_idx=stage_idx + 1,
            )
            self.downsample_layers.append(downsample_layer)

        # configure ConvNext1D blocks
        self.stages = nn.ModuleList()
        for stage_idx in range(len(config.depths)):
            input_channels = config.n_filters * (2**stage_idx)
            stage = nn.ModuleList(
                [VibeVoiceConvNext1dLayer(config, hidden_size=input_channels) for _ in range(config.depths[stage_idx])]
            )
            self.stages.append(stage)

        self.head = VibeVoiceCausalConv1d(
            in_channels=input_channels,
            out_channels=config.hidden_size,
            kernel_size=config.kernel_size,
            bias=config.bias,
            layer_idx=len(config.downsampling_ratios) + 1,
        )

    def forward(self, hidden_states, padding_cache=None, batch_mask=None):
        for layer_idx, downsample_layer in enumerate(self.downsample_layers):
            hidden_states = downsample_layer(hidden_states, padding_cache=padding_cache, batch_mask=batch_mask)
            for block in self.stages[layer_idx]:
                hidden_states = block(hidden_states)
        hidden_states = self.head(hidden_states, padding_cache=padding_cache, batch_mask=batch_mask)
        return hidden_states.permute(0, 2, 1)


@auto_docstring
class VibeVoiceAcousticTokenizerModel(VibeVoiceAcousticTokenizerPreTrainedModel):
    """VibeVoice speech tokenizer model combining encoder and decoder for acoustic tokens"""

    def __init__(self, config):
        super().__init__(config)

        self.encoder = VibeVoiceAcousticTokenizerEncoder(config)
        self.decoder = VibeVoiceAcousticTokenizerDecoder(config)
        self.vae_std = config.vae_std

        # Initialize weights
        self.post_init()

    @can_return_tuple
    @auto_docstring
    def encode(self, audio, padding_cache=None, batch_mask=None, use_cache=False, sample=True):
        r"""
        audio (`torch.FloatTensor` of shape `(batch_size, channels, sequence_length)`):
            Input audio waveform to be encoded into latent representations.
        padding_cache (`VibeVoiceConv1dCache`, *optional*):
            Cache object for streaming mode to maintain convolution states across layers.
        batch_mask (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
            Indices identifying each sample in the batch for cache management.
        use_cache (`bool`, *optional*):
            Whether to use caching for convolution states.
        sample (`bool`, *optional*):
            Whether to sample from the output distribution or return the latent as is.
        """

        if use_cache and padding_cache is None:
            padding_cache = VibeVoiceConv1dCache(num_layers=len(self.config.downsampling_ratios) + 2)

        latents = self.encoder(audio, padding_cache=padding_cache, batch_mask=batch_mask)

        if sample:
            batch_size = audio.shape[0]
            noise_std = self.vae_std * torch.ones(batch_size, device=latents.device, dtype=latents.dtype)
            while noise_std.dim() < latents.dim():
                noise_std = noise_std.unsqueeze(-1)
            latents = latents + noise_std * torch.randn_like(latents)

        return VibeVoiceAcousticTokenizerEncoderOutput(
            latents=latents,
            padding_cache=padding_cache if use_cache else None,
        )

    @can_return_tuple
    @auto_docstring
    def forward(self, audio, padding_cache=None, batch_mask=None, use_cache=False, sample=True):
        r"""
        audio (`torch.FloatTensor` of shape `(batch_size, channels, sequence_length)`):
            Input audio waveform to be encoded into latent representations.
        padding_cache (`VibeVoiceConv1dCache`, *optional*):
            Cache object for streaming mode to maintain convolution states across layers. Note only used by decoder.
        batch_mask (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
            Indices identifying each sample in the batch for cache management.
        use_cache (`bool`, *optional*):
            Whether to use caching for convolution states.
        sample (`bool`, *optional*):
            Whether to sample from the output distribution of the encoder, or return the latent as is.
        """
        encoder_output = self.encode(audio, sample=sample)
        decoder_output = self.decode(
            encoder_output.latents, padding_cache=padding_cache, batch_mask=batch_mask, use_cache=use_cache
        )
        return VibeVoiceAcousticTokenizerOutput(
            audio=decoder_output.audio,
            latents=encoder_output.latents,
            padding_cache=decoder_output.padding_cache,
        )

    @can_return_tuple
    @auto_docstring
    def decode(self, latents, padding_cache=None, batch_mask=None, use_cache=False):
        r"""
        latents (`torch.FloatTensor` of shape `(batch_size, channels, sequence_length)`):
            Input latent representations to be decoded back into audio waveforms.
        padding_cache (`VibeVoiceConv1dCache`, *optional*):
            Cache object for streaming mode to maintain convolution states across layers.
        batch_mask (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
            Indices identifying each sample in the batch for cache management.
        use_cache (`bool`, *optional*):
            Whether to use caching for convolution states.
        """

        if use_cache and padding_cache is None:
            padding_cache = VibeVoiceConv1dCache(num_layers=len(self.config.upsampling_ratios) + 2)

        latents = latents.permute(0, 2, 1)
        audio = self.decoder(latents, padding_cache=padding_cache, batch_mask=batch_mask)
        return VibeVoiceAcousticTokenizerDecoderOutput(audio=audio, padding_cache=padding_cache)


__all__ = ["VibeVoiceAcousticTokenizerModel"]
