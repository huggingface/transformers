#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/videoprism/modular_videoprism.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_videoprism.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
import math
from collections.abc import Callable
from dataclasses import dataclass

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.init import _calculate_fan_in_and_fan_out

from ... import initialization as init
from ...activations import ACT2FN
from ...masking_utils import create_causal_mask
from ...modeling_layers import GradientCheckpointingLayer
from ...modeling_outputs import BaseModelOutput, ImageClassifierOutput
from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
from ...processing_utils import Unpack
from ...utils import ModelOutput, TransformersKwargs, auto_docstring, torch_int
from .configuration_videoprism import VideoPrismConfig, VideoPrismTextConfig, VideoPrismVisionConfig


@dataclass
class BaseModelOutputWithSpatialAndTemporalStates(ModelOutput):
    """
    Base class for model outputs that include spatial and temporal states.

    Args:
        last_hidden_state (Optional[torch.FloatTensor]):
            The last hidden state of the model, typically of shape
            (batch_size, num_patches * num_frames, hidden_size).

        temporal_hidden_state (Optional[torch.FloatTensor]):
            The last hidden_state of the temporal encoder, typically of shape
            (batch_size * num_patches, num_frames, hidden_size).

        spatial_hidden_state (Optional[torch.FloatTensor]):
            The last hidden_state of the spatial encoder, typically of shape
            (batch_size * num_frames, num_patches, hidden_size).
    """

    last_hidden_state: torch.FloatTensor | None = None
    temporal_hidden_state: torch.FloatTensor | None = None
    spatial_hidden_state: torch.FloatTensor | None = None


@dataclass
class VideoPrismClipOutput(ModelOutput):
    """
    Base class for VideoPrismClip model outputs.
    """

    logits_per_video: torch.FloatTensor | None = None
    logits_per_text: torch.FloatTensor | None = None
    video_embeds: torch.FloatTensor | None = None
    text_embeds: torch.FloatTensor | None = None


@dataclass
class VideoPrismVideoOutput(ModelOutput):
    """
    Base class for VideoPrismVideo model outputs.
    """

    video_last_hidden_state: torch.FloatTensor | None = None
    auxiliary_output: torch.FloatTensor | None = None
    attention_pooling_output: torch.FloatTensor | None = None


class VideoPrismTubeletEmbeddings(nn.Module):
    """
    Construct VideoPrism Tubelet embeddings.

    This module turns a batch of videos of shape (batch_size, num_frames, num_channels, height, width) into a tensor of
    shape (batch_size, seq_len, hidden_size) to be consumed by a Transformer encoder.

    The seq_len (the number of patches) equals (number of frames // tubelet_size[0]) * (height // tubelet_size[1]) *
    (width // tubelet_size[2]).
    """

    def __init__(self, config: VideoPrismVisionConfig):
        super().__init__()
        self.config = config
        self.num_frames = config.num_frames
        self.image_size = (
            config.image_size
            if isinstance(self.config.image_size, tuple)
            else (self.config.image_size, self.config.image_size)
        )
        self.patch_size = config.tubelet_size
        self.embed_dim = config.hidden_size

        self.projection = nn.Conv3d(
            config.num_channels, config.hidden_size, kernel_size=config.tubelet_size, stride=config.tubelet_size
        )
        self.pos_emb_shape = [self.image_size[0] // self.patch_size[1], self.image_size[1] // self.patch_size[2]]
        self.num_patches = self.pos_emb_shape[0] * self.pos_emb_shape[1]

    def forward(self, pixel_values_videos: torch.Tensor, interpolate_pos_encoding: bool = False) -> torch.Tensor:
        batch_size, num_frames, num_channels, height, width = pixel_values_videos.shape
        if not interpolate_pos_encoding and (height != self.image_size[0] or width != self.image_size[1]):
            raise ValueError(
                f"Image size ({height}*{width}) doesn't match model ({self.image_size[0]}*{self.image_size[1]}). Set interpolate_pos_encoding=True to automatically resize the model position embeddings."
            )
        # permute to (batch_size, num_channels, num_frames, height, width)
        pixel_values_videos = pixel_values_videos.permute(0, 2, 1, 3, 4)

        hidden_states = self.projection(pixel_values_videos)
        # flatten the spatial part and permute to (B, T, num_patches, dim)
        hidden_states = hidden_states.flatten(3).permute(0, 2, 3, 1)
        # combine batch and time dimension
        batch_size, num_frames, num_patches, hidden_size = hidden_states.shape
        hidden_states = hidden_states.reshape(batch_size * num_frames, num_patches, hidden_size)

        return hidden_states


class VideoPrismSpatialEmbeddings(nn.Module):
    """
    VideoPrism Spatial Embeddings.

    Creates embeddings from a video using VideoPrismSpatialTubeletEmbeddings and adds positional embeddings.
    """

    def __init__(self, config: VideoPrismVisionConfig):
        super().__init__()
        self.config = config
        self.patch_embeddings = VideoPrismTubeletEmbeddings(config)
        self.position_embeddings = nn.Parameter(torch.zeros(1, self.patch_embeddings.num_patches, config.hidden_size))
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.patch_size = config.tubelet_size[1:]
        self.tubelet_size = config.tubelet_size

    # Adapted from transformers.models.vit.modeling_vit.ViTEmbeddings.interpolate_pos_encoding
    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:
        """
        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution
        images. This method is also adapted to support torch.jit tracing.

        Adapted from:
        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and
        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211
        """

        num_patches = embeddings.shape[1]
        num_positions = self.position_embeddings.shape[1]

        # always interpolate when tracing to ensure the exported model works for dynamic input shapes
        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:
            return self.position_embeddings

        dim = embeddings.shape[-1]

        num_row_patches = height // self.patch_size[0]
        num_col_patches = width // self.patch_size[1]

        sqrt_num_positions = torch_int(num_positions**0.5)
        patch_pos_embed = self.position_embeddings.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)
        patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)

        patch_pos_embed = nn.functional.interpolate(
            patch_pos_embed,
            size=(num_row_patches, num_col_patches),
            mode="bilinear",
            antialias=True,
        )

        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)
        return patch_pos_embed

    def forward(
        self, pixel_values_videos: torch.Tensor, interpolate_pos_encoding: bool | None = False
    ) -> torch.Tensor:
        b, t, c, h, w = pixel_values_videos.shape
        assert h == w, "Input image height and width must be the same"
        embeddings = self.patch_embeddings(pixel_values_videos, interpolate_pos_encoding)

        # add positional encoding to each token
        if interpolate_pos_encoding:
            embeddings = embeddings + self.interpolate_pos_encoding(embeddings, h, w)
        else:
            embeddings = embeddings + self.position_embeddings

        embeddings = self.dropout(embeddings)

        return embeddings


class VideoPrismTemporalEmbeddings(nn.Module):
    """
    VideoPrism Temporal Embeddings.

    Receives embeddings from spatial encoder, reshapes the hidden state to
    (batch_size * num_patches, num_frames, hidden_size) and adds positional embeddings.
    """

    def __init__(self, config: VideoPrismVisionConfig):
        super().__init__()
        self.config = config

        self.position_embeddings = nn.Parameter(torch.zeros(1, self.config.num_frames, config.hidden_size))
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    # Adapted from transformers.models.vit.modeling_vit.ViTEmbeddings.interpolate_pos_encoding
    def interpolate_pos_encoding(self, embeddings: torch.Tensor) -> torch.Tensor:
        """
        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution
        images. This method is also adapted to support torch.jit tracing.

        Adapted from:
        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and
        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211
        """
        target_emb_length = embeddings.shape[1]
        source_emb_length = self.position_embeddings.shape[1]

        # always interpolate when tracing to ensure the exported model works for dynamic input shapes
        if not torch.jit.is_tracing() and target_emb_length == source_emb_length:
            return self.position_embeddings

        source_emb = self.position_embeddings
        dim = embeddings.shape[-1]
        source_emb = source_emb.unsqueeze(1)
        source_emb = nn.functional.interpolate(
            source_emb,
            size=(target_emb_length, dim),
            mode="bilinear",
            antialias=True,
        )

        return source_emb.squeeze(1)

    def forward(
        self,
        pixel_values_videos: torch.Tensor,
        input_shape: torch.Size,
        interpolate_pos_encoding: bool | None = False,
    ) -> torch.Tensor:
        if input_shape is not None:
            b, t, c, h, w = input_shape
        _, features, dim = pixel_values_videos.shape
        hidden_states = pixel_values_videos.view(b, t, features, dim)
        hidden_states = hidden_states.permute(0, 2, 1, 3)
        embeddings = hidden_states.reshape(b * features, t, dim)

        # add positional encoding to each token
        if interpolate_pos_encoding:
            embeddings = embeddings + self.interpolate_pos_encoding(embeddings)
        else:
            embeddings = embeddings + self.position_embeddings
        embeddings = self.dropout(embeddings)
        return embeddings


def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: torch.Tensor | None,
    scaling: float,
    dropout: float = 0.0,
    softcap: float | None = None,
    **kwargs: Unpack[TransformersKwargs],
):
    # Take the dot product between "query" and "key" to get the raw attention scores.
    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling

    if softcap is not None:
        attn_weights = attn_weights / softcap
        attn_weights = torch.tanh(attn_weights)
        attn_weights = attn_weights * softcap
    if attention_mask is not None:
        attn_weights = attn_weights + attention_mask.expand(*attn_weights.shape)

    # Normalize the attention scores to probabilities.
    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
    attn_output = torch.matmul(attn_weights, value)
    attn_output = attn_output.transpose(1, 2).contiguous()
    return attn_output, attn_weights


class VideoPrismSelfAttention(nn.Module):
    def __init__(self, config: VideoPrismVisionConfig | VideoPrismTextConfig):
        super().__init__()
        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, "embedding_size"):
            raise ValueError(
                f"The hidden size {config.hidden_size} is not a multiple of the number of attention "
                f"heads {config.num_attention_heads}."
            )

        self.config = config
        self.num_attention_heads = config.num_attention_heads
        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)
        self.all_head_size = self.num_attention_heads * self.attention_head_size
        self.dropout_prob = config.attention_probs_dropout_prob
        self.scale = self.attention_head_size**-0.5
        self.query = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)
        self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)
        self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: torch.Tensor | None,
        **kwargs: Unpack[TransformersKwargs],
    ) -> tuple[torch.Tensor, torch.Tensor]:
        batch_size = hidden_states.shape[0]
        new_shape = batch_size, -1, self.num_attention_heads, self.attention_head_size
        query = self.query(hidden_states).view(*new_shape).transpose(1, 2)
        key = self.key(hidden_states).view(*new_shape).transpose(1, 2)
        value = self.value(hidden_states).view(*new_shape).transpose(1, 2)

        attention_interface: Callable = eager_attention_forward
        if self.config._attn_implementation != "eager":
            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

        context_layer, attention_probs = attention_interface(
            self,
            query,
            key,
            value,
            attention_mask,
            scaling=self.scale,
            dropout=0.0 if not self.training else self.dropout_prob,
            softcap=self.config.attn_logit_softcapping,
            **kwargs,
        )

        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
        context_layer = context_layer.reshape(new_context_layer_shape)

        return (context_layer, attention_probs)


class VideoPrismSelfOutput(nn.Module):
    """
    The residual connection is defined in VideoPrismLayer instead of here (as is the case with other models), due to the
    layernorm applied before each block.
    """

    def __init__(self, config: VideoPrismConfig):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states)
        return hidden_states


class VideoPrismAttention(nn.Module):
    def __init__(self, config: VideoPrismConfig):
        super().__init__()
        self.attention = VideoPrismSelfAttention(config)
        self.output = VideoPrismSelfOutput(config)

    def forward(
        self, hidden_states: torch.Tensor, attention_mask: torch.Tensor, **kwargs: Unpack[TransformersKwargs]
    ) -> torch.Tensor:
        self_attn_output, _ = self.attention(hidden_states, attention_mask, **kwargs)
        output = self.output(self_attn_output, hidden_states)
        return output


class VideoPrismLayerNorm(nn.LayerNorm):
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        return F.layer_norm(hidden_states, self.normalized_shape, self.weight + 1, self.bias, self.eps)


class VideoPrismIntermediate(nn.Module):
    def __init__(self, config: VideoPrismConfig):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        if isinstance(config.hidden_act, str):
            self.intermediate_act_fn = ACT2FN[config.hidden_act]
        else:
            self.intermediate_act_fn = config.hidden_act

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        hidden_states = self.dense(hidden_states)
        hidden_states = self.intermediate_act_fn(hidden_states)
        hidden_states = self.dropout(hidden_states)

        return hidden_states


class VideoPrismOutput(nn.Module):
    def __init__(self, config: VideoPrismConfig):
        super().__init__()
        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = hidden_states + input_tensor
        return hidden_states


class VideoPrismLayer(GradientCheckpointingLayer):
    """This corresponds to the EncoderBlock class in the scenic/videoprism implementation."""

    def __init__(self, config: VideoPrismVisionConfig | VideoPrismTextConfig):
        super().__init__()
        self.config = config
        self.attention = VideoPrismAttention(config)
        self.intermediate = VideoPrismIntermediate(config)
        self.output = VideoPrismOutput(config)
        self.layernorm_before = VideoPrismLayerNorm(self.config.hidden_size, eps=self.config.layer_norm_eps)
        self.layernorm_after = VideoPrismLayerNorm(self.config.hidden_size, eps=self.config.layer_norm_eps)

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: torch.Tensor | None = None,
        **kwargs: Unpack[TransformersKwargs],
    ) -> torch.Tensor:
        hidden_states_norm = self.layernorm_before(hidden_states)
        attention_output = self.attention(hidden_states_norm, attention_mask, **kwargs)

        # first residual connection
        hidden_states = attention_output + hidden_states

        # in VideoPrism, layernorm is also applied after self-attention
        layer_output = self.layernorm_after(hidden_states)
        layer_output = self.intermediate(layer_output)

        # second residual connection is done here
        layer_output = self.output(layer_output, hidden_states)

        return layer_output


class VideoPrismSpatialEncoder(nn.Module):
    def __init__(self, config: VideoPrismVisionConfig):
        super().__init__()
        self.config = config
        self.layer = nn.ModuleList([VideoPrismLayer(config) for _ in range(config.num_spatial_layers)])
        self.gradient_checkpointing = False

    def forward(self, hidden_states: torch.Tensor) -> BaseModelOutput:
        for i, layer_module in enumerate(self.layer):
            hidden_states = layer_module(hidden_states)

        return BaseModelOutput(last_hidden_state=hidden_states)


class VideoPrismTemporalEncoder(nn.Module):
    def __init__(self, config: VideoPrismVisionConfig):
        super().__init__()
        self.config = config
        self.layer = nn.ModuleList([VideoPrismLayer(config) for _ in range(config.num_temporal_layers)])
        self.gradient_checkpointing = False

    def forward(self, hidden_states: torch.Tensor) -> BaseModelOutput:
        for i, layer_module in enumerate(self.layer):
            hidden_states = layer_module(hidden_states)

        return BaseModelOutput(last_hidden_state=hidden_states)


class VideoPrismAuxiliaryEncoder(nn.Module):
    def __init__(self, config: VideoPrismVisionConfig):
        super().__init__()
        self.config = config
        self.layer = nn.ModuleList([VideoPrismLayer(self.config) for _ in range(config.num_auxiliary_layers)])
        self.gradient_checkpointing = False

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: torch.Tensor | None = None,
        **kwargs: Unpack[TransformersKwargs],
    ) -> BaseModelOutput:
        for i, layer_module in enumerate(self.layer):
            hidden_states = layer_module(hidden_states, attention_mask, **kwargs)

        return BaseModelOutput(last_hidden_state=hidden_states)


class VideoPrismTextEncoder(nn.Module):
    def __init__(self, config: VideoPrismTextConfig):
        super().__init__()
        self.config = config
        self.layer = nn.ModuleList([VideoPrismLayer(config) for _ in range(config.num_text_layers)])
        self.gradient_checkpointing = False

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: torch.Tensor | None = None,
        **kwargs: Unpack[TransformersKwargs],
    ) -> BaseModelOutput:
        for i, layer_module in enumerate(self.layer):
            hidden_states = layer_module(hidden_states, attention_mask, **kwargs)

        return BaseModelOutput(last_hidden_state=hidden_states)


def variance_scaling_(tensor, mode="fan_in", distribution="normal"):
    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)
    if mode == "fan_in":
        denom = fan_in
    elif mode == "fan_out":
        denom = fan_out
    elif mode == "fan_avg":
        denom = (fan_in + fan_out) / 2

    variance = 1.0 / denom

    if distribution == "truncated_normal":
        init.trunc_normal_(tensor, std=math.sqrt(variance) / 0.87962566103423978)
    elif distribution == "normal":
        init.normal_(tensor, std=math.sqrt(variance))
    elif distribution == "uniform":
        bound = math.sqrt(3 * variance)
        init.uniform_(tensor, -bound, bound)
    else:
        raise ValueError(f"invalid distribution {distribution}")


def lecun_normal_(tensor):
    variance_scaling_(tensor, mode="fan_in", distribution="truncated_normal")


@auto_docstring
class VideoPrismPreTrainedModel(PreTrainedModel):
    config: VideoPrismConfig
    base_model_prefix = "videoprism"
    main_input_name = "pixel_values_videos"
    input_modalities = ("video", "text")
    supports_gradient_checkpointing = True
    _no_split_modules = [
        "VideoPrismSpatialEmbeddings",
        "VideoPrismTemporalEmbeddings",
        "VideoPrismSpatialEncoder",
        "VideoPrismTemporalEncoder",
        "VideoPrismAuxiliaryEncoder",
        "VideoPrismTextEncoder",
        "VideoPrismMultiheadAttentionPoolingHead",
    ]
    _supports_sdpa = True
    _supports_flash_attn = True
    _supports_attention_backend = True
    _supports_flex_attention = True

    def _init_weights(self, module):
        if isinstance(module, (nn.Linear, nn.Conv3d)):
            lecun_normal_(module.weight)
            init.zeros_(module.bias)

        elif isinstance(module, nn.LayerNorm):
            init.zeros_(module.bias)
            init.ones_(module.weight)


@auto_docstring(
    custom_intro="""
    The bare VideoPrism vision encoder outputting raw hidden-states without any specific head on top. This model is the backbone encoder used in VideoPrismVideoModel.
    """
)
class VideoPrismVisionModel(VideoPrismPreTrainedModel):
    config: VideoPrismVisionConfig

    def __init__(self, config: VideoPrismVisionConfig):
        super().__init__(config)
        self.config = config
        self.layernorm1 = VideoPrismLayerNorm(self.config.hidden_size, eps=self.config.layer_norm_eps)
        self.layernorm2 = VideoPrismLayerNorm(self.config.hidden_size, eps=self.config.layer_norm_eps)
        self.spatial_embeddings = VideoPrismSpatialEmbeddings(self.config)
        self.temporal_embeddings = VideoPrismTemporalEmbeddings(self.config)
        self.spatial_encoder = VideoPrismSpatialEncoder(self.config)
        self.temporal_encoder = VideoPrismTemporalEncoder(self.config)
        self.post_init()

    def get_input_embeddings(self):
        return self.spatial_embeddings.patch_embeddings

    @auto_docstring
    def forward(
        self,
        pixel_values_videos: torch.FloatTensor | None = None,
        interpolate_pos_encoding: bool | None = False,
        **kwargs: Unpack[TransformersKwargs],
    ) -> BaseModelOutputWithSpatialAndTemporalStates:
        r"""
        Args:
            pixel_values_videos (`torch.FloatTensor`):
                Pixel values of the video frames of shape (batch_size, num_frames, num_channels, height, width).
            interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):
                Whether to interpolate positional encodings to match input size.

        Example:

        ```python
        >>> from transformers import VideoPrismVideoProcessor, VideoPrismVisionModel
        >>> import torch

        >>> processor = VideoPrismVideoProcessor.from_pretrained("google/videoprism")
        >>> model = VideoPrismVisionModel.from_pretrained("google/videoprism")

        >>> video = "sample_video.mp4"
        >>> inputs = processor(videos=video)
        >>> with torch.no_grad():
        ...     outputs = model(**inputs)
        ...     features = outputs.last_hidden_state
        ```
        """
        if pixel_values_videos is None:
            raise ValueError("You have to specify pixel_values_videos")

        input_shape = pixel_values_videos.shape
        spatial_embeds = self.spatial_embeddings(pixel_values_videos, interpolate_pos_encoding)
        spatial_encoder_outputs: BaseModelOutput = self.spatial_encoder(hidden_states=spatial_embeds, **kwargs)
        # shape of spatial_sequence_output is (B * num_frames, num_patches, dim)
        spatial_sequence_output = spatial_encoder_outputs.last_hidden_state
        features = self.layernorm1(spatial_sequence_output)

        temporal_embeds = self.temporal_embeddings(features, input_shape, interpolate_pos_encoding)
        temporal_encoder_outputs: BaseModelOutput = self.temporal_encoder(hidden_states=temporal_embeds, **kwargs)
        # shape of temporal_sequence_output is (B * num_patches, num_frames, dim)
        temporal_sequence_output = temporal_encoder_outputs.last_hidden_state
        features = self.layernorm2(temporal_sequence_output)
        _, num_frames, dim = features.shape
        features = features.view(input_shape[0], -1, num_frames, dim).permute(0, 2, 1, 3).contiguous()
        _, num_frames, num_patches, dim = features.shape
        features = features.view(input_shape[0], num_frames * num_patches, -1)

        return BaseModelOutputWithSpatialAndTemporalStates(
            last_hidden_state=features,
            temporal_hidden_state=temporal_sequence_output,
            spatial_hidden_state=spatial_sequence_output,
        )


class VideoPrismMultiheadAttentionPoolingHead(nn.Module):
    def __init__(self, config: VideoPrismVisionConfig):
        super().__init__()
        self.config = config
        self.num_attention_heads = self.config.num_attention_heads
        self.attention_head_size = int(self.config.intermediate_size / self.config.num_attention_heads)
        self.all_head_size = self.num_attention_heads * self.attention_head_size
        self.dropout_prob = self.config.attention_probs_dropout_prob
        # PerDimScale
        self.dim = int(self.config.intermediate_size / self.config.num_attention_heads)
        self.per_dim_scale = nn.Parameter(torch.zeros(self.dim))
        r_softplus_0 = 1.442695041
        scale = torch.tensor(r_softplus_0 / (self.dim**0.5))
        softplus = nn.functional.softplus(self.per_dim_scale)
        scale = scale * softplus
        self.register_buffer("scale", scale)

        self.pooling_attention_query = nn.Parameter(torch.zeros(1, 1, self.config.hidden_size))
        self.query = nn.Linear(self.config.hidden_size, self.config.intermediate_size, bias=self.config.qkv_bias)
        self.key = nn.Linear(self.config.hidden_size, self.config.intermediate_size, bias=self.config.qkv_bias)
        self.value = nn.Linear(self.config.hidden_size, self.config.intermediate_size, bias=self.config.qkv_bias)
        self.projection = nn.Linear(self.config.intermediate_size, self.config.hidden_size, bias=self.config.qkv_bias)
        self.layernorm = VideoPrismLayerNorm(self.config.hidden_size, eps=self.config.layer_norm_eps)
        self.dim = int(self.config.intermediate_size / self.config.num_attention_heads)

    def forward(
        self,
        hidden_states: torch.FloatTensor,
        attention_mask: torch.LongTensor | None = None,
        **kwargs: Unpack[TransformersKwargs],
    ) -> tuple[torch.FloatTensor, torch.FloatTensor]:
        batch_size, seq_length, hidden_size = hidden_states.shape
        query = self.pooling_attention_query.expand(batch_size, -1, -1)
        query_layer = (
            self.query(query).view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)
        )
        query_layer = query_layer * self.scale.expand(*query_layer.shape)

        key_layer = (
            self.key(hidden_states)
            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)
            .transpose(1, 2)
        )
        value_layer = (
            self.value(hidden_states)
            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)
            .transpose(1, 2)
        )

        attention_interface: Callable = eager_attention_forward
        if self.config._attn_implementation != "eager":
            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

        context_layer, attention_probs = attention_interface(
            self,
            query_layer,
            key_layer,
            value_layer,
            attention_mask,
            scaling=1.0,
            dropout=0.0 if not self.training else self.dropout_prob,
            softcap=None,
            **kwargs,
        )

        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
        context_layer = context_layer.reshape(new_context_layer_shape)
        outputs = self.projection(context_layer)
        outputs = self.layernorm(outputs)
        return (outputs, attention_probs)


def l2norm(x: torch.FloatTensor, dim: int = -1, eps: float = 1e-6):
    """This function is intended to align with the l2norm implementation in the FLA library."""
    inv_norm = torch.rsqrt((x * x).sum(dim=dim, keepdim=True) + eps)
    return x * inv_norm


@auto_docstring(
    custom_intro="""
    The bare VideoPrism text encoder outputting raw hidden-states without any specific head on top. This model is used in VideoPrismClipModel.
    """
)
class VideoPrismTextModel(VideoPrismPreTrainedModel):
    config: VideoPrismTextConfig

    def __init__(self, config: VideoPrismTextConfig):
        super().__init__(config)
        self.config = config
        self.text_encoder = VideoPrismTextEncoder(self.config)
        self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)
        self.cls_emb = nn.Parameter(torch.zeros(1, 1, config.hidden_size))
        self.layernorm = VideoPrismLayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.normalize = config.apply_l2_norm
        self.post_init()

    def create_sinusoidal_positions(self, num_pos: int, dim: int) -> torch.Tensor:
        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2, dtype=torch.int64) / (dim - 2)))
        sinusoid_inp = torch.einsum("i , j -> i j", torch.arange(num_pos, dtype=torch.int64).float(), inv_freq).float()
        return torch.cat((torch.sin(sinusoid_inp), torch.cos(sinusoid_inp)), dim=1)

    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor | None = None,
        **kwargs: Unpack[TransformersKwargs],
    ) -> BaseModelOutput:
        r"""
        Args:
            input_ids (`torch.Tensor`):
                Input token IDs.
            attention_mask (`torch.Tensor`, *optional*):
                Attention mask to avoid performing attention on padding token indices.
        """
        batch_size, seq_length = input_ids.shape
        hidden_states = self.token_embeddings(input_ids)
        hidden_states = hidden_states * (self.config.hidden_size**0.5)

        cls_padding = torch.ones(batch_size, 1)
        input_ids = torch.cat((input_ids, cls_padding), dim=1)
        attention_mask = torch.cat((attention_mask, cls_padding), dim=1) if attention_mask is not None else None

        if attention_mask is not None:
            attention_mask = create_causal_mask(
                config=self.config,
                input_embeds=hidden_states,
                attention_mask=attention_mask,
                cache_position=torch.arange(hidden_states.shape[1] + 1, device=hidden_states.device),
                past_key_values=None,
            )

        features = hidden_states + self.create_sinusoidal_positions(seq_length, self.config.hidden_size)
        cls_emb = self.cls_emb * (self.config.hidden_size**0.5)
        cls_emb = cls_emb.expand(features.shape[0], -1, -1)
        features = torch.cat((features, cls_emb), dim=1)
        text_encoder_output = self.text_encoder(features, attention_mask)
        features = text_encoder_output.last_hidden_state
        features = self.layernorm(features)
        text_embeddings = features[:, -1]

        if self.normalize:
            text_embeddings = l2norm(text_embeddings, dim=-1)

        return BaseModelOutput(
            last_hidden_state=text_embeddings,
        )


@auto_docstring(
    custom_intro="""
    VideoPrism video model consisting of the vision encoder backbone with auxiliary encoder layers and an attention pooling head on top. This model is used in VideoPrismClipModel.
    """
)
class VideoPrismVideoModel(VideoPrismPreTrainedModel):
    config: VideoPrismVisionConfig

    def __init__(self, config: VideoPrismVisionConfig):
        super().__init__(config)
        self.config = config
        self.backbone = VideoPrismVisionModel(self.config)
        self.auxiliary_encoder = VideoPrismAuxiliaryEncoder(self.config)
        self.contrastive_vision_pooler = VideoPrismMultiheadAttentionPoolingHead(self.config)
        self.normalize = self.config.apply_l2_norm
        self.post_init()

    def get_input_embeddings(self):
        return self.backbone.spatial_embeddings.patch_embeddings

    def forward(
        self,
        pixel_values_videos: torch.FloatTensor,
        interpolate_pos_encoding: bool | None = False,
        **kwargs: Unpack[TransformersKwargs],
    ) -> VideoPrismVideoOutput:
        r"""
        Args:
            pixel_values_videos (`torch.FloatTensor`):
                Pixel values of the video frames.
            interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):
                Whether to interpolate positional encodings to match input size.
        """
        backbone_outputs = self.backbone(
            pixel_values_videos=pixel_values_videos, interpolate_pos_encoding=interpolate_pos_encoding, **kwargs
        )
        video_features = backbone_outputs.last_hidden_state
        auxiliary_output = self.auxiliary_encoder(video_features)
        auxiliary_output_features = auxiliary_output.last_hidden_state
        contrastive_vision_pooler_output = self.contrastive_vision_pooler(auxiliary_output_features, **kwargs)
        video_embeddings = contrastive_vision_pooler_output[0]
        if self.normalize:
            video_embeddings = l2norm(video_embeddings, dim=-1)

        return VideoPrismVideoOutput(
            video_last_hidden_state=video_embeddings,
            auxiliary_output=auxiliary_output,
            attention_pooling_output=contrastive_vision_pooler_output,
        )


@auto_docstring(
    custom_intro="""
    VideoPrism model for video-text contrastive learning. This model consists of a VideoPrismVideoModel and a VideoPrismTextModel, and computes similarity scores between video and text inputs.
    """
)
class VideoPrismClipModel(VideoPrismPreTrainedModel):
    def __init__(self, config: VideoPrismConfig):
        super().__init__(config)
        self.config = config
        self.vision_config = config.vision_config
        self.text_config = config.text_config
        self.video_model = VideoPrismVideoModel(self.vision_config)
        self.text_model = VideoPrismTextModel(self.text_config)
        self.post_init()

    def forward(
        self,
        pixel_values_videos: torch.FloatTensor,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor | None = None,
        interpolate_pos_encoding: bool | None = False,
        temperature: float | None = None,
        **kwargs: Unpack[TransformersKwargs],
    ) -> VideoPrismClipOutput:
        r"""
        Args:
            pixel_values_videos (`torch.FloatTensor`):
                Pixel values of the video frames.
            input_ids (`torch.Tensor`):
                Input token IDs for text.
            attention_mask (`torch.Tensor`, *optional*):
                Attention mask for text inputs.
            interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):
                Whether to interpolate positional encodings.
            temperature (`float`, *optional*):
                Temperature parameter for scaling similarity scores.

        Example:

        ```python
        >>> from transformers import VideoPrismProcessor, VideoPrismClipModel
        >>> import torch

        >>> processor = VideoPrismProcessor.from_pretrained("google/videoprism")
        >>> model = VideoPrismClipModel.from_pretrained("google/videoprism")

        >>> video = "sample_video.mp4"
        >>> texts = ["a dog", "a cat"]
        >>> inputs = processor(videos=video, texts=texts, return_tensors="pt", padding=True)

        >>> with torch.no_grad():
        ...     outputs = model(**inputs)
        ...     logits_per_video = outputs.logits_per_video
        ```
        """
        video_model_outputs = self.video_model(
            pixel_values_videos=pixel_values_videos, interpolate_pos_encoding=interpolate_pos_encoding, **kwargs
        )
        text_model_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask, **kwargs)

        video_embeddings = video_model_outputs.video_last_hidden_state
        text_embeddings = text_model_outputs.last_hidden_state
        emb_dim = video_embeddings[0].shape[-1]
        assert emb_dim == text_embeddings[0].shape[-1]

        video_embeds = video_embeddings.reshape(-1, emb_dim)
        text_embeds = text_embeddings.reshape(-1, emb_dim)
        similarity_matrix = torch.matmul(video_embeds, text_embeds.T)

        if temperature is not None:
            similarity_matrix /= temperature

        logits_per_video = torch.exp(similarity_matrix)
        logits_per_text = logits_per_video.T
        logits_per_video = logits_per_video / torch.sum(logits_per_video, dim=0, keepdims=True)
        logits_per_text = logits_per_text / torch.sum(logits_per_text, dim=0, keepdims=True)

        return VideoPrismClipOutput(
            logits_per_video=logits_per_video,
            logits_per_text=logits_per_text,
            video_embeds=video_embeds,
            text_embeds=text_embeds,
        )


@auto_docstring(
    custom_intro="""
    VideoPrism Model transformer with a video classification head on top (a linear layer on top of the attention pooler).
    """
)
class VideoPrismForVideoClassification(VideoPrismPreTrainedModel):
    config: VideoPrismVisionConfig

    def __init__(self, config: VideoPrismVisionConfig):
        super().__init__(config)
        self.config = config
        self.encoder = VideoPrismVisionModel(self.config)
        self.contrastive_vision_pooler = VideoPrismMultiheadAttentionPoolingHead(self.config)
        self.classifier = nn.Linear(self.config.hidden_size, self.config.num_labels)
        self.post_init()

    def get_input_embeddings(self):
        return self.encoder.spatial_embeddings.patch_embeddings

    def forward(
        self,
        pixel_values_videos: torch.FloatTensor,
        labels: torch.LongTensor | None = None,
        interpolate_pos_encoding: bool | None = False,
        **kwargs: Unpack[TransformersKwargs],
    ) -> ImageClassifierOutput:
        r"""
        Args:
            pixel_values_videos (`torch.FloatTensor`):
                Pixel values of the video frames.
            labels (`torch.LongTensor`, *optional*):
                Video classification labels.
            interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):
                Whether to interpolate positional encodings.

        Example:

        ```python
        >>> from transformers import VideoPrismVideoProcessor, VideoPrismForVideoClassification
        >>> import torch

        >>> processor = VideoPrismVideoProcessor("google/videoprism")
        >>> model = VideoPrismForVideoClassification.from_pretrained("google/videoprism", num_labels=1000)

        >>> video = "sample_video.mp4"
        >>> inputs = processor(videos=video, return_tensors="pt")

        >>> with torch.no_grad():
        ...     outputs = model(**inputs)
        ...     logits = outputs.logits
        ```
        """
        encoder_outputs = self.encoder(
            pixel_values_videos=pixel_values_videos, interpolate_pos_encoding=interpolate_pos_encoding, **kwargs
        )
        sequence_output = encoder_outputs.last_hidden_state
        pooled_output = self.contrastive_vision_pooler(sequence_output, **kwargs)[0]
        logits = self.classifier(pooled_output)
        loss = None
        if labels is not None:
            loss = self.loss_function(labels, logits, self.config, **kwargs)

        return ImageClassifierOutput(
            loss=loss,
            logits=logits,
            hidden_states=encoder_outputs.last_hidden_state,
        )


__all__ = [
    "VideoPrismVisionModel",
    "VideoPrismPreTrainedModel",
    "VideoPrismVideoModel",
    "VideoPrismTextModel",
    "VideoPrismClipModel",
    "VideoPrismForVideoClassification",
]
