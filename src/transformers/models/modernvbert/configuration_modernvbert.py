#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/modernvbert/modular_modernvbert.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_modernvbert.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
from typing import Any, Literal, Optional

from ...configuration_utils import PretrainedConfig
from ...utils import logging
from ..modernbert import ModernBertConfig
from ..siglip import SiglipVisionConfig


logger = logging.get_logger(__name__)


class ModernVBertConfig(PretrainedConfig):
    r"""
    This is the configuration class to store the configuration of a `ModernVBert` model. It is used to
    instantiate a ModernVBert model according to the specified arguments and defines the model architecture.
    e.g. [ModernVBERT/modernvbert](https://huggingface.co/ModernVBERT/modernvbert).

    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs.
    See the documentation for [`PretrainedConfig`] for more details.

    Args:
        text_config (`PretrainedConfig` or `dict`, optional):
            Custom text config or a dict with a `text_model_name` key for the text encoder. If `None`, the
            default text backbone defined by `DEFAULT_TEXT_MODEL_NAME` is used.
        vision_config (`PretrainedConfig` or `dict`, optional):
            Custom vision config or a dict with a `vision_model_name` key for the vision encoder. If `None`, the
            default vision backbone defined by `DEFAULT_VISION_MODEL_NAME` is used.
        image_token_id (`int`, optional, defaults to 128257):
            Token id reserved for image tokens inserted into the text stream.
        pixel_shuffle_factor (`int`, optional, defaults to 4):
            Scale factor used by any pixel-shuffle / upsampling operations in the vision head.
        vocab_size (`int`, optional):
            Vocabulary size of the text model. Defines the number of different tokens that can be represented
            by the `inputs_ids` passed when calling [`ModernVBertModel`]. If not provided, will be taken from
            the `text_config`.
        hidden_size (`int`, optional):
            Dimensionality of the encoder layers and the pooler layer. If not provided, will be taken from
            the `text_config`.
        num_hidden_layers (`int`, optional):
            Number of hidden layers in the text model. If not provided, will be taken from
            the `text_config`.
        initializer_range (`float`, optional, defaults to 0.02):
            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.
        initializer_cutoff_factor (`float`, optional, defaults to 2.0):
            The cutoff factor for the truncated_normal_initializer for initializing all weight matrices.
        classifier_pooling (`str`, optional, defaults to "cls"):
            The pooling strategy to use for classification tasks. Can be either `"cls"` or `"mean"`.
        classifier_dropout (`float`, optional, defaults to 0.0):
            The dropout probability for the classification head.
        classifier_bias (`bool`, optional, defaults to `False`):
            Whether to add a bias term to the classification head.
        classifier_dropout (`float`, optional, defaults to 0.0):
            The dropout probability for the classification head.
        classifier_bias (`bool`, optional, defaults to `False`):
            Whether to add a bias term to the classification head.

    Example:
    ```python
    >>> from modernvbert import ModernVBertConfig

    >>> # Initializing configuration
    >>> configuration = ModernVBertConfig()

    >>> # Initializing a model from the configuration (model class is implemented in
    >>> # `modernvbert.modeling_modernvbert`)

    >>> from modernvbert import ModernVBertModel
    >>> model = ModernVBertModel(configuration)

    >>> # Accessing the model configuration
    >>> cfg = model.config
    ```"""

    model_type = "modernvbert"
    sub_configs: dict[str, Any] = {"text_config": ModernBertConfig, "vision_config": SiglipVisionConfig}

    def __init__(
        self,
        text_config=None,
        vision_config=None,
        image_token_id: Optional[int] = 50407,
        pixel_shuffle_factor: Optional[int] = 4,
        vocab_size: Optional[int] = None,
        hidden_size: Optional[int] = None,
        num_hidden_layers: Optional[int] = None,
        initializer_range: Optional[float] = 0.02,
        initializer_cutoff_factor: Optional[float] = 2.0,
        classifier_pooling: Literal["cls", "mean"] = "cls",
        classifier_dropout: Optional[float] = 0.0,
        classifier_bias: Optional[bool] = False,
        **kwargs,
    ):
        if classifier_pooling not in ["cls", "mean"]:
            raise ValueError(
                f'Invalid value for `classifier_pooling`, should be either "cls" or "mean", but is {classifier_pooling}.'
            )

        if text_config is None:
            text_config = self.sub_configs["text_config"]()
        elif isinstance(text_config, dict):
            text_config = self.sub_configs["text_config"](**text_config)
        self.text_config = text_config

        if vision_config is None:
            vision_config = self.sub_configs["vision_config"]()
        elif isinstance(vision_config, dict):
            vision_config = self.sub_configs["vision_config"](**vision_config)
        self.vision_config = vision_config

        # Common model parameters overrides
        self.vocab_size = self._resolve_text_config_param("vocab_size", vocab_size)
        self.hidden_size = self._resolve_text_config_param("hidden_size", hidden_size)
        self.num_hidden_layers = self._resolve_text_config_param("num_hidden_layers", num_hidden_layers)

        self.pixel_shuffle_factor = pixel_shuffle_factor
        self.initializer_range = initializer_range
        self.initializer_cutoff_factor = initializer_cutoff_factor
        self.classifier_pooling = classifier_pooling
        self.classifier_dropout = classifier_dropout
        self.classifier_bias = classifier_bias

        super().__init__(image_token_id=image_token_id, **kwargs)

    def _resolve_text_config_param(self, param_name: str, param_value: Optional[int]):
        if param_value is not None:
            logger.warning(f"Overriding `{param_name}` of the `text_config`.")
            setattr(self.text_config, param_name, param_value)
            return param_value
        return getattr(self.text_config, param_name)

    @classmethod
    def from_pretrained_models(
        cls,
        text_model_name: str,
        vision_model_name: str,
        vocab_size: Optional[int] = None,
        **kwargs,
    ) -> "PretrainedConfig":
        text_model_config = cls.sub_configs["text_config"].from_pretrained(text_model_name)
        vision_model_config = cls.sub_configs["vision_config"].from_pretrained(vision_model_name)

        if vocab_size is not None:
            # Update the text model config with the new vocab size
            text_model_config.tie_word_embeddings = False
            text_model_config.vocab_size = vocab_size

        return cls(
            text_config=text_model_config,
            vision_config=vision_model_config,
            **kwargs,
        )


__all__ = ["ModernVBertConfig"]
