#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/pp_ocrv5_server_det/modular_pp_ocrv5_server_det.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_pp_ocrv5_server_det.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
from dataclasses import dataclass
from typing import Any, Optional, Union

import torch
import torch.nn as nn
import torch.nn.functional as F

from ...modeling_outputs import BaseModelOutputWithNoAttention
from ...modeling_utils import PreTrainedModel
from ...utils import ModelOutput, auto_docstring
from .configuration_pp_ocrv5_server_det import PPOCRV5ServerDetConfig


class LearnableAffineBlock(nn.Module):
    """
    Applies a learnable affine transformation (element-wise scaling and shifting) to the input tensor.
    This is often used after normalization or activation layers to provide additional modeling flexibility.

    Args:
        scale_value (`float`, *optional*, defaults to 1.0):
            The initial value for the learnable scale parameter (`gamma`).
        bias_value (`float`, *optional*, defaults to 0.0):
            The initial value for the learnable bias parameter (`beta`).
    """

    def __init__(self, scale_value: float = 1.0, bias_value: float = 0.0):
        super().__init__()
        self.scale = nn.Parameter(torch.tensor([scale_value]))
        self.bias = nn.Parameter(torch.tensor([bias_value]))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass of the LearnableAffineBlock.

        Args:
            x (`torch.FloatTensor` of shape `(batch_size, channels, height, width)`):
                The input feature map from the previous layer.

        Returns:
            `torch.FloatTensor`: The transformed output tensor of the same shape as the input `x`.
                Computed as: $y = \text{scale} \\cdot x + \text{bias}$
        """
        return self.scale * x + self.bias


class ConvBNAct(nn.Module):
    """
    Standard sequence of Convolution, Batch Normalization, and optional Activation/LAB.
    Args:
        in_channels (`int`):
            Number of input channels.
        out_channels (`int`):
            Number of output channels.
        kernel_size (`int`, *optional*, defaults to 3):
            Size of the convolving kernel.
        stride (`int`, *optional*, defaults to 1):
            Stride of the convolution.
        padding (`Union[int, str]`, *optional*, defaults to 1):
            Zero-padding added to both sides of the input. If string, typically "same".
        groups (`int`, *optional*, defaults to 1):
            Number of blocked connections from input channels to output channels.
        use_act (`bool`, *optional*, defaults to True):
            Whether to apply the ReLU activation function.
        use_lab (`bool`, *optional*, defaults to False):
            Whether to apply the Learnable Affine Block (LAB) after activation.
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int = 3,
        stride: int = 1,
        padding: Union[int, str] = 1,
        groups: int = 1,
        use_act: bool = True,
        use_lab: bool = False,
    ) -> None:
        super().__init__()
        self.use_act = use_act
        self.use_lab = use_lab
        self.conv = nn.Conv2d(
            in_channels,
            out_channels,
            kernel_size,
            stride,
            padding=padding if isinstance(padding, str) else (kernel_size - 1) // 2,
            groups=groups,
            bias=False,
        )

        self.bn = nn.BatchNorm2d(out_channels, momentum=0.9)

        if self.use_act:
            self.act = nn.ReLU()
            if self.use_lab:
                self.lab = LearnableAffineBlock()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass of the ConvBNAct module.

        Args:
            x (`torch.FloatTensor` of shape `(batch_size, in_channels, height, width)`):
                Input feature map.

        Returns:
            `torch.FloatTensor`: Output feature map after convolution, normalization, and activation.
                Shape is `(batch_size, out_channels, out_height, out_width)`.
        """
        x = self.conv(x)
        x = self.bn(x)
        if self.use_act:
            x = self.act(x)
            if self.use_lab:
                x = self.lab(x)
        return x


class LightConvBNAct(nn.Module):
    """
    Lightweight version of ConvBNAct using Pointwise Convolution followed by Depthwise Convolution.
    This effectively separates spatial and channel-wise processing to reduce parameters.

    Args:
        in_channels (`int`):
            Number of input channels.
        out_channels (`int`):
            Number of output channels.
        kernel_size (`int`):
            Size of the kernel for the depthwise convolution step.
        use_lab (`bool`, *optional*, defaults to False):
            Whether to apply the Learnable Affine Block (LAB) in both sub-layers.
        **kwargs:
            Additional arguments passed to the sub-ConvBNAct layers.
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        use_lab: bool = False,
        **kwargs,
    ):
        super().__init__()
        self.conv1 = ConvBNAct(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=1,
            use_act=False,
            use_lab=use_lab,
        )
        self.conv2 = ConvBNAct(
            in_channels=out_channels,
            out_channels=out_channels,
            kernel_size=kernel_size,
            groups=out_channels,
            use_act=True,
            use_lab=use_lab,
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.conv1(x)
        x = self.conv2(x)
        return x


class StemBlock(nn.Module):
    """
    Stem block of PPHGNetV2, performing initial feature extraction and spatial downsampling.
    It splits the input into two branches: one for max pooling and another for convolution,
    then concatenates them to enrich feature representation.

    Args:
        in_channels (`int`):
            Number of input image channels (typically 3 for RGB).
        mid_channels (`int`):
            Intermediate channel dimension used across the stem convolutions.
        out_channels (`int`):
            Final output channel dimension of the stem block.
        use_lab (`bool`, *optional*, defaults to `False`):
            Whether to use Learnable Affine Block (LAB) in the internal ConvBNAct layers.
    """

    def __init__(
        self,
        in_channels: int,
        mid_channels: int,
        out_channels: int,
        use_lab: bool = False,
    ):
        super().__init__()
        self.stem1 = ConvBNAct(
            in_channels=in_channels,
            out_channels=mid_channels,
            kernel_size=3,
            stride=2,
            use_lab=use_lab,
        )
        self.stem2a = ConvBNAct(
            in_channels=mid_channels,
            out_channels=mid_channels // 2,
            kernel_size=2,
            stride=1,
            padding="same",
            use_lab=use_lab,
        )
        self.stem2b = ConvBNAct(
            in_channels=mid_channels // 2,
            out_channels=mid_channels,
            kernel_size=2,
            stride=1,
            padding="same",
            use_lab=use_lab,
        )
        self.stem3 = ConvBNAct(
            in_channels=mid_channels * 2,
            out_channels=mid_channels,
            kernel_size=3,
            stride=2,
            use_lab=use_lab,
        )
        self.stem4 = ConvBNAct(
            in_channels=mid_channels,
            out_channels=out_channels,
            kernel_size=1,
            stride=1,
            use_lab=use_lab,
        )

        self.padding = [0, 1, 0, 1]

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass of the StemBlock.

        Args:
            x (`torch.FloatTensor` of shape `(batch_size, in_channels, height, width)`):
                Input image tensor.

        Returns:
            `torch.FloatTensor`:
                Processed feature map of shape `(batch_size, out_channels, height/4, width/4)`.
        """
        x = self.stem1(x)
        x2 = self.stem2a(x)
        x2 = self.stem2b(x2)
        x1 = F.max_pool2d(F.pad(x, self.padding), kernel_size=2, stride=1, padding=0)
        x = torch.cat([x1, x2], dim=1)
        x = self.stem3(x)
        x = self.stem4(x)
        return x


class HGV2_Block(nn.Module):
    """
    HGV2_Block (Hierarchical Grouping Variable Block V2), the fundamental building block of PPHGNetV2 stages.
    It uses a dense connection style to collect multi-scale features and a squeeze-excitation
    aggregation to refine the final output.

    Args:
        in_channels (`int`):
            Number of input channels.
        mid_channels (`int`):
            Hidden channel dimension for each convolutional layer in the block.
        out_channels (`int`):
            Final output channel dimension after feature aggregation.
        kernel_size (`int`, *optional*, defaults to 3):
            Size of the convolution kernel for each layer.
        layer_num (`int`, *optional*, defaults to 6):
            Number of convolutional layers to be densely stacked.
        identity (`bool`, *optional*, defaults to `False`):
            Whether to add a residual connection between the input and the final output.
        light_block (`bool`, *optional*, defaults to `True`):
            Whether to use `LightConvBNAct` (depthwise separable) instead of standard `ConvBNAct`.
        use_lab (`bool`, *optional*, defaults to `False`):
            Whether to use Learnable Affine Block (LAB) in the internal layers.
    """

    def __init__(
        self,
        in_channels: int,
        mid_channels: int,
        out_channels: int,
        kernel_size: int = 3,
        layer_num: int = 6,
        identity: bool = False,
        light_block: bool = True,
        use_lab: bool = False,
    ):
        super().__init__()
        self.identity = identity

        self.layers = nn.ModuleList()
        block_type = LightConvBNAct if light_block else ConvBNAct
        for i in range(layer_num):
            self.layers.append(
                block_type(
                    in_channels=in_channels if i == 0 else mid_channels,
                    out_channels=mid_channels,
                    stride=1,
                    kernel_size=kernel_size,
                    use_lab=use_lab,
                )
            )
        # feature aggregation
        total_channels = in_channels + layer_num * mid_channels
        self.aggregation_squeeze_conv = ConvBNAct(
            in_channels=total_channels,
            out_channels=out_channels // 2,
            kernel_size=1,
            stride=1,
            use_lab=use_lab,
        )
        self.aggregation_excitation_conv = ConvBNAct(
            in_channels=out_channels // 2,
            out_channels=out_channels,
            kernel_size=1,
            stride=1,
            use_lab=use_lab,
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass of the HGV2_Block.

        Args:
            x (`torch.FloatTensor` of shape `(batch_size, in_channels, height, width)`):
                Input feature map.

        Returns:
            `torch.FloatTensor`:
                The aggregated and optionally residual output of shape `(batch_size, out_channels, height, width)`.
        """
        identity = x
        output = []
        output.append(x)
        for layer in self.layers:
            x = layer(x)
            output.append(x)
        x = torch.cat(output, dim=1)
        x = self.aggregation_squeeze_conv(x)
        x = self.aggregation_excitation_conv(x)
        if self.identity:
            x += identity
        return x


class HGV2_Stage(nn.Module):
    """
    HGV2_Stage consists of an optional downsampling layer followed by a sequence of `HGV2_Block`s.

    Args:
        in_channels (`int`):
            Number of input channels from the previous stage or stem.
        mid_channels (`int`):
            Hidden channel dimension within each `HGV2_Block`.
        out_channels (`int`):
            Final output channel dimension for this stage.
        block_num (`int`):
            Number of `HGV2_Block` units to stack in this stage.
        layer_num (`int`, *optional*, defaults to 6):
            Number of layers inside each `HGV2_Block`.
        is_downsample (`bool`, *optional*, defaults to `True`):
            Whether to apply a stride-2 depthwise convolution at the start of the stage.
        light_block (`bool`, *optional*, defaults to `True`):
            Whether to use depthwise separable convolutions in the blocks.
        kernel_size (`int`, *optional*, defaults to 3):
            Kernel size for the convolutions within the blocks.
        use_lab (`bool`, *optional*, defaults to `False`):
            Whether to use Learnable Affine Block (LAB) in the convolutions.
        stride (`int`, *optional*, defaults to 2):
            Stride for the downsampling layer.
    """

    def __init__(
        self,
        in_channels: int,
        mid_channels: int,
        out_channels: int,
        block_num: int,
        layer_num: int = 6,
        is_downsample: bool = True,
        light_block: bool = True,
        kernel_size: int = 3,
        use_lab: bool = False,
        stride: int = 2,
    ):
        super().__init__()
        self.is_downsample = is_downsample
        if self.is_downsample:
            self.downsample = ConvBNAct(
                in_channels=in_channels,
                out_channels=in_channels,
                kernel_size=3,
                stride=stride,
                groups=in_channels,
                use_act=False,
                use_lab=use_lab,
            )

        blocks_list = []
        for i in range(block_num):
            blocks_list.append(
                HGV2_Block(
                    in_channels=in_channels if i == 0 else out_channels,
                    mid_channels=mid_channels,
                    out_channels=out_channels,
                    kernel_size=kernel_size,
                    layer_num=layer_num,
                    identity=i != 0,
                    light_block=light_block,
                    use_lab=use_lab,
                )
            )
        self.blocks = nn.Sequential(*blocks_list)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass of the HGV2_Stage.

        Args:
            x (`torch.FloatTensor` of shape `(batch_size, in_channels, height, width)`):
                Input feature map.

        Returns:
            `torch.FloatTensor`:
                Processed feature map of shape `(batch_size, out_channels, height/stride, width/stride)`.
        """
        if self.is_downsample:
            x = self.downsample(x)
        x = self.blocks(x)
        return x


class PPHGNetV2(nn.Module):
    """
    PPHGNetV2 (Paddle High-Performance GPU Network V2) backbone.
    Extracts multi-scale hierarchical features from input images for downstream detection or classification.

    Args:
        config (`PPOCRV5ServerDetConfig`):
            Configuration object containing model hyperparameters:
            - **backbone_config**: Parameters for each HGV2 stage.
            - **out_indices**: Indices of stages to return features from.
            - **use_lab**: Global flag for Learnable Affine Block.
            - **use_last_conv**: Whether to apply final global pooling and classification head.
    """

    def __init__(self, config: PPOCRV5ServerDetConfig):
        super().__init__()
        self.use_lab = config.use_lab
        self.use_last_conv = config.use_last_conv
        self.class_expand = config.class_expand
        self.class_num = config.class_num
        self.out_indices = config.out_indices
        self.out_channels = []

        # stem
        self.stem = StemBlock(
            in_channels=config.stem_channels[0],
            mid_channels=config.stem_channels[1],
            out_channels=config.stem_channels[2],
            use_lab=config.use_lab,
        )

        # stages
        self.stages = nn.ModuleList()
        for i, k in enumerate(config.backbone_config):
            (
                in_channels,
                mid_channels,
                out_channels,
                block_num,
                is_downsample,
                light_block,
                kernel_size,
                layer_num,
                stride,
            ) = config.backbone_config[k]
            self.stages.append(
                HGV2_Stage(
                    in_channels,
                    mid_channels,
                    out_channels,
                    block_num,
                    layer_num,
                    is_downsample,
                    light_block,
                    kernel_size,
                    config.use_lab,
                    stride,
                )
            )
            if i in self.out_indices:
                self.out_channels.append(out_channels)

        self.avg_pool = nn.AdaptiveAvgPool2d(1)

        if self.use_last_conv:
            self.last_conv = nn.Conv2d(
                in_channels=out_channels,
                out_channels=self.class_expand,
                kernel_size=1,
                stride=1,
                padding=0,
                bias=False,
            )
            self.act = nn.ReLU()
            if self.use_lab:
                self.lab = LearnableAffineBlock()
            self.dropout = nn.Dropout(p=config.dropout_prob)

        self.flatten = nn.Flatten(start_dim=1, end_dim=-1)

        self._init_weights()

    def _init_weights(self):
        """Initializes model weights using Kaiming normal and constant schemes."""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1.0)
                nn.init.constant_(m.bias, 0.0)
            elif isinstance(m, nn.Linear):
                nn.init.constant_(m.bias, 0.0)

    def forward(
        self, hidden_state: torch.Tensor, output_hidden_states: bool = False, return_dict: bool = True
    ) -> tuple[list[torch.Tensor], torch.Tensor, Optional[tuple[torch.Tensor, ...]]]:
        """
        Forward pass of PPHGNetV2.

        Args:
            hidden_state (`torch.FloatTensor` of shape `(batch_size, 3, height, width)`):
                Input image tensor (pixel values).
            output_hidden_states (`bool`, *optional*, defaults to `False`):
                Whether to return all intermediate stage outputs.
            return_dict (`bool`, *optional*, defaults to `True`):
                Whether to return a structured output dictionary (placeholder for compatibility).

        Returns:
            `tuple(list, torch.FloatTensor, tuple)`:
                - **out** (`list` of `torch.FloatTensor`): Selected multi-scale features for the neck (e.g., c2, c3, c4, c5).
                - **hidden_state** (`torch.FloatTensor`): Final processed feature map from the last stage.
                - **hidden_states** (`tuple` of `torch.FloatTensor`, *optional*): All intermediate states, returned only if `output_hidden_states` is `True`.
        """
        hidden_states = () if output_hidden_states else None
        if output_hidden_states:
            hidden_states = hidden_states + (hidden_state,)

        hidden_state = self.stem(hidden_state)
        if output_hidden_states:
            hidden_states = hidden_states + (hidden_state,)
        out = []
        for i, stage in enumerate(self.stages):
            if output_hidden_states:
                hidden_states = hidden_states + (hidden_state,)
            hidden_state = stage(hidden_state)
            if i in self.out_indices:
                out.append(hidden_state)

        return out, hidden_state, hidden_states


class DSConv(nn.Module):
    """
    Depthwise Separable Convolution block with an expanded intermediate state and residual connection.
    This block mimics the inverted residual structure to reduce computation while maintaining capacity.

    Args:
        in_channels (`int`):
            Number of input channels.
        out_channels (`int`):
            Number of output channels.
        kernel_size (`int`):
            Size of the convolving kernel for the depthwise step.
        padding (`Union[int, str]`):
            Padding for the depthwise convolution.
        stride (`int`, *optional*, defaults to 1):
            Stride for the spatial downsampling.
        groups (`int`, *optional*):
            Number of blocked connections. Defaults to `in_channels` for depthwise convolution.
        if_act (`bool`, *optional*, defaults to `True`):
            Whether to use an activation function in the bottleneck.
        act (`str`, *optional*, defaults to `"relu"`):
            Activation type, supports `"relu"` or `"hardswish"`.
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        padding: Union[int, str],
        stride: int = 1,
        groups: Optional[int] = None,
        if_act: bool = True,
        act: str = "relu",
        **kwargs,
    ):
        super().__init__()
        if groups is None:
            groups = in_channels
        self.if_act = if_act
        self.act = act
        self.conv1 = nn.Conv2d(
            in_channels=in_channels,
            out_channels=in_channels,
            kernel_size=kernel_size,
            stride=stride,
            padding=padding,
            groups=groups,
            bias=False,
        )

        self.bn1 = nn.BatchNorm2d(num_features=in_channels, momentum=0.9)

        self.conv2 = nn.Conv2d(
            in_channels=in_channels,
            out_channels=int(in_channels * 4),
            kernel_size=1,
            stride=1,
            bias=False,
        )

        self.bn2 = nn.BatchNorm2d(num_features=int(in_channels * 4))

        self.conv3 = nn.Conv2d(
            in_channels=int(in_channels * 4),
            out_channels=out_channels,
            kernel_size=1,
            stride=1,
            bias=False,
        )
        self._c = [in_channels, out_channels]
        if in_channels != out_channels:
            self.conv_end = nn.Conv2d(
                in_channels=in_channels,
                out_channels=out_channels,
                kernel_size=1,
                stride=1,
                bias=False,
            )

    def forward(self, inputs: torch.Tensor) -> torch.Tensor:
        """
        Forward pass of DSConv.

        Args:
            inputs (`torch.FloatTensor` of shape `(batch_size, in_channels, height, width)`):
                The input feature map.

        Returns:
            `torch.FloatTensor`: Output feature map of shape `(batch_size, out_channels, out_height, out_width)`.
        """
        x = self.conv1(inputs)
        x = self.bn1(x)

        x = self.conv2(x)
        x = self.bn2(x)
        if self.if_act:
            if self.act == "relu":
                x = F.relu(x)
            elif self.act == "hardswish":
                x = F.hardswish(x)
            else:
                print(f"The activation function({self.act}) is selected incorrectly.")
                exit()

        x = self.conv3(x)
        if self._c[0] != self._c[1]:
            x = x + self.conv_end(inputs)
        return x


class IntraCLBlock(nn.Module):
    """
    Intra-Class Relationship Block. It uses multi-scale convolutions (7x7, 5x5, 3x3)
    and asymmetric kernels (e.g., 7x1, 1x7) to capture long-range spatial dependencies
    within text regions.

    Args:
        intraclblock_config (`dict`, *optional*):
            Configuration dictionary specifying kernel sizes and paddings for all sub-layers.
        in_channels (`int`, *optional*, defaults to 96):
            Number of channels in the input feature map.
        reduce_factor (`int`, *optional*, defaults to 4):
            The factor used to compress channels for efficiency during relationship modeling.
    """

    def __init__(self, intraclblock_config: dict | None = None, in_channels: int = 96, reduce_factor: int = 4):
        super().__init__()

        reduced_ch = in_channels // reduce_factor

        self.conv1x1_reduce_channel = nn.Conv2d(in_channels, reduced_ch, *intraclblock_config["reduce_channel"])
        self.conv1x1_return_channel = nn.Conv2d(reduced_ch, in_channels, *intraclblock_config["return_channel"])

        self.v_layer_7x1 = nn.Conv2d(reduced_ch, reduced_ch, *intraclblock_config["v_layer_7x1"])
        self.v_layer_5x1 = nn.Conv2d(reduced_ch, reduced_ch, *intraclblock_config["v_layer_5x1"])
        self.v_layer_3x1 = nn.Conv2d(reduced_ch, reduced_ch, *intraclblock_config["v_layer_3x1"])

        self.q_layer_1x7 = nn.Conv2d(reduced_ch, reduced_ch, *intraclblock_config["q_layer_1x7"])
        self.q_layer_1x5 = nn.Conv2d(reduced_ch, reduced_ch, *intraclblock_config["q_layer_1x5"])
        self.q_layer_1x3 = nn.Conv2d(reduced_ch, reduced_ch, *intraclblock_config["q_layer_1x3"])

        self.c_layer_7x7 = nn.Conv2d(reduced_ch, reduced_ch, *intraclblock_config["c_layer_7x7"])
        self.c_layer_5x5 = nn.Conv2d(reduced_ch, reduced_ch, *intraclblock_config["c_layer_5x5"])
        self.c_layer_3x3 = nn.Conv2d(reduced_ch, reduced_ch, *intraclblock_config["c_layer_3x3"])

        self.bn = nn.BatchNorm2d(in_channels)
        self.relu = nn.ReLU()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass of IntraCLBlock.

        Args:
            x (`torch.FloatTensor` of shape `(batch_size, in_channels, height, width)`):
                The input feature map from LKPAN stages.

        Returns:
            `torch.FloatTensor`: Refined feature map with the same shape as input,
                enhanced by spatial relationship modeling.
        """
        x_new = self.conv1x1_reduce_channel(x)

        x_7 = self.c_layer_7x7(x_new) + self.v_layer_7x1(x_new) + self.q_layer_1x7(x_new)
        x_5 = self.c_layer_5x5(x_7) + self.v_layer_5x1(x_7) + self.q_layer_1x5(x_7)
        x_3 = self.c_layer_3x3(x_5) + self.v_layer_3x1(x_5) + self.q_layer_1x3(x_5)

        x_relation = self.conv1x1_return_channel(x_3)
        x_relation = self.bn(x_relation)
        x_relation = self.relu(x_relation)

        return x + x_relation


class LKPAN(nn.Module):
    """
    Large Kernel Path Aggregation Network (Neck).
    It fuses features from multiple backbone stages (C2-C5) using a combination of
    top-down and bottom-up paths, enhanced by large kernel convolutions.

    Args:
        config (`PPOCRV5ServerDetConfig`):
            Configuration object containing `neck_out_channels`, `mode`, and `interpolate_mode`.
        in_channels (`list` of `int`):
            The channel counts of the input feature maps from the backbone stages.
    """

    def __init__(self, config: Any, in_channels: list[int]):
        super().__init__()
        self.interpolate_mode = config.interpolate_mode
        self.weight_init = nn.init.kaiming_uniform_

        if config.mode == "lite":
            p_layer = DSConv
        elif config.mode == "large":
            p_layer = nn.Conv2d
        else:
            raise ValueError(f"mode can only be one of ['lite', 'large'], but received {config.mode}")

        self.ins_conv = nn.ModuleList()
        self.inp_conv = nn.ModuleList()
        self.pan_head_conv = nn.ModuleList()
        self.pan_lat_conv = nn.ModuleList()

        for i in range(len(in_channels)):
            conv = nn.Conv2d(
                in_channels=in_channels[i], out_channels=config.neck_out_channels, kernel_size=1, bias=False
            )

            self.weight_init(conv.weight)
            self.ins_conv.append(conv)

            inp_conv = p_layer(
                in_channels=config.neck_out_channels,
                out_channels=config.neck_out_channels // 4,
                kernel_size=9,
                padding=4,
                bias=False,
            )

            self.weight_init(inp_conv.weight)
            self.inp_conv.append(inp_conv)

            if i > 0:
                pan_head = nn.Conv2d(
                    in_channels=config.neck_out_channels // 4,
                    out_channels=config.neck_out_channels // 4,
                    kernel_size=3,
                    padding=1,
                    stride=2,
                    bias=False,
                )
                self.weight_init(pan_head.weight)
                self.pan_head_conv.append(pan_head)

            pan_lat = p_layer(
                in_channels=config.neck_out_channels // 4,
                out_channels=config.neck_out_channels // 4,
                kernel_size=9,
                padding=4,
                bias=False,
            )
            self.weight_init(pan_lat.weight)
            self.pan_lat_conv.append(pan_lat)

        self.incl1 = IntraCLBlock(
            config.intraclblock_config, config.neck_out_channels // 4, reduce_factor=config.reduce_factor
        )
        self.incl2 = IntraCLBlock(
            config.intraclblock_config, config.neck_out_channels // 4, reduce_factor=config.reduce_factor
        )
        self.incl3 = IntraCLBlock(
            config.intraclblock_config, config.neck_out_channels // 4, reduce_factor=config.reduce_factor
        )
        self.incl4 = IntraCLBlock(
            config.intraclblock_config, config.neck_out_channels // 4, reduce_factor=config.reduce_factor
        )

    def forward(self, x: list[torch.Tensor]) -> torch.Tensor:
        """
        Forward pass of LKPAN.

        Args:
            x (`list` of `torch.FloatTensor`):
                Multi-scale features `[c2, c3, c4, c5]` from the backbone.

        Returns:
            `torch.FloatTensor`:
                Fused feature map of shape `(batch_size, neck_out_channels, height/4, width/4)`.
                This tensor is a concatenation of multi-scale refined features, ready for the head.
        """
        c2, c3, c4, c5 = x

        in5 = self.ins_conv[3](c5)
        in4 = self.ins_conv[2](c4)
        in3 = self.ins_conv[1](c3)
        in2 = self.ins_conv[0](c2)

        out4 = in4 + F.interpolate(in5, scale_factor=2, mode=self.interpolate_mode)
        out3 = in3 + F.interpolate(out4, scale_factor=2, mode=self.interpolate_mode)
        out2 = in2 + F.interpolate(out3, scale_factor=2, mode=self.interpolate_mode)

        f5 = self.inp_conv[3](in5)
        f4 = self.inp_conv[2](out4)
        f3 = self.inp_conv[1](out3)
        f2 = self.inp_conv[0](out2)

        pan3 = f3 + self.pan_head_conv[0](f2)
        pan4 = f4 + self.pan_head_conv[1](pan3)
        pan5 = f5 + self.pan_head_conv[2](pan4)

        p2 = self.pan_lat_conv[0](f2)
        p3 = self.pan_lat_conv[1](pan3)
        p4 = self.pan_lat_conv[2](pan4)
        p5 = self.pan_lat_conv[3](pan5)

        p5 = self.incl4(p5)
        p4 = self.incl3(p4)
        p3 = self.incl2(p3)
        p2 = self.incl1(p2)

        p5 = F.interpolate(p5, scale_factor=8, mode=self.interpolate_mode)
        p4 = F.interpolate(p4, scale_factor=4, mode=self.interpolate_mode)
        p3 = F.interpolate(p3, scale_factor=2, mode=self.interpolate_mode)

        fuse = torch.cat([p5, p4, p3, p2], dim=1)
        return fuse


class ConvBNLayer(nn.Module):
    """
    A basic wrapper for Convolution-BatchNorm-Activation, typically used for head components.

    Args:
        in_channels (`int`): Input channel count.
        out_channels (`int`): Output channel count.
        kernel_size (`int`): Size of the kernel.
        stride (`int`): Stride for the convolution.
        padding (`Union[int, str]`): Padding value or strategy.
        groups (`int`, *optional*, defaults to 1): Grouped convolution parameter.
        if_act (`bool`, *optional*, defaults to `True`): Whether to apply activation.
        act (`str`, *optional*): Type of activation ("relu" or "hardswish").
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int,
        padding: Union[int, str],
        groups: int = 1,
        if_act: bool = True,
        act: Optional[str] = None,
    ):
        super().__init__()
        self.if_act = if_act
        self.act = act
        self.conv = nn.Conv2d(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=kernel_size,
            stride=stride,
            padding=padding,
            groups=groups,
            bias=False,
        )
        nn.init.kaiming_normal_(self.conv.weight)

        self.bn = nn.BatchNorm2d(out_channels, momentum=0.9)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass of ConvBNLayer.

        Args:
            x (`torch.FloatTensor` of shape `(batch_size, in_channels, height, width)`):
                Input tensor.

        Returns:
            `torch.FloatTensor`: Output tensor of shape `(batch_size, out_channels, out_height, out_width)`.
        """
        x = self.conv(x)
        x = self.bn(x)
        if self.if_act:
            if self.act == "relu":
                x = F.relu(x)
            elif self.act == "hardswish":
                x = F.hardswish(x)
            else:
                print(f"The activation function({self.act}) is selected incorrectly.")
                exit()
        return x


class Head(nn.Module):
    """
    Standard segmentation head for generating probability maps. It uses transposed
    convolutions to upsample the feature map back to the original image size.

    Args:
        in_channels (`int`):
            Number of input channels from the neck (e.g., LKPAN).
        kernel_list (`List[int]`, *optional*, defaults to `[3, 2, 2]`):
            List of kernel sizes for the sequence of [Conv2d, ConvTranspose2d, ConvTranspose2d].
    """

    def __init__(
        self,
        in_channels: int,
        kernel_list: list[int] = [3, 2, 2],
    ):
        super().__init__()

        self.conv1 = nn.Conv2d(
            in_channels=in_channels,
            out_channels=in_channels // 4,
            kernel_size=kernel_list[0],
            padding=int(kernel_list[0] // 2),
            bias=False,
        )
        self.conv_bn1 = nn.BatchNorm2d(in_channels // 4, momentum=0.9)
        self.relu1 = nn.ReLU()

        nn.init.constant_(self.conv_bn1.weight, 1.0)
        nn.init.constant_(self.conv_bn1.bias, 1e-4)

        self.conv2 = nn.ConvTranspose2d(
            in_channels=in_channels // 4,
            out_channels=in_channels // 4,
            kernel_size=kernel_list[1],
            stride=2,
        )
        nn.init.kaiming_uniform_(self.conv2.weight)

        self.conv_bn2 = nn.BatchNorm2d(in_channels // 4, momentum=0.9)
        self.relu2 = nn.ReLU()

        nn.init.constant_(self.conv_bn2.weight, 1.0)
        nn.init.constant_(self.conv_bn2.bias, 1e-4)

        self.conv3 = nn.ConvTranspose2d(
            in_channels=in_channels // 4,
            out_channels=1,
            kernel_size=kernel_list[2],
            stride=2,
        )
        nn.init.kaiming_uniform_(self.conv3.weight)

    def forward(
        self, x: torch.Tensor, return_f: bool = False
    ) -> Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]]:
        """
        Forward pass of the Head.

        Args:
            x (`torch.FloatTensor` of shape `(batch_size, in_channels, height, width)`):
                Input feature map.
            return_f (`bool`, *optional*, defaults to `False`):
                Whether to return the intermediate feature map before the final convolution.

        Returns:
            `torch.FloatTensor` or `tuple(torch.FloatTensor, torch.FloatTensor)`:
                - **x** (`torch.FloatTensor`): Final probability map of shape `(batch_size, 1, H*4, W*4)`.
                - **f** (`torch.FloatTensor`, *optional*): Intermediate features, returned only if `return_f` is `True`.
        """
        x = self.conv1(x)
        x = self.conv_bn1(x)
        x = self.relu1(x)
        x = self.conv2(x)
        x = self.conv_bn2(x)
        x = self.relu2(x)
        if return_f is True:
            f = x
        x = self.conv3(x)
        x = torch.sigmoid(x)
        if return_f is True:
            return x, f
        return x


class DBHead(nn.Module):
    """
    Differentiable Binarization (DB) Head wrapper.

    Args:
        in_channels (`int`): Input channel depth.
        k (`int`, *optional*, defaults to 50): Amplification factor for the binarization step.
        kernel_list (`List[int]`, *optional*, defaults to `[3, 2, 2]`): Kernel sizes for the internal Head.
    """

    def __init__(self, in_channels: int, k: int = 50, kernel_list: list[int] = [3, 2, 2]):
        super().__init__()
        self.k = k
        self.binarize = Head(in_channels=in_channels, kernel_list=kernel_list)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass to generate the initial shrink map.

        Args:
            x (`torch.FloatTensor`): Input feature map.

        Returns:
            `torch.FloatTensor`: Shrink probability map.
        """
        shrink_maps = self.binarize(x)
        return shrink_maps


class LocalModule(nn.Module):
    """
    Local Refinement Module that refines the initial probability map by
    concatenating it with higher-resolution features.

    Args:
        in_c (`int`): Number of channels in the feature map `x`.
        mid_c (`int`): Hidden channel size for the refinement layers.
        act (`str`): Activation function name.
    """

    def __init__(self, in_c: int, mid_c: int, act: str):
        super().__init__()
        self.last_3 = ConvBNLayer(in_c + 1, mid_c, 3, 1, 1, act=act)
        self.last_1 = nn.Conv2d(
            in_channels=mid_c,
            out_channels=1,
            kernel_size=1,
            stride=1,
            padding=0,
        )

    def forward(self, x: torch.Tensor, init_map: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x (`torch.FloatTensor`): Upsampled intermediate feature map.
            init_map (`torch.FloatTensor`): Initial probability map (shrink map).

        Returns:
            `torch.FloatTensor`: Refined single-channel logit map.
        """
        outf = torch.cat([init_map, x], dim=1)
        # last Conv
        out = self.last_1(self.last_3(outf))
        return out


class PFHeadLocal(DBHead):
    """
    PFHeadLocal implements the Progressive Fusion Head with Local refinement,
    the core detection head of PP-OCRv5.

    Args:
        config (`PPOCRV5ServerDetConfig`):
            Configuration object containing parameters for upsampling, mode selection,
            and refinement hidden channels.
    """

    def __init__(self, config: PPOCRV5ServerDetConfig):
        super().__init__(in_channels=config.neck_out_channels, k=config.k, kernel_list=config.kernel_list)

        self.up_conv = nn.Upsample(scale_factor=config.scale_factor, mode=config.interpolate_mode)
        if config.mode == "large":
            mid_ch = config.neck_out_channels // 4
        elif config.mode == "small":
            mid_ch = config.neck_out_channels // 8
        else:
            raise ValueError(f"mode must be 'large' or 'small', currently {config.mode}")
        self.cbn_layer = LocalModule(config.neck_out_channels // 4, mid_ch, config.hidden_act)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass of PFHeadLocal, combining base shrink maps and locally refined maps.

        Args:
            x (`torch.FloatTensor` of shape `(batch_size, neck_out_channels, H, W)`):
                Fused feature map from the neck.

        Returns:
            `torch.FloatTensor`:
                The final refined text detection probability map, calculated as the
                average of the base map and the refined local map.
        """
        shrink_maps, f = self.binarize(x, return_f=True)
        base_maps = shrink_maps
        cbn_maps = self.cbn_layer(self.up_conv(f), shrink_maps)
        cbn_maps = torch.sigmoid(cbn_maps)

        return 0.5 * (base_maps + cbn_maps)


@dataclass
class PPOCRV5ServerDetModelOutput(ModelOutput):
    """
    Output class for the PPOCRV5ServerDetModel.

    Args:
        logits (`torch.FloatTensor` of shape `(batch_size, 1, height, width)`, *optional*):
            Binary segmentation probability maps from the head. Higher values indicate
            higher probability of text presence.
        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, channels, height/32, width/32)`, *optional*):
            Sequence of hidden-states from the last stage of the backbone.
        hidden_states (`tuple(torch.FloatTensor)`, *optional*):
            Tuple of `torch.FloatTensor` (one for the output of each stage) of various shapes.
            Returned if `output_hidden_states=True` is passed or `config.output_hidden_states=True`.
    """

    logits: Optional[torch.FloatTensor] = None
    last_hidden_state: Optional[torch.FloatTensor] = None
    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None


class PPOCRV5ServerDetPreTrainedModel(PreTrainedModel):
    """
    Base class for all PPOCRV5 Server Det pre-trained models. Handles model initialization,
    configuration, and loading of pre-trained weights, following the Transformers library conventions.
    """

    config: PPOCRV5ServerDetConfig
    base_model_prefix = "pp_ocrv5_server_det"
    main_input_name = "pixel_values"
    input_modalities = ("image",)


@auto_docstring(custom_intro="The PPOCRV5 Server Det model.")
class PPOCRV5ServerDetModel(PPOCRV5ServerDetPreTrainedModel):
    """
    Core PPOCRV5 Server Det model.
    Integration of PPHGNetV2 (Backbone), LKPAN (Neck), and PFHeadLocal (Head).
    """

    def __init__(self, config: PPOCRV5ServerDetConfig):
        """
        Initialize the PPOCRV5ServerDetModel with the specified configuration.

        Args:
            config (PPOCRV5ServerDetConfig): Configuration object containing all model hyperparameters.
        """
        super().__init__(config)

        self.backbone = PPHGNetV2(config)
        self.neck = LKPAN(config, in_channels=self.backbone.out_channels)
        self.head = PFHeadLocal(config)
        self.post_init()

    def forward(
        self,
        hidden_state: torch.FloatTensor,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[tuple[torch.FloatTensor], PPOCRV5ServerDetModelOutput]:
        """
        Forward pass of the PPOCRV5ServerDetModel.

        Args:
            hidden_state (`torch.FloatTensor` of shape `(batch_size, 3, height, width)`):
                Input image pixels.
            output_hidden_states (`bool`, *optional*):
                Whether to return all intermediate features.
            return_dict (`bool`, *optional*):
                Whether to return a `PPOCRV5ServerDetModelOutput` instead of a plain tuple.

        Returns:
            `PPOCRV5ServerDetModelOutput` or `tuple(torch.FloatTensor)`:
                A `PPOCRV5ServerDetModelOutput` (if `return_dict=True` is passed or `config.use_return_dict=True`)
                containing the segmentation logits and optional hidden states.
        """
        hidden_state, last_hidden_state, all_hidden_states = self.backbone(hidden_state, output_hidden_states)
        hidden_state = self.neck(hidden_state)
        hidden_state = self.head(hidden_state)
        if not return_dict:
            output = (last_hidden_state,)
            if output_hidden_states:
                output += (all_hidden_states,)
            output += (hidden_state,)
            return output
        return PPOCRV5ServerDetModelOutput(
            logits=hidden_state,
            last_hidden_state=last_hidden_state,
            hidden_states=all_hidden_states if output_hidden_states else None,
        )


@dataclass
class PPOCRV5ServerDetForObjectDetectionOutput(BaseModelOutputWithNoAttention):
    """
    Output class for PPOCRV5ServerDetForObjectDetection.

    Args:
        logits (`torch.FloatTensor` of shape `(batch_size, 1, height, width)`, *optional*):
            The predicted text mask.
        last_hidden_state (`torch.FloatTensor`, *optional*):
            Last stage features from the backbone.
        hidden_states (`tuple(torch.FloatTensor)`, *optional*):
            Intermediate stage features.
    """

    logits: Optional[torch.FloatTensor] = None
    shape: Optional[torch.FloatTensor] = None


@auto_docstring(custom_intro="ObjectDetection for the PPOCRV5 Server Det model.")
class PPOCRV5ServerDetForObjectDetection(PPOCRV5ServerDetPreTrainedModel):
    """
    PPOCRV5 Server Det model for object (text) detection tasks. Wraps the core PPOCRV5ServerDetModel
    and returns outputs compatible with the Transformers object detection API.
    """

    def __init__(self, config: PPOCRV5ServerDetConfig):
        """
        Initialize the PPOCRV5ServerDetForObjectDetection with the specified configuration.

        Args:
            config (PPOCRV5ServerDetConfig): Configuration object containing all model hyperparameters.
        """
        super().__init__(config)
        self.model = PPOCRV5ServerDetModel(config)
        self.post_init()

    def forward(
        self,
        pixel_values: torch.FloatTensor,
        labels: Optional[list[dict]] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        **kwargs,
    ) -> Union[tuple[torch.FloatTensor], PPOCRV5ServerDetForObjectDetectionOutput]:
        """
        Forward pass of the PPOCRV5 detection model.

        Args:
            pixel_values (`torch.FloatTensor` of shape `(batch_size, 3, height, width)`):
                Pixel values of the input images.
            labels (`list[dict]`, *optional*):
                Ground truth for training (not implemented in this forward pass).
            output_hidden_states (`bool`, *optional*):
                Whether to return backbone's intermediate states.
            return_dict (`bool`, *optional*):
                Whether to return a structured output object.

        Returns:
            `PPOCRV5ServerDetForObjectDetectionOutput` or `tuple(torch.FloatTensor)`:
                The detection result containing `logits` (segmentation mask).
        """
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        outputs = self.model(pixel_values, output_hidden_states=output_hidden_states, return_dict=return_dict)

        if not return_dict:
            output = (outputs[0],)
            if output_hidden_states:
                output += (outputs[1], outputs[2])
            else:
                output += (outputs[1],)

            return output

        return PPOCRV5ServerDetForObjectDetectionOutput(
            logits=outputs.logits,
            last_hidden_state=outputs.last_hidden_state,
            hidden_states=outputs.hidden_states if output_hidden_states else None,
        )


__all__ = ["PPOCRV5ServerDetForObjectDetection", "PPOCRV5ServerDetModel", "PPOCRV5ServerDetPreTrainedModel"]
