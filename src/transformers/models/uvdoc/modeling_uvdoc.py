#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/uvdoc/modular_uvdoc.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_uvdoc.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
from dataclasses import dataclass
from typing import Any, Optional, Union

import torch
import torch.nn as nn
import torch.nn.functional as F

from ...modeling_outputs import BaseModelOutputWithNoAttention
from ...modeling_utils import PreTrainedModel
from ...utils import ModelOutput
from .configuration_uvdoc import UVDocConfig


def conv3x3(in_channels: int, out_channels: int, kernel_size: int, stride: int = 1) -> nn.Conv2d:
    """
    Build a 3x3 standard convolutional layer with symmetric padding to maintain feature map dimensions.
    Used for basic ResNet blocks to ensure input/output size consistency when stride=1.

    Args:
        in_channels (`int`): Number of input channels
        out_channels (`int`): Number of output channels
        kernel_size (`int`): Size of convolutional kernel (typically 3)
        stride (`int`, *optional*, defaults to 1): Stride of the convolution

    Returns:
        `nn.Conv2d`: 3x3 convolutional layer instance
    """
    return nn.Conv2d(
        in_channels,
        out_channels,
        kernel_size=kernel_size,
        stride=stride,
        padding=kernel_size // 2,
    )


def dilated_conv(
    in_channels: int, out_channels: int, kernel_size: int, dilation: int, stride: int = 1
) -> nn.Sequential:
    """
    Build a dilated (atrous) convolutional layer to expand receptive field without increasing parameters/computation.
    Critical for capturing long-range geometric dependencies in distorted document images for rectification tasks.

    Args:
        in_channels (`int`): Number of input channels
        out_channels (`int`): Number of output channels
        kernel_size (`int`): Size of convolutional kernel (typically 3)
        dilation (`int`): Dilation rate (expansion factor) to control receptive field size
        stride (`int`, *optional*, defaults to 1): Stride of the convolution

    Returns:
        `nn.Sequential`: Sequential layer containing dilated convolution
    """
    model = nn.Sequential(
        nn.Conv2d(
            in_channels,
            out_channels,
            kernel_size=kernel_size,
            stride=stride,
            padding=dilation * (kernel_size // 2),
            dilation=dilation,
        )
    )
    return model


class ResidualBlockWithDilation(nn.Module):
    """
    Residual block with optional dilated convolution for UVDoc backbone.
    Uses standard convolution for downsampling layers and dilated convolution for middle layers
    to balance spatial resolution and receptive field size.
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int = 1,
        downsample: Optional[nn.Sequential] = None,
        is_activation: bool = True,
        is_top: bool = False,
    ) -> None:
        """
        Initialize residual block with dilation support.

        Args:
            in_channels (`int`): Number of input channels
            out_channels (`int`): Number of output channels
            kernel_size (`int`): Size of convolutional kernel
            stride (`int`, *optional*, defaults to 1): Stride of the first convolution
            downsample (`nn.Sequential`, *optional*, defaults to None):
                Downsampling layer for residual connection (when stride != 1 or channel mismatch)
            is_activation (`bool`, *optional*, defaults to True):
                Whether to apply ReLU activation (always True for UVDoc)
            is_top (`bool`, *optional*, defaults to False):
                Whether this is the first block in the layer (uses standard conv instead of dilated conv)
        """
        super().__init__()
        self.stride = stride
        self.downsample = downsample
        self.is_activation = is_activation
        self.is_top = is_top
        if self.stride != 1 or self.is_top:
            self.conv1 = conv3x3(in_channels, out_channels, kernel_size, self.stride)
            self.conv2 = conv3x3(out_channels, out_channels, kernel_size)
        else:
            self.conv1 = dilated_conv(in_channels, out_channels, kernel_size, dilation=3)
            self.conv2 = dilated_conv(out_channels, out_channels, kernel_size, dilation=3)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU()
        self.bn2 = nn.BatchNorm2d(out_channels)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass of residual block with dilation.

        Args:
            x (`torch.Tensor`): Input tensor of shape [B, C, H, W]

        Returns:
            `torch.Tensor`: Output tensor of shape [B, C_out, H_out, W_out]
        """
        residual = x
        if self.downsample is not None:
            residual = self.downsample(x)
        out1 = self.relu(self.bn1(self.conv1(x)))
        out2 = self.bn2(self.conv2(out1))
        out2 += residual
        out = self.relu(out2)
        return out


class ResnetStraight(nn.Module):
    """
    Modified ResNet backbone for UVDoc with dilated residual blocks.
    Extracts multi-scale features from document images through progressive downsampling,
    using dilated convolution to maintain large receptive fields.
    """

    def __init__(
        self,
        num_filter: int,
        map_num: list[int],
        block_nums: list[int] = [3, 4, 6, 3],
        kernel_size: int = 5,
        stride: list[int] = [1, 1, 2, 2],
    ) -> None:
        """
        Initialize ResNet backbone for UVDoc downsampling.

        Args:
            num_filter (`int`): Base number of convolutional filters
            map_num (`List[int]`): Channel scaling factors for each ResNet layer
            block_nums (`List[int]`, *optional*, defaults to [3,4,6,3]):
                Number of residual blocks per layer
            kernel_size (`int`, *optional*, defaults to 5): Size of convolutional kernel
            stride (`List[int]`, *optional*, defaults to [1,1,2,2]):
                Stride values for each ResNet layer (controls downsampling rate)
        """
        super().__init__()
        self.in_channels = num_filter * map_num[0]
        self.stride = stride
        self.relu = nn.ReLU()
        self.block_nums = block_nums
        self.kernel_size = kernel_size
        for layer_idx, (map_num_val, block_num, stride_val) in enumerate(zip(map_num[:3], block_nums[:3], stride[:3])):
            layer = self.blocklayer(
                num_filter * map_num_val,
                block_num,
                kernel_size=self.kernel_size,
                stride=stride_val,
            )
            setattr(self, f"layer{layer_idx + 1}", layer)

    def blocklayer(self, out_channels: int, block_nums: int, kernel_size: int, stride: int = 1) -> nn.Sequential:
        """
        Build a single ResNet layer containing multiple residual blocks.

        Args:
            out_channels (`int`): Number of output channels for the layer
            block_nums (`int`): Number of residual blocks in the layer
            kernel_size (`int`): Size of convolutional kernel
            stride (`int`, *optional*, defaults to 1): Stride for the first block (downsampling)

        Returns:
            `nn.Sequential`: ResNet layer with multiple residual blocks
        """
        downsample = None
        if stride != 1 or self.in_channels != out_channels:
            downsample = nn.Sequential(
                conv3x3(
                    self.in_channels,
                    out_channels,
                    kernel_size=kernel_size,
                    stride=stride,
                ),
                nn.BatchNorm2d(out_channels),
            )

        layers = []
        for i in range(block_nums):
            layers.append(
                ResidualBlockWithDilation(
                    in_channels=self.in_channels if i == 0 else out_channels,
                    out_channels=out_channels,
                    kernel_size=kernel_size,
                    stride=stride if i == 0 else 1,
                    downsample=downsample if i == 0 else None,
                    is_top=i == 0,
                )
            )
        self.in_channels = out_channels
        return nn.Sequential(*layers)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass of ResNet backbone (downsampling path).

        Args:
            x (`torch.Tensor`): Input tensor of shape [B, C, H, W]

        Returns:
            `torch.Tensor`: Output feature map from the last ResNet layer (layer3)
        """
        out1 = self.layer1(x)
        out2 = self.layer2(out1)
        out3 = self.layer3(out2)
        return out3


@dataclass
class UVDocModelOutput(ModelOutput):
    """
    Output class for UVDoc model forward pass.

    Args:
        logits (`torch.FloatTensor`, *optional*):
            Rectified document image tensor of shape [B, C, H, W]
        last_hidden_state (`torch.FloatTensor`, *optional*):
            Last hidden state from bridge layers of shape [B, C, H, W]
        hidden_states (`tuple(torch.FloatTensor)`, *optional*):
            Tuple of hidden states from each bridge layer (if output_hidden_states=True)
    """

    logits: Optional[torch.FloatTensor] = None
    last_hidden_state: Optional[torch.FloatTensor] = None
    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None


class UVDocPreTrainedModel(PreTrainedModel):
    """
    Base class for all UVDoc pre-trained models.
    Inherits from Hugging Face PreTrainedModel and sets UVDoc-specific configurations.
    """

    config: UVDocConfig
    base_model_prefix = "uvdoc"
    main_input_name = "pixel_values"
    input_modalities = ("image",)


def dilated_conv_bn_act(in_channels: int, out_channels: int, dilation: int) -> nn.Sequential:
    """
    Build a dilated convolution block with BatchNorm and ReLU activation.
    Used for UVDoc bridge layers to extract multi-scale geometric features.

    Args:
        in_channels (`int`): Number of input channels
        out_channels (`int`): Number of output channels
        dilation (`int`): Dilation rate for dilated convolution

    Returns:
        `nn.Sequential`: Dilated conv â†’ BN â†’ ReLU block
    """
    model = nn.Sequential(
        nn.Conv2d(
            in_channels,
            out_channels,
            bias=False,
            kernel_size=3,
            stride=1,
            padding=dilation,
            dilation=dilation,
        ),
        nn.BatchNorm2d(out_channels),
        nn.ReLU(),
    )
    return model


class UVDocModel(UVDocPreTrainedModel):
    """
    Core UVDoc model for document rectification.
    Combines ResNet backbone, multi-scale bridge layers, and spatial transformation
    to correct perspective distortion in document images.
    """

    def __init__(self, config: UVDocConfig) -> None:
        """
        Initialize UVDoc core model with configuration.

        Args:
            config (`UVDocConfig`): UVDoc model configuration
        """
        super().__init__(config)

        self.upsample_size = config.upsample_size
        self.upsample_mode = config.upsample_mode
        self.resnet_head = nn.Sequential(
            nn.Conv2d(
                config.in_channels,
                config.num_filter * config.map_num[0],
                bias=False,
                kernel_size=config.kernel_size,
                stride=2,
                padding=config.kernel_size // 2,
            ),
            nn.BatchNorm2d(config.num_filter * config.map_num[0]),
            nn.ReLU(),
            nn.Conv2d(
                config.num_filter * config.map_num[0],
                config.num_filter * config.map_num[0],
                bias=False,
                kernel_size=config.kernel_size,
                stride=2,
                padding=config.kernel_size // 2,
            ),
            nn.BatchNorm2d(config.num_filter * config.map_num[0]),
            nn.ReLU(),
        )

        self.resnet_down = ResnetStraight(
            config.num_filter,
            config.map_num,
            block_nums=config.block_nums,
            kernel_size=config.kernel_size,
            stride=config.stride,
        )

        bridge_in_channels = config.num_filter * config.map_num[2]

        def _build_bridge(bridge_key: str) -> nn.Sequential:
            """
            Build bridge layer with specified dilation values from config.
            Supports both single dilation rate and multiple dilation rates.

            Args:
                bridge_key (`str`): Key for dilation values in config (e.g., "bridge_1")

            Returns:
                `nn.Sequential`: Bridge layer with dilated convolution blocks
            """
            dilation = config.dilation_values[bridge_key]
            if isinstance(dilation, int):
                return nn.Sequential(dilated_conv_bn_act(bridge_in_channels, bridge_in_channels, dilation=dilation))
            else:
                return nn.Sequential(
                    *[dilated_conv_bn_act(bridge_in_channels, bridge_in_channels, dilation=d) for d in dilation]
                )

        self.bridge_1 = _build_bridge("bridge_1")
        self.bridge_2 = _build_bridge("bridge_2")
        self.bridge_3 = _build_bridge("bridge_3")
        self.bridge_4 = _build_bridge("bridge_4")
        self.bridge_5 = _build_bridge("bridge_5")
        self.bridge_6 = _build_bridge("bridge_6")

        self.bridge_concat = nn.Sequential(
            nn.Conv2d(
                config.num_filter * config.map_num[2] * 6,
                config.num_filter * config.map_num[2],
                bias=False,
                kernel_size=1,
                stride=1,
                padding=0,
            ),
            nn.BatchNorm2d(config.num_filter * config.map_num[2]),
            nn.ReLU(),
        )

        self.out_point_positions2D = nn.Sequential(
            nn.Conv2d(
                config.num_filter * config.map_num[2],
                config.num_filter * config.map_num[0],
                bias=False,
                kernel_size=config.kernel_size,
                stride=1,
                padding=config.kernel_size // 2,
                padding_mode=config.padding_mode,
            ),
            nn.BatchNorm2d(config.num_filter * config.map_num[0]),
            nn.PReLU(),
            nn.Conv2d(
                config.num_filter * config.map_num[0],
                2,
                kernel_size=config.kernel_size,
                stride=1,
                padding=config.kernel_size // 2,
                padding_mode=config.padding_mode,
            ),
        )

        self.post_init()

    def forward(
        self,
        hidden_state: torch.FloatTensor,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        **kwargs: Any,
    ) -> Union[tuple[torch.FloatTensor, ...], UVDocModelOutput]:
        """
        Forward pass of UVDoc core model for document rectification.

        Args:
            hidden_state (`torch.FloatTensor`): Input image tensor of shape [B, C, H, W]
            output_hidden_states (`bool`, *optional*):
                Whether to return hidden states from bridge layers
            return_dict (`bool`, *optional*):
                Whether to return a UVDocModelOutput object instead of a plain tuple

        Returns:
            `Union[Tuple[torch.FloatTensor, ...], UVDocModelOutput]`:
                - If return_dict=True: UVDocModelOutput with logits, last_hidden_state, hidden_states
                - If return_dict=False: Tuple containing (last_hidden_state, [hidden_states], logits)
        """
        hidden_states = () if output_hidden_states else None

        image = hidden_state
        h_ori, w_ori = hidden_state.shape[2:]
        hidden_state = F.interpolate(
            hidden_state,
            size=(self.upsample_size[0], self.upsample_size[1]),
            mode=self.upsample_mode,
            align_corners=True,
        )
        hidden_state = self.resnet_head(hidden_state)
        resnet_down = self.resnet_down(hidden_state)
        if output_hidden_states:
            hidden_states = hidden_states + (resnet_down,)
        bridge_1 = self.bridge_1(resnet_down)
        if output_hidden_states:
            hidden_states = hidden_states + (bridge_1,)
        bridge_2 = self.bridge_2(resnet_down)
        if output_hidden_states:
            hidden_states = hidden_states + (bridge_2,)
        bridge_3 = self.bridge_3(resnet_down)
        if output_hidden_states:
            hidden_states = hidden_states + (bridge_3,)
        bridge_4 = self.bridge_4(resnet_down)
        if output_hidden_states:
            hidden_states = hidden_states + (bridge_4,)
        bridge_5 = self.bridge_5(resnet_down)
        if output_hidden_states:
            hidden_states = hidden_states + (bridge_5,)
        bridge_6 = self.bridge_6(resnet_down)
        if output_hidden_states:
            hidden_states = hidden_states + (bridge_6,)
        last_hidden_state = bridge_6

        bridge_concat = torch.cat([bridge_1, bridge_2, bridge_3, bridge_4, bridge_5, bridge_6], dim=1)
        bridge = self.bridge_concat(bridge_concat)

        out_point_positions2D = self.out_point_positions2D(bridge)

        bm_up = F.interpolate(
            out_point_positions2D,
            size=(h_ori, w_ori),
            mode=self.upsample_mode,
            align_corners=True,
        )
        bm = bm_up.permute(0, 2, 3, 1)
        out = F.grid_sample(image, bm, align_corners=True)

        if not return_dict:
            output = (last_hidden_state,)
            if output_hidden_states:
                output += (hidden_states,)
            output += (out,)
            return output

        return UVDocModelOutput(
            logits=out,
            last_hidden_state=last_hidden_state,
            hidden_states=hidden_states if output_hidden_states else None,
        )


@dataclass
class UVDocForDocumentRectificationOutput(BaseModelOutputWithNoAttention):
    """
    Output class for UVDocForDocumentRectification forward pass.
    Extends BaseModelOutputWithNoAttention with document rectification logits.

    Args:
        logits (`torch.FloatTensor`, *optional*):
            Rectified document image tensor of shape [B, C, H, W]
        shape (`torch.FloatTensor`, *optional*):
            Reserved for future use (shape information)
    """

    logits: Optional[torch.FloatTensor] = None
    shape: Optional[torch.FloatTensor] = None


class UVDocForDocumentRectification(UVDocPreTrainedModel):
    """
    Wrapper class for UVDoc model focused on document rectification task.
    Provides a user-friendly interface for inference/training with standard Hugging Face API.
    """

    _keys_to_ignore_on_load_missing = ["num_batches_tracked"]

    def __init__(self, config: UVDocConfig) -> None:
        """
        Initialize UVDoc document rectification wrapper model.

        Args:
            config (`UVDocConfig`): UVDoc model configuration
        """
        super().__init__(config)
        self.model = UVDocModel(config)
        self.post_init()

    def forward(
        self,
        pixel_values: torch.FloatTensor,
        labels: Optional[list[dict[str, Any]]] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        **kwargs: Any,
    ) -> Union[tuple[torch.FloatTensor, ...], UVDocForDocumentRectificationOutput]:
        """
        Forward pass of UVDoc document rectification model.

        Args:
            pixel_values (`torch.FloatTensor`): Input image tensor of shape [B, C, H, W] (preprocessed)
            labels (`List[Dict[str, Any]]`, *optional*):
                Training labels (not used in inference)
            output_hidden_states (`bool`, *optional*):
                Whether to return hidden states from the core model
            return_dict (`bool`, *optional*):
                Whether to return a UVDocForDocumentRectificationOutput object instead of a plain tuple

        Returns:
            `Union[Tuple[torch.FloatTensor, ...], UVDocForDocumentRectificationOutput]`:
                - If return_dict=True: Structured output with logits, last_hidden_state, hidden_states
                - If return_dict=False: Plain tuple with corresponding values
        """
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        outputs = self.model(pixel_values, output_hidden_states=output_hidden_states, return_dict=return_dict)

        if not return_dict:
            output = (outputs[0],)
            if output_hidden_states:
                output += (outputs[1], outputs[2])
            else:
                output += (outputs[1],)

            return output
        return UVDocForDocumentRectificationOutput(
            logits=outputs.logits,
            last_hidden_state=outputs.last_hidden_state,
            hidden_states=outputs.hidden_states if output_hidden_states else None,
        )


__all__ = ["UVDocForDocumentRectification", "UVDocModel", "UVDocPreTrainedModel"]
