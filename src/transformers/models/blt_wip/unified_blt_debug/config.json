{
  "args": {
    "alpha_depth": "disabled",
    "architecture": "vanilla",
    "attn_bias_type": "block_causal",
    "attn_impl": "xformers",
    "attn_to_keep": "all",
    "conv_kernel_size": null,
    "cross_attn_all_layers_decoder": true,
    "cross_attn_all_layers_encoder": false,
    "cross_attn_decoder": true,
    "cross_attn_encoder": true,
    "cross_attn_init_by_pooling": true,
    "cross_attn_k": 2,
    "cross_attn_nheads": 16,
    "cross_attn_use_flex_attention": true,
    "cross_attn_window_decoder": null,
    "cross_attn_window_encoder": null,
    "custom_bwd": false,
    "dim": 512,
    "dim_global": 2048,
    "dim_local_decoder": 1024,
    "dim_local_encoder": 1024,
    "dim_patch_emb": null,
    "dim_token": null,
    "dim_token_emb": null,
    "downsampling_by_pooling": "max",
    "dropout": 0.0,
    "encoder_enable_byte_group_hash": false,
    "encoder_enable_byte_ngrams": false,
    "encoder_hash_byte_group_nb_functions": 1,
    "encoder_hash_byte_group_size": [
      3,
      4,
      5,
      6,
      7,
      8
    ],
    "encoder_hash_byte_group_vocab": 500002,
    "encoder_lm_loss": false,
    "encoder_ngram_table_dir": null,
    "encoder_ngram_to_size_str": null,
    "encoder_preds_low_entropy_toks": null,
    "encoder_preds_random_toks": null,
    "entropy_model_checkpoint_dir": null,
    "entropy_model_is_ngram_model": false,
    "eos_id": 2,
    "ffn_dim_multiplier": 1.0,
    "full_logging_n_layers": 4,
    "fuse_sequence_parallel": false,
    "global_local_decoder_residual_layer": null,
    "head_dim": null,
    "init_base_std": null,
    "init_std_factor": "current_depth",
    "init_use_depth": "current",
    "init_use_gaussian": true,
    "layer_ckpt": "none",
    "local_attention_window_len": 512,
    "log_patch_lengths": false,
    "loss_parallel": false,
    "max_encoder_seq_length": 24576,
    "max_length": 256,
    "max_patch_length": null,
    "max_seqlen": 4096,
    "monotonicity": false,
    "multiple_of": 256,
    "n_heads": 8,
    "n_heads_global": 16,
    "n_heads_local_decoder": 16,
    "n_heads_local_encoder": 16,
    "n_kv_heads": null,
    "n_kv_heads_global": null,
    "n_layers": 8,
    "n_layers_global": 25,
    "n_layers_local_decoder": 9,
    "n_layers_local_encoder": 1,
    "ngram_vocab_sizes": null,
    "non_linearity": "swiglu",
    "norm_affine": true,
    "norm_eps": 1e-05,
    "norm_type": "rmsnorm",
    "output_size": -1,
    "pad_to_max_length": true,
    "patch_in_forward": true,
    "patch_size": 4.5,
    "patching_batch_size": 1,
    "patching_device": "cuda",
    "patching_mode": "entropy",
    "patching_threshold": 1.335442066192627,
    "patching_threshold_add": null,
    "patching_thresholds_str": null,
    "pm_size": 0,
    "pre_norm": true,
    "recompute_attn": false,
    "recompute_fc1_out": false,
    "recompute_fc3_out": false,
    "rope_theta": 500000.0,
    "rope_use_fp32_in_outer_product": true,
    "seed": 42,
    "sequence_parallel": false,
    "share_encoder_decoder_emb": true,
    "tie_local_encoder_decoder": false,
    "tie_local_encoder_decoder_logits": false,
    "tokenize_with_bpe_delimiter": false,
    "use_fsdp": true,
    "use_local_encoder_transformer": true,
    "use_rope": true,
    "vocab_size": 260,
    "weight_tying": false
  },
  "patch_in_forward": true,
  "realtime_patching": true,
  "patching_mode": "entropy",
  "patch_size": 4.5,
  "patching_threshold": 1.335442066192627,
  "patching_threshold_add": null,
  "max_patch_length": null,
  "patching_batch_size": 1,
  "patching_device": "cuda",
  "monotonicity": false,
  "patcher_vocab_size": 260,
  "patcher_dim": 768,
  "patcher_n_layers": 14,
  "patcher_n_heads": 12,
  "patcher_head_dim": null,
  "patcher_n_kv_heads": null,
  "patcher_max_seqlen": 8192,
  "patcher_norm_eps": 1e-05,
  "patcher_dropout": 0.0,
  "patcher_sliding_window": 512,
  "patcher_ffn_dim_multiplier": 1.0,
  "patcher_multiple_of": 256,
  "patcher_rope_theta": 10000.0,
  "patcher_rope_use_fp32_in_outer_product": false,
  "patcher_attn_impl": "xformers",
  "patcher_attn_bias_type": "local_block_causal",
  "patcher_init_base_std": null,
  "patcher_init_std_factor": "current_depth",
  "patcher_dim_token_emb": null,
  "patcher_weight_tying": false,
  "patcher_bos_token_id": 1,
  "patcher_eos_token_id": 2
}