#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/isaac/modular_isaac.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_isaac.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
import math
from collections.abc import Sequence
from typing import Any

import torch

from ...feature_extraction_utils import BatchFeature
from ...image_processing_utils_fast import BaseImageProcessorFast, SizeDict
from ...image_utils import PILImageResampling
from ...processing_utils import Unpack
from ...tokenization_utils import TensorType
from ...utils import auto_docstring
from .image_processing_isaac import IsaacImageProcessorKwargs


# Vision preprocessing constants
VISION_MEAN = (0.5, 0.5, 0.5)
VISION_STD = (0.5, 0.5, 0.5)
VISION_SCALE = 1 / 255


def _normalize_rgb_values(
    values: float | Sequence[float] | tuple[float, ...],
    *,
    name: str,
) -> tuple[float, float, float]:
    """Coerce RGB normalization parameters into a 3-tuple of floats."""
    if isinstance(values, (list, tuple)):
        if len(values) == 3:
            return tuple(float(v) for v in values)
        if len(values) == 1:
            value = float(values[0])
            return (value, value, value)
        raise ValueError(f"`{name}` must have length 1 or 3 when provided as a sequence. Got length {len(values)}.")

    value = float(values)
    return (value, value, value)


# ============================================================================
# Configuration
# ============================================================================

MAX_PIXELS = 60_000_000  # 60â€‘megapixel ceiling â‰ˆ 8200 Ã— 7300 px


def get_scaled_image_size(
    scale: float,
    original_size: int,
    patch_size: int,
    pixel_shuffle_scale: int,
) -> int:
    scaled_size = scale * original_size
    divisor = patch_size * pixel_shuffle_scale
    scaled_size = math.ceil(scaled_size / divisor) * divisor
    scaled_size = max(divisor, scaled_size)
    return int(scaled_size)


def get_image_size_for_max_num_patches(
    image_height: int,
    image_width: int,
    patch_size: int,
    max_num_patches: int,
    min_num_patches: int | None = None,
    eps: float = 1e-5,
    pixel_shuffle_scale: int = 1,
) -> tuple[int, int]:
    r"""Compute a target resolution whose patch grid satisfies patching parametrization.

    Args:
        image_height (`int`):
            Height in pixels of the source image prior to any resizing.
        image_width (`int`):
            Width in pixels of the source image prior to any resizing.
        patch_size (`int`):
            Size of the square patch used by the vision encoder.
        max_num_patches (`int`):
            Upper bound on `(height / patch_size) * (width / patch_size)` after resizing.
        min_num_patches (`int`, *optional*):
            Lower bound on the number of patches. When provided the image will be scaled up if necessary.
        eps (`float`, *optional*, defaults to 1e-5):
            Convergence tolerance for the internal binary search to determing the target dimensions.
        pixel_shuffle_scale (`int`, *optional*, defaults to 1):
            Additional stride multiplier applied when pixel shuffle later reduces spatial resolution.

    Returns:
        `tuple[int, int]`: Height and width (in pixels) that are multiples of `patch_size * pixel_shuffle_scale`
        and respect both the maximum and optional minimum patch-count constraints.
    """

    # Ensure divisibility
    divisor = patch_size * pixel_shuffle_scale
    adjusted_height = math.ceil(image_height / divisor) * divisor
    adjusted_height = max(divisor, adjusted_height)
    adjusted_width = math.ceil(image_width / divisor) * divisor
    adjusted_width = max(divisor, adjusted_width)

    num_patches = (adjusted_height / patch_size) * (adjusted_width / patch_size)

    if min_num_patches is not None and num_patches < min_num_patches:
        # Scale up
        scale_min, scale_max = 1.0, 100.0
        while (scale_max - scale_min) >= eps:
            scale = (scale_min + scale_max) / 2
            target_height = get_scaled_image_size(scale, image_height, patch_size, pixel_shuffle_scale)
            target_width = get_scaled_image_size(scale, image_width, patch_size, pixel_shuffle_scale)
            num_patches = (target_height / patch_size) * (target_width / patch_size)
            if num_patches >= min_num_patches:
                scale_max = scale
            else:
                scale_min = scale
        scale = scale_max
        target_height = get_scaled_image_size(scale, image_height, patch_size, pixel_shuffle_scale)
        target_width = get_scaled_image_size(scale, image_width, patch_size, pixel_shuffle_scale)
        return target_height, target_width
    elif num_patches <= max_num_patches:
        return adjusted_height, adjusted_width
    else:
        # Scale down
        scale_min, scale_max = eps / 10, 1.0
        while (scale_max - scale_min) >= eps:
            scale = (scale_min + scale_max) / 2
            target_height = get_scaled_image_size(scale, image_height, patch_size, pixel_shuffle_scale)
            target_width = get_scaled_image_size(scale, image_width, patch_size, pixel_shuffle_scale)
            num_patches = (target_height / patch_size) * (target_width / patch_size)
            if num_patches <= max_num_patches:
                scale_min = scale
            else:
                scale_max = scale
        scale = scale_min
        target_height = get_scaled_image_size(scale, image_height, patch_size, pixel_shuffle_scale)
        target_width = get_scaled_image_size(scale, image_width, patch_size, pixel_shuffle_scale)
        return target_height, target_width


def patchify_vision(image: torch.Tensor, patch_size: int) -> torch.Tensor:
    r"""Convert normalized images into flattened ViT-style patches.

    Args:
        image (`torch.Tensor`):
            Tensor of shape `(num_images, height, width, channels)`.
        patch_size (`int`):
            Edge length of the square patches

    Returns:
        `torch.Tensor`:
            Patch tensor where each position stores the flattened pixels belonging to that patch.

    Raises:
        ValueError: If `height` or `width` is not divisible by `patch_size`.
    """
    num_images, height, width, channels = image.shape
    if height % patch_size or width % patch_size:
        raise ValueError(f"Dimensions of images {image.shape} are not divisible by patch_size={patch_size}.")
    patches = image.reshape(num_images, height // patch_size, patch_size, width // patch_size, patch_size, channels)
    patches = patches.permute(0, 1, 3, 2, 4, 5)
    patches = patches.reshape(
        num_images, height // patch_size, width // patch_size, channels * patch_size * patch_size
    )
    return patches


def _compute_residual_p_frames(frames: torch.Tensor, is_p_frame: list[bool]) -> torch.Tensor:
    """Compute residuals for P-frames to stay in sync with the training pipeline."""
    if not any(is_p_frame):
        return frames

    frame_indices = torch.arange(len(is_p_frame), device=frames.device)
    i_frame_mask = torch.tensor([not flag for flag in is_p_frame], device=frames.device)
    last_i_indices = torch.cummax((i_frame_mask * (1 + frame_indices)), dim=0).values.long() - 1
    p_indices = frame_indices[torch.tensor(is_p_frame, device=frames.device)]
    frames[p_indices] = frames[p_indices] - frames[last_i_indices[p_indices]]
    return frames


@auto_docstring
class IsaacImageProcessorFast(BaseImageProcessorFast):
    r"""Fast torch-based image processor for Isaac vision inputs."""

    slow_image_processor_class = "IsaacImageProcessor"

    resample = PILImageResampling.BILINEAR
    model_input_names = ["patches", "token_grids"]
    valid_kwargs = IsaacImageProcessorKwargs
    unused_kwargs = ["size", "do_center_crop", "crop_size"]

    def __init__(
        self,
        *,
        patch_size: int = 16,
        max_num_patches: int = 256,
        min_num_patches: int | None = None,
        pixel_shuffle_scale: int = 1,
        do_rescale: bool = True,
        rescale_factor: float | None = None,
        do_normalize: bool = True,
        image_mean: float | Sequence[float] | None = None,
        image_std: float | Sequence[float] | None = None,
        do_convert_rgb: bool = True,
        **kwargs: Unpack[IsaacImageProcessorKwargs],
    ) -> None:
        super().__init__(**kwargs)

        if pixel_shuffle_scale < 1:
            raise ValueError("`pixel_shuffle_scale` must be >= 1")

        mean_values = _normalize_rgb_values(image_mean if image_mean is not None else VISION_MEAN, name="image_mean")
        std_values = _normalize_rgb_values(image_std if image_std is not None else VISION_STD, name="image_std")

        self.patch_size = patch_size
        self.max_num_patches = max_num_patches
        self.min_num_patches = min_num_patches
        self.pixel_shuffle_scale = pixel_shuffle_scale
        self.do_rescale = do_rescale
        self.rescale_factor = VISION_SCALE if rescale_factor is None else float(rescale_factor)
        self.do_normalize = do_normalize
        self.image_mean = list(mean_values)
        self.image_std = list(std_values)
        self.do_convert_rgb = do_convert_rgb

    def _validate_preprocess_kwargs(self, **kwargs):
        # Allow callers to omit resize-related placeholders that BaseImageProcessorFast checks for.
        kwargs.pop("do_resize", None)
        return super()._validate_preprocess_kwargs(**kwargs)

    def _preprocess(
        self,
        images: list["torch.Tensor"],
        do_resize: bool,
        patch_size: int,
        max_num_patches: int,
        interpolation: Any | None,
        do_rescale: bool,
        rescale_factor: float,
        do_normalize: bool,
        image_mean: float | Sequence[float] | None,
        image_std: float | Sequence[float] | None,
        return_tensors: str | TensorType | None,
        *,
        min_num_patches: int | None = None,
        pixel_shuffle_scale: int | None = None,
        do_convert_rgb: bool | None = None,
        **kwargs,
    ) -> BatchFeature:
        if TVF is None:
            raise ImportError("torchvision is required for IsaacImageProcessorFast but is not installed.")

        min_num_patches = min_num_patches if min_num_patches is not None else self.min_num_patches
        pixel_shuffle_scale = pixel_shuffle_scale if pixel_shuffle_scale is not None else self.pixel_shuffle_scale
        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb
        do_rescale = self.do_rescale if do_rescale is None else do_rescale
        do_normalize = self.do_normalize if do_normalize is None else do_normalize
        rescale_factor = self.rescale_factor if rescale_factor is None else rescale_factor

        mean_values = _normalize_rgb_values(
            image_mean if image_mean is not None else self.image_mean, name="image_mean"
        )
        std_values = _normalize_rgb_values(image_std if image_std is not None else self.image_std, name="image_std")

        patches_list: list[torch.Tensor] = []
        token_grids: list[torch.Tensor] = []
        virtual_dims: list[list[int]] = []
        real_dims: list[list[int]] = []

        for image in images:
            if image.ndim != 3:
                raise ValueError("Expected channel-first image tensor with shape (C, H, W).")

            channels, original_height, original_width = image.shape
            if do_convert_rgb and channels == 1:
                image = image.repeat(3, 1, 1)
                channels = 3

            if original_height * original_width > MAX_PIXELS:
                raise ValueError(f"Image (w={original_width}, h={original_height}) > MAX=`{MAX_PIXELS}`")

            target_height, target_width = get_image_size_for_max_num_patches(
                original_height,
                original_width,
                patch_size,
                max_num_patches,
                min_num_patches=min_num_patches,
                pixel_shuffle_scale=pixel_shuffle_scale,
            )

            if do_resize:
                size_dict = SizeDict(height=target_height, width=target_width)
                image = self.resize(image=image, size=size_dict, interpolation=interpolation)
            else:
                if ((original_height % patch_size) != 0) or ((original_width % patch_size) != 0):
                    raise ValueError("Image dimensions must be divisible by patch_size when resize is disabled.")

            # Apply rescaling and normalization as needed
            image = self.rescale_and_normalize(
                image,
                do_rescale,
                rescale_factor,
                do_normalize,
                list(mean_values),
                list(std_values),
            )

            # Convert to NHWC for residual P-frame adjustment and patch extraction
            nhwc_image = image.permute(1, 2, 0).unsqueeze(0)
            nhwc_image = _compute_residual_p_frames(nhwc_image, is_p_frame=[False])

            patches = patchify_vision(nhwc_image, patch_size=patch_size).squeeze(0)
            height_tokens, width_tokens, _ = patches.shape

            patches_list.append(patches.unsqueeze(0))
            token_grids.append(torch.tensor([height_tokens, width_tokens], dtype=torch.long, device=patches.device))

            real_dims.append([1, height_tokens, width_tokens])
            if pixel_shuffle_scale > 1:
                if (height_tokens % pixel_shuffle_scale) or (width_tokens % pixel_shuffle_scale):
                    raise ValueError(
                        "Spatial dimensions must be divisible by pixel_shuffle_scale when pixel shuffle is enabled."
                    )
                virtual_dims.append([1, height_tokens // pixel_shuffle_scale, width_tokens // pixel_shuffle_scale])
            else:
                virtual_dims.append([1, height_tokens, width_tokens])

        patches_tensor = torch.cat(patches_list, dim=0)
        token_grids_tensor = torch.stack(token_grids, dim=0)
        virtual_dims_tensor = torch.tensor(virtual_dims, dtype=torch.long, device=patches_tensor.device)
        real_dims_tensor = torch.tensor(real_dims, dtype=torch.long, device=patches_tensor.device)

        batch_feature = BatchFeature(
            data={
                "patches": patches_tensor,
                "token_grids": token_grids_tensor,
                "virtual_pixel_size": virtual_dims_tensor,
                "real_pixel_size": real_dims_tensor,
            },
            tensor_type=return_tensors,
        )
        return batch_feature


__all__ = ["IsaacImageProcessorFast"]
