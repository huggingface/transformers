#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/isaac/modular_isaac.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_isaac.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
import math
from collections.abc import Sequence

import numpy as np
import torch
import torch.nn.functional as F

from ...feature_extraction_utils import BatchFeature
from ...image_processing_utils import BaseImageProcessor
from ...image_transforms import convert_to_rgb
from ...image_utils import (
    ImageInput,
    make_flat_list_of_images,
    to_numpy_array,
    valid_images,
    validate_preprocess_arguments,
)
from ...processing_utils import ImagesKwargs
from ...tokenization_utils import TensorType
from ...utils import filter_out_non_signature_kwargs


class IsaacImageProcessorKwargs(ImagesKwargs):
    patch_size: int | None
    max_num_patches: int | None
    min_num_patches: int | None
    pixel_shuffle_scale: int | None
    do_rescale: bool | None
    rescale_factor: float | None
    do_normalize: bool | None
    image_mean: float | Sequence[float] | None
    image_std: float | Sequence[float] | None
    do_convert_rgb: bool | None


# Vision preprocessing constants
VISION_MEAN = (0.5, 0.5, 0.5)
VISION_STD = (0.5, 0.5, 0.5)
VISION_SCALE = 1 / 255


def _normalize_rgb_values(
    values: float | Sequence[float] | tuple[float, ...],
    *,
    name: str,
) -> tuple[float, float, float]:
    """Coerce RGB normalization parameters into a 3-tuple of floats."""
    if isinstance(values, (list, tuple)):
        if len(values) == 3:
            return tuple(float(v) for v in values)
        if len(values) == 1:
            value = float(values[0])
            return (value, value, value)
        raise ValueError(f"`{name}` must have length 1 or 3 when provided as a sequence. Got length {len(values)}.")

    value = float(values)
    return (value, value, value)


def _make_writeable(arr: np.ndarray) -> np.ndarray:
    if arr.flags.writeable:
        return arr
    try:
        arr.setflags(write=True)
        return arr
    except ValueError:
        return arr.copy()


# ============================================================================
# Configuration
# ============================================================================

MAX_PIXELS = 60_000_000  # 60â€‘megapixel ceiling â‰ˆ 8200 Ã— 7300 px


def get_scaled_image_size(
    scale: float,
    original_size: int,
    patch_size: int,
    pixel_shuffle_scale: int,
) -> int:
    scaled_size = scale * original_size
    divisor = patch_size * pixel_shuffle_scale
    scaled_size = math.ceil(scaled_size / divisor) * divisor
    scaled_size = max(divisor, scaled_size)
    return int(scaled_size)


def get_image_size_for_max_num_patches(
    image_height: int,
    image_width: int,
    patch_size: int,
    max_num_patches: int,
    min_num_patches: int | None = None,
    eps: float = 1e-5,
    pixel_shuffle_scale: int = 1,
) -> tuple[int, int]:
    r"""Compute a target resolution whose patch grid satisfies patching parametrization.

    Args:
        image_height (`int`):
            Height in pixels of the source image prior to any resizing.
        image_width (`int`):
            Width in pixels of the source image prior to any resizing.
        patch_size (`int`):
            Size of the square patch used by the vision encoder.
        max_num_patches (`int`):
            Upper bound on `(height / patch_size) * (width / patch_size)` after resizing.
        min_num_patches (`int`, *optional*):
            Lower bound on the number of patches. When provided the image will be scaled up if necessary.
        eps (`float`, *optional*, defaults to 1e-5):
            Convergence tolerance for the internal binary search to determing the target dimensions.
        pixel_shuffle_scale (`int`, *optional*, defaults to 1):
            Additional stride multiplier applied when pixel shuffle later reduces spatial resolution.

    Returns:
        `tuple[int, int]`: Height and width (in pixels) that are multiples of `patch_size * pixel_shuffle_scale`
        and respect both the maximum and optional minimum patch-count constraints.
    """

    # Ensure divisibility
    divisor = patch_size * pixel_shuffle_scale
    adjusted_height = math.ceil(image_height / divisor) * divisor
    adjusted_height = max(divisor, adjusted_height)
    adjusted_width = math.ceil(image_width / divisor) * divisor
    adjusted_width = max(divisor, adjusted_width)

    num_patches = (adjusted_height / patch_size) * (adjusted_width / patch_size)

    if min_num_patches is not None and num_patches < min_num_patches:
        # Scale up
        scale_min, scale_max = 1.0, 100.0
        while (scale_max - scale_min) >= eps:
            scale = (scale_min + scale_max) / 2
            target_height = get_scaled_image_size(scale, image_height, patch_size, pixel_shuffle_scale)
            target_width = get_scaled_image_size(scale, image_width, patch_size, pixel_shuffle_scale)
            num_patches = (target_height / patch_size) * (target_width / patch_size)
            if num_patches >= min_num_patches:
                scale_max = scale
            else:
                scale_min = scale
        scale = scale_max
        target_height = get_scaled_image_size(scale, image_height, patch_size, pixel_shuffle_scale)
        target_width = get_scaled_image_size(scale, image_width, patch_size, pixel_shuffle_scale)
        return target_height, target_width
    elif num_patches <= max_num_patches:
        return adjusted_height, adjusted_width
    else:
        # Scale down
        scale_min, scale_max = eps / 10, 1.0
        while (scale_max - scale_min) >= eps:
            scale = (scale_min + scale_max) / 2
            target_height = get_scaled_image_size(scale, image_height, patch_size, pixel_shuffle_scale)
            target_width = get_scaled_image_size(scale, image_width, patch_size, pixel_shuffle_scale)
            num_patches = (target_height / patch_size) * (target_width / patch_size)
            if num_patches <= max_num_patches:
                scale_min = scale
            else:
                scale_max = scale
        scale = scale_min
        target_height = get_scaled_image_size(scale, image_height, patch_size, pixel_shuffle_scale)
        target_width = get_scaled_image_size(scale, image_width, patch_size, pixel_shuffle_scale)
        return target_height, target_width


def patchify_vision(image: torch.Tensor, patch_size: int) -> torch.Tensor:
    r"""Convert normalized images into flattened ViT-style patches.

    Args:
        image (`torch.Tensor`):
            Tensor of shape `(num_images, height, width, channels)`.
        patch_size (`int`):
            Edge length of the square patches

    Returns:
        `torch.Tensor`:
            Patch tensor where each position stores the flattened pixels belonging to that patch.

    Raises:
        ValueError: If `height` or `width` is not divisible by `patch_size`.
    """
    num_images, height, width, channels = image.shape
    if height % patch_size or width % patch_size:
        raise ValueError(f"Dimensions of images {image.shape} are not divisible by patch_size={patch_size}.")
    patches = image.reshape(num_images, height // patch_size, patch_size, width // patch_size, patch_size, channels)
    patches = patches.permute(0, 1, 3, 2, 4, 5)
    patches = patches.reshape(
        num_images, height // patch_size, width // patch_size, channels * patch_size * patch_size
    )
    return patches


def _prepare_image_tensor(
    image: torch.Tensor, scale: float, mean: tuple[float, ...], std: tuple[float, ...]
) -> torch.Tensor:
    """Mirror the prepare_image_tensor utility used in the training pipelines."""
    if not torch.is_floating_point(image):
        image = image.float()

    rescaled = image * scale
    mean_tensor = torch.tensor(mean, dtype=torch.float32, device=rescaled.device).view(1, 1, 1, -1)
    std_tensor = torch.tensor(std, dtype=torch.float32, device=rescaled.device).view(1, 1, 1, -1)
    normalized = (rescaled - mean_tensor) / std_tensor
    return normalized


def _compute_residual_p_frames(frames: torch.Tensor, is_p_frame: list[bool]) -> torch.Tensor:
    """Compute residuals for P-frames to stay in sync with the training pipeline."""
    if not any(is_p_frame):
        return frames

    frame_indices = torch.arange(len(is_p_frame), device=frames.device)
    i_frame_mask = torch.tensor([not flag for flag in is_p_frame], device=frames.device)
    last_i_indices = torch.cummax((i_frame_mask * (1 + frame_indices)), dim=0).values.long() - 1
    p_indices = frame_indices[torch.tensor(is_p_frame, device=frames.device)]
    frames[p_indices] = frames[p_indices] - frames[last_i_indices[p_indices]]
    return frames


class IsaacImageProcessor(BaseImageProcessor):
    """Image processor that prepares RGB frames for the Isaac vision encoder."""

    model_input_names = ["patches", "token_grids"]

    def __init__(
        self,
        patch_size: int = 16,
        max_num_patches: int = 256,
        min_num_patches: int | None = None,
        pixel_shuffle_scale: int = 1,
        do_rescale: bool = True,
        rescale_factor: float | None = None,
        do_normalize: bool = True,
        image_mean: float | Sequence[float] | None = None,
        image_std: float | Sequence[float] | None = None,
        do_convert_rgb: bool = True,
        resize_mode: str = "bilinear",
        align_corners: bool = False,
        **kwargs,
    ) -> None:
        super().__init__(**kwargs)

        if pixel_shuffle_scale < 1:
            raise ValueError("`pixel_shuffle_scale` must be >= 1")

        rescale_value = VISION_SCALE if rescale_factor is None else float(rescale_factor)
        mean_value = VISION_MEAN if image_mean is None else image_mean
        std_value = VISION_STD if image_std is None else image_std

        self.patch_size = patch_size
        self.max_num_patches = max_num_patches
        self.min_num_patches = min_num_patches
        self.pixel_shuffle_scale = pixel_shuffle_scale
        self.do_rescale = do_rescale
        self.rescale_factor = rescale_value
        self.do_normalize = do_normalize
        self.image_mean = _normalize_rgb_values(mean_value, name="image_mean")
        self.image_std = _normalize_rgb_values(std_value, name="image_std")
        self.do_convert_rgb = do_convert_rgb
        self.resize_mode = resize_mode
        self.align_corners = align_corners

    @filter_out_non_signature_kwargs()
    def preprocess(
        self,
        images: ImageInput,
        patch_size: int | None = None,
        max_num_patches: int | None = None,
        min_num_patches: int | None = None,
        pixel_shuffle_scale: int | None = None,
        do_rescale: bool | None = None,
        rescale_factor: float | None = None,
        do_normalize: bool | None = None,
        image_mean: float | Sequence[float] | None = None,
        image_std: float | Sequence[float] | None = None,
        do_convert_rgb: bool | None = None,
        return_tensors: str | TensorType | None = None,
    ) -> BatchFeature:
        patch_size = patch_size if patch_size is not None else self.patch_size
        max_num_patches = max_num_patches if max_num_patches is not None else self.max_num_patches
        min_num_patches = min_num_patches if min_num_patches is not None else self.min_num_patches
        pixel_shuffle_scale = pixel_shuffle_scale if pixel_shuffle_scale is not None else self.pixel_shuffle_scale
        do_rescale = self.do_rescale if do_rescale is None else do_rescale
        rescale_factor = self.rescale_factor if rescale_factor is None else rescale_factor
        do_normalize = self.do_normalize if do_normalize is None else do_normalize
        image_mean = self.image_mean if image_mean is None else _normalize_rgb_values(image_mean, name="image_mean")
        image_std = self.image_std if image_std is None else _normalize_rgb_values(image_std, name="image_std")
        do_convert_rgb = self.do_convert_rgb if do_convert_rgb is None else do_convert_rgb

        images = self.fetch_images(images)
        images = make_flat_list_of_images(images)

        if not images:
            raise ValueError("Received an empty list of images for preprocessing.")
        if do_convert_rgb:
            images = [convert_to_rgb(image) for image in images]

        if not valid_images(images):
            raise ValueError(
                "Invalid image type. Expected PIL images, numpy arrays, or tensors convertible to numpy arrays."
            )

        validate_preprocess_arguments(
            do_rescale=do_rescale,
            rescale_factor=rescale_factor,
            do_normalize=do_normalize,
            image_mean=image_mean,
            image_std=image_std,
        )

        patches_list = []
        token_grids = []
        virtual_dims = []
        real_dims = []

        for image in images:
            np_image = to_numpy_array(image)

            if np_image.ndim == 2:
                np_image = np.repeat(np_image[..., None], 3, axis=-1)

            height, width = np_image.shape[:2]
            if height * width > MAX_PIXELS:
                raise ValueError(f"Image (w={width}, h={height}) > MAX=`{MAX_PIXELS}`")

            torch_image = torch.from_numpy(_make_writeable(np_image))
            patches, vidims, rdims = self._process_single_image(
                torch_image,
                patch_size=patch_size,
                max_num_patches=max_num_patches,
                min_num_patches=min_num_patches,
                pixel_shuffle_scale=pixel_shuffle_scale,
                do_rescale=do_rescale,
                rescale_factor=rescale_factor,
                do_normalize=do_normalize,
                image_mean=image_mean,
                image_std=image_std,
            )

            patches_list.append(patches)
            token_grids.append(torch.tensor([patches.size(1), patches.size(2)], dtype=torch.long))
            virtual_dims.append(vidims)
            real_dims.append(rdims)

        patches_tensor = torch.cat(patches_list, dim=0)
        token_grid_tensor = torch.stack(token_grids, dim=0)
        virtual_dims_tensor = torch.tensor(virtual_dims, dtype=torch.long)
        real_dims_tensor = torch.tensor(real_dims, dtype=torch.long)

        data = {
            "patches": patches_tensor,
            "token_grids": token_grid_tensor,
            "virtual_pixel_size": virtual_dims_tensor,
            "real_pixel_size": real_dims_tensor,
        }

        return BatchFeature(data=data, tensor_type=return_tensors)

    def _process_single_image(
        self,
        image: torch.Tensor,
        *,
        patch_size: int,
        max_num_patches: int,
        min_num_patches: int | None,
        pixel_shuffle_scale: int,
        do_rescale: bool,
        rescale_factor: float,
        do_normalize: bool,
        image_mean: tuple[float, ...],
        image_std: tuple[float, ...],
    ) -> tuple[torch.Tensor, list[int], list[int]]:
        image_uint8 = image.unsqueeze(0)  # (1, H, W, C)
        image_chw = image_uint8.permute(0, 3, 1, 2)  # (1, C, H, W)

        _, _, orig_height, orig_width = image_chw.shape
        target_height, target_width = get_image_size_for_max_num_patches(
            orig_height,
            orig_width,
            patch_size,
            max_num_patches,
            min_num_patches=min_num_patches,
            pixel_shuffle_scale=pixel_shuffle_scale,
        )

        if self.resize_mode in {"linear", "bilinear", "bicubic", "trilinear"}:
            resized = F.interpolate(
                image_chw,
                size=(target_height, target_width),
                mode=self.resize_mode,
                align_corners=self.align_corners,
            )
        else:
            resized = F.interpolate(
                image_chw,
                size=(target_height, target_width),
                mode=self.resize_mode,
            )

        resized = resized.permute(0, 2, 3, 1)  # (1, H, W, C)

        scale = rescale_factor if do_rescale else 1.0
        mean = image_mean if do_normalize else (0.0, 0.0, 0.0)
        std = image_std if do_normalize else (1.0, 1.0, 1.0)
        resized = _prepare_image_tensor(resized, scale=scale, mean=mean, std=std)

        resized = _compute_residual_p_frames(resized, is_p_frame=[False])

        patches = patchify_vision(resized, patch_size=patch_size)
        _, h_patches, w_patches, _ = patches.shape

        real_dims = [1, h_patches, w_patches]
        if pixel_shuffle_scale > 1:
            if (h_patches % pixel_shuffle_scale) or (w_patches % pixel_shuffle_scale):
                raise ValueError(
                    "Spatial dimensions must be divisible by pixel_shuffle_scale when pixel shuffle is enabled."
                )
            virtual_dims = [1, h_patches // pixel_shuffle_scale, w_patches // pixel_shuffle_scale]
        else:
            virtual_dims = real_dims.copy()

        return patches, virtual_dims, real_dims


__all__ = ["IsaacImageProcessor"]
