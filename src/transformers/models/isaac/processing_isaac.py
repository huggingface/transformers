#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/isaac/modular_isaac.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_isaac.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
import math
import re
from collections.abc import Sequence

import PIL.Image
import torch
from genesis.public.tensorstream.tensor_stream import Event, Stream, TensorStream, TextType, VisionType, create_stream
from genesis.public.tensorstream.tensor_stream_utils import slice as ts_slice
from genesis.public.tensorstream.tensor_stream_utils import tensor_stream_token_view

from ...feature_extraction_utils import BatchFeature
from ...image_processing_utils_fast import BaseImageProcessorFast
from ...processing_utils import ProcessorMixin
from ...tokenization_utils import TensorType
from .configuration_isaac import IsaacConfig
from .image_processing_isaac import IsaacImageProcessor


# Vision preprocessing constants
VISION_MEAN = (0.5, 0.5, 0.5)
VISION_STD = (0.5, 0.5, 0.5)
VISION_SCALE = 1 / 255


def _normalize_rgb_values(
    values: float | Sequence[float] | tuple[float, ...],
    *,
    name: str,
) -> tuple[float, float, float]:
    """Coerce RGB normalization parameters into a 3-tuple of floats."""
    if isinstance(values, (list, tuple)):
        if len(values) == 3:
            return tuple(float(v) for v in values)
        if len(values) == 1:
            value = float(values[0])
            return (value, value, value)
        raise ValueError(f"`{name}` must have length 1 or 3 when provided as a sequence. Got length {len(values)}.")

    value = float(values)
    return (value, value, value)


# ============================================================================
# Processor Components
# ============================================================================


def create_text_event(tokenizer: AutoTokenizer, text: str, time: float = 0.0) -> Event:
    r"""Wrap a text into an `Event` compatible with the multimodal TensorStream.

    Args:
        tokenizer (`AutoTokenizer`):
            Tokenizer used to convert text into model vocabulary ids.
        text (`str`):
            Plain-text fragment to encode.
        time (`float`, *optional*, defaults to 0.0):
            Timeline coordinate associated with the event. Both start and end times use the same value because text
            segments are instantaneous in the scheduler.

    Returns:
        `Event`: Event carrying a `(num_tokens, 1)` tensor of token ids with matching
        metadata so that downstream processors can compute modality-specific embeddings.
    """
    tokens = tokenizer.encode(text, add_special_tokens=False, return_tensors="pt").squeeze(0)

    # Calculate dimensions for the event
    num_tokens = len(tokens)
    dims_virtual = [num_tokens, 1]  # [sequence_length, 1]
    dims_real = dims_virtual.copy()

    # Ensure tokens has the right shape for tensor_stream_token_view
    # It expects a 2D tensor where sum(dim=-1) gives the token IDs
    if tokens.dim() == 1:
        tokens = tokens.unsqueeze(-1)

    return Event(
        data=tokens,
        type=TextType.text,
        time=(time, time),
        dims_virtual=dims_virtual,
        dims_real=dims_real,
        idx_range=(0, num_tokens),
    )


# ============================================================================
# Processor
# ============================================================================


class IsaacProcessor(ProcessorMixin):
    attributes = ["image_processor", "tokenizer"]
    image_processor_class = ("IsaacImageProcessor", "IsaacImageProcessorFast")
    tokenizer_class = ("Qwen2Tokenizer", "Qwen2TokenizerFast")

    def __init__(
        self,
        image_processor: IsaacImageProcessor | BaseImageProcessorFast | None = None,
        tokenizer: Qwen2Tokenizer | None = None,
        *,
        vision_token: str = "<image>",
        max_sequence_length: int = 16384,
        vision_patch_size: int = 16,
        vision_max_num_patches: int = 256,
        vision_min_num_patches: int | None = None,
        pixel_shuffle_scale: int = 1,
        rescale_factor: float | None = None,
        image_mean: float | Sequence[float] | None = None,
        image_std: float | Sequence[float] | None = None,
        vision_attn_implementation: str | None = None,
        config: IsaacConfig | dict | None = None,
        **kwargs,
    ) -> None:
        if tokenizer is None:
            raise ValueError("`tokenizer` must be provided to initialize IsaacProcessor.")

        if isinstance(config, dict):
            config = IsaacConfig(**config)

        if config is not None:
            vision_patch_size = config.video_patch_size
            vision_max_num_patches = config.vision_max_num_patches
            vision_min_num_patches = config.vision_min_num_patches
            pixel_shuffle_scale = config.pixel_shuffle_scale
            max_sequence_length = config.max_sequence_length
            vision_token = config.vision_token
            vision_attn_implementation = config.vision_attn_implementation
            rescale_factor = config.vision_rescale_factor
            image_mean = tuple(config.vision_mean)
            image_std = tuple(config.vision_std)

        resolved_rescale_factor = float(rescale_factor) if rescale_factor is not None else float(VISION_SCALE)
        resolved_image_mean = _normalize_rgb_values(
            image_mean if image_mean is not None else VISION_MEAN,
            name="image_mean",
        )
        resolved_image_std = _normalize_rgb_values(
            image_std if image_std is not None else VISION_STD,
            name="image_std",
        )

        if image_processor is None:
            image_processor = IsaacImageProcessor(
                patch_size=vision_patch_size,
                max_num_patches=vision_max_num_patches,
                min_num_patches=vision_min_num_patches,
                pixel_shuffle_scale=pixel_shuffle_scale,
                rescale_factor=resolved_rescale_factor,
                image_mean=resolved_image_mean,
                image_std=resolved_image_std,
            )
        else:
            vision_patch_size = getattr(image_processor, "patch_size", vision_patch_size)
            vision_max_num_patches = getattr(image_processor, "max_num_patches", vision_max_num_patches)
            vision_min_num_patches = getattr(image_processor, "min_num_patches", vision_min_num_patches)
            pixel_shuffle_scale = getattr(image_processor, "pixel_shuffle_scale", pixel_shuffle_scale)
            resolved_rescale_factor = getattr(image_processor, "rescale_factor", resolved_rescale_factor)
            resolved_image_mean = _normalize_rgb_values(
                getattr(image_processor, "image_mean", resolved_image_mean),
                name="image_mean",
            )
            resolved_image_std = _normalize_rgb_values(
                getattr(image_processor, "image_std", resolved_image_std),
                name="image_std",
            )

        if config is not None:
            config.vision_rescale_factor = resolved_rescale_factor
            config.vision_mean = resolved_image_mean
            config.vision_std = resolved_image_std

        super().__init__(image_processor, tokenizer)
        self.current_processor = self.image_processor
        self.config = config

        # Mirror tokenizer chat template so ProcessorMixin.apply_chat_template works.
        self.chat_template = getattr(self.tokenizer, "chat_template", None)

        self.vision_token = vision_token
        self.max_sequence_length = max_sequence_length
        self.vision_attn_implementation = vision_attn_implementation

        self.patch_size = getattr(self.image_processor, "patch_size", vision_patch_size)
        self.max_num_patches = getattr(self.image_processor, "max_num_patches", vision_max_num_patches)
        self.min_num_patches = getattr(self.image_processor, "min_num_patches", vision_min_num_patches)
        self.pixel_shuffle_scale = getattr(self.image_processor, "pixel_shuffle_scale", pixel_shuffle_scale)
        self.rescale_factor = getattr(self.image_processor, "rescale_factor", resolved_rescale_factor)
        self.image_mean = tuple(getattr(self.image_processor, "image_mean", resolved_image_mean))
        self.image_std = tuple(getattr(self.image_processor, "image_std", resolved_image_std))

    def build_event_stream_simple(
        self,
        text: str,
        images: list[PIL.Image.Image] | None = None,
    ) -> Stream:
        events = []
        # Process text and images
        # Find all occurrences of vision token

        pattern = re.escape(self.vision_token)
        parts = re.split(f"({pattern})", text)  # Keep the delimiter in the result

        image_idx = 0
        for current_time, part in enumerate(parts):
            if part == self.vision_token:
                # Replace vision token with image event
                if images is None or image_idx >= len(images):
                    raise ValueError("Encountered vision token without a corresponding image.")

                features = self.image_processor(
                    images=images[image_idx],
                    return_tensors=TensorType.PYTORCH,
                )

                patches = features["patches"][0]  # (H_tokens, W_tokens, embed)
                virtual_dims = features["virtual_pixel_size"][0].tolist()
                real_dims = features["real_pixel_size"][0].tolist()

                vision_event = Event(
                    data=patches.reshape(-1, patches.shape[-1]),
                    type=VisionType.image,
                    time=(current_time, current_time),
                    dims_virtual=virtual_dims,
                    dims_real=real_dims,
                    idx_range=(0, math.prod(virtual_dims)),
                )
                events.append(vision_event)
                image_idx += 1
            elif part:  # Non-empty text part
                # tokens = self.text_processor.tokenize(part, add_special_tokens=False)
                text_event = create_text_event(self.tokenizer, part, time=current_time)
                events.append(text_event)

        # Create stream without scheduling (events already in order)
        return create_stream(events, priority=[TextType.text, VisionType.image], schedule=True)

    def __call__(
        self,
        text: str | list[str],
        images: PIL.Image.Image | list[PIL.Image.Image] | None = None,
        return_tensors: str | TensorType | None = TensorType.PYTORCH,
        **kwargs,
    ) -> BatchFeature:
        """
        Process text and images into TensorStream format.
        Args:
            text: Input text or list of texts with vision tokens
            images: PIL image or list of images (optional)
            return_tensors: Format for output tensors

        Returns:
            BatchFeature with input_ids and tensor_stream
        """
        # Normalize inputs to lists
        if isinstance(text, str):
            texts = [text]
        else:
            texts = text

        if images is not None:
            if isinstance(images, PIL.Image.Image):
                images_list = [images]
            else:
                images_list = images
        else:
            images_list = None

        if len(texts) != 1:
            raise ValueError("IsaacProcessor currently supports batch_size=1")
        if images_list is not None:
            # Count vision tokens in text to validate image count
            vision_token_count = texts[0].count(self.vision_token)
            if vision_token_count != len(images_list):
                raise ValueError(
                    f"Number of {self.vision_token} tokens in text ({vision_token_count}) "
                    f"must match number of images ({len(images_list)})"
                )

        # Build event stream
        stream = self.build_event_stream_simple(
            text=texts[0],
            images=images_list,
        )

        # Create TensorStream
        tensor_stream = TensorStream([stream])

        # Slice to max length if needed
        _, T = tensor_stream.shape
        if T > self.max_sequence_length:
            tensor_stream = ts_slice(tensor_stream, start=T - self.max_sequence_length, end=T)

        # Get token view
        tokens = tensor_stream_token_view(tensor_stream)
        if return_tensors in (TensorType.PYTORCH, "pt"):
            input_ids = torch.as_tensor(tokens, dtype=torch.long)
        else:
            input_ids = tokens

        data = {
            "input_ids": input_ids,
            "tensor_stream": tensor_stream,
        }

        return BatchFeature(data=data)


__all__ = ["IsaacProcessor"]
