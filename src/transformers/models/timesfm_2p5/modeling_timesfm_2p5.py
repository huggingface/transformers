#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/timesfm_2p5/modular_timesfm_2p5.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_timesfm_2p5.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# coding=utf-8
# Copyright 2025 the HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import math
from collections.abc import Sequence
from dataclasses import dataclass
from typing import Optional, Union

import torch
import torch.nn as nn
import torch.nn.functional as F

from ...integrations import use_kernel_forward_from_hub
from ...modeling_outputs import BaseModelOutput
from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update
from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
from ...utils import auto_docstring, can_return_tuple
from ...utils.deprecation import deprecate_kwarg
from .configuration_timesfm_2p5 import Timesfm2P5Config


@dataclass
class Timesfm2P5Output(BaseModelOutput):
    r"""
    loc (`torch.Tensor` of shape `(batch_size, )`):
        The mean of the time series inputs.
    scale (`torch.Tensor` of shape `(batch_size,)`):
        The scale of the time series inputs.
    """

    loc: Optional[torch.Tensor] = None
    scale: Optional[torch.Tensor] = None


@dataclass
class Timesfm2P5OutputForPrediction(BaseModelOutput):
    r"""
    mean_predictions (`torch.Tensor` of shape `(batch_size, sequence_length)`):
        The mean predictions of the time series.
    full_predictions (`torch.Tensor` of shape `(batch_size, sequence_length)`):
        The full predictions of the time series including the mean and the quantiles.
    loss (`torch.Tensor` of shape `(1,)`, *optional*, returned when `future_values` is provided):
        The loss of the Timesfm2P5 model.
    """

    mean_predictions: Optional[torch.Tensor] = None
    full_predictions: Optional[torch.Tensor] = None
    loss: Optional[Union[torch.Tensor, float]] = None


class Timesfm2P5MLP(nn.Module):
    """
    TimesFM 2.5 MLP layer with configurable activation.

    This is a feedforward network with two linear layers and configurable activation.
    """

    def __init__(self, config: Timesfm2P5Config):
        super().__init__()
        hidden_size = config.hidden_size
        intermediate_size = config.intermediate_size
        use_bias = config.use_bias

        self.ff0 = nn.Linear(hidden_size, intermediate_size, bias=use_bias)
        self.ff1 = nn.Linear(intermediate_size, hidden_size, bias=use_bias)

        # No activation function - TimesFM 2.5 MLP has no activation between ff0 and ff1

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        hidden = self.ff0(x)
        # No activation applied - TimesFM 2.5 MLP applies ff1 directly to ff0 output
        output = self.ff1(hidden)
        return output


class Timesfm2P5ResidualBlock(nn.Module):
    """
    TimesFM 2.5 residual block with configurable activation and bias.

    This implements the ResidualBlock from TimesFM 2.5 which supports:
    - Configurable activation functions (relu, swish/silu, none)
    - Optional bias in linear layers
    - Residual connection from input to output
    """

    def __init__(
        self, input_dims: int, hidden_dims: int, output_dims: int, use_bias: bool = True, activation: str = "swish"
    ):
        super().__init__()
        self.input_dims = input_dims
        self.hidden_dims = hidden_dims
        self.output_dims = output_dims
        self.use_bias = use_bias
        self.activation_type = activation

        # Linear layers
        self.hidden_layer = nn.Linear(input_dims, hidden_dims, bias=use_bias)
        self.output_layer = nn.Linear(hidden_dims, output_dims, bias=use_bias)
        self.residual_layer = nn.Linear(input_dims, output_dims, bias=use_bias)

        # Activation function
        if activation == "relu":
            self.activation = nn.ReLU()
        elif activation == "swish" or activation == "silu":
            self.activation = nn.SiLU()
        elif activation == "none":
            self.activation = nn.Identity()
        else:
            raise ValueError(f"Activation '{activation}' not supported. Choose from 'relu', 'swish', or 'none'.")

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass of the residual block.

        Args:
            x: Input tensor of shape (batch_size, ..., input_dims)

        Returns:
            Output tensor of shape (batch_size, ..., output_dims)
        """
        hidden = self.hidden_layer(x)
        hidden = self.activation(hidden)
        output = self.output_layer(hidden)
        residual = self.residual_layer(x)
        return output + residual


@use_kernel_forward_from_hub("RMSNorm")
class Timesfm2P5RMSNorm(nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        """
        Timesfm2P5RMSNorm is equivalent to T5LayerNorm
        """
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states):
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states.to(input_dtype)

    def extra_repr(self):
        return f"{tuple(self.weight.shape)}, eps={self.variance_epsilon}"


class Timesfm2P5RotaryEmbedding(nn.Module):
    inv_freq: torch.Tensor  # fix linting for `register_buffer`

    def __init__(self, config: Timesfm2P5Config, device=None):
        super().__init__()
        # BC: "rope_type" was originally "type"
        if hasattr(config, "rope_scaling") and isinstance(config.rope_scaling, dict):
            self.rope_type = config.rope_scaling.get("rope_type", config.rope_scaling.get("type"))
        else:
            self.rope_type = "default"
        self.max_seq_len_cached = config.max_position_embeddings
        self.original_max_seq_len = config.max_position_embeddings

        self.config = config
        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]

        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)
        self.register_buffer("inv_freq", inv_freq, persistent=False)
        self.original_inv_freq = self.inv_freq

    @torch.no_grad()
    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)
    def forward(self, x, position_ids):
        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        position_ids_expanded = position_ids[:, None, :].float()

        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != "mps" else "cpu"
        with torch.autocast(device_type=device_type, enabled=False):  # Force float32
            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
            emb = torch.cat((freqs, freqs), dim=-1)
            cos = emb.cos() * self.attention_scaling
            sin = emb.sin() * self.attention_scaling

        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)


def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    """Applies Rotary Position Embedding to the query and key tensors.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`, *optional*):
            Deprecated and unused.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """
    cos = cos.unsqueeze(unsqueeze_dim)
    sin = sin.unsqueeze(unsqueeze_dim)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)


def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    dropout: float = 0.0,
    scaling: Optional[float] = None,
    softcap: Optional[float] = None,
    **kwargs,
) -> tuple[torch.Tensor, torch.Tensor]:
    if scaling is None:
        scaling = module.head_dim**-0.5

    key_states = repeat_kv(key, module.num_key_value_groups)
    value_states = repeat_kv(value, module.num_key_value_groups)

    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling

    if softcap is not None:
        attn_weights = attn_weights / softcap
        attn_weights = torch.tanh(attn_weights)
        attn_weights = attn_weights * softcap
    if attention_mask is not None:  # no matter the length, we just slice it
        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
        attn_weights = attn_weights + causal_mask

    # upcast attention to fp32
    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
    attn_output = torch.matmul(attn_weights, value_states)
    attn_output = attn_output.transpose(1, 2).contiguous()
    return attn_output, attn_weights


class Timesfm2P5Attention(nn.Module):
    """
    TimesFM 2.5 attention extends Gemma2Attention but overrides the forward to implement
    the exact TimesFM 2.5 operations: QK normalization + per-dimension scaling
    """

    def __init__(self, config: Timesfm2P5Config, layer_idx: int):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        self.head_dim = getattr(config, "head_dim", config.hidden_size // config.num_attention_heads)
        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads
        self.scaling = config.query_pre_attn_scalar**-0.5
        self.attention_dropout = self.config.attention_dropout
        self.is_causal = not getattr(config, "use_bidirectional_attention", False)

        self.q_proj = nn.Linear(
            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias
        )
        self.k_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.v_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.o_proj = nn.Linear(
            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias
        )
        self.attn_logit_softcapping = self.config.attn_logit_softcapping
        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == "sliding_attention" else None

        # Add QK normalization specific to TimesFM 2.5
        self.use_qk_norm = getattr(config, "use_qk_norm", True)
        if self.use_qk_norm:
            self.query_ln = Timesfm2P5RMSNorm(self.head_dim, eps=config.rms_norm_eps)
            self.key_ln = Timesfm2P5RMSNorm(self.head_dim, eps=config.rms_norm_eps)

        # Add per-dimension scaling parameter (same as TimesFmAttention)
        self.use_per_dim_scale = getattr(config, "use_per_dim_scale", True)
        if self.use_per_dim_scale:
            self.scaling = nn.Parameter(torch.empty((self.head_dim,)))

    @deprecate_kwarg("past_key_value", new_name="past_key_values", version="4.58")
    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: tuple[torch.Tensor, torch.Tensor],
        attention_mask: Optional[torch.Tensor],
        past_key_values=None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs,
    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:
        """Forward with TimesFM 2.5 specific QK normalization and per-dimension scaling."""
        input_shape = hidden_states.shape[:-1]
        hidden_shape = (*input_shape, -1, self.head_dim)

        # Linear projections
        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)

        # Apply QK normalization (TimesFM 2.5 specific)
        if self.use_qk_norm:
            query_states = self.query_ln(query_states)
            key_states = self.key_ln(key_states)

        # Apply per-dimension scaling to query (TimesFM 2.5 specific)
        query_states = self._scale_query(query_states)

        # Apply rotary position embeddings
        cos, sin = position_embeddings
        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)

        # Handle past key/value for caching
        if past_key_values is not None:
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}
            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)

        # Use the standard attention computation from Gemma2
        attention_interface = eager_attention_forward
        if self.config._attn_implementation != "eager":
            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

        # Attention computation with custom scaling disabled (we use per_dim_scale instead)
        attn_output, attn_weights = attention_interface(
            self,
            query_states,
            key_states,
            value_states,
            attention_mask,
            dropout=self.attention_dropout if self.training else 0.0,
            scaling=1.0,  # No scaling - we already applied per_dim_scale to queries
            sliding_window=getattr(self, "sliding_window", None),
            softcap=getattr(self, "attn_logit_softcapping", None),
            **kwargs,
        )

        attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        attn_output = self.o_proj(attn_output)
        return attn_output, attn_weights

    def _scale_query(self, query: torch.Tensor) -> torch.Tensor:
        """Per-dimension scaling - exact copy from TimesFmAttention."""
        if not self.use_per_dim_scale:
            return query
        scale = F.softplus(self.scaling).mul(1.442695041 / math.sqrt(self.head_dim))
        return query * scale[None, None, None, :]


class Timesfm2P5DecoderLayer(nn.Module):
    """
    TimesFM 2.5 Transformer decoder layer.

    This layer consists of:
    - Self-attention with rotary embeddings and QK normalization
    - MLP feedforward network with configurable activation
    - RMS normalization
    """

    def __init__(self, config: Timesfm2P5Config, layer_idx: int):
        super().__init__()

        # Attention layers
        self.self_attn = Timesfm2P5Attention(config, layer_idx=layer_idx)

        # Normalization layers
        self.pre_attn_ln = Timesfm2P5RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attn_ln = Timesfm2P5RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.pre_ff_ln = Timesfm2P5RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_ff_ln = Timesfm2P5RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

        # MLP
        self.mlp = Timesfm2P5MLP(config)

    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.Tensor] = None,
        output_attentions: bool = False,
        **kwargs,
    ) -> tuple[Optional[torch.Tensor], torch.Tensor]:
        # Self-Attention with pre and post normalization
        residual = hidden_states
        hidden_states = self.pre_attn_ln(hidden_states)
        hidden_states, scores = self.self_attn(
            hidden_states=hidden_states,
            position_embeddings=position_embeddings,
            attention_mask=attention_mask,
            position_ids=position_ids,
            output_attentions=output_attentions,
            **kwargs,
        )
        hidden_states = self.post_attn_ln(hidden_states) + residual

        # MLP with pre and post normalization
        residual = hidden_states
        hidden_states = self.pre_ff_ln(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = self.post_ff_ln(hidden_states) + residual

        return scores, hidden_states


@auto_docstring
class Timesfm2P5PreTrainedModel(PreTrainedModel):
    config: Timesfm2P5Config
    base_model_prefix = "timesfm_2p5"
    _no_split_modules = ["Timesfm2P5DecoderLayer"]
    main_input_name = "past_values"
    _supports_sdpa = True
    config_class = Timesfm2P5Config

    def _init_weights(self, module):
        super()._init_weights(module)
        if isinstance(module, Timesfm2P5Attention):
            # Initialize scaling parameter
            nn.init.ones_(module.scaling)


class Timesfm2P5Model(Timesfm2P5PreTrainedModel):
    """
    TimesFM 2.5 model - standalone implementation (not inheriting from TimesFmModel).

    Uses TimesFM 2.5 specific architecture:
    - Timesfm2P5ResidualBlock for input projection
    - Timesfm2P5DecoderLayer for transformer layers
    - No frequency embedding (model adapts automatically)
    - No positional embedding (uses rotary embeddings)
    """

    def __init__(self, config: Timesfm2P5Config):
        super().__init__(config)
        self.config = config

        # Input projection with TimesFM 2.5 ResidualBlock
        # Note: tokenizer uses bias=True (different from transformer layers)
        self.input_ff_layer = Timesfm2P5ResidualBlock(
            input_dims=2 * config.patch_length,  # 64 (32*2)
            hidden_dims=config.hidden_size,  # 1280 (not intermediate_size)
            output_dims=config.hidden_size,  # 1280
            use_bias=True,  # tokenizer uses bias=True
            activation=config.activation,  # "swish"
        )

        # TimesFM 2.5 has NO frequency embedding - model adapts automatically
        # (This is a key difference from TimesFM 2.0)

        # Transformer layers with TimesFM 2.5 specific components
        self.layers = nn.ModuleList(
            [Timesfm2P5DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
        )

        # TimesFM 2.5 uses rotary embeddings - add rotary embedding component
        self.rotary_emb = Timesfm2P5RotaryEmbedding(config)

        # Initialize weights and apply final processing
        # self.post_init()  # Temporarily disabled due to initialization issue

    def _revin(self, x: torch.Tensor, loc: torch.Tensor, scale: torch.Tensor, reverse: bool = False) -> torch.Tensor:
        """
        Reversible Instance Normalization (RevIN) - exact copy from original TimesFM.

        Args:
            x: Input tensor
            loc: Location (mean) for normalization
            scale: Scale (std) for normalization
            reverse: If True, denormalize; if False, normalize
        """
        if not reverse:
            # Normalize: (x - mean) / std
            return (x - loc) / scale
        else:
            # Denormalize: x * std + mean
            return x * scale + loc

    def forward(
        self,
        past_values: torch.Tensor,
        past_values_padding: torch.LongTensor,
        output_attentions: bool = False,
        output_hidden_states: bool = False,
    ):
        """
        TimesFM 2.5 forward pass with proper normalization following original implementation.

        Args:
            past_values: Input tensor of shape (batch_size, sequence_length)
            past_values_padding: Padding tensor of shape (batch_size, sequence_length)
            output_attentions: Whether to return attention weights
            output_hidden_states: Whether to return hidden states

        Returns:
            Timesfm2P5Output containing last_hidden_state and normalization statistics
        """
        batch_size = past_values.shape[0]

        # Step 1: Patch the inputs (reshape to [B, N, P] where P=patch_length=32)
        patched_inputs = past_values.view(batch_size, -1, self.config.patch_length)
        # Use only the context portion of padding for patching
        context_padding = past_values_padding[:, : past_values.shape[1]]
        patched_masks = context_padding.view(batch_size, -1, self.config.patch_length)

        # Step 2: Compute normalization statistics per sequence (following original TimesFM 2.5)
        # Calculate mean and std across the non-padded values for each sequence
        # Mask out padded values (where context_padding == 1)
        valid_mask = (context_padding == 0).float()  # 1 for valid, 0 for padded

        # Compute mean across valid values only
        loc = (past_values * valid_mask).sum(dim=1) / (valid_mask.sum(dim=1) + 1e-8)  # [B]

        # Compute std across valid values only (matching original TimesFM exactly)
        scale = torch.zeros_like(loc)
        for i in range(batch_size):
            valid_values = past_values[i, valid_mask[i] == 1]
            if valid_values.numel() > 0:
                scale[i] = valid_values.std(unbiased=False)

        # Ensure minimum scale to avoid division by zero (same as original)
        scale = torch.clamp(scale, min=1e-8)

        # Step 3: Apply normalization to patched inputs
        # Normalize each patch using sequence-level statistics
        loc_expanded = loc.view(batch_size, 1, 1)  # [B, 1, 1]
        scale_expanded = scale.view(batch_size, 1, 1)  # [B, 1, 1]

        normalized_patches = self._revin(patched_inputs, loc_expanded, scale_expanded, reverse=False)

        # Step 4: Set padded patches to zero (original TimesFM behavior)
        patched_pad_mask = (patched_masks.sum(dim=-1, keepdim=True) == self.config.patch_length).float()
        normalized_patches = normalized_patches * (1.0 - patched_pad_mask)

        # Step 5: TimesFM 2.5 preprocessing - concatenate inputs and padding indicators
        tokenizer_inputs = torch.cat(
            [normalized_patches, patched_pad_mask.expand(-1, -1, self.config.patch_length)], dim=-1
        )

        # Step 6: Input embedding through tokenizer (ResidualBlock: 64 -> 1280)
        # Ensure dtype compatibility for mixed precision training
        tokenizer_inputs = tokenizer_inputs.to(dtype=self.dtype)
        input_embeddings = self.input_ff_layer(tokenizer_inputs)

        # Step 7: Create position embeddings for RoPE
        sequence_length = input_embeddings.shape[1]
        position_ids = torch.arange(sequence_length, device=input_embeddings.device).unsqueeze(0)
        position_embeddings = self.rotary_emb(input_embeddings, position_ids)

        # Step 8: Pass through transformer layers
        hidden_states = input_embeddings
        all_hidden_states = () if output_hidden_states else None
        all_attentions = () if output_attentions else None

        for layer in self.layers:
            if output_hidden_states:
                all_hidden_states = all_hidden_states + (hidden_states,)

            layer_outputs = layer(
                hidden_states=hidden_states,
                position_embeddings=position_embeddings,
                attention_mask=None,  # TimesFM 2.5 doesn't use attention mask
                position_ids=position_ids,
                output_attentions=output_attentions,
            )

            if output_attentions:
                attention_weights, hidden_states = layer_outputs
                all_attentions = all_attentions + (attention_weights,)
            else:
                _, hidden_states = layer_outputs

        if output_hidden_states:
            all_hidden_states = all_hidden_states + (hidden_states,)

        # Return with normalization statistics for use by prediction model
        return Timesfm2P5Output(
            last_hidden_state=hidden_states,
            hidden_states=all_hidden_states,
            attentions=all_attentions,
            loc=loc,
            scale=scale,
        )


class Timesfm2P5ModelForPrediction(Timesfm2P5PreTrainedModel):
    """
    TimesFM 2.5 model for quantile and mean prediction.

    Inherits from TimesFmModelForPrediction but uses:
    - Timesfm2P5Model as the decoder
    - Separate output projections for point and quantile predictions (matching original TimesFM 2.5)
    """

    def __init__(self, config: Timesfm2P5Config):
        super().__init__(config)

        # Now override with TimesFM 2.5 specific components
        self.config = config
        self.context_len = config.context_length
        self.horizon_len = config.horizon_length

        # Override decoder with TimesFM 2.5 model
        self.decoder = Timesfm2P5Model(config)

        # quantile and mean output
        self.horizon_ff_layer = Timesfm2P5ResidualBlock(
            input_dims=config.hidden_size,
            output_dims=config.horizon_length * (1 + len(config.quantiles)),
            hidden_dims=config.intermediate_size,
        )

        # Replace the parent's horizon_ff_layer with TimesFM 2.5 separate output projections
        # Point prediction projection: 1280 -> 1280
        self.output_projection_point = Timesfm2P5ResidualBlock(
            input_dims=config.hidden_size,  # 1280
            hidden_dims=config.hidden_size,  # 1280
            output_dims=config.hidden_size,  # 1280
            use_bias=config.use_bias,  # False
            activation=config.activation,  # "swish"
        )

        # Quantile prediction projection: hidden_size -> output_quantile_len * (num_quantiles + 1)
        # Original: 1024 * 10 = 10240 (9 quantiles + 1 extra)
        output_quantile_len = getattr(config, "output_quantile_len", 1024)  # Default from original
        quantile_output_size = output_quantile_len * (len(config.quantiles) + 1)
        self.output_projection_quantiles = Timesfm2P5ResidualBlock(
            input_dims=config.hidden_size,
            hidden_dims=config.hidden_size,
            output_dims=quantile_output_size,  # Dynamic based on horizon_length
            use_bias=config.use_bias,  # False
            activation=config.activation,  # "swish"
        )

        # Initialize weights and apply final processing
        self.post_init()

    def _preprocess(
        self, inputs: Sequence[torch.Tensor], freq: Sequence[int]
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Formats and pads raw inputs to feed into the model.

        This function both pads each time series to match the context length, and
        pads the inputs to meet the SPMD shape requirement.

        Args:
          inputs: A list of 1d Tensors. Each Tensor is the context time series of
            a single forecast task.
          freq: list of frequencies

        Returns:
        A tuple of:
        - the padded input time series to meet the model required context.
        - the padding indicator.
        - the number of padded examples for SPMD so that each core has the same
            number (a multiple of `batch_size`) of examples.
        """
        input_ts, input_padding, inp_freq = [], [], []

        for i, ts in enumerate(inputs):
            input_len = ts.shape[0]
            padding = torch.zeros(input_len + self.horizon_len, dtype=ts.dtype, device=ts.device)
            if input_len < self.context_len:
                num_front_pad = self.context_len - input_len
                ts = torch.cat([torch.zeros(num_front_pad, dtype=ts.dtype, device=ts.device), ts], dim=0)
                padding = torch.cat([torch.ones(num_front_pad, dtype=ts.dtype, device=padding.device), padding], dim=0)
            elif input_len > self.context_len:
                ts = ts[-self.context_len :]
                padding = padding[-(self.context_len + self.horizon_len) :]

            input_ts.append(ts)
            input_padding.append(padding)
            inp_freq.append(freq[i])

        return (
            torch.stack(input_ts, dim=0),
            torch.stack(input_padding, dim=0),
            torch.tensor(inp_freq, dtype=torch.int32).reshape(-1, 1),
        )

    def _postprocess_output(
        self, model_output: torch.Tensor, stats: tuple[torch.Tensor, torch.Tensor]
    ) -> torch.Tensor:
        """
        Postprocess output of stacked transformer - TimesFM 2.5 version with separate projections.

        Args:
            model_output: Output from decoder [B, N, D] where D=hidden_size=1280
            stats: Tuple of (mu, sigma) for normalization

        Returns:
            output_ts: [B, N, horizon_length, quantiles+1] tensor
        """
        # Apply separate output projections (matching original TimesFM 2.5)
        point_output = self.output_projection_point(model_output)  # [B, N, 1280]
        quantile_output = self.output_projection_quantiles(model_output)  # [B, N, 9216]

        # Reshape outputs to match expected format
        b, n, _ = model_output.shape

        # Point predictions: [B, N, 1280] -> [B, N, horizon_length, 1]
        # Since point output is 1280 and we need horizon_length patches
        num_patches = point_output.shape[-1] // self.config.horizon_length  # 1280 / 128 = 10
        point_reshaped = point_output.view(b, n, num_patches, self.config.horizon_length)
        # Take the mean prediction (index 5 in original TimesFM 2.5)
        point_final = point_reshaped[
            :, :, self.config.decode_index : self.config.decode_index + 1, :
        ]  # [B, N, 1, 128]
        point_final = point_final.permute(0, 1, 3, 2)  # [B, N, 128, 1]

        # Quantile predictions: [B, N, quantile_output_size] -> [B, N, output_quantile_len, num_quantiles+1]
        output_quantile_len = self.config.output_quantile_len
        num_quantiles_plus_one = len(self.config.quantiles) + 1  # 9 quantiles + 1 = 10 total
        quantile_reshaped = quantile_output.view(
            b,
            n,
            output_quantile_len,
            num_quantiles_plus_one,
        )  # [B, N, output_quantile_len, num_quantiles+1]
        # Take the first horizon_length entries and only the quantiles we want
        quantile_final = quantile_reshaped[
            :, :, : self.config.horizon_length, : len(self.config.quantiles)
        ]  # [B, N, 128, 9]

        # Combine point and quantile predictions: [B, N, 128, 1] + [B, N, 128, 9] -> [B, N, 128, 10]
        output_ts = torch.cat([point_final, quantile_final], dim=-1)

        # Apply normalization (same as parent)
        mu, sigma = stats
        return output_ts * sigma[:, None, None, None] + mu[:, None, None, None]

    def _quantile_loss(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        losses = []
        for i, q in enumerate(self.config.quantiles):
            errors = targets - predictions[..., i]
            loss = torch.max((q - 1) * errors, q * errors)
            losses.append(loss.mean())
        return torch.stack(losses).mean()

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        past_values: Sequence[torch.Tensor],
        window_size: Optional[int] = None,
        future_values: Optional[torch.Tensor] = None,
        forecast_context_len: Optional[int] = None,
        return_forecast_on_context: bool = False,
        truncate_negative: bool = False,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
    ) -> Timesfm2P5OutputForPrediction:
        """
        TimesFM 2.5 forward method matching original forecast operations.

        TimesFM 2.5 simplified API - no frequency parameter needed as the model
        automatically adapts to different time series frequencies.

        Args:
            past_values (`Sequence[torch.Tensor]`):
                Past values of the time series that serves as input to the model.
                Each tensor is a 1D time series of variable length.
            window_size (`int`, *optional*):
                Window size of trend + residual decomposition. If None then we do not do decomposition.
            future_values (`torch.Tensor`, *optional*):
                Optional future time series values to be used for loss computation.
            forecast_context_len (`int`, *optional*):
                Optional max context length.
            return_forecast_on_context (`bool`, *optional*):
                True to return the forecast on the context when available.
            truncate_negative (`bool`, *optional*):
                Truncate to only non-negative values if any contexts have non-negative values.
            return_dict (`bool`, *optional*):
                Whether or not to return a ModelOutput instead of a plain tuple.
            output_attentions (`bool`, *optional*):
                Whether to output the attentions.
            output_hidden_states (`bool`, *optional*):
                Whether to output the hidden states.

        Returns:
            Timesfm2P5OutputForPrediction: Output with mean_predictions, full_predictions, and optional loss.
        """
        if forecast_context_len is None:
            fcontext_len = self.context_len
        else:
            fcontext_len = forecast_context_len

        # Get device from first input tensor
        device = past_values[0].device

        # Truncate inputs to forecast_context_len
        inputs = [ts[-fcontext_len:] for ts in past_values]
        inp_min = torch.min(torch.stack([torch.min(ts) for ts in inputs]))

        if window_size is not None:
            new_inputs = []
            for ts in inputs:
                new_inputs.extend(self._timesfm_moving_average(ts, window_size))
            inputs = new_inputs

        # TimesFM 2.5 doesn't use frequency - set dummy freq for internal compatibility
        freq = [0] * len(inputs)  # TimesFM 2.5 simplified API

        if output_attentions is None:
            output_attentions = self.config.output_attentions
        if output_hidden_states is None:
            output_hidden_states = self.config.output_hidden_states

        input_ts, input_padding, inp_freq = self._preprocess(inputs, freq)
        # Move tensors to the same device as input
        input_ts = input_ts.to(device)
        input_padding = input_padding.to(device)
        inp_freq = inp_freq.to(device)

        # Handle default values for output flags
        if output_attentions is None:
            output_attentions = self.config.output_attentions
        if output_hidden_states is None:
            output_hidden_states = self.config.output_hidden_states

        # Call the base model which handles normalization internally
        model_outputs = self.decoder(
            past_values=input_ts,
            past_values_padding=input_padding,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
        )

        # Extract normalization statistics computed by the base model
        loc = model_outputs.loc  # [B]
        scale = model_outputs.scale  # [B]
        hidden_states = model_outputs.last_hidden_state  # [B, N, 1280]

        # Apply output projections
        point_output = self.output_projection_point(hidden_states)  # [B, N, 1280]
        quantile_output = self.output_projection_quantiles(hidden_states)  # [B, N, 10240]

        # Apply denormalization to get final predictions
        b, n = hidden_states.shape[:2]

        # Reshape point predictions: [B, N, hidden_size] -> [B, N, horizon_length, quantiles_per_step]
        horizon_length = self.config.horizon_length
        quantiles_per_step = point_output.shape[-1] // horizon_length
        point_reshaped = point_output.view(b, n, horizon_length, quantiles_per_step)
        decode_index = min(5, quantiles_per_step - 1)  # Median index, clamp to available quantiles
        mean_preds_norm = point_reshaped[:, :, :, decode_index]  # [B, N, horizon_length]

        # Flatten and take first horizon_length values
        mean_preds_flat = mean_preds_norm.reshape(b, -1)[:, :horizon_length]  # [B, horizon_length]

        # Denormalize using stats from base model
        loc_pred = loc.view(b, 1)  # [B, 1]
        scale_pred = scale.view(b, 1)  # [B, 1]
        mean_predictions = mean_preds_flat * scale_pred + loc_pred

        # Process quantile predictions similarly
        quantile_output_size = quantile_output.shape[-1]
        quantiles_per_horizon_step = quantile_output_size // horizon_length
        quantile_reshaped = quantile_output.view(
            b, n, horizon_length, quantiles_per_horizon_step
        )  # [B, N, horizon_length, quantiles_per_horizon_step]

        # Take quantiles: we need num_quantiles quantiles, plus the median is at the end
        # The original TimesFM expects [B, horizon, quantiles] shape
        num_quantiles = len(self.config.quantiles)  # From config
        available_quantile_indices = min(quantiles_per_horizon_step, num_quantiles)
        quantile_indices = list(range(available_quantile_indices))
        quantile_preds_norm = quantile_reshaped[
            :, :, :, quantile_indices
        ]  # [B, N, horizon_length, available_quantiles]

        # Flatten and take first horizon values, then reshape properly
        quantile_preds_flat = quantile_preds_norm.reshape(b, -1, available_quantile_indices)[
            :, :horizon_length, :
        ]  # [B, horizon_length, available_quantiles]

        # Add median (point prediction) as final quantile
        mean_preds_expanded = mean_preds_flat.unsqueeze(-1)  # [B, horizon_length, 1]
        quantile_preds_with_median = torch.cat(
            [quantile_preds_flat, mean_preds_expanded], dim=-1
        )  # [B, horizon_length, available_quantiles+1]

        # Denormalize all quantiles
        total_quantiles = available_quantile_indices + 1
        scale_expanded_for_quantiles = scale_pred.unsqueeze(-1).expand(
            -1, horizon_length, total_quantiles
        )  # [B, horizon_length, total_quantiles]
        loc_expanded_for_quantiles = loc_pred.unsqueeze(-1).expand(
            -1, horizon_length, total_quantiles
        )  # [B, horizon_length, total_quantiles]
        quantile_predictions = quantile_preds_with_median * scale_expanded_for_quantiles + loc_expanded_for_quantiles

        # Apply truncate_negative if requested (same logic as parent class)
        if inp_min >= 0 and truncate_negative:
            zero_tensor = torch.tensor(0.0, device=mean_predictions.device, dtype=mean_predictions.dtype)
            mean_predictions = torch.maximum(mean_predictions, zero_tensor)
            quantile_predictions = torch.maximum(quantile_predictions, zero_tensor)

        # Note: return_forecast_on_context is not fully supported in TimesFM 2.5 single-pass approach
        # The parent class uses iterative decoding which allows for context forecasting
        # For now, we ignore this parameter but keep it for API compatibility
        if return_forecast_on_context:
            # TODO: Implement context forecasting for TimesFM 2.5 if needed
            # Currently, TimesFM 2.5 does direct horizon prediction without iterative decoding
            pass

        # Compute loss if future_values is provided (inherited from TimesFmModelForPrediction)
        loss = None
        if future_values is not None:
            mse_loss = F.mse_loss(mean_predictions, future_values)
            quantile_loss = self._quantile_loss(
                quantile_predictions[:, :, :-1], future_values
            )  # Exclude mean from quantiles
            loss = mse_loss + quantile_loss

        return Timesfm2P5OutputForPrediction(
            mean_predictions=mean_predictions,
            full_predictions=quantile_predictions,
            loss=loss,
            hidden_states=model_outputs.hidden_states if output_hidden_states else None,
            attentions=model_outputs.attentions if output_attentions else None,
        )

    @staticmethod
    def _timesfm_2p5_moving_average(arr: torch.Tensor, window_size: int) -> list[torch.Tensor]:
        """Calculates the moving average using PyTorch's convolution function."""
        # Pad with zeros to handle initial window positions
        arr_padded = F.pad(arr, (window_size - 1, 0), "constant", 0)
        # Create a convolution kernel
        kernel = torch.ones(window_size, dtype=arr.dtype, device=arr.device) / window_size
        # Apply convolution to calculate the moving average
        smoothed_arr = F.conv1d(arr_padded.view(1, 1, -1), kernel.view(1, 1, -1)).squeeze()
        return [smoothed_arr, arr - smoothed_arr]

    @staticmethod
    def _revin(
        x: torch.Tensor,
        mu: torch.Tensor,
        sigma: torch.Tensor,
        reverse: bool = False,
    ) -> torch.Tensor:
        """Reversible instance normalization - exact copy from original TimesFM."""
        _TOLERANCE = 1e-6

        if len(mu.shape) == len(x.shape) - 1:
            mu = mu[..., None]
            sigma = sigma[..., None]
        elif len(mu.shape) == len(x.shape) - 2:
            mu = mu[..., None, None]
            sigma = sigma[..., None, None]

        if reverse:
            return x * sigma + mu
        else:
            return (x - mu) / torch.where(sigma < _TOLERANCE, 1.0, sigma)

    @staticmethod
    def _timesfm_moving_average(arr: torch.Tensor, window_size: int) -> list[torch.Tensor]:
        """Calculates the moving average using PyTorch's convolution function."""
        # Pad with zeros to handle initial window positions
        arr_padded = F.pad(arr, (window_size - 1, 0), "constant", 0)
        # Create a convolution kernel
        kernel = torch.ones(window_size, dtype=arr.dtype, device=arr.device) / window_size
        # Apply convolution to calculate the moving average
        smoothed_arr = F.conv1d(arr_padded.view(1, 1, -1), kernel.view(1, 1, -1)).squeeze()
        return [smoothed_arr, arr - smoothed_arr]


__all__ = ["Timesfm2P5ModelForPrediction", "Timesfm2P5PreTrainedModel", "Timesfm2P5Model"]
