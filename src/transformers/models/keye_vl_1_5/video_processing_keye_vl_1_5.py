#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/keye_vl_1_5/modular_keye_vl_1_5.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_keye_vl_1_5.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# coding=utf-8
# Copyright 2025 The Kwai Keye Team and The HuggingFace Inc. team. All rights reserved.
#
# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
# and OPT implementations in this library. It has been modified from its
# original forms to accommodate minor architectural differences compared
# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


import os
from collections import namedtuple
from typing import Any, Optional, Union

import numpy as np
import torch
import torch.nn as nn

from ...image_processing_utils import BatchFeature
from ...image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, ChannelDimension, PILImageResampling, SizeDict
from ...models.qwen2_vl.image_processing_qwen2_vl import smart_resize
from ...processing_utils import Unpack, VideosKwargs
from ...utils import TensorType, add_start_docstrings, logging
from ...utils.import_utils import requires
from ...video_processing_utils import BASE_VIDEO_PROCESSOR_DOCSTRING, BaseVideoProcessor


logger = logging.get_logger(__name__)

try:
    from keye_vl_utils import BicubicVideoFusionOpProcessor
except Exception as err:
    logger.warning_once(
        f"Failed to import BicubicVideoFusionOpProcessor from keye_vl_utils."
        f"Error info {str(err)}."
    )
    BicubicVideoFusionOpProcessor = None


class KeyeVL1_5VideoProcessorInitKwargs(VideosKwargs):
    min_pixels: Optional[int]
    max_pixels: Optional[int]
    patch_size: Optional[int]
    temporal_patch_size: Optional[int]
    merge_size: Optional[int]


def select_slow_fast_frames(
    frames: torch.FloatTensor, frame_types: torch.LongTensor
) -> tuple[torch.FloatTensor, torch.FloatTensor]:
    """
    Selects frames from a tensor based on a mask list.

    Args:
        frames (torch.FloatTensor): A tensor of shape (nframes, channel, height, width).
        frame_types (torch.LongTensor): A int tensor of shape (nframes, ), 1 for fast frames and 0 for slow frames.

    Returns:
        Tuple[torch.FloatTensor, torch.FloatTensor]: A tuple containing two tensors:
            - slow_frames: Frames which the type is 0.
            - fast_frames: Frames where the type is 1.
    """
    nframes, _, _, _ = frames.shape
    if frame_types.shape[-1] != nframes:
        raise ValueError("Length of mask must be equal to the number of frames.")

    mask = frame_types == 0

    slow_frames = frames[mask]
    fast_frames = frames[~mask]

    return slow_frames, fast_frames


@add_start_docstrings(
    "Constructs a fast Keye-VL-1.5 video processor that dynamically resizes videos based on the original videos.",
    BASE_VIDEO_PROCESSOR_DOCSTRING,
    """
        min_pixels (`int`, *optional*, defaults to `28 * 28 * 4`):
            The min pixels of the image to resize the image.
        max_pixels (`int`, *optional*, defaults to `28 * 28 * 1280`):
            The max pixels of the image to resize the image.
        patch_size (`int`, *optional*, defaults to 14):
            The spacial patch size of the vision encoder.
        temporal_patch_size (`int`, *optional*, defaults to 1):
            The temporal patch size of the vision encoder.
        merge_size (`int`, *optional*, defaults to 2):
            The merge size of the vision encoder to llm encoder.
    """,
)
@requires(backends=("torchvision",))
class KeyeVL1_5VideoProcessor(BaseVideoProcessor):
    resample = PILImageResampling.BILINEAR
    size = {"shortest_edge": 128 * 28 * 28, "longest_edge": 28 * 28 * 768}
    image_mean = OPENAI_CLIP_MEAN
    image_std = OPENAI_CLIP_STD
    do_resize = True
    do_rescale = True
    do_normalize = True
    do_convert_rgb = True
    min_pixels = 128 * 28 * 28
    max_pixels = 28 * 28 * 768
    patch_size = 14
    temporal_patch_size = 2
    merge_size = 2
    model_input_names = [
        "pixel_values_videos",
        "video_grid_thw",
        "fast_pixel_values_videos",
        "fast_video_grid_thw",
        "num_frames",
    ]
    ResizeArugs = namedtuple("ResizeArugs", "height,width,resized_height,resized_width")

    def __init__(self, **kwargs: Unpack[KeyeVL1_5VideoProcessorInitKwargs]):
        size = kwargs.pop("size", None)
        min_pixels = kwargs.pop("min_pixels", None)
        max_pixels = kwargs.pop("max_pixels", None)
        size.pop("height", None)
        size.pop("width", None)
        # backward compatibility: override size with min_pixels and max_pixels if they are provided
        size = size or self.size
        if min_pixels is not None:
            size["shortest_edge"] = min_pixels
            size.pop("min_pixels", None)
        if max_pixels is not None:
            size["longest_edge"] = max_pixels
            size.pop("max_pixels", None)
        if "shortest_edge" not in size or "longest_edge" not in size:
            raise ValueError("size must contain 'shortest_edge' and 'longest_edge' keys.")

        try:
            bicubic = BicubicVideoFusionOpProcessor()
        except Exception as err:
            logger.warning_once(
                f"Failed to build object from BicubicVideoFusionOpProcessor class.Error info {str(err)}."
            )
            bicubic = None

        self.bicubic = bicubic
        self.enable_fusion_op = bool(int(os.environ.get("ENABLE_FUSION_PROCESSOR_OP", 1))) and (bicubic is not None)

        if self.enable_fusion_op:
            logger.warning_once("Fusion op is enabled to processing videos.")
        else:
            logger.warning_once("Fusion op is disabled to processing videos.")

        super().__init__(size=size, min_pixels=min_pixels, max_pixels=max_pixels, **kwargs)
        if self.temporal_patch_size != 1:
            raise ValueError("temporal_patch_size != 1 is not supported yet.")

    @staticmethod
    def group_videos_indices(
        arugs_list: list[tuple[int]],
    ) -> list[list[int]]:
        indices = list(range(len(arugs_list)))
        indices.sort(key=lambda idx: arugs_list[idx])
        last = (-10000,) * len(arugs_list[0])
        groups = list()
        for idx in indices:
            if last != arugs_list[idx]:
                groups.append([idx])
            else:
                groups[-1].append(idx)
            last = arugs_list[idx]

        return groups

    def _preprocess_native_op(
        self,
        videos: list[torch.Tensor],
        videos_kwargs: dict[str, Any],
        do_resize: bool = True,
        do_rescale: bool = True,
        do_normalize: bool = True,
        rescale_factor: float = 1 / 255,
        image_mean: Union[list[float], float] = OPENAI_CLIP_MEAN,
        image_std: Union[list[float], float] = OPENAI_CLIP_STD,
        size: Optional[list[SizeDict]] = None,
        factor: int = 28,
        patch_size: int = 14,
        merge_size: int = 2,
        min_pixels: int = 28 * 28 * 4,
        max_pixels: int = 28 * 28 * 1280,
        resample: PILImageResampling = PILImageResampling.BILINEAR,
        input_data_format: ChannelDimension = ChannelDimension.FIRST,
        temporal_patch_size: int = 1,
        **kwargs,
    ) -> dict[str, torch.Tensor]:
        resample = resample.name.lower()
        num_frames = list()
        batch_slow_frames = list()
        batch_fast_frames = list()
        batch_slow_argus = list()
        batch_fast_argus = list()

        num_videos = len(videos)
        batch_frame_types = videos_kwargs.get("frame_types", [None] * num_videos)
        batch_width = videos_kwargs.get("width", [None] * num_videos)
        batch_height = videos_kwargs.get("height", [None] * num_videos)
        batch_fast_width = videos_kwargs.get("fast_width", [None] * num_videos)
        batch_fast_height = videos_kwargs.get("fast_height", [None] * num_videos)

        for index, frames in enumerate(videos):
            if isinstance(frames, np.ndarray):
                frames = torch.from_numpy(frames)
            nframes, channel, ori_height, ori_width = frames.shape
            num_frames.append(nframes)
            if nframes <= 0:
                raise ValueError("No frames in video.")

            if batch_frame_types[index] is None:
                # default to all slow frames
                batch_frame_types[index] = torch.zeros((nframes,), dtype=torch.long)
            frame_types = batch_frame_types[index]
            slow_frames, fast_frames = select_slow_fast_frames(frames, frame_types)
            has_fast_frames = fast_frames.shape[0] > 0
            batch_slow_frames.append(slow_frames)

            resized_width = batch_width[index]
            resized_height = batch_height[index]

            if resized_height is None or resized_width is None:
                resized_height, resized_width = smart_resize(
                    ori_height,
                    ori_width,
                    factor=factor,
                    min_pixels=min_pixels,
                    max_pixels=max_pixels,
                )
            batch_slow_argus.append(self.ResizeArugs(ori_height, ori_width, resized_height, resized_width))

            if has_fast_frames:
                batch_fast_frames.append(fast_frames)

                fast_resized_width = batch_fast_width[index]
                fast_resized_height = batch_fast_height[index]

                if fast_resized_height is None or fast_resized_width is None:
                    fast_resized_height, fast_resized_width = smart_resize(
                        ori_height,
                        ori_width,
                        factor=factor,
                        min_pixels=min_pixels,
                        max_pixels=max_pixels,
                    )
                batch_fast_argus.append(
                    self.ResizeArugs(ori_height, ori_width, fast_resized_height, fast_resized_width)
                )

        num_slow_videos = len(batch_slow_frames)
        num_fast_videos = len(batch_fast_frames)

        batch_argus = batch_slow_argus + batch_fast_argus
        batch_frames = batch_slow_frames + batch_fast_frames
        batch_nframes = [tensor.shape[0] for tensor in batch_frames]

        if do_resize:
            groups = self.group_videos_indices(batch_argus)

            resized_frames = [None for _ in batch_frames]
            for group in groups:
                group_frames = torch.concat([batch_frames[i] for i in group], dim=0)
                resized_height, resized_width = (
                    batch_argus[group[0]].resized_height,
                    batch_argus[group[0]].resized_width,
                )

                resized_group_frames = nn.functional.interpolate(
                    group_frames,
                    [resized_height, resized_width],
                    mode=resample,
                    antialias=True,
                ).float()

                resized_group_frames = torch.split(resized_group_frames, [batch_nframes[i] for i in group], dim=0)

                for i in range(len(group)):
                    resized_frames[group[i]] = resized_group_frames[i]
            batch_frames = resized_frames

        pixel_values_videos = list()
        video_grid_thw = list()
        for i, frames in enumerate(batch_frames):
            frames = self.rescale_and_normalize(
                frames, do_rescale, rescale_factor, do_normalize, image_mean, image_std
            )
            patches = frames
            grid_t, channel, height, width = patches.shape
            if grid_t % temporal_patch_size != 0:
                repeat = temporal_patch_size - (grid_t - 1) % temporal_patch_size - 1
                repeats = patches[-1:].expand(repeat, -1, -1, -1)
                patches = torch.concat([patches, repeats], dim=1)

            grid_t = patches.shape[0] // temporal_patch_size
            grid_h, grid_w = height // patch_size, width // patch_size

            patches = patches.reshape(
                grid_t,
                temporal_patch_size,
                channel,
                grid_h,
                patch_size,
                grid_w,
                patch_size,
            )
            patches = patches.permute(0, 3, 5, 2, 1, 4, 6)
            patches = patches.reshape(grid_t * grid_h * grid_w, channel, patch_size, patch_size)
            pixel_values_videos.append(patches)
            video_grid_thw.append([grid_t, grid_h, grid_w])
        slow_pixel_values_videos, fast_pixel_values_videos = (
            pixel_values_videos[:num_slow_videos],
            pixel_values_videos[num_slow_videos:],
        )
        slow_video_grid_thw, fast_video_grid_thw = video_grid_thw[:num_slow_videos], video_grid_thw[num_slow_videos:]

        slow_video_grid_thw = torch.LongTensor(slow_video_grid_thw)
        slow_pixel_values_videos = torch.concat(slow_pixel_values_videos, dim=0)

        fast_video_grid_thw = (
            torch.LongTensor(fast_video_grid_thw)
            if num_fast_videos > 0
            else torch.empty(size=(0, 1), dtype=torch.float32)
        )
        fast_pixel_values_videos = (
            torch.concat(fast_pixel_values_videos, dim=0)
            if num_fast_videos > 0
            else torch.empty(size=(0, 3), dtype=torch.long)
        )

        return {
            "pixel_values_videos": slow_pixel_values_videos,
            "fast_pixel_values_videos": fast_pixel_values_videos,
            "video_grid_thw": slow_video_grid_thw,
            "fast_video_grid_thw": fast_video_grid_thw,
            "num_frames": torch.LongTensor(num_frames),
        }

    def _preprocess_fusion_op(
        self,
        videos: list[torch.Tensor],
        videos_kwargs: dict[str, Any],
        do_resize: bool = True,
        do_rescale: bool = True,
        do_normalize: bool = True,
        rescale_factor: float = 1 / 255,
        image_mean: Union[list[float], float] = OPENAI_CLIP_MEAN,
        image_std: Union[list[float], float] = OPENAI_CLIP_STD,
        size: Optional[list[SizeDict]] = None,
        factor: int = 28,
        patch_size: int = 14,
        merge_size: int = 2,
        min_pixels: int = 28 * 28 * 4,
        max_pixels: int = 28 * 28 * 1280,
        resample: PILImageResampling = PILImageResampling.BILINEAR,
        temporal_patch_size: int = 1,
        **kwargs,
    ) -> dict[str, torch.Tensor]:
        scale = round(1 / rescale_factor)

        if not (do_resize and do_normalize and do_rescale):
            raise ValueError("When the fusion op is enabled, `do_resize`, `do_normalize`, `do_rescale` must be True.")

        if scale != 255:
            raise ValueError("When the fusion op is enabled, `rescale_factor` must be 1 / 255.")

        logger.warning_once(
            "When the fusion op is enabled, `resample` becomes ineffective, and we will use bicubic interpolation."
        )

        num_frames = list()
        batch_slow_frames = list()
        batch_fast_frames = list()

        num_videos = len(videos)
        batch_frame_types = videos_kwargs.get("frame_types", [None] * num_videos)
        batch_width = videos_kwargs.get("width", [None] * num_videos)
        batch_height = videos_kwargs.get("height", [None] * num_videos)
        batch_fast_width = videos_kwargs.get("fast_width", [None] * num_videos)
        batch_fast_height = videos_kwargs.get("fast_height", [None] * num_videos)

        for index, frames in enumerate(videos):
            if isinstance(frames, np.ndarray):
                frames = torch.from_numpy(frames)
            nframes, channel, ori_height, ori_width = frames.shape
            num_frames.append(nframes)
            if nframes <= 0:
                raise ValueError("No frames in video.")

            if batch_frame_types[index] is None:
                # default to all slow frames
                batch_frame_types[index] = torch.zeros((nframes,), dtype=torch.long)
            frame_types = batch_frame_types[index]
            slow_indices = (frame_types == 0).nonzero().flatten().tolist()
            fast_indices = (frame_types == 1).nonzero().flatten().tolist()
            has_fast_frames = len(fast_indices) > 0
            resized_width = batch_width[index] or 0
            resized_height = batch_height[index] or 0
            fast_width = batch_fast_width[index] or 0
            fast_height = batch_fast_height[index] or 0

            slow_inputs = self.bicubic.interp(
                frames,
                nframes,
                slow_indices,
                ori_height,
                ori_width,
                resized_height,
                resized_width,
                patch=patch_size,
                factor=factor,
                min_pixels=min_pixels,
                max_pixels=max_pixels,
                scale=scale,
                image_mean=image_mean,
                image_std=image_std,
            )
            batch_slow_frames.append(slow_inputs)

            if has_fast_frames:
                fast_inputs = self.bicubic.interp(
                    frames,
                    nframes,
                    fast_indices,
                    ori_height,
                    ori_width,
                    fast_height,
                    fast_width,
                    patch=patch_size,
                    factor=factor,
                    min_pixels=min_pixels,
                    max_pixels=max_pixels,
                    scale=scale,
                    image_mean=image_mean,
                    image_std=image_std,
                )
                batch_fast_frames.append(fast_inputs)

        if len(batch_slow_frames) == 0:
            raise ValueError("Slow frames should not be empty.")

        slow_pixel_values_videos_list = [
            video["pixel_values_videos"] for video in batch_slow_frames if video is not None
        ]
        slow_video_grid_thw_list = [video["video_grid_thw"] for video in batch_slow_frames if video is not None]

        slow_pixel_values_videos = torch.concat(slow_pixel_values_videos_list, dim=0)
        slow_video_grid_thw = torch.concat(slow_video_grid_thw_list, dim=0)

        fast_pixel_values_videos_list = [
            video["pixel_values_videos"] for video in batch_fast_frames if video is not None
        ]
        fast_video_grid_thw_list = [video["video_grid_thw"] for video in batch_fast_frames if video is not None]

        fast_pixel_values_videos = (
            torch.concat(fast_pixel_values_videos_list, dim=0)
            if fast_pixel_values_videos_list
            else torch.empty(size=(0, 1), dtype=torch.float32)
        )
        fast_video_grid_thw = (
            torch.concat(fast_video_grid_thw_list, dim=0)
            if fast_video_grid_thw_list
            else torch.empty(size=(0, 3), dtype=torch.long)
        )

        return {
            "pixel_values_videos": slow_pixel_values_videos,
            "fast_pixel_values_videos": fast_pixel_values_videos,
            "video_grid_thw": slow_video_grid_thw,
            "fast_video_grid_thw": fast_video_grid_thw,
            "num_frames": torch.LongTensor(num_frames),
        }

    def _preprocess(
        self,
        videos: list[torch.Tensor],
        videos_kwargs: dict[str, Any],
        do_resize: Optional[bool] = None,
        do_normalize: Optional[bool] = None,
        do_rescale: Optional[bool] = None,
        rescale_factor: Optional[float] = None,
        size: Optional[Union[SizeDict, list[SizeDict]]] = None,
        image_mean: Optional[Union[float, list[float]]] = None,
        image_std: Optional[Union[float, list[float]]] = None,
        min_pixels: Optional[int] = None,
        max_pixels: Optional[int] = None,
        patch_size: Optional[int] = None,
        temporal_patch_size: Optional[int] = None,
        merge_size: Optional[int] = None,
        return_tensors: Optional[Union[str, TensorType]] = None,
        device: Optional[Union[str, torch.device]] = None,
        resample: PILImageResampling = PILImageResampling.BILINEAR,
        **kwargs,
    ):
        do_resize = do_resize if do_resize is not None else self.do_resize
        do_normalize = do_normalize if do_normalize is not None else self.do_normalize
        do_rescale = do_rescale if do_rescale is not None else self.do_rescale
        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor
        image_mean = image_mean if image_mean is not None else self.image_mean
        image_std = image_std if image_std is not None else self.image_std
        min_pixels = min_pixels if min_pixels is not None else self.min_pixels
        max_pixels = max_pixels if max_pixels is not None else self.max_pixels
        patch_size = patch_size if patch_size is not None else self.patch_size
        temporal_patch_size = temporal_patch_size if temporal_patch_size is not None else self.temporal_patch_size
        merge_size = merge_size if merge_size is not None else self.merge_size

        size = size or self.size
        if not isinstance(size, list):
            size = [size for _ in videos]

        if len(videos) != len(size):
            raise ValueError("Lengths of videos and size must be equal.")

        if self.enable_fusion_op:
            preprocess_op = self._preprocess_fusion_op
        else:
            preprocess_op = self._preprocess_native_op

        video_inputs = preprocess_op(
            videos=videos,
            videos_kwargs=videos_kwargs,
            do_resize=do_resize,
            do_rescale=do_rescale,
            do_normalize=do_normalize,
            rescale_factor=rescale_factor,
            image_mean=image_mean,
            image_std=image_std,
            factor=merge_size * patch_size,
            patch_size=patch_size,
            merge_size=merge_size,
            min_pixels=min_pixels,
            max_pixels=max_pixels,
            resample=resample,
            size=size,
            temporal_patch_size=temporal_patch_size,
            device=device,
            **kwargs,
        )

        return BatchFeature(
            data=video_inputs,
            tensor_type=return_tensors,
        )

    def get_num_of_video_patches(self, num_frames: int, height: int, width: int, videos_kwargs=None):
        """
        A utility that returns number of video patches a given video size.

        Args:
            num_frames (`int`):
                Number of frames in the input video.
            height (`int`):
                Height of the input video.
            width (`int`):
                Width of the input video.
            videos_kwargs (`dict`, *optional*)
                Any kwargs to override defaults of the video processor.
        Returns:
            `Tuple(int, int)`: Number of placeholder tokens required and number of patches per image.
        """
        min_pixels = videos_kwargs.get("min_pixels", None) or self.size["shortest_edge"]
        max_pixels = videos_kwargs.get("max_pixels", None) or self.size["longest_edge"]
        patch_size = videos_kwargs.get("patch_size", None) or self.patch_size
        merge_size = videos_kwargs.get("merge_size", None) or self.merge_size
        temporal_patch_size = videos_kwargs.get("temporal_patch_size", None) or self.temporal_patch_size

        if temporal_patch_size != 1:
            raise ValueError("temporal_patch_size != 1 is not supported yet.")

        factor = patch_size * merge_size
        resized_height, resized_width = smart_resize(
            height, width, factor, min_pixels=min_pixels, max_pixels=max_pixels
        )
        grid_h, grid_w = resized_height // patch_size, resized_width // patch_size
        grid_t = num_frames // temporal_patch_size
        return grid_t * grid_h * grid_w


__all__ = ["KeyeVL1_5VideoProcessor"]
