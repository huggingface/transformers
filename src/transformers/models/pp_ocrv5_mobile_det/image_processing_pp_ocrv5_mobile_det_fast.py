#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/pp_ocrv5_mobile_det/modular_pp_ocrv5_mobile_det.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_pp_ocrv5_mobile_det.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨

import math
from typing import Optional, Union

import cv2
import numpy as np
import pyclipper
import torch
from torchvision.transforms.v2.functional import InterpolationMode

from ...feature_extraction_utils import BatchFeature
from ...image_processing_utils_fast import BaseImageProcessorFast
from ...image_utils import (
    PILImageResampling,
    SizeDict,
)
from ...utils import auto_docstring
from ...utils.generic import TensorType


def unclip(box, unclip_ratio):
    """
    Expands (dilates) a detected text bounding box to recover the full text region.
    The expansion distance is computed based on the contour area and perimeter,
    and Pyclipper is used to perform smooth contour offsetting.

    Args:
        box (np.ndarray): Input contour of shape (N, 2), where N is the number of points.
        unclip_ratio (float): Expansion ratio, typically greater than 1.0.

    Returns:
        np.ndarray: Expanded contour of shape (M, 2).
    """
    area = cv2.contourArea(box)
    length = cv2.arcLength(box, True)
    distance = area * unclip_ratio / length
    offset = pyclipper.PyclipperOffset()
    offset.AddPath(box, pyclipper.JT_ROUND, pyclipper.ET_CLOSEDPOLYGON)
    try:
        expanded = np.array(offset.Execute(distance))
    except ValueError:
        expanded = np.array(offset.Execute(distance)[0])
    return expanded


def get_mini_boxes(contour):
    """
    Computes the minimum-area bounding rectangle for a given contour and returns
    its four corners in a consistent order (top-left, bottom-left, bottom-right, top-right).

    Args:
        contour (np.ndarray): Input contour of shape (N, 1, 2).

    Returns:
        tuple:
            - box (list): List of four corner points in order.
            - sside (float): Length of the shorter side of the bounding rectangle.
    """
    bounding_box = cv2.minAreaRect(contour)
    points = sorted(cv2.boxPoints(bounding_box), key=lambda x: x[0])

    index_1, index_2, index_3, index_4 = 0, 1, 2, 3
    if points[1][1] > points[0][1]:
        index_1 = 0
        index_4 = 1
    else:
        index_1 = 1
        index_4 = 0
    if points[3][1] > points[2][1]:
        index_2 = 2
        index_3 = 3
    else:
        index_2 = 3
        index_3 = 2

    box = [points[index_1], points[index_2], points[index_3], points[index_4]]
    return box, min(bounding_box[1])


def box_score_fast(bitmap, _box):
    """
    Computes the mean score of a bounding box region in the prediction map using
    a fast approach with axis-aligned bounding boxes.

    Args:
        bitmap (np.ndarray): Binary or float prediction map of shape (H, W).
        _box (np.ndarray): Bounding box polygon of shape (N, 2).

    Returns:
        float: Mean score within the bounding box region.
    """
    h, w = bitmap.shape[:2]
    box = _box.copy()
    xmin = max(0, min(math.floor(box[:, 0].min()), w - 1))
    xmax = max(0, min(math.ceil(box[:, 0].max()), w - 1))
    ymin = max(0, min(math.floor(box[:, 1].min()), h - 1))
    ymax = max(0, min(math.ceil(box[:, 1].max()), h - 1))

    mask = np.zeros((ymax - ymin + 1, xmax - xmin + 1), dtype=np.uint8)
    box[:, 0] = box[:, 0] - xmin
    box[:, 1] = box[:, 1] - ymin
    cv2.fillPoly(mask, box.reshape(1, -1, 2).astype(np.int32), 1)
    return cv2.mean(bitmap[ymin : ymax + 1, xmin : xmax + 1], mask)[0]


def box_score_slow(bitmap, contour):
    """
    Computes the mean score of a contour region in the prediction map using
    the exact polygon shape, which is slower but more accurate.

    Args:
        bitmap (np.ndarray): Binary or float prediction map of shape (H, W).
        contour (np.ndarray): Contour polygon of shape (N, 2).

    Returns:
        float: Mean score within the contour region.
    """
    h, w = bitmap.shape[:2]
    contour = contour.copy()
    contour = np.reshape(contour, (-1, 2))

    xmin = np.clip(np.min(contour[:, 0]), 0, w - 1)
    xmax = np.clip(np.max(contour[:, 0]), 0, w - 1)
    ymin = np.clip(np.min(contour[:, 1]), 0, h - 1)
    ymax = np.clip(np.max(contour[:, 1]), 0, h - 1)

    mask = np.zeros((ymax - ymin + 1, xmax - xmin + 1), dtype=np.uint8)

    contour[:, 0] = contour[:, 0] - xmin
    contour[:, 1] = contour[:, 1] - ymin

    cv2.fillPoly(mask, contour.reshape(1, -1, 2).astype(np.int32), 1)
    return cv2.mean(bitmap[ymin : ymax + 1, xmin : xmax + 1], mask)[0]


def polygons_from_bitmap(
    pred,
    _bitmap,
    dest_width,
    dest_height,
    box_thresh,
    unclip_ratio,
    min_size,
    max_candidates,
):
    """
    Extracts text polygons from a binary segmentation map.

    Args:
        pred (np.ndarray): Raw prediction map of shape (H, W).
        _bitmap (np.ndarray): Binarized segmentation map of shape (H, W).
        dest_width (int): Original image width for scaling back.
        dest_height (int): Original image height for scaling back.
        box_thresh (float): Score threshold for filtering low-confidence boxes.
        unclip_ratio (float): Expansion ratio for contour unclipping.
        min_size (int): Minimum side length of valid boxes.
        max_candidates (int): Maximum number of contours to process.

    Returns:
        tuple:
            - boxes (list): List of polygons, each of shape (N, 2).
            - scores (list): List of corresponding scores.
    """

    bitmap = _bitmap
    height, width = bitmap.shape
    width_scale = dest_width / width
    height_scale = dest_height / height
    boxes = []
    scores = []

    contours, _ = cv2.findContours((bitmap * 255).astype(np.uint8), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)

    for contour in contours[:max_candidates]:
        epsilon = 0.002 * cv2.arcLength(contour, True)
        approx = cv2.approxPolyDP(contour, epsilon, True)
        points = approx.reshape((-1, 2))
        if points.shape[0] < 4:
            continue

        score = box_score_fast(pred, points.reshape(-1, 2))
        if box_thresh > score:
            continue

        if points.shape[0] > 2:
            box = unclip(points, unclip_ratio)
            if len(box) > 1:
                continue
        else:
            continue
        box = box.reshape(-1, 2)

        if len(box) > 0:
            _, sside = get_mini_boxes(box.reshape((-1, 1, 2)))
            if sside < min_size + 2:
                continue
        else:
            continue

        box = np.array(box)
        for i in range(box.shape[0]):
            box[i, 0] = max(0, min(round(box[i, 0] * width_scale), dest_width))
            box[i, 1] = max(0, min(round(box[i, 1] * height_scale), dest_height))

        boxes.append(box)
        scores.append(score)
    return boxes, scores


def boxes_from_bitmap(
    pred,
    _bitmap,
    dest_width,
    dest_height,
    box_thresh,
    unclip_ratio,
    min_size,
    max_candidates,
    score_mode,
):
    """
    Extracts axis-aligned or rotated bounding boxes from a binary segmentation map.

    Args:
        pred (np.ndarray): Raw prediction map of shape (H, W).
        _bitmap (np.ndarray): Binarized segmentation map of shape (H, W).
        dest_width (int): Original image width for scaling back.
        dest_height (int): Original image height for scaling back.
        box_thresh (float): Score threshold for filtering low-confidence boxes.
        unclip_ratio (float): Expansion ratio for contour unclipping.
        min_size (int): Minimum side length of valid boxes.
        max_candidates (int): Maximum number of contours to process.
        score_mode (str): Scoring mode, either "fast" or "slow".

    Returns:
        tuple:
            - boxes (np.ndarray): Array of boxes, each of shape (4, 2).
            - scores (list): List of corresponding scores.
    """

    bitmap = _bitmap
    height, width = bitmap.shape
    width_scale = dest_width / width
    height_scale = dest_height / height

    outs = cv2.findContours((bitmap * 255).astype(np.uint8), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)
    if len(outs) == 3:
        _, contours, _ = outs[0], outs[1], outs[2]
    elif len(outs) == 2:
        contours, _ = outs[0], outs[1]

    num_contours = min(len(contours), max_candidates)

    boxes = []
    scores = []
    for index in range(num_contours):
        contour = contours[index]
        points, sside = get_mini_boxes(contour)
        if sside < min_size:
            continue
        points = np.array(points)
        if score_mode == "fast":
            score = box_score_fast(pred, points.reshape(-1, 2))
        else:
            score = box_score_slow(pred, contour)
        if box_thresh > score:
            continue
        box = unclip(points, unclip_ratio).reshape(-1, 1, 2)
        box, sside = get_mini_boxes(box)
        if sside < min_size + 2:
            continue

        box = np.array(box)
        for i in range(box.shape[0]):
            box[i, 0] = max(0, min(round(box[i, 0] * width_scale), dest_width))
            box[i, 1] = max(0, min(round(box[i, 1] * height_scale), dest_height))

        boxes.append(box.astype(np.int16))
        scores.append(score)
    return np.array(boxes, dtype=np.int16), scores


def process(
    pred, size, thresh, box_type, box_thresh, unclip_ratio, use_dilation, min_size, max_candidates, score_mode
):
    """
    Main post-processing function to convert model predictions into text boxes.

    Args:
        pred (torch.Tensor): Model output of shape (1, H, W).
        size (torch.Tensor): Original image size (height, width).
        thresh (float): Threshold for binarizing the prediction map.
        box_type (str): Type of boxes to extract, either "quad" or "poly".
        box_thresh (float): Score threshold for filtering boxes.
        unclip_ratio (float): Expansion ratio for unclipping.
        use_dilation (bool): Whether to apply dilation on the segmentation mask.
        min_size (int): Minimum side length of valid boxes.
        max_candidates (int): Maximum number of boxes to extract.
        score_mode (str): Scoring mode, either "fast" or "slow".

    Returns:
        tuple:
            - boxes (list or np.ndarray): Extracted text boxes.
            - scores (list): Corresponding confidence scores.
    """
    pred = pred[0, :, :].cpu().detach().numpy()
    segmentation = pred > thresh
    dilation_kernel = None if not use_dilation else np.array([[1, 1], [1, 1]])
    src_h, src_w = size.cpu().detach().numpy()
    if dilation_kernel is not None:
        mask = cv2.dilate(
            np.array(segmentation).astype(np.uint8),
            dilation_kernel,
        )
    else:
        mask = segmentation
    if box_type == "poly":
        boxes, scores = polygons_from_bitmap(
            pred,
            mask,
            src_w,
            src_h,
            box_thresh,
            unclip_ratio,
            min_size,
            max_candidates,
        )
    elif box_type == "quad":
        boxes, scores = boxes_from_bitmap(
            pred, mask, src_w, src_h, box_thresh, unclip_ratio, min_size, max_candidates, score_mode
        )
    else:
        raise ValueError("box_type can only be one of ['quad', 'poly']")
    return boxes, scores


@auto_docstring(custom_intro="ImageProcessorFast for the PPOCRV5 Mobile Det model.")
class PPOCRV5MobileDetImageProcessorFast(BaseImageProcessorFast):
    """
    Image processor for PPOCRV5 Mobile Det model, handling preprocessing (resizing, normalization)
    and post-processing (converting model outputs to text boxes).
    """
    resample = 2
    image_mean = [0.406, 0.456, 0.485]
    image_std = [0.225, 0.224, 0.229]
    size = {"height": 960, "width": 960}
    do_resize = True
    do_rescale = True
    do_normalize = True
    limit_side_len = 960
    limit_type = "max"
    max_side_limit = 4000

    def __init__(self, **kwargs) -> None:
        super().__init__(**kwargs)

    def _preprocess(
        self,
        images: list[torch.Tensor],
        size: Optional[list[dict[str, int]]],
        do_resize: bool,
        do_rescale: bool,
        rescale_factor: float,
        do_normalize: bool,
        image_mean: Optional[Union[float, list[float]]],
        image_std: Optional[Union[float, list[float]]],
        return_tensors: Optional[Union[str, TensorType]],
        interpolation: Optional[InterpolationMode] = None,
        resample: Optional[PILImageResampling] = None,
        **kwargs,
    ) -> BatchFeature:
        """
        Preprocesses a list of images for input to the PPOCRV5 Mobile Det model.

        Args:
            images (list[torch.Tensor]): List of input images.
            size (list[dict[str, int]]): Target size for each image.
            do_resize (bool): Whether to resize images.
            do_rescale (bool): Whether to rescale pixel values.
            rescale_factor (float): Rescale factor.
            do_normalize (bool): Whether to normalize images.
            image_mean (list[float] or float): Mean values for normalization.
            image_std (list[float] or float): Std values for normalization.
            return_tensors (str or TensorType): Type of tensors to return.
            interpolation (InterpolationMode): Interpolation mode for resizing.
            resample (PILImageResampling): PIL resampling mode (unused).

        Returns:
            BatchFeature: Preprocessed images and additional information.
        """
        data = {}
        resize_imgs, target_sizes = [], []
        if do_resize:
            # interpolation = InterpolationMode.BILINEAR
            for image in images:
                size, shape = self.get_image_size(image, self.limit_side_len, self.limit_type, self.max_side_limit)
                img = self.resize(image, size=size, interpolation=interpolation)
                resize_imgs.append(img)
                target_sizes.append(shape)
            images = resize_imgs

        processed_images = []
        for image in images:
            image = self.rescale_and_normalize(image, do_rescale, rescale_factor, do_normalize, image_mean, image_std)
            processed_images.append(image)

        images = processed_images
        images = [image[[2, 1, 0], :, :] for image in images]

        data.update({"pixel_values": torch.stack(images, dim=0), "target_sizes": target_sizes})
        encoded_inputs = BatchFeature(data, tensor_type=return_tensors)

        return encoded_inputs

    def post_process_object_detection(
        self,
        preds,
        threshold: float = 0.5,
        target_sizes: Union[TensorType, list[tuple]] = None,
        thresh: float = 0.3,
        box_thresh=0.6,
        max_candidates=1000,
        min_size=3,
        unclip_ratio=1.5,
        use_dilation=False,
        score_mode="fast",
        box_type="quad",
    ):
        """
        Converts model outputs into detected text boxes.

        Args:
            preds (torch.Tensor): Model outputs.
            threshold (float): Confidence threshold (unused).
            target_sizes (TensorType or list[tuple]): Original image sizes.
            thresh (float): Binarization threshold.
            box_thresh (float): Box score threshold.
            max_candidates (int): Maximum number of boxes.
            min_size (int): Minimum box size.
            unclip_ratio (float): Expansion ratio.
            use_dilation (bool): Whether to dilate the mask.
            score_mode (str): Scoring mode.
            box_type (str): Box type, "quad" or "poly".

        Returns:
            list[dict]: List of detection results.
        """
        assert score_mode in [
            "slow",
            "fast",
        ], f"Score mode must be in [slow, fast] but got: {score_mode}"

        return self.postprocess(
            preds=preds.logits,
            thresh=thresh,
            target_sizes=target_sizes,
            box_thresh=box_thresh,
            max_candidates=max_candidates,
            min_size=min_size,
            unclip_ratio=unclip_ratio,
            use_dilation=use_dilation,
            score_mode=score_mode,
            box_type=box_type,
        )

    def postprocess(
        self,
        preds,
        thresh=0.3,
        target_sizes=None,
        box_thresh=0.6,
        max_candidates=1000,
        min_size=3,
        unclip_ratio=1.5,
        use_dilation=False,
        score_mode="fast",
        box_type="quad",
    ):
        """
        Post-processes model outputs to extract text boxes.

        Args:
            preds (torch.Tensor): Model logits.
            thresh (float): Binarization threshold.
            target_sizes (list[tuple]): Original image sizes.
            box_thresh (float): Box score threshold.
            max_candidates (int): Maximum number of boxes.
            min_size (int): Minimum box size.
            unclip_ratio (float): Expansion ratio.
            use_dilation (bool): Whether to dilate the mask.
            score_mode (str): Scoring mode.
            box_type (str): Box type.

        Returns:
            list[dict]: List of detection results.
        """
        results = []
        for pred, size in zip(preds, target_sizes):
            box, score = process(
                pred=pred,
                size=size,
                thresh=thresh,
                box_type=box_type,
                box_thresh=box_thresh,
                unclip_ratio=unclip_ratio,
                use_dilation=use_dilation,
                min_size=min_size,
                max_candidates=max_candidates,
                score_mode=score_mode,
            )

            results.append(
                {
                    "boxes": box,
                    "scores": score,
                }
            )
        return results

    def get_image_size(
        self,
        img: torch.Tensor,
        limit_side_len: Union[int, None],
        limit_type: Union[str, None],
        max_side_limit: Union[int, None] = None,
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """
        Computes the target size for resizing an image while preserving aspect ratio.

        Args:
            img (torch.Tensor): Input image.
            limit_side_len (int): Maximum or minimum side length.
            limit_type (str): Resizing strategy: "max", "min", or "resize_long".
            max_side_limit (int): Maximum allowed side length.

        Returns:
            tuple:
                - SizeDict: Target size.
                - torch.Tensor: Original size.
        """
        limit_side_len = limit_side_len or self.limit_side_len
        limit_type = limit_type or self.limit_type
        c, h, w = img.shape
        h, w = int(h), int(w)

        if limit_type == "max":
            if max(h, w) > limit_side_len:
                ratio = float(limit_side_len) / max(h, w)
            else:
                ratio = 1.0
        elif limit_type == "min":
            if min(h, w) < limit_side_len:
                ratio = float(limit_side_len) / min(h, w)
            else:
                ratio = 1.0
        elif limit_type == "resize_long":
            ratio = float(limit_side_len) / max(h, w)
        else:
            raise Exception(f"not support limit type: {limit_type}")

        resize_h = int(h * ratio)
        resize_w = int(w * ratio)

        if max_side_limit is not None and max(resize_h, resize_w) > max_side_limit:
            ratio = float(max_side_limit) / max(resize_h, resize_w)
            resize_h = int(resize_h * ratio)
            resize_w = int(resize_w * ratio)

        resize_h = max(int(round(resize_h / 32) * 32), 32)
        resize_w = max(int(round(resize_w / 32) * 32), 32)

        if resize_h == h and resize_w == w:
            return SizeDict(height=resize_h, width=resize_w), torch.tensor([h, w], dtype=torch.float32)

        if resize_w <= 0 or resize_h <= 0:
            return None, (None, None)

        return SizeDict(height=resize_h, width=resize_w), torch.tensor([h, w], dtype=torch.float32)


__all__ = ["PPOCRV5MobileDetImageProcessorFast"]
