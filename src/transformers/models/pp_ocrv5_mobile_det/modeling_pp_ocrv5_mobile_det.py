#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
#           This file was automatically generated from src/transformers/models/pp_ocrv5_mobile_det/modular_pp_ocrv5_mobile_det.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_pp_ocrv5_mobile_det.py file directly. One of our CI enforces this.
#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
from dataclasses import dataclass
from typing import Optional, Union

import torch
import torch.nn as nn
import torch.nn.functional as F

from ...modeling_outputs import BaseModelOutputWithNoAttention
from ...modeling_utils import PreTrainedModel
from ...utils import ModelOutput, auto_docstring
from .configuration_pp_ocrv5_mobile_det import PPOCRV5MobileDetConfig


def make_divisible(v, divisor: int = 16, min_value=None):
    """
    Ensures that the input value `v` is rounded to the nearest multiple of `divisor`,
    with a minimum value constraint. This is used to adjust channel dimensions for
    hardware-efficient neural network inference (especially on mobile devices).

    Args:
        v (float): Input value to be adjusted (typically channel count).
        divisor (int, optional): The divisor to align the value with. Defaults to 16.
        min_value (int, optional): Minimum allowed value after adjustment. If None,
            defaults to `divisor`.

    Returns:
        int: Adjusted value that is a multiple of `divisor` and meets the minimum value requirement.
    """
    if min_value is None:
        min_value = divisor
    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)
    if new_v < 0.9 * v:
        new_v += divisor
    return new_v


class LearnableAffineBlock(nn.Module):
    """
    Learnable affine transformation block that applies scale and bias to the input tensor.
    Both scale and bias are trainable parameters, allowing the model to learn optimal
    linear transformations for feature normalization or enhancement.
    """

    def __init__(self, scale_value=1.0, bias_value=0.0):
        """
        Initialize the LearnableAffineBlock with initial scale and bias values.

        Args:
            scale_value (float, optional): Initial value for the scale parameter. Defaults to 1.0.
            bias_value (float, optional): Initial value for the bias parameter. Defaults to 0.0.
        """
        super().__init__()
        self.scale = nn.Parameter(torch.tensor([scale_value]))
        self.bias = nn.Parameter(torch.tensor([bias_value]))

    def forward(self, hidden_state: torch.Tensor):
        """
        Apply the affine transformation to the input hidden state.

        Args:
            hidden_state (torch.Tensor): Input feature tensor of shape (B, C, H, W).

        Returns:
            torch.Tensor: Transformed feature tensor after applying scale and bias.
        """
        return self.scale * hidden_state + self.bias


class Act(nn.Module):
    """
    Activation block with a trainable affine transformation applied after the non-linear activation.
    Supports two activation functions: Hardswish (hswish) for mobile-efficient inference and ReLU.
    """

    def __init__(self, act="hswish"):
        """
        Initialize the activation block with the specified non-linear activation.

        Args:
            act (str, optional): Type of activation function to use. Options are "hswish" and "relu".
                Defaults to "hswish".
        """
        super().__init__()
        if act == "hswish":
            self.act = nn.Hardswish()
        else:
            assert act == "relu"
            self.act = nn.ReLU()
        self.lab = LearnableAffineBlock()

    def forward(self, hidden_state: torch.Tensor):
        """
        Apply the non-linear activation followed by the learnable affine transformation.

        Args:
            hidden_state (torch.Tensor): Input feature tensor of shape (B, C, H, W).

        Returns:
            torch.Tensor: Activated and transformed feature tensor.
        """
        return self.lab(self.act(hidden_state))


class ConvBNLayer(nn.Module):
    """
    Convolution-Batch Normalization layer block, a fundamental building block for modern CNNs.
    Applies 2D convolution followed by batch normalization, with He Kaiming initialization for the convolution weights.
    """

    def __init__(self, in_channels, out_channels, kernel_size, stride, groups=1):
        """
        Initialize the ConvBNLayer with specified convolution and batch normalization parameters.

        Args:
            in_channels (int): Number of input channels for the convolution layer.
            out_channels (int): Number of output channels for the convolution layer.
            kernel_size (int): Size of the convolution kernel (square kernel).
            stride (int): Stride of the convolution operation.
            groups (int, optional): Number of groups for grouped convolution (used for depthwise convolutions).
                Defaults to 1 (standard convolution).
        """
        super().__init__()
        self.conv = nn.Conv2d(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=kernel_size,
            stride=stride,
            padding=(kernel_size - 1) // 2,
            groups=groups,
            bias=False,
        )
        nn.init.kaiming_normal_(self.conv.weight)

        self.bn = nn.BatchNorm2d(out_channels, momentum=0.9)

    def forward(self, hidden_state: torch.Tensor):
        """
        Apply convolution followed by batch normalization to the input tensor.

        Args:
            hidden_state (torch.Tensor): Input feature tensor of shape (B, in_channels, H, W).

        Returns:
            torch.Tensor: Output feature tensor of shape (B, out_channels, H', W').
        """
        hidden_state = self.conv(hidden_state)
        hidden_state = self.bn(hidden_state)
        return hidden_state


class LearnableRepLayer(nn.Module):
    """
    Learnable Reparameterization Layer (RepLayer) that fuses multiple convolution branches
    (kxk and 1x1) with an optional identity branch. This layer enables structural reparameterization
    for efficient inference while maintaining training flexibility.
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        act: str,
        stride: int,
        num_conv_branches: int,
        groups: int = 1,
    ):
        """
        Initialize the LearnableRepLayer with multiple convolution branches and optional identity connection.

        Args:
            in_channels (int): Number of input channels.
            out_channels (int): Number of output channels.
            kernel_size (int): Size of the kxk convolution kernel.
            act (str): Activation function type (passed to Act block).
            stride (int): Stride of the convolution operations.
            num_conv_branches (int): Number of kxk convolution branches to stack.
            groups (int, optional): Number of groups for grouped convolution. Defaults to 1.
        """
        super().__init__()
        self.groups = groups
        self.stride = stride
        self.kernel_size = kernel_size
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.num_conv_branches = num_conv_branches
        self.padding = (kernel_size - 1) // 2

        self.identity = (
            nn.BatchNorm2d(num_features=in_channels, momentum=0.9)
            if out_channels == in_channels and stride == 1
            else None
        )

        self.conv_kxk = nn.ModuleList(
            [
                ConvBNLayer(
                    in_channels,
                    out_channels,
                    kernel_size,
                    stride,
                    groups=groups,
                )
                for _ in range(self.num_conv_branches)
            ]
        )

        self.conv_1x1 = ConvBNLayer(in_channels, out_channels, 1, stride, groups=groups) if kernel_size > 1 else None

        self.lab = LearnableAffineBlock()
        self.act = Act(act=act)

    def forward(self, hidden_state: torch.Tensor):
        """
        Forward pass of the LearnableRepLayer, fusing all enabled branches and applying post-processing.

        Args:
            hidden_state (torch.Tensor): Input feature tensor of shape (B, in_channels, H, W).

        Returns:
            torch.Tensor: Output feature tensor of shape (B, out_channels, H', W').
        """
        output = 0
        if self.identity is not None:
            output += self.identity(hidden_state)

        if self.conv_1x1 is not None:
            output += self.conv_1x1(hidden_state)

        for conv in self.conv_kxk:
            output += conv(hidden_state)

        hidden_state = self.lab(output)
        if self.stride != 2:
            hidden_state = self.act(hidden_state)
        return hidden_state


class SELayer(nn.Module):
    """
    Squeeze-and-Excitation (SE) Layer for channel-wise feature recalibration.
    This layer adaptively scales channel features based on their importance,
    improving the model's ability to capture informative features.
    """

    def __init__(self, channel, reduction=4):
        """
        Initialize the SELayer with channel reduction factor.

        Args:
            channel (int): Number of input/output channels.
            reduction (int, optional): Reduction factor for the squeeze operation (controls the size of the hidden layer).
                Defaults to 4.
        """
        super().__init__()

        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.conv1 = nn.Conv2d(
            in_channels=channel,
            out_channels=channel // reduction,
            kernel_size=1,
            stride=1,
            padding=0,
        )
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(
            in_channels=channel // reduction,
            out_channels=channel,
            kernel_size=1,
            stride=1,
            padding=0,
        )
        self.hardsigmoid = nn.Hardsigmoid()

    def forward(self, hidden_state: torch.Tensor):
        """
        Apply squeeze-and-excitation to the input feature tensor.

        Args:
            hidden_state (torch.Tensor): Input feature tensor of shape (B, C, H, W).

        Returns:
            torch.Tensor: Recalibrated feature tensor of shape (B, C, H, W).
        """
        identity = hidden_state
        hidden_state = self.avg_pool(hidden_state)
        hidden_state = self.conv1(hidden_state)
        hidden_state = self.relu(hidden_state)
        hidden_state = self.conv2(hidden_state)
        hidden_state = self.hardsigmoid(hidden_state)
        hidden_state = torch.multiply(identity, hidden_state)
        return hidden_state


class LCNetV3Block(nn.Module):
    """
    Lightweight Convolutional Network V3 (LCNetV3) Block, the core building block of the PPOCRV5 Mobile Det backbone.
    Consists of a depthwise LearnableRepLayer, an optional SE Layer, and a pointwise LearnableRepLayer.
    Optimized for mobile devices with low computational complexity and high efficiency.
    """

    def __init__(self, in_channels, out_channels, act, dw_size, stride, use_se, conv_kxk_num, reduction):
        """
        Initialize the LCNetV3Block with specified parameters for depthwise and pointwise layers.

        Args:
            in_channels (int): Number of input channels.
            out_channels (int): Number of output channels.
            act (str): Activation function type (passed to Act block).
            dw_size (int): Kernel size for the depthwise convolution.
            stride (int): Stride of the depthwise convolution.
            use_se (bool): Whether to enable the SE Layer for channel recalibration.
            conv_kxk_num (int): Number of kxk convolution branches in LearnableRepLayer.
            reduction (int): Reduction factor for the SE Layer (if enabled).
        """
        super().__init__()
        self.use_se = use_se
        self.dw_conv = LearnableRepLayer(
            in_channels=in_channels,
            out_channels=in_channels,
            kernel_size=dw_size,
            act=act,
            stride=stride,
            groups=in_channels,
            num_conv_branches=conv_kxk_num,
        )
        if use_se:
            self.se = SELayer(in_channels, reduction=reduction)
        self.pw_conv = LearnableRepLayer(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=1,
            act=act,
            stride=1,
            num_conv_branches=conv_kxk_num,
        )

    def forward(self, hidden_state: torch.Tensor):
        """
        Forward pass of the LCNetV3Block, applying depthwise convolution, optional SE, and pointwise convolution.

        Args:
            hidden_state (torch.Tensor): Input feature tensor of shape (B, in_channels, H, W).

        Returns:
            torch.Tensor: Output feature tensor of shape (B, out_channels, H', W').
        """
        hidden_state = self.dw_conv(hidden_state)
        if self.use_se:
            hidden_state = self.se(hidden_state)
        hidden_state = self.pw_conv(hidden_state)
        return hidden_state


class PPOCRV5MobileDetBackbone(nn.Module):
    """
    Backbone network for PPOCRV5 Mobile Det, built with LCNetV3Blocks.
    Extracts multi-scale feature maps from input images, which are passed to the neck network for further fusion.
    Optimized for mobile devices with lightweight, efficient layers and channel scaling.
    """

    def __init__(self, config: PPOCRV5MobileDetConfig):
        """
        Initialize the PPOCRV5MobileDetBackbone with the specified model configuration.

        Args:
            config (PPOCRV5MobileDetConfig): Configuration object containing backbone hyperparameters.
        """
        super().__init__()

        self.backbone_config = config.backbone_config
        self.out_channels = make_divisible(config.backbone_out_channels * config.scale, config.divisor)

        self.conv1 = ConvBNLayer(
            in_channels=3,
            out_channels=make_divisible(16 * config.scale, config.divisor),
            kernel_size=3,
            stride=2,
        )

        def _build_blocks(block_key):
            return nn.Sequential(
                *[
                    LCNetV3Block(
                        in_channels=make_divisible(in_c * config.scale, config.divisor),
                        out_channels=make_divisible(out_c * config.scale, config.divisor),
                        act=config.hidden_act,
                        dw_size=k,
                        stride=s,
                        use_se=se,
                        conv_kxk_num=config.conv_kxk_num,
                        reduction=config.reduction,
                    )
                    for i, (k, in_c, out_c, s, se) in enumerate(self.backbone_config[block_key])
                ]
            )

        self.blocks2 = _build_blocks("blocks2")
        self.blocks3 = _build_blocks("blocks3")
        self.blocks4 = _build_blocks("blocks4")
        self.blocks5 = _build_blocks("blocks5")
        self.blocks6 = _build_blocks("blocks6")

        mv_c = self.backbone_config["layer_list_out_channels"]

        self.out_channels = [
            make_divisible(self.backbone_config["blocks3"][-1][2] * config.scale, config.divisor),
            make_divisible(self.backbone_config["blocks4"][-1][2] * config.scale, config.divisor),
            make_divisible(self.backbone_config["blocks5"][-1][2] * config.scale, config.divisor),
            make_divisible(self.backbone_config["blocks6"][-1][2] * config.scale, config.divisor),
        ]

        self.layer_list = nn.ModuleList(
            [
                nn.Conv2d(self.out_channels[0], int(mv_c[0] * config.scale), 1, 1, 0),
                nn.Conv2d(self.out_channels[1], int(mv_c[1] * config.scale), 1, 1, 0),
                nn.Conv2d(self.out_channels[2], int(mv_c[2] * config.scale), 1, 1, 0),
                nn.Conv2d(self.out_channels[3], int(mv_c[3] * config.scale), 1, 1, 0),
            ]
        )
        self.out_channels = [
            int(mv_c[0] * config.scale),
            int(mv_c[1] * config.scale),
            int(mv_c[2] * config.scale),
            int(mv_c[3] * config.scale),
        ]

    def forward(self, hidden_state: torch.Tensor, output_hidden_states: bool, return_dict: bool = True):
        """
        Forward pass of the backbone network, extracting multi-scale feature maps and optional hidden states.

        Args:
            hidden_state (torch.Tensor): Input image tensor of shape (B, 3, H, W).
            output_hidden_states (bool): Whether to return all intermediate hidden states for analysis.
            return_dict (bool, optional): Unused (for consistency with other modules). Defaults to True.

        Returns:
            tuple:
                - list[torch.Tensor]: Multi-scale feature maps after projection (4 feature maps).
                - torch.Tensor: Last hidden state (output of blocks6).
                - tuple[torch.Tensor, ...]: All intermediate hidden states (if output_hidden_states is True).
        """
        hidden_states = () if output_hidden_states else None

        out_list = []
        if output_hidden_states:
            hidden_states = hidden_states + (hidden_state,)
        hidden_state = self.conv1(hidden_state)
        if output_hidden_states:
            hidden_states = hidden_states + (hidden_state,)
        hidden_state = self.blocks2(hidden_state)
        if output_hidden_states:
            hidden_states = hidden_states + (hidden_state,)
        hidden_state = self.blocks3(hidden_state)
        if output_hidden_states:
            hidden_states = hidden_states + (hidden_state,)
        out_list.append(hidden_state)
        hidden_state = self.blocks4(hidden_state)
        if output_hidden_states:
            hidden_states = hidden_states + (hidden_state,)
        out_list.append(hidden_state)
        hidden_state = self.blocks5(hidden_state)
        if output_hidden_states:
            hidden_states = hidden_states + (hidden_state,)
        out_list.append(hidden_state)
        hidden_state = self.blocks6(hidden_state)
        if output_hidden_states:
            hidden_states = hidden_states + (hidden_state,)
        out_list.append(hidden_state)
        last_hidden_state = hidden_state
        out_list[0] = self.layer_list[0](out_list[0])
        out_list[1] = self.layer_list[1](out_list[1])
        out_list[2] = self.layer_list[2](out_list[2])
        out_list[3] = self.layer_list[3](out_list[3])

        return out_list, last_hidden_state, hidden_states


class SEModule(nn.Module):
    """
    Simplified Squeeze-and-Excitation (SE) Module for the neck network.
    Applies channel-wise recalibration with a clamped activation to stabilize training.
    """

    def __init__(self, in_channels, reduction=4):
        """
        Initialize the SEModule with channel reduction factor.

        Args:
            in_channels (int): Number of input/output channels.
            reduction (int, optional): Reduction factor for the squeeze operation. Defaults to 4.
        """
        super().__init__()

        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.conv1 = nn.Conv2d(
            in_channels=in_channels,
            out_channels=in_channels // reduction,
            kernel_size=1,
            stride=1,
            padding=0,
        )
        self.conv2 = nn.Conv2d(
            in_channels=in_channels // reduction,
            out_channels=in_channels,
            kernel_size=1,
            stride=1,
            padding=0,
        )

    def forward(self, inputs):
        """
        Apply simplified squeeze-and-excitation to the input tensor.

        Args:
            inputs (torch.Tensor): Input feature tensor of shape (B, C, H, W).

        Returns:
            torch.Tensor: Recalibrated feature tensor of shape (B, C, H, W).
        """
        outputs = self.avg_pool(inputs)
        outputs = self.conv1(outputs)
        outputs = F.relu(outputs)
        outputs = self.conv2(outputs)
        outputs = torch.clamp(0.2 * outputs + 0.5, min=0.0, max=1.0)
        return inputs * outputs


class RSELayer(nn.Module):
    """
    Residual Squeeze-and-Excitation (RSE) Layer for the neck network.
    Combines a 1x1/3x3 convolution with an SE Module and an optional residual shortcut connection.
    """

    def __init__(self, in_channels, out_channels, kernel_size, shortcut=True):
        """
        Initialize the RSELayer with convolution and residual connection parameters.

        Args:
            in_channels (int): Number of input channels.
            out_channels (int): Number of output channels.
            kernel_size (int): Size of the convolution kernel.
            shortcut (bool, optional): Whether to enable the residual shortcut connection. Defaults to True.
        """
        super().__init__()
        self.out_channels = out_channels
        self.in_conv = nn.Conv2d(
            in_channels=in_channels,
            out_channels=self.out_channels,
            kernel_size=kernel_size,
            padding=int(kernel_size // 2),
            bias=False,
        )
        nn.init.kaiming_uniform_(self.in_conv.weight)
        self.se_block = SEModule(self.out_channels)
        self.shortcut = shortcut

    def forward(self, ins):
        """
        Forward pass of the RSELayer, applying convolution, SE recalibration, and optional residual connection.

        Args:
            ins (torch.Tensor): Input feature tensor of shape (B, in_channels, H, W).

        Returns:
            torch.Tensor: Output feature tensor of shape (B, out_channels, H, W).
        """
        x = self.in_conv(ins)
        if self.shortcut:
            out = x + self.se_block(x)
        else:
            out = self.se_block(x)
        return out


class PPOCRV5MobileDetNeck(nn.Module):
    """
    Neck network for PPOCRV5 Mobile Det, responsible for multi-scale feature fusion.
    Uses RSELayers to process backbone features and upsampling to fuse features at the same spatial scale,
    then concatenates the fused features for input to the head network.
    """

    def __init__(self, config: PPOCRV5MobileDetConfig, in_channels: list[int]):
        """
        Initialize the PPOCRV5MobileDetNeck with the specified model configuration and input channels.

        Args:
            config (PPOCRV5MobileDetConfig): Configuration object containing neck hyperparameters.
            in_channels (list[int]): List of input channels from the backbone's multi-scale feature maps.
        """
        super().__init__()
        self.interpolate_mode = config.interpolate_mode

        self.ins_conv = nn.ModuleList()
        self.inp_conv = nn.ModuleList()

        for i in range(len(in_channels)):
            self.ins_conv.append(
                RSELayer(in_channels[i], config.neck_out_channels, kernel_size=1, shortcut=config.shortcut)
            )
            self.inp_conv.append(
                RSELayer(
                    config.neck_out_channels, config.neck_out_channels // 4, kernel_size=3, shortcut=config.shortcut
                )
            )

    def forward(self, x):
        """
        Forward pass of the neck network, fusing multi-scale backbone features.

        Args:
            x (list[torch.Tensor]): List of multi-scale feature maps from the backbone (4 feature maps).

        Returns:
            torch.Tensor: Concatenated fused feature tensor of shape (B, C, H, W).
        """
        c2, c3, c4, c5 = x

        in5 = self.ins_conv[3](c5)
        in4 = self.ins_conv[2](c4)
        in3 = self.ins_conv[1](c3)
        in2 = self.ins_conv[0](c2)

        out4 = in4 + F.interpolate(in5, scale_factor=2, mode=self.interpolate_mode)  # 1/16
        out3 = in3 + F.interpolate(out4, scale_factor=2, mode=self.interpolate_mode)  # 1/8
        out2 = in2 + F.interpolate(out3, scale_factor=2, mode=self.interpolate_mode)  # 1/4

        p5 = self.inp_conv[3](in5)
        p4 = self.inp_conv[2](out4)
        p3 = self.inp_conv[1](out3)
        p2 = self.inp_conv[0](out2)

        p5 = F.interpolate(p5, scale_factor=8, mode=self.interpolate_mode)
        p4 = F.interpolate(p4, scale_factor=4, mode=self.interpolate_mode)
        p3 = F.interpolate(p3, scale_factor=2, mode=self.interpolate_mode)

        fuse = torch.cat([p5, p4, p3, p2], dim=1)
        return fuse


class Head(nn.Module):
    """
    Head sub-module for PPOCRV5 Mobile Det, responsible for generating text segmentation maps.
    Uses two transposed convolutions for upsampling to recover the original image spatial scale,
    and a sigmoid activation to produce binary segmentation logits.
    """

    def __init__(self, in_channels, kernel_list=[3, 2, 2]):
        """
        Initialize the Head sub-module with convolution and upsampling parameters.

        Args:
            in_channels (int): Number of input channels from the neck network.
            kernel_list (list[int], optional): List of kernel sizes for the three convolution layers:
                [conv1 kernel, conv2 transposed kernel, conv3 transposed kernel]. Defaults to [3, 2, 2].
        """
        super().__init__()

        self.conv1 = nn.Conv2d(
            in_channels=in_channels,
            out_channels=in_channels // 4,
            kernel_size=kernel_list[0],
            padding=int(kernel_list[0] // 2),
            bias=False,
        )
        self.conv_bn1 = nn.BatchNorm2d(in_channels // 4, momentum=0.9)
        self.relu1 = nn.ReLU()

        nn.init.constant_(self.conv_bn1.weight, 1.0)
        nn.init.constant_(self.conv_bn1.bias, 1e-4)

        self.conv2 = nn.ConvTranspose2d(
            in_channels=in_channels // 4,
            out_channels=in_channels // 4,
            kernel_size=kernel_list[1],
            stride=2,
        )
        nn.init.kaiming_uniform_(self.conv2.weight)
        self.conv_bn2 = nn.BatchNorm2d(in_channels // 4, momentum=0.9)
        self.relu2 = nn.ReLU()

        nn.init.constant_(self.conv_bn2.weight, 1.0)
        nn.init.constant_(self.conv_bn2.bias, 1e-4)

        self.conv3 = nn.ConvTranspose2d(
            in_channels=in_channels // 4,
            out_channels=1,
            kernel_size=kernel_list[2],
            stride=2,
        )
        nn.init.kaiming_uniform_(self.conv3.weight)

    def forward(self, x):
        """
        Forward pass of the Head sub-module, generating binary segmentation logits.

        Args:
            x (torch.Tensor): Input feature tensor of shape (B, in_channels, H, W).

        Returns:
            torch.Tensor: Binary segmentation logits of shape (B, 1, H', W') (original image scale).
        """
        x = self.conv1(x)
        x = self.conv_bn1(x)
        x = self.relu1(x)
        x = self.conv2(x)
        x = self.conv_bn2(x)
        x = self.relu2(x)
        x = self.conv3(x)
        x = torch.sigmoid(x)
        return x


class PPOCRV5MobileDetHead(nn.Module):
    """
    Head network for PPOCRV5 Mobile Det, wrapping the Head sub-module to generate text segmentation maps.
    Contains the `k` parameter for candidate box selection during post-processing.
    """

    def __init__(self, config: PPOCRV5MobileDetConfig):
        """
        Initialize the PPOCRV5MobileDetHead with the specified model configuration.

        Args:
            config (PPOCRV5MobileDetConfig): Configuration object containing head hyperparameters.
        """
        super().__init__()
        self.k = config.k
        self.binarize = Head(config.neck_out_channels, config.kernel_list)

    def forward(self, x):
        """
        Forward pass of the head network, generating text segmentation (shrink) maps.

        Args:
            x (torch.Tensor): Input feature tensor of shape (B, C, H, W) from the neck network.

        Returns:
            torch.Tensor: Binary segmentation shrink maps of shape (B, 1, H', W').
        """
        shrink_maps = self.binarize(x)
        return shrink_maps


@dataclass
class PPOCRV5MobileDetModelOutput(ModelOutput):
    """
    Output class for the PPOCRV5MobileDetModel.

    Args:
        logits (torch.FloatTensor, optional): Binary segmentation logits from the head network,
            shape (B, 1, H, W).
        last_hidden_state (torch.FloatTensor, optional): Last hidden state from the backbone network,
            shape (B, C, H, W).
        hidden_states (tuple[torch.FloatTensor], optional): Tuple of all intermediate hidden states from the backbone,
            if `output_hidden_states` is True.
    """

    logits: Optional[torch.FloatTensor] = None
    last_hidden_state: Optional[torch.FloatTensor] = None
    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None


class PPOCRV5MobileDetPreTrainedModel(PreTrainedModel):
    """
    Base class for all PPOCRV5 Mobile Det pre-trained models. Handles model initialization,
    configuration, and loading of pre-trained weights, following the Transformers library conventions.
    """

    config: PPOCRV5MobileDetConfig
    base_model_prefix = "pp_ocrv5_mobile_det"
    main_input_name = "pixel_values"
    input_modalities = ("image",)


@auto_docstring(custom_intro="The PPOCRV5 Mobile Det model.")
class PPOCRV5MobileDetModel(PPOCRV5MobileDetPreTrainedModel):
    """
    Core PPOCRV5 Mobile Det model, consisting of Backbone, Neck, and Head networks.
    Generates binary text segmentation maps for text detection tasks.
    """

    def __init__(self, config: PPOCRV5MobileDetConfig):
        """
        Initialize the PPOCRV5MobileDetModel with the specified configuration.

        Args:
            config (PPOCRV5MobileDetConfig): Configuration object containing all model hyperparameters.
        """
        super().__init__(config)

        self.backbone = PPOCRV5MobileDetBackbone(config)
        self.neck = PPOCRV5MobileDetNeck(config, self.backbone.out_channels)
        self.head = PPOCRV5MobileDetHead(config)

    def forward(
        self,
        hidden_state: torch.FloatTensor,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        **kwargs,
    ) -> Union[tuple[torch.FloatTensor], PPOCRV5MobileDetModelOutput]:
        """
        Forward pass of the PPOCRV5MobileDetModel, processing input images to generate segmentation logits.

        Args:
            hidden_state (torch.FloatTensor): Input image tensor of shape (B, 3, H, W) (pixel values).
            output_hidden_states (bool, optional): Whether to return all intermediate hidden states from the backbone.
                If None, uses the configuration's `output_hidden_states` value.
            return_dict (bool, optional): Whether to return a `PPOCRV5MobileDetModelOutput` object or a tuple.
                If None, uses the configuration's `use_return_dict` value.

        Returns:
            Union[tuple[torch.FloatTensor], PPOCRV5MobileDetModelOutput]: Model output containing segmentation logits,
                last hidden state, and optional hidden states.
        """
        hidden_state, last_hidden_state, all_hidden_states = self.backbone(hidden_state, output_hidden_states)
        hidden_state = self.neck(hidden_state)
        hidden_state = self.head(hidden_state)

        if not return_dict:
            output = (last_hidden_state,)
            if output_hidden_states:
                output += (all_hidden_states,)
            output += (hidden_state,)
            return output

        return PPOCRV5MobileDetModelOutput(
            logits=hidden_state,
            last_hidden_state=last_hidden_state,
            hidden_states=all_hidden_states if output_hidden_states else None,
        )


@dataclass
class PPOCRV5MobileDetForObjectDetectionOutput(BaseModelOutputWithNoAttention):
    """
    Output class for PPOCRV5MobileDetForObjectDetection. Extends BaseModelOutputWithNoAttention
    to include text segmentation logits.

    Args:
        logits (torch.FloatTensor, optional): Binary segmentation logits from the head network,
            shape (B, 1, H, W).
        shape (torch.FloatTensor, optional): Unused placeholder for consistency with object detection output formats.
        last_hidden_state (torch.FloatTensor, optional): Last hidden state from the backbone network.
        hidden_states (tuple[torch.FloatTensor], optional): Tuple of all intermediate hidden states from the backbone,
            if `output_hidden_states` is True.
    """

    logits: Optional[torch.FloatTensor] = None
    shape: Optional[torch.FloatTensor] = None


@auto_docstring(custom_intro="ObjectDetection for the PPOCRV5 Mobile Det model.")
class PPOCRV5MobileDetForObjectDetection(PPOCRV5MobileDetPreTrainedModel):
    """
    PPOCRV5 Mobile Det model for object (text) detection tasks. Wraps the core PPOCRV5MobileDetModel
    and returns outputs compatible with the Transformers object detection API.
    """

    def __init__(self, config: PPOCRV5MobileDetConfig):
        """
        Initialize the PPOCRV5MobileDetForObjectDetection with the specified configuration.

        Args:
            config (PPOCRV5MobileDetConfig): Configuration object containing all model hyperparameters.
        """
        super().__init__(config)
        self.model = PPOCRV5MobileDetModel(config)
        self.post_init()

    def forward(
        self,
        pixel_values: torch.FloatTensor,
        labels: Optional[list[dict]] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        **kwargs,
    ) -> Union[tuple[torch.FloatTensor], PPOCRV5MobileDetForObjectDetectionOutput]:
        """
        Forward pass of the PPOCRV5MobileDetForObjectDetection model, processing input images to generate
        text detection logits.

        Args:
            pixel_values (torch.FloatTensor): Input image tensor of shape (B, 3, H, W) (preprocessed pixel values).
            labels (list[dict], optional): Unused placeholder for training (object detection labels). Defaults to None.
            output_hidden_states (bool, optional): Whether to return all intermediate hidden states from the backbone.
                If None, uses the configuration's `output_hidden_states` value.
            return_dict (bool, optional): Whether to return a `PPOCRV5MobileDetForObjectDetectionOutput` object or a tuple.
                If None, uses the configuration's `use_return_dict` value.
            **kwargs: Additional unused keyword arguments for compatibility.

        Returns:
            Union[tuple[torch.FloatTensor], PPOCRV5MobileDetForObjectDetectionOutput]: Detection output containing
                segmentation logits, last hidden state, and optional hidden states.
        """
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        outputs = self.model(pixel_values, output_hidden_states=output_hidden_states, return_dict=return_dict)

        if not return_dict:
            output = (outputs[0],)
            if output_hidden_states:
                output += (outputs[1], outputs[2])
            else:
                output += (outputs[1],)

            return output

        return PPOCRV5MobileDetForObjectDetectionOutput(
            logits=outputs.logits,
            last_hidden_state=outputs.last_hidden_state,
            hidden_states=outputs.hidden_states if output_hidden_states else None,
        )


__all__ = ["PPOCRV5MobileDetForObjectDetection", "PPOCRV5MobileDetModel", "PPOCRV5MobileDetPreTrainedModel"]
