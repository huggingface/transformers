#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
#           This file was automatically generated from src/transformers/models/pp_ocrv5_mobile_det/modular_pp_ocrv5_mobile_det.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_pp_ocrv5_mobile_det.py file directly. One of our CI enforces this.
#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
import math
from typing import Optional, Union

import cv2
import numpy as np

from ...feature_extraction_utils import BatchFeature
from ...image_processing_utils import BaseImageProcessor
from ...image_transforms import flip_channel_order, resize, to_channel_dimension_format
from ...image_utils import (
    ChannelDimension,
    ImageInput,
    PILImageResampling,
    infer_channel_dimension_format,
    make_flat_list_of_images,
    to_numpy_array,
    valid_images,
    validate_preprocess_arguments,
)
from ...utils import auto_docstring, filter_out_non_signature_kwargs
from ...utils.generic import TensorType


def unclip(box, unclip_ratio):
    """
    Expands (dilates) a detected text bounding box to recover the full text region.

    Args:
        box (np.ndarray): Input contour of shape (N, 2), where N is the number of points.
        unclip_ratio (float): Expansion ratio, typically greater than 1.0.

    Returns:
        np.ndarray: Expanded contour of shape (M, 2).
    """
    box = np.array(box).reshape(-1, 2)

    area = cv2.contourArea(box)
    length = cv2.arcLength(box, True)
    if length == 0:
        return box
    distance = area * unclip_ratio / length

    points = np.concatenate([box, box[0:1]], axis=0)
    new_points = []

    for i in range(len(box)):
        p1 = points[i]
        p0 = points[i - 1]
        p2 = points[i + 1]

        def get_normal(pa, pb):
            direction = pb - pa
            norm = np.linalg.norm(direction)
            if norm == 0:
                return np.array([0, 0])
            return np.array([direction[1], -direction[0]]) / norm

        v1 = get_normal(p0, p1)
        v2 = get_normal(p1, p2)
        combined_v = v1 + v2
        cos_theta = np.dot(v1, v2)

        denom = 1 + cos_theta
        if denom < 1e-6:
            scale = distance
        else:
            scale = distance * np.sqrt(2 / denom)

        new_point = p1 + combined_v * (scale / (np.linalg.norm(combined_v) + 1e-6))
        new_points.append(new_point)

    return np.array(new_points, dtype=np.float32)


def get_mini_boxes(contour):
    """
    Computes the minimum-area bounding rectangle for a given contour and returns
    its four corners in a consistent order (top-left, bottom-left, bottom-right, top-right).

    Args:
        contour (np.ndarray): Input contour of shape (N, 1, 2).

    Returns:
        tuple:
            - box (list): List of four corner points in order.
            - sside (float): Length of the shorter side of the bounding rectangle.
    """
    bounding_box = cv2.minAreaRect(contour)
    points = sorted(cv2.boxPoints(bounding_box), key=lambda x: x[0])

    index_1, index_2, index_3, index_4 = 0, 1, 2, 3
    if points[1][1] > points[0][1]:
        index_1 = 0
        index_4 = 1
    else:
        index_1 = 1
        index_4 = 0
    if points[3][1] > points[2][1]:
        index_2 = 2
        index_3 = 3
    else:
        index_2 = 3
        index_3 = 2

    box = [points[index_1], points[index_2], points[index_3], points[index_4]]
    return box, min(bounding_box[1])


def box_score_fast(bitmap, _box):
    """
    Computes the mean score of a bounding box region in the prediction map using
    a fast approach with axis-aligned bounding boxes.

    Args:
        bitmap (np.ndarray): Binary or float prediction map of shape (H, W).
        _box (np.ndarray): Bounding box polygon of shape (N, 2).

    Returns:
        float: Mean score within the bounding box region.
    """
    h, w = bitmap.shape[:2]
    box = _box.copy()
    xmin = max(0, min(math.floor(box[:, 0].min()), w - 1))
    xmax = max(0, min(math.ceil(box[:, 0].max()), w - 1))
    ymin = max(0, min(math.floor(box[:, 1].min()), h - 1))
    ymax = max(0, min(math.ceil(box[:, 1].max()), h - 1))

    mask = np.zeros((ymax - ymin + 1, xmax - xmin + 1), dtype=np.uint8)
    box[:, 0] = box[:, 0] - xmin
    box[:, 1] = box[:, 1] - ymin
    cv2.fillPoly(mask, box.reshape(1, -1, 2).astype(np.int32), 1)
    return cv2.mean(bitmap[ymin : ymax + 1, xmin : xmax + 1], mask)[0]


def box_score_slow(bitmap, contour):
    """
    Computes the mean score of a contour region in the prediction map using
    the exact polygon shape, which is slower but more accurate.

    Args:
        bitmap (np.ndarray): Binary or float prediction map of shape (H, W).
        contour (np.ndarray): Contour polygon of shape (N, 2).

    Returns:
        float: Mean score within the contour region.
    """
    h, w = bitmap.shape[:2]
    contour = contour.copy()
    contour = np.reshape(contour, (-1, 2))

    xmin = np.clip(np.min(contour[:, 0]), 0, w - 1)
    xmax = np.clip(np.max(contour[:, 0]), 0, w - 1)
    ymin = np.clip(np.min(contour[:, 1]), 0, h - 1)
    ymax = np.clip(np.max(contour[:, 1]), 0, h - 1)

    mask = np.zeros((ymax - ymin + 1, xmax - xmin + 1), dtype=np.uint8)

    contour[:, 0] = contour[:, 0] - xmin
    contour[:, 1] = contour[:, 1] - ymin

    cv2.fillPoly(mask, contour.reshape(1, -1, 2).astype(np.int32), 1)
    return cv2.mean(bitmap[ymin : ymax + 1, xmin : xmax + 1], mask)[0]


def polygons_from_bitmap(
    pred,
    _bitmap,
    dest_width,
    dest_height,
    box_thresh,
    unclip_ratio,
    min_size,
    max_candidates,
):
    """
    Extracts text polygons from a binary segmentation map.

    Args:
        pred (np.ndarray): Raw prediction map of shape (H, W).
        _bitmap (np.ndarray): Binarized segmentation map of shape (H, W).
        dest_width (int): Original image width for scaling back.
        dest_height (int): Original image height for scaling back.
        box_thresh (float): Score threshold for filtering low-confidence boxes.
        unclip_ratio (float): Expansion ratio for contour unclipping.
        min_size (int): Minimum side length of valid boxes.
        max_candidates (int): Maximum number of contours to process.

    Returns:
        tuple:
            - boxes (list): List of polygons, each of shape (N, 2).
            - scores (list): List of corresponding scores.
    """

    bitmap = _bitmap
    height, width = bitmap.shape
    width_scale = dest_width / width
    height_scale = dest_height / height
    boxes = []
    scores = []

    contours, _ = cv2.findContours((bitmap * 255).astype(np.uint8), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)

    for contour in contours[:max_candidates]:
        epsilon = 0.002 * cv2.arcLength(contour, True)
        approx = cv2.approxPolyDP(contour, epsilon, True)
        points = approx.reshape((-1, 2))
        if points.shape[0] < 4:
            continue

        score = box_score_fast(pred, points.reshape(-1, 2))
        if box_thresh > score:
            continue

        if points.shape[0] > 2:
            box = unclip(points, unclip_ratio)
            if len(box) > 1:
                continue
        else:
            continue
        box = box.reshape(-1, 2)

        if len(box) > 0:
            _, sside = get_mini_boxes(box.reshape((-1, 1, 2)))
            if sside < min_size + 2:
                continue
        else:
            continue

        box = np.array(box)
        for i in range(box.shape[0]):
            box[i, 0] = max(0, min(round(box[i, 0] * width_scale), dest_width))
            box[i, 1] = max(0, min(round(box[i, 1] * height_scale), dest_height))

        boxes.append(box)
        scores.append(score)
    return boxes, scores


def boxes_from_bitmap(
    pred,
    _bitmap,
    dest_width,
    dest_height,
    box_thresh,
    unclip_ratio,
    min_size,
    max_candidates,
    score_mode,
):
    """
    Extracts axis-aligned or rotated bounding boxes from a binary segmentation map.

    Args:
        pred (np.ndarray): Raw prediction map of shape (H, W).
        _bitmap (np.ndarray): Binarized segmentation map of shape (H, W).
        dest_width (int): Original image width for scaling back.
        dest_height (int): Original image height for scaling back.
        box_thresh (float): Score threshold for filtering low-confidence boxes.
        unclip_ratio (float): Expansion ratio for contour unclipping.
        min_size (int): Minimum side length of valid boxes.
        max_candidates (int): Maximum number of contours to process.
        score_mode (str): Scoring mode, either "fast" or "slow".

    Returns:
        tuple:
            - boxes (np.ndarray): Array of boxes, each of shape (4, 2).
            - scores (list): List of corresponding scores.
    """

    bitmap = _bitmap
    height, width = bitmap.shape
    width_scale = dest_width / width
    height_scale = dest_height / height

    outs = cv2.findContours((bitmap * 255).astype(np.uint8), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)
    if len(outs) == 3:
        _, contours, _ = outs[0], outs[1], outs[2]
    elif len(outs) == 2:
        contours, _ = outs[0], outs[1]

    num_contours = min(len(contours), max_candidates)

    boxes = []
    scores = []
    for index in range(num_contours):
        contour = contours[index]
        points, sside = get_mini_boxes(contour)
        if sside < min_size:
            continue
        points = np.array(points)
        if score_mode == "fast":
            score = box_score_fast(pred, points.reshape(-1, 2))
        else:
            score = box_score_slow(pred, contour)
        if box_thresh > score:
            continue
        box = unclip(points, unclip_ratio).reshape(-1, 1, 2)
        box, sside = get_mini_boxes(box)
        if sside < min_size + 2:
            continue

        box = np.array(box)
        for i in range(box.shape[0]):
            box[i, 0] = max(0, min(round(box[i, 0] * width_scale), dest_width))
            box[i, 1] = max(0, min(round(box[i, 1] * height_scale), dest_height))

        boxes.append(box.astype(np.int16))
        scores.append(score)
    return np.array(boxes, dtype=np.int16), scores


def process(
    pred, size, thresh, box_type, box_thresh, unclip_ratio, use_dilation, min_size, max_candidates, score_mode
):
    """
    Main post-processing function to convert model predictions into text boxes.

    Args:
        pred (torch.Tensor): Model output of shape (1, H, W).
        size (torch.Tensor): Original image size (height, width).
        thresh (float): Threshold for binarizing the prediction map.
        box_type (str): Type of boxes to extract, either "quad" or "poly".
        box_thresh (float): Score threshold for filtering boxes.
        unclip_ratio (float): Expansion ratio for unclipping.
        use_dilation (bool): Whether to apply dilation on the segmentation mask.
        min_size (int): Minimum side length of valid boxes.
        max_candidates (int): Maximum number of boxes to extract.
        score_mode (str): Scoring mode, either "fast" or "slow".

    Returns:
        tuple:
            - boxes (list or np.ndarray): Extracted text boxes.
            - scores (list): Corresponding confidence scores.
    """
    pred = pred[0, :, :].cpu().detach().numpy()
    segmentation = pred > thresh
    dilation_kernel = None if not use_dilation else np.array([[1, 1], [1, 1]])
    src_h, src_w = size.cpu().detach().numpy()
    if dilation_kernel is not None:
        mask = cv2.dilate(
            np.array(segmentation).astype(np.uint8),
            dilation_kernel,
        )
    else:
        mask = segmentation
    if box_type == "poly":
        boxes, scores = polygons_from_bitmap(
            pred,
            mask,
            src_w,
            src_h,
            box_thresh,
            unclip_ratio,
            min_size,
            max_candidates,
        )
    elif box_type == "quad":
        boxes, scores = boxes_from_bitmap(
            pred, mask, src_w, src_h, box_thresh, unclip_ratio, min_size, max_candidates, score_mode
        )
    else:
        raise ValueError("box_type can only be one of ['quad', 'poly']")
    return boxes, scores


@auto_docstring(custom_intro="ImageProcessor for the PPOCRV5 Mobile Det model.")
class PPOCRV5MobileDetImageProcessor(BaseImageProcessor):
    model_input_names = ["pixel_values"]
    """
    Image Processor for the PPOCRV5 Mobile Det text detection model.

    This class handles all image preprocessing (resizing, rescaling, normalization, channel flipping)
    and post-processing (converting model logits to detected text boxes) required for the PPOCRV5 Mobile Det model.
    It ensures input images are formatted correctly for model inference and converts model outputs into human-interpretable
    text bounding boxes.

    Key features:
    - Aspect-ratio preserving image resizing with side length limits.
    - RGB to BGR channel flipping (compatible with PaddlePaddle's original model).
    - Standard image normalization and rescaling.
    - Post-processing to extract quadrilateral or polygonal text boxes from segmentation maps.

    Attributes:
        model_input_names (List[str]): List of input names expected by the model (only "pixel_values" for this processor).
        limit_side_len (int): Maximum/minimum side length for image resizing (depending on `limit_type`).
        limit_type (str): Resizing strategy ("max", "min", or "resize_long").
        max_side_limit (int): Hard maximum limit for the longest image side to prevent excessive memory usage.
        do_resize (bool): Whether to resize input images.
        size (dict[str, int]): Default target size for resizing (height, width).
        resample (PILImageResampling): Resampling mode for image resizing.
        do_rescale (bool): Whether to rescale pixel values from [0, 255] to [0, 1].
        rescale_factor (Union[int, float]): Factor used for pixel value rescaling (1/255 by default).
        do_normalize (bool): Whether to normalize images using mean and standard deviation.
        image_mean (Union[float, List[float]]): Mean values for image normalization (BGR order, compatible with model).
        image_std (Union[float, List[float]]): Standard deviation values for image normalization (BGR order).
    """

    def __init__(
        self,
        limit_side_len: int = 960,
        limit_type: str = "max",
        max_side_limit: int = 4000,
        do_resize: bool = True,
        size: Optional[dict[str, int]] = None,
        resample: Optional[PILImageResampling] = PILImageResampling.BICUBIC,
        do_rescale: bool = True,
        rescale_factor: Union[int, float] = 1 / 255,
        do_normalize: bool = True,
        image_mean: Optional[Union[float, list[float]]] = [0.406, 0.456, 0.485],
        image_std: Optional[Union[float, list[float]]] = [0.225, 0.224, 0.229],
        **kwargs,
    ) -> None:
        super().__init__(**kwargs)
        size = size if size is not None else {"height": 960, "width": 960}

        self.limit_side_len = limit_side_len
        self.limit_type = limit_type
        self.max_side_limit = max_side_limit

        self.do_resize = do_resize
        self.size = size
        self.do_rescale = do_rescale
        self.rescale_factor = rescale_factor
        self.do_normalize = do_normalize
        self.image_mean = image_mean
        self.image_std = image_std
        self.resample = resample

    @filter_out_non_signature_kwargs()
    def preprocess(
        self,
        images: ImageInput,
        limit_side_len: int = 960,
        limit_type: str = "max",
        max_side_limit: int = 4000,
        size: Optional[dict[str, int]] = None,
        do_resize: Optional[bool] = None,
        resample: Optional[PILImageResampling] = None,
        do_rescale: Optional[bool] = None,
        rescale_factor: Optional[Union[int, float]] = None,
        do_normalize: Optional[bool] = None,
        image_mean: Optional[Union[float, list[float]]] = None,
        image_std: Optional[Union[float, list[float]]] = None,
        return_tensors: Optional[Union[TensorType, str]] = None,
        data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,
        input_data_format: Optional[Union[str, ChannelDimension]] = None,
    ) -> BatchFeature:
        size = self.size if size is None else size
        limit_side_len = self.limit_side_len if limit_side_len is None else limit_side_len
        limit_type = self.limit_type if limit_type is None else limit_type
        max_side_limit = max_side_limit if max_side_limit is not None else self.max_side_limit
        do_resize = self.do_resize if do_resize is None else do_resize
        resample = self.resample if resample is None else resample
        do_rescale = self.do_rescale if do_rescale is None else do_rescale
        rescale_factor = self.rescale_factor if rescale_factor is None else rescale_factor
        do_normalize = self.do_normalize if do_normalize is None else do_normalize
        image_mean = self.image_mean if image_mean is None else image_mean
        image_std = self.image_std if image_std is None else image_std

        images = make_flat_list_of_images(images)

        validate_preprocess_arguments(
            do_rescale=do_rescale,
            rescale_factor=rescale_factor,
            do_normalize=do_normalize,
            image_mean=image_mean,
            image_std=image_std,
            size=size,
            do_resize=do_resize,
            resample=resample,
        )

        if not valid_images(images):
            raise ValueError("Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, or torch.Tensor")

        # All transformations expect numpy arrays
        images = [to_numpy_array(image) for image in images]

        if input_data_format is None:
            input_data_format = infer_channel_dimension_format(images[0])

        # transformations
        resize_imgs, target_sizes = [], []
        if do_resize:
            for image in images:
                size, shape = self.get_image_size(image, self.limit_side_len, self.limit_type, max_side_limit)
                try:
                    img = resize(
                        image,
                        size=(size["height"], size["width"]),
                        resample=resample,
                        input_data_format=input_data_format,
                    )
                except Exception as e:
                    print(size)
                    raise RuntimeError(f"Failed to resize image: {e}") from e

                resize_imgs.append(img)
                target_sizes.append(shape)
            images = resize_imgs

        if do_rescale:
            images = [self.rescale(image, rescale_factor, input_data_format=input_data_format) for image in images]

        if do_normalize:
            images = [
                self.normalize(image, image_mean, image_std, input_data_format=input_data_format) for image in images
            ]
        # flip color channels from RGB to BGR
        images = [flip_channel_order(image, input_data_format=input_data_format) for image in images]
        images = [
            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images
        ]

        encoded_inputs = BatchFeature(
            data={"pixel_values": images, "target_sizes": target_sizes}, tensor_type=return_tensors
        )

        return encoded_inputs

    def post_process_object_detection(
        self,
        preds,
        threshold: float = 0.5,
        target_sizes=None,
        thresh=0.3,
        box_thresh=0.6,
        max_candidates=1000,
        min_size=3,
        unclip_ratio=1.5,
        use_dilation=False,
        score_mode="fast",
        box_type="quad",
    ):
        """
        Converts model outputs into detected text boxes.

        Args:
            preds (torch.Tensor): Model outputs.
            threshold (float): Confidence threshold (unused).
            target_sizes (TensorType or list[tuple]): Original image sizes.
            thresh (float): Binarization threshold.
            box_thresh (float): Box score threshold.
            max_candidates (int): Maximum number of boxes.
            min_size (int): Minimum box size.
            unclip_ratio (float): Expansion ratio.
            use_dilation (bool): Whether to dilate the mask.
            score_mode (str): Scoring mode.
            box_type (str): Box type, "quad" or "poly".

        Returns:
            list[dict]: List of detection results.
        """
        assert score_mode in [
            "slow",
            "fast",
        ], f"Score mode must be in [slow, fast] but got: {score_mode}"
        return self.postprocess(
            preds=preds.logits,
            target_sizes=target_sizes,
            thresh=thresh,
            box_thresh=box_thresh,
            max_candidates=max_candidates,
            min_size=min_size,
            unclip_ratio=unclip_ratio,
            use_dilation=use_dilation,
            score_mode=score_mode,
            box_type=box_type,
        )

    def postprocess(
        self,
        preds,
        target_sizes,
        thresh=0.3,
        box_thresh=0.6,
        max_candidates=1000,
        min_size=3,
        unclip_ratio=1.5,
        use_dilation=False,
        score_mode="fast",
        box_type="quad",
    ):
        """
        Post-processes model outputs to extract text boxes.

        Args:
            preds (torch.Tensor): Model logits.
            thresh (float): Binarization threshold.
            target_sizes (list[tuple]): Original image sizes.
            box_thresh (float): Box score threshold.
            max_candidates (int): Maximum number of boxes.
            min_size (int): Minimum box size.
            unclip_ratio (float): Expansion ratio.
            use_dilation (bool): Whether to dilate the mask.
            score_mode (str): Scoring mode.
            box_type (str): Box type.

        Returns:
            list[dict]: List of detection results.
        """
        results = []
        for pred, size in zip(preds, target_sizes):
            box, score = process(
                pred=pred,
                size=size,
                thresh=thresh,
                box_type=box_type,
                box_thresh=box_thresh,
                unclip_ratio=unclip_ratio,
                use_dilation=use_dilation,
                min_size=min_size,
                max_candidates=max_candidates,
                score_mode=score_mode,
            )

            results.append({"scores": score, "boxes": box})
        return results

    def get_image_size(
        self,
        img: np.ndarray,
        limit_side_len: Union[int, None],
        limit_type: Union[str, None],
        max_side_limit: Union[int, None] = None,
    ) -> tuple[dict, np.ndarray]:
        """
        Computes the target size for resizing an image while preserving aspect ratio.

        Args:
            img (torch.Tensor): Input image.
            limit_side_len (int): Maximum or minimum side length.
            limit_type (str): Resizing strategy: "max", "min", or "resize_long".
            max_side_limit (int): Maximum allowed side length.

        Returns:
            tuple:
                - SizeDict: Target size.
                - torch.Tensor: Original size.
        """
        limit_side_len = limit_side_len or self.limit_side_len
        limit_type = limit_type or self.limit_type
        h, w, c = img.shape

        if limit_type == "max":
            if max(h, w) > limit_side_len:
                if h > w:
                    ratio = float(limit_side_len) / h
                else:
                    ratio = float(limit_side_len) / w
            else:
                ratio = 1.0
        elif limit_type == "min":
            if min(h, w) < limit_side_len:
                if h < w:
                    ratio = float(limit_side_len) / h
                else:
                    ratio = float(limit_side_len) / w
            else:
                ratio = 1.0
        elif limit_type == "resize_long":
            ratio = float(limit_side_len) / max(h, w)
        else:
            raise Exception("not support limit type, image ")
        resize_h = int(h * ratio)
        resize_w = int(w * ratio)

        if max(resize_h, resize_w) > max_side_limit:
            ratio = float(max_side_limit) / max(resize_h, resize_w)
            resize_h, resize_w = int(resize_h * ratio), int(resize_w * ratio)

        resize_h = max(int(round(resize_h / 32) * 32), 32)
        resize_w = max(int(round(resize_w / 32) * 32), 32)

        if resize_h == h and resize_w == w:
            return {"height": resize_h, "width": resize_w}, np.array([h, w])

        if int(resize_w) <= 0 or int(resize_h) <= 0:
            return None, (None, None)

        return {"height": resize_h, "width": resize_w}, np.array([h, w])


__all__ = ["PPOCRV5MobileDetImageProcessor"]
