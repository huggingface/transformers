#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
#           This file was automatically generated from src/transformers/models/llasa/modular_llasa.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_llasa.py file directly. One of our CI enforces this.
#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
from ...tokenization_utils_fast import PreTrainedTokenizerFast


# TODO use OrderedDict? Should only make difference for Python < 3.7
TTS_TOKENS_DICT = {
    "text_generation_start": "<|TEXT_GENERATION_START|>",
    "text_generation_end": "<|TEXT_GENERATION_END|>",
    "text_understanding_start": "<|TEXT_UNDERSTANDING_START|>",
    "text_understanding_end": "<|TEXT_UNDERSTANDING_END|>",
    "speech_generation_start": "<|SPEECH_GENERATION_START|>",
    "speech_generation_end": "<|SPEECH_GENERATION_END|>",
    "speech_understanding_start": "<|SPEECH_UNDERSTANDING_START|>",
    "speech_understanding_end": "<|SPEECH_UNDERSTANDING_END|>",
}


# Tokenizer for LLaSA as created in their training script:
# https://github.com/zhenye234/LLaSA_training/blob/main/train_tts.py#L214-L237
# - setting model_max_length, padding_side, pad_token, and more tokens (for speech and llasa)
# From `Llama-3.2-1B-Instruct` config, seems to be `PreTrainedTokenizerFast`:
# https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/blob/main/tokenizer_config.json#L2061
# Model max length:
# - config: https://github.com/zhenye234/LLaSA_training/blob/ef5c2605927190ba40656d09b3a9e10df6631149/config.json#L23
# - training script: https://github.com/zhenye234/LLaSA_training/blob/ef5c2605927190ba40656d09b3a9e10df6631149/train_tts.py#L216
# For codebook size, see https://github.com/zhenye234/LLaSA_training/blob/ef5c2605927190ba40656d09b3a9e10df6631149/train_tts.py#L235C58-L235C63
# and paper (Table 1, last row): https://arxiv.org/abs/2502.04128
# NOTE: would normally make sense to use `LlamaTokenizerFast` but it seems to add an extra token in original vocabulary
# -- moreover `Llama-3.2-1B-Instruct` uses that tokenizer class instead of `LlamaTokenizerFast`: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/blob/main/tokenizer_config.json#L2061
class LlasaTokenizer(PreTrainedTokenizerFast):
    padding_side = "right"

    def __init__(self, model_max_length=2048, codebook_size=65536, llasa_start_end_tokens=None, *args, **kwargs):
        super().__init__(model_max_length=model_max_length, *args, **kwargs)

        # TODO: correct way to overwrite variables?
        self.pad_token = self.eos_token

        # codebook indices
        if llasa_start_end_tokens is None:
            llasa_start_end_tokens = TTS_TOKENS_DICT
        self.llasa_token = llasa_start_end_tokens
        self.codebook_size = codebook_size
        self.speech_tokens = [f"<|s_{i}|>" for i in range(codebook_size)]

    @classmethod
    def from_pretrained_llm(cls, *args, **kwargs):
        """
        Load the tokenizer from a pre-trained LLM model, and add relevant speech and Llasa tokens.
        """
        tokenizer = super().from_pretrained(*args, **kwargs)
        tokenizer.add_tokens(list(tokenizer.llasa_token.values()) + tokenizer.speech_tokens)
        return tokenizer


__all__ = ["LlasaTokenizer"]
