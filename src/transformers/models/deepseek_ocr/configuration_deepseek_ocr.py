#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/deepseek_ocr/modular_deepseek_ocr.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_deepseek_ocr.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# Copyright 2025 Deepseek-AI and the HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from typing import Optional

from ...configuration_utils import PreTrainedConfig
from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params
from ..auto import CONFIG_MAPPING, AutoConfig


class DeepseekOcrSamConfig(PreTrainedConfig):
    model_type = "deepseek_ocr_sam_vision"
    base_config_key = "sam_config"

    def __init__(
        self,
        hidden_size=768,
        num_hidden_layers=12,
        num_attention_heads=12,
        num_channels=3,
        image_size=1024,
        patch_size=16,
        hidden_act="gelu",
        layer_norm_eps=1e-6,
        attention_dropout=0.0,
        initializer_range=1e-10,
        qkv_bias=True,
        use_abs_pos=True,
        use_rel_pos=True,
        window_size=14,
        global_attn_indexes=None,
        mlp_ratio=4.0,
        output_channels=256,
        downsample_channels=None,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.hidden_size = hidden_size
        self.num_hidden_layers = num_hidden_layers
        self.num_attention_heads = num_attention_heads
        self.num_channels = num_channels
        self.image_size = image_size
        self.patch_size = patch_size
        self.hidden_act = hidden_act
        self.layer_norm_eps = layer_norm_eps
        self.attention_dropout = attention_dropout
        self.initializer_range = initializer_range
        self.qkv_bias = qkv_bias
        self.use_abs_pos = use_abs_pos
        self.use_rel_pos = use_rel_pos
        self.window_size = window_size
        self.global_attn_indexes = global_attn_indexes if global_attn_indexes is not None else [2, 5, 8, 11]
        self.mlp_ratio = mlp_ratio
        self.output_channels = output_channels
        self.downsample_channels = downsample_channels if downsample_channels is not None else [512, 1024]
        self.mlp_dim = int(hidden_size * mlp_ratio)
        self.out_channels = output_channels


class DeepseekOcrCLIPVisionConfig(PreTrainedConfig):
    model_type = "deepseek_ocr_clip_vision"
    base_config_key = "clip_vision_config"

    def __init__(
        self,
        hidden_size=1024,
        intermediate_size=4096,
        projection_dim=768,
        num_hidden_layers=24,
        num_attention_heads=16,
        num_channels=3,
        image_size=224,
        patch_size=14,
        hidden_act="quick_gelu",
        layer_norm_eps=1e-5,
        attention_dropout=0.0,
        initializer_range=0.02,
        initializer_factor=1.0,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size
        self.projection_dim = projection_dim
        self.num_hidden_layers = num_hidden_layers
        self.num_attention_heads = num_attention_heads
        self.num_channels = num_channels
        self.image_size = image_size
        self.patch_size = patch_size
        self.hidden_act = hidden_act
        self.layer_norm_eps = layer_norm_eps
        self.attention_dropout = attention_dropout
        self.initializer_range = initializer_range
        self.initializer_factor = initializer_factor


class DeepseekOcrProjectorConfig(PreTrainedConfig):
    model_type = "deepseek_ocr_projector"
    base_config_key = "projector_config"

    def __init__(
        self,
        input_dim=2048,
        n_embed=1280,
        projector_type="linear",
        depth=1,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.input_dim = input_dim
        self.n_embed = n_embed
        self.projector_type = projector_type
        self.depth = depth


class DeepseekOcrVisionConfig(PreTrainedConfig):
    model_type = "deepseek_ocr_vision"
    base_config_key = "vision_config"
    sub_configs = {
        "sam_config": DeepseekOcrSamConfig,
        "clip_config": DeepseekOcrCLIPVisionConfig,
    }

    def __init__(self, sam_config=None, clip_config=None, **kwargs):
        super().__init__(**kwargs)

        if sam_config is None:
            self.sam_config = DeepseekOcrSamConfig()
        elif isinstance(sam_config, dict):
            self.sam_config = DeepseekOcrSamConfig(**sam_config)
        else:
            self.sam_config = sam_config

        if clip_config is None:
            self.clip_config = DeepseekOcrCLIPVisionConfig()
        elif isinstance(clip_config, dict):
            self.clip_config = DeepseekOcrCLIPVisionConfig(**clip_config)
        else:
            self.clip_config = clip_config

        # Aggregate commonly accessed vision attributes.
        self.image_size = self.sam_config.image_size
        self.patch_size = self.sam_config.patch_size


class DeepseekOcrTextConfig(PreTrainedConfig):
    r"""
    This is the configuration class to store the configuration of a [`DeepseekOcrTextModel`]. It is used to instantiate a DeepSeek
    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the
    defaults will yield a similar configuration to that of DeepSeek-V2-Lite" [deepseek-ai/DeepSeek-V2-Lite"](https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite").
    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the
    documentation from [`PreTrainedConfig`] for more information.

    Args:
        vocab_size (`int`, *optional*, defaults to 32000):
            Vocabulary size of the DeepSeek model. Defines the number of different tokens that can be represented by the
            `input_ids` passed when calling [`DeepseekOcrTextModel`].
        hidden_size (`int`, *optional*, defaults to 4096):
            Dimension of the hidden representations.
        intermediate_size (`int`, *optional*, defaults to 11008):
            Dimension of the MLP representations.
        num_hidden_layers (`int`, *optional*, defaults to 32):
            Number of hidden layers in the Transformer decoder.
        num_attention_heads (`int`, *optional*, defaults to 32):
            Number of attention heads for each attention layer in the Transformer decoder.
        num_key_value_heads (`int`, *optional*):
            The number of key-value heads used to implement Grouped Query Attention (GQA). If
            `num_key_value_heads=num_attention_heads`, the model will use Multi-Head Attention (MHA). If
            `num_key_value_heads=1`, the model will use Multi-Query Attention (MQA). Otherwise, GQA is used.
        hidden_act (`str` or `function`, *optional*, defaults to `"silu"`):
            The non-linear activation function (function or string) in the decoder.
        max_position_embeddings (`int`, *optional*, defaults to 2048):
            The maximum sequence length that this model might ever be used with.
        initializer_range (`float`, *optional*, defaults to 0.02):
            The standard deviation of the truncated normal initializer for initializing all weight matrices.
        rms_norm_eps (`float`, *optional*, defaults to 1e-06):
            The epsilon value used by the RMS normalization layers.
        use_cache (`bool`, *optional*, defaults to `True`):
            Whether or not the model should return the last key/value attentions (useful for inference optimization).
        pad_token_id (`int`, *optional*):
            Padding token ID.
        bos_token_id (`int`, *optional*, defaults to 1):
            Beginning-of-sequence token ID.
        eos_token_id (`int`, *optional*, defaults to 2):
            End-of-sequence token ID.
        tie_word_embeddings (`bool`, *optional*, defaults to `False`):
            Whether to tie input and output embeddings.
        rope_parameters (`RopeParameters`, *optional*):
            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain
            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE
            with longer `max_position_embeddings`.
        attention_bias (`bool`, *optional*, defaults to `False`):
            Whether to use a bias in the query, key, value, and output projection layers during self-attention.
        attention_dropout (`float`, *optional*, defaults to 0.0):
            The dropout probability applied to attention weights.
        mlp_bias (`bool`, *optional*, defaults to `False`):
            Whether to use a bias term in the MLP layers.
        first_k_dense_replace (`int`, *optional*, defaults to 0):
            Number of dense layers in the shallow layers before switching to MoE layers.
        kv_lora_rank (`int`, *optional*, defaults to 512):
            Rank of the LoRA decomposition for key-value projections.
        q_lora_rank (`int`, *optional*, defaults to 1536):
            Rank of the LoRA decomposition for query projections.
            Specifically, it determines the dimensionality to which the query (q) vectors are compressed before being expanded back to their original size.
            It reduces computational overhead while maintaining model performance.
        n_group (`int`, *optional*):
            Number of groups for routed experts.
        n_routed_experts (`int`, *optional*, defaults to 64):
            Number of routed experts (None indicates a dense model).
        n_shared_experts (`int`, *optional*, defaults to 2):
            Number of shared experts (None indicates a dense model).
        qk_nope_head_dim (`int`, *optional*, defaults to 128):
            The head dimension for the QK (query-key) projections when using NOPE (Neural Operator Position Encoding).
        qk_rope_head_dim (`int`, *optional*, defaults to 64):
            The head dimension for QK projections when using RoPE.
        routed_scaling_factor (`float`, *optional*, defaults to 1.0):
            Scaling factor for routed experts in MoE models.
        topk_group (`int`, *optional*):
            Number of selected groups per token for expert selection.
        topk_method (`str`, *optional*, defaults to `"greedy"`):
            The method used for selecting top-k experts in the routed gate mechanism.
        v_head_dim (`int`, *optional*, defaults to 128):
            The dimension of value projections in the attention layers.
        num_experts_per_tok (`int`, *optional*):
            The number of experts selected per token. If `None`, the model behaves as a dense Transformer.
        moe_intermediate_size (`int`, *optional*, defaults to 1407):
            Dimension of the MoE (Mixture of Experts) representations.

    ```python
    >>> from transformers import DeepseekOcrTextModel, DeepseekOcrTextConfig
    >>> # Initializing a DeepSeek-V2 style configuration
    >>> configuration = DeepseekOcrTextConfig()
    >>> # Accessing the model configuration
    >>> model = DeepseekOcrTextModel(configuration)
    >>> print(model.config)
    ```
    """

    model_type = "deepseek_ocr_text"
    keys_to_ignore_at_inference = ["past_key_values"]

    base_model_tp_plan = {
        "layers.*.self_attn.q_proj": "colwise",
        "layers.*.self_attn.q_a_proj": "colwise",
        "layers.*.self_attn.q_b_proj": "colwise",
        "layers.*.self_attn.kv_b_proj": "colwise",
        "layers.*.self_attn.o_proj": "rowwise",
        "layers.*.mlp.gate_proj": "colwise",
        "layers.*.mlp.up_proj": "colwise",
        "layers.*.mlp.down_proj": "rowwise",
    }
    base_model_pp_plan = {
        "embed_tokens": (["input_ids"], ["inputs_embeds"]),
        "layers": (["hidden_states", "attention_mask"], ["hidden_states"]),
        "norm": (["hidden_states"], ["hidden_states"]),
    }

    def __init__(
        self,
        vocab_size: Optional[int] = 32000,
        hidden_size: Optional[int] = 4096,
        intermediate_size: Optional[int] = 11008,
        num_hidden_layers: Optional[int] = 32,
        num_attention_heads: Optional[int] = 32,
        num_key_value_heads: Optional[int] = None,
        hidden_act: Optional[str] = "silu",
        max_position_embeddings: Optional[int] = 2048,
        initializer_range: Optional[float] = 0.02,
        rms_norm_eps: Optional[int] = 1e-6,
        use_cache: Optional[bool] = True,
        pad_token_id: Optional[int] = None,
        bos_token_id: Optional[int] = 1,
        eos_token_id: Optional[int] = 2,
        tie_word_embeddings: Optional[bool] = False,
        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,
        attention_bias: Optional[bool] = False,
        attention_dropout: Optional[float] = 0.0,
        mlp_bias: Optional[bool] = False,
        first_k_dense_replace: Optional[int] = 0,
        kv_lora_rank: Optional[int] = 512,
        q_lora_rank: Optional[int] = 1536,
        n_group: Optional[int] = None,
        n_routed_experts: Optional[int] = 64,
        n_shared_experts: Optional[int] = 2,
        qk_nope_head_dim: Optional[int] = 128,
        qk_rope_head_dim: Optional[int] = 64,
        routed_scaling_factor: Optional[float] = 1.0,
        topk_group: Optional[int] = None,
        topk_method: Optional[str] = "greedy",
        v_head_dim: Optional[int] = 128,
        num_experts_per_tok: Optional[int] = None,
        moe_intermediate_size: Optional[int] = 1407,
        **kwargs,
    ):
        self.first_k_dense_replace = first_k_dense_replace
        self.kv_lora_rank = kv_lora_rank
        self.q_lora_rank = q_lora_rank
        self.n_group = n_group
        self.n_routed_experts = n_routed_experts
        self.n_shared_experts = n_shared_experts
        self.qk_nope_head_dim = qk_nope_head_dim
        self.qk_rope_head_dim = qk_rope_head_dim
        self.routed_scaling_factor = routed_scaling_factor
        self.topk_group = topk_group
        self.topk_method = topk_method
        self.v_head_dim = v_head_dim
        self.num_experts_per_tok = num_experts_per_tok
        self.moe_intermediate_size = moe_intermediate_size
        self.vocab_size = vocab_size
        self.max_position_embeddings = max_position_embeddings
        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size
        self.num_hidden_layers = num_hidden_layers
        self.num_attention_heads = num_attention_heads

        # for backward compatibility
        if num_key_value_heads is None:
            num_key_value_heads = num_attention_heads

        self.num_key_value_heads = num_key_value_heads
        self.hidden_act = hidden_act
        self.initializer_range = initializer_range
        self.rms_norm_eps = rms_norm_eps
        self.use_cache = use_cache
        self.attention_bias = attention_bias
        self.attention_dropout = attention_dropout
        self.mlp_bias = mlp_bias

        self.head_dim = qk_rope_head_dim
        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`
        rope_scaling = kwargs.pop("rope_scaling", None)
        self.rope_parameters = rope_scaling or rope_parameters

        # Validate the correctness of rotary position embeddings parameters
        rope_theta = kwargs.get("rope_theta", 10000.0)
        standardize_rope_params(self, rope_theta=rope_theta)
        rope_config_validation(self)

        super().__init__(
            pad_token_id=pad_token_id,
            bos_token_id=bos_token_id,
            eos_token_id=eos_token_id,
            tie_word_embeddings=tie_word_embeddings,
            **kwargs,
        )


class DeepseekOcrConfig(PreTrainedConfig):
    r"""
    This is the configuration class to store the configuration of a [`DeepseekOcrForConditionalGeneration`]. It is used to instantiate a
    DeepseekOCR model according to the specified arguments, defining the model architecture. Instantiating a configuration
    with the defaults will yield a similar configuration to that of the DeepseekOCR
    [deepseek-ai/deepseek-ocr](https://huggingface.co/deepseek-ai/deepseek-ocr) architecture.

    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the
    documentation from [`PreTrainedConfig`] for more information.

    Args:
        text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `DeepseekV2Config`):
            The config object or dictionary of the text backbone (DeepSeek-V2).
        vision_config (`DeepseekOcrVisionConfig` or `dict`, *optional*):
            The config object or dictionary of the vision encoders (SAM and CLIP).
        projector_config (`DeepseekOcrProjectorConfig` or `dict`, *optional*):
            The config object or dictionary of the projector that maps vision features to text embedding space.
        candidate_resolutions (`list`, *optional*, defaults to `[[1024, 1024]]`):
            List of candidate image resolutions for adaptive image processing.
        global_view_pos (`str`, *optional*, defaults to `"head"`):
            Position of the global view in the image sequence.
        tile_tag (`str`, *optional*, defaults to `"2D"`):
            Tag format for image tiles.
        image_token_index (`int`, *optional*, defaults to 100015):
            The index representing image tokens in the model's token vocabulary.

    Example:

    ```python
    >>> from transformers import DeepseekOcrConfig, DeepseekOcrForConditionalGeneration

    >>> # Initializing a DeepseekOCR configuration
    >>> configuration = DeepseekOcrConfig()

    >>> # Initializing a model (with random weights) from the configuration
    >>> model = DeepseekOcrForConditionalGeneration(configuration)

    >>> # Accessing the model configuration
    >>> configuration = model.config
    ```"""

    model_type = "deepseek_ocr"
    sub_configs = {
        "text_config": AutoConfig,
        "vision_config": DeepseekOcrVisionConfig,
        "projector_config": DeepseekOcrProjectorConfig,
    }

    def __init__(
        self,
        text_config=None,
        vision_config=None,
        projector_config=None,
        candidate_resolutions=None,
        global_view_pos="head",
        tile_tag="2D",
        image_token_index=100015,
        image_grid_pinpoints=None,
        vision_feature_layer=None,
        vision_feature_select_strategy="default",
        **kwargs,
    ):
        if candidate_resolutions is None:
            candidate_resolutions = [[1024, 1024]]

        self.candidate_resolutions = candidate_resolutions
        self.global_view_pos = global_view_pos
        self.tile_tag = tile_tag
        self.image_token_index = image_token_index
        self.image_token_id = image_token_index
        self.image_grid_pinpoints = image_grid_pinpoints if image_grid_pinpoints is not None else [[1024, 1024]]
        self.vision_feature_layer = vision_feature_layer
        self.vision_feature_select_strategy = vision_feature_select_strategy

        if text_config is None:
            text_config = CONFIG_MAPPING["deepseek_v2"](
                hidden_size=1280,
                intermediate_size=6848,
                num_hidden_layers=12,
                num_attention_heads=10,
                num_key_value_heads=10,
                moe_intermediate_size=896,
                n_routed_experts=64,
                n_shared_experts=2,
                num_experts_per_tok=6,
                first_k_dense_replace=1,
                vocab_size=129280,
                max_position_embeddings=8192,
                use_mla=False,
            )
        elif isinstance(text_config, dict):
            text_config["model_type"] = text_config.get("model_type", "deepseek_v2")
            text_config = CONFIG_MAPPING[text_config["model_type"]](**text_config)

        self.text_config = text_config

        if vision_config is None:
            self.vision_config = DeepseekOcrVisionConfig()
        elif isinstance(vision_config, dict):
            self.vision_config = DeepseekOcrVisionConfig(**vision_config)
        else:
            self.vision_config = vision_config

        if projector_config is None:
            self.projector_config = DeepseekOcrProjectorConfig()
        elif isinstance(projector_config, dict):
            self.projector_config = DeepseekOcrProjectorConfig(**projector_config)
        else:
            self.projector_config = projector_config

        self.hidden_size = self.text_config.hidden_size
        self.vocab_size = self.text_config.vocab_size

        super().__init__(**kwargs)


__all__ = [
    "DeepseekOcrConfig",
    "DeepseekOcrVisionConfig",
    "DeepseekOcrSamConfig",
    "DeepseekOcrCLIPVisionConfig",
    "DeepseekOcrProjectorConfig",
]
