"""\nComprehensive Kafka Adapter for StreamingSentimentPipeline\n\nThis module provides a production-grade Kafka adapter implementation with:\n- Full Kafka consumer/producer integration using kafka-python\n- Multiple serialization format support (JSON, Avro, MessagePack)\n- Async message processing with batch handling\n- Consumer group management and offset tracking\n- Comprehensive error handling and resilience patterns\n- Performance optimization for high-throughput scenarios\n- Schema evolution and versioning support\n- Dead Letter Queue (DLQ) handling\n- Comprehensive monitoring and metrics\n\nArchitecture follows the patterns described in:\n- docs/streaming_architecture_design.md\n- docs/streaming_best_practices.md\n"""\n\nimport asyncio\nimport json\nimport logging\nimport time\nimport uuid\nfrom abc import abstractmethod\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timezone\nfrom typing import AsyncIterator, Dict, Any, List, Optional, Union, Callable, Set\nfrom enum import Enum\nfrom collections import defaultdict, deque\nimport hashlib\nimport pickle\nfrom contextlib import asynccontextmanager\nimport weakref\n\ntry:\n    from kafka import KafkaConsumer, KafkaProducer\n    from kafka.admin import KafkaAdminClient, ConfigResource, ConfigResourceType\n    from kafka.errors import KafkaError, KafkaTimeoutError, ConsumerStoppedError\n    from kafka.protocol.admin import DescribeConfigsRequest\n    from kafka.protocol.offset import OffsetResetStrategy\n    from kafka.structs import TopicPartition\n    KAFKA_AVAILABLE = True\nexcept ImportError:\n    KAFKA_AVAILABLE = False\n    # Mock classes for development without kafka-python\n    class KafkaConsumer:\n        def __init__(self, *args, **kwargs): pass\n        def subscribe(self, *args, **kwargs): pass\n        def poll(self, *args, **kwargs): return {}\n        def commit(self, *args, **kwargs): pass\n        def close(self, *args, **kwargs): pass\n        def pause(self, *args, **kwargs): pass\n        def resume(self, *args, **kwargs): pass\n        def assignment(self): return set()\n        def position(self, *args, **kwargs): return 0\n        def highwater(self, *args, **kwargs): return 0\n        def seek(self, *args, **kwargs): pass\n    class KafkaProducer: \n        def __init__(self, *args, **kwargs): pass\n        def send(self, *args, **kwargs): pass\n        def flush(self, *args, **kwargs): pass\n        def close(self, *args, **kwargs): pass\n    KafkaConsumer = type('MockKafkaConsumer', (), {})\n    KafkaProducer = type('MockKafkaProducer', (), {})\n    KafkaAdminClient = type('MockKafkaAdminClient', (), {})\n    KafkaError = Exception\n    ConsumerStoppedError = Exception\n\ntry:\n    import msgpack\n    MSGPACK_AVAILABLE = True\nexcept ImportError:\n    MSGPACK_AVAILABLE = False\n\n\n# =============================================================================\n# Configuration and Data Structures\n# =============================================================================\n\nclass SerializationFormat(Enum):\n    \"\"\"Supported serialization formats.\"\"\"\n    JSON = \"json\"\n    MSGPACK = \"msgpack\"\n    AVRO = \"avro\"\n    PROTOBUF = \"protobuf\"\n    RAW = \"raw\"\n\n\nclass KafkaSecurityProtocol(Enum):\n    \"\"\"Kafka security protocols.\"\"\"\n    PLAINTEXT = \"PLAINTEXT\"\n    SSL = \"SSL\"\n    SASL_PLAINTEXT = \"SASL_PLAINTEXT\"\n    SASL_SSL = \"SASL_SSL\"\n\n\nclass AckStrategy(Enum):\n    \"\"\"Acknowledgment strategies.\"\"\"\n    AUTO = \"auto\"\n    MANUAL = \"manual\"\n    TRANSACTIONAL = \"transactional\"\n\n\n@dataclass\nclass KafkaMessage:\n    \"\"\"Canonical Kafka message representation.\"\"\"\n    # Core message data\n    value: Union[str, bytes, Dict[str, Any]]\n    key: Optional[Union[str, bytes]] = None\n    topic: str = \"\"\n    partition: int = -1\n    offset: int = -1\n    timestamp: Optional[float] = None\n    \n    # Metadata\n    message_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    headers: Dict[str, Union[str, bytes]] = field(default_factory=dict)\n    compression: Optional[str] = None\n    \n    # Processing metadata\n    retry_count: int = 0\n    idempotency_key: Optional[str] = None\n    dlq: bool = False\n    \n    # Schema and versioning\n    schema_version: str = \"1.0\"\n    serialization_format: SerializationFormat = SerializationFormat.JSON\n    \n    def __post_init__(self):\n        if self.timestamp is None:\n            self.timestamp = time.time()\n        if self.idempotency_key is None:\n            self.idempotency_key = self._generate_idempotency_key()\n    \n    def _generate_idempotency_key(self) -> str:\n        \"\"\"Generate idempotency key based on message content.\"\"\"\n        content = f\"{self.topic}:{self.partition}:{self.offset}:{self.value}\"\n        return hashlib.sha256(content.encode()).hexdigest()[:16]\n    \n    def to_text(self) -> str:\n        \"\"\"Extract text content for sentiment analysis.\"\"\"\n        if isinstance(self.value, str):\n            return self.value\n        elif isinstance(self.value, bytes):\n            try:\n                return self.value.decode('utf-8')\n            except UnicodeDecodeError:\n                return self.value.decode('utf-8', errors='ignore')\n        elif isinstance(self.value, dict):\n            # Try common text fields\n            for field in ['text', 'message', 'content', 'body', 'data']:\n                if field in self.value and isinstance(self.value[field], str):\n                    return self.value[field]\n            # Fallback to JSON string\n            return json.dumps(self.value)\n        else:\n            return str(self.value)