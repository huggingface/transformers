<!---
Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

<p align="center">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg">
    <source media="(prefers-color-scheme: light)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg">
    <img alt="Biblioth√®que Hugging Face Transformers" src="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg" width="352" height="59" style="max-width: 100%;">
  </picture>
  <br/>
  <br/>
</p>

<p align="center">
    <a href="https://circleci.com/gh/huggingface/transformers"><img alt="Build" src="https://img.shields.io/circleci/build/github/huggingface/transformers/main"></a>
    <a href="https://github.com/huggingface/transformers/blob/main/LICENSE"><img alt="GitHub" src="https://img.shields.io/github/license/huggingface/transformers.svg?color=blue"></a>
    <a href="https://huggingface.co/docs/transformers/index"><img alt="Documentation" src="https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online"></a>
    <a href="https://github.com/huggingface/transformers/releases"><img alt="GitHub release" src="https://img.shields.io/github/release/huggingface/transformers.svg"></a>
    <a href="https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md"><img alt="Contributor Covenant" src="https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg"></a>
    <a href="https://zenodo.org/badge/latestdoi/155220641"><img src="https://zenodo.org/badge/155220641.svg" alt="DOI"></a>
</p>

<h4 align="center">
    <p>
        <a href="https://github.com/huggingface/transformers/">English</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hans.md">ÁÆÄ‰Ωì‰∏≠Êñá</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hant.md">ÁπÅÈ´î‰∏≠Êñá</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ko.md">ÌïúÍµ≠Ïñ¥</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_es.md">Espa√±ol</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ja.md">Êó•Êú¨Ë™û</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_hd.md">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ru.md">–†—É—Å—Å–∫–∏–π</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md">–†ortugu√™s</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_te.md">‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å</a> |
        <b>Fran√ßais</b> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_de.md">Deutsch</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_vi.md">Ti·∫øng Vi·ªát</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ar.md">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ur.md">ÿßÿ±ÿØŸà</a> |
    </p>
</h4>

<h3 align="center">
    <p>Apprentissage automatique de pointe pour JAX, PyTorch et TensorFlow</p>
</h3>

<h3 align="center">
    <a href="https://hf.co/course"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png"></a>
</h3>

ü§ó Transformers fournit des milliers de mod√®les pr√©-entra√Æn√©s pour effectuer des t√¢ches sur diff√©rentes modalit√©s telles que le texte, la vision et l'audio.

Ces mod√®les peuvent √™tre appliqu√©s √† :

* üìù Texte, pour des t√¢ches telles que la classification de texte, l'extraction d'informations, la r√©ponse aux questions, le r√©sum√©, la traduction et la g√©n√©ration de texte, dans plus de 100 langues.
* üñºÔ∏è Images, pour des t√¢ches telles que la classification d'images, la d√©tection d'objets et la segmentation.
* üó£Ô∏è Audio, pour des t√¢ches telles que la reconnaissance vocale et la classification audio.

Les mod√®les de transformer peuvent √©galement effectuer des t√¢ches sur **plusieurs modalit√©s combin√©es**, telles que la r√©ponse aux questions sur des tableaux, la reconnaissance optique de caract√®res, l'extraction d'informations √† partir de documents num√©ris√©s, la classification vid√©o et la r√©ponse aux questions visuelles.

ü§ó Transformers fournit des API pour t√©l√©charger et utiliser rapidement ces mod√®les pr√©-entra√Æn√©s sur un texte donn√©, les affiner sur vos propres ensembles de donn√©es, puis les partager avec la communaut√© sur notre [hub de mod√®les](https://huggingface.co/models). En m√™me temps, chaque module Python d√©finissant une architecture est compl√®tement ind√©pendant et peut √™tre modifi√© pour permettre des exp√©riences de recherche rapides.

ü§ó Transformers est soutenu par les trois biblioth√®ques d'apprentissage profond les plus populaires ‚Äî [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) et [TensorFlow](https://www.tensorflow.org/) ‚Äî avec une int√©gration transparente entre eux. Il est facile de former vos mod√®les avec l'un avant de les charger pour l'inf√©rence avec l'autre.

## D√©mos en ligne

Vous pouvez tester la plupart de nos mod√®les directement sur leurs pages du [hub de mod√®les](https://huggingface.co/models). Nous proposons √©galement [l'h√©bergement priv√© de mod√®les, le versionning et une API d'inf√©rence](https://huggingface.co/pricing) pour des mod√®les publics et priv√©s.

Voici quelques exemples :

En traitement du langage naturel :
- [Compl√©tion de mots masqu√©s avec BERT](https://huggingface.co/google-bert/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France)
- [Reconnaissance d'entit√©s nomm√©es avec Electra](https://huggingface.co/dbmdz/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city)
- [G√©n√©ration de texte avec GPT-2](https://huggingface.co/openai-community/gpt2?text=A+long+time+ago%2C+)
- [Inf√©rence de langage naturel avec RoBERTa](https://huggingface.co/FacebookAI/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal)
- [R√©sum√© avec BART](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+was+the+first+structure+to+reach+a+height+of+300+metres.+Due+to+the+addition+of+a+broadcasting+aerial+at+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct)
- [R√©ponse aux questions avec DistilBERT](https://huggingface.co/distilbert/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species)
- [Traduction avec T5](https://huggingface.co/google-t5/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)

En vision par ordinateur :
- [Classification d'images avec ViT](https://huggingface.co/google/vit-base-patch16-224)
- [D√©tection d'objets avec DETR](https://huggingface.co/facebook/detr-resnet-50)
- [Segmentation s√©mantique avec SegFormer](https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512)
- [Segmentation panoptique avec MaskFormer](https://huggingface.co/facebook/maskformer-swin-small-coco)
- [Estimation de profondeur avec DPT](https://huggingface.co/docs/transformers/model_doc/dpt)
- [Classification vid√©o avec VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae)
- [Segmentation universelle avec OneFormer](https://huggingface.co/shi-labs/oneformer_ade20k_dinat_large)

En audio :
- [Reconnaissance automatique de la parole avec Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base-960h)
- [Spotting de mots-cl√©s avec Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)
- [Classification audio avec Audio Spectrogram Transformer](https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593)

Dans les t√¢ches multimodales :
- [R√©ponses aux questions sur table avec TAPAS](https://huggingface.co/google/tapas-base-finetuned-wtq)
- [R√©ponses aux questions visuelles avec ViLT](https://huggingface.co/dandelin/vilt-b32-finetuned-vqa)
- [Classification d'images sans √©tiquette avec CLIP](https://huggingface.co/openai/clip-vit-large-patch14)
- [R√©ponses aux questions sur les documents avec LayoutLM](https://huggingface.co/impira/layoutlm-document-qa)
- [Classification vid√©o sans √©tiquette avec X-CLIP](https://huggingface.co/docs/transformers/model_doc/xclip)


## 100 projets utilisant Transformers

Transformers est plus qu'une bo√Æte √† outils pour utiliser des mod√®les pr√©-entra√Æn√©s : c'est une communaut√© de projets construits autour de lui et du Hub Hugging Face. Nous voulons que Transformers permette aux d√©veloppeurs, chercheurs, √©tudiants, professeurs, ing√©nieurs et √† quiconque d'imaginer et de r√©aliser leurs projets de r√™ve.

Afin de c√©l√©brer les 100 000 √©toiles de transformers, nous avons d√©cid√© de mettre en avant la communaut√© et avons cr√©√© la page [awesome-transformers](./awesome-transformers.md) qui r√©pertorie 100 projets incroyables construits autour de transformers.

Si vous poss√©dez ou utilisez un projet que vous pensez devoir figurer dans la liste, veuillez ouvrir une pull request pour l'ajouter !

## Si vous recherchez un support personnalis√© de la part de l'√©quipe Hugging Face

<a target="_blank" href="https://huggingface.co/support">
    <img alt="Programme d'acc√©l√©ration des experts HuggingFace" src="https://cdn-media.huggingface.co/marketing/transformers/new-support-improved.png" style="max-width: 600px; border: 1px solid #eee; border-radius: 4px; box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);">
</a><br>

## Tour rapide

Pour utiliser imm√©diatement un mod√®le sur une entr√©e donn√©e (texte, image, audio,...), nous fournissons l'API `pipeline`. Les pipelines regroupent un mod√®le pr√©-entra√Æn√© avec la pr√©paration des donn√©es qui a √©t√© utilis√©e lors de l'entra√Ænement de ce mod√®le. Voici comment utiliser rapidement un pipeline pour classer des textes en positif ou n√©gatif :

```python
>>> from transformers import pipeline

# Allouer un pipeline pour l'analyse de sentiment
>>> classifieur = pipeline('sentiment-analysis')
>>> classifieur("Nous sommes tr√®s heureux d'introduire le pipeline dans le r√©f√©rentiel transformers.")
[{'label': 'POSITIF', 'score': 0.9996980428695679}]
```

La deuxi√®me ligne de code t√©l√©charge et met en cache le mod√®le pr√©-entra√Æn√© utilis√© par le pipeline, tandis que la troisi√®me l'√©value sur le texte donn√©. Ici, la r√©ponse est "positive" avec une confiance de 99,97%.

De nombreuses t√¢ches ont une pipeline pr√©-entra√Æn√© pr√™t √† l'emploi, en NLP, mais aussi en vision par ordinateur et en parole. Par exemple, nous pouvons facilement extraire les objets d√©tect√©s dans une image :

```python
>>> import requests
>>> from PIL import Image
>>> from transformers import pipeline

# T√©l√©charger une image avec de jolis chats
>>> url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png"
>>> donnees_image = requests.get(url, stream=True).raw
>>> image = Image.open(donnees_image)

# Allouer un pipeline pour la d√©tection d'objets
>>> detecteur_objets = pipeline('object-detection')
>>> detecteur_objets(image)
[{'score': 0.9982201457023621,
  'label': 't√©l√©commande',
  'box': {'xmin': 40, 'ymin': 70, 'xmax': 175, 'ymax': 117}},
 {'score': 0.9960021376609802,
  'label': 't√©l√©commande',
  'box': {'xmin': 333, 'ymin': 72, 'xmax': 368, 'ymax': 187}},
 {'score': 0.9954745173454285,
  'label': 'canap√©',
  'box': {'xmin': 0, 'ymin': 1, 'xmax': 639, 'ymax': 473}},
 {'score': 0.9988006353378296,
  'label': 'chat',
  'box': {'xmin': 13, 'ymin': 52, 'xmax': 314, 'ymax': 470}},
 {'score': 0.9986783862113953,
  'label': 'chat',
  'box': {'xmin': 345, 'ymin': 23, 'xmax': 640, 'ymax': 368}}]
```

Ici, nous obtenons une liste d'objets d√©tect√©s dans l'image, avec une bo√Æte entourant l'objet et un score de confiance. Voici l'image originale √† gauche, avec les pr√©dictions affich√©es √† droite :

<h3 align="center">
    <a><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png" width="400"></a>
    <a><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample_post_processed.png" width="400"></a>
</h3>

Vous pouvez en savoir plus sur les t√¢ches support√©es par l'API pipeline dans [ce tutoriel](https://huggingface.co/docs/transformers/task_summary).

En plus de `pipeline`, pour t√©l√©charger et utiliser n'importe lequel des mod√®les pr√©-entra√Æn√©s sur votre t√¢che donn√©e, il suffit de trois lignes de code. Voici la version PyTorch :

```python
>>> from transformers import AutoTokenizer, AutoModel

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
>>> model = AutoModel.from_pretrained("google-bert/bert-base-uncased")

inputs = tokenizer("Bonjour le monde !", return_tensors="pt")
outputs = model(**inputs)
```

Et voici le code √©quivalent pour TensorFlow :

```python
from transformers import AutoTokenizer, TFAutoModel

tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
model = TFAutoModel.from_pretrained("google-bert/bert-base-uncased")

inputs = tokenizer("Bonjour le monde !", return_tensors="tf")
outputs = model(**inputs)
```

Le tokenizer est responsable de toutes les √©tapes de pr√©traitement que le mod√®le pr√©entra√Æn√© attend et peut √™tre appel√© directement sur une seule cha√Æne de caract√®res (comme dans les exemples ci-dessus) ou sur une liste. Il produira un dictionnaire que vous pouvez utiliser dans votre code ou simplement passer directement √† votre mod√®le en utilisant l'op√©rateur de d√©ballage **.

Le mod√®le lui-m√™me est un module [`nn.Module` PyTorch](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) ou un mod√®le [`tf.keras.Model` TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/Model) (selon votre backend) que vous pouvez utiliser comme d'habitude. [Ce tutoriel](https://huggingface.co/docs/transformers/training) explique comment int√©grer un tel mod√®le dans une boucle d'entra√Ænement classique PyTorch ou TensorFlow, ou comment utiliser notre API `Trainer` pour affiner rapidement sur un nouvel ensemble de donn√©es.

## Pourquoi devrais-je utiliser transformers ?

1. Des mod√®les de pointe faciles √† utiliser :
    - Hautes performances en compr√©hension et g√©n√©ration de langage naturel, en vision par ordinateur et en t√¢ches audio.
    - Faible barri√®re √† l'entr√©e pour les √©ducateurs et les praticiens.
    - Peu d'abstractions visibles pour l'utilisateur avec seulement trois classes √† apprendre.
    - Une API unifi√©e pour utiliser tous nos mod√®les pr√©entra√Æn√©s.

1. Co√ªts informatiques r√©duits, empreinte carbone plus petite :
    - Les chercheurs peuvent partager des mod√®les entra√Æn√©s au lieu de toujours les r√©entra√Æner.
    - Les praticiens peuvent r√©duire le temps de calcul et les co√ªts de production.
    - Des dizaines d'architectures avec plus de 400 000 mod√®les pr√©entra√Æn√©s dans toutes les modalit√©s.

1. Choisissez le bon framework pour chaque partie de la vie d'un mod√®le :
    - Entra√Ænez des mod√®les de pointe en 3 lignes de code.
    - Transf√©rer un seul mod√®le entre les frameworks TF2.0/PyTorch/JAX √† volont√©.
    - Choisissez facilement le bon framework pour l'entra√Ænement, l'√©valuation et la production.

1. Personnalisez facilement un mod√®le ou un exemple selon vos besoins :
    - Nous fournissons des exemples pour chaque architecture afin de reproduire les r√©sultats publi√©s par ses auteurs originaux.
    - Les d√©tails internes du mod√®le sont expos√©s de mani√®re aussi coh√©rente que possible.
    - Les fichiers de mod√®le peuvent √™tre utilis√©s ind√©pendamment de la biblioth√®que pour des exp√©riences rapides.

## Pourquoi ne devrais-je pas utiliser transformers ?

- Cette biblioth√®que n'est pas une bo√Æte √† outils modulaire de blocs de construction pour les r√©seaux neuronaux. Le code dans les fichiers de mod√®le n'est pas refactored avec des abstractions suppl√©mentaires √† dessein, afin que les chercheurs puissent it√©rer rapidement sur chacun des mod√®les sans plonger dans des abstractions/fichiers suppl√©mentaires.
- L'API d'entra√Ænement n'est pas destin√©e √† fonctionner avec n'importe quel mod√®le, mais elle est optimis√©e pour fonctionner avec les mod√®les fournis par la biblioth√®que. Pour des boucles g√©n√©riques d'apprentissage automatique, vous devriez utiliser une autre biblioth√®que (√©ventuellement, [Accelerate](https://huggingface.co/docs/accelerate)).
- Bien que nous nous efforcions de pr√©senter autant de cas d'utilisation que possible, les scripts de notre [dossier d'exemples](https://github.com/huggingface/transformers/tree/main/examples) ne sont que cela : des exemples. Il est pr√©vu qu'ils ne fonctionnent pas imm√©diatement sur votre probl√®me sp√©cifique et que vous devrez probablement modifier quelques lignes de code pour les adapter √† vos besoins.

## Installation

### Avec pip

Ce r√©f√©rentiel est test√© sur Python 3.9+, Flax 0.4.1+, PyTorch 2.0+ et TensorFlow 2.6+.

Vous devriez installer ü§ó Transformers dans un [environnement virtuel](https://docs.python.org/3/library/venv.html). Si vous n'√™tes pas familier avec les environnements virtuels Python, consultez le [guide utilisateur](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).

D'abord, cr√©ez un environnement virtuel avec la version de Python que vous allez utiliser et activez-le.

Ensuite, vous devrez installer au moins l'un de Flax, PyTorch ou TensorFlow.
Veuillez vous r√©f√©rer √† la page d'installation de [TensorFlow](https://www.tensorflow.org/install/), de [PyTorch](https://pytorch.org/get-started/locally/#start-locally) et/ou de [Flax](https://github.com/google/flax#quick-install) et [Jax](https://github.com/google/jax#installation) pour conna√Ætre la commande d'installation sp√©cifique √† votre plateforme.

Lorsqu'un de ces backends est install√©, ü§ó Transformers peut √™tre install√© avec pip comme suit :

```bash
pip install transformers
```

Si vous souhaitez jouer avec les exemples ou avez besoin de la derni√®re version du code et ne pouvez pas attendre une nouvelle version, vous devez [installer la biblioth√®que √† partir de la source](https://huggingface.co/docs/transformers/installation#installing-from-source).

### Avec conda

ü§ó Transformers peut √™tre install√© avec conda comme suit :

```shell
conda install conda-forge::transformers
```

> **_NOTE:_** L'installation de `transformers` depuis le canal `huggingface` est obsol√®te.

Suivez les pages d'installation de Flax, PyTorch ou TensorFlow pour voir comment les installer avec conda.

> **_NOTE:_** Sur Windows, on peut vous demander d'activer le mode d√©veloppeur pour b√©n√©ficier de la mise en cache. Si ce n'est pas une option pour vous, veuillez nous le faire savoir dans [cette issue](https://github.com/huggingface/huggingface_hub/issues/1062).

## Architectures de mod√®les

**[Tous les points de contr√¥le](https://huggingface.co/models)** de mod√®le fournis par ü§ó Transformers sont int√©gr√©s de mani√®re transparente depuis le [hub de mod√®les](https://huggingface.co/models) huggingface.co, o√π ils sont t√©l√©charg√©s directement par les [utilisateurs](https://huggingface.co/users) et les [organisations](https://huggingface.co/organizations).

Nombre actuel de points de contr√¥le : ![](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen)

ü§ó Transformers fournit actuellement les architectures suivantes: consultez [ici](https://huggingface.co/docs/transformers/model_summary) pour un r√©sum√© global de chacune d'entre elles.

Pour v√©rifier si chaque mod√®le a une impl√©mentation en Flax, PyTorch ou TensorFlow, ou s'il a un tokenizer associ√© pris en charge par la biblioth√®que ü§ó Tokenizers, consultez [ce tableau](https://huggingface.co/docs/transformers/index#supported-frameworks).

Ces impl√©mentations ont √©t√© test√©es sur plusieurs ensembles de donn√©es (voir les scripts d'exemple) et devraient correspondre aux performances des impl√©mentations originales. Vous pouvez trouver plus de d√©tails sur les performances dans la section Exemples de la [documentation](https://github.com/huggingface/transformers/tree/main/examples).

## En savoir plus

| Section | Description |
|-|-|
| [Documentation](https://huggingface.co/docs/transformers/) | Documentation compl√®te de l'API et tutoriels |
| [R√©sum√© des t√¢ches](https://huggingface.co/docs/transformers/task_summary) | T√¢ches prises en charge par les ü§ó Transformers |
| [Tutoriel de pr√©traitement](https://huggingface.co/docs/transformers/preprocessing) | Utilisation de la classe `Tokenizer` pour pr√©parer les donn√©es pour les mod√®les |
| [Entra√Ænement et ajustement fin](https://huggingface.co/docs/transformers/training) | Utilisation des mod√®les fournis par les ü§ó Transformers dans une boucle d'entra√Ænement PyTorch/TensorFlow et de l'API `Trainer` |
| [Tour rapide : Scripts d'ajustement fin/d'utilisation](https://github.com/huggingface/transformers/tree/main/examples) | Scripts d'exemple pour ajuster finement les mod√®les sur une large gamme de t√¢ches |
| [Partage et t√©l√©versement de mod√®les](https://huggingface.co/docs/transformers/model_sharing) | T√©l√©chargez et partagez vos mod√®les ajust√©s avec la communaut√© |

## Citation

Nous disposons d√©sormais d'un [article](https://www.aclweb.org/anthology/2020.emnlp-demos.6/) que vous pouvez citer pour la biblioth√®que ü§ó Transformers :
```bibtex
@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R√©mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}
```
