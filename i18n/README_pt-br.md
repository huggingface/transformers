<!---
Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

<p align="center">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg">
    <source media="(prefers-color-scheme: light)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg">
    <img alt="Hugging Face Transformers Library" src="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg" width="352" height="59" style="max-width: 100%;">
  </picture>
  <br/>
  <br/>
</p>

<p align="center">
    <a href="https://circleci.com/gh/huggingface/transformers"><img alt="Build" src="https://img.shields.io/circleci/build/github/huggingface/transformers/main"></a>
    <a href="https://github.com/huggingface/transformers/blob/main/LICENSE"><img alt="GitHub" src="https://img.shields.io/github/license/huggingface/transformers.svg?color=blue"></a>
    <a href="https://huggingface.co/docs/transformers/index"><img alt="Documentation" src="https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online"></a>
    <a href="https://github.com/huggingface/transformers/releases"><img alt="GitHub release" src="https://img.shields.io/github/release/huggingface/transformers.svg"></a>
    <a href="https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md"><img alt="Contributor Covenant" src="https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg"></a>
    <a href="https://zenodo.org/badge/latestdoi/155220641"><img src="https://zenodo.org/badge/155220641.svg" alt="DOI"></a>
</p>

<h4 align="center">
    <p>
        <a href="https://github.com/huggingface/transformers/">English</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hans.md">ÁÆÄ‰Ωì‰∏≠Êñá</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hant.md">ÁπÅÈ´î‰∏≠Êñá</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ko.md">ÌïúÍµ≠Ïñ¥</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_es.md">Espa√±ol</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ja.md">Êó•Êú¨Ë™û</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_hd.md">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ru.md">–†—É—Å—Å–∫–∏–π</a> |
        <b>–†ortugu√™s</b> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_te.md">‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_fr.md">Fran√ßais</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_de.md">Deutsch</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_vi.md">Ti·∫øng Vi·ªát</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ar.md">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ur.md">ÿßÿ±ÿØŸà</a> |
    </p>
</h4>

<h3 align="center">
    <p>Aprendizado de m√°quina de √∫ltima gera√ß√£o para JAX, PyTorch e TensorFlow</p>
</h3>

<h3 align="center">
    <a href="https://hf.co/course"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png"></a>
</h3>


A biblioteca ü§ó Transformers oferece milhares de modelos pr√©-treinados para executar tarefas em diferentes modalidades, como texto, vis√£o e √°udio.

Esses modelos podem ser aplicados a:

* üìù Texto, para tarefas como classifica√ß√£o de texto, extra√ß√£o de informa√ß√µes, resposta a perguntas, sumariza√ß√£o, tradu√ß√£o, gera√ß√£o de texto, em mais de 100 idiomas.
* üñºÔ∏è Imagens, para tarefas como classifica√ß√£o de imagens, detec√ß√£o de objetos e segmenta√ß√£o.
* üó£Ô∏è √Åudio, para tarefas como reconhecimento de fala e classifica√ß√£o de √°udio.

Os modelos Transformer tamb√©m podem executar tarefas em diversas modalidades combinadas, como responder a perguntas em tabelas, reconhecimento √≥ptico de caracteres, extra√ß√£o de informa√ß√µes de documentos digitalizados, classifica√ß√£o de v√≠deo e resposta a perguntas visuais.


A biblioteca ü§ó Transformers oferece APIs para baixar e usar rapidamente esses modelos pr√©-treinados em um texto espec√≠fico, ajust√°-los em seus pr√≥prios conjuntos de dados e, em seguida, compartilh√°-los com a comunidade em nosso [model hub](https://huggingface.co/models). Ao mesmo tempo, cada m√≥dulo Python que define uma arquitetura √© totalmente independente e pode ser modificado para permitir experimentos de pesquisa r√°pidos.

A biblioteca ü§ó Transformers √© respaldada pelas tr√™s bibliotecas de aprendizado profundo mais populares ‚Äî [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) e [TensorFlow](https://www.tensorflow.org/) ‚Äî com uma integra√ß√£o perfeita entre elas. √â simples treinar seus modelos com uma delas antes de carreg√°-los para infer√™ncia com a outra

## Demonstra√ß√£o Online

Voc√™ pode testar a maioria de nossos modelos diretamente em suas p√°ginas a partir do [model hub](https://huggingface.co/models). Tamb√©m oferecemos [hospedagem de modelos privados, versionamento e uma API de infer√™ncia](https://huggingface.co/pricing)
para modelos p√∫blicos e privados.

Aqui est√£o alguns exemplos:

Em Processamento de Linguagem Natural:

- [Completar palavra mascarada com BERT](https://huggingface.co/google-bert/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France)
- [Reconhecimento de Entidades Nomeadas com Electra](https://huggingface.co/dbmdz/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city)
- [Gera√ß√£o de texto com GPT-2](https://huggingface.co/openai-community/gpt2?text=A+long+time+ago%2C)
- [Infer√™ncia de Linguagem Natural com RoBERTa](https://huggingface.co/FacebookAI/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal)
- [Sumariza√ß√£o com BART](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+was+the+first+structure+to+reach+a+height+of+300+metres.+Due+to+the+addition+of+a+broadcasting+aerial+at+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct)
- [Resposta a perguntas com DistilBERT](https://huggingface.co/distilbert/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species)
- [Tradu√ß√£o com T5](https://huggingface.co/google-t5/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)


Em Vis√£o Computacional:
- [Classifica√ß√£o de Imagens com ViT](https://huggingface.co/google/vit-base-patch16-224)
- [Detec√ß√£o de Objetos com DETR](https://huggingface.co/facebook/detr-resnet-50)
- [Segmenta√ß√£o Sem√¢ntica com SegFormer](https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512)
- [Segmenta√ß√£o Pan√≥ptica com MaskFormer](https://huggingface.co/facebook/maskformer-swin-small-coco)
- [Estimativa de Profundidade com DPT](https://huggingface.co/docs/transformers/model_doc/dpt)
- [Classifica√ß√£o de V√≠deo com VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae)
- [Segmenta√ß√£o Universal com OneFormer](https://huggingface.co/shi-labs/oneformer_ade20k_dinat_large)


Em √Åudio:
- [Reconhecimento Autom√°tico de Fala com Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base-960h)
- [Detec√ß√£o de Palavras-Chave com Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)
- [Classifica√ß√£o de √Åudio com Transformer de Espectrograma de √Åudio](https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593)

Em Tarefas Multimodais:
- [Respostas de Perguntas em Tabelas com TAPAS](https://huggingface.co/google/tapas-base-finetuned-wtq)
- [Respostas de Perguntas Visuais com ViLT](https://huggingface.co/dandelin/vilt-b32-finetuned-vqa)
- [Classifica√ß√£o de Imagens sem Anota√ß√£o com CLIP](https://huggingface.co/openai/clip-vit-large-patch14)
- [Respostas de Perguntas em Documentos com LayoutLM](https://huggingface.co/impira/layoutlm-document-qa)
- [Classifica√ß√£o de V√≠deo sem Anota√ß√£o com X-CLIP](https://huggingface.co/docs/transformers/model_doc/xclip)

## 100 Projetos Usando Transformers

Transformers √© mais do que um conjunto de ferramentas para usar modelos pr√©-treinados: √© uma comunidade de projetos constru√≠dos ao seu redor e o Hugging Face Hub. Queremos que o Transformers permita que desenvolvedores, pesquisadores, estudantes, professores, engenheiros e qualquer outra pessoa construa seus projetos dos sonhos.

Para celebrar as 100.000 estrelas do Transformers, decidimos destacar a comunidade e criamos a p√°gina [awesome-transformers](./awesome-transformers.md), que lista 100 projetos incr√≠veis constru√≠dos nas proximidades dos Transformers.

Se voc√™ possui ou utiliza um projeto que acredita que deveria fazer parte da lista, abra um PR para adicion√°-lo!

## Se voc√™ est√° procurando suporte personalizado da equipe Hugging Face

<a target="_blank" href="https://huggingface.co/support">
    <img alt="HuggingFace Expert Acceleration Program" src="https://cdn-media.huggingface.co/marketing/transformers/new-support-improved.png" style="max-width: 600px; border: 1px solid #eee; border-radius: 4px; box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);">
</a><br>


## Tour R√°pido

Para usar imediatamente um modelo em uma entrada espec√≠fica (texto, imagem, √°udio, ...), oferecemos a API `pipeline`. Os pipelines agrupam um modelo pr√©-treinado com o pr√©-processamento que foi usado durante o treinamento desse modelo. Aqui est√° como usar rapidamente um pipeline para classificar textos como positivos ou negativos:

```python
from transformers import pipeline

# Carregue o pipeline de classifica√ß√£o de texto
>>> classifier = pipeline("sentiment-analysis")

# Classifique o texto como positivo ou negativo
>>> classifier("Estamos muito felizes em apresentar o pipeline no reposit√≥rio dos transformers.")
[{'label': 'POSITIVE', 'score': 0.9996980428695679}]
```

A segunda linha de c√≥digo baixa e armazena em cache o modelo pr√©-treinado usado pelo pipeline, enquanto a terceira linha o avalia no texto fornecido. Neste exemplo, a resposta √© "positiva" com uma confian√ßa de 99,97%.

Muitas tarefas t√™m um `pipeline` pr√©-treinado pronto para uso, n√£o apenas em PNL, mas tamb√©m em vis√£o computacional e processamento de √°udio. Por exemplo, podemos facilmente extrair objetos detectados em uma imagem:

``` python
>>> import requests
>>> from PIL import Image
>>> from transformers import pipeline

# Download an image with cute cats
>>> url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png"
>>> image_data = requests.get(url, stream=True).raw
>>> image = Image.open(image_data)

# Allocate a pipeline for object detection
>>> object_detector = pipeline('object-detection')
>>> object_detector(image)
[{'score': 0.9982201457023621,
  'label': 'remote',
  'box': {'xmin': 40, 'ymin': 70, 'xmax': 175, 'ymax': 117}},
 {'score': 0.9960021376609802,
  'label': 'remote',
  'box': {'xmin': 333, 'ymin': 72, 'xmax': 368, 'ymax': 187}},
 {'score': 0.9954745173454285,
  'label': 'couch',
  'box': {'xmin': 0, 'ymin': 1, 'xmax': 639, 'ymax': 473}},
 {'score': 0.9988006353378296,
  'label': 'cat',
  'box': {'xmin': 13, 'ymin': 52, 'xmax': 314, 'ymax': 470}},
 {'score': 0.9986783862113953,
  'label': 'cat',
  'box': {'xmin': 345, 'ymin': 23, 'xmax': 640, 'ymax': 368}}]
```


Aqui obtemos uma lista de objetos detectados na imagem, com uma caixa envolvendo o objeto e uma pontua√ß√£o de confian√ßa. Aqui est√° a imagem original √† esquerda, com as previs√µes exibidas √† direita:

<h3 align="center">
    <a><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png" width="400"></a>
    <a><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample_post_processed.png" width="400"></a>
</h3>

Voc√™ pode aprender mais sobre as tarefas suportadas pela API `pipeline` em [este tutorial](https://huggingface.co/docs/transformers/task_summary).


Al√©m do `pipeline`, para baixar e usar qualquer um dos modelos pr√©-treinados em sua tarefa espec√≠fica, tudo o que √© necess√°rio s√£o tr√™s linhas de c√≥digo. Aqui est√° a vers√£o em PyTorch:

```python
>>> from transformers import AutoTokenizer, AutoModel

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
>>> model = AutoModel.from_pretrained("google-bert/bert-base-uncased")

>>> inputs = tokenizer("Hello world!", return_tensors="pt")
>>> outputs = model(**inputs)
```

E aqui est√° o c√≥digo equivalente para TensorFlow:

```python
>>> from transformers import AutoTokenizer, TFAutoModel

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
>>> model = TFAutoModel.from_pretrained("google-bert/bert-base-uncased")

>>> inputs = tokenizer("Hello world!", return_tensors="tf")
>>> outputs = model(**inputs)
```

O tokenizador √© respons√°vel por todo o pr√©-processamento que o modelo pr√©-treinado espera, e pode ser chamado diretamente em uma √∫nica string (como nos exemplos acima) ou em uma lista. Ele produzir√° um dicion√°rio que voc√™ pode usar no c√≥digo subsequente ou simplesmente passar diretamente para o seu modelo usando o operador de descompacta√ß√£o de argumentos **.

O modelo em si √© um [Pytorch `nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)  ou um [TensorFlow `tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)(dependendo do seu back-end) que voc√™ pode usar como de costume. [Este tutorial](https://huggingface.co/docs/transformers/training) explica como integrar esse modelo em um ciclo de treinamento cl√°ssico do PyTorch ou TensorFlow, ou como usar nossa API `Trainer` para ajuste fino r√°pido em um novo conjunto de dados.

## Por que devo usar transformers?

1. Modelos state-of-the-art f√°ceis de usar:
    - Alto desempenho em compreens√£o e gera√ß√£o de linguagem natural, vis√£o computacional e tarefas de √°udio.
    - Barreira de entrada baixa para educadores e profissionais.
    - Poucas abstra√ß√µes vis√≠veis para o usu√°rio, com apenas tr√™s classes para aprender.
    - Uma API unificada para usar todos os nossos modelos pr√©-treinados.

1. Menores custos de computa√ß√£o, menor pegada de carbono:
    - Pesquisadores podem compartilhar modelos treinados em vez de treinar sempre do zero.
    - Profissionais podem reduzir o tempo de computa√ß√£o e os custos de produ√ß√£o.
    - Dezenas de arquiteturas com mais de 60.000 modelos pr√©-treinados em todas as modalidades.

1. Escolha o framework certo para cada parte da vida de um modelo:
    - Treine modelos state-of-the-art em 3 linhas de c√≥digo.
    - Mova um √∫nico modelo entre frameworks TF2.0/PyTorch/JAX √† vontade.
    - Escolha o framework certo de forma cont√≠nua para treinamento, avalia√ß√£o e produ√ß√£o.

1. Personalize facilmente um modelo ou um exemplo para atender √†s suas necessidades:
    - Fornecemos exemplos para cada arquitetura para reproduzir os resultados publicados pelos autores originais.
    - Os detalhes internos do modelo s√£o expostos de maneira consistente.
    - Os arquivos do modelo podem ser usados de forma independente da biblioteca para experimentos r√°pidos.

## Por que n√£o devo usar transformers?

- Esta biblioteca n√£o √© uma caixa de ferramentas modular para construir redes neurais. O c√≥digo nos arquivos do modelo n√£o √© refatorado com abstra√ß√µes adicionais de prop√≥sito, para que os pesquisadores possam iterar rapidamente em cada um dos modelos sem se aprofundar em abstra√ß√µes/arquivos adicionais.
- A API de treinamento n√£o √© projetada para funcionar com qualquer modelo, mas √© otimizada para funcionar com os modelos fornecidos pela biblioteca. Para loops de aprendizado de m√°quina gen√©ricos, voc√™ deve usar outra biblioteca (possivelmente, [Accelerate](https://huggingface.co/docs/accelerate)).
- Embora nos esforcemos para apresentar o maior n√∫mero poss√≠vel de casos de uso, os scripts em nossa [pasta de exemplos](https://github.com/huggingface/transformers/tree/main/examples) s√£o apenas isso: exemplos. √â esperado que eles n√£o funcionem prontos para uso em seu problema espec√≠fico e que seja necess√°rio modificar algumas linhas de c√≥digo para adapt√°-los √†s suas necessidades.



### Com pip

Este reposit√≥rio √© testado no Python 3.8+, Flax 0.4.1+, PyTorch 1.11+ e TensorFlow 2.6+.

Voc√™ deve instalar o ü§ó Transformers em um [ambiente virtual](https://docs.python.org/3/library/venv.html). Se voc√™ n√£o est√° familiarizado com ambientes virtuais em Python, confira o [guia do usu√°rio](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).

Primeiro, crie um ambiente virtual com a vers√£o do Python que voc√™ vai usar e ative-o.

Em seguida, voc√™ precisar√° instalar pelo menos um dos back-ends Flax, PyTorch ou TensorFlow.
Consulte a [p√°gina de instala√ß√£o do TensorFlow](https://www.tensorflow.org/install/), a [p√°gina de instala√ß√£o do PyTorch](https://pytorch.org/get-started/locally/#start-locally) e/ou [Flax](https://github.com/google/flax#quick-install) e [Jax](https://github.com/google/jax#installation) p√°ginas de instala√ß√£o para obter o comando de instala√ß√£o espec√≠fico para a sua plataforma.

Quando um desses back-ends estiver instalado, o ü§ó Transformers pode ser instalado usando pip da seguinte forma:

```bash
pip install transformers
```
Se voc√™ deseja experimentar com os exemplos ou precisa da vers√£o mais recente do c√≥digo e n√£o pode esperar por um novo lan√ßamento, voc√™ deve instalar a [biblioteca a partir do c√≥digo-fonte](https://huggingface.co/docs/transformers/installation#installing-from-source).

### Com conda

O ü§ó Transformers pode ser instalado com conda da seguinte forma:

```bash
conda install conda-forge::transformers
```

> **_NOTA:_**  Instalar `transformers` pelo canal `huggingface` est√° obsoleto.

Siga as p√°ginas de instala√ß√£o do Flax, PyTorch ou TensorFlow para ver como instal√°-los com conda.

Siga as p√°ginas de instala√ß√£o do Flax, PyTorch ou TensorFlow para ver como instal√°-los com o conda.

> **_NOTA:_**  No Windows, voc√™ pode ser solicitado a ativar o Modo de Desenvolvedor para aproveitar o cache. Se isso n√£o for uma op√ß√£o para voc√™, por favor nos avise [neste problema](https://github.com/huggingface/huggingface_hub/issues/1062).

## Arquiteturas de Modelos

**[Todos os pontos de verifica√ß√£o de modelo](https://huggingface.co/models)** fornecidos pelo ü§ó Transformers s√£o integrados de forma transparente do [model hub](https://huggingface.co/models) do huggingface.co, onde s√£o carregados diretamente por [usu√°rios](https://huggingface.co/users) e [organiza√ß√µes](https://huggingface.co/organizations).

N√∫mero atual de pontos de verifica√ß√£o: ![](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen)

ü§ó Transformers atualmente fornece as seguintes arquiteturas: veja [aqui](https://huggingface.co/docs/transformers/model_summary) para um resumo de alto n√≠vel de cada uma delas.

Para verificar se cada modelo tem uma implementa√ß√£o em Flax, PyTorch ou TensorFlow, ou possui um tokenizador associado com a biblioteca ü§ó Tokenizers, consulte [esta tabela](https://huggingface.co/docs/transformers/index#supported-frameworks).

Essas implementa√ß√µes foram testadas em v√°rios conjuntos de dados (veja os scripts de exemplo) e devem corresponder ao desempenho das implementa√ß√µes originais. Voc√™ pode encontrar mais detalhes sobre o desempenho na se√ß√£o de Exemplos da [documenta√ß√£o](https://github.com/huggingface/transformers/tree/main/examples).


## Saiba mais

| Se√ß√£o | Descri√ß√£o |
|-|-|
| [Documenta√ß√£o](https://huggingface.co/docs/transformers/) | Documenta√ß√£o completa da API e tutoriais |
| [Resumo de Tarefas](https://huggingface.co/docs/transformers/task_summary) | Tarefas suportadas pelo ü§ó Transformers |
| [Tutorial de Pr√©-processamento](https://huggingface.co/docs/transformers/preprocessing) | Usando a classe `Tokenizer` para preparar dados para os modelos |
| [Treinamento e Ajuste Fino](https://huggingface.co/docs/transformers/training) | Usando os modelos fornecidos pelo ü§ó Transformers em um loop de treinamento PyTorch/TensorFlow e a API `Trainer` |
| [Tour R√°pido: Scripts de Ajuste Fino/Utiliza√ß√£o](https://github.com/huggingface/transformers/tree/main/examples) | Scripts de exemplo para ajuste fino de modelos em uma ampla gama de tarefas |
| [Compartilhamento e Envio de Modelos](https://huggingface.co/docs/transformers/model_sharing) | Envie e compartilhe seus modelos ajustados com a comunidade |

## Cita√ß√£o

Agora temos um [artigo](https://www.aclweb.org/anthology/2020.emnlp-demos.6/) que voc√™ pode citar para a biblioteca ü§ó Transformers:
```bibtex
@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R√©mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = out,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}
```
