<!---
Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

<p align="center">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg">
    <source media="(prefers-color-scheme: light)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg">
    <img alt="Hugging Face Transformers Library" src="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg" width="352" height="59" style="max-width: 100%;">
  </picture>
  <br/>
  <br/>
</p>

<p align="center">
    <a href="https://circleci.com/gh/huggingface/transformers"><img alt="Build" src="https://img.shields.io/circleci/build/github/huggingface/transformers/main"></a>
    <a href="https://github.com/huggingface/transformers/blob/main/LICENSE"><img alt="GitHub" src="https://img.shields.io/github/license/huggingface/transformers.svg?color=blue"></a>
    <a href="https://huggingface.co/docs/transformers/index"><img alt="Documentation" src="https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online"></a>
    <a href="https://github.com/huggingface/transformers/releases"><img alt="GitHub release" src="https://img.shields.io/github/release/huggingface/transformers.svg"></a>
    <a href="https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md"><img alt="Contributor Covenant" src="https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg"></a>
    <a href="https://zenodo.org/badge/latestdoi/155220641"><img src="https://zenodo.org/badge/155220641.svg" alt="DOI"></a>
</p>

<h4 align="center">
    <p>
        <a href="https://github.com/huggingface/transformers/">English</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hans.md">ç®€ä½“ä¸­æ–‡</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hant.md">ç¹é«”ä¸­æ–‡</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ko.md">í•œêµ­ì–´</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_es.md">EspaÃ±ol</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ja.md">æ—¥æœ¬èª</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_hd.md">à¤¹à¤¿à¤¨à¥à¤¦à¥€</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ru.md">Ğ ÑƒÑÑĞºĞ¸Ğ¹</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md">Ğ ortuguÃªs</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_te.md">à°¤à±†à°²à±à°—à±</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_fr.md">FranÃ§ais</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_de.md">Deutsch</a> |
        <b>Tiáº¿ng viá»‡t</b> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ar.md">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©</a> |
    </p>
</h4>

<h3 align="center">
    <p>CÃ´ng nghá»‡ Há»c mÃ¡y tiÃªn tiáº¿n cho JAX, PyTorch vÃ  TensorFlow</p>
</h3>

<h3 align="center">
    <a href="https://hf.co/course"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png"></a>
</h3>

ğŸ¤— Transformers cung cáº¥p hÃ ng ngÃ n mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c Ä‘á»ƒ thá»±c hiá»‡n cÃ¡c nhiá»‡m vá»¥ trÃªn cÃ¡c modalities khÃ¡c nhau nhÆ° vÄƒn báº£n, hÃ¬nh áº£nh vÃ  Ã¢m thanh.

CÃ¡c mÃ´ hÃ¬nh nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng vÃ o:

* ğŸ“ VÄƒn báº£n, cho cÃ¡c nhiá»‡m vá»¥ nhÆ° phÃ¢n loáº¡i vÄƒn báº£n, trÃ­ch xuáº¥t thÃ´ng tin, tráº£ lá»i cÃ¢u há»i, tÃ³m táº¯t, dá»‹ch thuáº­t vÃ  sinh vÄƒn báº£n, trong hÆ¡n 100 ngÃ´n ngá»¯.
* ğŸ–¼ï¸ HÃ¬nh áº£nh, cho cÃ¡c nhiá»‡m vá»¥ nhÆ° phÃ¢n loáº¡i hÃ¬nh áº£nh, nháº­n diá»‡n Ä‘á»‘i tÆ°á»£ng vÃ  phÃ¢n Ä‘oáº¡n.
* ğŸ—£ï¸ Ã‚m thanh, cho cÃ¡c nhiá»‡m vá»¥ nhÆ° nháº­n dáº¡ng giá»ng nÃ³i vÃ  phÃ¢n loáº¡i Ã¢m thanh.

CÃ¡c mÃ´ hÃ¬nh Transformer cÅ©ng cÃ³ thá»ƒ thá»±c hiá»‡n cÃ¡c nhiá»‡m vá»¥ trÃªn **nhiá»u modalities káº¿t há»£p**, nhÆ° tráº£ lá»i cÃ¢u há»i vá» báº£ng, nháº­n dáº¡ng kÃ½ tá»± quang há»c, trÃ­ch xuáº¥t thÃ´ng tin tá»« tÃ i liá»‡u quÃ©t, phÃ¢n loáº¡i video vÃ  tráº£ lá»i cÃ¢u há»i hÃ¬nh áº£nh.

ğŸ¤— Transformers cung cáº¥p cÃ¡c API Ä‘á»ƒ táº£i xuá»‘ng vÃ  sá»­ dá»¥ng nhanh chÃ³ng cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c Ä‘Ã³ trÃªn vÄƒn báº£n cá»¥ thá»ƒ, Ä‘iá»u chá»‰nh chÃºng trÃªn táº­p dá»¯ liá»‡u cá»§a riÃªng báº¡n vÃ  sau Ä‘Ã³ chia sáº» chÃºng vá»›i cá»™ng Ä‘á»“ng trÃªn [model hub](https://huggingface.co/models) cá»§a chÃºng tÃ´i. Äá»“ng thá»i, má»—i module python xÃ¡c Ä‘á»‹nh má»™t kiáº¿n trÃºc lÃ  hoÃ n toÃ n Ä‘á»™c láº­p vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­a Ä‘á»•i Ä‘á»ƒ cho phÃ©p thá»±c hiá»‡n nhanh cÃ¡c thÃ­ nghiá»‡m nghiÃªn cá»©u.

ğŸ¤— Transformers Ä‘Æ°á»£c há»— trá»£ bá»Ÿi ba thÆ° viá»‡n há»c sÃ¢u phá»• biáº¿n nháº¥t â€” [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) vÃ  [TensorFlow](https://www.tensorflow.org/) â€” vá»›i tÃ­ch há»£p mÆ°á»£t mÃ  giá»¯a chÃºng. Viá»‡c huáº¥n luyá»‡n mÃ´ hÃ¬nh cá»§a báº¡n vá»›i má»™t thÆ° viá»‡n trÆ°á»›c khi táº£i chÃºng Ä‘á»ƒ sá»­ dá»¥ng trong suy luáº­n vá»›i thÆ° viá»‡n khÃ¡c lÃ  ráº¥t dá»… dÃ ng.

## CÃ¡c demo trá»±c tuyáº¿n

Báº¡n cÃ³ thá»ƒ kiá»ƒm tra háº§u háº¿t cÃ¡c mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i trá»±c tiáº¿p trÃªn trang cá»§a chÃºng tá»« [model hub](https://huggingface.co/models). ChÃºng tÃ´i cÅ©ng cung cáº¥p [dá»‹ch vá»¥ lÆ°u trá»¯ mÃ´ hÃ¬nh riÃªng tÆ°, phiÃªn báº£n vÃ  API suy luáº­n](https://huggingface.co/pricing) cho cÃ¡c mÃ´ hÃ¬nh cÃ´ng khai vÃ  riÃªng tÆ°.

DÆ°á»›i Ä‘Ã¢y lÃ  má»™t sá»‘ vÃ­ dá»¥:

Trong Xá»­ lÃ½ NgÃ´n ngá»¯ Tá»± nhiÃªn:
- [HoÃ n thÃ nh tá»« vá»¥ng vá» tá»« vá»›i BERT](https://huggingface.co/google-bert/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France)
- [Nháº­n dáº¡ng thá»±c thá»ƒ Ä‘áº·t tÃªn vá»›i Electra](https://huggingface.co/dbmdz/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city)
- [Táº¡o vÄƒn báº£n tá»± nhiÃªn vá»›i Mistral](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)
- [Suy luáº­n NgÃ´n ngá»¯ Tá»± nhiÃªn vá»›i RoBERTa](https://huggingface.co/FacebookAI/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal)
- [TÃ³m táº¯t vÄƒn báº£n vá»›i BART](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+was+the+first+structure+to+reach+a+height+of+300+metres.+Due+to+the+addition+of+a+broadcasting+aerial+at+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct)
- [Tráº£ lá»i cÃ¢u há»i vá»›i DistilBERT](https://huggingface.co/distilbert/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species)
- [Dá»‹ch vÄƒn báº£n vá»›i T5](https://huggingface.co/google-t5/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)

Trong Thá»‹ giÃ¡c MÃ¡y tÃ­nh:
- [PhÃ¢n loáº¡i hÃ¬nh áº£nh vá»›i ViT](https://huggingface.co/google/vit-base-patch16-224)
- [PhÃ¡t hiá»‡n Ä‘á»‘i tÆ°á»£ng vá»›i DETR](https://huggingface.co/facebook/detr-resnet-50)
- [PhÃ¢n Ä‘oáº¡n ngá»¯ nghÄ©a vá»›i SegFormer](https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512)
- [PhÃ¢n Ä‘oáº¡n toÃ n diá»‡n vá»›i Mask2Former](https://huggingface.co/facebook/mask2former-swin-large-coco-panoptic)
- [Æ¯á»›c lÆ°á»£ng Ä‘á»™ sÃ¢u vá»›i Depth Anything](https://huggingface.co/docs/transformers/main/model_doc/depth_anything)
- [PhÃ¢n loáº¡i video vá»›i VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae)
- [PhÃ¢n Ä‘oáº¡n toÃ n cáº§u vá»›i OneFormer](https://huggingface.co/shi-labs/oneformer_ade20k_dinat_large)

Trong Ã¢m thanh:
- [Nháº­n dáº¡ng giá»ng nÃ³i tá»± Ä‘á»™ng vá»›i Whisper](https://huggingface.co/openai/whisper-large-v3)
- [PhÃ¡t hiá»‡n tá»« khÃ³a vá»›i Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)
- [PhÃ¢n loáº¡i Ã¢m thanh vá»›i Audio Spectrogram Transformer](https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593)

Trong cÃ¡c nhiá»‡m vá»¥ Ä‘a phÆ°Æ¡ng thá»©c:
- [Tráº£ lá»i cÃ¢u há»i vá» báº£ng vá»›i TAPAS](https://huggingface.co/google/tapas-base-finetuned-wtq)
- [Tráº£ lá»i cÃ¢u há»i hÃ¬nh áº£nh vá»›i ViLT](https://huggingface.co/dandelin/vilt-b32-finetuned-vqa)
- [MÃ´ táº£ hÃ¬nh áº£nh vá»›i LLaVa](https://huggingface.co/llava-hf/llava-1.5-7b-hf)
- [PhÃ¢n loáº¡i hÃ¬nh áº£nh khÃ´ng cáº§n nhÃ£n vá»›i SigLIP](https://huggingface.co/google/siglip-so400m-patch14-384)
- [Tráº£ lá»i cÃ¢u há»i vÄƒn báº£n tÃ i liá»‡u vá»›i LayoutLM](https://huggingface.co/impira/layoutlm-document-qa)
- [PhÃ¢n loáº¡i video khÃ´ng cáº§n nhÃ£n vá»›i X-CLIP](https://huggingface.co/docs/transformers/model_doc/xclip)
- [PhÃ¡t hiá»‡n Ä‘á»‘i tÆ°á»£ng khÃ´ng cáº§n nhÃ£n vá»›i OWLv2](https://huggingface.co/docs/transformers/en/model_doc/owlv2)
- [PhÃ¢n Ä‘oáº¡n hÃ¬nh áº£nh khÃ´ng cáº§n nhÃ£n vá»›i CLIPSeg](https://huggingface.co/docs/transformers/model_doc/clipseg)
- [Táº¡o máº·t náº¡ tá»± Ä‘á»™ng vá»›i SAM](https://huggingface.co/docs/transformers/model_doc/sam)


## 100 dá»± Ã¡n sá»­ dá»¥ng Transformers

Transformers khÃ´ng chá»‰ lÃ  má»™t bá»™ cÃ´ng cá»¥ Ä‘á»ƒ sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c: Ä‘Ã³ lÃ  má»™t cá»™ng Ä‘á»“ng cÃ¡c dá»± Ã¡n xÃ¢y dá»±ng xung quanh nÃ³ vÃ  Hugging Face Hub. ChÃºng tÃ´i muá»‘n Transformers giÃºp cÃ¡c nhÃ  phÃ¡t triá»ƒn, nhÃ  nghiÃªn cá»©u, sinh viÃªn, giÃ¡o sÆ°, ká»¹ sÆ° vÃ  báº¥t ká»³ ai khÃ¡c xÃ¢y dá»±ng nhá»¯ng dá»± Ã¡n mÆ¡ Æ°á»›c cá»§a há».

Äá»ƒ ká»· niá»‡m 100.000 sao cá»§a transformers, chÃºng tÃ´i Ä‘Ã£ quyáº¿t Ä‘á»‹nh táº­p trung vÃ o cá»™ng Ä‘á»“ng vÃ  táº¡o ra trang [awesome-transformers](./awesome-transformers.md) liá»‡t kÃª 100 dá»± Ã¡n tuyá»‡t vá»i Ä‘Æ°á»£c xÃ¢y dá»±ng xung quanh transformers.

Náº¿u báº¡n sá»Ÿ há»¯u hoáº·c sá»­ dá»¥ng má»™t dá»± Ã¡n mÃ  báº¡n tin ráº±ng nÃªn Ä‘Æ°á»£c thÃªm vÃ o danh sÃ¡ch, vui lÃ²ng má»Ÿ má»™t PR Ä‘á»ƒ thÃªm nÃ³!

## Náº¿u báº¡n Ä‘ang tÃ¬m kiáº¿m há»— trá»£ tÃ¹y chá»‰nh tá»« Ä‘á»™i ngÅ© Hugging Face

<a target="_blank" href="https://huggingface.co/support">
    <img alt="HuggingFace Expert Acceleration Program" src="https://cdn-media.huggingface.co/marketing/transformers/new-support-improved.png" style="max-width: 600px; border: 1px solid #eee; border-radius: 4px; box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);">
</a><br>

## HÃ nh trÃ¬nh nhanh

Äá»ƒ ngay láº­p tá»©c sá»­ dá»¥ng má»™t mÃ´ hÃ¬nh trÃªn má»™t Ä‘áº§u vÃ o cá»¥ thá»ƒ (vÄƒn báº£n, hÃ¬nh áº£nh, Ã¢m thanh, ...), chÃºng tÃ´i cung cáº¥p API `pipeline`. Pipelines nhÃ³m má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c vá»›i quÃ¡ trÃ¬nh tiá»n xá»­ lÃ½ Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n cá»§a mÃ´ hÃ¬nh Ä‘Ã³. DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡ch sá»­ dá»¥ng nhanh má»™t pipeline Ä‘á»ƒ phÃ¢n loáº¡i vÄƒn báº£n tÃ­ch cá»±c so vá»›i tiÃªu cá»±c:

```python
>>> from transformers import pipeline

# Cáº¥p phÃ¡t má»™t pipeline cho phÃ¢n tÃ­ch cáº£m xÃºc
>>> classifier = pipeline('sentiment-analysis')
>>> classifier('We are very happy to introduce pipeline to the transformers repository.')
[{'label': 'POSITIVE', 'score': 0.9996980428695679}]
```

DÃ²ng code thá»© hai táº£i xuá»‘ng vÃ  lÆ°u trá»¯ bá»™ mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi pipeline, trong khi dÃ²ng thá»© ba Ä‘Ã¡nh giÃ¡ nÃ³ trÃªn vÄƒn báº£n Ä‘Ã£ cho. á» Ä‘Ã¢y, cÃ¢u tráº£ lá»i lÃ  "tÃ­ch cá»±c" vá»›i Ä‘á»™ tin cáº­y lÃ  99,97%.

Nhiá»u nhiá»‡m vá»¥ cÃ³ sáºµn má»™t `pipeline` Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c, trong NLP nhÆ°ng cÅ©ng trong thá»‹ giÃ¡c mÃ¡y tÃ­nh vÃ  giá»ng nÃ³i. VÃ­ dá»¥, chÃºng ta cÃ³ thá»ƒ dá»… dÃ ng trÃ­ch xuáº¥t cÃ¡c Ä‘á»‘i tÆ°á»£ng Ä‘Æ°á»£c phÃ¡t hiá»‡n trong má»™t hÃ¬nh áº£nh:

``` python
>>> import requests
>>> from PIL import Image
>>> from transformers import pipeline

# Táº£i xuá»‘ng má»™t hÃ¬nh áº£nh vá»›i nhá»¯ng con mÃ¨o dá»… thÆ°Æ¡ng
>>> url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png"
>>> image_data = requests.get(url, stream=True).raw
>>> image = Image.open(image_data)

# Cáº¥p phÃ¡t má»™t pipeline cho phÃ¡t hiá»‡n Ä‘á»‘i tÆ°á»£ng
>>> object_detector = pipeline('object-detection')
>>> object_detector(image)
[{'score': 0.9982201457023621,
  'label': 'remote',
  'box': {'xmin': 40, 'ymin': 70, 'xmax': 175, 'ymax': 117}},
 {'score': 0.9960021376609802,
  'label': 'remote',
  'box': {'xmin': 333, 'ymin': 72, 'xmax': 368, 'ymax': 187}},
 {'score': 0.9954745173454285,
  'label': 'couch',
  'box': {'xmin': 0, 'ymin': 1, 'xmax': 639, 'ymax': 473}},
 {'score': 0.9988006353378296,
  'label': 'cat',
  'box': {'xmin': 13, 'ymin': 52, 'xmax': 314, 'ymax': 470}},
 {'score': 0.9986783862113953,
  'label': 'cat',
  'box': {'xmin': 345, 'ymin': 23, 'xmax': 640, 'ymax': 368}}]
```

á» Ä‘Ã¢y, chÃºng ta nháº­n Ä‘Æ°á»£c má»™t danh sÃ¡ch cÃ¡c Ä‘á»‘i tÆ°á»£ng Ä‘Æ°á»£c phÃ¡t hiá»‡n trong hÃ¬nh áº£nh, vá»›i má»™t há»™p bao quanh Ä‘á»‘i tÆ°á»£ng vÃ  má»™t Ä‘iá»ƒm Ä‘Ã¡nh giÃ¡ Ä‘á»™ tin cáº­y. ÄÃ¢y lÃ  hÃ¬nh áº£nh gá»‘c á»Ÿ bÃªn trÃ¡i, vá»›i cÃ¡c dá»± Ä‘oÃ¡n hiá»ƒn thá»‹ á»Ÿ bÃªn pháº£i:

<h3 align="center">
    <a><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png" width="400"></a>
    <a><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample_post_processed.png" width="400"></a>
</h3>

Báº¡n cÃ³ thá»ƒ tÃ¬m hiá»ƒu thÃªm vá» cÃ¡c nhiá»‡m vá»¥ Ä‘Æ°á»£c há»— trá»£ bá»Ÿi API `pipeline` trong [hÆ°á»›ng dáº«n nÃ y](https://huggingface.co/docs/transformers/task_summary).

NgoÃ i `pipeline`, Ä‘á»ƒ táº£i xuá»‘ng vÃ  sá»­ dá»¥ng báº¥t ká»³ mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c nÃ o cho nhiá»‡m vá»¥ cá»¥ thá»ƒ cá»§a báº¡n, chá»‰ cáº§n ba dÃ²ng code. ÄÃ¢y lÃ  phiÃªn báº£n PyTorch:
```python
>>> from transformers import AutoTokenizer, AutoModel

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
>>> model = AutoModel.from_pretrained("google-bert/bert-base-uncased")

>>> inputs = tokenizer("Hello world!", return_tensors="pt")
>>> outputs = model(**inputs)
```

VÃ  Ä‘Ã¢y lÃ  mÃ£ tÆ°Æ¡ng Ä‘Æ°Æ¡ng cho TensorFlow:
```python
>>> from transformers import AutoTokenizer, TFAutoModel

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
>>> model = TFAutoModel.from_pretrained("google-bert/bert-base-uncased")

>>> inputs = tokenizer("Hello world!", return_tensors="tf")
>>> outputs = model(**inputs)
```

Tokenizer lÃ  thÃ nh pháº§n chá»‹u trÃ¡ch nhiá»‡m cho viá»‡c tiá»n xá»­ lÃ½ mÃ  mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c mong Ä‘á»£i vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c gá»i trá»±c tiáº¿p trÃªn má»™t chuá»—i Ä‘Æ¡n (nhÆ° trong cÃ¡c vÃ­ dá»¥ trÃªn) hoáº·c má»™t danh sÃ¡ch. NÃ³ sáº½ xuáº¥t ra má»™t tá»« Ä‘iá»ƒn mÃ  báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng trong mÃ£ phá»¥ thuá»™c hoáº·c Ä‘Æ¡n giáº£n lÃ  truyá»n trá»±c tiáº¿p cho mÃ´ hÃ¬nh cá»§a báº¡n báº±ng cÃ¡ch sá»­ dá»¥ng toÃ¡n tá»­ ** Ä‘á»ƒ giáº£i nÃ©n Ä‘á»‘i sá»‘.

ChÃ­nh mÃ´ hÃ¬nh lÃ  má»™t [Pytorch `nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) thÃ´ng thÆ°á»ng hoáº·c má»™t [TensorFlow `tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) (tÃ¹y thuá»™c vÃ o backend cá»§a báº¡n) mÃ  báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng nhÆ° bÃ¬nh thÆ°á»ng. [HÆ°á»›ng dáº«n nÃ y](https://huggingface.co/docs/transformers/training) giáº£i thÃ­ch cÃ¡ch tÃ­ch há»£p má»™t mÃ´ hÃ¬nh nhÆ° váº­y vÃ o má»™t vÃ²ng láº·p huáº¥n luyá»‡n cá»• Ä‘iá»ƒn PyTorch hoáº·c TensorFlow, hoáº·c cÃ¡ch sá»­ dá»¥ng API `Trainer` cá»§a chÃºng tÃ´i Ä‘á»ƒ tinh chá»‰nh nhanh chÃ³ng trÃªn má»™t bá»™ dá»¯ liá»‡u má»›i.

## Táº¡i sao tÃ´i nÃªn sá»­ dá»¥ng transformers?

1. CÃ¡c mÃ´ hÃ¬nh tiÃªn tiáº¿n dá»… sá»­ dá»¥ng:
    - Hiá»‡u suáº¥t cao trong viá»‡c hiá»ƒu vÃ  táº¡o ra ngÃ´n ngá»¯ tá»± nhiÃªn, thá»‹ giÃ¡c mÃ¡y tÃ­nh vÃ  Ã¢m thanh.
    - NgÆ°á»¡ng vÃ o tháº¥p cho giáº£ng viÃªn vÃ  ngÆ°á»i thá»±c hÃ nh.
    - Ãt trá»«u tÆ°á»£ng dÃ nh cho ngÆ°á»i dÃ¹ng vá»›i chá»‰ ba lá»›p há»c.
    - Má»™t API thá»‘ng nháº¥t Ä‘á»ƒ sá»­ dá»¥ng táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c cá»§a chÃºng tÃ´i.

2. Giáº£m chi phÃ­ tÃ­nh toÃ¡n, lÃ m giáº£m lÆ°á»£ng khÃ­ tháº£i carbon:
    - CÃ¡c nhÃ  nghiÃªn cá»©u cÃ³ thá»ƒ chia sáº» cÃ¡c mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n thay vÃ¬ luÃ´n luÃ´n huáº¥n luyá»‡n láº¡i.
    - NgÆ°á»i thá»±c hÃ nh cÃ³ thá»ƒ giáº£m thá»i gian tÃ­nh toÃ¡n vÃ  chi phÃ­ sáº£n xuáº¥t.
    - HÃ ng chá»¥c kiáº¿n trÃºc vá»›i hÆ¡n 400.000 mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c trÃªn táº¥t cáº£ cÃ¡c phÆ°Æ¡ng phÃ¡p.

3. Lá»±a chá»n framework phÃ¹ há»£p cho má»i giai Ä‘oáº¡n cá»§a mÃ´ hÃ¬nh:
    - Huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh tiÃªn tiáº¿n chá»‰ trong 3 dÃ²ng code.
    - Di chuyá»ƒn má»™t mÃ´ hÃ¬nh duy nháº¥t giá»¯a cÃ¡c framework TF2.0/PyTorch/JAX theo Ã½ muá»‘n.
    - Dá»… dÃ ng chá»n framework phÃ¹ há»£p cho huáº¥n luyá»‡n, Ä‘Ã¡nh giÃ¡ vÃ  sáº£n xuáº¥t.

4. Dá»… dÃ ng tÃ¹y chá»‰nh má»™t mÃ´ hÃ¬nh hoáº·c má»™t vÃ­ dá»¥ theo nhu cáº§u cá»§a báº¡n:
    - ChÃºng tÃ´i cung cáº¥p cÃ¡c vÃ­ dá»¥ cho má»—i kiáº¿n trÃºc Ä‘á»ƒ tÃ¡i táº¡o káº¿t quáº£ Ä‘Æ°á»£c cÃ´ng bá»‘ bá»Ÿi cÃ¡c tÃ¡c giáº£ gá»‘c.
    - CÃ¡c thÃ nh pháº§n ná»™i táº¡i cá»§a mÃ´ hÃ¬nh Ä‘Æ°á»£c tiáº¿t lá»™ má»™t cÃ¡ch nháº¥t quÃ¡n nháº¥t cÃ³ thá»ƒ.
    - CÃ¡c tá»‡p mÃ´ hÃ¬nh cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»™c láº­p vá»›i thÆ° viá»‡n Ä‘á»ƒ thá»±c hiá»‡n cÃ¡c thá»­ nghiá»‡m nhanh chÃ³ng.

## Táº¡i sao tÃ´i khÃ´ng nÃªn sá»­ dá»¥ng transformers?

- ThÆ° viá»‡n nÃ y khÃ´ng pháº£i lÃ  má»™t bá»™ cÃ´ng cá»¥ modul cho cÃ¡c khá»‘i xÃ¢y dá»±ng máº¡ng neural. MÃ£ trong cÃ¡c tá»‡p mÃ´ hÃ¬nh khÃ´ng Ä‘Æ°á»£c tÃ¡i cáº¥u trÃºc vá»›i cÃ¡c trá»«u tÆ°á»£ng bá»• sung má»™t cÃ¡ch cá»‘ Ã½, Ä‘á»ƒ cÃ¡c nhÃ  nghiÃªn cá»©u cÃ³ thá»ƒ láº·p nhanh trÃªn tá»«ng mÃ´ hÃ¬nh mÃ  khÃ´ng cáº§n Ä‘Ã o sÃ¢u vÃ o cÃ¡c trá»«u tÆ°á»£ng/tá»‡p bá»• sung.
- API huáº¥n luyá»‡n khÃ´ng Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ hoáº¡t Ä‘á»™ng trÃªn báº¥t ká»³ mÃ´ hÃ¬nh nÃ o, mÃ  Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a Ä‘á»ƒ hoáº¡t Ä‘á»™ng vá»›i cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c cung cáº¥p bá»Ÿi thÆ° viá»‡n. Äá»‘i vá»›i vÃ²ng láº·p há»c mÃ¡y chung, báº¡n nÃªn sá»­ dá»¥ng má»™t thÆ° viá»‡n khÃ¡c (cÃ³ thá»ƒ lÃ  [Accelerate](https://huggingface.co/docs/accelerate)).
- Máº·c dÃ¹ chÃºng tÃ´i cá»‘ gáº¯ng trÃ¬nh bÃ y cÃ ng nhiá»u trÆ°á»ng há»£p sá»­ dá»¥ng cÃ ng tá»‘t, nhÆ°ng cÃ¡c táº­p lá»‡nh trong thÆ° má»¥c [examples](https://github.com/huggingface/transformers/tree/main/examples) chá»‰ lÃ  vÃ­ dá»¥. Dá»± kiáº¿n ráº±ng chÃºng sáº½ khÃ´ng hoáº¡t Ä‘á»™ng ngay tá»©c kháº¯c trÃªn váº¥n Ä‘á» cá»¥ thá»ƒ cá»§a báº¡n vÃ  báº¡n sáº½ pháº£i thay Ä‘á»•i má»™t sá»‘ dÃ²ng mÃ£ Ä‘á»ƒ thÃ­ch nghi vá»›i nhu cáº§u cá»§a báº¡n.

## CÃ i Ä‘áº·t

### Sá»­ dá»¥ng pip

ThÆ° viá»‡n nÃ y Ä‘Æ°á»£c kiá»ƒm tra trÃªn Python 3.8+, Flax 0.4.1+, PyTorch 1.11+ vÃ  TensorFlow 2.6+.

Báº¡n nÃªn cÃ i Ä‘áº·t ğŸ¤— Transformers trong má»™t [mÃ´i trÆ°á»ng áº£o Python](https://docs.python.org/3/library/venv.html). Náº¿u báº¡n chÆ°a quen vá»›i mÃ´i trÆ°á»ng áº£o Python, hÃ£y xem [hÆ°á»›ng dáº«n sá»­ dá»¥ng](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).

TrÆ°á»›c tiÃªn, táº¡o má»™t mÃ´i trÆ°á»ng áº£o vá»›i phiÃªn báº£n Python báº¡n sáº½ sá»­ dá»¥ng vÃ  kÃ­ch hoáº¡t nÃ³.

Sau Ä‘Ã³, báº¡n sáº½ cáº§n cÃ i Ä‘áº·t Ã­t nháº¥t má»™t trong sá»‘ cÃ¡c framework Flax, PyTorch hoáº·c TensorFlow.
Vui lÃ²ng tham kháº£o [trang cÃ i Ä‘áº·t TensorFlow](https://www.tensorflow.org/install/), [trang cÃ i Ä‘áº·t PyTorch](https://pytorch.org/get-started/locally/#start-locally) vÃ /hoáº·c [Flax](https://github.com/google/flax#quick-install) vÃ  [Jax](https://github.com/google/jax#installation) Ä‘á»ƒ biáº¿t lá»‡nh cÃ i Ä‘áº·t cá»¥ thá»ƒ cho ná»n táº£ng cá»§a báº¡n.

Khi Ä‘Ã£ cÃ i Ä‘áº·t má»™t trong cÃ¡c backend Ä‘Ã³, ğŸ¤— Transformers cÃ³ thá»ƒ Ä‘Æ°á»£c cÃ i Ä‘áº·t báº±ng pip nhÆ° sau:

```bash
pip install transformers
```

Náº¿u báº¡n muá»‘n thá»±c hiá»‡n cÃ¡c vÃ­ dá»¥ hoáº·c cáº§n phiÃªn báº£n má»›i nháº¥t cá»§a mÃ£ vÃ  khÃ´ng thá»ƒ chá» Ä‘á»£i cho má»™t phiÃªn báº£n má»›i, báº¡n pháº£i [cÃ i Ä‘áº·t thÆ° viá»‡n tá»« nguá»“n](https://huggingface.co/docs/transformers/installation#installing-from-source).

### Vá»›i conda

ğŸ¤— Transformers cÃ³ thá»ƒ Ä‘Æ°á»£c cÃ i Ä‘áº·t báº±ng conda nhÆ° sau:

```shell script
conda install conda-forge::transformers
```

> **_GHI CHÃš:_** CÃ i Ä‘áº·t `transformers` tá»« kÃªnh `huggingface` Ä‘Ã£ bá»‹ lá»—i thá»i.

HÃ£y lÃ m theo trang cÃ i Ä‘áº·t cá»§a Flax, PyTorch hoáº·c TensorFlow Ä‘á»ƒ xem cÃ¡ch cÃ i Ä‘áº·t chÃºng báº±ng conda.

> **_GHI CHÃš:_** TrÃªn Windows, báº¡n cÃ³ thá»ƒ Ä‘Æ°á»£c yÃªu cáº§u kÃ­ch hoáº¡t Cháº¿ Ä‘á»™ phÃ¡t triá»ƒn Ä‘á»ƒ táº­n dá»¥ng viá»‡c lÆ°u cache. Náº¿u Ä‘iá»u nÃ y khÃ´ng pháº£i lÃ  má»™t lá»±a chá»n cho báº¡n, hÃ£y cho chÃºng tÃ´i biáº¿t trong [váº¥n Ä‘á» nÃ y](https://github.com/huggingface/huggingface_hub/issues/1062).

## Kiáº¿n trÃºc mÃ´ hÃ¬nh

**[Táº¥t cáº£ cÃ¡c Ä‘iá»ƒm kiá»ƒm tra mÃ´ hÃ¬nh](https://huggingface.co/models)** Ä‘Æ°á»£c cung cáº¥p bá»Ÿi ğŸ¤— Transformers Ä‘Æ°á»£c tÃ­ch há»£p má»™t cÃ¡ch mÆ°á»£t mÃ  tá»« trung tÃ¢m mÃ´ hÃ¬nh huggingface.co [model hub](https://huggingface.co/models), nÆ¡i chÃºng Ä‘Æ°á»£c táº£i lÃªn trá»±c tiáº¿p bá»Ÿi [ngÆ°á»i dÃ¹ng](https://huggingface.co/users) vÃ  [tá»• chá»©c](https://huggingface.co/organizations).

Sá»‘ lÆ°á»£ng Ä‘iá»ƒm kiá»ƒm tra hiá»‡n táº¡i: ![](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen)

ğŸ¤— Transformers hiá»‡n Ä‘ang cung cáº¥p cÃ¡c kiáº¿n trÃºc sau Ä‘Ã¢y: xem [á»Ÿ Ä‘Ã¢y](https://huggingface.co/docs/transformers/model_summary) Ä‘á»ƒ cÃ³ má»™t tÃ³m táº¯t tá»•ng quan vá» má»—i kiáº¿n trÃºc.

Äá»ƒ kiá»ƒm tra xem má»—i mÃ´ hÃ¬nh cÃ³ má»™t phiÃªn báº£n thá»±c hiá»‡n trong Flax, PyTorch hoáº·c TensorFlow, hoáº·c cÃ³ má»™t tokenizer liÃªn quan Ä‘Æ°á»£c há»— trá»£ bá»Ÿi thÆ° viá»‡n ğŸ¤— Tokenizers, vui lÃ²ng tham kháº£o [báº£ng nÃ y](https://huggingface.co/docs/transformers/index#supported-frameworks).

Nhá»¯ng phiÃªn báº£n nÃ y Ä‘Ã£ Ä‘Æ°á»£c kiá»ƒm tra trÃªn má»™t sá»‘ táº­p dá»¯ liá»‡u (xem cÃ¡c táº­p lá»‡nh vÃ­ dá»¥) vÃ  nÃªn tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i hiá»‡u suáº¥t cá»§a cÃ¡c phiÃªn báº£n gá»‘c. Báº¡n cÃ³ thá»ƒ tÃ¬m tháº¥y thÃªm thÃ´ng tin vá» hiá»‡u suáº¥t trong pháº§n VÃ­ dá»¥ cá»§a [tÃ i liá»‡u](https://github.com/huggingface/transformers/tree/main/examples).


## TÃ¬m hiá»ƒu thÃªm

| Pháº§n | MÃ´ táº£ |
|-|-|
| [TÃ i liá»‡u](https://huggingface.co/docs/transformers/) | ToÃ n bá»™ tÃ i liá»‡u API vÃ  hÆ°á»›ng dáº«n |
| [TÃ³m táº¯t nhiá»‡m vá»¥](https://huggingface.co/docs/transformers/task_summary) | CÃ¡c nhiá»‡m vá»¥ Ä‘Æ°á»£c há»— trá»£ bá»Ÿi ğŸ¤— Transformers |
| [HÆ°á»›ng dáº«n tiá»n xá»­ lÃ½](https://huggingface.co/docs/transformers/preprocessing) | Sá»­ dá»¥ng lá»›p `Tokenizer` Ä‘á»ƒ chuáº©n bá»‹ dá»¯ liá»‡u cho cÃ¡c mÃ´ hÃ¬nh |
| [Huáº¥n luyá»‡n vÃ  Ä‘iá»u chá»‰nh](https://huggingface.co/docs/transformers/training) | Sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c cung cáº¥p bá»Ÿi ğŸ¤— Transformers trong vÃ²ng láº·p huáº¥n luyá»‡n PyTorch/TensorFlow vÃ  API `Trainer` |
| [HÆ°á»›ng dáº«n nhanh: Äiá»u chá»‰nh/sá»­ dá»¥ng cÃ¡c ká»‹ch báº£n](https://github.com/huggingface/transformers/tree/main/examples) | CÃ¡c ká»‹ch báº£n vÃ­ dá»¥ Ä‘á»ƒ Ä‘iá»u chá»‰nh mÃ´ hÃ¬nh trÃªn nhiá»u nhiá»‡m vá»¥ khÃ¡c nhau |
| [Chia sáº» vÃ  táº£i lÃªn mÃ´ hÃ¬nh](https://huggingface.co/docs/transformers/model_sharing) | Táº£i lÃªn vÃ  chia sáº» cÃ¡c mÃ´ hÃ¬nh Ä‘Ã£ Ä‘iá»u chá»‰nh cá»§a báº¡n vá»›i cá»™ng Ä‘á»“ng |

## TrÃ­ch dáº«n

BÃ¢y giá» chÃºng ta cÃ³ má»™t [bÃ i bÃ¡o](https://www.aclweb.org/anthology/2020.emnlp-demos.6/) mÃ  báº¡n cÃ³ thá»ƒ trÃ­ch dáº«n cho thÆ° viá»‡n ğŸ¤— Transformers:
```bibtex
@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and RÃ©mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}
```
