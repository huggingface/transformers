<!---
Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

<!---
A useful guide for English-Traditional Chinese translation of Hugging Face documentation
- Add space around English words and numbers when they appear between Chinese characters. E.g., å…± 100 å¤šç¨®èªè¨€; ä½¿ç”¨ transformers å‡½å¼åº«ã€‚
- Use square quotes, e.g.,ã€Œå¼•ç”¨ã€
- Some of terms in the file can be found at National Academy for Educational Research (https://terms.naer.edu.tw/), an official website providing bilingual translations between English and Traditional Chinese.

Dictionary

API: API (ä¸ç¿»è­¯ï¼‰
add: åŠ å…¥
checkpoint: æª¢æŸ¥é»
code: ç¨‹å¼ç¢¼
community: ç¤¾ç¾¤
confidence: ä¿¡è³´åº¦
dataset: è³‡æ–™é›†
documentation: æ–‡ä»¶
example: åŸºæœ¬ç¿»è­¯ç‚ºã€Œç¯„ä¾‹ã€ï¼Œæˆ–ä¾èªæ„ç¿»ç‚ºã€Œä¾‹å­ã€
finetune: å¾®èª¿
Hugging Face: Hugging Faceï¼ˆä¸ç¿»è­¯ï¼‰
implementation: å¯¦ä½œ
inference: æ¨è«–
library: å‡½å¼åº«
module: æ¨¡çµ„
NLP/Natural Language Processing: ä»¥ NLP å‡ºç¾æ™‚ä¸ç¿»è­¯ï¼Œä»¥ Natural Language Processing å‡ºç¾æ™‚ç¿»è­¯ç‚ºè‡ªç„¶èªè¨€è™•ç†
online demos: ç·šä¸ŠDemo
pipeline: pipelineï¼ˆä¸ç¿»è­¯ï¼‰
pretrained/pretrain: é è¨“ç·´
Python data structures (e.g., list, set, dict): ç¿»è­¯ç‚ºä¸²åˆ—ï¼Œé›†åˆï¼Œå­—å…¸ï¼Œä¸¦ç”¨æ‹¬è™Ÿæ¨™è¨»åŸè‹±æ–‡
repository: repositoryï¼ˆä¸ç¿»è­¯ï¼‰
summary: æ¦‚è¦½
token-: token-ï¼ˆä¸ç¿»è­¯ï¼‰
Trainer: Trainerï¼ˆä¸ç¿»è­¯ï¼‰
transformer: transformerï¼ˆä¸ç¿»è­¯ï¼‰
tutorial: æ•™å­¸
user: ä½¿ç”¨è€…
-->

<p align="center">
    <br>
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers_logo_name.png" width="400"/>
    <br>
</p>
<p align="center">
    <a href="https://circleci.com/gh/huggingface/transformers"><img alt="Build" src="https://img.shields.io/circleci/build/github/huggingface/transformers/main"></a>
    <a href="https://github.com/huggingface/transformers/blob/main/LICENSE"><img alt="GitHub" src="https://img.shields.io/github/license/huggingface/transformers.svg?color=blue"></a>
    <a href="https://huggingface.co/docs/transformers/index"><img alt="Documentation" src="https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online"></a>
    <a href="https://github.com/huggingface/transformers/releases"><img alt="GitHub release" src="https://img.shields.io/github/release/huggingface/transformers.svg"></a>
    <a href="https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md"><img alt="Contributor Covenant" src="https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg"></a>
    <a href="https://zenodo.org/badge/latestdoi/155220641"><img src="https://zenodo.org/badge/155220641.svg" alt="DOI"></a>
</p>

<h4 align="center">
    <p>
        <a href="https://github.com/huggingface/transformers/">English</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hans.md">ç®€ä½“ä¸­æ–‡</a> |
        <b>ç¹é«”ä¸­æ–‡</b> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ko.md">í•œêµ­ì–´</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_es.md">EspaÃ±ol</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ja.md">æ—¥æœ¬èª</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_hd.md">à¤¹à¤¿à¤¨à¥à¤¦à¥€</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ru.md">Ğ ÑƒÑÑĞºĞ¸Ğ¹</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md">Ğ ortuguÃªs</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_te.md">à°¤à±†à°²à±à°—à±</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_fr.md">FranÃ§ais</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_de.md">Deutsch</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_vi.md">Tiáº¿ng Viá»‡t</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ar.md">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ur.md">Ø§Ø±Ø¯Ùˆ</a> |
    </p>
</h4>

<h3 align="center">
    <p>ç‚º Jaxã€PyTorch ä»¥åŠ TensorFlow æ‰“é€ çš„å…ˆé€²è‡ªç„¶èªè¨€è™•ç†å‡½å¼åº«</p>
</h3>

<h3 align="center">
    <a href="https://hf.co/course"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png"></a>
</h3>

ğŸ¤— Transformers æä¾›äº†æ•¸ä»¥åƒè¨ˆçš„é è¨“ç·´æ¨¡å‹ï¼Œæ”¯æ´ 100 å¤šç¨®èªè¨€çš„æ–‡æœ¬åˆ†é¡ã€è³‡è¨Šæ“·å–ã€å•ç­”ã€æ‘˜è¦ã€ç¿»è­¯ã€æ–‡æœ¬ç”Ÿæˆã€‚å®ƒçš„å®—æ—¨æ˜¯è®“æœ€å…ˆé€²çš„ NLP æŠ€è¡“äººäººæ˜“ç”¨ã€‚

ğŸ¤— Transformers æä¾›äº†ä¾¿æ–¼å¿«é€Ÿä¸‹è¼‰å’Œä½¿ç”¨çš„APIï¼Œè®“ä½ å¯ä»¥å°‡é è¨“ç·´æ¨¡å‹ç”¨åœ¨çµ¦å®šæ–‡æœ¬ã€åœ¨ä½ çš„è³‡æ–™é›†ä¸Šå¾®èª¿ç„¶å¾Œç¶“ç”± [model hub](https://huggingface.co/models) èˆ‡ç¤¾ç¾¤å…±äº«ã€‚åŒæ™‚ï¼Œæ¯å€‹å®šç¾©çš„ Python æ¨¡çµ„æ¶æ§‹å‡å®Œå…¨ç¨ç«‹ï¼Œæ–¹ä¾¿ä¿®æ”¹å’Œå¿«é€Ÿç ”ç©¶å¯¦é©—ã€‚

ğŸ¤— Transformers æ”¯æ´ä¸‰å€‹æœ€ç†±é–€çš„æ·±åº¦å­¸ç¿’å‡½å¼åº«ï¼š [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) ä»¥åŠ [TensorFlow](https://www.tensorflow.org/) â€” ä¸¦èˆ‡ä¹‹å®Œç¾æ•´åˆã€‚ä½ å¯ä»¥ç›´æ¥ä½¿ç”¨å…¶ä¸­ä¸€å€‹æ¡†æ¶è¨“ç·´ä½ çš„æ¨¡å‹ï¼Œç„¶å¾Œç”¨å¦ä¸€å€‹è¼‰å…¥å’Œæ¨è«–ã€‚

## ç·šä¸ŠDemo

ä½ å¯ä»¥ç›´æ¥åœ¨ [model hub](https://huggingface.co/models) ä¸Šæ¸¬è©¦å¤§å¤šæ•¸çš„æ¨¡å‹ã€‚æˆ‘å€‘ä¹Ÿæä¾›äº† [ç§æœ‰æ¨¡å‹è¨—ç®¡ã€æ¨¡å‹ç‰ˆæœ¬ç®¡ç†ä»¥åŠæ¨è«–API](https://huggingface.co/pricing)ã€‚

é€™è£¡æ˜¯ä¸€äº›ç¯„ä¾‹ï¼š
- [ç”¨ BERT åšé®è“‹å¡«è©](https://huggingface.co/google-bert/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France)
- [ç”¨ Electra åšå°ˆæœ‰åè©è¾¨è­˜](https://huggingface.co/dbmdz/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city)
- [ç”¨ GPT-2 åšæ–‡æœ¬ç”Ÿæˆ](https://huggingface.co/openai-community/gpt2?text=A+long+time+ago%2C+)
- [ç”¨ RoBERTa åšè‡ªç„¶èªè¨€æ¨è«–](https://huggingface.co/FacebookAI/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal)
- [ç”¨ BART åšæ–‡æœ¬æ‘˜è¦](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+was+the+first+structure+to+reach+a+height+of+300+metres.+Due+to+the+addition+of+a+broadcasting+aerial+at+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct)
- [ç”¨ DistilBERT åšå•ç­”](https://huggingface.co/distilbert/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species)
- [ç”¨ T5 åšç¿»è­¯](https://huggingface.co/google-t5/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)

**[Write With Transformer](https://transformer.huggingface.co)**ï¼Œç”± Hugging Face åœ˜éšŠæ‰€æ‰“é€ ï¼Œæ˜¯ä¸€å€‹æ–‡æœ¬ç”Ÿæˆçš„å®˜æ–¹ demoã€‚

## å¦‚æœä½ åœ¨å°‹æ‰¾ç”± Hugging Face åœ˜éšŠæ‰€æä¾›çš„å®¢è£½åŒ–æ”¯æ´æœå‹™

<a target="_blank" href="https://huggingface.co/support">
    <img alt="HuggingFace Expert Acceleration Program" src="https://huggingface.co/front/thumbnails/support.png" style="max-width: 600px; border: 1px solid #eee; border-radius: 4px; box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);">
</a><br>

## å¿«é€Ÿä¸Šæ‰‹

æˆ‘å€‘ç‚ºå¿«é€Ÿä½¿ç”¨æ¨¡å‹æä¾›äº† `pipeline` APIã€‚ Pipeline åŒ…å«äº†é è¨“ç·´æ¨¡å‹å’Œå°æ‡‰çš„æ–‡æœ¬é è™•ç†ã€‚ä¸‹é¢æ˜¯ä¸€å€‹å¿«é€Ÿä½¿ç”¨ pipeline å»åˆ¤æ–·æ­£è² é¢æƒ…ç·’çš„ä¾‹å­ï¼š

```python
>>> from transformers import pipeline

# ä½¿ç”¨æƒ…ç·’åˆ†æ pipeline
>>> classifier = pipeline('sentiment-analysis')
>>> classifier('We are very happy to introduce pipeline to the transformers repository.')
[{'label': 'POSITIVE', 'score': 0.9996980428695679}]
```

ç¬¬äºŒè¡Œç¨‹å¼ç¢¼ä¸‹è¼‰ä¸¦å¿«å– pipeline ä½¿ç”¨çš„é è¨“ç·´æ¨¡å‹ï¼Œè€Œç¬¬ä¸‰è¡Œç¨‹å¼ç¢¼å‰‡åœ¨çµ¦å®šçš„æ–‡æœ¬ä¸Šé€²è¡Œäº†è©•ä¼°ã€‚é€™è£¡çš„ç­”æ¡ˆâ€œæ­£é¢â€ (positive) å…·æœ‰ 99.97% çš„ä¿¡è³´åº¦ã€‚

è¨±å¤šçš„ NLP ä»»å‹™éƒ½æœ‰éš¨é¸å³ç”¨çš„é è¨“ç·´ `pipeline`ã€‚ä¾‹å¦‚ï¼Œæˆ‘å€‘å¯ä»¥è¼•é¬†åœ°å¾çµ¦å®šæ–‡æœ¬ä¸­æ“·å–å•é¡Œç­”æ¡ˆï¼š

``` python
>>> from transformers import pipeline

# ä½¿ç”¨å•ç­” pipeline
>>> question_answerer = pipeline('question-answering')
>>> question_answerer({
...     'question': 'What is the name of the repository ?',
...     'context': 'Pipeline has been included in the huggingface/transformers repository'
... })
{'score': 0.30970096588134766, 'start': 34, 'end': 58, 'answer': 'huggingface/transformers'}

```

é™¤äº†æä¾›å•é¡Œè§£ç­”ï¼Œé è¨“ç·´æ¨¡å‹é‚„æä¾›äº†å°æ‡‰çš„ä¿¡è³´åº¦åˆ†æ•¸ä»¥åŠè§£ç­”åœ¨ tokenized å¾Œçš„æ–‡æœ¬ä¸­é–‹å§‹å’ŒçµæŸçš„ä½ç½®ã€‚ä½ å¯ä»¥å¾[é€™å€‹æ•™å­¸](https://huggingface.co/docs/transformers/task_summary)äº†è§£æ›´å¤š `pipeline` APIæ”¯æ´çš„ä»»å‹™ã€‚

è¦åœ¨ä½ çš„ä»»å‹™ä¸­ä¸‹è¼‰å’Œä½¿ç”¨ä»»ä½•é è¨“ç·´æ¨¡å‹å¾ˆç°¡å–®ï¼Œåªéœ€ä¸‰è¡Œç¨‹å¼ç¢¼ã€‚é€™è£¡æ˜¯ PyTorch ç‰ˆçš„ç¯„ä¾‹ï¼š
```python
>>> from transformers import AutoTokenizer, AutoModel

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
>>> model = AutoModel.from_pretrained("google-bert/bert-base-uncased")

>>> inputs = tokenizer("Hello world!", return_tensors="pt")
>>> outputs = model(**inputs)
```
é€™è£¡æ˜¯å°æ‡‰çš„ TensorFlow ç¨‹å¼ç¢¼ï¼š
```python
>>> from transformers import AutoTokenizer, TFAutoModel

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
>>> model = TFAutoModel.from_pretrained("google-bert/bert-base-uncased")

>>> inputs = tokenizer("Hello world!", return_tensors="tf")
>>> outputs = model(**inputs)
```

Tokenizer ç‚ºæ‰€æœ‰çš„é è¨“ç·´æ¨¡å‹æä¾›äº†é è™•ç†ï¼Œä¸¦å¯ä»¥ç›´æ¥è½‰æ›å–®ä¸€å­—ä¸²ï¼ˆæ¯”å¦‚ä¸Šé¢çš„ä¾‹å­ï¼‰æˆ–ä¸²åˆ— (list)ã€‚å®ƒæœƒè¼¸å‡ºä¸€å€‹çš„å­—å…¸ (dict) è®“ä½ å¯ä»¥åœ¨ä¸‹æ¸¸ç¨‹å¼ç¢¼è£¡ä½¿ç”¨æˆ–ç›´æ¥è—‰ç”± `**` é‹ç®—å¼å‚³çµ¦æ¨¡å‹ã€‚

æ¨¡å‹æœ¬èº«æ˜¯ä¸€å€‹å¸¸è¦çš„ [Pytorch `nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) æˆ– [TensorFlow `tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)ï¼ˆå–æ±ºæ–¼ä½ çš„å¾Œç«¯ï¼‰ï¼Œå¯ä¾å¸¸è¦æ–¹å¼ä½¿ç”¨ã€‚ [é€™å€‹æ•™å­¸](https://huggingface.co/transformers/training.html)è§£é‡‹äº†å¦‚ä½•å°‡é€™æ¨£çš„æ¨¡å‹æ•´åˆåˆ°ä¸€èˆ¬çš„ PyTorch æˆ– TensorFlow è¨“ç·´è¿´åœˆä¸­ï¼Œæˆ–æ˜¯å¦‚ä½•ä½¿ç”¨æˆ‘å€‘çš„ `Trainer` API åœ¨ä¸€å€‹æ–°çš„è³‡æ–™é›†ä¸Šå¿«é€Ÿé€²è¡Œå¾®èª¿ã€‚

## ç‚ºä»€éº¼è¦ç”¨ transformersï¼Ÿ

1. ä¾¿æ–¼ä½¿ç”¨çš„å…ˆé€²æ¨¡å‹ï¼š
    - NLU å’Œ NLG ä¸Šæ€§èƒ½å“è¶Š
    - å°æ•™å­¸å’Œå¯¦ä½œå‹å¥½ä¸”ä½é–€æª»
    - é«˜åº¦æŠ½è±¡ï¼Œä½¿ç”¨è€…åªé ˆå­¸ç¿’ 3 å€‹é¡åˆ¥
    - å°æ‰€æœ‰æ¨¡å‹ä½¿ç”¨çš„åˆ¶å¼åŒ–API

1. æ›´ä½çš„é‹ç®—æˆæœ¬ï¼Œæ›´å°‘çš„ç¢³æ’æ”¾ï¼š
    - ç ”ç©¶äººå“¡å¯ä»¥åˆ†äº«å·²è¨“ç·´çš„æ¨¡å‹è€Œéæ¯æ¬¡å¾é ­é–‹å§‹è¨“ç·´
    - å·¥ç¨‹å¸«å¯ä»¥æ¸›å°‘è¨ˆç®—æ™‚é–“ä»¥åŠç”Ÿç”¢æˆæœ¬
    - æ•¸åç¨®æ¨¡å‹æ¶æ§‹ã€å…©åƒå¤šå€‹é è¨“ç·´æ¨¡å‹ã€100å¤šç¨®èªè¨€æ”¯æ´

1. å°æ–¼æ¨¡å‹ç”Ÿå‘½é€±æœŸçš„æ¯ä¸€å€‹éƒ¨åˆ†éƒ½é¢é¢ä¿±åˆ°ï¼š
    - è¨“ç·´å…ˆé€²çš„æ¨¡å‹ï¼Œåªéœ€ 3 è¡Œç¨‹å¼ç¢¼
    - æ¨¡å‹å¯ä»¥åœ¨ä¸åŒæ·±åº¦å­¸ç¿’æ¡†æ¶ä¹‹é–“ä»»æ„è½‰æ›
    - ç‚ºè¨“ç·´ã€è©•ä¼°å’Œç”Ÿç”¢é¸æ“‡æœ€é©åˆçš„æ¡†æ¶ï¼Œä¸¦å®Œç¾éŠœæ¥

1. ç‚ºä½ çš„éœ€æ±‚è¼•é¬†å®¢è£½åŒ–å°ˆå±¬æ¨¡å‹å’Œç¯„ä¾‹ï¼š
    - æˆ‘å€‘ç‚ºæ¯ç¨®æ¨¡å‹æ¶æ§‹æä¾›äº†å¤šå€‹ç¯„ä¾‹ä¾†é‡ç¾åŸè«–æ–‡çµæœ
    - ä¸€è‡´çš„æ¨¡å‹å…§éƒ¨æ¶æ§‹
    - æ¨¡å‹æª”æ¡ˆå¯å–®ç¨ä½¿ç”¨ï¼Œä¾¿æ–¼ä¿®æ”¹å’Œå¿«é€Ÿå¯¦é©—

## ä»€éº¼æƒ…æ³ä¸‹æˆ‘ä¸è©²ç”¨ transformersï¼Ÿ

- æœ¬å‡½å¼åº«ä¸¦ä¸æ˜¯æ¨¡çµ„åŒ–çš„ç¥ç¶“ç¶²çµ¡å·¥å…·ç®±ã€‚æ¨¡å‹æ–‡ä»¶ä¸­çš„ç¨‹å¼ç¢¼ä¸¦æœªåšé¡å¤–çš„æŠ½è±¡å°è£ï¼Œä»¥ä¾¿ç ”ç©¶äººå“¡å¿«é€Ÿåœ°ç¿»é–±åŠä¿®æ”¹ç¨‹å¼ç¢¼ï¼Œè€Œä¸æœƒæ·±é™·è¤‡é›œçš„é¡åˆ¥åŒ…è£ä¹‹ä¸­ã€‚
- `Trainer` API ä¸¦éç›¸å®¹ä»»ä½•æ¨¡å‹ï¼Œå®ƒåªç‚ºæœ¬å‡½å¼åº«ä¸­çš„æ¨¡å‹æœ€ä½³åŒ–ã€‚å°æ–¼ä¸€èˆ¬çš„æ©Ÿå™¨å­¸ç¿’ç”¨é€”ï¼Œè«‹ä½¿ç”¨å…¶ä»–å‡½å¼åº«ã€‚
- å„˜ç®¡æˆ‘å€‘å·²ç›¡åŠ›è€Œç‚ºï¼Œ[examples ç›®éŒ„](https://github.com/huggingface/transformers/tree/main/examples)ä¸­çš„è…³æœ¬ä¹Ÿåƒ…ç‚ºç¯„ä¾‹è€Œå·²ã€‚å°æ–¼ç‰¹å®šå•é¡Œï¼Œå®ƒå€‘ä¸¦ä¸ä¸€å®šéš¨é¸å³ç”¨ï¼Œå¯èƒ½éœ€è¦ä¿®æ”¹å¹¾è¡Œç¨‹å¼ç¢¼ä»¥ç¬¦åˆéœ€æ±‚ã€‚

## å®‰è£

### ä½¿ç”¨ pip

é€™å€‹ Repository å·²åœ¨ Python 3.8+ã€Flax 0.4.1+ã€PyTorch 1.11+ å’Œ TensorFlow 2.6+ ä¸‹ç¶“éæ¸¬è©¦ã€‚

ä½ å¯ä»¥åœ¨[è™›æ“¬ç’°å¢ƒ](https://docs.python.org/3/library/venv.html)ä¸­å®‰è£ ğŸ¤— Transformersã€‚å¦‚æœä½ é‚„ä¸ç†Ÿæ‚‰ Python çš„è™›æ“¬ç’°å¢ƒï¼Œè«‹é–±æ­¤[ä½¿ç”¨è€…æŒ‡å¼•](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/)ã€‚

é¦–å…ˆï¼Œç”¨ä½ æ‰“ç®—ä½¿ç”¨çš„ç‰ˆæœ¬çš„ Python å‰µå»ºä¸€å€‹è™›æ“¬ç’°å¢ƒä¸¦é€²å…¥ã€‚

ç„¶å¾Œï¼Œä½ éœ€è¦å®‰è£ Flaxã€PyTorch æˆ– TensorFlow å…¶ä¸­ä¹‹ä¸€ã€‚å°æ–¼è©²å¦‚ä½•åœ¨ä½ ä½¿ç”¨çš„å¹³å°ä¸Šå®‰è£é€™äº›æ¡†æ¶ï¼Œè«‹åƒé–± [TensorFlow å®‰è£é é¢](https://www.tensorflow.org/install/), [PyTorch å®‰è£é é¢](https://pytorch.org/get-started/locally/#start-locally) æˆ– [Flax å®‰è£é é¢](https://github.com/google/flax#quick-install)ã€‚

ç•¶å…¶ä¸­ä¸€å€‹å¾Œç«¯å®‰è£æˆåŠŸå¾Œï¼ŒğŸ¤— Transformers å¯ä¾æ­¤å®‰è£ï¼š

```bash
pip install transformers
```

å¦‚æœä½ æƒ³è¦è©¦è©¦ç¯„ä¾‹æˆ–è€…æƒ³åœ¨æ­£å¼ç™¼å¸ƒå‰ä½¿ç”¨æœ€æ–°é–‹ç™¼ä¸­çš„ç¨‹å¼ç¢¼ï¼Œä½ å¿…é ˆ[å¾åŸå§‹ç¢¼å®‰è£](https://huggingface.co/docs/transformers/installation#installing-from-source)ã€‚

### ä½¿ç”¨ conda

ğŸ¤— Transformers å¯ä»¥è—‰ç”± conda ä¾æ­¤å®‰è£ï¼š

```shell script
conda install conda-forge::transformers
```

> **_ç­†è¨˜:_** å¾ `huggingface` é »é“å®‰è£ `transformers` å·²è¢«æ·˜æ±°ã€‚

è¦è—‰ç”± conda å®‰è£ Flaxã€PyTorch æˆ– TensorFlow å…¶ä¸­ä¹‹ä¸€ï¼Œè«‹åƒé–±å®ƒå€‘å„è‡ªå®‰è£é é¢çš„èªªæ˜ã€‚

## æ¨¡å‹æ¶æ§‹

**ğŸ¤— Transformers æ”¯æ´çš„[æ‰€æœ‰çš„æ¨¡å‹æª¢æŸ¥é»](https://huggingface.co/models)**ï¼Œç”±[ä½¿ç”¨è€…](https://huggingface.co/users)å’Œ[çµ„ç¹”](https://huggingface.co/organizations)ä¸Šå‚³ï¼Œå‡èˆ‡ huggingface.co [model hub](https://huggingface.co) å®Œç¾çµåˆã€‚

ç›®å‰çš„æª¢æŸ¥é»æ•¸é‡ï¼š ![](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen)

ğŸ¤— Transformers ç›®å‰æ”¯æ´ä»¥ä¸‹çš„æ¶æ§‹: æ¨¡å‹æ¦‚è¦½è«‹åƒé–±[é€™è£¡](https://huggingface.co/docs/transformers/model_summary).

è¦æª¢æŸ¥æŸå€‹æ¨¡å‹æ˜¯å¦å·²æœ‰ Flaxã€PyTorch æˆ– TensorFlow çš„å¯¦ä½œï¼Œæˆ–å…¶æ˜¯å¦åœ¨ğŸ¤— Tokenizers å‡½å¼åº«ä¸­æœ‰å°æ‡‰çš„ tokenizerï¼Œæ•¬è«‹åƒé–±[æ­¤è¡¨](https://huggingface.co/docs/transformers/index#supported-frameworks)ã€‚

é€™äº›å¯¦ä½œå‡å·²æ–¼å¤šå€‹è³‡æ–™é›†æ¸¬è©¦ï¼ˆè«‹åƒé–±ç¯„ä¾‹è…³æœ¬ï¼‰ä¸¦æ‡‰èˆ‡åŸç‰ˆå¯¦ä½œè¡¨ç¾ç›¸ç•¶ã€‚ä½ å¯ä»¥åœ¨ç¯„ä¾‹æ–‡ä»¶çš„[æ­¤ç¯€](https://huggingface.co/docs/transformers/examples)ä¸­äº†è§£å¯¦ä½œçš„ç´°ç¯€ã€‚


## äº†è§£æ›´å¤š

| ç« ç¯€ | æè¿° |
|-|-|
| [æ–‡ä»¶](https://huggingface.co/transformers/) | å®Œæ•´çš„ API æ–‡ä»¶å’Œæ•™å­¸ |
| [ä»»å‹™æ¦‚è¦½](https://huggingface.co/docs/transformers/task_summary) | ğŸ¤— Transformers æ”¯æ´çš„ä»»å‹™ |
| [é è™•ç†æ•™å­¸](https://huggingface.co/docs/transformers/preprocessing) | ä½¿ç”¨ `Tokenizer` ä¾†ç‚ºæ¨¡å‹æº–å‚™è³‡æ–™ |
| [è¨“ç·´å’Œå¾®èª¿](https://huggingface.co/docs/transformers/training) | ä½¿ç”¨ PyTorch/TensorFlow çš„å…§å»ºçš„è¨“ç·´æ–¹å¼æˆ–æ–¼ `Trainer` API ä¸­ä½¿ç”¨ ğŸ¤— Transformers æä¾›çš„æ¨¡å‹ |
| [å¿«é€Ÿä¸Šæ‰‹ï¼šå¾®èª¿å’Œç¯„ä¾‹è…³æœ¬](https://github.com/huggingface/transformers/tree/main/examples) | ç‚ºå„ç¨®ä»»å‹™æä¾›çš„ç¯„ä¾‹è…³æœ¬ |
| [æ¨¡å‹åˆ†äº«å’Œä¸Šå‚³](https://huggingface.co/docs/transformers/model_sharing) | ä¸Šå‚³ä¸¦èˆ‡ç¤¾ç¾¤åˆ†äº«ä½ å¾®èª¿çš„æ¨¡å‹ |
| [é·ç§»](https://huggingface.co/docs/transformers/migration) | å¾ `pytorch-transformers` æˆ– `pytorch-pretrained-bert` é·ç§»åˆ° ğŸ¤— Transformers |

## å¼•ç”¨

æˆ‘å€‘å·²å°‡æ­¤å‡½å¼åº«çš„[è«–æ–‡](https://www.aclweb.org/anthology/2020.emnlp-demos.6/)æ­£å¼ç™¼è¡¨ã€‚å¦‚æœä½ ä½¿ç”¨äº† ğŸ¤— Transformers å‡½å¼åº«ï¼Œå¯ä»¥å¼•ç”¨ï¼š
```bibtex
@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and RÃ©mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}
```
