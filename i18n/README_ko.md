<!---
Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

<p align="center">
    <br>
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers_logo_name.png" width="400"/>
    <br>
</p>
<p align="center">
    <a href="https://circleci.com/gh/huggingface/transformers"><img alt="Build" src="https://img.shields.io/circleci/build/github/huggingface/transformers/main"></a>
    <a href="https://github.com/huggingface/transformers/blob/main/LICENSE"><img alt="GitHub" src="https://img.shields.io/github/license/huggingface/transformers.svg?color=blue"></a>
    <a href="https://huggingface.co/docs/transformers/index"><img alt="Documentation" src="https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online"></a>
    <a href="https://github.com/huggingface/transformers/releases"><img alt="GitHub release" src="https://img.shields.io/github/release/huggingface/transformers.svg"></a>
    <a href="https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md"><img alt="Contributor Covenant" src="https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg"></a>
    <a href="https://zenodo.org/badge/latestdoi/155220641"><img src="https://zenodo.org/badge/155220641.svg" alt="DOI"></a>
</p>

<h4 align="center">
    <p>
        <a href="https://github.com/huggingface/transformers/">English</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hans.md">ç®€ä½“ä¸­æ–‡</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hant.md">ç¹é«”ä¸­æ–‡</a> |
        <b>í•œêµ­ì–´</b> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_es.md">EspaÃ±ol</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ja.md">æ—¥æœ¬èª</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_hd.md">à¤¹à¤¿à¤¨à¥à¤¦à¥€</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ru.md">Ğ ÑƒÑÑĞºĞ¸Ğ¹</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md">Ğ ortuguÃªs</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_te.md">à°¤à±†à°²à±à°—à±</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_fr.md">FranÃ§ais</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_de.md">Deutsch</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_vi.md">Tiáº¿ng Viá»‡t</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ar.md">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ur.md">Ø§Ø±Ø¯Ùˆ</a> |

    </p>
</h4>

<h3 align="center">
    <p> Jax, Pytorch, TensorFlowë¥¼ ìœ„í•œ ìµœì²¨ë‹¨ ìì—°ì–´ì²˜ë¦¬</p>
</h3>

<h3 align="center">
    <a href="https://hf.co/course"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png"></a>
</h3>

ğŸ¤— TransformersëŠ” ë¶„ë¥˜, ì •ë³´ ì¶”ì¶œ, ì§ˆë¬¸ ë‹µë³€, ìš”ì•½, ë²ˆì—­, ë¬¸ì¥ ìƒì„± ë“±ì„ 100ê°œ ì´ìƒì˜ ì–¸ì–´ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ìˆ˜ì²œê°œì˜ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì„ ì œê³µí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ëª©í‘œëŠ” ëª¨ë‘ê°€ ìµœì²¨ë‹¨ì˜ NLP ê¸°ìˆ ì„ ì‰½ê²Œ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ğŸ¤— TransformersëŠ” ì´ëŸ¬í•œ ì‚¬ì „í•™ìŠµ ëª¨ë¸ì„ ë¹ ë¥´ê²Œ ë‹¤ìš´ë¡œë“œí•´ íŠ¹ì • í…ìŠ¤íŠ¸ì— ì‚¬ìš©í•˜ê³ , ì›í•˜ëŠ” ë°ì´í„°ë¡œ fine-tuningí•´ ì»¤ë®¤ë‹ˆí‹°ë‚˜ ìš°ë¦¬ì˜ [ëª¨ë¸ í—ˆë¸Œ](https://huggingface.co/models)ì— ê³µìœ í•  ìˆ˜ ìˆë„ë¡ APIë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë˜í•œ, ëª¨ë¸ êµ¬ì¡°ë¥¼ ì •ì˜í•˜ëŠ” ê° íŒŒì´ì¬ ëª¨ë“ˆì€ ì™„ì „íˆ ë…ë¦½ì ì´ì—¬ì„œ ì—°êµ¬ ì‹¤í—˜ì„ ìœ„í•´ ì†ì‰½ê²Œ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ¤— TransformersëŠ” ê°€ì¥ ìœ ëª…í•œ 3ê°œì˜ ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. ì´ë“¤ì€ ì„œë¡œ ì™„ë²½íˆ ì—°ë™ë©ë‹ˆë‹¤ â€” [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/), [TensorFlow](https://www.tensorflow.org/). ê°„ë‹¨í•˜ê²Œ ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¤‘ í•˜ë‚˜ë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ê³ , ë˜ ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì¶”ë¡ ì„ ìœ„í•´ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ì˜¨ë¼ì¸ ë°ëª¨

ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ì„ [ëª¨ë¸ í—ˆë¸Œ](https://huggingface.co/models) í˜ì´ì§€ì—ì„œ ë°”ë¡œ í…ŒìŠ¤íŠ¸í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê³µê°œ ë° ë¹„ê³µê°œ ëª¨ë¸ì„ ìœ„í•œ [ë¹„ê³µê°œ ëª¨ë¸ í˜¸ìŠ¤íŒ…, ë²„ì „ ê´€ë¦¬, ì¶”ë¡  API](https://huggingface.co/pricing)ë„ ì œê³µí•©ë‹ˆë‹¤.

ì˜ˆì‹œ:
- [BERTë¡œ ë§ˆìŠ¤í‚¹ëœ ë‹¨ì–´ ì™„ì„±í•˜ê¸°](https://huggingface.co/google-bert/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France)
- [Electraë¥¼ ì´ìš©í•œ ê°œì²´ëª… ì¸ì‹](https://huggingface.co/dbmdz/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city)
- [GPT-2ë¡œ í…ìŠ¤íŠ¸ ìƒì„±í•˜ê¸°](https://huggingface.co/openai-community/gpt2?text=A+long+time+ago%2C+)
- [RoBERTaë¡œ ìì—°ì–´ ì¶”ë¡ í•˜ê¸°](https://huggingface.co/FacebookAI/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal)
- [BARTë¥¼ ì´ìš©í•œ ìš”ì•½](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+was+the+first+structure+to+reach+a+height+of+300+metres.+Due+to+the+addition+of+a+broadcasting+aerial+at+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct)
- [DistilBERTë¥¼ ì´ìš©í•œ ì§ˆë¬¸ ë‹µë³€](https://huggingface.co/distilbert/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species)
- [T5ë¡œ ë²ˆì—­í•˜ê¸°](https://huggingface.co/google-t5/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)

**[Transformerì™€ ê¸€ì“°ê¸°](https://transformer.huggingface.co)** ëŠ” ì´ ì €ì¥ì†Œì˜ í…ìŠ¤íŠ¸ ìƒì„± ëŠ¥ë ¥ì— ê´€í•œ Hugging Face íŒ€ì˜ ê³µì‹ ë°ëª¨ì…ë‹ˆë‹¤.

## Hugging Face íŒ€ì˜ ì»¤ìŠ¤í…€ ì§€ì›ì„ ì›í•œë‹¤ë©´

<a target="_blank" href="https://huggingface.co/support">
    <img alt="HuggingFace Expert Acceleration Program" src="https://huggingface.co/front/thumbnails/support.png" style="max-width: 600px; border: 1px solid #eee; border-radius: 4px; box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);">
</a><br>

## í€µ íˆ¬ì–´

ì›í•˜ëŠ” í…ìŠ¤íŠ¸ì— ë°”ë¡œ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡, ìš°ë¦¬ëŠ” `pipeline` APIë¥¼ ì œê³µí•©ë‹ˆë‹¤. Pipelineì€ ì‚¬ì „í•™ìŠµ ëª¨ë¸ê³¼ ê·¸ ëª¨ë¸ì„ í•™ìŠµí•  ë•Œ ì ìš©í•œ ì „ì²˜ë¦¬ ë°©ì‹ì„ í•˜ë‚˜ë¡œ í•©ì¹©ë‹ˆë‹¤. ë‹¤ìŒì€ ê¸ì •ì ì¸ í…ìŠ¤íŠ¸ì™€ ë¶€ì •ì ì¸ í…ìŠ¤íŠ¸ë¥¼ ë¶„ë¥˜í•˜ê¸° ìœ„í•´ pipelineì„ ì‚¬ìš©í•œ ê°„ë‹¨í•œ ì˜ˆì‹œì…ë‹ˆë‹¤:

```python
>>> from transformers import pipeline

# Allocate a pipeline for sentiment-analysis
>>> classifier = pipeline('sentiment-analysis')
>>> classifier('We are very happy to introduce pipeline to the transformers repository.')
[{'label': 'POSITIVE', 'score': 0.9996980428695679}]
```

ì½”ë“œì˜ ë‘ë²ˆì§¸ ì¤„ì€ pipelineì´ ì‚¬ìš©í•˜ëŠ” ì‚¬ì „í•™ìŠµ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ìºì‹œë¡œ ì €ì¥í•©ë‹ˆë‹¤. ì„¸ë²ˆì§¸ ì¤„ì—ì„  ê·¸ ëª¨ë¸ì´ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ í‰ê°€í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ ëª¨ë¸ì€ 99.97%ì˜ í™•ë¥ ë¡œ í…ìŠ¤íŠ¸ê°€ ê¸ì •ì ì´ë¼ê³  í‰ê°€í–ˆìŠµë‹ˆë‹¤.

ë§ì€ NLP ê³¼ì œë“¤ì„ `pipeline`ìœ¼ë¡œ ë°”ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì§ˆë¬¸ê³¼ ë¬¸ë§¥ì´ ì£¼ì–´ì§€ë©´ ì†ì‰½ê²Œ ë‹µë³€ì„ ì¶”ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

``` python
>>> from transformers import pipeline

# Allocate a pipeline for question-answering
>>> question_answerer = pipeline('question-answering')
>>> question_answerer({
...     'question': 'What is the name of the repository ?',
...     'context': 'Pipeline has been included in the huggingface/transformers repository'
... })
{'score': 0.30970096588134766, 'start': 34, 'end': 58, 'answer': 'huggingface/transformers'}

```

ë‹µë³€ë¿ë§Œ ì•„ë‹ˆë¼, ì—¬ê¸°ì— ì‚¬ìš©ëœ ì‚¬ì „í•™ìŠµ ëª¨ë¸ì€ í™•ì‹ ë„ì™€ í† í¬ë‚˜ì´ì¦ˆëœ ë¬¸ì¥ ì† ë‹µë³€ì˜ ì‹œì‘ì , ëì ê¹Œì§€ ë°˜í™˜í•©ë‹ˆë‹¤. [ì´ íŠœí† ë¦¬ì–¼](https://huggingface.co/docs/transformers/task_summary)ì—ì„œ `pipeline` APIê°€ ì§€ì›í•˜ëŠ” ë‹¤ì–‘í•œ ê³¼ì œë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì½”ë“œ 3ì¤„ë¡œ ì›í•˜ëŠ” ê³¼ì œì— ë§ê²Œ ì‚¬ì „í•™ìŠµ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œ ë°›ê³  ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒì€ PyTorch ë²„ì „ì…ë‹ˆë‹¤:
```python
>>> from transformers import AutoTokenizer, AutoModel

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
>>> model = AutoModel.from_pretrained("google-bert/bert-base-uncased")

>>> inputs = tokenizer("Hello world!", return_tensors="pt")
>>> outputs = model(**inputs)
```
ë‹¤ìŒì€ TensorFlow ë²„ì „ì…ë‹ˆë‹¤:
```python
>>> from transformers import AutoTokenizer, TFAutoModel

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
>>> model = TFAutoModel.from_pretrained("google-bert/bert-base-uncased")

>>> inputs = tokenizer("Hello world!", return_tensors="tf")
>>> outputs = model(**inputs)
```

í† í¬ë‚˜ì´ì €ëŠ” ì‚¬ì „í•™ìŠµ ëª¨ë¸ì˜ ëª¨ë“  ì „ì²˜ë¦¬ë¥¼ ì±…ì„ì§‘ë‹ˆë‹¤. ê·¸ë¦¬ê³  (ìœ„ì˜ ì˜ˆì‹œì²˜ëŸ¼) 1ê°œì˜ ìŠ¤íŠ¸ë§ì´ë‚˜ ë¦¬ìŠ¤íŠ¸ë„ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í† í¬ë‚˜ì´ì €ëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•˜ëŠ”ë°, ì´ëŠ” ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì½”ë“œì— ì‚¬ìš©í•˜ê±°ë‚˜ ì–¸íŒ¨í‚¹ ì—°ì‚°ì ** ë¥¼ ì´ìš©í•´ ëª¨ë¸ì— ë°”ë¡œ ì „ë‹¬í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

ëª¨ë¸ ìì²´ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” [Pytorch `nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)ë‚˜ [TensorFlow `tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)ì…ë‹ˆë‹¤. [ì´ íŠœí† ë¦¬ì–¼](https://huggingface.co/transformers/training.html)ì€ ì´ëŸ¬í•œ ëª¨ë¸ì„ í‘œì¤€ì ì¸ PyTorchë‚˜ TensorFlow í•™ìŠµ ê³¼ì •ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•, ë˜ëŠ” ìƒˆë¡œìš´ ë°ì´í„°ë¡œ fine-tuneí•˜ê¸° ìœ„í•´ `Trainer` APIë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•´ì¤ë‹ˆë‹¤.

## ì™œ transformersë¥¼ ì‚¬ìš©í•´ì•¼ í• ê¹Œìš”?

1. ì†ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ìµœì²¨ë‹¨ ëª¨ë¸:
    - NLUì™€ NLG ê³¼ì œì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.
    - êµìœ¡ì ì‹¤ë¬´ìì—ê²Œ ì§„ì… ì¥ë²½ì´ ë‚®ìŠµë‹ˆë‹¤.
    - 3ê°œì˜ í´ë˜ìŠ¤ë§Œ ë°°ìš°ë©´ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - í•˜ë‚˜ì˜ APIë¡œ ëª¨ë“  ì‚¬ì „í•™ìŠµ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

1. ë” ì ì€ ê³„ì‚° ë¹„ìš©, ë” ì ì€ íƒ„ì†Œ ë°œìêµ­:
    - ì—°êµ¬ìë“¤ì€ ëª¨ë¸ì„ ê³„ì† ë‹¤ì‹œ í•™ìŠµì‹œí‚¤ëŠ” ëŒ€ì‹  í•™ìŠµëœ ëª¨ë¸ì„ ê³µìœ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - ì‹¤ë¬´ìë“¤ì€ í•™ìŠµì— í•„ìš”í•œ ì‹œê°„ê³¼ ë¹„ìš©ì„ ì ˆì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - ìˆ˜ì‹­ê°œì˜ ëª¨ë¸ êµ¬ì¡°, 2,000ê°œ ì´ìƒì˜ ì‚¬ì „í•™ìŠµ ëª¨ë¸, 100ê°œ ì´ìƒì˜ ì–¸ì–´ë¡œ í•™ìŠµëœ ëª¨ë¸ ë“±.

1. ëª¨ë¸ì˜ ê° ìƒì• ì£¼ê¸°ì— ì í•©í•œ í”„ë ˆì„ì›Œí¬:
    - ì½”ë“œ 3ì¤„ë¡œ ìµœì²¨ë‹¨ ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”.
    - ììœ ë¡­ê²Œ ëª¨ë¸ì„ TF2.0ë‚˜ PyTorch í”„ë ˆì„ì›Œí¬ë¡œ ë³€í™˜í•˜ì„¸ìš”.
    - í•™ìŠµ, í‰ê°€, ê³µê°œ ë“± ê° ë‹¨ê³„ì— ë§ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì›í•˜ëŠ”ëŒ€ë¡œ ì„ íƒí•˜ì„¸ìš”.

1. í•„ìš”í•œ ëŒ€ë¡œ ëª¨ë¸ì´ë‚˜ ì˜ˆì‹œë¥¼ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆí•˜ì„¸ìš”:
    - ìš°ë¦¬ëŠ” ì €ìê°€ ê³µê°œí•œ ê²°ê³¼ë¥¼ ì¬í˜„í•˜ê¸° ìœ„í•´ ê° ëª¨ë¸ êµ¬ì¡°ì˜ ì˜ˆì‹œë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    - ëª¨ë¸ ë‚´ë¶€ êµ¬ì¡°ëŠ” ê°€ëŠ¥í•œ ì¼ê´€ì ìœ¼ë¡œ ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤.
    - ë¹ ë¥¸ ì‹¤í—˜ì„ ìœ„í•´ ëª¨ë¸ íŒŒì¼ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ë…ë¦½ì ìœ¼ë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ì™œ transformersë¥¼ ì‚¬ìš©í•˜ì§€ ë§ì•„ì•¼ í• ê¹Œìš”?

- ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ì‹ ê²½ë§ ë¸”ë¡ì„ ë§Œë“¤ê¸° ìœ„í•œ ëª¨ë“ˆì´ ì•„ë‹™ë‹ˆë‹¤. ì—°êµ¬ìë“¤ì´ ì—¬ëŸ¬ íŒŒì¼ì„ ì‚´í´ë³´ì§€ ì•Šê³  ë°”ë¡œ ê° ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡, ëª¨ë¸ íŒŒì¼ ì½”ë“œì˜ ì¶”ìƒí™” ìˆ˜ì¤€ì„ ì ì •í•˜ê²Œ ìœ ì§€í–ˆìŠµë‹ˆë‹¤.
- í•™ìŠµ APIëŠ” ëª¨ë“  ëª¨ë¸ì— ì ìš©í•  ìˆ˜ ìˆë„ë¡ ë§Œë“¤ì–´ì§€ì§„ ì•Šì•˜ì§€ë§Œ, ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì œê³µí•˜ëŠ” ëª¨ë¸ë“¤ì— ì ìš©í•  ìˆ˜ ìˆë„ë¡ ìµœì í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ë¨¸ì‹  ëŸ¬ë‹ì„ ìœ„í•´ì„ , ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
- ê°€ëŠ¥í•œ ë§ì€ ì‚¬ìš© ì˜ˆì‹œë¥¼ ë³´ì—¬ë“œë¦¬ê³  ì‹¶ì–´ì„œ, [ì˜ˆì‹œ í´ë”](https://github.com/huggingface/transformers/tree/main/examples)ì˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤. ì´ ìŠ¤í¬ë¦½íŠ¸ë“¤ì„ ìˆ˜ì • ì—†ì´ íŠ¹ì •í•œ ë¬¸ì œì— ë°”ë¡œ ì ìš©í•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•„ìš”ì— ë§ê²Œ ì¼ë¶€ ì½”ë“œë¥¼ ìˆ˜ì •í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ì„¤ì¹˜

### pipë¡œ ì„¤ì¹˜í•˜ê¸°

ì´ ì €ì¥ì†ŒëŠ” Python 3.8+, Flax 0.4.1+, PyTorch 1.11+, TensorFlow 2.6+ì—ì„œ í…ŒìŠ¤íŠ¸ ë˜ì—ˆìŠµë‹ˆë‹¤.

[ê°€ìƒ í™˜ê²½](https://docs.python.org/3/library/venv.html)ì— ğŸ¤— Transformersë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”. Python ê°€ìƒ í™˜ê²½ì— ìµìˆ™í•˜ì§€ ì•Šë‹¤ë©´, [ì‚¬ìš©ì ê°€ì´ë“œ](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/)ë¥¼ í™•ì¸í•˜ì„¸ìš”.

ìš°ì„ , ì‚¬ìš©í•  Python ë²„ì „ìœ¼ë¡œ ê°€ìƒ í™˜ê²½ì„ ë§Œë“¤ê³  ì‹¤í–‰í•˜ì„¸ìš”.

ê·¸ ë‹¤ìŒ, Flax, PyTorch, TensorFlow ì¤‘ ì ì–´ë„ í•˜ë‚˜ëŠ” ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
í”Œë«í¼ì— ë§ëŠ” ì„¤ì¹˜ ëª…ë ¹ì–´ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ [TensorFlow ì„¤ì¹˜ í˜ì´ì§€](https://www.tensorflow.org/install/), [PyTorch ì„¤ì¹˜ í˜ì´ì§€](https://pytorch.org/get-started/locally/#start-locally), [Flax ì„¤ì¹˜ í˜ì´ì§€](https://github.com/google/flax#quick-install)ë¥¼ í™•ì¸í•˜ì„¸ìš”.

ì´ë“¤ ì¤‘ ì ì–´ë„ í•˜ë‚˜ê°€ ì„¤ì¹˜ë˜ì—ˆë‹¤ë©´, ğŸ¤— TransformersëŠ” ë‹¤ìŒê³¼ ê°™ì´ pipì„ ì´ìš©í•´ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
pip install transformers
```

ì˜ˆì‹œë“¤ì„ ì²´í—˜í•´ë³´ê³  ì‹¶ê±°ë‚˜, ìµœìµœìµœì²¨ë‹¨ ì½”ë“œë¥¼ ì›í•˜ê±°ë‚˜, ìƒˆë¡œìš´ ë²„ì „ì´ ë‚˜ì˜¬ ë•Œê¹Œì§€ ê¸°ë‹¤ë¦´ ìˆ˜ ì—†ë‹¤ë©´ [ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì†ŒìŠ¤ì—ì„œ ë°”ë¡œ ì„¤ì¹˜](https://huggingface.co/docs/transformers/installation#installing-from-source)í•˜ì…”ì•¼ í•©ë‹ˆë‹¤.

### condaë¡œ ì„¤ì¹˜í•˜ê¸°

ğŸ¤— TransformersëŠ” ë‹¤ìŒê³¼ ê°™ì´ condaë¡œ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```shell script
conda install conda-forge::transformers
```

> **_ë…¸íŠ¸:_** `huggingface` ì±„ë„ì—ì„œ `transformers`ë¥¼ ì„¤ì¹˜í•˜ëŠ” ê²ƒì€ ì‚¬ìš©ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.

Flax, PyTorch, TensorFlow ì„¤ì¹˜ í˜ì´ì§€ì—ì„œ ì´ë“¤ì„ condaë¡œ ì„¤ì¹˜í•˜ëŠ” ë°©ë²•ì„ í™•ì¸í•˜ì„¸ìš”.

## ëª¨ë¸ êµ¬ì¡°

**ğŸ¤— Transformersê°€ ì œê³µí•˜ëŠ” [ëª¨ë“  ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸](https://huggingface.co/models)** ëŠ” huggingface.co [ëª¨ë¸ í—ˆë¸Œ](https://huggingface.co)ì— ì™„ë²½íˆ ì—°ë™ë˜ì–´ ìˆìŠµë‹ˆë‹¤. [ê°œì¸](https://huggingface.co/users)ê³¼ [ê¸°ê´€](https://huggingface.co/organizations)ì´ ëª¨ë¸ í—ˆë¸Œì— ì§ì ‘ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ì˜ ê°œìˆ˜: ![](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen)

ğŸ¤— TransformersëŠ” ë‹¤ìŒ ëª¨ë¸ë“¤ì„ ì œê³µí•©ë‹ˆë‹¤: ê° ëª¨ë¸ì˜ ìš”ì•½ì€ [ì—¬ê¸°](https://huggingface.co/docs/transformers/model_summary)ì„œ í™•ì¸í•˜ì„¸ìš”.

ê° ëª¨ë¸ì´ Flax, PyTorch, TensorFlowìœ¼ë¡œ êµ¬í˜„ë˜ì—ˆëŠ”ì§€ ë˜ëŠ” ğŸ¤— Tokenizers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì§€ì›í•˜ëŠ” í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ëŠ”ì§€ í™•ì¸í•˜ë ¤ë©´, [ì´ í‘œ](https://huggingface.co/docs/transformers/index#supported-frameworks)ë¥¼ í™•ì¸í•˜ì„¸ìš”.

ì´ êµ¬í˜„ì€ ì—¬ëŸ¬ ë°ì´í„°ë¡œ ê²€ì¦ë˜ì—ˆê³  (ì˜ˆì‹œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì°¸ê³ í•˜ì„¸ìš”) ì˜¤ë¦¬ì§€ë„ êµ¬í˜„ì˜ ì„±ëŠ¥ê³¼ ê°™ì•„ì•¼ í•©ë‹ˆë‹¤. [ë„íë¨¼íŠ¸](https://huggingface.co/docs/transformers/examples)ì˜ Examples ì„¹ì…˜ì—ì„œ ì„±ëŠ¥ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ë” ì•Œì•„ë³´ê¸°

| ì„¹ì…˜ | ì„¤ëª… |
|-|-|
| [ë„íë¨¼íŠ¸](https://huggingface.co/transformers/) | ì „ì²´ API ë„íë¨¼íŠ¸ì™€ íŠœí† ë¦¬ì–¼ |
| [ê³¼ì œ ìš”ì•½](https://huggingface.co/docs/transformers/task_summary) | ğŸ¤— Transformersê°€ ì§€ì›í•˜ëŠ” ê³¼ì œë“¤ |
| [ì „ì²˜ë¦¬ íŠœí† ë¦¬ì–¼](https://huggingface.co/docs/transformers/preprocessing) | `Tokenizer` í´ë˜ìŠ¤ë¥¼ ì´ìš©í•´ ëª¨ë¸ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„í•˜ê¸° |
| [í•™ìŠµê³¼ fine-tuning](https://huggingface.co/docs/transformers/training) | ğŸ¤— Transformersê°€ ì œê³µí•˜ëŠ” ëª¨ë¸ PyTorch/TensorFlow í•™ìŠµ ê³¼ì •ê³¼ `Trainer` APIì—ì„œ ì‚¬ìš©í•˜ê¸° |
| [í€µ íˆ¬ì–´: Fine-tuning/ì‚¬ìš© ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples) | ë‹¤ì–‘í•œ ê³¼ì œì—ì„œ ëª¨ë¸ fine-tuningí•˜ëŠ” ì˜ˆì‹œ ìŠ¤í¬ë¦½íŠ¸ |
| [ëª¨ë¸ ê³µìœ  ë° ì—…ë¡œë“œ](https://huggingface.co/docs/transformers/model_sharing) | ì»¤ë®¤ë‹ˆí‹°ì— fine-tuneëœ ëª¨ë¸ì„ ì—…ë¡œë“œ ë° ê³µìœ í•˜ê¸° |
| [ë§ˆì´ê·¸ë ˆì´ì…˜](https://huggingface.co/docs/transformers/migration) | `pytorch-transformers`ë‚˜ `pytorch-pretrained-bert`ì—ì„œ ğŸ¤— Transformersë¡œ ì´ë™í•˜ê¸°|

## ì¸ìš©

ğŸ¤— Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì¸ìš©í•˜ê³  ì‹¶ë‹¤ë©´, ì´ [ë…¼ë¬¸](https://www.aclweb.org/anthology/2020.emnlp-demos.6/)ì„ ì¸ìš©í•´ ì£¼ì„¸ìš”:
```bibtex
@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and RÃ©mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}
```
