<!---
Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

<!---
A useful guide for English-Chinese translation of Hugging Face documentation
- Add space around English words and numbers when they appear between Chinese characters. E.g., å…± 100 å¤šç§è¯­è¨€; ä½¿ç”¨ transformers åº“ã€‚
- Use square quotes, e.g.,ã€Œå¼•ç”¨ã€

Dictionary

Hugging Face: æŠ±æŠ±è„¸
token: è¯ç¬¦ï¼ˆå¹¶ç”¨æ‹¬å·æ ‡æ³¨åŸè‹±æ–‡ï¼‰
tokenize: è¯ç¬¦åŒ–ï¼ˆå¹¶ç”¨æ‹¬å·æ ‡æ³¨åŸè‹±æ–‡ï¼‰
tokenizer: è¯ç¬¦åŒ–å™¨ï¼ˆå¹¶ç”¨æ‹¬å·æ ‡æ³¨åŸè‹±æ–‡ï¼‰
transformer: transformerï¼ˆä¸ç¿»è¯‘ï¼‰
pipeline: æµæ°´çº¿
API: API (ä¸ç¿»è¯‘ï¼‰
inference: æ¨ç†
Trainer: è®­ç»ƒå™¨ã€‚å½“ä½œä¸ºç±»åå‡ºç°æ—¶ä¸ç¿»è¯‘ã€‚
pretrained/pretrain: é¢„è®­ç»ƒ
finetune: å¾®è°ƒ
community: ç¤¾åŒº
example: å½“ç‰¹æŒ‡ä»“åº“ä¸­ example ç›®å½•æ—¶ç¿»è¯‘ä¸ºã€Œç”¨ä¾‹ã€
Python data structures (e.g., list, set, dict): ç¿»è¯‘ä¸ºåˆ—è¡¨ï¼Œé›†åˆï¼Œè¯å…¸ï¼Œå¹¶ç”¨æ‹¬å·æ ‡æ³¨åŸè‹±æ–‡
NLP/Natural Language Processing: ä»¥ NLP å‡ºç°æ—¶ä¸ç¿»è¯‘ï¼Œä»¥ Natural Language Processing å‡ºç°æ—¶ç¿»è¯‘ä¸ºè‡ªç„¶è¯­è¨€å¤„ç†
checkpoint: æ£€æŸ¥ç‚¹
-->

<p align="center">
    <br>
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers_logo_name.png" width="400"/>
    <br>
</p>
<p align="center">
    <a href="https://circleci.com/gh/huggingface/transformers"><img alt="Build" src="https://img.shields.io/circleci/build/github/huggingface/transformers/main"></a>
    <a href="https://github.com/huggingface/transformers/blob/main/LICENSE"><img alt="GitHub" src="https://img.shields.io/github/license/huggingface/transformers.svg?color=blue"></a>
    <a href="https://huggingface.co/docs/transformers/index"><img alt="Documentation" src="https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online"></a>
    <a href="https://github.com/huggingface/transformers/releases"><img alt="GitHub release" src="https://img.shields.io/github/release/huggingface/transformers.svg"></a>
    <a href="https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md"><img alt="Contributor Covenant" src="https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg"></a>
    <a href="https://zenodo.org/badge/latestdoi/155220641"><img src="https://zenodo.org/badge/155220641.svg" alt="DOI"></a>
</p>

<h4 align="center">
    <p>
        <a href="https://github.com/huggingface/transformers/">English</a> |
        <b>ç®€ä½“ä¸­æ–‡</b> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hant.md">ç¹é«”ä¸­æ–‡</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ko.md">í•œêµ­ì–´</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_es.md">EspaÃ±ol</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ja.md">æ—¥æœ¬èª</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_hd.md">à¤¹à¤¿à¤¨à¥à¤¦à¥€</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ru.md">Ğ ÑƒÑÑĞºĞ¸Ğ¹</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md">Ğ ortuguÃªs</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_te.md">à°¤à±†à°²à±à°—à±</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_fr.md">FranÃ§ais</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_de.md">Deutsch</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_vi.md">Tiáº¿ng Viá»‡t</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ar.md">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ur.md">Ø§Ø±Ø¯Ùˆ</a> |
    </p>
</h4>

<h3 align="center">
    <p>ä¸º Jaxã€PyTorch å’Œ TensorFlow æ‰“é€ çš„å…ˆè¿›çš„è‡ªç„¶è¯­è¨€å¤„ç†</p>
</h3>

<h3 align="center">
    <a href="https://hf.co/course"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png"></a>
</h3>

ğŸ¤— Transformers æä¾›äº†æ•°ä»¥åƒè®¡çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ”¯æŒ 100 å¤šç§è¯­è¨€çš„æ–‡æœ¬åˆ†ç±»ã€ä¿¡æ¯æŠ½å–ã€é—®ç­”ã€æ‘˜è¦ã€ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆã€‚å®ƒçš„å®—æ—¨æ˜¯è®©æœ€å…ˆè¿›çš„ NLP æŠ€æœ¯äººäººæ˜“ç”¨ã€‚

ğŸ¤— Transformers æä¾›äº†ä¾¿äºå¿«é€Ÿä¸‹è½½å’Œä½¿ç”¨çš„APIï¼Œè®©ä½ å¯ä»¥æŠŠé¢„è®­ç»ƒæ¨¡å‹ç”¨åœ¨ç»™å®šæ–‡æœ¬ã€åœ¨ä½ çš„æ•°æ®é›†ä¸Šå¾®è°ƒç„¶åé€šè¿‡ [model hub](https://huggingface.co/models) ä¸ç¤¾åŒºå…±äº«ã€‚åŒæ—¶ï¼Œæ¯ä¸ªå®šä¹‰çš„ Python æ¨¡å—å‡å®Œå…¨ç‹¬ç«‹ï¼Œæ–¹ä¾¿ä¿®æ”¹å’Œå¿«é€Ÿç ”ç©¶å®éªŒã€‚

ğŸ¤— Transformers æ”¯æŒä¸‰ä¸ªæœ€çƒ­é—¨çš„æ·±åº¦å­¦ä¹ åº“ï¼š [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) ä»¥åŠ [TensorFlow](https://www.tensorflow.org/) â€” å¹¶ä¸ä¹‹æ— ç¼æ•´åˆã€‚ä½ å¯ä»¥ç›´æ¥ä½¿ç”¨ä¸€ä¸ªæ¡†æ¶è®­ç»ƒä½ çš„æ¨¡å‹ç„¶åç”¨å¦ä¸€ä¸ªåŠ è½½å’Œæ¨ç†ã€‚

## åœ¨çº¿æ¼”ç¤º

ä½ å¯ä»¥ç›´æ¥åœ¨æ¨¡å‹é¡µé¢ä¸Šæµ‹è¯•å¤§å¤šæ•° [model hub](https://huggingface.co/models) ä¸Šçš„æ¨¡å‹ã€‚ æˆ‘ä»¬ä¹Ÿæä¾›äº† [ç§æœ‰æ¨¡å‹æ‰˜ç®¡ã€æ¨¡å‹ç‰ˆæœ¬ç®¡ç†ä»¥åŠæ¨ç†API](https://huggingface.co/pricing)ã€‚

è¿™é‡Œæ˜¯ä¸€äº›ä¾‹å­ï¼š
- [ç”¨ BERT åšæ©ç å¡«è¯](https://huggingface.co/google-bert/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France)
- [ç”¨ Electra åšå‘½åå®ä½“è¯†åˆ«](https://huggingface.co/dbmdz/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city)
- [ç”¨ GPT-2 åšæ–‡æœ¬ç”Ÿæˆ](https://huggingface.co/openai-community/gpt2?text=A+long+time+ago%2C+)
- [ç”¨ RoBERTa åšè‡ªç„¶è¯­è¨€æ¨ç†](https://huggingface.co/FacebookAI/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal)
- [ç”¨ BART åšæ–‡æœ¬æ‘˜è¦](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+was+the+first+structure+to+reach+a+height+of+300+metres.+Due+to+the+addition+of+a+broadcasting+aerial+at+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct)
- [ç”¨ DistilBERT åšé—®ç­”](https://huggingface.co/distilbert/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species)
- [ç”¨ T5 åšç¿»è¯‘](https://huggingface.co/google-t5/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)

**[Write With Transformer](https://transformer.huggingface.co)**ï¼Œç”±æŠ±æŠ±è„¸å›¢é˜Ÿæ‰“é€ ï¼Œæ˜¯ä¸€ä¸ªæ–‡æœ¬ç”Ÿæˆçš„å®˜æ–¹ demoã€‚

## å¦‚æœä½ åœ¨å¯»æ‰¾ç”±æŠ±æŠ±è„¸å›¢é˜Ÿæä¾›çš„å®šåˆ¶åŒ–æ”¯æŒæœåŠ¡

<a target="_blank" href="https://huggingface.co/support">
    <img alt="HuggingFace Expert Acceleration Program" src="https://huggingface.co/front/thumbnails/support.png" style="max-width: 600px; border: 1px solid #eee; border-radius: 4px; box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);">
</a><br>

## å¿«é€Ÿä¸Šæ‰‹

æˆ‘ä»¬ä¸ºå¿«é€Ÿä½¿ç”¨æ¨¡å‹æä¾›äº† `pipeline` ï¼ˆæµæ°´çº¿ï¼‰APIã€‚æµæ°´çº¿èšåˆäº†é¢„è®­ç»ƒæ¨¡å‹å’Œå¯¹åº”çš„æ–‡æœ¬é¢„å¤„ç†ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªå¿«é€Ÿä½¿ç”¨æµæ°´çº¿å»åˆ¤æ–­æ­£è´Ÿé¢æƒ…ç»ªçš„ä¾‹å­ï¼š

```python
>>> from transformers import pipeline

# ä½¿ç”¨æƒ…ç»ªåˆ†ææµæ°´çº¿
>>> classifier = pipeline('sentiment-analysis')
>>> classifier('We are very happy to introduce pipeline to the transformers repository.')
[{'label': 'POSITIVE', 'score': 0.9996980428695679}]
```

ç¬¬äºŒè¡Œä»£ç ä¸‹è½½å¹¶ç¼“å­˜äº†æµæ°´çº¿ä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œè€Œç¬¬ä¸‰è¡Œä»£ç åˆ™åœ¨ç»™å®šçš„æ–‡æœ¬ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚è¿™é‡Œçš„ç­”æ¡ˆâ€œæ­£é¢â€ (positive) å…·æœ‰ 99 çš„ç½®ä¿¡åº¦ã€‚

è®¸å¤šçš„ NLP ä»»åŠ¡éƒ½æœ‰å¼€ç®±å³ç”¨çš„é¢„è®­ç»ƒæµæ°´çº¿ã€‚æ¯”å¦‚è¯´ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾çš„ä»ç»™å®šæ–‡æœ¬ä¸­æŠ½å–é—®é¢˜ç­”æ¡ˆï¼š

``` python
>>> from transformers import pipeline

# ä½¿ç”¨é—®ç­”æµæ°´çº¿
>>> question_answerer = pipeline('question-answering')
>>> question_answerer({
...     'question': 'What is the name of the repository ?',
...     'context': 'Pipeline has been included in the huggingface/transformers repository'
... })
{'score': 0.30970096588134766, 'start': 34, 'end': 58, 'answer': 'huggingface/transformers'}

```

é™¤äº†ç»™å‡ºç­”æ¡ˆï¼Œé¢„è®­ç»ƒæ¨¡å‹è¿˜ç»™å‡ºäº†å¯¹åº”çš„ç½®ä¿¡åº¦åˆ†æ•°ã€ç­”æ¡ˆåœ¨è¯ç¬¦åŒ– (tokenized) åçš„æ–‡æœ¬ä¸­å¼€å§‹å’Œç»“æŸçš„ä½ç½®ã€‚ä½ å¯ä»¥ä»[è¿™ä¸ªæ•™ç¨‹](https://huggingface.co/docs/transformers/task_summary)äº†è§£æ›´å¤šæµæ°´çº¿APIæ”¯æŒçš„ä»»åŠ¡ã€‚

è¦åœ¨ä½ çš„ä»»åŠ¡ä¸Šä¸‹è½½å’Œä½¿ç”¨ä»»æ„é¢„è®­ç»ƒæ¨¡å‹ä¹Ÿå¾ˆç®€å•ï¼Œåªéœ€ä¸‰è¡Œä»£ç ã€‚è¿™é‡Œæ˜¯ PyTorch ç‰ˆçš„ç¤ºä¾‹ï¼š
```python
>>> from transformers import AutoTokenizer, AutoModel

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
>>> model = AutoModel.from_pretrained("google-bert/bert-base-uncased")

>>> inputs = tokenizer("Hello world!", return_tensors="pt")
>>> outputs = model(**inputs)
```
è¿™é‡Œæ˜¯ç­‰æ•ˆçš„ TensorFlow ä»£ç ï¼š
```python
>>> from transformers import AutoTokenizer, TFAutoModel

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
>>> model = TFAutoModel.from_pretrained("google-bert/bert-base-uncased")

>>> inputs = tokenizer("Hello world!", return_tensors="tf")
>>> outputs = model(**inputs)
```

è¯ç¬¦åŒ–å™¨ (tokenizer) ä¸ºæ‰€æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹æä¾›äº†é¢„å¤„ç†ï¼Œå¹¶å¯ä»¥ç›´æ¥å¯¹å•ä¸ªå­—ç¬¦ä¸²è¿›è¡Œè°ƒç”¨ï¼ˆæ¯”å¦‚ä¸Šé¢çš„ä¾‹å­ï¼‰æˆ–å¯¹åˆ—è¡¨ (list) è°ƒç”¨ã€‚å®ƒä¼šè¾“å‡ºä¸€ä¸ªä½ å¯ä»¥åœ¨ä¸‹æ¸¸ä»£ç é‡Œä½¿ç”¨æˆ–ç›´æ¥é€šè¿‡ `**` è§£åŒ…è¡¨è¾¾å¼ä¼ ç»™æ¨¡å‹çš„è¯å…¸ (dict)ã€‚

æ¨¡å‹æœ¬èº«æ˜¯ä¸€ä¸ªå¸¸è§„çš„ [Pytorch `nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) æˆ– [TensorFlow `tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)ï¼ˆå–å†³äºä½ çš„åç«¯ï¼‰ï¼Œå¯ä»¥å¸¸è§„æ–¹å¼ä½¿ç”¨ã€‚ [è¿™ä¸ªæ•™ç¨‹](https://huggingface.co/transformers/training.html)è§£é‡Šäº†å¦‚ä½•å°†è¿™æ ·çš„æ¨¡å‹æ•´åˆåˆ°ç»å…¸çš„ PyTorch æˆ– TensorFlow è®­ç»ƒå¾ªç¯ä¸­ï¼Œæˆ–æ˜¯å¦‚ä½•ä½¿ç”¨æˆ‘ä»¬çš„ `Trainer` è®­ç»ƒå™¨ï¼‰API æ¥åœ¨ä¸€ä¸ªæ–°çš„æ•°æ®é›†ä¸Šå¿«é€Ÿå¾®è°ƒã€‚

## ä¸ºä»€ä¹ˆè¦ç”¨ transformersï¼Ÿ

1. ä¾¿äºä½¿ç”¨çš„å…ˆè¿›æ¨¡å‹ï¼š
    - NLU å’Œ NLG ä¸Šè¡¨ç°ä¼˜è¶Š
    - å¯¹æ•™å­¦å’Œå®è·µå‹å¥½ä¸”ä½é—¨æ§›
    - é«˜çº§æŠ½è±¡ï¼Œåªéœ€äº†è§£ä¸‰ä¸ªç±»
    - å¯¹æ‰€æœ‰æ¨¡å‹ç»Ÿä¸€çš„API

1. æ›´ä½è®¡ç®—å¼€é”€ï¼Œæ›´å°‘çš„ç¢³æ’æ”¾ï¼š
    - ç ”ç©¶äººå‘˜å¯ä»¥åˆ†äº«å·²è®­ç»ƒçš„æ¨¡å‹è€Œéæ¯æ¬¡ä»å¤´å¼€å§‹è®­ç»ƒ
    - å·¥ç¨‹å¸ˆå¯ä»¥å‡å°‘è®¡ç®—ç”¨æ—¶å’Œç”Ÿäº§ç¯å¢ƒå¼€é”€
    - æ•°åç§æ¨¡å‹æ¶æ„ã€ä¸¤åƒå¤šä¸ªé¢„è®­ç»ƒæ¨¡å‹ã€100å¤šç§è¯­è¨€æ”¯æŒ

1. å¯¹äºæ¨¡å‹ç”Ÿå‘½å‘¨æœŸçš„æ¯ä¸€ä¸ªéƒ¨åˆ†éƒ½é¢é¢ä¿±åˆ°ï¼š
    - è®­ç»ƒå…ˆè¿›çš„æ¨¡å‹ï¼Œåªéœ€ 3 è¡Œä»£ç 
    - æ¨¡å‹åœ¨ä¸åŒæ·±åº¦å­¦ä¹ æ¡†æ¶é—´ä»»æ„è½¬ç§»ï¼Œéšä½ å¿ƒæ„
    - ä¸ºè®­ç»ƒã€è¯„ä¼°å’Œç”Ÿäº§é€‰æ‹©æœ€é€‚åˆçš„æ¡†æ¶ï¼Œè¡”æ¥æ— ç¼

1. ä¸ºä½ çš„éœ€æ±‚è½»æ¾å®šåˆ¶ä¸“å±æ¨¡å‹å’Œç”¨ä¾‹ï¼š
    - æˆ‘ä»¬ä¸ºæ¯ç§æ¨¡å‹æ¶æ„æä¾›äº†å¤šä¸ªç”¨ä¾‹æ¥å¤ç°åŸè®ºæ–‡ç»“æœ
    - æ¨¡å‹å†…éƒ¨ç»“æ„ä¿æŒé€æ˜ä¸€è‡´
    - æ¨¡å‹æ–‡ä»¶å¯å•ç‹¬ä½¿ç”¨ï¼Œæ–¹ä¾¿é­”æ”¹å’Œå¿«é€Ÿå®éªŒ

## ä»€ä¹ˆæƒ…å†µä¸‹æˆ‘ä¸è¯¥ç”¨ transformersï¼Ÿ

- æœ¬åº“å¹¶ä¸æ˜¯æ¨¡å—åŒ–çš„ç¥ç»ç½‘ç»œå·¥å…·ç®±ã€‚æ¨¡å‹æ–‡ä»¶ä¸­çš„ä»£ç ç‰¹æ„å‘ˆè‹¥ç’ç‰ï¼Œæœªç»é¢å¤–æŠ½è±¡å°è£…ï¼Œä»¥ä¾¿ç ”ç©¶äººå‘˜å¿«é€Ÿè¿­ä»£é­”æ”¹è€Œä¸è‡´æººäºæŠ½è±¡å’Œæ–‡ä»¶è·³è½¬ä¹‹ä¸­ã€‚
- `Trainer` API å¹¶éå…¼å®¹ä»»ä½•æ¨¡å‹ï¼Œåªä¸ºæœ¬åº“ä¹‹æ¨¡å‹ä¼˜åŒ–ã€‚è‹¥æ˜¯åœ¨å¯»æ‰¾é€‚ç”¨äºé€šç”¨æœºå™¨å­¦ä¹ çš„è®­ç»ƒå¾ªç¯å®ç°ï¼Œè¯·å¦è§…ä»–åº“ã€‚
- å°½ç®¡æˆ‘ä»¬å·²å°½åŠ›è€Œä¸ºï¼Œ[examples ç›®å½•](https://github.com/huggingface/transformers/tree/main/examples)ä¸­çš„è„šæœ¬ä¹Ÿä»…ä¸ºç”¨ä¾‹è€Œå·²ã€‚å¯¹äºä½ çš„ç‰¹å®šé—®é¢˜ï¼Œå®ƒä»¬å¹¶ä¸ä¸€å®šå¼€ç®±å³ç”¨ï¼Œå¯èƒ½éœ€è¦æ”¹å‡ è¡Œä»£ç ä»¥é€‚ä¹‹ã€‚

## å®‰è£…

### ä½¿ç”¨ pip

è¿™ä¸ªä»“åº“å·²åœ¨ Python 3.9+ã€Flax 0.4.1+ã€PyTorch 2.0+ å’Œ TensorFlow 2.6+ ä¸‹ç»è¿‡æµ‹è¯•ã€‚

ä½ å¯ä»¥åœ¨[è™šæ‹Ÿç¯å¢ƒ](https://docs.python.org/3/library/venv.html)ä¸­å®‰è£… ğŸ¤— Transformersã€‚å¦‚æœä½ è¿˜ä¸ç†Ÿæ‚‰ Python çš„è™šæ‹Ÿç¯å¢ƒï¼Œè¯·é˜…æ­¤[ç”¨æˆ·è¯´æ˜](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/)ã€‚

é¦–å…ˆï¼Œç”¨ä½ æ‰“ç®—ä½¿ç”¨çš„ç‰ˆæœ¬çš„ Python åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿç¯å¢ƒå¹¶æ¿€æ´»ã€‚

ç„¶åï¼Œä½ éœ€è¦å®‰è£… Flaxã€PyTorch æˆ– TensorFlow å…¶ä¸­ä¹‹ä¸€ã€‚å…³äºåœ¨ä½ ä½¿ç”¨çš„å¹³å°ä¸Šå®‰è£…è¿™äº›æ¡†æ¶ï¼Œè¯·å‚é˜… [TensorFlow å®‰è£…é¡µ](https://www.tensorflow.org/install/), [PyTorch å®‰è£…é¡µ](https://pytorch.org/get-started/locally/#start-locally) æˆ– [Flax å®‰è£…é¡µ](https://github.com/google/flax#quick-install)ã€‚

å½“è¿™äº›åç«¯ä¹‹ä¸€å®‰è£…æˆåŠŸåï¼Œ ğŸ¤— Transformers å¯ä¾æ­¤å®‰è£…ï¼š

```bash
pip install transformers
```

å¦‚æœä½ æƒ³è¦è¯•è¯•ç”¨ä¾‹æˆ–è€…æƒ³åœ¨æ­£å¼å‘å¸ƒå‰ä½¿ç”¨æœ€æ–°çš„å¼€å‘ä¸­ä»£ç ï¼Œä½ å¾—[ä»æºä»£ç å®‰è£…](https://huggingface.co/docs/transformers/installation#installing-from-source)ã€‚

### ä½¿ç”¨ conda

ğŸ¤— Transformers å¯ä»¥é€šè¿‡ conda ä¾æ­¤å®‰è£…ï¼š

```shell script
conda install conda-forge::transformers
```

> **_ç¬”è®°:_** ä» `huggingface` æ¸ é“å®‰è£… `transformers` å·²è¢«åºŸå¼ƒã€‚

è¦é€šè¿‡ conda å®‰è£… Flaxã€PyTorch æˆ– TensorFlow å…¶ä¸­ä¹‹ä¸€ï¼Œè¯·å‚é˜…å®ƒä»¬å„è‡ªå®‰è£…é¡µçš„è¯´æ˜ã€‚

## æ¨¡å‹æ¶æ„

ğŸ¤— Transformers æ”¯æŒçš„[**æ‰€æœ‰çš„æ¨¡å‹æ£€æŸ¥ç‚¹**](https://huggingface.co/models)ç”±[ç”¨æˆ·](https://huggingface.co/users)å’Œ[ç»„ç»‡](https://huggingface.co/organizations)ä¸Šä¼ ï¼Œå‡ä¸ huggingface.co [model hub](https://huggingface.co) æ— ç¼æ•´åˆã€‚

ç›®å‰çš„æ£€æŸ¥ç‚¹æ•°é‡ï¼š ![](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen)

ğŸ¤— Transformers ç›®å‰æ”¯æŒå¦‚ä¸‹çš„æ¶æ„: æ¨¡å‹æ¦‚è¿°è¯·é˜…[è¿™é‡Œ](https://huggingface.co/docs/transformers/model_summary).

è¦æ£€æŸ¥æŸä¸ªæ¨¡å‹æ˜¯å¦å·²æœ‰ Flaxã€PyTorch æˆ– TensorFlow çš„å®ç°ï¼Œæˆ–å…¶æ˜¯å¦åœ¨ ğŸ¤— Tokenizers åº“ä¸­æœ‰å¯¹åº”è¯ç¬¦åŒ–å™¨ï¼ˆtokenizerï¼‰ï¼Œæ•¬è¯·å‚é˜…[æ­¤è¡¨](https://huggingface.co/docs/transformers/index#supported-frameworks)ã€‚

è¿™äº›å®ç°å‡å·²äºå¤šä¸ªæ•°æ®é›†æµ‹è¯•ï¼ˆè¯·å‚çœ‹ç”¨ä¾‹è„šæœ¬ï¼‰å¹¶åº”äºåŸç‰ˆå®ç°è¡¨ç°ç›¸å½“ã€‚ä½ å¯ä»¥åœ¨ç”¨ä¾‹æ–‡æ¡£çš„[æ­¤èŠ‚](https://huggingface.co/docs/transformers/examples)ä¸­äº†è§£è¡¨ç°çš„ç»†èŠ‚ã€‚


## äº†è§£æ›´å¤š

| ç« èŠ‚ | æè¿° |
|-|-|
| [æ–‡æ¡£](https://huggingface.co/docs/transformers/) | å®Œæ•´çš„ API æ–‡æ¡£å’Œæ•™ç¨‹ |
| [ä»»åŠ¡æ€»ç»“](https://huggingface.co/docs/transformers/task_summary) | ğŸ¤— Transformers æ”¯æŒçš„ä»»åŠ¡ |
| [é¢„å¤„ç†æ•™ç¨‹](https://huggingface.co/docs/transformers/preprocessing) | ä½¿ç”¨ `Tokenizer` æ¥ä¸ºæ¨¡å‹å‡†å¤‡æ•°æ® |
| [è®­ç»ƒå’Œå¾®è°ƒ](https://huggingface.co/docs/transformers/training) | åœ¨ PyTorch/TensorFlow çš„è®­ç»ƒå¾ªç¯æˆ– `Trainer` API ä¸­ä½¿ç”¨ ğŸ¤— Transformers æä¾›çš„æ¨¡å‹ |
| [å¿«é€Ÿä¸Šæ‰‹ï¼šå¾®è°ƒå’Œç”¨ä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples) | ä¸ºå„ç§ä»»åŠ¡æä¾›çš„ç”¨ä¾‹è„šæœ¬ |
| [æ¨¡å‹åˆ†äº«å’Œä¸Šä¼ ](https://huggingface.co/docs/transformers/model_sharing) | å’Œç¤¾åŒºä¸Šä¼ å’Œåˆ†äº«ä½ å¾®è°ƒçš„æ¨¡å‹ |
| [è¿ç§»](https://huggingface.co/docs/transformers/migration) | ä» `pytorch-transformers` æˆ– `pytorch-pretrained-bert` è¿ç§»åˆ° ğŸ¤— Transformers |

## å¼•ç”¨

æˆ‘ä»¬å·²å°†æ­¤åº“çš„[è®ºæ–‡](https://www.aclweb.org/anthology/2020.emnlp-demos.6/)æ­£å¼å‘è¡¨ï¼Œå¦‚æœä½ ä½¿ç”¨äº† ğŸ¤— Transformers åº“ï¼Œè¯·å¼•ç”¨:
```bibtex
@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and RÃ©mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}
```
