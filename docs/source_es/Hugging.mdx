<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Haciendo fine-tuning a un modelo pre-entrenado

El uso de un modelo pre-entrenado tiene importantes ventajas. Reduce los costes de computaci√≥n, su huella de carbono, y te permite utilizar modelos de √∫ltima generaci√≥n sin tener que entrenar uno desde cero. ü§ó Transformers proporciona acceso a miles de modelos pre-entrenados en una amplia gama de tareas. Cuando utilizas un modelo pre-entrenado, lo entrenas con un dataset espec√≠fico para tu tarea. Esto se conoce como fine-tuning, una t√©cnica de entrenamiento incre√≠blemente poderosa. En este tutorial haremos fine-tuning a un modelo pre-entrenado con un framework de Deep Learning de tu elecci√≥n:

- Aplicar fine-tuning a un modelo pre-entrenado con ü§ó Transformers [Trainer](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/trainer#transformers.Trainer).
- Aplicar fine-tuning a un modelo pre-entrenado en TensorFlow con Keras.
- Aplicar fine-tuning** a un modelo pre-entrenado en PyTorch nativo.

## Prepara un dataset

[https://youtu.be/_BZearw7f0w](https://youtu.be/_BZearw7f0w)

Antes de aplicar fine-tuning a un modelo pre-entrenado, descarga un dataset y prep√°ralo para el entrenamiento. El tutorial anterior nos ense√±√≥ c√≥mo procesar los datos para el entrenamiento, y ahora es la oportunidad de poner a prueba estas habilidades.

Comienza cargando el dataset de [Yelp Reviews](https://huggingface.co/datasets/yelp_review_full):

```python
from datasets import load_dataset

# Cargamos el dataset
dataset = load_dataset("yelp_review_full")
dataset[100]

# Salida
{'label': 0,
 'text': 'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\nThe cashier took my friends\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\"serving off their orders\\" when they didn\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\nThe manager was rude when giving me my order. She didn\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\nI\'ve eaten at various McDonalds restaurants for over 30 years. I\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'}
```

Como ya sabes, necesitas un tokenizador para procesar el texto e incluir una estrategia para el padding y el truncamiento, manejando cualquier tama√±o de una variable secuencial. Para procesar tu dataset en un solo paso, utiliza el m√©todo ü§ó Datasets [map](https://huggingface.co/docs/datasets/process#map) para aplicar una funci√≥n de preprocesamiento sobre todo el dataset:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

tokenized_datasets = dataset.map(tokenize_function, batched=True)
```

Si lo deseas, puedes crear un subconjunto m√°s peque√±o del dataset completo para aplicarle fine-tuning y as√≠ reducir el tiempo.

```python
small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))
```

## Fine-tuning con [Trainer]

[https://youtu.be/nvBXf7s7vTI](https://youtu.be/nvBXf7s7vTI)

ü§ó Transformers proporciona una clase **[Trainer**](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/trainer#transformers.Trainer) optimizada para el entrenamiento de modelos ü§ó Transformers, haciendo m√°s f√°cil el inicio del entrenamiento sin necesidad de escribir manualmente su propio ciclo. La API del [Trainer](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/trainer#transformers.Trainer) soporta una amplia gama de opciones de entrenamiento y caracter√≠sticas como el logging, el gradient accumulation y la mixed precision.

Comienza cargando tu modelo y especifica el n√∫mero de labels previstas. A partir del Card Dataset Yelp Review, que como ya sabemos tiene 5 labels:

Note: Cada dataset deber√≠a ir acompa√±ado de un [Dataset Card](https://huggingface.co/docs/datasets/v1.12.0/dataset_card.html) para promover el uso responsable, y alertar al usuario de cualquier posible sesgo dentro del conjunto de datos.

```python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=5)
```

> Ver√°s una advertencia acerca de que algunos de los pesos pre-entrenados que no est√°n siendo utilizados y que algunos pesos est√°n siendo inicializados al azar. No te preocupes, esto es completamente normal. El head/cabezal pre-entrenado del modelo BERT se descarta y se sustituye por un head de clasificaci√≥n inicializado aleatoriamente. Puedes aplicar fine-tuning a este nuevo head del modelo en su tarea de clasificaci√≥n de secuencias, haciendo transfer learning del modelo pre-entrenado.
>

## Hiperpar√°metros de entrenamiento

A continuaci√≥n, crea una clase [TrainingArguments](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/trainer#transformers.TrainingArguments) que contenga todos los hiperpar√°metros que puedes ajustar, as√≠ como los indicadores para activar las diferentes opciones de entrenamiento. Para este tutorial puedes empezar con los [hiperpar√°metros](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) de entrenamiento por defecto, pero si√©ntete libre de experimentar con ellos para encontrar tu configuraci√≥n √≥ptima.

Especifica d√≥nde vas a guardar los checkpoints de tu entrenamiento:

```python
from transformers import TrainingArguments

training_args = TrainingArguments(output_dir="test_trainer")
```

## M√©tricas
El [Trainer](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/trainer#transformers.Trainer) no eval√∫a autom√°ticamente el rendimiento del modelo durante el entrenamiento. Tendr√°s que pasarle a [Trainer](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/trainer#transformers.Trainer) una funci√≥n para calcular y hacer un reporte de las m√©tricas. La librer√≠a de ü§ó Datasets proporciona una funci√≥n de [accuracy](https://huggingface.co/metrics/accuracy) simple que puedes cargar con la funci√≥n `load_metric` (ver este [tutorial](https://huggingface.co/docs/datasets/metrics) para m√°s informaci√≥n):

```python
import numpy as np
from datasets import load_metric

metric = load_metric("accuracy")
```

Definamos la funci√≥n `compute_metric` para calcular el accuracy de tus predicciones. Antes de pasar tus predicciones a procesar, necesitas convertir las predicciones a logits (recuerda que todos los modelos de ü§ó Transformers devuelven logits).

```python
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
```

Si quieres controlar tus m√©tricas de evaluaci√≥n durante el fine-tuning, especifica el par√°metro `evaluation_strategy` en tus argumentos de entrenamiento para que el modelo tenga en cuenta la m√©trica de evaluaci√≥n al final de cada √©poca:

```python
from transformers import TrainingArguments

training_args = TrainingArguments(output_dir="test_trainer", evaluation_strategy="epoch")
```

## Trainer

Crea un objeto [Trainer](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/trainer#transformers.Trainer) con su modelo, argumentos de entrenamiento, conjuntos de datos de entrenamiento y de prueba, y su funci√≥n de evaluaci√≥n:

```python
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)
```

A continuaci√≥n, aplique fine-tuning a su modelo llamando a [train()](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/trainer#transformers.Trainer.train):

```python
trainer.train()
```

## Fine-tuning con Keras

[https://youtu.be/rnTGBy2ax1c](https://youtu.be/rnTGBy2ax1c)

Los modelos de ü§ó Transformers tambi√©n permite realizar el entrenamiento en TensorFlow con la API de Keras. S√≥lo es necesario hacer algunos cambios antes de hacer fine-tuning.

## Convierte el dataset al formato de TensorFlow

El [DefaultDataCollator](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/data_collator#transformers.DefaultDataCollator) junta los tensores en un batch para que el modelo se entrene en √©l. Aseg√∫rate de especificar `return_tensors` para devolver los tensores de TensorFlow:

```python
from transformers import DefaultDataCollator

data_collator = DefaultDataCollator(return_tensors="tf")
```

> [Trainer](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/trainer#transformers.Trainer) utiliza [DataCollatorWithPadding](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/data_collator#transformers.DataCollatorWithPadding) por defecto, por lo que no es necesario especificar expl√≠citamente un intercalador de datos.
>

A continuaci√≥n, convierte los datasets0 tokenizados en datasets de TensorFlow con el m√©todo `[to_tf_dataset](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.to_tf_dataset)`. Especifica tus entradas en `columns`, y tu etiqueta en `label_cols`:

```python
tf_train_dataset = small_train_dataset.to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = small_eval_dataset.to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

## Compilar y ajustar(fit)

Carguemos un modelo TensorFlow con el n√∫mero esperado de labels:

```python
import tensorflow as tf
from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=5)
```

A continuaci√≥n, compila y aplica fine-tuning a tu modelo con `fit` como lo har√≠as con cualquier otro modelo Keras:

```python
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=tf.metrics.SparseCategoricalAccuracy(),
)

model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)
```

## Fine-tune en PyTorch nativo

[https://youtu.be/Dh9CL8fyG80](https://youtu.be/Dh9CL8fyG80)

El [Trainer](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/trainer#transformers.Trainer) se encarga del ciclo de entrenamiento y permite aplicar fine-tuning a un modelo en una sola l√≠nea de c√≥digo. Para los usuarios que prefieren escribir su propio ciclo de entrenamiento, tambi√©n puedes aplicar fine-tuning a un modelo ü§ó Transformers en PyTorch nativo.

En este punto, es posible que necesites reiniciar tu notebook o ejecutar el siguiente c√≥digo para liberar algo de memoria:

```python
del model
del pytorch_model
del trainer
torch.cuda.empty_cache()
```

A continuaci√≥n, haz un post-proceso manualmente al 
`tokenized_dataset` y as√≠ prepararlo para el entrenamiento.

1. Elimina la columna de `text` porque el modelo no acepta texto en crudo como entrada:

```python
tokenized_datasets = tokenized_datasets.remove_columns(["text"])
```

1. Cambia el nombre de la columna de `label` a `labels` porque el modelo espera que el argumento se llame `labels`:

```python
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
```

1. Establece el formato del dataset para devolver tensores PyTorch en lugar de listas:

```python
tokenized_datasets.set_format("torch")
```

A continuaci√≥n, crea un subconjunto m√°s peque√±o del dataset, como se ha mostrado anteriormente, para acelerar el fine-tuning:

```python
small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))
```

## DataLoader

Crea un DataLoader para sus datasets de entrenamiento y de prueba para poder iterar sobre batches de datos:

```python
from torch.utils.data import DataLoader

train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)
eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)
```

Cargue su modelo con el n√∫mero de labels previstas:

```python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=5)
```

## Optimizador y learning rate

Crea un optimizador y el learning rate para aplicar fine-tuning al modelo. Vamos a utilizar el optimizador [AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) de PyTorch:

```python
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
```

Crea el learning rate por defecto desde el [Trainer](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/trainer#transformers.Trainer):

```python
from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    name="linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps
)
```

Por √∫ltimo, especifique el entorno de ejecuci√≥n para utilizar una GPU si tiene acceso a una. De lo contrario, el entrenamiento en una CPU puede llevar varias horas en lugar de un par de minutos.

```python
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
```

> Consigue acceso gratuito a una GPU en la nube si es que no tienes este recurso de forma local, con un notebook alojado como [Colaboratory](https://colab.research.google.com/) o [SageMaker StudioLab](https://studiolab.sagemaker.aws/).
>

Genial, ¬°ahora est√°s listo para entrenar! ü•≥

## Ciclo de entrenamiento

Para hacer un seguimiento del progreso del entrenamiento, utiliza la librer√≠a [tqdm](https://tqdm.github.io/) para a√±adir una barra de progreso sobre el n√∫mero de pasos de entrenamiento:

```python
from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

## M√©tricas

De la misma manera que necesitas a√±adir una funci√≥n de evaluaci√≥n al [Trainer](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/trainer#transformers.Trainer), necesitas hacer lo mismo cuando escribas tu propio ciclo de entrenamiento. Pero en lugar de calcular y reportar la m√©trica al final de cada √©poca, esta vez acumular√°s todos los batches con [add_batch](https://huggingface.co/docs/datasets/package_reference/main_classes?highlight=add_batch#datasets.Metric.add_batch) y calcular√°s la m√©trica al final.

```python
metric = load_metric("accuracy")
model.eval()
for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()
```

## Recursos adicionales

Para m√°s ejemplos de Fine-tuning, consulta:

ü§ó [Transformers Examples](https://github.com/huggingface/transformers/tree/master/examples) incluye scripts para entrenar tareas comunes de NLP en PyTorch y TensorFlow.

ü§ó [Transformers Notebooks](https://huggingface.co/docs/transformers/notebooks) contiene varios notebooks sobre c√≥mo aplicar fine-tuning un modelo para tareas espec√≠ficas en PyTorch y TensorFlow.