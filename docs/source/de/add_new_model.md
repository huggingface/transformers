<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the

‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Wie kann ich ein Modell zu ü§ó Transformers hinzuf√ºgen?

Die ü§ó Transformers-Bibliothek ist dank der Beitr√§ge der Community oft in der Lage, neue Modelle anzubieten. Aber das kann ein anspruchsvolles Projekt sein und erfordert eine eingehende Kenntnis der ü§ó Transformers-Bibliothek und des zu implementierenden Modells. Bei Hugging Face versuchen wir, mehr Mitgliedern der Community die M√∂glichkeit zu geben, aktiv Modelle hinzuzuf√ºgen, und wir haben diese Anleitung zusammengestellt, die Sie durch den Prozess des Hinzuf√ºgens eines PyTorch-Modells f√ºhrt (stellen Sie sicher, dass Sie [PyTorch installiert haben](https://pytorch.org/get-started/locally/)).

<Tip>

Wenn Sie daran interessiert sind, ein TensorFlow-Modell zu implementieren, werfen Sie einen Blick in die Anleitung [How to convert a ü§ó Transformers model to TensorFlow](add_tensorflow_model)!

</Tip>

Auf dem Weg dorthin, werden Sie:

- Einblicke in bew√§hrte Open-Source-Verfahren erhalten
- die Konstruktionsprinzipien hinter einer der beliebtesten Deep-Learning-Bibliotheken verstehen
- lernen Sie, wie Sie gro√üe Modelle effizient testen k√∂nnen
- lernen Sie, wie Sie Python-Hilfsprogramme wie `black`, `ruff` und `make fix-copies` integrieren, um sauberen und lesbaren Code zu gew√§hrleisten

Ein Mitglied des Hugging Face-Teams wird Ihnen dabei zur Seite stehen, damit Sie nicht alleine sind. ü§ó ‚ù§Ô∏è

Um loszulegen, √∂ffnen Sie eine [New model addition](https://github.com/huggingface/transformers/issues/new?assignees=&labels=New+model&template=new-model-addition.yml) Ausgabe f√ºr das Modell, das Sie in ü§ó Transformers sehen m√∂chten. Wenn Sie nicht besonders w√§hlerisch sind, wenn es darum geht, ein bestimmtes Modell beizusteuern, k√∂nnen Sie nach dem [New model label](https://github.com/huggingface/transformers/labels/New%20model) filtern, um zu sehen, ob es noch unbeanspruchte Modellanfragen gibt, und daran arbeiten.

Sobald Sie eine neue Modellanfrage er√∂ffnet haben, sollten Sie sich zun√§chst mit ü§ó Transformers vertraut machen, falls Sie das noch nicht sind!

## Allgemeiner √úberblick √ºber ü§ó Transformers

Zun√§chst sollten Sie sich einen allgemeinen √úberblick √ºber ü§ó Transformers verschaffen. ü§ó Transformers ist eine sehr meinungsfreudige Bibliothek, es ist also m√∂glich, dass
Es besteht also die M√∂glichkeit, dass Sie mit einigen der Philosophien oder Designentscheidungen der Bibliothek nicht einverstanden sind. Aus unserer Erfahrung heraus haben wir jedoch
dass die grundlegenden Designentscheidungen und Philosophien der Bibliothek entscheidend sind, um ü§ó Transformers effizient zu skalieren.
Transformatoren zu skalieren und gleichzeitig die Wartungskosten auf einem vern√ºnftigen Niveau zu halten.

Ein guter erster Ansatzpunkt, um die Bibliothek besser zu verstehen, ist die Lekt√ºre der [Dokumentation unserer Philosophie](Philosophie). Als Ergebnis unserer Arbeitsweise gibt es einige Entscheidungen, die wir versuchen, auf alle Modelle anzuwenden:

- Komposition wird im Allgemeinen gegen√ºber Abstraktion bevorzugt
- Die Duplizierung von Code ist nicht immer schlecht, wenn sie die Lesbarkeit oder Zug√§nglichkeit eines Modells stark verbessert
- Modelldateien sind so in sich geschlossen wie m√∂glich, so dass Sie, wenn Sie den Code eines bestimmten Modells lesen, idealerweise nur
  in die entsprechende Datei `modeling_....py` schauen m√ºssen.

Unserer Meinung nach ist der Code der Bibliothek nicht nur ein Mittel, um ein Produkt bereitzustellen, *z.B.* die M√∂glichkeit, BERT f√ºr
Inferenz zu verwenden, sondern auch als das Produkt selbst, das wir verbessern wollen. Wenn Sie also ein Modell hinzuf√ºgen, ist der Benutzer nicht nur die
Person, die Ihr Modell verwenden wird, sondern auch jeder, der Ihren Code liest, zu verstehen versucht und ihn m√∂glicherweise verbessert.

Lassen Sie uns daher ein wenig tiefer in das allgemeine Design der Bibliothek einsteigen.

### √úberblick √ºber die Modelle

Um ein Modell erfolgreich hinzuzuf√ºgen, ist es wichtig, die Interaktion zwischen Ihrem Modell und seiner Konfiguration zu verstehen,
[`PreTrainedModel`] und [`PretrainedConfig`]. Als Beispiel werden wir
das Modell, das zu ü§ó Transformers hinzugef√ºgt werden soll, `BrandNewBert` nennen.

Schauen wir uns das mal an:

<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers_overview.png"/>

Wie Sie sehen, machen wir in ü§ó Transformers von der Vererbung Gebrauch, aber wir beschr√§nken die Abstraktionsebene auf ein absolutes Minimum.
Minimum. Es gibt nie mehr als zwei Abstraktionsebenen f√ºr ein Modell in der Bibliothek. `BrandNewBertModel`
erbt von `BrandNewBertPreTrainedModel`, das wiederum von [`PreTrainedModel`] erbt und
das war's. In der Regel wollen wir sicherstellen, dass ein neues Modell nur von
[`PreTrainedModel`] abh√§ngt. Die wichtigen Funktionalit√§ten, die jedem neuen Modell automatisch zur Verf√ºgung gestellt werden, sind
Modell automatisch bereitgestellt werden, sind [`~PreTrainedModel.from_pretrained`] und
[`~PreTrainedModel.save_pretrained`], die f√ºr die Serialisierung und Deserialisierung verwendet werden. Alle
anderen wichtigen Funktionalit√§ten, wie `BrandNewBertModel.forward` sollten vollst√§ndig in der neuen
Skript `modeling_brand_new_bert.py` definiert werden. Als n√§chstes wollen wir sicherstellen, dass ein Modell mit einer bestimmten Kopfebene, wie z.B.
`BrandNewBertForMaskedLM` nicht von `BrandNewBertModel` erbt, sondern `BrandNewBertModel` verwendet
als Komponente, die im Forward Pass aufgerufen werden kann, um die Abstraktionsebene niedrig zu halten. Jedes neue Modell erfordert eine
Konfigurationsklasse, genannt `BrandNewBertConfig`. Diese Konfiguration wird immer als ein Attribut in
[PreTrainedModel] gespeichert und kann daher √ºber das Attribut `config` f√ºr alle Klassen aufgerufen werden
die von `BrandNewBertPreTrainedModel` erben:

```python
model = BrandNewBertModel.from_pretrained("brandy/brand_new_bert")
model.config  # model has access to its config
```

√Ñhnlich wie das Modell erbt die Konfiguration grundlegende Serialisierungs- und Deserialisierungsfunktionalit√§ten von
[`PretrainedConfig`]. Beachten Sie, dass die Konfiguration und das Modell immer in zwei verschiedene Formate serialisiert werden
unterschiedliche Formate serialisiert werden - das Modell in eine *pytorch_model.bin* Datei und die Konfiguration in eine *config.json* Datei. Aufruf von
[`~PreTrainedModel.save_pretrained`] wird automatisch
[`~PretrainedConfig.save_pretrained`] auf, so dass sowohl das Modell als auch die Konfiguration gespeichert werden.


### Code-Stil

Wenn Sie Ihr neues Modell kodieren, sollten Sie daran denken, dass Transformers eine Bibliothek mit vielen Meinungen ist und dass wir selbst ein paar Macken haben
wie der Code geschrieben werden sollte :-)

1. Der Vorw√§rtsdurchlauf Ihres Modells sollte vollst√§ndig in die Modellierungsdatei geschrieben werden und dabei v√∂llig unabh√§ngig von anderen
   Modellen in der Bibliothek. Wenn Sie einen Block aus einem anderen Modell wiederverwenden m√∂chten, kopieren Sie den Code und f√ºgen ihn mit einem
   `# Kopiert von` ein (siehe [hier](https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/roberta/modeling_roberta.py#L160)
   f√ºr ein gutes Beispiel und [hier](pr_checks#check-copies) f√ºr weitere Dokumentation zu Copied from). 
2. Der Code sollte vollst√§ndig verst√§ndlich sein, auch f√ºr einen Nicht-Muttersprachler. Das hei√üt, Sie sollten
   beschreibende Variablennamen w√§hlen und Abk√ºrzungen vermeiden. Ein Beispiel: `activation` ist `act` vorzuziehen.
   Von Variablennamen mit nur einem Buchstaben wird dringend abgeraten, es sei denn, es handelt sich um einen Index in einer for-Schleife.
3. Generell ziehen wir l√§ngeren expliziten Code einem kurzen magischen Code vor.
4. Vermeiden Sie die Unterklassifizierung von `nn.Sequential` in PyTorch, sondern unterklassifizieren Sie `nn.Module` und schreiben Sie den Vorw√§rtspass, so dass jeder
   so dass jeder, der Ihren Code verwendet, ihn schnell debuggen kann, indem er Druckanweisungen oder Haltepunkte hinzuf√ºgt.
5. Ihre Funktionssignatur sollte mit einer Typ-Annotation versehen sein. Im √úbrigen sind gute Variablennamen viel lesbarer und verst√§ndlicher
   verst√§ndlicher als Typ-Anmerkungen.

### √úbersicht der Tokenizer

Noch nicht ganz fertig :-( Dieser Abschnitt wird bald hinzugef√ºgt!

## Schritt-f√ºr-Schritt-Rezept zum Hinzuf√ºgen eines Modells zu ü§ó Transformers

Jeder hat andere Vorlieben, was die Portierung eines Modells angeht. Daher kann es sehr hilfreich sein, wenn Sie sich Zusammenfassungen ansehen
wie andere Mitwirkende Modelle auf Hugging Face portiert haben. Hier ist eine Liste von Blogbeitr√§gen aus der Community, wie man ein Modell portiert:

1. [Portierung eines GPT2-Modells](https://medium.com/huggingface/from-tensorflow-to-pytorch-265f40ef2a28) von [Thomas](https://huggingface.co/thomwolf)
2. [Portierung des WMT19 MT-Modells](https://huggingface.co/blog/porting-fsmt) von [Stas](https://huggingface.co/stas)

Aus Erfahrung k√∂nnen wir Ihnen sagen, dass die wichtigsten Dinge, die Sie beim Hinzuf√ºgen eines Modells beachten m√ºssen, sind:

- Erfinden Sie das Rad nicht neu! Die meisten Teile des Codes, den Sie f√ºr das neue ü§ó Transformers-Modell hinzuf√ºgen werden, existieren bereits
  irgendwo in ü§ó Transformers. Nehmen Sie sich etwas Zeit, um √§hnliche, bereits vorhandene Modelle und Tokenizer zu finden, die Sie kopieren k√∂nnen
  von. [grep](https://www.gnu.org/software/grep/) und [rg](https://github.com/BurntSushi/ripgrep) sind Ihre
  Freunde. Beachten Sie, dass es sehr gut m√∂glich ist, dass der Tokenizer Ihres Modells auf einer Modellimplementierung basiert und
  und der Modellierungscode Ihres Modells auf einer anderen. *Z.B.* Der Modellierungscode von FSMT basiert auf BART, w√§hrend der Tokenizer-Code von FSMT
  auf XLM basiert.
- Es handelt sich eher um eine technische als um eine wissenschaftliche Herausforderung. Sie sollten mehr Zeit auf die Schaffung einer
  eine effiziente Debugging-Umgebung zu schaffen, als zu versuchen, alle theoretischen Aspekte des Modells in dem Papier zu verstehen.
- Bitten Sie um Hilfe, wenn Sie nicht weiterkommen! Modelle sind der Kernbestandteil von ü§ó Transformers, so dass wir bei Hugging Face mehr als
  mehr als gl√ºcklich, Ihnen bei jedem Schritt zu helfen, um Ihr Modell hinzuzuf√ºgen. Z√∂gern Sie nicht zu fragen, wenn Sie merken, dass Sie nicht weiterkommen.
  Fortschritte machen.

Im Folgenden versuchen wir, Ihnen ein allgemeines Rezept an die Hand zu geben, das uns bei der Portierung eines Modells auf ü§ó Transformers am n√ºtzlichsten erschien.

Die folgende Liste ist eine Zusammenfassung all dessen, was getan werden muss, um ein Modell hinzuzuf√ºgen und kann von Ihnen als To-Do verwendet werden
Liste verwenden:

‚òê (Optional) Verstehen der theoretischen Aspekte des Modells<br>
‚òê Vorbereiten der ü§ó Transformers-Entwicklungsumgebung<br>
‚òê Debugging-Umgebung des urspr√ºnglichen Repositorys eingerichtet<br>
‚òê Skript erstellt, das den Durchlauf `forward()` unter Verwendung des urspr√ºnglichen Repositorys und des Checkpoints erfolgreich durchf√ºhrt<br>
‚òê Erfolgreich das Modellskelett zu ü§ó Transformers hinzugef√ºgt<br>
‚òê Erfolgreiche Umwandlung des urspr√ºnglichen Pr√ºfpunkts in den ü§ó Transformers-Pr√ºfpunkt<br>
‚òê Erfolgreich den Durchlauf `forward()` in ü§ó Transformers ausgef√ºhrt, der eine identische Ausgabe wie der urspr√ºngliche Pr√ºfpunkt liefert<br>
‚òê Modell-Tests in ü§ó Transformers abgeschlossen<br>
‚òê Erfolgreich Tokenizer in ü§ó Transformers hinzugef√ºgt<br>
‚òê End-to-End-Integrationstests ausgef√ºhrt<br>
‚òê Docs fertiggestellt<br>
‚òê Modellgewichte in den Hub hochgeladen<br>
‚òê Die Pull-Anfrage eingereicht<br>
‚òê (Optional) Hinzuf√ºgen eines Demo-Notizbuchs

F√ºr den Anfang empfehlen wir in der Regel, mit einem guten theoretischen Verst√§ndnis von `BrandNewBert` zu beginnen. Wie auch immer,
wenn Sie es vorziehen, die theoretischen Aspekte des Modells *on-the-job* zu verstehen, dann ist es v√∂llig in Ordnung, direkt in die
in die Code-Basis von `BrandNewBert` einzutauchen. Diese Option k√∂nnte f√ºr Sie besser geeignet sein, wenn Ihre technischen F√§higkeiten besser sind als
als Ihre theoretischen F√§higkeiten, wenn Sie Schwierigkeiten haben, die Arbeit von `BrandNewBert` zu verstehen, oder wenn Sie einfach Spa√ü am Programmieren
mehr Spa√ü am Programmieren haben als am Lesen wissenschaftlicher Abhandlungen.

### 1. (Optional) Theoretische Aspekte von BrandNewBert

Sie sollten sich etwas Zeit nehmen, um die Abhandlung von *BrandNewBert* zu lesen, falls eine solche Beschreibung existiert. M√∂glicherweise gibt es gro√üe
Abschnitte des Papiers, die schwer zu verstehen sind. Wenn das der Fall ist, ist das in Ordnung - machen Sie sich keine Sorgen! Das Ziel ist
ist es nicht, ein tiefes theoretisches Verst√§ndnis des Papiers zu erlangen, sondern die notwendigen Informationen zu extrahieren, um
das Modell effektiv in ü§ó Transformers zu implementieren. Das hei√üt, Sie m√ºssen nicht zu viel Zeit auf die
theoretischen Aspekten verbringen, sondern sich lieber auf die praktischen Aspekte konzentrieren, n√§mlich:

- Welche Art von Modell ist *brand_new_bert*? BERT-√§hnliches Modell nur f√ºr den Encoder? GPT2-√§hnliches reines Decoder-Modell? BART-√§hnliches
  Encoder-Decoder-Modell? Sehen Sie sich die [model_summary](model_summary) an, wenn Sie mit den Unterschieden zwischen diesen Modellen nicht vertraut sind.
- Was sind die Anwendungen von *brand_new_bert*? Textklassifizierung? Texterzeugung? Seq2Seq-Aufgaben, *z.B.,*
  Zusammenfassungen?
- Was ist die neue Eigenschaft des Modells, die es von BERT/GPT-2/BART unterscheidet?
- Welches der bereits existierenden [ü§ó Transformers-Modelle](https://huggingface.co/transformers/#contents) ist am √§hnlichsten
  √§hnlich wie *brand_new_bert*?
- Welche Art von Tokenizer wird verwendet? Ein Satzteil-Tokenisierer? Ein Wortst√ºck-Tokenisierer? Ist es derselbe Tokenisierer, der f√ºr
  f√ºr BERT oder BART?

Nachdem Sie das Gef√ºhl haben, einen guten √úberblick √ºber die Architektur des Modells erhalten zu haben, k√∂nnen Sie dem
Hugging Face Team schreiben und Ihre Fragen stellen. Dazu k√∂nnen Fragen zur Architektur des Modells geh√∂ren,
seiner Aufmerksamkeitsebene usw. Wir werden Ihnen gerne weiterhelfen.

### 2. Bereiten Sie als n√§chstes Ihre Umgebung vor

1. Forken Sie das [Repository](https://github.com/huggingface/transformers), indem Sie auf der Seite des Repositorys auf die Schaltfl√§che 'Fork' klicken.
   Seite des Repositorys klicken. Dadurch wird eine Kopie des Codes unter Ihrem GitHub-Benutzerkonto erstellt.

2. Klonen Sie Ihren `transformers` Fork auf Ihre lokale Festplatte und f√ºgen Sie das Basis-Repository als Remote hinzu:

```bash
git clone https://github.com/[your Github handle]/transformers.git
cd transformers
git remote add upstream https://github.com/huggingface/transformers.git
```

3. Richten Sie eine Entwicklungsumgebung ein, indem Sie z.B. den folgenden Befehl ausf√ºhren:

```bash
python -m venv .env
source .env/bin/activate
pip install -e ".[dev]"
```

Abh√§ngig von Ihrem Betriebssystem und da die Anzahl der optionalen Abh√§ngigkeiten von Transformers w√§chst, kann es sein, dass Sie bei diesem Befehl einen
Fehler mit diesem Befehl. Stellen Sie in diesem Fall sicher, dass Sie das Deep Learning Framework, mit dem Sie arbeiten, installieren
(PyTorch, TensorFlow und/oder Flax) und f√ºhren Sie es aus:

```bash
pip install -e ".[quality]"
```

was f√ºr die meisten Anwendungsf√§lle ausreichend sein sollte. Sie k√∂nnen dann zum √ºbergeordneten Verzeichnis zur√ºckkehren

```bash
cd ..
```

4. Wir empfehlen, die PyTorch-Version von *brand_new_bert* zu Transformers hinzuzuf√ºgen. Um PyTorch zu installieren, folgen Sie bitte den
   Anweisungen auf https://pytorch.org/get-started/locally/.

**Anmerkung:** Sie m√ºssen CUDA nicht installiert haben. Es reicht aus, das neue Modell auf der CPU zum Laufen zu bringen.

5. Um *brand_new_bert* zu portieren, ben√∂tigen Sie au√üerdem Zugriff auf das Original-Repository:

```bash
git clone https://github.com/org_that_created_brand_new_bert_org/brand_new_bert.git
cd brand_new_bert
pip install -e .
```

Jetzt haben Sie eine Entwicklungsumgebung eingerichtet, um *brand_new_bert* auf ü§ó Transformers zu portieren.

### 3.-4. F√ºhren Sie einen Pre-Training-Checkpoint mit dem Original-Repository durch

Zun√§chst werden Sie mit dem urspr√ºnglichen *brand_new_bert* Repository arbeiten. Oft ist die urspr√ºngliche Implementierung sehr
"forschungslastig". Das bedeutet, dass es an Dokumentation mangeln kann und der Code schwer zu verstehen sein kann. Aber das sollte
genau Ihre Motivation sein, *brand_new_bert* neu zu implementieren. Eines unserer Hauptziele bei Hugging Face ist es, *die Menschen dazu zu bringen
auf den Schultern von Giganten zu stehen*, was sich hier sehr gut darin ausdr√ºckt, dass wir ein funktionierendes Modell nehmen und es umschreiben, um es so
es so **zug√§nglich, benutzerfreundlich und sch√∂n** wie m√∂glich zu machen. Dies ist die wichtigste Motivation f√ºr die Neuimplementierung von
Modelle in ü§ó Transformers umzuwandeln - der Versuch, komplexe neue NLP-Technologie f√ºr **jeden** zug√§nglich zu machen.

Sie sollten damit beginnen, indem Sie in das Original-Repository eintauchen.

Die erfolgreiche Ausf√ºhrung des offiziellen Pre-Trainingsmodells im Original-Repository ist oft **der schwierigste** Schritt.
Unserer Erfahrung nach ist es sehr wichtig, dass Sie einige Zeit damit verbringen, sich mit der urspr√ºnglichen Code-Basis vertraut zu machen. Sie m√ºssen
das Folgende herausfinden:

- Wo finden Sie die vortrainierten Gewichte?
- Wie l√§dt man die vorab trainierten Gewichte in das entsprechende Modell?
- Wie kann der Tokenizer unabh√§ngig vom Modell ausgef√ºhrt werden?
- Verfolgen Sie einen Forward Pass, damit Sie wissen, welche Klassen und Funktionen f√ºr einen einfachen Forward Pass erforderlich sind. Normalerweise,
  m√ºssen Sie nur diese Funktionen reimplementieren.
- Sie m√ºssen in der Lage sein, die wichtigen Komponenten des Modells zu finden: Wo befindet sich die Klasse des Modells? Gibt es Unterklassen des Modells,
  *z.B.* EncoderModel, DecoderModel? Wo befindet sich die Selbstaufmerksamkeitsschicht? Gibt es mehrere verschiedene Aufmerksamkeitsebenen,
  *z.B.* *Selbstaufmerksamkeit*, *Kreuzaufmerksamkeit*...?
- Wie k√∂nnen Sie das Modell in der urspr√ºnglichen Umgebung des Repo debuggen? M√ºssen Sie *print* Anweisungen hinzuf√ºgen, k√∂nnen Sie
  mit einem interaktiven Debugger wie *ipdb* arbeiten oder sollten Sie eine effiziente IDE zum Debuggen des Modells verwenden, wie z.B. PyCharm?

Es ist sehr wichtig, dass Sie, bevor Sie mit der Portierung beginnen, den Code im Original-Repository **effizient** debuggen k√∂nnen
Repository k√∂nnen! Denken Sie auch daran, dass Sie mit einer Open-Source-Bibliothek arbeiten, also z√∂gern Sie nicht, ein Problem oder
oder sogar eine Pull-Anfrage im Original-Repository zu stellen. Die Betreuer dieses Repositorys sind wahrscheinlich sehr froh dar√ºber
dass jemand in ihren Code schaut!

An diesem Punkt liegt es wirklich an Ihnen, welche Debugging-Umgebung und Strategie Sie zum Debuggen des urspr√ºnglichen
Modell zu debuggen. Wir raten dringend davon ab, eine kostspielige GPU-Umgebung einzurichten, sondern arbeiten Sie einfach auf einer CPU, sowohl wenn Sie mit dem
in das urspr√ºngliche Repository einzutauchen und auch, wenn Sie beginnen, die ü§ó Transformers-Implementierung des Modells zu schreiben. Nur
ganz am Ende, wenn das Modell bereits erfolgreich auf ü§ó Transformers portiert wurde, sollte man √ºberpr√ºfen, ob das
Modell auch auf der GPU wie erwartet funktioniert.

Im Allgemeinen gibt es zwei m√∂gliche Debugging-Umgebungen f√ºr die Ausf√ºhrung des Originalmodells

- [Jupyter notebooks](https://jupyter.org/) / [google colab](https://colab.research.google.com/notebooks/intro.ipynb)
- Lokale Python-Skripte.

Jupyter-Notebooks haben den Vorteil, dass sie eine zellenweise Ausf√ºhrung erm√∂glichen, was hilfreich sein kann, um logische Komponenten besser voneinander zu trennen und
logische Komponenten voneinander zu trennen und schnellere Debugging-Zyklen zu haben, da Zwischenergebnisse gespeichert werden k√∂nnen. Au√üerdem,
Au√üerdem lassen sich Notebooks oft leichter mit anderen Mitwirkenden teilen, was sehr hilfreich sein kann, wenn Sie das Hugging Face Team um Hilfe bitten m√∂chten.
Face Team um Hilfe bitten. Wenn Sie mit Jupyter-Notizb√ºchern vertraut sind, empfehlen wir Ihnen dringend, mit ihnen zu arbeiten.

Der offensichtliche Nachteil von Jupyter-Notizb√ºchern ist, dass Sie, wenn Sie nicht daran gew√∂hnt sind, mit ihnen zu arbeiten, einige Zeit damit verbringen m√ºssen
einige Zeit damit verbringen m√ºssen, sich an die neue Programmierumgebung zu gew√∂hnen, und dass Sie m√∂glicherweise Ihre bekannten Debugging-Tools nicht mehr verwenden k√∂nnen
wie z.B. `ipdb` nicht mehr verwenden k√∂nnen.

F√ºr jede Codebasis ist es immer ein guter erster Schritt, einen **kleinen** vortrainierten Checkpoint zu laden und in der Lage zu sein, einen
einzelnen Vorw√§rtsdurchlauf mit einem Dummy-Integer-Vektor von Eingabe-IDs als Eingabe zu reproduzieren. Ein solches Skript k√∂nnte wie folgt aussehen (in
Pseudocode):

```python
model = BrandNewBertModel.load_pretrained_checkpoint("/path/to/checkpoint/")
input_ids = [0, 4, 5, 2, 3, 7, 9]  # vector of input ids
original_output = model.predict(input_ids)
```

Was die Debugging-Strategie anbelangt, so k√∂nnen Sie im Allgemeinen aus mehreren Strategien w√§hlen:

- Zerlegen Sie das urspr√ºngliche Modell in viele kleine testbare Komponenten und f√ºhren Sie f√ºr jede dieser Komponenten einen Vorw√§rtsdurchlauf zur
  √úberpr√ºfung
- Zerlegen Sie das urspr√ºngliche Modell nur in den urspr√ºnglichen *Tokenizer* und das urspr√ºngliche *Modell*, f√ºhren Sie einen Vorw√§rtsdurchlauf f√ºr diese Komponenten durch
  und verwenden Sie dazwischenliegende Druckanweisungen oder Haltepunkte zur √úberpr√ºfung.

Auch hier bleibt es Ihnen √ºberlassen, welche Strategie Sie w√§hlen. Oft ist die eine oder die andere Strategie vorteilhaft, je nach der urspr√ºnglichen Codebasis
Basis.

Wenn die urspr√ºngliche Codebasis es Ihnen erlaubt, das Modell in kleinere Teilkomponenten zu zerlegen, *z.B.* wenn die urspr√ºngliche
Code-Basis problemlos im Eager-Modus ausgef√ºhrt werden kann, lohnt es sich in der Regel, dies zu tun. Es gibt einige wichtige Vorteile
am Anfang den schwierigeren Weg zu gehen:

- Wenn Sie sp√§ter das urspr√ºngliche Modell mit der Hugging Face-Implementierung vergleichen, k√∂nnen Sie automatisch √ºberpr√ºfen, ob
  f√ºr jede Komponente einzeln √ºberpr√ºfen, ob die entsprechende Komponente der ü§ó Transformers-Implementierung √ºbereinstimmt, anstatt sich auf
  anstatt sich auf den visuellen Vergleich √ºber Druckanweisungen zu verlassen
- k√∂nnen Sie das gro√üe Problem der Portierung eines Modells in kleinere Probleme der Portierung einzelner Komponenten zerlegen
  einzelnen Komponenten zu zerlegen und so Ihre Arbeit besser zu strukturieren
- Die Aufteilung des Modells in logisch sinnvolle Komponenten hilft Ihnen, einen besseren √úberblick √ºber das Design des Modells zu bekommen
  und somit das Modell besser zu verstehen
- In einem sp√§teren Stadium helfen Ihnen diese komponentenweisen Tests dabei, sicherzustellen, dass keine Regressionen auftreten, w√§hrend Sie fortfahren
  Ihren Code √§ndern

[Lysandre's](https://gist.github.com/LysandreJik/db4c948f6b4483960de5cbac598ad4ed) Integrationstests f√ºr ELECTRA
gibt ein sch√∂nes Beispiel daf√ºr, wie dies geschehen kann.

Wenn die urspr√ºngliche Codebasis jedoch sehr komplex ist oder nur die Ausf√ºhrung von Zwischenkomponenten in einem kompilierten Modus erlaubt,
k√∂nnte es zu zeitaufw√§ndig oder sogar unm√∂glich sein, das Modell in kleinere testbare Teilkomponenten zu zerlegen. Ein gutes
Beispiel ist die [T5's MeshTensorFlow](https://github.com/tensorflow/mesh/tree/master/mesh_tensorflow) Bibliothek, die sehr komplex ist
sehr komplex ist und keine einfache M√∂glichkeit bietet, das Modell in seine Unterkomponenten zu zerlegen. Bei solchen Bibliotheken ist man
oft auf die √úberpr√ºfung von Druckanweisungen angewiesen.

Unabh√§ngig davon, welche Strategie Sie w√§hlen, ist die empfohlene Vorgehensweise oft die gleiche, n√§mlich dass Sie mit der Fehlersuche in den
die Anfangsebenen zuerst und die Endebenen zuletzt debuggen.

Es wird empfohlen, dass Sie die Ausgaben der folgenden Ebenen abrufen, entweder durch Druckanweisungen oder Unterkomponentenfunktionen
Schichten in der folgenden Reihenfolge abrufen:

1. Rufen Sie die Eingabe-IDs ab, die an das Modell √ºbergeben wurden
2. Rufen Sie die Worteinbettungen ab
3. Rufen Sie die Eingabe der ersten Transformer-Schicht ab
4. Rufen Sie die Ausgabe der ersten Transformer-Schicht ab
5. Rufen Sie die Ausgabe der folgenden n - 1 Transformer-Schichten ab
6. Rufen Sie die Ausgabe des gesamten BrandNewBert Modells ab

Die Eingabe-IDs sollten dabei aus einem Array von Ganzzahlen bestehen, *z.B.* `input_ids = [0, 4, 4, 3, 2, 4, 1, 7, 19]`

Die Ausgaben der folgenden Schichten bestehen oft aus mehrdimensionalen Float-Arrays und k√∂nnen wie folgt aussehen:

```
[[
 [-0.1465, -0.6501,  0.1993,  ...,  0.1451,  0.3430,  0.6024],
 [-0.4417, -0.5920,  0.3450,  ..., -0.3062,  0.6182,  0.7132],
 [-0.5009, -0.7122,  0.4548,  ..., -0.3662,  0.6091,  0.7648],
 ...,
 [-0.5613, -0.6332,  0.4324,  ..., -0.3792,  0.7372,  0.9288],
 [-0.5416, -0.6345,  0.4180,  ..., -0.3564,  0.6992,  0.9191],
 [-0.5334, -0.6403,  0.4271,  ..., -0.3339,  0.6533,  0.8694]]],
```

Wir erwarten, dass jedes zu ü§ó Transformers hinzugef√ºgte Modell eine Reihe von Integrationstests besteht, was bedeutet, dass das urspr√ºngliche
Modell und die neu implementierte Version in ü§ó Transformers exakt dieselbe Ausgabe liefern m√ºssen, und zwar mit einer Genauigkeit von 0,001!
Da es normal ist, dass das exakt gleiche Modell, das in verschiedenen Bibliotheken geschrieben wurde, je nach Bibliotheksrahmen eine leicht unterschiedliche Ausgabe liefern kann
eine leicht unterschiedliche Ausgabe liefern kann, akzeptieren wir eine Fehlertoleranz von 1e-3 (0,001). Es reicht nicht aus, wenn das Modell
fast das gleiche Ergebnis liefert, sie m√ºssen fast identisch sein. Daher werden Sie sicherlich die Zwischenergebnisse
Zwischenergebnisse der ü§ó Transformers-Version mehrfach mit den Zwischenergebnissen der urspr√ºnglichen Implementierung von
*brand_new_bert* vergleichen. In diesem Fall ist eine **effiziente** Debugging-Umgebung des urspr√ºnglichen Repositorys absolut
wichtig ist. Hier sind einige Ratschl√§ge, um Ihre Debugging-Umgebung so effizient wie m√∂glich zu gestalten.

- Finden Sie den besten Weg, um Zwischenergebnisse zu debuggen. Ist das urspr√ºngliche Repository in PyTorch geschrieben? Dann sollten Sie
  dann sollten Sie sich wahrscheinlich die Zeit nehmen, ein l√§ngeres Skript zu schreiben, das das urspr√ºngliche Modell in kleinere Unterkomponenten zerlegt, um
  Zwischenwerte abzurufen. Ist das urspr√ºngliche Repository in Tensorflow 1 geschrieben? Dann m√ºssen Sie sich m√∂glicherweise auf die
  TensorFlow Druckoperationen wie [tf.print](https://www.tensorflow.org/api_docs/python/tf/print) verlassen, um die
  Zwischenwerte auszugeben. Ist das urspr√ºngliche Repository in Jax geschrieben? Dann stellen Sie sicher, dass das Modell **nicht jitted** ist, wenn
  wenn Sie den Vorw√§rtsdurchlauf ausf√ºhren, *z.B.* schauen Sie sich [dieser Link](https://github.com/google/jax/issues/196) an.
- Verwenden Sie den kleinsten vortrainierten Pr√ºfpunkt, den Sie finden k√∂nnen. Je kleiner der Pr√ºfpunkt ist, desto schneller wird Ihr Debugging-Zyklus
  wird. Es ist nicht effizient, wenn Ihr vorab trainiertes Modell so gro√ü ist, dass Ihr Vorw√§rtsdurchlauf mehr als 10 Sekunden dauert.
  Falls nur sehr gro√üe Checkpoints verf√ºgbar sind, kann es sinnvoller sein, ein Dummy-Modell in der neuen
  Umgebung mit zuf√§llig initialisierten Gewichten zu erstellen und diese Gewichte zum Vergleich mit der ü§ó Transformers-Version
  Ihres Modells
- Vergewissern Sie sich, dass Sie den einfachsten Weg w√§hlen, um einen Forward Pass im urspr√ºnglichen Repository aufzurufen. Idealerweise sollten Sie
  die Funktion im originalen Repository finden, die **nur** einen einzigen Vorw√§rtspass aufruft, *d.h.* die oft aufgerufen wird
  Vorhersagen", "Auswerten", "Vorw√§rts" oder "Aufruf" genannt wird. Sie wollen keine Funktion debuggen, die `forward` aufruft
  mehrfach aufruft, *z.B.* um Text zu erzeugen, wie `autoregressive_sample`, `generate`.
- Versuchen Sie, die Tokenisierung vom *Forward*-Pass des Modells zu trennen. Wenn das Original-Repository Beispiele zeigt, bei denen
  Sie eine Zeichenkette eingeben m√ºssen, dann versuchen Sie herauszufinden, an welcher Stelle im Vorw√§rtsaufruf die Zeichenketteneingabe in Eingabe-IDs ge√§ndert wird
  ge√§ndert wird und beginnen Sie an dieser Stelle. Das k√∂nnte bedeuten, dass Sie m√∂glicherweise selbst ein kleines Skript schreiben oder den
  Originalcode so √§ndern m√ºssen, dass Sie die ids direkt eingeben k√∂nnen, anstatt eine Zeichenkette einzugeben.
- Vergewissern Sie sich, dass sich das Modell in Ihrem Debugging-Setup **nicht** im Trainingsmodus befindet, der oft dazu f√ºhrt, dass das Modell
  Dies f√ºhrt h√§ufig zu zuf√§lligen Ergebnissen, da das Modell mehrere Dropout-Schichten enth√§lt. Stellen Sie sicher, dass der Vorw√§rtsdurchlauf in Ihrer Debugging
  Umgebung **deterministisch** ist, damit die Dropout-Schichten nicht verwendet werden. Oder verwenden Sie *transformers.utils.set_seed*.
  wenn sich die alte und die neue Implementierung im selben Framework befinden.

Im folgenden Abschnitt finden Sie genauere Details/Tipps, wie Sie dies f√ºr *brand_new_bert* tun k√∂nnen.

### 5.-14. Portierung von BrandNewBert auf ü§ó Transformatoren

Als n√§chstes k√∂nnen Sie endlich damit beginnen, neuen Code zu ü§ó Transformers hinzuzuf√ºgen. Gehen Sie in den Klon Ihres ü§ó Transformers Forks:

```bash
cd transformers
```

In dem speziellen Fall, dass Sie ein Modell hinzuf√ºgen, dessen Architektur genau mit der Modellarchitektur eines
Modells √ºbereinstimmt, m√ºssen Sie nur ein Konvertierungsskript hinzuf√ºgen, wie in [diesem Abschnitt](#write-a-conversion-script) beschrieben.
In diesem Fall k√∂nnen Sie einfach die gesamte Modellarchitektur des bereits vorhandenen Modells wiederverwenden.

Andernfalls beginnen wir mit der Erstellung eines neuen Modells. Sie haben hier zwei M√∂glichkeiten:

- `transformers-cli add-new-model-like`, um ein neues Modell wie ein bestehendes hinzuzuf√ºgen
- `transformers-cli add-new-model`, um ein neues Modell aus unserer Vorlage hinzuzuf√ºgen (sieht dann aus wie BERT oder Bart, je nachdem, welche Art von Modell Sie w√§hlen)

In beiden F√§llen werden Sie mit einem Fragebogen aufgefordert, die grundlegenden Informationen zu Ihrem Modell auszuf√ºllen. F√ºr den zweiten Befehl m√ºssen Sie `cookiecutter` installieren, weitere Informationen dazu finden Sie [hier](https://github.com/huggingface/transformers/tree/main/templates/adding_a_new_model).

**Er√∂ffnen Sie einen Pull Request auf dem Haupt-Repositorium huggingface/transformers**

Bevor Sie mit der Anpassung des automatisch generierten Codes beginnen, ist es nun an der Zeit, einen "Work in progress (WIP)" Pull
Anfrage, *z.B.* "[WIP] Add *brand_new_bert*", in ü§ó Transformers zu √∂ffnen, damit Sie und das Hugging Face Team
Seite an Seite an der Integration des Modells in ü§ó Transformers arbeiten k√∂nnen.

Sie sollten Folgendes tun:

1. Erstellen Sie eine Verzweigung mit einem beschreibenden Namen von Ihrer Hauptverzweigung

```bash
git checkout -b add_brand_new_bert
```

2. Best√§tigen Sie den automatisch generierten Code:

```bash
git add .
git commit
```

3. Abrufen und zur√ºcksetzen auf die aktuelle Haupt

```bash
git fetch upstream
git rebase upstream/main
```

4. √úbertragen Sie die √Ñnderungen auf Ihr Konto mit:

```bash
git push -u origin a-descriptive-name-for-my-changes
```

5. Wenn Sie zufrieden sind, gehen Sie auf die Webseite Ihrer Abspaltung auf GitHub. Klicken Sie auf "Pull request". Stellen Sie sicher, dass Sie das
   GitHub-Handle einiger Mitglieder des Hugging Face-Teams als Reviewer hinzuzuf√ºgen, damit das Hugging Face-Team √ºber zuk√ºnftige √Ñnderungen informiert wird.
   zuk√ºnftige √Ñnderungen benachrichtigt wird.

6. √Ñndern Sie den PR in einen Entwurf, indem Sie auf der rechten Seite der GitHub-Pull-Request-Webseite auf "In Entwurf umwandeln" klicken.

Vergessen Sie im Folgenden nicht, wenn Sie Fortschritte gemacht haben, Ihre Arbeit zu committen und in Ihr Konto zu pushen, damit sie in der Pull-Anfrage erscheint.
damit sie in der Pull-Anfrage angezeigt wird. Au√üerdem sollten Sie darauf achten, dass Sie Ihre Arbeit von Zeit zu Zeit mit dem aktuellen main
von Zeit zu Zeit zu aktualisieren, indem Sie dies tun:

```bash
git fetch upstream
git merge upstream/main
```

Generell sollten Sie alle Fragen, die Sie in Bezug auf das Modell oder Ihre Implementierung haben, in Ihrem PR stellen und
in der PR diskutiert/gel√∂st werden. Auf diese Weise wird das Hugging Face Team immer benachrichtigt, wenn Sie neuen Code einreichen oder
wenn Sie eine Frage haben. Es ist oft sehr hilfreich, das Hugging Face-Team auf Ihren hinzugef√ºgten Code hinzuweisen, damit das Hugging Face-Team Ihr Problem oder Ihre Frage besser verstehen kann.
Face-Team Ihr Problem oder Ihre Frage besser verstehen kann.

Gehen Sie dazu auf die Registerkarte "Ge√§nderte Dateien", auf der Sie alle Ihre √Ñnderungen sehen, gehen Sie zu einer Zeile, zu der Sie eine Frage stellen m√∂chten
eine Frage stellen m√∂chten, und klicken Sie auf das "+"-Symbol, um einen Kommentar hinzuzuf√ºgen. Wenn eine Frage oder ein Problem gel√∂st wurde,
k√∂nnen Sie auf die Schaltfl√§che "L√∂sen" des erstellten Kommentars klicken.

Auf dieselbe Weise wird das Hugging Face-Team Kommentare √∂ffnen, wenn es Ihren Code √ºberpr√ºft. Wir empfehlen, die meisten Fragen
auf GitHub in Ihrem PR zu stellen. F√ºr einige sehr allgemeine Fragen, die f√ºr die √ñffentlichkeit nicht sehr n√ºtzlich sind, k√∂nnen Sie das
Hugging Face Team per Slack oder E-Mail zu stellen.

**5. Passen Sie den Code der generierten Modelle f√ºr brand_new_bert** an.

Zun√§chst werden wir uns nur auf das Modell selbst konzentrieren und uns nicht um den Tokenizer k√ºmmern. Den gesamten relevanten Code sollten Sie
finden Sie in den generierten Dateien `src/transformers/models/brand_new_bert/modeling_brand_new_bert.py` und
`src/transformers/models/brand_new_bert/configuration_brand_new_bert.py`.

Jetzt k√∂nnen Sie endlich mit dem Programmieren beginnen :). Der generierte Code in
`src/transformers/models/brand_new_bert/modeling_brand_new_bert.py` wird entweder die gleiche Architektur wie BERT haben, wenn
wenn es sich um ein reines Encoder-Modell handelt oder BART, wenn es sich um ein Encoder-Decoder-Modell handelt. An diesem Punkt sollten Sie sich daran erinnern, was
was Sie am Anfang √ºber die theoretischen Aspekte des Modells gelernt haben: *Wie unterscheidet sich das Modell von BERT oder
BART?*". Implementieren Sie diese √Ñnderungen, was oft bedeutet, dass Sie die *Selbstaufmerksamkeitsschicht*, die Reihenfolge der Normalisierungsschicht usw. √§ndern m√ºssen.
Schicht usw... Auch hier ist es oft n√ºtzlich, sich die √§hnliche Architektur bereits bestehender Modelle in Transformers anzusehen, um ein besseres Gef√ºhl daf√ºr zu bekommen
ein besseres Gef√ºhl daf√ºr zu bekommen, wie Ihr Modell implementiert werden sollte.

**Beachten Sie**, dass Sie an diesem Punkt nicht sehr sicher sein m√ºssen, dass Ihr Code v√∂llig korrekt oder sauber ist. Vielmehr ist es
Sie sollten vielmehr eine erste *unbereinigte*, kopierte Version des urspr√ºnglichen Codes in
src/transformers/models/brand_new_bert/modeling_brand_new_bert.py" hinzuzuf√ºgen, bis Sie das Gef√ºhl haben, dass der gesamte notwendige Code
hinzugef√ºgt wurde. Unserer Erfahrung nach ist es viel effizienter, schnell eine erste Version des erforderlichen Codes hinzuzuf√ºgen und
den Code iterativ mit dem Konvertierungsskript zu verbessern/korrigieren, wie im n√§chsten Abschnitt beschrieben. Das einzige, was
zu diesem Zeitpunkt funktionieren muss, ist, dass Sie die ü§ó Transformers-Implementierung von *brand_new_bert* instanziieren k√∂nnen, *d.h.* der
folgende Befehl sollte funktionieren:

```python
from transformers import BrandNewBertModel, BrandNewBertConfig

model = BrandNewBertModel(BrandNewBertConfig())
```

Der obige Befehl erstellt ein Modell gem√§√ü den Standardparametern, die in `BrandNewBertConfig()` definiert sind, mit
zuf√§lligen Gewichten und stellt damit sicher, dass die `init()` Methoden aller Komponenten funktionieren.

Beachten Sie, dass alle zuf√§lligen Initialisierungen in der Methode `_init_weights` Ihres `BrandnewBertPreTrainedModel` stattfinden sollten.
Klasse erfolgen sollte. Sie sollte alle Blattmodule in Abh√§ngigkeit von den Variablen der Konfiguration initialisieren. Hier ist ein Beispiel mit der
BERT `_init_weights` Methode:

```py
def _init_weights(self, module):
    """Initialize the weights"""
    if isinstance(module, nn.Linear):
        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
        if module.bias is not None:
            module.bias.data.zero_()
    elif isinstance(module, nn.Embedding):
        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
        if module.padding_idx is not None:
            module.weight.data[module.padding_idx].zero_()
    elif isinstance(module, nn.LayerNorm):
        module.bias.data.zero_()
        module.weight.data.fill_(1.0)
```

Sie k√∂nnen weitere benutzerdefinierte Schemata verwenden, wenn Sie eine spezielle Initialisierung f√ºr einige Module ben√∂tigen. Zum Beispiel in
`Wav2Vec2ForPreTraining` m√ºssen die letzten beiden linearen Schichten die Initialisierung des regul√§ren PyTorch `nn.Linear` haben.
aber alle anderen sollten eine Initialisierung wie oben verwenden. Dies ist wie folgt kodiert:

```py
def _init_weights(self, module):
    """Initialize the weights"""
    if isinstance(module, Wav2Vec2ForPreTraining):
        module.project_hid.reset_parameters()
        module.project_q.reset_parameters()
        module.project_hid._is_hf_initialized = True
        module.project_q._is_hf_initialized = True
    elif isinstance(module, nn.Linear):
        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
        if module.bias is not None:
            module.bias.data.zero_()
```

Das Flag `_is_hf_initialized` wird intern verwendet, um sicherzustellen, dass wir ein Submodul nur einmal initialisieren. Wenn Sie es auf
`True` f√ºr `module.project_q` und `module.project_hid` setzen, stellen wir sicher, dass die benutzerdefinierte Initialisierung, die wir vorgenommen haben, sp√§ter nicht √ºberschrieben wird,
die Funktion `_init_weights` nicht auf sie angewendet wird.

**6. Schreiben Sie ein Konvertierungsskript**

Als n√§chstes sollten Sie ein Konvertierungsskript schreiben, mit dem Sie den Checkpoint, den Sie zum Debuggen von *brand_new_bert* im
im urspr√ºnglichen Repository in einen Pr√ºfpunkt konvertieren, der mit Ihrer gerade erstellten ü§ó Transformers-Implementierung von
*brand_new_bert*. Es ist nicht ratsam, das Konvertierungsskript von Grund auf neu zu schreiben, sondern die bereits
bestehenden Konvertierungsskripten in ü§ó Transformers nach einem Skript zu suchen, das f√ºr die Konvertierung eines √§hnlichen Modells verwendet wurde, das im
demselben Framework wie *brand_new_bert* geschrieben wurde. Normalerweise reicht es aus, ein bereits vorhandenes Konvertierungsskript zu kopieren und
es f√ºr Ihren Anwendungsfall leicht anzupassen. Z√∂gern Sie nicht, das Hugging Face Team zu bitten, Sie auf ein √§hnliches, bereits vorhandenes
Konvertierungsskript f√ºr Ihr Modell zu finden.

- Wenn Sie ein Modell von TensorFlow nach PyTorch portieren, ist ein guter Ausgangspunkt das Konvertierungsskript von BERT [hier](https://github.com/huggingface/transformers/blob/7acfa95afb8194f8f9c1f4d2c6028224dbed35a2/src/transformers/models/bert/modeling_bert.py#L91)
- Wenn Sie ein Modell von PyTorch nach PyTorch portieren, ist ein guter Ausgangspunkt das Konvertierungsskript von BART [hier](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bart/convert_bart_original_pytorch_checkpoint_to_pytorch.py)

Im Folgenden werden wir kurz erkl√§ren, wie PyTorch-Modelle Ebenengewichte speichern und Ebenennamen definieren. In PyTorch wird der
Name einer Ebene durch den Namen des Klassenattributs definiert, das Sie der Ebene geben. Lassen Sie uns ein Dummy-Modell in
PyTorch, das wir `SimpleModel` nennen, wie folgt:

```python
from torch import nn


class SimpleModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.dense = nn.Linear(10, 10)
        self.intermediate = nn.Linear(10, 10)
        self.layer_norm = nn.LayerNorm(10)
```

Jetzt k√∂nnen wir eine Instanz dieser Modelldefinition erstellen, die alle Gewichte ausf√ºllt: `dense`, `intermediate`,
`layer_norm` mit zuf√§lligen Gewichten. Wir k√∂nnen das Modell ausdrucken, um seine Architektur zu sehen

```python
model = SimpleModel()

print(model)
```

Dies gibt folgendes aus:

```
SimpleModel(
  (dense): Linear(in_features=10, out_features=10, bias=True)
  (intermediate): Linear(in_features=10, out_features=10, bias=True)
  (layer_norm): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
)
```

Wir k√∂nnen sehen, dass die Ebenennamen durch den Namen des Klassenattributs in PyTorch definiert sind. Sie k√∂nnen die Gewichtswerte
Werte einer bestimmten Ebene anzeigen lassen:

```python
print(model.dense.weight.data)
```

um zu sehen, dass die Gewichte zuf√§llig initialisiert wurden

```
tensor([[-0.0818,  0.2207, -0.0749, -0.0030,  0.0045, -0.1569, -0.1598,  0.0212,
         -0.2077,  0.2157],
        [ 0.1044,  0.0201,  0.0990,  0.2482,  0.3116,  0.2509,  0.2866, -0.2190,
          0.2166, -0.0212],
        [-0.2000,  0.1107, -0.1999, -0.3119,  0.1559,  0.0993,  0.1776, -0.1950,
         -0.1023, -0.0447],
        [-0.0888, -0.1092,  0.2281,  0.0336,  0.1817, -0.0115,  0.2096,  0.1415,
         -0.1876, -0.2467],
        [ 0.2208, -0.2352, -0.1426, -0.2636, -0.2889, -0.2061, -0.2849, -0.0465,
          0.2577,  0.0402],
        [ 0.1502,  0.2465,  0.2566,  0.0693,  0.2352, -0.0530,  0.1859, -0.0604,
          0.2132,  0.1680],
        [ 0.1733, -0.2407, -0.1721,  0.1484,  0.0358, -0.0633, -0.0721, -0.0090,
          0.2707, -0.2509],
        [-0.1173,  0.1561,  0.2945,  0.0595, -0.1996,  0.2988, -0.0802,  0.0407,
          0.1829, -0.1568],
        [-0.1164, -0.2228, -0.0403,  0.0428,  0.1339,  0.0047,  0.1967,  0.2923,
          0.0333, -0.0536],
        [-0.1492, -0.1616,  0.1057,  0.1950, -0.2807, -0.2710, -0.1586,  0.0739,
          0.2220,  0.2358]]).
```

Im Konvertierungsskript sollten Sie diese zuf√§llig initialisierten Gewichte mit den genauen Gewichten der
entsprechenden Ebene im Kontrollpunkt. *Z.B.*

```python
# retrieve matching layer weights, e.g. by
# recursive algorithm
layer_name = "dense"
pretrained_weight = array_of_dense_layer

model_pointer = getattr(model, "dense")

model_pointer.weight.data = torch.from_numpy(pretrained_weight)
```

Dabei m√ºssen Sie sicherstellen, dass jedes zuf√§llig initialisierte Gewicht Ihres PyTorch-Modells und sein entsprechendes
Checkpoint-Gewicht in **Form und Name** genau √ºbereinstimmen. Zu diesem Zweck ist es **notwendig**, assert
Anweisungen f√ºr die Form hinzuzuf√ºgen und die Namen der Checkpoint-Gewichte auszugeben. Sie sollten z.B. Anweisungen hinzuf√ºgen wie:

```python
assert (
    model_pointer.weight.shape == pretrained_weight.shape
), f"Pointer shape of random weight {model_pointer.shape} and array shape of checkpoint weight {pretrained_weight.shape} mismatched"
```

Au√üerdem sollten Sie die Namen der beiden Gewichte ausdrucken, um sicherzustellen, dass sie √ºbereinstimmen, *z.B.*.

```python
logger.info(f"Initialize PyTorch weight {layer_name} from {pretrained_weight.name}")
```

Wenn entweder die Form oder der Name nicht √ºbereinstimmt, haben Sie wahrscheinlich das falsche Kontrollpunktgewicht einer zuf√§llig
Ebene der ü§ó Transformers-Implementierung zugewiesen.

Eine falsche Form ist h√∂chstwahrscheinlich auf eine falsche Einstellung der Konfigurationsparameter in `BrandNewBertConfig()` zur√ºckzuf√ºhren, die
nicht genau mit denen √ºbereinstimmen, die f√ºr den zu konvertierenden Pr√ºfpunkt verwendet wurden. Es k√∂nnte aber auch sein, dass
die PyTorch-Implementierung eines Layers erfordert, dass das Gewicht vorher transponiert wird.

Schlie√ülich sollten Sie auch √ºberpr√ºfen, ob **alle** erforderlichen Gewichte initialisiert sind und alle Checkpoint-Gewichte ausgeben, die
die nicht zur Initialisierung verwendet wurden, um sicherzustellen, dass das Modell korrekt konvertiert wurde. Es ist v√∂llig normal, dass die
Konvertierungsversuche entweder mit einer falschen Shape-Anweisung oder einer falschen Namenszuweisung fehlschlagen. Das liegt h√∂chstwahrscheinlich daran, dass entweder
Sie haben falsche Parameter in `BrandNewBertConfig()` verwendet, haben eine falsche Architektur in der ü§ó Transformers
Implementierung, Sie haben einen Fehler in den `init()` Funktionen einer der Komponenten der ü§ó Transformers
Implementierung oder Sie m√ºssen eine der Kontrollpunktgewichte transponieren.

Dieser Schritt sollte mit dem vorherigen Schritt wiederholt werden, bis alle Gewichte des Kontrollpunkts korrekt in das
Transformers-Modell geladen sind. Nachdem Sie den Pr√ºfpunkt korrekt in die ü§ó Transformers-Implementierung geladen haben, k√∂nnen Sie das Modell
das Modell unter einem Ordner Ihrer Wahl `/path/to/converted/checkpoint/folder` speichern, der dann sowohl ein
Datei `pytorch_model.bin` und eine Datei `config.json` enthalten sollte:

```python
model.save_pretrained("/path/to/converted/checkpoint/folder")
```

**7. Implementieren Sie den Vorw√§rtspass**

Nachdem es Ihnen gelungen ist, die trainierten Gewichte korrekt in die ü§ó Transformers-Implementierung zu laden, sollten Sie nun daf√ºr sorgen
sicherstellen, dass der Forward Pass korrekt implementiert ist. In [Machen Sie sich mit dem urspr√ºnglichen Repository vertraut](#3-4-f√ºhren-sie-einen-pre-training-checkpoint-mit-dem-original-repository-durch) haben Sie bereits ein Skript erstellt, das einen Forward Pass
Durchlauf des Modells unter Verwendung des Original-Repositorys durchf√ºhrt. Jetzt sollten Sie ein analoges Skript schreiben, das die ü§ó Transformers
Implementierung anstelle der Originalimplementierung verwenden. Es sollte wie folgt aussehen:

```python
model = BrandNewBertModel.from_pretrained("/path/to/converted/checkpoint/folder")
input_ids = [0, 4, 4, 3, 2, 4, 1, 7, 19]
output = model(input_ids).last_hidden_states
```

Es ist sehr wahrscheinlich, dass die ü§ó Transformers-Implementierung und die urspr√ºngliche Modell-Implementierung nicht genau die gleiche Ausgabe liefern.
beim ersten Mal nicht die gleiche Ausgabe liefern oder dass der Vorw√§rtsdurchlauf einen Fehler ausl√∂st. Seien Sie nicht entt√§uscht - das ist zu erwarten! Erstens,
sollten Sie sicherstellen, dass der Vorw√§rtsdurchlauf keine Fehler ausl√∂st. Es passiert oft, dass die falschen Dimensionen verwendet werden
verwendet werden, was zu einem *Dimensionality mismatch* Fehler f√ºhrt oder dass der falsche Datentyp verwendet wird, *z.B.* `torch.long`
anstelle von `torch.float32`. Z√∂gern Sie nicht, das Hugging Face Team um Hilfe zu bitten, wenn Sie bestimmte Fehler nicht l√∂sen k√∂nnen.
bestimmte Fehler nicht l√∂sen k√∂nnen.

Um sicherzustellen, dass die Implementierung von ü§ó Transformers korrekt funktioniert, m√ºssen Sie sicherstellen, dass die Ausgaben
einer Genauigkeit von `1e-3` entsprechen. Zun√§chst sollten Sie sicherstellen, dass die Ausgabeformen identisch sind, *d.h.*.
Die Ausgabeform *outputs.shape* sollte f√ºr das Skript der ü§ó Transformers-Implementierung und die urspr√ºngliche
Implementierung ergeben. Als n√§chstes sollten Sie sicherstellen, dass auch die Ausgabewerte identisch sind. Dies ist einer der schwierigsten
Teile des Hinzuf√ºgens eines neuen Modells. H√§ufige Fehler, warum die Ausgaben nicht identisch sind, sind:

- Einige Ebenen wurden nicht hinzugef√ºgt, *d.h.* eine *Aktivierungsebene* wurde nicht hinzugef√ºgt, oder die Restverbindung wurde vergessen
- Die Worteinbettungsmatrix wurde nicht gebunden
- Es werden die falschen Positionseinbettungen verwendet, da die urspr√ºngliche Implementierung einen Offset verwendet
- Dropout wird w√§hrend des Vorw√§rtsdurchlaufs angewendet. Um dies zu beheben, stellen Sie sicher, dass *model.training auf False* steht und dass keine Dropout
  Schicht w√§hrend des Vorw√§rtsdurchlaufs f√§lschlicherweise aktiviert wird, *d.h.* √ºbergeben Sie *self.training* an [PyTorch's functional dropout](https://pytorch.org/docs/stable/nn.functional.html?highlight=dropout#torch.nn.functional.dropout)

Der beste Weg, das Problem zu beheben, besteht normalerweise darin, sich den Vorw√§rtsdurchlauf der urspr√ºnglichen Implementierung und die ü§ó
Transformers-Implementierung nebeneinander zu sehen und zu pr√ºfen, ob es Unterschiede gibt. Idealerweise sollten Sie die
Zwischenergebnisse beider Implementierungen des Vorw√§rtsdurchlaufs debuggen/ausdrucken, um die genaue Position im Netzwerk zu finden, an der die ü§ó
Transformers-Implementierung eine andere Ausgabe zeigt als die urspr√ºngliche Implementierung. Stellen Sie zun√§chst sicher, dass die
hartcodierten `input_ids` in beiden Skripten identisch sind. √úberpr√ºfen Sie dann, ob die Ausgaben der ersten Transformation von
der `input_ids` (normalerweise die Worteinbettungen) identisch sind. Und dann arbeiten Sie sich bis zur allerletzten Schicht des
Netzwerks. Irgendwann werden Sie einen Unterschied zwischen den beiden Implementierungen feststellen, der Sie auf den Fehler
in der Implementierung von ü§ó Transformers hinweist. Unserer Erfahrung nach ist ein einfacher und effizienter Weg, viele Druckanweisungen hinzuzuf√ºgen
sowohl in der Original-Implementierung als auch in der ü§ó Transformers-Implementierung an den gleichen Stellen im Netzwerk
hinzuzuf√ºgen und nacheinander Druckanweisungen zu entfernen, die dieselben Werte f√ºr Zwischenpr√§sentationen anzeigen.

Wenn Sie sicher sind, dass beide Implementierungen die gleiche Ausgabe liefern, √ºberpr√ºfen Sie die Ausgaben mit
`torch.allclose(original_output, output, atol=1e-3)` √ºberpr√ºfen, haben Sie den schwierigsten Teil hinter sich! Herzlichen Gl√ºckwunsch - die
Arbeit, die noch zu erledigen ist, sollte ein Kinderspiel sein üòä.

**8. Hinzuf√ºgen aller notwendigen Modelltests**

An diesem Punkt haben Sie erfolgreich ein neues Modell hinzugef√ºgt. Es ist jedoch sehr gut m√∂glich, dass das Modell noch nicht
noch nicht vollst√§ndig mit dem erforderlichen Design √ºbereinstimmt. Um sicherzustellen, dass die Implementierung vollst√§ndig kompatibel mit ü§ó Transformers ist, sollten alle
gemeinsamen Tests bestehen. Der Cookiecutter sollte automatisch eine Testdatei f√ºr Ihr Modell hinzugef√ºgt haben, wahrscheinlich unter
demselben `tests/models/brand_new_bert/test_modeling_brand_new_bert.py`. F√ºhren Sie diese Testdatei aus, um zu √ºberpr√ºfen, ob alle g√§ngigen
Tests bestehen:

```bash
pytest tests/models/brand_new_bert/test_modeling_brand_new_bert.py
```

Nachdem Sie alle allgemeinen Tests festgelegt haben, m√ºssen Sie nun sicherstellen, dass all die sch√∂ne Arbeit, die Sie geleistet haben, gut getestet ist, damit

- a) die Community Ihre Arbeit leicht nachvollziehen kann, indem sie sich spezifische Tests von *brand_new_bert* ansieht
- b) zuk√ºnftige √Ñnderungen an Ihrem Modell keine wichtigen Funktionen des Modells zerst√∂ren.

Als erstes sollten Sie Integrationstests hinzuf√ºgen. Diese Integrationstests tun im Wesentlichen dasselbe wie die Debugging-Skripte
die Sie zuvor zur Implementierung des Modells in ü§ó Transformers verwendet haben. Eine Vorlage f√ºr diese Modelltests wurde bereits von dem
Cookiecutter hinzugef√ºgt, die `BrandNewBertModelIntegrationTests` hei√üt und nur noch von Ihnen ausgef√ºllt werden muss. Um sicherzustellen, dass diese
Tests erfolgreich sind, f√ºhren Sie

```bash
RUN_SLOW=1 pytest -sv tests/models/brand_new_bert/test_modeling_brand_new_bert.py::BrandNewBertModelIntegrationTests
```

<Tip>

Falls Sie Windows verwenden, sollten Sie `RUN_SLOW=1` durch `SET RUN_SLOW=1` ersetzen.

</Tip>

Zweitens sollten alle Funktionen, die speziell f√ºr *brand_new_bert* sind, zus√§tzlich in einem separaten Test getestet werden unter
`BrandNewBertModelTester`/`BrandNewBertModelTest`. Dieser Teil wird oft vergessen, ist aber in zweierlei Hinsicht √§u√üerst n√ºtzlich
Weise:

- Er hilft dabei, das Wissen, das Sie w√§hrend der Modellerweiterung erworben haben, an die Community weiterzugeben, indem er zeigt, wie die
  speziellen Funktionen von *brand_new_bert* funktionieren sollten.
- K√ºnftige Mitwirkende k√∂nnen √Ñnderungen am Modell schnell testen, indem sie diese speziellen Tests ausf√ºhren.


**9. Implementieren Sie den Tokenizer**

Als n√§chstes sollten wir den Tokenizer von *brand_new_bert* hinzuf√ºgen. Normalerweise ist der Tokenizer √§quivalent oder sehr √§hnlich zu einem
bereits vorhandenen Tokenizer von ü§ó Transformers.

Es ist sehr wichtig, die urspr√ºngliche Tokenizer-Datei zu finden/extrahieren und es zu schaffen, diese Datei in die ü§ó
Transformers Implementierung des Tokenizers zu laden.

Um sicherzustellen, dass der Tokenizer korrekt funktioniert, empfiehlt es sich, zun√§chst ein Skript im urspr√ºnglichen Repository zu erstellen
zu erstellen, das eine Zeichenkette eingibt und die `input_ids` zur√ºckgibt. Es k√∂nnte etwa so aussehen (in Pseudocode):

```python
input_str = "This is a long example input string containing special characters .$?-, numbers 2872 234 12 and words."
model = BrandNewBertModel.load_pretrained_checkpoint("/path/to/checkpoint/")
input_ids = model.tokenize(input_str)
```

M√∂glicherweise m√ºssen Sie noch einmal einen Blick in das urspr√ºngliche Repository werfen, um die richtige Tokenizer-Funktion zu finden, oder Sie m√ºssen
Sie m√ºssen vielleicht sogar √Ñnderungen an Ihrem Klon des Original-Repositorys vornehmen, um nur die `input_ids` auszugeben. Nach dem Schreiben
ein funktionierendes Tokenisierungsskript geschrieben, das das urspr√ºngliche Repository verwendet, sollten Sie ein analoges Skript f√ºr ü§ó Transformers
erstellt werden. Es sollte √§hnlich wie dieses aussehen:

```python
from transformers import BrandNewBertTokenizer

input_str = "This is a long example input string containing special characters .$?-, numbers 2872 234 12 and words."

tokenizer = BrandNewBertTokenizer.from_pretrained("/path/to/tokenizer/folder/")

input_ids = tokenizer(input_str).input_ids
```

Wenn beide `input_ids` die gleichen Werte ergeben, sollte als letzter Schritt auch eine Tokenizer-Testdatei hinzugef√ºgt werden.

Analog zu den Modellierungstestdateien von *brand_new_bert* sollten auch die Tokenisierungs-Testdateien von *brand_new_bert*
eine Reihe von fest kodierten Integrationstests enthalten.

**10. F√ºhren Sie End-to-End-Integrationstests aus**

Nachdem Sie den Tokenizer hinzugef√ºgt haben, sollten Sie auch ein paar End-to-End-Integrationstests, die sowohl das Modell als auch den
Tokenizer zu `tests/models/brand_new_bert/test_modeling_brand_new_bert.py` in ü§ó Transformers.
Ein solcher Test sollte bei einem aussagekr√§ftigen
Text-zu-Text-Beispiel zeigen, dass die Implementierung von ü§ó Transformers wie erwartet funktioniert. Ein aussagekr√§ftiges Text-zu-Text-Beispiel kann
z.B. *ein Quell-zu-Ziel-√úbersetzungspaar, ein Artikel-zu-Zusammenfassung-Paar, ein Frage-zu-Antwort-Paar, usw... Wenn keiner der
der portierten Pr√ºfpunkte in einer nachgelagerten Aufgabe feinabgestimmt wurde, gen√ºgt es, sich einfach auf die Modelltests zu verlassen. In einem
letzten Schritt, um sicherzustellen, dass das Modell voll funktionsf√§hig ist, sollten Sie alle Tests auch auf der GPU durchf√ºhren. Es kann
Es kann vorkommen, dass Sie vergessen haben, einige `.to(self.device)` Anweisungen zu internen Tensoren des Modells hinzuzuf√ºgen, was in einem solchen
Test zu einem Fehler f√ºhren w√ºrde. Falls Sie keinen Zugang zu einem Grafikprozessor haben, kann das Hugging Face Team diese Tests f√ºr Sie durchf√ºhren.
Tests f√ºr Sie √ºbernehmen.

**11. Docstring hinzuf√ºgen**

Nun sind alle notwendigen Funktionen f√ºr *brand_new_bert* hinzugef√ºgt - Sie sind fast fertig! Das Einzige, was Sie noch hinzuf√ºgen m√ºssen, ist
ein sch√∂ner Docstring und eine Doku-Seite. Der Cookiecutter sollte eine Vorlagendatei namens
`docs/source/model_doc/brand_new_bert.md` hinzugef√ºgt haben, die Sie ausf√ºllen sollten. Die Benutzer Ihres Modells werden in der Regel zuerst einen Blick auf
diese Seite ansehen, bevor sie Ihr Modell verwenden. Daher muss die Dokumentation verst√§ndlich und pr√§gnant sein. Es ist sehr n√ºtzlich f√ºr
die Gemeinschaft, einige *Tipps* hinzuzuf√ºgen, um zu zeigen, wie das Modell verwendet werden sollte. Z√∂gern Sie nicht, das Hugging Face-Team anzupingen
bez√ºglich der Docstrings.

Stellen Sie als n√§chstes sicher, dass der zu `src/transformers/models/brand_new_bert/modeling_brand_new_bert.py` hinzugef√ºgte docstring
korrekt ist und alle erforderlichen Eingaben und Ausgaben enth√§lt. Wir haben eine ausf√ºhrliche Anleitung zum Schreiben von Dokumentationen und unserem Docstring-Format [hier](writing-documentation). Es ist immer gut, sich daran zu erinnern, dass die Dokumentation
mindestens so sorgf√§ltig behandelt werden sollte wie der Code in ü§ó Transformers, denn die Dokumentation ist in der Regel der erste Kontaktpunkt der
Ber√ºhrungspunkt der Community mit dem Modell ist.

**Code refactor**

Gro√üartig, jetzt haben Sie den gesamten erforderlichen Code f√ºr *brand_new_bert* hinzugef√ºgt. An diesem Punkt sollten Sie einige m√∂gliche
falschen Codestil korrigieren, indem Sie ausf√ºhren:

```bash
make style
```

und √ºberpr√ºfen Sie, ob Ihr Kodierungsstil die Qualit√§tspr√ºfung besteht:

```bash
make quality
```

Es gibt noch ein paar andere sehr strenge Designtests in ü§ó Transformers, die m√∂glicherweise noch fehlschlagen, was sich in den
den Tests Ihres Pull Requests. Dies liegt oft an fehlenden Informationen im Docstring oder an einer falschen
Benennung. Das Hugging Face Team wird Ihnen sicherlich helfen, wenn Sie hier nicht weiterkommen.

Und schlie√ülich ist es immer eine gute Idee, den eigenen Code zu refaktorisieren, nachdem man sichergestellt hat, dass er korrekt funktioniert. Wenn alle
Tests bestanden haben, ist es nun an der Zeit, den hinzugef√ºgten Code noch einmal durchzugehen und einige √úberarbeitungen vorzunehmen.

Sie haben nun den Codierungsteil abgeschlossen, herzlichen Gl√ºckwunsch! üéâ Sie sind gro√üartig! üòé

**12. Laden Sie die Modelle in den Model Hub hoch**

In diesem letzten Teil sollten Sie alle Checkpoints konvertieren und in den Modell-Hub hochladen und eine Modellkarte f√ºr jeden
hochgeladenen Modell-Kontrollpunkt. Sie k√∂nnen sich mit den Hub-Funktionen vertraut machen, indem Sie unsere [Model sharing and uploading Page](model_sharing) lesen. Hier sollten Sie mit dem Hugging Face-Team zusammenarbeiten, um einen passenden Namen f√ºr jeden
Checkpoint festzulegen und die erforderlichen Zugriffsrechte zu erhalten, um das Modell unter der Organisation des Autors *brand_new_bert* hochladen zu k√∂nnen.
*brand_new_bert*. Die Methode `push_to_hub`, die in allen Modellen in `transformers` vorhanden ist, ist ein schneller und effizienter Weg, Ihren Checkpoint in den Hub zu pushen. Ein kleines Snippet ist unten eingef√ºgt:

```python
brand_new_bert.push_to_hub("brand_new_bert")
# Uncomment the following line to push to an organization.
# brand_new_bert.push_to_hub("<organization>/brand_new_bert")
```

Es lohnt sich, etwas Zeit darauf zu verwenden, f√ºr jeden Kontrollpunkt passende Musterkarten zu erstellen. Die Modellkarten sollten die
spezifischen Merkmale dieses bestimmten Pr√ºfpunkts hervorheben, * z.B.* auf welchem Datensatz wurde der Pr√ºfpunkt
vortrainiert/abgestimmt? F√ºr welche nachgelagerte Aufgabe sollte das Modell verwendet werden? Und f√ºgen Sie auch etwas Code bei, wie Sie
wie das Modell korrekt verwendet wird.

**13. (Optional) Notizbuch hinzuf√ºgen**

Es ist sehr hilfreich, ein Notizbuch hinzuzuf√ºgen, in dem im Detail gezeigt wird, wie *brand_new_bert* f√ºr Schlussfolgerungen verwendet werden kann und/oder
bei einer nachgelagerten Aufgabe feinabgestimmt wird. Dies ist nicht zwingend erforderlich, um Ihren PR zusammenzuf√ºhren, aber sehr n√ºtzlich f√ºr die Gemeinschaft.

**14. Reichen Sie Ihren fertigen PR ein**

Sie sind jetzt mit der Programmierung fertig und k√∂nnen zum letzten Schritt √ºbergehen, n√§mlich der Zusammenf√ºhrung Ihres PR mit main. Normalerweise hat das
Hugging Face Team Ihnen an diesem Punkt bereits geholfen haben, aber es lohnt sich, sich etwas Zeit zu nehmen, um Ihrem fertigen
PR eine sch√∂ne Beschreibung zu geben und eventuell Kommentare zu Ihrem Code hinzuzuf√ºgen, wenn Sie Ihren Gutachter auf bestimmte Designentscheidungen hinweisen wollen.
Gutachter hinweisen wollen.

### Teilen Sie Ihre Arbeit!!

Jetzt ist es an der Zeit, von der Community Anerkennung f√ºr Ihre Arbeit zu bekommen! Die Fertigstellung einer Modellerg√§nzung ist ein wichtiger
Beitrag zu Transformers und der gesamten NLP-Gemeinschaft. Ihr Code und die portierten vortrainierten Modelle werden sicherlich
von Hunderten und vielleicht sogar Tausenden von Entwicklern und Forschern genutzt werden. Sie sollten stolz auf Ihre Arbeit sein und Ihre
Ihre Leistung mit der Gemeinschaft teilen.

**Sie haben ein weiteres Modell erstellt, das f√ºr jeden in der Community super einfach zug√§nglich ist! ü§Ø**
