<!--Copyright 2023 The HuggingFace Team. All rights reserved.
Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.
-->

# Adapter mit ü§ó PEFT laden

[[open-in-colab]]

Die [Parameter-Efficient Fine Tuning (PEFT)](https://huggingface.co/blog/peft) Methoden frieren die vorab trainierten Modellparameter w√§hrend der Feinabstimmung ein und f√ºgen eine kleine Anzahl trainierbarer Parameter (die Adapter) hinzu. Die Adapter werden trainiert, um aufgabenspezifische Informationen zu lernen. Es hat sich gezeigt, dass dieser Ansatz sehr speichereffizient ist und weniger Rechenleistung beansprucht, w√§hrend die Ergebnisse mit denen eines vollst√§ndig feinabgestimmten Modells vergleichbar sind. 

Adapter, die mit PEFT trainiert wurden, sind in der Regel um eine Gr√∂√üenordnung kleiner als das vollst√§ndige Modell, so dass sie bequem gemeinsam genutzt, gespeichert und geladen werden k√∂nnen.

<div class="flex flex-col justify-center">
  <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/PEFT-hub-screenshot.png"/>
  <figcaption class="text-center">Die Adaptergewichte f√ºr ein OPTForCausalLM-Modell, die auf dem Hub gespeichert sind, sind nur ~6MB gro√ü, verglichen mit der vollen Gr√∂√üe der Modellgewichte, die ~700MB betragen k√∂nnen.</figcaption>
</div>

Wenn Sie mehr √ºber die ü§ó PEFT-Bibliothek erfahren m√∂chten, sehen Sie sich die [Dokumentation](https://huggingface.co/docs/peft/index) an.

## Setup

Starten Sie mit der Installation von ü§ó PEFT:

```bash
pip install peft
```

Wenn Sie die brandneuen Funktionen ausprobieren m√∂chten, sollten Sie die Bibliothek aus dem Quellcode installieren:

```bash
pip install git+https://github.com/huggingface/peft.git
```

## Unterst√ºtzte PEFT-Modelle

Transformers unterst√ºtzt nativ einige PEFT-Methoden, d.h. Sie k√∂nnen lokal oder auf dem Hub gespeicherte Adaptergewichte laden und sie mit wenigen Zeilen Code einfach ausf√ºhren oder trainieren. Die folgenden Methoden werden unterst√ºtzt:

- [Low Rank Adapters](https://huggingface.co/docs/peft/conceptual_guides/lora)
- [IA3](https://huggingface.co/docs/peft/conceptual_guides/ia3)
- [AdaLoRA](https://huggingface.co/papers/2303.10512)

Wenn Sie andere PEFT-Methoden, wie z.B. Prompt Learning oder Prompt Tuning, verwenden m√∂chten, oder √ºber die ü§ó PEFT-Bibliothek im Allgemeinen, lesen Sie bitte die [Dokumentation](https://huggingface.co/docs/peft/index).


## Laden Sie einen PEFT-Adapter

Um ein PEFT-Adaptermodell von ü§ó Transformers zu laden und zu verwenden, stellen Sie sicher, dass das Hub-Repository oder das lokale Verzeichnis eine `adapter_config.json`-Datei und die Adaptergewichte enth√§lt, wie im obigen Beispielbild gezeigt. Dann k√∂nnen Sie das PEFT-Adaptermodell mit der Klasse `AutoModelFor` laden. Um zum Beispiel ein PEFT-Adaptermodell f√ºr die kausale Sprachmodellierung zu laden:

1. Geben Sie die PEFT-Modell-ID an.
2. √ºbergeben Sie es an die Klasse [`AutoModelForCausalLM`].

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

peft_model_id = "ybelkada/opt-350m-lora"
model = AutoModelForCausalLM.from_pretrained(peft_model_id)
```

<Tip>

Sie k√∂nnen einen PEFT-Adapter entweder mit einer `AutoModelFor`-Klasse oder der Basismodellklasse wie `OPTForCausalLM` oder `LlamaForCausalLM` laden.

</Tip>

Sie k√∂nnen einen PEFT-Adapter auch laden, indem Sie die Methode `load_adapter` aufrufen:

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "facebook/opt-350m"
peft_model_id = "ybelkada/opt-350m-lora"

model = AutoModelForCausalLM.from_pretrained(model_id)
model.load_adapter(peft_model_id)
```

## Laden in 8bit oder 4bit

Die `bitsandbytes`-Integration unterst√ºtzt Datentypen mit 8bit und 4bit Genauigkeit, was f√ºr das Laden gro√üer Modelle n√ºtzlich ist, weil es Speicher spart (lesen Sie den `bitsandbytes`-Integrations [guide](./quantization#bitsandbytes-integration), um mehr zu erfahren). F√ºgen Sie die Parameter `load_in_8bit` oder `load_in_4bit` zu [`~PreTrainedModel.from_pretrained`] hinzu und setzen Sie `device_map="auto"`, um das Modell effektiv auf Ihre Hardware zu verteilen:

```py
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

peft_model_id = "ybelkada/opt-350m-lora"
model = AutoModelForCausalLM.from_pretrained(peft_model_id, quantization_config=BitsAndBytesConfig(load_in_8bit=True))
```

## Einen neuen Adapter hinzuf√ºgen

Sie k√∂nnen [`~peft.PeftModel.add_adapter`] verwenden, um einen neuen Adapter zu einem Modell mit einem bestehenden Adapter hinzuzuf√ºgen, solange der neue Adapter vom gleichen Typ ist wie der aktuelle Adapter. Wenn Sie zum Beispiel einen bestehenden LoRA-Adapter an ein Modell angeh√§ngt haben:

```py
from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
from peft import PeftConfig

model_id = "facebook/opt-350m"
model = AutoModelForCausalLM.from_pretrained(model_id)

lora_config = LoraConfig(
    target_modules=["q_proj", "k_proj"],
    init_lora_weights=False
)

model.add_adapter(lora_config, adapter_name="adapter_1")
```

Um einen neuen Adapter hinzuzuf√ºgen:

```py
# attach new adapter with same config
model.add_adapter(lora_config, adapter_name="adapter_2")
```

Jetzt k√∂nnen Sie mit [`~peft.PeftModel.set_adapter`] festlegen, welcher Adapter verwendet werden soll:

```py
# use adapter_1
model.set_adapter("adapter_1")
output = model.generate(**inputs)
print(tokenizer.decode(output_disabled[0], skip_special_tokens=True))

# use adapter_2
model.set_adapter("adapter_2")
output_enabled = model.generate(**inputs)
print(tokenizer.decode(output_enabled[0], skip_special_tokens=True))
```

## Aktivieren und Deaktivieren von Adaptern

Sobald Sie einen Adapter zu einem Modell hinzugef√ºgt haben, k√∂nnen Sie das Adaptermodul aktivieren oder deaktivieren. So aktivieren Sie das Adaptermodul:

```py
from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
from peft import PeftConfig

model_id = "facebook/opt-350m"
adapter_model_id = "ybelkada/opt-350m-lora"
tokenizer = AutoTokenizer.from_pretrained(model_id)
text = "Hello"
inputs = tokenizer(text, return_tensors="pt")

model = AutoModelForCausalLM.from_pretrained(model_id)
peft_config = PeftConfig.from_pretrained(adapter_model_id)

# to initiate with random weights
peft_config.init_lora_weights = False

model.add_adapter(peft_config)
model.enable_adapters()
output = model.generate(**inputs)
```

So deaktivieren Sie das Adaptermodul:

```py
model.disable_adapters()
output = model.generate(**inputs)
```

## PEFT-Adapter trainieren

PEFT-Adapter werden von der Klasse [`Trainer`] unterst√ºtzt, so dass Sie einen Adapter f√ºr Ihren speziellen Anwendungsfall trainieren k√∂nnen. Dazu m√ºssen Sie nur ein paar weitere Codezeilen hinzuf√ºgen. Zum Beispiel, um einen LoRA-Adapter zu trainieren:

<Tip>

Wenn Sie mit der Feinabstimmung eines Modells mit [`Trainer`] noch nicht vertraut sind, werfen Sie einen Blick auf das Tutorial [Feinabstimmung eines vortrainierten Modells](Training).

</Tip>

1. Definieren Sie Ihre Adapterkonfiguration mit dem Aufgabentyp und den Hyperparametern (siehe [`~peft.LoraConfig`] f√ºr weitere Details dar√ºber, was die Hyperparameter tun).

```py
from peft import LoraConfig

peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="CAUSAL_LM",
)
```

2. F√ºgen Sie dem Modell einen Adapter hinzu.

```py
model.add_adapter(peft_config)
```

3. Jetzt k√∂nnen Sie das Modell an [`Trainer`] √ºbergeben!

```py
trainer = Trainer(model=model, ...)
trainer.train()
```

So speichern Sie Ihren trainierten Adapter und laden ihn wieder:

```py
model.save_pretrained(save_dir)
model = AutoModelForCausalLM.from_pretrained(save_dir)
```

<!--
TODO: (@younesbelkada @stevhliu)
-   Link to PEFT docs for further details
-   Trainer  
-   8-bit / 4-bit examples ?
-->
