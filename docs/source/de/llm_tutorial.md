<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->


# Generation with LLMs

[[open-in-colab]]

LLMs (Large Language Models) sind die Schl√ºsselkomponente bei der Texterstellung. Kurz gesagt, bestehen sie aus gro√üen, vortrainierten Transformationsmodellen, die darauf trainiert sind, das n√§chste Wort (oder genauer gesagt Token) aus einem Eingabetext vorherzusagen. Da sie jeweils ein Token vorhersagen, m√ºssen Sie etwas Aufw√§ndigeres tun, um neue S√§tze zu generieren, als nur das Modell aufzurufen - Sie m√ºssen eine autoregressive Generierung durchf√ºhren.

Die autoregressive Generierung ist ein Verfahren zur Inferenzzeit, bei dem ein Modell mit seinen eigenen generierten Ausgaben iterativ aufgerufen wird, wenn einige anf√§ngliche Eingaben vorliegen. In ü§ó Transformers wird dies von der Methode [`~generation.GenerationMixin.generate`] √ºbernommen, die allen Modellen mit generativen F√§higkeiten zur Verf√ºgung steht.

Dieses Tutorial zeigt Ihnen, wie Sie:

* Text mit einem LLM generieren
* Vermeiden Sie h√§ufige Fallstricke
* N√§chste Schritte, damit Sie das Beste aus Ihrem LLM herausholen k√∂nnen

Bevor Sie beginnen, stellen Sie sicher, dass Sie alle erforderlichen Bibliotheken installiert haben:

```bash
pip install transformers bitsandbytes>=0.39.0 -q
```


## Text generieren

Ein Sprachmodell, das f√ºr [causal language modeling](tasks/language_modeling) trainiert wurde, nimmt eine Folge von Text-Token als Eingabe und gibt die Wahrscheinlichkeitsverteilung f√ºr das n√§chste Token zur√ºck.

<!-- [GIF 1 -- FWD PASS] -->
<figure class="image table text-center m-0 w-full">
    <video
        style="max-width: 90%; margin: auto;"
        autoplay loop muted playsinline
        src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_1_1080p.mov"
    ></video>
    <figcaption>"Forward pass of an LLM"</figcaption>
</figure>

Ein wichtiger Aspekt der autoregressiven Generierung mit LLMs ist die Auswahl des n√§chsten Tokens aus dieser Wahrscheinlichkeitsverteilung. In diesem Schritt ist alles m√∂glich, solange Sie am Ende ein Token f√ºr die n√§chste Iteration haben. Das hei√üt, es kann so einfach sein wie die Auswahl des wahrscheinlichsten Tokens aus der Wahrscheinlichkeitsverteilung oder so komplex wie die Anwendung von einem Dutzend Transformationen vor der Stichprobenziehung aus der resultierenden Verteilung.

<!-- [GIF 2 -- TEXT GENERATION] -->
<figure class="image table text-center m-0 w-full">
    <video
        style="max-width: 90%; margin: auto;"
        autoplay loop muted playsinline
        src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_2_1080p.mov"
    ></video>
    <figcaption>"Die autoregressive Generierung w√§hlt iterativ das n√§chste Token aus einer Wahrscheinlichkeitsverteilung aus, um Text zu erzeugen"</figcaption>
</figure>

Der oben dargestellte Prozess wird iterativ wiederholt, bis eine bestimmte Abbruchbedingung erreicht ist. Im Idealfall wird die Abbruchbedingung vom Modell vorgegeben, das lernen sollte, wann es ein Ende-der-Sequenz-Token (EOS) ausgeben muss. Ist dies nicht der Fall, stoppt die Generierung, wenn eine vordefinierte Maximall√§nge erreicht ist.

Damit sich Ihr Modell so verh√§lt, wie Sie es f√ºr Ihre Aufgabe erwarten, m√ºssen Sie den Schritt der Token-Auswahl und die Abbruchbedingung richtig einstellen. Aus diesem Grund haben wir zu jedem Modell eine [`~generation.GenerationConfig`]-Datei, die eine gute generative Standardparametrisierung enth√§lt und zusammen mit Ihrem Modell geladen wird.

Lassen Sie uns √ºber Code sprechen!

<Tip>

Wenn Sie an der grundlegenden Verwendung von LLMs interessiert sind, ist unsere High-Level-Schnittstelle [`Pipeline`](pipeline_tutorial) ein guter Ausgangspunkt. LLMs erfordern jedoch oft fortgeschrittene Funktionen wie Quantisierung und Feinsteuerung des Token-Auswahlschritts, was am besten √ºber [`~generation.GenerationMixin.generate`] erfolgt. Die autoregressive Generierung mit LLMs ist ebenfalls ressourcenintensiv und sollte f√ºr einen angemessenen Durchsatz auf einer GPU ausgef√ºhrt werden.

</Tip>

<!-- TODO: update example to llama 2 (or a newer popular baseline) when it becomes ungated -->
Zun√§chst m√ºssen Sie das Modell laden.

```py
>>> from transformers import AutoModelForCausalLM

>>> model = AutoModelForCausalLM.from_pretrained(
...     "openlm-research/open_llama_7b", device_map="auto", load_in_4bit=True
... )
```

Sie werden zwei Flags in dem Aufruf `from_pretrained` bemerken:

 - `device_map` stellt sicher, dass das Modell auf Ihre GPU(s) √ºbertragen wird
 - `load_in_4bit` wendet [dynamische 4-Bit-Quantisierung](main_classes/quantization) an, um die Ressourcenanforderungen massiv zu reduzieren

Es gibt noch andere M√∂glichkeiten, ein Modell zu initialisieren, aber dies ist eine gute Grundlage, um mit einem LLM zu beginnen.

Als n√§chstes m√ºssen Sie Ihre Texteingabe mit einem [tokenizer](tokenizer_summary) vorverarbeiten.

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("openlm-research/open_llama_7b")
>>> model_inputs = tokenizer(["A list of colors: red, blue"], return_tensors="pt").to("cuda")
```

Die Variable `model_inputs` enth√§lt die tokenisierte Texteingabe sowie die Aufmerksamkeitsmaske. Obwohl [`~generation.GenerationMixin.generate`] sein Bestes tut, um die Aufmerksamkeitsmaske abzuleiten, wenn sie nicht √ºbergeben wird, empfehlen wir, sie f√ºr optimale Ergebnisse wann immer m√∂glich zu √ºbergeben.

Rufen Sie schlie√ülich die Methode [~generation.GenerationMixin.generate] auf, um die generierten Token zur√ºckzugeben, die vor dem Drucken in Text umgewandelt werden sollten.

```py
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'A list of colors: red, blue, green, yellow, black, white, and brown'
```

Und das war's! Mit ein paar Zeilen Code k√∂nnen Sie sich die Macht eines LLM zunutze machen.


## H√§ufige Fallstricke

Es gibt viele [Generierungsstrategien](generation_strategies), und manchmal sind die Standardwerte f√ºr Ihren Anwendungsfall vielleicht nicht geeignet. Wenn Ihre Ausgaben nicht mit dem √ºbereinstimmen, was Sie erwarten, haben wir eine Liste der h√§ufigsten Fallstricke erstellt und wie Sie diese vermeiden k√∂nnen.

```py
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("openlm-research/open_llama_7b")
>>> tokenizer.pad_token = tokenizer.eos_token  # Llama has no pad token by default
>>> model = AutoModelForCausalLM.from_pretrained(
...     "openlm-research/open_llama_7b", device_map="auto", load_in_4bit=True
... )
```

### Generierte Ausgabe ist zu kurz/lang

Wenn in der Datei [~generation.GenerationConfig`] nichts angegeben ist, gibt `generate` standardm√§√üig bis zu 20 Token zur√ºck. Wir empfehlen dringend, `max_new_tokens` in Ihrem `generate`-Aufruf manuell zu setzen, um die maximale Anzahl neuer Token zu kontrollieren, die zur√ºckgegeben werden k√∂nnen. Beachten Sie, dass LLMs (genauer gesagt, [decoder-only models](https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt)) auch die Eingabeaufforderung als Teil der Ausgabe zur√ºckgeben.


```py
>>> model_inputs = tokenizer(["A sequence of numbers: 1, 2"], return_tensors="pt").to("cuda")

>>> # By default, the output will contain up to 20 tokens
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'A sequence of numbers: 1, 2, 3, 4, 5'

>>> # Setting `max_new_tokens` allows you to control the maximum length
>>> generated_ids = model.generate(**model_inputs, max_new_tokens=50)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'A sequence of numbers: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,'
```

### Falscher Generierungsmodus

Standardm√§√üig und sofern nicht in der Datei [~generation.GenerationConfig`] angegeben, w√§hlt `generate` bei jeder Iteration das wahrscheinlichste Token aus (gierige Dekodierung). Je nach Aufgabe kann dies unerw√ºnscht sein; kreative Aufgaben wie Chatbots oder das Schreiben eines Aufsatzes profitieren vom Sampling. Andererseits profitieren Aufgaben, bei denen es auf die Eingabe ankommt, wie z.B. Audiotranskription oder √úbersetzung, von der gierigen Dekodierung. Aktivieren Sie das Sampling mit `do_sample=True`. Mehr zu diesem Thema erfahren Sie in diesem [Blogbeitrag] (https://huggingface.co/blog/how-to-generate).

```py
>>> # Set seed or reproducibility -- you don't need this unless you want full reproducibility
>>> from transformers import set_seed
>>> set_seed(0)

>>> model_inputs = tokenizer(["I am a cat."], return_tensors="pt").to("cuda")

>>> # LLM + greedy decoding = repetitive, boring output
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'I am a cat. I am a cat. I am a cat. I am a cat'

>>> # With sampling, the output becomes more creative!
>>> generated_ids = model.generate(**model_inputs, do_sample=True)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'I am a cat.\nI just need to be. I am always.\nEvery time'
```

### Falsche Auff√ºllseite

LLMs sind [decoder-only](https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt)-Architekturen, d.h. sie iterieren weiter √ºber Ihre Eingabeaufforderung. Wenn Ihre Eingaben nicht die gleiche L√§nge haben, m√ºssen sie aufgef√ºllt werden. Da LLMs nicht darauf trainiert sind, mit aufgef√ºllten Token fortzufahren, muss Ihre Eingabe links aufgef√ºllt werden. Vergessen Sie auch nicht, die Aufmerksamkeitsmaske an generate zu √ºbergeben!

```py
>>> # The tokenizer initialized above has right-padding active by default: the 1st sequence,
>>> # which is shorter, has padding on the right side. Generation fails.
>>> model_inputs = tokenizer(
...     ["1, 2, 3", "A, B, C, D, E"], padding=True, return_tensors="pt"
... ).to("cuda")
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids[0], skip_special_tokens=True)[0]
''

>>> # With left-padding, it works as expected!
>>> tokenizer = AutoTokenizer.from_pretrained("openlm-research/open_llama_7b", padding_side="left")
>>> tokenizer.pad_token = tokenizer.eos_token  # Llama has no pad token by default
>>> model_inputs = tokenizer(
...     ["1, 2, 3", "A, B, C, D, E"], padding=True, return_tensors="pt"
... ).to("cuda")
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'1, 2, 3, 4, 5, 6,'
```

<!-- TODO: when the prompting guide is ready, mention the importance of setting the right prompt in this section -->

## Weitere Ressourcen

W√§hrend der Prozess der autoregressiven Generierung relativ einfach ist, kann die optimale Nutzung Ihres LLM ein schwieriges Unterfangen sein, da es viele bewegliche Teile gibt. F√ºr Ihre n√§chsten Schritte, die Ihnen helfen, tiefer in die LLM-Nutzung und das Verst√§ndnis einzutauchen:

<!-- TODO: mit neuen Anleitungen vervollst√§ndigen -->
### Fortgeschrittene Nutzung generieren

1. [Leitfaden](generation_strategies) zur Steuerung verschiedener Generierungsmethoden, zur Einrichtung der Generierungskonfigurationsdatei und zum Streaming der Ausgabe;
2. API-Referenz zu [`~generation.GenerationConfig`], [`~generation.GenerationMixin.generate`] und [generate-bezogene Klassen](internal/generation_utils).

### LLM-Ranglisten

1. [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard), das sich auf die Qualit√§t der Open-Source-Modelle konzentriert;
2. [Open LLM-Perf Leaderboard](https://huggingface.co/spaces/optimum/llm-perf-leaderboard), das sich auf den LLM-Durchsatz konzentriert.

### Latenz und Durchsatz

1. [Leitfaden](main_classes/quantization) zur dynamischen Quantisierung, der Ihnen zeigt, wie Sie Ihren Speicherbedarf drastisch reduzieren k√∂nnen.

### Verwandte Bibliotheken

1. [text-generation-inference](https://github.com/huggingface/text-generation-inference), ein produktionsreifer Server f√ºr LLMs;
2. [`optimum`](https://github.com/huggingface/optimum), eine Erweiterung von ü§ó Transformers, die f√ºr bestimmte Hardware-Ger√§te optimiert.
