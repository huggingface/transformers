<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Modelos multilingu√≠sticos para infer√™ncia

[[open-in-colab]]

Existem v√°rios modelos multilingu√≠sticos no ü§ó Transformers e seus usos para infer√™ncia diferem dos modelos monol√≠ngues.
No entanto, nem *todos* os usos dos modelos multil√≠ngues s√£o t√£o diferentes.
Alguns modelos, como o [google-bert/bert-base-multilingual-uncased](https://huggingface.co/google-bert/bert-base-multilingual-uncased),
podem ser usados como se fossem monol√≠ngues. Este guia ir√° te ajudar a usar modelos multil√≠ngues cujo uso difere
para o prop√≥sito de infer√™ncia.

## XLM

O XLM tem dez checkpoints diferentes dos quais apenas um √© monol√≠ngue.
Os nove checkpoints restantes do modelo s√£o subdivididos em duas categorias:
checkpoints que usam de language embeddings e os que n√£o.

### XLM com language embeddings

Os seguintes modelos de XLM usam language embeddings para especificar a linguagem utilizada para a infer√™ncia.

- `FacebookAI/xlm-mlm-ende-1024` (Masked language modeling, English-German)
- `FacebookAI/xlm-mlm-enfr-1024` (Masked language modeling, English-French)
- `FacebookAI/xlm-mlm-enro-1024` (Masked language modeling, English-Romanian)
- `FacebookAI/xlm-mlm-xnli15-1024` (Masked language modeling, XNLI languages)
- `FacebookAI/xlm-mlm-tlm-xnli15-1024` (Masked language modeling + translation, XNLI languages)
- `FacebookAI/xlm-clm-enfr-1024` (Causal language modeling, English-French)
- `FacebookAI/xlm-clm-ende-1024` (Causal language modeling, English-German)

Os language embeddings s√£o representados por um tensor de mesma dimens√£o que os `input_ids` passados ao modelo.
Os valores destes tensores dependem do idioma utilizado e se identificam pelos atributos `lang2id` e `id2lang` do tokenizador.

Neste exemplo, carregamos o checkpoint `FacebookAI/xlm-clm-enfr-1024`(Causal language modeling, English-French):

```py
>>> import torch
>>> from transformers import XLMTokenizer, XLMWithLMHeadModel

>>> tokenizer = XLMTokenizer.from_pretrained("FacebookAI/xlm-clm-enfr-1024")
>>> model = XLMWithLMHeadModel.from_pretrained("FacebookAI/xlm-clm-enfr-1024")
```

O atributo `lang2id` do tokenizador mostra os idiomas deste modelo e seus ids:

```py
>>> print(tokenizer.lang2id)
{'en': 0, 'fr': 1}
```

Em seguida, cria-se um input de exemplo:

```py
>>> input_ids = torch.tensor([tokenizer.encode("Wikipedia was used to")])  # batch size of 1
```

Estabelece-se o id do idioma, por exemplo `"en"`, e utiliza-se o mesmo para definir a language embedding.
A language embedding √© um tensor preenchido com `0`, que √© o id de idioma para o ingl√™s.
Este tensor deve ser do mesmo tamanho que os `input_ids`.

```py
>>> language_id = tokenizer.lang2id["en"]  # 0
>>> langs = torch.tensor([language_id] * input_ids.shape[1])  # torch.tensor([0, 0, 0, ..., 0])

>>> # We reshape it to be of size (batch_size, sequence_length)
>>> langs = langs.view(1, -1)  # is now of shape [1, sequence_length] (we have a batch size of 1)
```

Agora voc√™ pode passar os `input_ids` e a language embedding ao modelo:

```py
>>> outputs = model(input_ids, langs=langs)
```

O script [run_generation.py](https://github.com/huggingface/transformers/tree/master/examples/pytorch/text-generation/run_generation.py) pode gerar um texto com language embeddings utilizando os checkpoints `xlm-clm`.

### XLM sem language embeddings

Os seguintes modelos XLM n√£o requerem o uso de language embeddings durante a infer√™ncia:

- `FacebookAI/xlm-mlm-17-1280` (Modelagem de linguagem com m√°scara, 17 idiomas)
- `FacebookAI/xlm-mlm-100-1280` (Modelagem de linguagem com m√°scara, 100 idiomas)

Estes modelos s√£o utilizados para representa√ß√µes gen√©ricas de frase diferentemente dos checkpoints XLM anteriores.

## BERT

Os seguintes modelos do BERT podem ser utilizados para tarefas multilingu√≠sticas:

- `google-bert/bert-base-multilingual-uncased` (Modelagem de linguagem com m√°scara + Previs√£o de frases, 102 idiomas)
- `google-bert/bert-base-multilingual-cased` (Modelagem de linguagem com m√°scara + Previs√£o de frases, 104 idiomas)

Estes modelos n√£o requerem language embeddings durante a infer√™ncia. Devem identificar a linguagem a partir
do contexto e realizar a infer√™ncia em sequ√™ncia.

## XLM-RoBERTa

Os seguintes modelos do XLM-RoBERTa podem ser utilizados para tarefas multilingu√≠sticas:

- `FacebookAI/xlm-roberta-base` (Modelagem de linguagem com m√°scara, 100 idiomas)
- `FacebookAI/xlm-roberta-large` Modelagem de linguagem com m√°scara, 100 idiomas)

O XLM-RoBERTa foi treinado com 2,5 TB de dados do CommonCrawl rec√©m-criados e testados em 100 idiomas.
Proporciona fortes vantagens sobre os modelos multilingu√≠sticos publicados anteriormente como o mBERT e o XLM em tarefas
subsequentes como a classifica√ß√£o, a rotulagem de sequ√™ncias e √† respostas a perguntas.

## M2M100

Os seguintes modelos de M2M100 podem ser utilizados para tradu√ß√µes multilingu√≠sticas:

- `facebook/m2m100_418M` (Tradu√ß√£o)
- `facebook/m2m100_1.2B` (Tradu√ß√£o)

Neste exemplo, o checkpoint `facebook/m2m100_418M` √© carregado para traduzir do mandarim ao ingl√™s. √â poss√≠vel
estabelecer o idioma de origem no tokenizador:

```py
>>> from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer

>>> en_text = "Do not meddle in the affairs of wizards, for they are subtle and quick to anger."
>>> chinese_text = "‰∏çË¶ÅÊèíÊâãÂ∑´Â∏´ÁöÑ‰∫ãÂãô, Âõ†ÁÇ∫‰ªñÂÄëÊòØÂæÆÂ¶ôÁöÑ, ÂæàÂø´Â∞±ÊúÉÁôºÊÄí."

>>> tokenizer = M2M100Tokenizer.from_pretrained("facebook/m2m100_418M", src_lang="zh")
>>> model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_418M")
```

Tokeniza√ß√£o do texto:

```py
>>> encoded_zh = tokenizer(chinese_text, return_tensors="pt")
```

O M2M100 for√ßa o id do idioma de destino como o primeiro token gerado para traduzir ao idioma de destino.
√â definido o `forced_bos_token_id` como `en` no m√©todo `generate` para traduzir ao ingl√™s.

```py
>>> generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id("en"))
>>> tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
'Do not interfere with the matters of the witches, because they are delicate and will soon be angry.'
```

## MBart

Os seguintes modelos do MBart podem ser utilizados para tradu√ß√£o multilingu√≠stica:

- `facebook/mbart-large-50-one-to-many-mmt` (Tradu√ß√£o autom√°tica multilingu√≠stica de um a v√°rios, 50 idiomas)
- `facebook/mbart-large-50-many-to-many-mmt` (Tradu√ß√£o autom√°tica multilingu√≠stica de v√°rios a v√°rios, 50 idiomas)
- `facebook/mbart-large-50-many-to-one-mmt` (Tradu√ß√£o autom√°tica multilingu√≠stica v√°rios a um, 50 idiomas)
- `facebook/mbart-large-50` (Tradu√ß√£o multilingu√≠stica, 50 idiomas)
- `facebook/mbart-large-cc25`

Neste exemplo, carrega-se o checkpoint `facebook/mbart-large-50-many-to-many-mmt` para traduzir do finland√™s ao ingl√™s.
Pode-se definir o idioma de origem no tokenizador:

```py
>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

>>> en_text = "Do not meddle in the affairs of wizards, for they are subtle and quick to anger."
>>> fi_text = "√Ñl√§ sekaannu velhojen asioihin, sill√§ ne ovat hienovaraisia ja nopeasti vihaisia."

>>> tokenizer = AutoTokenizer.from_pretrained("facebook/mbart-large-50-many-to-many-mmt", src_lang="fi_FI")
>>> model = AutoModelForSeq2SeqLM.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")
```

Tokenizando o texto:

```py
>>> encoded_en = tokenizer(en_text, return_tensors="pt")
```

O MBart for√ßa o id do idioma de destino como o primeiro token gerado para traduzir ao idioma de destino.
√â definido o `forced_bos_token_id` como `en` no m√©todo `generate` para traduzir ao ingl√™s.

```py
>>> generated_tokens = model.generate(**encoded_en, forced_bos_token_id=tokenizer.lang_code_to_id("en_XX"))
>>> tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
"Don't interfere with the wizard's affairs, because they are subtle, will soon get angry."
```

Se estiver usando o checkpoint `facebook/mbart-large-50-many-to-one-mmt` n√£o ser√° necess√°rio for√ßar o id do idioma de destino
como sendo o primeiro token generado, caso contr√°rio a usagem √© a mesma.
