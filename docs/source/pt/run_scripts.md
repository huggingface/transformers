<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Treinamento a partir de um script

Junto com os ü§ó Transformers [notebooks](./noteboks/README), tamb√©m h√° scripts de exemplo demonstrando como treinar um modelo para uma tarefa com [PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch), [TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow) ou [JAX/Flax](https://github.com/huggingface/transformers/tree/main/examples/flax).

Voc√™ tamb√©m encontrar√° scripts que usamos em nossos [projetos de pesquisa](https://github.com/huggingface/transformers/tree/main/examples/research_projects) e [exemplos legados](https://github.com/huggingface/transformers/tree/main/examples/legacy) que s√£o principalmente contribui√ß√µes da comunidade. Esses scripts n√£o s√£o mantidos ativamente e exigem uma vers√£o espec√≠fica de ü§ó Transformers que provavelmente ser√° incompat√≠vel com a vers√£o mais recente da biblioteca.

N√£o se espera que os scripts de exemplo funcionem imediatamente em todos os problemas, voc√™ pode precisar adaptar o script ao problema que est√° tentando resolver. Para ajud√°-lo com isso, a maioria dos scripts exp√µe totalmente como os dados s√£o pr√©-processados, permitindo que voc√™ os edite conforme necess√°rio para seu caso de uso.

Para qualquer recurso que voc√™ gostaria de implementar em um script de exemplo, discuta-o no [f√≥rum](https://discuss.huggingface.co/) ou em uma [issue](https://github.com/huggingface/transformers/issues) antes de enviar um Pull Request. Embora recebamos corre√ß√µes de bugs, √© improv√°vel que mesclaremos um Pull Request que adicione mais funcionalidades ao custo de legibilidade.

Este guia mostrar√° como executar um exemplo de script de treinamento de sumariza√ß√£o em [PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization) e [TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/summarization). Espera-se que todos os exemplos funcionem com ambas as estruturas, a menos que especificado de outra forma.

## Configura√ß√£o

Para executar com √™xito a vers√£o mais recente dos scripts de exemplo, voc√™ precisa **instalar o ü§ó Transformers da fonte** em um novo ambiente virtual:

```bash
git clone https://github.com/huggingface/transformers
cd transformers
pip install .
```

Para vers√µes mais antigas dos scripts de exemplo, clique no bot√£o abaixo:

<details>
  <summary>Exemplos para vers√µes antigas dos ü§ó Transformers</summary>
	<ul>
		<li><a href="https://github.com/huggingface/transformers/tree/v4.5.1/examples">v4.5.1</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v4.4.2/examples">v4.4.2</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v4.3.3/examples">v4.3.3</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v4.2.2/examples">v4.2.2</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v4.1.1/examples">v4.1.1</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v4.0.1/examples">v4.0.1</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v3.5.1/examples">v3.5.1</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v3.4.0/examples">v3.4.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v3.3.1/examples">v3.3.1</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v3.2.0/examples">v3.2.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v3.1.0/examples">v3.1.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v3.0.2/examples">v3.0.2</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.11.0/examples">v2.11.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.10.0/examples">v2.10.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.9.1/examples">v2.9.1</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.8.0/examples">v2.8.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.7.0/examples">v2.7.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.6.0/examples">v2.6.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.5.1/examples">v2.5.1</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.4.0/examples">v2.4.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.3.0/examples">v2.3.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.2.0/examples">v2.2.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.1.0/examples">v2.1.1</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.0.0/examples">v2.0.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v1.2.0/examples">v1.2.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v1.1.0/examples">v1.1.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v1.0.0/examples">v1.0.0</a></li>
	</ul>
</details>

Em seguida, mude seu clone atual dos ü§ó Transformers para uma vers√£o espec√≠fica, como v3.5.1, por exemplo:

```bash
git checkout tags/v3.5.1
```

Depois de configurar a vers√£o correta da biblioteca, navegue at√© a pasta de exemplo de sua escolha e instale os requisitos espec√≠ficos do exemplo:

```bash
pip install -r requirements.txt
```

## Executando um script

<frameworkcontent>
<pt>

O script de exemplo baixa e pr√©-processa um conjunto de dados da biblioteca ü§ó [Datasets](https://huggingface.co/docs/datasets/). Em seguida, o script ajusta um conjunto de dados com o [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) em uma arquitetura que oferece suporte √† sumariza√ß√£o. O exemplo a seguir mostra como ajustar [T5-small](https://huggingface.co/google-t5/t5-small) no conjunto de dados [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail). O modelo T5 requer um argumento `source_prefix` adicional devido √† forma como foi treinado. Este prompt informa ao T5 que esta √© uma tarefa de sumariza√ß√£o.

```bash
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```
</pt>
<tf>
Este outro script de exemplo baixa e pr√©-processa um conjunto de dados da biblioteca ü§ó [Datasets](https://huggingface.co/docs/datasets/). Em seguida, o script ajusta um conjunto de dados usando Keras em uma arquitetura que oferece suporte √† sumariza√ß√£o. O exemplo a seguir mostra como ajustar [T5-small](https://huggingface.co/google-t5/t5-small) no conjunto de dados [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail). O modelo T5 requer um argumento `source_prefix` adicional devido √† forma como foi treinado. Este prompt informa ao T5 que esta √© uma tarefa de sumariza√ß√£o.

```bash
python examples/tensorflow/summarization/run_summarization.py  \
    --model_name_or_path google-t5/t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --output_dir /tmp/tst-summarization  \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 16 \
    --num_train_epochs 3 \
    --do_train \
    --do_eval
```
</tf>
</frameworkcontent>

## Treinamento distribu√≠do e precis√£o mista

O [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) oferece suporte a treinamento distribu√≠do e precis√£o mista, o que significa que voc√™ tamb√©m pode us√°-lo em um script. Para habilitar esses dois recursos:

- Adicione o argumento `fp16` para habilitar a precis√£o mista.
- Defina o n√∫mero de GPUs a serem usadas com o argumento `nproc_per_node`.

```bash
torchrun \
    --nproc_per_node 8 pytorch/summarization/run_summarization.py \
    --fp16 \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

Os scripts do TensorFlow utilizam um [`MirroredStrategy`](https://www.tensorflow.org/guide/distributed_training#mirroredstrategy) para treinamento distribu√≠do, e voc√™ n√£o precisa adicionar argumentos adicionais ao script de treinamento. O script do TensorFlow usar√° v√°rias GPUs por padr√£o, se estiverem dispon√≠veis.

## Executando um script em uma TPU

<frameworkcontent>
<pt>
As Unidades de Processamento de Tensor (TPUs) s√£o projetadas especificamente para acelerar o desempenho. O PyTorch oferece suporte a TPUs com o compilador de aprendizado profundo [XLA](https://www.tensorflow.org/xla) (consulte [aqui](https://github.com/pytorch/xla/blob/master/README.md) para mais detalhes). Para usar uma TPU, inicie o script `xla_spawn.py` e use o argumento `num_cores` para definir o n√∫mero de n√∫cleos de TPU que voc√™ deseja usar.

```bash
python xla_spawn.py --num_cores 8 \
    summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```
</pt>
<tf>

As Unidades de Processamento de Tensor (TPUs) s√£o projetadas especificamente para acelerar o desempenho. Os scripts do TensorFlow utilizam uma [`TPUStrategy`](https://www.tensorflow.org/guide/distributed_training#tpustrategy) para treinamento em TPUs. Para usar uma TPU, passe o nome do recurso TPU para o argumento `tpu`.

```bash
python run_summarization.py  \
    --tpu name_of_tpu_resource \
    --model_name_or_path google-t5/t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --output_dir /tmp/tst-summarization  \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 16 \
    --num_train_epochs 3 \
    --do_train \
    --do_eval
```
</tf>
</frameworkcontent>

## Execute um script com ü§ó Accelerate

ü§ó [Accelerate](https://huggingface.co/docs/accelerate) √© uma biblioteca somente do PyTorch que oferece um m√©todo unificado para treinar um modelo em v√°rios tipos de configura√ß√µes (CPU, multiplas GPUs, TPUs), mantendo visibilidade no loop de treinamento do PyTorch. Certifique-se de ter o ü§ó Accelerate instalado se ainda n√£o o tiver:

> Nota: Como o Accelerate est√° se desenvolvendo rapidamente, a vers√£o git do Accelerate deve ser instalada para executar os scripts

```bash
pip install git+https://github.com/huggingface/accelerate
```

Em vez do script `run_summarization.py`, voc√™ precisa usar o script `run_summarization_no_trainer.py`. Os scripts suportados pelo ü§ó Accelerate ter√£o um arquivo `task_no_trainer.py` na pasta. Comece executando o seguinte comando para criar e salvar um arquivo de configura√ß√£o:

```bash
accelerate config
```

Teste sua configura√ß√£o para garantir que ela esteja corretamente configurada :

```bash
accelerate test
```

Agora voc√™ est√° pronto para iniciar o treinamento:

```bash
accelerate launch run_summarization_no_trainer.py \
    --model_name_or_path google-t5/t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir ~/tmp/tst-summarization
```

## Usando um conjunto de dados personalizado

O script de resumo oferece suporte a conjuntos de dados personalizados, desde que sejam um arquivo CSV ou JSON. Ao usar seu pr√≥prio conjunto de dados, voc√™ precisa especificar v√°rios argumentos adicionais:

- `train_file` e `validation_file` especificam o caminho para seus arquivos de treinamento e valida√ß√£o respectivamente.
- `text_column` √© o texto de entrada para sumariza√ß√£o.
- `summary_column` √© o texto de destino para sa√≠da.

Um script para sumariza√ß√£o usando um conjunto de dados customizado ficaria assim:

```bash
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --train_file path_to_csv_or_jsonlines_file \
    --validation_file path_to_csv_or_jsonlines_file \
    --text_column text_column_name \
    --summary_column summary_column_name \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --overwrite_output_dir \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --predict_with_generate
```

## Testando um script 

Geralmente, √© uma boa ideia executar seu script em um n√∫mero menor de exemplos de conjuntos de dados para garantir que tudo funcione conforme o esperado antes de se comprometer com um conjunto de dados inteiro, que pode levar horas para ser conclu√≠do. Use os seguintes argumentos para truncar o conjunto de dados para um n√∫mero m√°ximo de amostras:

- `max_train_samples`
- `max_eval_samples`
- `max_predict_samples`

```bash
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --max_train_samples 50 \
    --max_eval_samples 50 \
    --max_predict_samples 50 \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

Nem todos os scripts de exemplo suportam o argumento `max_predict_samples`. Se voc√™ n√£o tiver certeza se seu script suporta este argumento, adicione o argumento `-h` para verificar:

```bash
examples/pytorch/summarization/run_summarization.py -h
```

## Retomar o treinamento a partir de um checkpoint

Outra op√ß√£o √∫til para habilitar √© retomar o treinamento de um checkpoint anterior. Isso garantir√° que voc√™ possa continuar de onde parou sem recome√ßar se o seu treinamento for interrompido. Existem dois m√©todos para retomar o treinamento a partir de um checkpoint.

O primeiro m√©todo usa o argumento `output_dir previous_output_dir` para retomar o treinamento do √∫ltimo checkpoint armazenado em `output_dir`. Neste caso, voc√™ deve remover `overwrite_output_dir`:

```bash
python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --output_dir previous_output_dir \
    --predict_with_generate
```

O segundo m√©todo usa o argumento `resume_from_checkpoint path_to_specific_checkpoint` para retomar o treinamento de uma pasta de checkpoint espec√≠fica.

```bash
python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --resume_from_checkpoint path_to_specific_checkpoint \
    --predict_with_generate
```

## Compartilhando seu modelo 

Todos os scripts podem enviar seu modelo final para o [Model Hub](https://huggingface.co/models). Certifique-se de estar conectado ao Hugging Face antes de come√ßar:

```bash
huggingface-cli login
```

Em seguida, adicione o argumento `push_to_hub` ao script. Este argumento criar√° um reposit√≥rio com seu nome de usu√°rio do Hugging Face e o nome da pasta especificado em `output_dir`.

Para dar um nome espec√≠fico ao seu reposit√≥rio, use o argumento `push_to_hub_model_id` para adicion√°-lo. O reposit√≥rio ser√° listado automaticamente em seu namespace.

O exemplo a seguir mostra como fazer upload de um modelo com um nome de reposit√≥rio espec√≠fico:

```bash
python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --push_to_hub \
    --push_to_hub_model_id finetuned-t5-cnn_dailymail \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```
