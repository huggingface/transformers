<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Tour r√°pido

[[open-in-colab]]

Comece a trabalhar com ü§ó Transformers! Comece usando [`pipeline`] para r√°pida infer√™ncia e facilmente carregue um modelo pr√©-treinado e um tokenizer com [AutoClass](./model_doc/auto) para resolver tarefas de texto, vis√£o ou √°udio.

<Tip>

Todos os exemplos de c√≥digo apresentados na documenta√ß√£o t√™m um bot√£o no canto superior direito para escolher se voc√™ deseja ocultar ou mostrar o c√≥digo no Pytorch ou no TensorFlow. Caso contr√°rio, √© esperado que funcione para ambos back-ends sem nenhuma altera√ß√£o.

</Tip>

## Pipeline

[`pipeline`] √© a maneira mais f√°cil de usar um modelo pr√©-treinado para uma dada tarefa.

<Youtube id="tiZFewofSLM"/>

A [`pipeline`] apoia diversas tarefas fora da caixa:

**Texto**:
* An√°lise sentimental: classifica a polaridade de um texto.
* Gera√ß√£o de texto (em Ingl√™s): gera texto a partir de uma entrada.
* Reconhecimento de entidade mencionada: legenda cada palavra com uma classe que a representa (pessoa, data, local, etc...)
* Respostas: extrai uma resposta dado algum contexto e uma quest√£o
* M√°scara de preenchimento: preenche o espa√ßo, dado um texto com m√°scaras de palavras.
* Sumariza√ß√£o: gera o resumo de um texto longo ou documento.
* Tradu√ß√£o: traduz texto para outra l√≠ngua.
* Extra√ß√£o de caracter√≠sticas: cria um tensor que representa o texto.

**Imagem**:
* Classifica√ß√£o de imagens: classifica uma imagem.
* Segmenta√ß√£o de imagem: classifica cada pixel da imagem.
* Detec√ß√£o de objetos: detecta objetos em uma imagem.

**Audio**:
* Classfica√ß√£o de √°udio: legenda um trecho de √°udio fornecido.
* Reconhecimento de fala autom√°tico: transcreve audio em texto.

<Tip>

Para mais detalhes sobre a [`pipeline`] e tarefas associadas, siga a documenta√ß√£o [aqui](./main_classes/pipelines).

</Tip>

### Uso da pipeline

No exemplo a seguir, voc√™ usar√° [`pipeline`] para an√°lise sentimental.

Instale as seguintes depend√™ncias se voc√™ ainda n√£o o fez:


<frameworkcontent>
<pt>
```bash
pip install torch
```
</pt>
<tf>
```bash
pip install tensorflow
```
</tf>
</frameworkcontent>

Importe [`pipeline`] e especifique a tarefa que deseja completar:

```py
>>> from transformers import pipeline

>>> classifier = pipeline("sentiment-analysis")
```

A pipeline baixa and armazena um [modelo pr√©-treinado](https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english) padr√£o e tokenizer para an√°lise sentimental. Agora voc√™ pode usar `classifier` no texto alvo:

```py
>>> classifier("We are very happy to show you the ü§ó Transformers library.")
[{'label': 'POSITIVE', 'score': 0.9998}]
```

Para mais de uma senten√ßa, passe uma lista para a [`pipeline`], a qual retornar√° uma lista de dicion√°rios:

```py
>>> results = classifier(["We are very happy to show you the ü§ó Transformers library.", "We hope you don't hate it."])
>>> for result in results:
...     print(f"label: {result['label']}, with score: {round(result['score'], 4)}")
label: POSITIVE, with score: 0.9998
label: NEGATIVE, with score: 0.5309
```

A [`pipeline`] tamb√©m pode iterar sobre um Dataset inteiro. Comece instalando a biblioteca de [ü§ó Datasets](https://huggingface.co/docs/datasets/):

```bash
pip install datasets
```

Crie uma [`pipeline`] com a tarefa que deseja resolver e o modelo que deseja usar.

```py
>>> import torch
>>> from transformers import pipeline

>>> speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h")
```

A seguir, carregue uma base de dados (confira a ü§ó [Inicia√ß√£o em Datasets](https://huggingface.co/docs/datasets/quickstart) para mais detalhes) que voc√™ gostaria de iterar sobre. Por exemplo, vamos carregar o dataset [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14):

```py
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")  # doctest: +IGNORE_RESULT
```

Precisamos garantir que a taxa de amostragem do conjunto de dados corresponda √† taxa de amostragem em que o facebook/wav2vec2-base-960h foi treinado.

```py
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))
```

Os arquivos de √°udio s√£o carregados e re-amostrados automaticamente ao chamar a coluna `"audio"`.
Vamos extrair as arrays de formas de onda originais das primeiras 4 amostras e pass√°-las como uma lista para o pipeline:

```py
>>> result = speech_recognizer(dataset[:4]["audio"])
>>> print([d["text"] for d in result])
['I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT', "FONDERING HOW I'D SET UP A JOIN TO HET WITH MY WIFE AND WHERE THE AP MIGHT BE", "I I'D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I'M NOT SEEING THE OPTION TO DO IT ON THE APSO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AND I'M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS", 'HOW DO I TURN A JOIN A COUNT']
```

Para um conjunto de dados maior onde as entradas s√£o maiores (como em fala ou vis√£o), ser√° necess√°rio passar um gerador em vez de uma lista que carregue todas as entradas na mem√≥ria. Consulte a [documenta√ß√£o do pipeline](./main_classes/pipelines) para mais informa√ß√µes.

### Use outro modelo e tokenizer na pipeline

A [`pipeline`] pode acomodar qualquer modelo do [Model Hub](https://huggingface.co/models), facilitando sua adapta√ß√£o para outros casos de uso. Por exemplo, se voc√™ quiser um modelo capaz de lidar com texto em franc√™s, use as tags no Model Hub para filtrar um modelo apropriado. O principal resultado filtrado retorna um [modelo BERT](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment) bil√≠ngue ajustado para an√°lise de sentimentos. √ìtimo, vamos usar este modelo!

```py
>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
```

<frameworkcontent>
<pt>
Use o [`AutoModelForSequenceClassification`] e [`AutoTokenizer`] para carregar o modelo pr√©-treinado e seu tokenizer associado (mais em `AutoClass` abaixo):

```py
>>> from transformers import AutoTokenizer, AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained(model_name)
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```
</pt>
<tf>

Use o [`TFAutoModelForSequenceClassification`] and [`AutoTokenizer`] para carregar o modelo pr√©-treinado e o tokenizer associado (mais em `TFAutoClass` abaixo):

```py
>>> from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

>>> model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```
</tf>
</frameworkcontent>

Ent√£o voc√™ pode especificar o modelo e o tokenizador na [`pipeline`] e aplicar o `classifier` no seu texto alvo:

```py
>>> classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
>>> classifier("Nous sommes tr√®s heureux de vous pr√©senter la biblioth√®que ü§ó Transformers.")
[{'label': '5 stars', 'score': 0.7273}]
```

Se voc√™ n√£o conseguir achar um modelo para o seu caso de uso, precisar√° usar fine-tune em um modelo pr√©-treinado nos seus dados. Veja nosso [tutorial de fine-tuning](./training) para descobrir como. Finalmente, depois que voc√™ tiver usado esse processo em seu modelo, considere compartilh√°-lo conosco (veja o tutorial [aqui](./model_sharing)) na plataforma Model Hub afim de democratizar NLP! ü§ó

## AutoClass

<Youtube id="AhChOFRegn4"/>

Por baixo dos panos, as classes [`AutoModelForSequenceClassification`] e [`AutoTokenizer`] trabalham juntas para fortificar o [`pipeline`]. Um [AutoClass](./model_doc/auto) √© um atalho que automaticamente recupera a arquitetura de um modelo pr√©-treinado a partir de seu nome ou caminho. Basta selecionar a `AutoClass` apropriada para sua tarefa e seu tokenizer associado com [`AutoTokenizer`].

Vamos voltar ao nosso exemplo e ver como voc√™ pode usar a `AutoClass` para replicar os resultados do [`pipeline`].

### AutoTokenizer

Um tokenizer √© respons√°vel por pr√©-processar o texto em um formato que seja compreens√≠vel para o modelo. Primeiro, o tokenizer dividir√° o texto em palavras chamadas *tokens*. Existem v√°rias regras que regem o processo de tokeniza√ß√£o, incluindo como dividir uma palavra e em que n√≠vel (saiba mais sobre tokeniza√ß√£o [aqui](./tokenizer_summary)). A coisa mais importante a lembrar, por√©m, √© que voc√™ precisa instanciar o tokenizer com o mesmo nome do modelo para garantir que est√° usando as mesmas regras de tokeniza√ß√£o com as quais um modelo foi pr√©-treinado.

Carregue um tokenizer com [`AutoTokenizer`]:

```py
>>> from transformers import AutoTokenizer

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```

Em seguida, o tokenizer converte os tokens em n√∫meros para construir um tensor como entrada para o modelo. Isso √© conhecido como o *vocabul√°rio* do modelo.

Passe o texto para o tokenizer:

```py
>>> encoding = tokenizer("We are very happy to show you the ü§ó Transformers library.")
>>> print(encoding)
{'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

O tokenizer retornar√° um dicion√°rio contendo:

* [input_ids](./glossary#input-ids): representa√ß√µes num√©ricas de seus tokens.
* [attention_mask](.glossary#attention-mask): indica quais tokens devem ser atendidos.

Assim como o [`pipeline`], o tokenizer aceitar√° uma lista de entradas. Al√©m disso, o tokenizer tamb√©m pode preencher e truncar o texto para retornar um lote com comprimento uniforme:

<frameworkcontent>
<pt>

```py
>>> pt_batch = tokenizer(
...     ["We are very happy to show you the ü§ó transformers library.", "We hope you don't hate it."],
...     padding=True,
...     truncation=True,
...     max_length=512,
...     return_tensors="pt",
... )
```
</pt>
<tf>

```py
>>> tf_batch = tokenizer(
...     ["We are very happy to show you the ü§ó Transformers library.", "We hope you don't hate it."],
...     padding=True,
...     truncation=True,
...     max_length=512,
...     return_tensors="tf",
... )
```
</tf>
</frameworkcontent>

Leia o tutorial de [pr√©-processamento](./pr√©-processamento) para obter mais detalhes sobre tokeniza√ß√£o.

### AutoModel

<frameworkcontent>
<pt>
ü§ó Transformers fornecem uma maneira simples e unificada de carregar inst√¢ncias pr√©-treinadas. Isso significa que voc√™ pode carregar um [`AutoModel`] como carregaria um [`AutoTokenizer`]. A √∫nica diferen√ßa √© selecionar o [`AutoModel`] correto para a tarefa. Como voc√™ est√° fazendo classifica√ß√£o de texto ou sequ√™ncia, carregue [`AutoModelForSequenceClassification`]:

```py
>>> from transformers import AutoModelForSequenceClassification

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
```

<Tip>

Veja o [sum√°rio de tarefas](./task_summary) para qual classe de [`AutoModel`] usar para cada tarefa.

</Tip>

Agora voc√™ pode passar seu grupo de entradas pr√©-processadas diretamente para o modelo. Voc√™ apenas tem que descompactar o dicion√°rio usando `**`:

```py
>>> pt_outputs = pt_model(**pt_batch)
```

O modelo gera as ativa√ß√µes finais no atributo `logits`. Aplique a fun√ß√£o softmax aos `logits` para recuperar as probabilidades:

```py
>>> from torch import nn

>>> pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
>>> print(pt_predictions)
tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],
        [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)
```
</pt>
<tf>
ü§ó Transformers fornecem uma maneira simples e unificada de carregar inst√¢ncias pr√©-treinadas. Isso significa que voc√™ pode carregar um [`TFAutoModel`] como carregaria um [`AutoTokenizer`]. A √∫nica diferen√ßa √© selecionar o [`TFAutoModel`] correto para a tarefa. Como voc√™ est√° fazendo classifica√ß√£o de texto ou sequ√™ncia, carregue [`TFAutoModelForSequenceClassification`]:

```py
>>> from transformers import TFAutoModelForSequenceClassification

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
```

<Tip>

Veja o [sum√°rio de tarefas](./task_summary) para qual classe de [`AutoModel`] usar para cada tarefa.

</Tip>

Agora voc√™ pode passar seu grupo de entradas pr√©-processadas diretamente para o modelo atrav√©s da passagem de chaves de dicion√°rios ao tensor.

```py
>>> tf_outputs = tf_model(tf_batch)
```

O modelo gera as ativa√ß√µes finais no atributo `logits`. Aplique a fun√ß√£o softmax aos `logits` para recuperar as probabilidades:

```py
>>> import tensorflow as tf

>>> tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)
>>> tf_predictions  # doctest: +IGNORE_RESULT
```
</tf>
</frameworkcontent>

<Tip>

Todos os modelos de ü§ó Transformers (PyTorch ou TensorFlow) geram tensores *antes* da fun√ß√£o de ativa√ß√£o final (como softmax) pois essa fun√ß√£o algumas vezes √© fundida com a perda.


</Tip>

Os modelos s√£o um standard [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) ou um [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) para que voc√™ possa us√°-los em seu loop de treinamento habitual. No entanto, para facilitar as coisas, ü§ó Transformers fornece uma classe [`Trainer`] para PyTorch que adiciona funcionalidade para treinamento distribu√≠do, precis√£o mista e muito mais. Para o TensorFlow, voc√™ pode usar o m√©todo `fit` de [Keras](https://keras.io/). Consulte o [tutorial de treinamento](./training) para obter mais detalhes.

<Tip>

As sa√≠das do modelo ü§ó Transformers s√£o classes de dados especiais para que seus atributos sejam preenchidos automaticamente em um IDE.
As sa√≠das do modelo tamb√©m se comportam como uma tupla ou um dicion√°rio (por exemplo, voc√™ pode indexar com um inteiro, uma parte ou uma string), caso em que os atributos `None` s√£o ignorados.

</Tip>

### Salvar um modelo

<frameworkcontent>
<pt>
Uma vez que seu modelo estiver afinado, voc√™ pode salv√°-lo com seu Tokenizer usando [`PreTrainedModel.save_pretrained`]:

```py
>>> pt_save_directory = "./pt_save_pretrained"
>>> tokenizer.save_pretrained(pt_save_directory)  # doctest: +IGNORE_RESULT
>>> pt_model.save_pretrained(pt_save_directory)
```

Quando voc√™ estiver pronto para us√°-lo novamente, recarregue com [`PreTrainedModel.from_pretrained`]:

```py
>>> pt_model = AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained")
```
</pt>
<tf>
Uma vez que seu modelo estiver afinado, voc√™ pode salv√°-lo com seu Tokenizer usando [`TFPreTrainedModel.save_pretrained`]:

```py
>>> tf_save_directory = "./tf_save_pretrained"
>>> tokenizer.save_pretrained(tf_save_directory)  # doctest: +IGNORE_RESULT
>>> tf_model.save_pretrained(tf_save_directory)
```

Quando voc√™ estiver pronto para us√°-lo novamente, recarregue com [`TFPreTrainedModel.from_pretrained`]

```py
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained("./tf_save_pretrained")
```
</tf>
</frameworkcontent>

Um recurso particularmente interessante dos ü§ó Transformers √© a capacidade de salvar um modelo e recarreg√°-lo como um modelo PyTorch ou TensorFlow. Use `from_pt` ou `from_tf` para converter o modelo de um framework para outro:

<frameworkcontent>
<pt>

```py
>>> from transformers import AutoModel

>>> tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
>>> pt_model = AutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)
```
</pt>
<tf>

```py
>>> from transformers import TFAutoModel

>>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)
```
</tf>
</frameworkcontent>