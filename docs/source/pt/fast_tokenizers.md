<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Usando os Tokenizers do ü§ó Tokenizers

O [`PreTrainedTokenizerFast`] depende da biblioteca [ü§ó Tokenizers](https://huggingface.co/docs/tokenizers). O Tokenizer obtido da biblioteca ü§ó Tokenizers pode ser carregado facilmente pelo ü§ó Transformers.

Antes de entrar nos detalhes, vamos come√ßar criando um tokenizer fict√≠cio em algumas linhas:

```python
>>> from tokenizers import Tokenizer
>>> from tokenizers.models import BPE
>>> from tokenizers.trainers import BpeTrainer
>>> from tokenizers.pre_tokenizers import Whitespace

>>> tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
>>> trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])

>>> tokenizer.pre_tokenizer = Whitespace()
>>> files = [...]
>>> tokenizer.train(files, trainer)
```

Agora temos um tokenizer treinado nos arquivos que foram definidos. N√≥s podemos continuar usando nessa execu√ß√£o ou salvar em um arquivo JSON para re-utilizar no futuro.

## Carregando diretamente de um objeto tokenizer

Vamos ver como aproveitar esse objeto tokenizer na biblioteca ü§ó Transformers. A classe [`PreTrainedTokenizerFast`] permite uma instancia√ß√£o f√°cil, aceitando o objeto *tokenizer* instanciado como um argumento:

```python
>>> from transformers import PreTrainedTokenizerFast

>>> fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)
```
Esse objeto pode ser utilizado com todos os m√©todos compartilhados pelos tokenizers dos ü§ó Transformers! V√° para [a p√°gina do tokenizer](main_classes/tokenizer) para mais informa√ß√µes.

## Carregando de um arquivo JSON

Para carregar um tokenizer de um arquivo JSON vamos primeiro come√ßar salvando nosso tokenizer:

```python
>>> tokenizer.save("tokenizer.json")
```

A pasta para qual salvamos esse arquivo pode ser passada para o m√©todo de inicializa√ß√£o do [`PreTrainedTokenizerFast`] usando o `tokenizer_file` par√¢metro:

```python
>>> from transformers import PreTrainedTokenizerFast

>>> fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file="tokenizer.json")
```

Esse objeto pode ser utilizado com todos os m√©todos compartilhados pelos tokenizers dos ü§ó Transformers! V√° para [a p√°gina do tokenizer](main_classes/tokenizer) para mais informa√ß√µes.