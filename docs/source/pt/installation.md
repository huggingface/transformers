<!---
Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Guia de Instala√ß√£o

Neste guia poder√° encontrar informa√ß√µes para a instala√ß√£o do ü§ó Transformers para qualquer biblioteca de
Machine Learning com a qual esteja a trabalhar. Al√©m disso, poder√° encontrar informa√ß√µes sobre como gerar cach√™s e
configurar o ü§ó Transformers para execu√ß√£o em modo offline (opcional).

ü§ó Transformers foi testado com Python 3.6+, PyTorch 1.1.0+, TensorFlow 2.0+, e Flax. Para instalar a biblioteca de
deep learning com que deseja trabalhar, siga as instru√ß√µes correspondentes listadas a seguir:

* [PyTorch](https://pytorch.org/get-started/locally/)
* [TensorFlow 2.0](https://www.tensorflow.org/install/pip)
* [Flax](https://flax.readthedocs.io/en/latest/)

## Instala√ß√£o pelo Pip

√â sugerido instalar o ü§ó Transformers num [ambiente virtual](https://docs.python.org/3/library/venv.html). Se precisar
de mais informa√ß√µes sobre ambientes virtuais em Python, consulte este [guia](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).
Um ambiente virtual facilitar√° a manipula√ß√£o e organiza√ß√£o de projetos e evita problemas de compatibilidade entre depend√™ncias.

Comece criando um ambiente virtual no diret√≥rio do seu projeto:

```bash
python -m venv .env
```

E para ativar o ambiente virtual:

```bash
source .env/bin/activate
```

Agora √â poss√≠vel instalar o ü§ó Transformers com o comando a seguir:

```bash
pip install transformers
```

Somente para a CPU, √© poss√≠vel instalar o ü§ó Transformers e a biblioteca de deep learning respectiva apenas numa linha.

Por exemplo, para instalar o ü§ó Transformers e o PyTorch, digite:

```bash
pip install transformers[torch]
```

ü§ó Transformers e TensorFlow 2.0:

```bash
pip install transformers[tf-cpu]
```

ü§ó Transformers e Flax:

```bash
pip install transformers[flax]
```

Por √∫ltimo, verifique se o ü§ó Transformers foi instalado com sucesso usando o seguinte comando para baixar um modelo pr√©-treinado:

```bash
python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"
```

Em seguida, imprima um r√≥tulo e sua pontua√ß√£o:

```bash
[{'label': 'POSITIVE', 'score': 0.9998704791069031}]
```

## Instala√ß√£o usando a fonte

Para instalar o ü§ó Transformers a partir da fonte use o seguinte comando:

```bash
pip install git+https://github.com/huggingface/transformers
```

O comando acima instalar√° a vers√£o `master` mais atual em vez da √∫ltima vers√£o est√°vel. A vers√£o `master` √© √∫til para
utilizar os √∫ltimos updates contidos em ü§ó Transformers. Por exemplo, um erro recente pode ter sido corrigido somente
ap√≥s a √∫ltima vers√£o est√°vel, antes que houvesse um novo lan√ßamento. No entanto, h√° a possibilidade que a vers√£o `master` n√£o esteja est√°vel.
A equipa trata de mant√©r a vers√£o `master` operacional e a maioria dos erros s√£o resolvidos em poucas horas ou dias.
Se encontrar quaisquer problemas, por favor abra um [Issue](https://github.com/huggingface/transformers/issues) para que o
mesmo possa ser corrigido o mais r√°pido poss√≠vel.

Verifique que o ü§ó Transformers est√° instalado corretamente usando o seguinte comando:

```bash
python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))"
```

## Instala√ß√£o edit√°vel

Uma instala√ß√£o edit√°vel ser√° necess√°ria caso desejas um dos seguintes:
* Usar a vers√£o `master` do c√≥digo fonte.
* Contribuir ao ü§ó Transformers e precisa testar mudan√ßas ao c√≥digo.

Para tal, clone o reposit√≥rio e instale o ü§ó Transformers com os seguintes comandos:

```bash
git clone https://github.com/huggingface/transformers.git
cd transformers
pip install -e .
```

Estes comandos v√£o ligar o diret√≥rio para o qual foi clonado o reposit√≥rio ao caminho de bibliotecas do Python.
O Python agora buscar√° dentro dos arquivos que foram clonados al√©m dos caminhos normais da biblioteca.
Por exemplo, se os pacotes do Python se encontram instalados no caminho `~/anaconda3/envs/main/lib/python3.7/site-packages/`,
o Python tamb√©m buscar√° m√≥dulos no diret√≥rio onde clonamos o reposit√≥rio `~/transformers/`.

<Tip warning={true}>

√â necess√°rio manter o diret√≥rio `transformers` se desejas continuar usando a biblioteca.

</Tip>

Assim, √â poss√≠vel atualizar sua c√≥pia local para com a √∫ltima vers√£o do ü§ó Transformers com o seguinte comando:

```bash
cd ~/transformers/
git pull
```

O ambiente de Python que foi criado para a instala√ß√£o do ü§ó Transformers encontrar√° a vers√£o `master` em execu√ß√µes seguintes.

## Instala√ß√£o usando o Conda

√â poss√≠vel instalar o ü§ó Transformers a partir do canal conda `conda-forge` com o seguinte comando:

```bash
conda install conda-forge::transformers
```

## Configura√ß√£o do Cach√™

Os modelos pr√©-treinados s√£o baixados e armazenados no cach√™ local, encontrado em `~/.cache/huggingface/transformers/`.
Este √© o diret√≥rio padr√£o determinado pela vari√°vel `TRANSFORMERS_CACHE` dentro do shell.
No Windows, este diret√≥rio pr√©-definido √© dado por `C:\Users\username\.cache\huggingface\transformers`.
√â poss√≠vel mudar as vari√°veis dentro do shell em ordem de prioridade para especificar um diret√≥rio de cach√™ diferente:

1. Vari√°vel de ambiente do shell (por padr√£o): `TRANSFORMERS_CACHE`.
2. Vari√°vel de ambiente do shell:`HF_HOME` + `transformers/`.
3. Vari√°vel de ambiente do shell: `XDG_CACHE_HOME` + `/huggingface/transformers`.

<Tip>

    O ü§ó Transformers usar√° as vari√°veis de ambiente do shell `PYTORCH_TRANSFORMERS_CACHE` ou `PYTORCH_PRETRAINED_BERT_CACHE`
    se estiver vindo de uma vers√£o anterior da biblioteca que tenha configurado essas vari√°veis de ambiente, a menos que
    voc√™ especifique a vari√°vel de ambiente do shell `TRANSFORMERS_CACHE`.

</Tip>


## Modo Offline

O ü§ó Transformers tamb√©m pode ser executado num ambiente de firewall ou fora da rede (offline) usando arquivos locais.
Para tal, configure a vari√°vel de ambiente de modo que `TRANSFORMERS_OFFLINE=1`.

<Tip>

Voc√™ pode adicionar o [ü§ó Datasets](https://huggingface.co/docs/datasets/) ao pipeline de treinamento offline declarando
    a vari√°vel de ambiente `HF_DATASETS_OFFLINE=1`.

</Tip>

Segue um exemplo de execu√ß√£o do programa numa rede padr√£o com firewall para inst√¢ncias externas, usando o seguinte comando:

```bash
python examples/pytorch/translation/run_translation.py --model_name_or_path google-t5/t5-small --dataset_name wmt16 --dataset_config ro-en ...
```

Execute esse mesmo programa numa inst√¢ncia offline com o seguinte comando:

```bash
HF_DATASETS_OFFLINE=1 TRANSFORMERS_OFFLINE=1 \
python examples/pytorch/translation/run_translation.py --model_name_or_path google-t5/t5-small --dataset_name wmt16 --dataset_config ro-en ...
```

O script agora deve ser executado sem travar ou expirar, pois procurar√° apenas por arquivos locais.

### Obtendo modelos e tokenizers para uso offline

Outra op√ß√£o para usar o ü§ó Transformers offline √© baixar os arquivos antes e depois apontar para o caminho local onde est√£o localizados. Existem tr√™s maneiras de fazer isso:

* Baixe um arquivo por meio da interface de usu√°rio do [Model Hub](https://huggingface.co/models) clicando no √≠cone ‚Üì.

    ![download-icon](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/download-icon.png)


* Use o pipeline do [`PreTrainedModel.from_pretrained`] e [`PreTrainedModel.save_pretrained`]:
    1. Baixa os arquivos previamente com [`PreTrainedModel.from_pretrained`]:

    ```py
    >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

    >>> tokenizer = AutoTokenizer.from_pretrained("bigscience/T0_3B")
    >>> model = AutoModelForSeq2SeqLM.from_pretrained("bigscience/T0_3B")
    ```


    2. Salve os arquivos em um diret√≥rio espec√≠fico com [`PreTrainedModel.save_pretrained`]:

    ```py
    >>> tokenizer.save_pretrained("./your/path/bigscience_t0")
    >>> model.save_pretrained("./your/path/bigscience_t0")
    ```

    3. Quando estiver offline, acesse os arquivos com [`PreTrainedModel.from_pretrained`] do diret√≥rio especificado:

    ```py
    >>> tokenizer = AutoTokenizer.from_pretrained("./your/path/bigscience_t0")
    >>> model = AutoModel.from_pretrained("./your/path/bigscience_t0")
    ```

* Baixando arquivos programaticamente com a biblioteca [huggingface_hub](https://github.com/huggingface/huggingface_hub/tree/main/src/huggingface_hub):

    1. Instale a biblioteca [huggingface_hub](https://github.com/huggingface/huggingface_hub/tree/main/src/huggingface_hub) em seu ambiente virtual:

    ```bash
    python -m pip install huggingface_hub
    ```

    2. Utiliza a fun√ß√£o [`hf_hub_download`](https://huggingface.co/docs/hub/adding-a-library#download-files-from-the-hub) para baixar um arquivo para um caminho espec√≠fico. Por exemplo, o comando a seguir baixar√° o arquivo `config.json` para o modelo [T0](https://huggingface.co/bigscience/T0_3B) no caminho desejado:

    ```py
    >>> from huggingface_hub import hf_hub_download

    >>> hf_hub_download(repo_id="bigscience/T0_3B", filename="config.json", cache_dir="./your/path/bigscience_t0")
    ```

Depois que o arquivo for baixado e armazenado no cach√™ local, especifique seu caminho local para carreg√°-lo e us√°-lo:

```py
>>> from transformers import AutoConfig

>>> config = AutoConfig.from_pretrained("./your/path/bigscience_t0/config.json")
```

<Tip>

Para obter mais detalhes sobre como baixar arquivos armazenados no Hub, consulte a se√ß√£o [How to download files from the Hub](https://huggingface.co/docs/hub/how-to-downstream).

</Tip>
