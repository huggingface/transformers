<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Exportando modelos para ONNX 

Se voc√™ precisar implantar modelos ü§ó Transformers em ambientes de produ√ß√£o, recomendamos
exporta-los para um formato serializado que pode ser carregado e executado em
tempos de execu√ß√£o e hardware. Neste guia, mostraremos como exportar modelos ü§ó Transformers
para [ONNX (Open Neural Network eXchange)](http://onnx.ai).

<Tip>

Uma vez exportado, um modelo pode ser otimizado para infer√™ncia por meio de t√©cnicas como
quantiza√ß√£o e poda. Se voc√™ estiver interessado em otimizar seus modelos para serem executados com
m√°xima efici√™ncia, confira a biblioteca [ü§ó Optimum
](https://github.com/huggingface/optimum).

</Tip>

ONNX √© um padr√£o aberto que define um conjunto comum de operadores e um formato de arquivo comum
para representar modelos de aprendizado profundo em uma ampla variedade de estruturas, incluindo PyTorch e
TensorFlow. Quando um modelo √© exportado para o formato ONNX, esses operadores s√£o usados para
construir um grafo computacional (muitas vezes chamado de _representa√ß√£o intermedi√°ria_) que
representa o fluxo de dados atrav√©s da rede neural.

Ao expor um grafo com operadores e tipos de dados padronizados, o ONNX facilita a
alternar entre os frameworks. Por exemplo, um modelo treinado em PyTorch pode ser exportado para
formato ONNX e depois importado no TensorFlow (e vice-versa).

ü§ó Transformers fornece um pacote [`transformers.onnx`](main_classes/onnx) que permite
que voc√™ converta os checkpoints do modelo em um grafo ONNX aproveitando os objetos de configura√ß√£o.
Esses objetos de configura√ß√£o v√™m prontos para v√°rias arquiteturas de modelo e s√£o
projetado para ser facilmente extens√≠vel a outras arquiteturas.

As configura√ß√µes prontas incluem as seguintes arquiteturas:

<!--This table is automatically generated by `make fix-copies`, do not fill manually!-->

- ALBERT
- BART
- BEiT
- BERT
- BigBird
- BigBird-Pegasus
- Blenderbot
- BlenderbotSmall
- BLOOM
- CamemBERT
- CLIP
- CodeGen
- Conditional DETR
- ConvBERT
- ConvNeXT
- Data2VecText
- Data2VecVision
- DeBERTa
- DeBERTa-v2
- DeiT
- DETR
- DistilBERT
- ELECTRA
- ERNIE
- FlauBERT
- GPT Neo
- GPT-J
- GroupViT
- I-BERT
- LayoutLM
- LayoutLMv3
- LeViT
- Longformer
- LongT5
- M2M100
- Marian
- mBART
- MobileBERT
- MobileViT
- MT5
- OpenAI GPT-2
- OWL-ViT
- Perceiver
- PLBart
- ResNet
- RoBERTa
- RoFormer
- SegFormer
- SqueezeBERT
- Swin Transformer
- T5
- Table Transformer
- Vision Encoder decoder
- ViT
- XLM
- XLM-RoBERTa
- XLM-RoBERTa-XL
- YOLOS

Nas pr√≥ximas duas se√ß√µes, mostraremos como:

* Exportar um modelo suportado usando o pacote `transformers.onnx`.
* Exportar um modelo personalizado para uma arquitetura sem suporte.

## Exportando um modelo para ONNX

Para exportar um modelo ü§ó Transformers para o ONNX, primeiro voc√™ precisa instalar algumas
depend√™ncias extras:

```bash
pip install transformers[onnx]
```

O pacote `transformers.onnx` pode ent√£o ser usado como um m√≥dulo Python:

```bash
python -m transformers.onnx --help

usage: Hugging Face Transformers ONNX exporter [-h] -m MODEL [--feature {causal-lm, ...}] [--opset OPSET] [--atol ATOL] output

positional arguments:
  output                Path indicating where to store generated ONNX model.

optional arguments:
  -h, --help            show this help message and exit
  -m MODEL, --model MODEL
                        Model ID on huggingface.co or path on disk to load model from.
  --feature {causal-lm, ...}
                        The type of features to export the model with.
  --opset OPSET         ONNX opset version to export the model with.
  --atol ATOL           Absolute difference tolerance when validating the model.
```

A exporta√ß√£o de um checkpoint usando uma configura√ß√£o pronta pode ser feita da seguinte forma:

```bash
python -m transformers.onnx --model=distilbert-base-uncased onnx/
```

Voc√™ deve ver os seguintes logs:

```bash
Validating ONNX model...
        -[‚úì] ONNX model output names match reference model ({'last_hidden_state'})
        - Validating ONNX Model output "last_hidden_state":
                -[‚úì] (2, 8, 768) matches (2, 8, 768)
                -[‚úì] all values close (atol: 1e-05)
All good, model saved at: onnx/model.onnx
```

Isso exporta um grafo ONNX do ponto de verifica√ß√£o definido pelo argumento `--model`. Nisso
Por exemplo, √© `distilbert-base-uncased`, mas pode ser qualquer checkpoint no Hugging
Face Hub ou um armazenado localmente.

O arquivo `model.onnx` resultante pode ser executado em um dos [muitos
aceleradores](https://onnx.ai/supported-tools.html#deployModel) que suportam o ONNX
padr√£o. Por exemplo, podemos carregar e executar o modelo com [ONNX
Tempo de execu√ß√£o](https://onnxruntime.ai/) da seguinte forma:

```python
>>> from transformers import AutoTokenizer
>>> from onnxruntime import InferenceSession

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
>>> session = InferenceSession("onnx/model.onnx")
>>> # ONNX Runtime expects NumPy arrays as input
>>> inputs = tokenizer("Using DistilBERT with ONNX Runtime!", return_tensors="np")
>>> outputs = session.run(output_names=["last_hidden_state"], input_feed=dict(inputs))
```

Os nomes de sa√≠da necess√°rios (como `["last_hidden_state"]`) podem ser obtidos pegando uma
 configura√ß√£o ONNX de cada modelo. Por exemplo, para DistilBERT temos:

```python
>>> from transformers.models.distilbert import DistilBertConfig, DistilBertOnnxConfig

>>> config = DistilBertConfig()
>>> onnx_config = DistilBertOnnxConfig(config)
>>> print(list(onnx_config.outputs.keys()))
["last_hidden_state"]
```

O processo √© id√™ntico para os checkpoints do TensorFlow no Hub. Por exemplo, podemos
exportar um checkpoint TensorFlow puro do [Keras
](https://huggingface.co/keras-io) da seguinte forma:

```bash
python -m transformers.onnx --model=keras-io/transformers-qa onnx/
```

Para exportar um modelo armazenado localmente, voc√™ precisar√° ter os pesos e
arquivos tokenizer armazenados em um diret√≥rio. Por exemplo, podemos carregar e salvar um checkpoint como:

```python
>>> from transformers import AutoTokenizer, AutoModelForSequenceClassification

>>> # Load tokenizer and PyTorch weights form the Hub
>>> tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
>>> pt_model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")
>>> # Save to disk
>>> tokenizer.save_pretrained("local-pt-checkpoint")
>>> pt_model.save_pretrained("local-pt-checkpoint")
```

Uma vez que o checkpoint √© salvo, podemos export√°-lo para o ONNX apontando o `--model`
argumento do pacote `transformers.onnx` para o diret√≥rio desejado:

```bash
python -m transformers.onnx --model=local-pt-checkpoint onnx/
```

```python
>>> from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

>>> # Load tokenizer and TensorFlow weights from the Hub
>>> tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")
>>> # Save to disk
>>> tokenizer.save_pretrained("local-tf-checkpoint")
>>> tf_model.save_pretrained("local-tf-checkpoint")
```

Uma vez que o checkpoint √© salvo, podemos export√°-lo para o ONNX apontando o `--model`
argumento do pacote `transformers.onnx` para o diret√≥rio desejado:

```bash
python -m transformers.onnx --model=local-tf-checkpoint onnx/
```

## Selecionando features para diferentes tarefas do modelo

Cada configura√ß√£o pronta vem com um conjunto de _features_ que permitem exportar
modelos para diferentes tipos de tarefas. Conforme mostrado na tabela abaixo, cada recurso √©
associado a uma `AutoClass` diferente:

| Feature                              | Auto Class                           |
| ------------------------------------ | ------------------------------------ |
| `causal-lm`, `causal-lm-with-past`   | `AutoModelForCausalLM`               |
| `default`, `default-with-past`       | `AutoModel`                          |
| `masked-lm`                          | `AutoModelForMaskedLM`               |
| `question-answering`                 | `AutoModelForQuestionAnswering`      |
| `seq2seq-lm`, `seq2seq-lm-with-past` | `AutoModelForSeq2SeqLM`              |
| `sequence-classification`            | `AutoModelForSequenceClassification` |
| `token-classification`               | `AutoModelForTokenClassification`    |

Para cada configura√ß√£o, voc√™ pode encontrar a lista de recursos suportados por meio do
[`~transformers.onnx.FeaturesManager`]. Por exemplo, para DistilBERT temos:

```python
>>> from transformers.onnx.features import FeaturesManager

>>> distilbert_features = list(FeaturesManager.get_supported_features_for_model_type("distilbert").keys())
>>> print(distilbert_features)
["default", "masked-lm", "causal-lm", "sequence-classification", "token-classification", "question-answering"]
```

Voc√™ pode ent√£o passar um desses recursos para o argumento `--feature` no
pacote `transformers.onnx`. Por exemplo, para exportar um modelo de classifica√ß√£o de texto, podemos
escolher um modelo ajustado no Hub e executar:

```bash
python -m transformers.onnx --model=distilbert-base-uncased-finetuned-sst-2-english \
                            --feature=sequence-classification onnx/
```

Isso exibe os seguintes logs:

```bash
Validating ONNX model...
        -[‚úì] ONNX model output names match reference model ({'logits'})
        - Validating ONNX Model output "logits":
                -[‚úì] (2, 2) matches (2, 2)
                -[‚úì] all values close (atol: 1e-05)
All good, model saved at: onnx/model.onnx
```

Observe que, neste caso, os nomes de sa√≠da do modelo ajustado s√£o `logits`
em vez do `last_hidden_state` que vimos com o checkpoint `distilbert-base-uncased`
mais cedo. Isso √© esperado, pois o modelo ajustado (fine-tuned) possui uma cabe√ßa de classifica√ß√£o de sequ√™ncia.

<Tip>

Os recursos que t√™m um sufixo `with-pass` (como `causal-lm-with-pass`) correspondem a
classes de modelo com estados ocultos pr√©-computados (chave e valores nos blocos de aten√ß√£o)
que pode ser usado para decodifica√ß√£o autorregressiva r√°pida.

</Tip>

<Tip>

Para modelos do tipo `VisionEncoderDecoder`, as partes do codificador e do decodificador s√£o
exportados separadamente como dois arquivos ONNX chamados `encoder_model.onnx` e `decoder_model.onnx` respectivamente.

</Tip>

## Exportando um modelo para uma arquitetura sem suporte

Se voc√™ deseja exportar um modelo cuja arquitetura n√£o √© suportada nativamente pela
biblioteca, h√° tr√™s etapas principais a seguir:

1. Implemente uma configura√ß√£o ONNX personalizada.
2. Exporte o modelo para o ONNX.
3. Valide as sa√≠das do PyTorch e dos modelos exportados.

Nesta se√ß√£o, veremos como o DistilBERT foi implementado para mostrar o que est√° envolvido
em cada passo.

### Implementando uma configura√ß√£o ONNX personalizada

Vamos come√ßar com o objeto de configura√ß√£o ONNX. Fornecemos tr√™s classes abstratas que
voc√™ deve herdar, dependendo do tipo de arquitetura de modelo que deseja exportar:

* Modelos baseados em codificador herdam de [`~onnx.config.OnnxConfig`]
* Modelos baseados em decodificador herdam de [`~onnx.config.OnnxConfigWithPast`]
* Os modelos codificador-decodificador herdam de [`~onnx.config.OnnxSeq2SeqConfigWithPast`]

<Tip>

Uma boa maneira de implementar uma configura√ß√£o ONNX personalizada √© observar as
implementa√ß√£o no arquivo `configuration_<model_name>.py` de uma arquitetura semelhante.

</Tip>

Como o DistilBERT √© um modelo baseado em codificador, sua configura√ß√£o √© herdada de
`OnnxConfig`:

```python
>>> from typing import Mapping, OrderedDict
>>> from transformers.onnx import OnnxConfig


>>> class DistilBertOnnxConfig(OnnxConfig):
...     @property
...     def inputs(self) -> Mapping[str, Mapping[int, str]]:
...         return OrderedDict(
...             [
...                 ("input_ids", {0: "batch", 1: "sequence"}),
...                 ("attention_mask", {0: "batch", 1: "sequence"}),
...             ]
...         )
```

Todo objeto de configura√ß√£o deve implementar a propriedade `inputs` e retornar um mapeamento,
onde cada chave corresponde a uma entrada esperada e cada valor indica o eixo 
dessa entrada. Para o DistilBERT, podemos ver que duas entradas s√£o necess√°rias: `input_ids` e
`attention_mask`. Essas entradas t√™m a mesma forma de `(batch_size, sequence_length)`
√© por isso que vemos os mesmos eixos usados na configura√ß√£o.

<Tip>

Notice that `inputs` property for `DistilBertOnnxConfig` returns an `OrderedDict`. This
ensures that the inputs are matched with their relative position within the
`PreTrainedModel.forward()` method when tracing the graph. We recommend using an
`OrderedDict` for the `inputs` and `outputs` properties when implementing custom ONNX
configurations.

Observe que a propriedade `inputs` para `DistilBertOnnxConfig` retorna um `OrderedDict`. Este
garante que as entradas sejam combinadas com sua posi√ß√£o relativa dentro do
m√©todo `PreTrainedModel.forward()` ao tra√ßar o grafo. Recomendamos o uso de um
`OrderedDict` para as propriedades `inputs` e `outputs` ao implementar configura√ß√µes personalizadas ONNX.

</Tip>

Depois de implementar uma configura√ß√£o ONNX, voc√™ pode instanci√°-la fornecendo a
configura√ß√£o do modelo base da seguinte forma:

```python
>>> from transformers import AutoConfig

>>> config = AutoConfig.from_pretrained("distilbert-base-uncased")
>>> onnx_config = DistilBertOnnxConfig(config)
```

O objeto resultante tem v√°rias propriedades √∫teis. Por exemplo, voc√™ pode visualizar o conjunto de operadores ONNX
 que ser√° usado durante a exporta√ß√£o:

```python
>>> print(onnx_config.default_onnx_opset)
11
```

Voc√™ tamb√©m pode visualizar as sa√≠das associadas ao modelo da seguinte forma:

```python
>>> print(onnx_config.outputs)
OrderedDict([("last_hidden_state", {0: "batch", 1: "sequence"})])
```

Observe que a propriedade outputs segue a mesma estrutura das entradas; ele retorna um
`OrderedDict` de sa√≠das nomeadas e suas formas. A estrutura de sa√≠da est√° ligada a
escolha do recurso com o qual a configura√ß√£o √© inicializada. Por padr√£o, a configura√ß√£o do ONNX
√© inicializada com o recurso `default` que corresponde √† exporta√ß√£o de um
modelo carregado com a classe `AutoModel`. Se voc√™ deseja exportar um modelo para outra tarefa,
apenas forne√ßa um recurso diferente para o argumento `task` quando voc√™ inicializar a configura√ß√£o ONNX
. Por exemplo, se quisermos exportar o DistilBERT com uma sequ√™ncia
de classifica√ß√£o, poder√≠amos usar:

```python
>>> from transformers import AutoConfig

>>> config = AutoConfig.from_pretrained("distilbert-base-uncased")
>>> onnx_config_for_seq_clf = DistilBertOnnxConfig(config, task="sequence-classification")
>>> print(onnx_config_for_seq_clf.outputs)
OrderedDict([('logits', {0: 'batch'})])
```

<Tip>

Todas as propriedades e m√©todos b√°sicos associados a [`~onnx.config.OnnxConfig`] e
as outras classes de configura√ß√£o podem ser substitu√≠das se necess√°rio. Confira [`BartOnnxConfig`]
para um exemplo avan√ßado.

</Tip>

### Exportando um modelo

Depois de ter implementado a configura√ß√£o do ONNX, o pr√≥ximo passo √© exportar o modelo.
Aqui podemos usar a fun√ß√£o `export()` fornecida pelo pacote `transformers.onnx`.
Esta fun√ß√£o espera a configura√ß√£o do ONNX, juntamente com o modelo base e o tokenizer,
e o caminho para salvar o arquivo exportado:

```python
>>> from pathlib import Path
>>> from transformers.onnx import export
>>> from transformers import AutoTokenizer, AutoModel

>>> onnx_path = Path("model.onnx")
>>> model_ckpt = "distilbert-base-uncased"
>>> base_model = AutoModel.from_pretrained(model_ckpt)
>>> tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

>>> onnx_inputs, onnx_outputs = export(tokenizer, base_model, onnx_config, onnx_config.default_onnx_opset, onnx_path)
```

Os `onnx_inputs` e `onnx_outputs` retornados pela fun√ß√£o `export()` s√£o listas de
 chaves definidas nas propriedades `inputs` e `outputs` da configura√ß√£o. Uma vez que o
modelo √© exportado, voc√™ pode testar se o modelo est√° bem formado da seguinte forma:

```python
>>> import onnx

>>> onnx_model = onnx.load("model.onnx")
>>> onnx.checker.check_model(onnx_model)
```

<Tip>

Se o seu modelo for maior que 2GB, voc√™ ver√° que muitos arquivos adicionais s√£o criados
durante a exporta√ß√£o. Isso √© _esperado_ porque o ONNX usa [Protocol
Buffers](https://developers.google.com/protocol-buffers/) para armazenar o modelo e estes
t√™m um limite de tamanho de 2GB. Veja a [ONNX
documenta√ß√£o](https://github.com/onnx/onnx/blob/master/docs/ExternalData.md) para
instru√ß√µes sobre como carregar modelos com dados externos.

</Tip>

### Validando a sa√≠da dos modelos

A etapa final √© validar se as sa√≠das do modelo base e exportado concordam
dentro de alguma toler√¢ncia absoluta. Aqui podemos usar a fun√ß√£o `validate_model_outputs()`
fornecida pelo pacote `transformers.onnx` da seguinte forma:

```python
>>> from transformers.onnx import validate_model_outputs

>>> validate_model_outputs(
...     onnx_config, tokenizer, base_model, onnx_path, onnx_outputs, onnx_config.atol_for_validation
... )
```

Esta fun√ß√£o usa o m√©todo [`~transformers.onnx.OnnxConfig.generate_dummy_inputs`] para
gerar entradas para o modelo base e o exportado, e a toler√¢ncia absoluta pode ser
definida na configura√ß√£o. Geralmente encontramos concord√¢ncia num√©rica em 1e-6 a 1e-4
de alcance, embora qualquer coisa menor que 1e-3 provavelmente esteja OK.

## Contribuindo com uma nova configura√ß√£o para ü§ó Transformers

Estamos procurando expandir o conjunto de configura√ß√µes prontas e receber contribui√ß√µes
da comunidade! Se voc√™ gostaria de contribuir para a biblioteca, voc√™
precisar√°:

* Implemente a configura√ß√£o do ONNX no arquivo `configuration_<model_name>.py` correspondente
Arquivo
* Incluir a arquitetura do modelo e recursos correspondentes em
  [`~onnx.features.FeatureManager`]
* Adicione sua arquitetura de modelo aos testes em `test_onnx_v2.py`

Confira como ficou a configura√ß√£o do [IBERT
](https://github.com/huggingface/transformers/pull/14868/files) para obter uma
id√©ia do que est√° envolvido.
