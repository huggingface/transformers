# ุงุณุชุฑุงุชูุฌูุงุช ุชูููุฏ ุงููุต

ูุนุฏ ุชูููุฏ ุงููุต ุฃูุฑูุง ุฃุณุงุณููุง ููุนุฏูุฏ ูู ููุงู ูุนุงูุฌุฉ ุงููุบุฉ ุงูุทุจูุนูุฉุ ูุซู ุชูููุฏ ุงููุต ุงูููุชูุญุ ูุงูุชูุฎูุตุ ูุงูุชุฑุฌูุฉุ ูุฃูุซุฑ ูู ุฐูู. ููุง ููุนุจ ุฏูุฑูุง ูู ูุฌููุนุฉ ูุชููุนุฉ ูู ุชุทุจููุงุช ุงูุทุฑุงุฆู ุงููุฎุชูุทุฉ ุงูุชู ูููู ุงููุต ูููุง ูุฅุฎุฑุงุฌ ูุซู ุชุญููู ุงูููุงู ุฅูู ูุตุ ูุงูุชุญููู ูู ุฑุคูุฉ ุฅูู ูุต. ุจุนุถ ุงูููุงุฐุฌ ุงูุชู ูููููุง ุชูููุฏ ุงููุต ุชุดูู GPT2ุ ูXLNetุ ูOpenAI GPTุ ูCTRLุ ูTransformerXLุ ูXLMุ ูBartุ ูT5ุ ูGITุ ูWhisper.

ุชููุฏ ุจุนุถ ุงูุฃูุซูุฉ ุงูุชู ุชุณุชุฎุฏู ุทุฑููุฉ [~generation.GenerationMixin.generate] ูุฅูุชุงุฌ ูุฎุฑุฌุงุช ูุตูุฉ ูููุงู ูุฎุชููุฉ:

- [ุชูุฎูุต ุงููุต](./tasks/summarization#inference)
- [ูุถุน ุนููุงู ููุตูุฑุฉ](./model_doc/git#transformers.GitForCausalLM.forward.example)
- [ูุณุฎ ุงูุตูุช](./model_doc/whisper#transformers.WhisperForConditionalGeneration.forward.example)

ูุงุญุธ ุฃู ุงููุฏุฎูุงุช ูุทุฑููุฉ ุงูุชูููุฏ ุชุนุชูุฏ ุนูู ุทุฑููุฉ ุงููููุฐุฌ. ูุชู ุฅุฑุฌุงุนูุง ุจูุงุณุทุฉ ูุฆุฉ ุงููุนุงูุฌุฉ ุงููุณุจูุฉ ูููููุฐุฌุ ูุซู AutoTokenizer ุฃู AutoProcessor. ุฅุฐุง ุฃูุดุฃุช ูุนุงูุฌุฉ ูุณุจูุฉ ูููููุฐุฌ ุฃูุซุฑ ูู ููุน ูุงุญุฏ ูู ุงูุฅุฏุฎุงูุ ููู ุจุชูุฑูุฑ ุฌููุน ุงูุฅุฏุฎุงูุงุช ุฅูู generate(). ููููู ูุนุฑูุฉ ุงููุฒูุฏ ุญูู ูุนุงูุฌุฉ ูุณุจูุฉ ูุฑุฏูุฉ ูููููุฐุฌ ูู ูุซุงุฆู ุงููููุฐุฌ ุงูููุงุจูุฉ.

ุชูุนุฑู ุนูููุฉ ุงุฎุชูุงุฑ ุงูุฑููุฒ ุงููููุฒุฉ ููุฅุฎุฑุงุฌ ูุชูููุฏ ุงููุต ุจุงุณู ูู ุงูุชุดููุฑุ ูููููู ุชุฎุตูุต ุงุณุชุฑุงุชูุฌูุฉ ูู ุงูุชุดููุฑ ุงูุชู ุณุชุณุชุฎุฏููุง ุทุฑููุฉ `generate()`. ูุง ูุคุฏู ุชุนุฏูู ุงุณุชุฑุงุชูุฌูุฉ ูู ุงูุชุดููุฑ ุฅูู ุชุบููุฑ ููู ุฃู ูุนููุงุช ูุงุจูุฉ ููุชุฏุฑูุจ. ููุน ุฐููุ ูููู ุฃู ูููู ูู ุชุฃุซูุฑ ููุญูุธ ุนูู ุฌูุฏุฉ ุงูุฅุฎุฑุงุฌ ุงููููุฏ. ูููู ุฃู ูุณุงุนุฏ ูู ุชูููู ุงูุชูุฑุงุฑ ูู ุงููุต ูุฌุนูู ุฃูุซุฑ ุชูุงุณููุง.

ูุตู ูุฐุง ุงูุฏููู ูุง ููู:

- ุชูููู ุงูุชูููุฏ ุงูุงูุชุฑุงุถู
- ุงุณุชุฑุงุชูุฌูุงุช ูู ุงูุชุดููุฑ ุงูุดุงุฆุนุฉ ูุจุงุฑุงูุชุฑุงุชูุง ุงูุฑุฆูุณูุฉ
- ุญูุธ ููุดุงุฑูุฉ ุชููููุงุช ุงูุชูููุฏ ุงููุฎุตุตุฉ ูุน ูููุฐุฌ ุงูุชุฏุฑูุจ ุงูุฏููู ุงูุฎุงุต ุจู ุนูู ๐ค Hub

## ุชูููู ุงูุชูููุฏ ุงูุงูุชุฑุงุถู ูููุต

ุชุชู ุชุนุฑูู ุงุณุชุฑุงุชูุฌูุฉ ูู ุงูุชุดููุฑ ููููุฐุฌ ูู ุชูููู ุงูุชูููุฏ ุงูุฎุงุต ุจู. ุนูุฏ ุงุณุชุฎุฏุงู ุงูููุงุฐุฌ ุงูููุฏุฑุจุฉ ูุณุจููุง ููุงุณุชูุชุงุฌ ุฏุงุฎู [`pipeline`]ุ ุชููู ุงูููุงุฐุฌ ุจุงุณุชุฏุนุงุก ุทุฑููุฉ `PreTrainedModel.generate()` ุงูุชู ุชุทุจู ุชูููู ุงูุชูููุฏ ุงูุงูุชุฑุงุถู ุชุญุช ุงูุบุทุงุก. ูุชู ุฃูุถูุง ุงุณุชุฎุฏุงู ุงูุชูููู ุงูุงูุชุฑุงุถู ุนูุฏูุง ูุง ูุชู ุญูุธ ุฃู ุชูููู ูุฎุตุต ูุน ุงููููุฐุฌ.

ุนูุฏูุง ุชููู ุจุชุญููู ูููุฐุฌ ุจุดูู ุตุฑูุญุ ููููู ูุญุต ุชูููู ุงูุชูููุฏ ุงูุฐู ูุฃุชู ูุนู ูู ุฎูุงู `model.generation_config`:

```python
>>> from transformers import AutoModelForCausalLM

>>> model = AutoModelForCausalLM.from_pretrained("distilbert/distilgpt2")
>>> model.generation_config
GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}
<BLANKLINE>
```

ููุดู ุทุจุงุนุฉ `model.generation_config` ููุท ุนู ุงูููู ุงูุชู ุชุฎุชูู ุนู ุชูููู ุงูุชูููุฏ ุงูุงูุชุฑุงุถูุ ููุง ูุฏุฑุฌ ุฃููุง ูู ุงูููู ุงูุงูุชุฑุงุถูุฉ.

ููุชุตุฑ ุชูููู ุงูุชูููุฏ ุงูุงูุชุฑุงุถู ุนูู ุญุฌู ุงูุฅุฎุฑุงุฌ ุงููุฏูุฌ ูุน ููุฌู ุงูุฅุฏุฎุงู ุฅูู ุญุฏ ุฃูุตู 20 ุฑูุฒูุง ูุชุฌูุจ ููุงุฌูุฉ ูููุฏ ุงูููุงุฑุฏ. ุงุณุชุฑุงุชูุฌูุฉ ูู ุงูุชุดููุฑ ุงูุงูุชุฑุงุถูุฉ ูู ุงูุจุญุซ ุงูุฌุดุนุ ูุงูุชู ุชุนุฏ ุฃุจุณุท ุงุณุชุฑุงุชูุฌูุฉ ูู ุชุดููุฑ ุชุฎุชุงุฑ ุฑูุฒูุง ูููุฒูุง ุจู ุฃุนูู ุงุญุชูุงู ูุฑูุฒ ูููุฒ ุงูุชุงูู. ุจุงููุณุจุฉ ููุนุฏูุฏ ูู ุงูููุงู ูุฃุญุฌุงู ุงูุฅุฎุฑุงุฌ ุงูุตุบูุฑุฉุ ูุนูู ูุฐุง ุจุดูู ุฌูุฏ. ููุน ุฐููุ ุนูุฏูุง ูุชู ุงุณุชุฎุฏุงูู ูุชูููุฏ ูุฎุฑุฌุงุช ุฃุทููุ ูููู ุฃู ูุจุฏุฃ ุงูุจุญุซ ุงูุฌุดุน ูู ุฅูุชุงุฌ ูุชุงุฆุฌ ูุชูุฑุฑุฉ ููุบุงูุฉ.

## ุชุฎุตูุต ุชูููุฏ ุงููุต

ููููู ุชุฌุงูุฒ ุฃู `generation_config` ุนู ุทุฑูู ุชูุฑูุฑ ุงูุจุงุฑุงูุชุฑุงุช ูููููุง ูุจุงุดุฑุฉู ุฅูู ุทุฑููุฉ [`generate`]:

```python
>>> my_model.generate(**inputs, num_beams=4, do_sample=True)  # doctest: +SKIP
```

ุญุชู ุฅุฐุง ูุงูุช ุงุณุชุฑุงุชูุฌูุฉ ูู ุงูุชุดููุฑ ุงูุงูุชุฑุงุถูุฉ ุชุนูู ุจุดูู ุฃุณุงุณู ููููุชูุ ููุง ูุฒุงู ุจุฅููุงูู ุถุจุท ุจุนุถ ุงูุฃุดูุงุก. ุจุนุถ ุงูุจุงุฑุงูุชุฑุงุช ุงูุชู ูุชู ุถุจุทูุง ุจุดูู ุดุงุฆุน ุชุดูู:

- `max_new_tokens`: ุงูุนุฏุฏ ุงูุฃูุตู ูู ุงูุฑููุฒ ุงููููุฒุฉ ุงูุชู ุณูุชู ุชูููุฏูุง. ูุจุนุจุงุฑุฉ ุฃุฎุฑูุ ุญุฌู ุชุณูุณู ุงูุฅุฎุฑุงุฌุ ูููุณ ุจูุง ูู ุฐูู ุงูุฑููุฒ ุงููููุฒุฉ ูู ุงูููุฌู. ูุจุฏูู ูุงุณุชุฎุฏุงู ุทูู ุงูุฅุฎุฑุงุฌ ููุนูุงุฑ ุฅููุงูุ ููููู ุงุฎุชูุงุฑ ุฅููุงู ุงูุชูููุฏ ูู ุฃู ููุช ูุชุฌุงูุฒ ููู ุงูุชูููุฏ ุงููุงูู ููุฏุงุฑูุง ูุนูููุง ูู ุงูููุช. ููุนุฑูุฉ ุงููุฒูุฏุ ุชุญูู ูู [`StoppingCriteria`].
- `num_beams`: ูู ุฎูุงู ุชุญุฏูุฏ ุนุฏุฏ ุงูุญุฒู ุฃูุจุฑ ูู 1ุ ูุฃูุช ุชููู ุจุดูู ูุนุงู ุจุงูุชุจุฏูู ูู ุงูุจุญุซ ุงูุฌุดุน ุฅูู ุงูุจุญุซ ุงูุดุนุงุนู. ุชูููู ูุฐู ุงูุงุณุชุฑุงุชูุฌูุฉ ุงูุนุฏูุฏ ูู ุงููุฑุถูุงุช ูู ูู ุฎุทูุฉ ุฒูููุฉ ูุชุฎุชุงุฑ ูู ุงูููุงูุฉ ุงููุฑุถูุฉ ุงูุชู ููุง ุฃุนูู ุงุญุชูุงู ุฅุฌูุงูู ููุชุณูุณู ุจุฃูููู. ุชุชูุซู ููุฒุฉ ูุฐู ุงูุงุณุชุฑุงุชูุฌูุฉ ูู ุชุญุฏูุฏ ุชุณูุณูุงุช ุนุงููุฉ ุงูุงุญุชูุงู ุชุจุฏุฃ ุจุฑููุฒ ูููุฒุฉ ุฃูููุฉ ููุฎูุถุฉ ุงูุงุญุชูุงู ูุงูุชู ุณุชุชุฌุงูููุง ุงูุจุญุซ ุงูุฌุดุน. ูู ุจุชุตูุฑ ููููุฉ ุนููู [ููุง](https://huggingface.co/spaces/m-ric/beam_search_visualizer).
- `do_sample`: ุฅุฐุง ุชู ุชุนูููู ุนูู `True`ุ ูุฅู ูุฐุง ุงูุจุงุฑุงูุชุฑ ููููู ุงุณุชุฑุงุชูุฌูุงุช ูู ุงูุชุดููุฑ ูุซู ุฃุฎุฐ ุงูุนููุงุช ูุชุนุฏุฏุฉ ุงูุญุฏูุฏุ ูุงูุจุญุซ ุงูุดุนุงุนู ูุชุนุฏุฏ ุงูุญุฏูุฏุ ูุฃุฎุฐ ุงูุนููุงุช ุงูุฃุนูู Kุ ูุฃุฎุฐ ุงูุนููุงุช ุงูุฃุนูู p. ุชููู ุฌููุน ูุฐู ุงูุงุณุชุฑุงุชูุฌูุงุช ุจุชุญุฏูุฏ ุงูุฑูุฒ ุงููููุฒ ุงูุชุงูู ูู ุชูุฒูุน ุงูุงุญุชูุงููุฉ ุนุจุฑ ุงูููุฑุฏุงุช ุจุฃููููุง ูุน ุชุนุฏููุงุช ูุญุฏุฏุฉ ููุงุณุชุฑุงุชูุฌูุฉ.
- `num_return_sequences`: ุนุฏุฏ ุชุณูุณูุงุช ุงููุฑุดุญูู ุงูุชู ุณูุชู ุฅุฑุฌุงุนูุง ููู ุฅุฏุฎุงู. ูุฐุง ุงูุฎูุงุฑ ูุชุงุญ ููุท ูุงุณุชุฑุงุชูุฌูุงุช ูู ุงูุชุดููุฑ ุงูุชู ุชุฏุนู ุนุฏุฉ ุชุณูุณูุงุช ูุฑุดุญุฉุ ุนูู ุณุจูู ุงููุซุงูุ ุงุฎุชูุงูุงุช ุงูุจุญุซ ุงูุดุนุงุนู ูุฃุฎุฐ ุงูุนููุงุช. ุชุนูุฏ ุงุณุชุฑุงุชูุฌูุงุช ูู ุงูุชุดููุฑ ูุซู ุงูุจุญุซ ุงูุฌุดุน ูุงูุจุญุซ ุงูุชุจุงููู ุชุณูุณู ุฅุฎุฑุงุฌ ูุงุญุฏ.

## ุญูุธ ุงุณุชุฑุงุชูุฌูุฉ ูู ุชุดููุฑ ูุฎุตุตุฉ ูุน ูููุฐุฌ

ุฅุฐุง ููุช ุชุฑุบุจ ูู ูุดุงุฑูุฉ ูููุฐุฌ ุงูุชุฏุฑูุจ ุงูุฏููู ุงูุฎุงุต ุจู ุจุชูููู ุชูููุฏ ูุญุฏุฏุ ูููููู:

- ุฅูุดุงุก ูุซูู ููุฆุฉ [`GenerationConfig`]
- ุชุญุฏูุฏ ุจุงุฑุงูุชุฑุงุช ุงุณุชุฑุงุชูุฌูุฉ ูู ุงูุชุดููุฑ
- ุญูุธ ุชูููู ุงูุชูููุฏ ุงูุฎุงุต ุจู ุจุงุณุชุฎุฏุงู [`GenerationConfig.save_pretrained`]ุ ูุงูุชุฃูุฏ ูู ุชุฑู ุญุฌุชู `config_file_name` ูุงุฑุบุฉ
- ูู ุจุชุนููู `push_to_hub` ุฅูู `True` ูุชุญููู ุชููููู ุฅูู ูุณุชูุฏุน ุงููููุฐุฌ

```python
>>> from transformers import AutoModelForCausalLM, GenerationConfig

>>> model = AutoModelForCausalLM.from_pretrained("my_account/my_model")  # doctest: +SKIP
>>> generation_config = GenerationConfig(
...     max_new_tokens=50, do_sample=True, top_k=50, eos_token_id=model.config.eos_token_id
... )
>>> generation_config.save_pretrained("my_account/my_model", push_to_hub=True)  # doctest: +SKIP
```

ููููู ุฃูุถูุง ุชุฎุฒูู ุงูุนุฏูุฏ ูู ุชููููุงุช ุงูุชูููุฏ ูู ุฏููู ูุงุญุฏุ ุจุงุณุชุฎุฏุงู ุญุฌุฉ `config_file_name` ูู [`GenerationConfig.save_pretrained`]. ููููู ูุงุญููุง ุงุณุชุฏุนุงุก ูุซูู ููุง ุจุงุณุชุฎุฏุงู [`GenerationConfig.from_pretrained`]. ูุฐุง ูููุฏ ุฅุฐุง ููุช ุชุฑูุฏ ุชุฎุฒูู ุงูุนุฏูุฏ ูู ุชููููุงุช ุงูุชูููุฏ ููููุฐุฌ ูุงุญุฏ (ุนูู ุณุจูู ุงููุซุงูุ ูุงุญุฏ ูุชูููุฏ ูุต ุฅุจุฏุงุนู ูุน ุฃุฎุฐ ุงูุนููุงุชุ ููุงุญุฏ ููุชูุฎูุต ุจุงุณุชุฎุฏุงู ุงูุจุญุซ ุงูุดุนุงุนู). ูุฌุจ ุฃู ูููู ูุฏูู ุงูุฃุฐููุงุช ุงูุตุญูุญุฉ ุนูู Hub ูุฅุถุงูุฉ ูููุงุช ุชูููู ุฅูู ูููุฐุฌ.

```python
>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig

>>> tokenizer = AutoTokenizer.from_pretrained("google-t5/t5-small")
>>> model = AutoModelForSeq2SeqLM.from_pretrained("google-t5/t5-small")

>>> translation_generation_config = GenerationConfig(
...     num_beams=4,
...     early_stopping=True,
...     decoder_start_token_id=0,
...     eos_token_id=model.config.eos_token_id,
...     pad_token=model.config.pad_token_id,
... )

>>> # Tip: add `push_to_hub=True` to push to the Hub
>>> translation_generation_config.save_pretrained("/tmp", "translation_generation_config.json")
>>> # Tip: add `push_to_hub=True` to push to the Hub
>>> translation_generation_config.save_pretrained("/tmp", "translation_generation_config.json")

>>> # You could then use the named generation config file to parameterize generation
>>> generation_config = GenerationConfig.from_pretrained("/tmp", "translation_generation_config.json")
>>> inputs = tokenizer("translate English to French: Configuration files are easy to use!", return_tensors="pt")
>>> outputs = model.generate(**inputs, generation_config=generation_config)
>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
['Les fichiers de configuration sont faciles ร utiliser!']
```

## ุงูุจุซ

ุชุฏุนู ุทุฑููุฉ `generate()` ุงูุจุซุ ูู ุฎูุงู ุฅุฏุฎุงููุง `streamer`. ูุชูุงูู ุฅุฏุฎุงู `streamer` ูุน ุฃู ูุซูู ูู ูุฆุฉ ุจูุง ุงูุทุฑู ุงูุชุงููุฉ: `put()` ู`end()`. ุฏุงุฎูููุงุ ูุชู ุงุณุชุฎุฏุงู `put()` ูุฏูุน ุงูุฑููุฒ ุงููููุฒุฉ ุงูุฌุฏูุฏุฉ ู`end()` ููุฅุดุงุฑุฉ ุฅูู ููุงูุฉ ุชูููุฏ ุงููุต.

<Tip warning={true}>

ูุง ูุฒุงู API ููุฆุงุช ุงูุจุซ ููุฏ ุงูุชุทููุฑ ููุฏ ูุชุบูุฑ ูู ุงููุณุชูุจู.

</Tip>

ูู ุงููุงุญูุฉ ุงูุนูููุฉุ ููููู ุฅูุดุงุก ูุฆุฉ ุจุซ ูุฎุตุตุฉ ูุฌููุน ุฃููุงุน ุงูุฃุบุฑุงุถ! ูุฏููุง ุฃูุถูุง ูุฆุงุช ุจุซ ุฃุณุงุณูุฉ ุฌุงูุฒุฉ ููุงุณุชุฎุฏุงู. ุนูู ุณุจูู ุงููุซุงูุ ููููู ุงุณุชุฎุฏุงู ูุฆุฉ [`TextStreamer`] ูุจุซ ุฅุฎุฑุงุฌ `generate()` ุฅูู ุดุงุดุชูุ ูููุฉ ูุงุญุฏุฉ ูู ูู ูุฑุฉ:

```python
>>> from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer

>>> tok = AutoTokenizer.from_pretrained("openai-community/gpt2")
>>> model = AutoModelForCausalLM.from_pretrained("openai-community/gpt2")
>>> inputs = tok(["An increasing sequence: one,"], return_tensors="pt")
>>> streamer = TextStreamer(tok)

>>> # Despite returning the usual output, the streamer will also print the generated text to stdout.
>>> _ = model.generate(**inputs, streamer=streamer, max_new_tokens=20)
An increasing sequence: one, two, three, four, five, six, seven, eight, nine, ten, eleven,
```

## ุชูููู ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ููููุงุชูุญ ูุงูููู

ุชุฏุนู ุทุฑููุฉ `generate()` ุชุฎุฒูู ููุงุชูุญ ูููู ุงูููุงุชูุญ ูุงูููู ุงููุคูุชุฉ ูุชุนุฒูุฒ ุงูููุงุกุฉ ูุชุฌูุจ ุฅุนุงุฏุฉ ุงูุญุณุงุจุงุช. ููุน ุฐููุ ูููู ุฃู ุชุดุบู ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ููููุงุชูุญ ูุงูููู ุฌุฒุกูุง ูุจูุฑูุง ูู ุงูุฐุงูุฑุฉุ ููุง ูุตุจุญ ุนูู ุฒุฌุงุฌุฉ ูุชูููุฏ ุงูุณูุงู ุงูุทูููุ ุฎุงุตุฉ ุจุงููุณุจุฉ ููููุงุฐุฌ ุงููุบููุฉ ูุจูุฑุฉ ุงูุญุฌู.

ูููู ุฃู ูุคุฏู ุชูููู ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ููููุงุชูุญ ูุงูููู ุนูุฏ ุงุณุชุฎุฏุงู `generate()` ุฅูู ุชูููู ูุชุทูุจุงุช ุงูุฐุงูุฑุฉ ุจุดูู ูุจูุฑ ุนูู ุญุณุงุจ ุงูุณุฑุนุฉ.

ูุณุชููู ุชูููู ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ููููุงุชูุญ ูุงูููู ูู `transformers` ุฅูู ุญุฏ ูุจูุฑ ูู ุงููุฑูุฉ [KIVI: Quantization Asymmetric 2bit Quantization for KV Cache] (https://arxiv.org/abs/2402.02750) ููุฏุนู ุญุงูููุง `quanto` ู`HQQ` ูุฎูููุงุช. ููุฒูุฏ ูู ุงููุนูููุงุช ุญูู ุทุฑููุฉ ุงูุนูู ุงูุฏุงุฎููุฉุ ุฑุงุฌุน ุงููุฑูุฉ.

ูุชูููู ุชูููู ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ููููุงุชูุญ ูุงููููุ ูุฌุจ ุงูุฅุดุงุฑุฉ ุฅูู `cache_implementation="quantized"` ูู `generation_config`. ูุฌุจ ุชูุฑูุฑ ุงูุญุฌุฌ ุงููุชุนููุฉ ุจุงูุชูููู ุฅูู `generation_config` ุฅูุง ูู `dict` ุฃู ููุซูู ููุฆุฉ [`QuantizedCacheConfig`]. ูุฌุจ ุงูุฅุดุงุฑุฉ ุฅูู ุฎูููุฉ ุงูุชูููู ุงูุชู ุณูุชู ุงุณุชุฎุฏุงููุง ูู [`QuantizedCacheConfig`]ุ ูุงูุงูุชุฑุงุถู ูู `quanto`.

<Tip warning={true}>

ูููู ุฃู ูููู ุชูููู ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ููููุงุชูุญ ูุงูููู ุถุงุฑูุง ุฅุฐุง ูุงู ุทูู ุงูุณูุงู ูุตูุฑูุง ูููุงู ุฐุงูุฑุฉ ูุตูู ุนุดูุงุฆู GPU ูุงููุฉ ูุชููุฑุฉ ูุชุดุบูููุง ุจุฏูู ุชูููู ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ููููุงุชูุญ ูุงูููู.

</Tip>

```python
>>> import torch
>>> from transformers import AutoTokenizer, AutoModelForCausalLM

>>> tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf")
>>> model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-chat-hf", torch_dtype=torch.float16).to("cuda:0")
>>> inputs = tokenizer("I like rock music because", return_tensors="pt").to(model.device)

>>> out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation="quantized", cache_config={"nbits": 4, "backend": "quanto"})
>>> print(tokenizer.batch_decode(out, skip_special_tokens=True)[0])
I like rock music because it's loud and energetic. It's a great way to express myself and rel
>>> out = model.generate(**inputs, do_sample=False, max_new_tokens=20)
>>> print(tokenizer.batch_decode(out, skip_special_tokens=True)[0])
I like rock music because it's loud and energetic. I like to listen to it when I'm feeling
```

## ููู ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ููููุงุชูุญ ูุงูููู ุฎุงุฑุฌ ุงูุฐุงูุฑุฉ

ุนูู ุบุฑุงุฑ ุชูููู ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ููููุงุชูุญ ูุงููููุ ุชูุฏู ูุฐู ุงูุงุณุชุฑุงุชูุฌูุฉ ุฅูู ุชูููู ุงุณุชุฎุฏุงู ุฐุงูุฑุฉ ูุตูู ุนุดูุงุฆู GPU.
ููู ุชููู ุจุฐูู ุนู ุทุฑูู ููู ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ููููุงุชูุญ ูุงูููู ููุนุธู ุงูุทุจูุงุช ุฅูู ูุญุฏุฉ ุงููุนุงูุฌุฉ ุงููุฑูุฒูุฉ.
ูุน ุชูุฏู ุทุฑููุฉ `forward()` ูููููุฐุฌ ุนุจุฑ ุงูุทุจูุงุชุ ุชุญุงูุธ ูุฐู ุงูุงุณุชุฑุงุชูุฌูุฉ ุนูู ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ููููุงุชูุญ ูุงูููู ููุทุจูุฉ ุงูุญุงููุฉ ุนูู GPU.
ูู ุงูููุช ููุณูุ ูููู ุจุงุณุชุฑุฏุงุฏ ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ููููุงุชูุญ ูุงูููู ููุทุจูุฉ ุงูุชุงููุฉ ุจุดูู ุบูุฑ ูุชุฒุงูู ูุฅุฑุณุงู ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ููููุงุชูุญ ูุงูููู ููุทุจูุฉ ุงูุณุงุจูุฉ ูุฑุฉ ุฃุฎุฑู ุฅูู ูุญุฏุฉ ุงููุนุงูุฌุฉ ุงููุฑูุฒูุฉ.
ุนูู ุนูุณ ุชูููู ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ููููุงุชูุญ ูุงููููุ ุชูุชุฌ ูุฐู ุงูุงุณุชุฑุงุชูุฌูุฉ ุฏุงุฆููุง ููุณ ุงููุชูุฌุฉ ูุซู ุชูููุฐ ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ููููุงุชูุญ ูุงูููู ุงูุงูุชุฑุงุถูุฉ.
ูุฐููุ ูููู ุงุณุชุฎุฏุงูู ูุจุฏูู ุฃู ูุฎุทุฉ ุงุญุชูุงุทูุฉ ูู.

ุงุนุชูุงุฏูุง ุนูู ูููุฐุฌู ูุฎุตุงุฆุต ูููุฉ ุงูุชูููุฏ ุงูุฎุงุตุฉ ุจู (ุญุฌู ุงูุณูุงูุ ูุนุฏุฏ ุงูุฑููุฒ ุงููููุฒุฉ ุงููููุฏุฉุ ูุนุฏุฏ ุงูุญุฒูุ ููุง ุฅูู ุฐูู)
ูุฏ ุชูุงุญุธ ุงูุฎูุงุถูุง ุทููููุง ูู ุฅูุชุงุฌูุฉ ุงูุชูููุฏ ููุงุฑูุฉ ุจุชูููุฐ ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ููููุงุชูุญ ูุงูููู ุงูุงูุชุฑุงุถูุฉ.

ูุชูููู ููู ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ููููุงุชูุญ ูุงูููู ุฎุงุฑุฌ ุงูุฐุงูุฑุฉุ ูู ุจุชูุฑูุฑ `cache_implementation="offloaded"` ูู `generation_config`.

```python
>>> import torch
>>> from transformers import AutoTokenizer, AutoModelForCausalLM
>>> ckpt = "microsoft/Phi-3-mini-4k-instruct"

>>> tokenizer = AutoTokenizer.from_pretrained(ckpt)
>>> model = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16).to("cuda:0")
>>> inputs = tokenizer("Fun fact: The shortest", return_tensors="pt").to(model.device)

>>> out = model.generate(**inputs, do_sample=False, max_new_tokens=23, cache_implementation="offloaded")
>>> print(tokenizer.batch_decode(out, skip_special_tokens=True)[0])
Fun fact: The shortest war in history was between Britain and Zanzibar on August 27, 1896.

>>> out = model.generate(**inputs, do_sample=False, max_new_tokens=23)
>>> print(tokenizer.batch_decode(out, skip_special_tokens=True)[0])
Fun fact: The shortest war in history was between Britain and Zanzibar on August 27, 1896.
```

<Tip warning={true}>

ูุช
### ุงูุจุญุซ ุงูุชุจุงููู

ุงูุชุฑุญุช ูุฑูุฉ ุนุงู 2022 [ุฅุทุงุฑ ุนูู ุชุจุงููู ูุชูููุฏ ุงููุตูุต ุงูุนุตุจูุฉ](https://arxiv.org/abs/2202.06417) ุงุณุชุฑุงุชูุฌูุฉ ูู ุชุดููุฑ ุงูุจุญุซ ุงูุชุจุงููู.
ููู ูุธูุฑ ูุชุงุฆุฌ ูุชูููุฉ ูุชูููุฏ ูุฎุฑุฌุงุช ุทูููุฉ ูุชูุงุณูุฉ ูุบูุฑ ููุฑุฑุฉ. ููุนุฑูุฉ ููููุฉ ุนูู ุงูุจุญุซ ุงูุชุจุงูููุ ุชุญูู ูู [ูุฐู ุงูุชุฏูููุฉ](https://huggingface.co/blog/introducing-csearch).

ููุงู ูุนูุงุฑุงู ุฑุฆูุณูุงู ููููุงู ูู ุงูุชุญูู ูู ุณููู ุงูุจุญุซ ุงูุชุจุงููู ูููุง `penalty_alpha` ู`top_k`:

```python
>>> from transformers import AutoTokenizer, AutoModelForCausalLM

>>> checkpoint = "openai-community/gpt2-large"
>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)

>>> prompt = "Hugging Face Company is"
>>> inputs = tokenizer(prompt, return_tensors="pt")

>>> outputs = model.generate(**inputs, penalty_alpha=0.6, top_k=4, max_new_tokens=100)
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
['Hugging Face Company is a family owned and operated business. We pride ourselves on being the best
in the business and our customer service is second to none.\n\nIf you have any questions about our
products or services, feel free to contact us at any time. We look forward to hearing from you!']
```

### ุงููุนุงููุฉ ูุชุนุฏุฏุฉ ุงูุญุฏูุฏ

ุนูู ุนูุณ ุงูุจุญุซ ุงูุดุฑู ุงูุฐู ูุฎุชุงุฑ ุฏุงุฆููุง ุฑูุฒูุง ูู ุฃุนูู ุงุญุชูุงู ูููู ุงูุฑูุฒ ุงูุชุงููุ ูุฅู ุงููุนุงููุฉ ูุชุนุฏุฏุฉ ุงูุญุฏูุฏ (ูุทูู ุนูููุง ุฃูุถูุง ุงููุนุงููุฉ ุงูุณูููุฉ) ุชุฎุชุงุฑ ุงูุฑูุฒ ุงูุชุงูู ุจุดูู ุนุดูุงุฆู ุจูุงุกู ุนูู ุชูุฒูุน ุงูุงุญุชูุงููุฉ ุนุจุฑ ุงูููุฑุฏุงุช ุจุงููุงูู ุงูุชู ูููุญูุง ุงููููุฐุฌ. ูู ุฑูุฒ ูู ุงุญุชูุงู ุบูุฑ ุตูุฑู ูุฏูู ูุฑุตุฉ ุฃู ูุชู ุงุฎุชูุงุฑูุ ููุง ูููู ูู

ุฎุทุฑ ุงูุชูุฑุงุฑ.

ูุชูููู ุงููุนุงููุฉ ูุชุนุฏุฏุฉ ุงูุญุฏูุฏุ ูู ุจุชุนููู `do_sample=True` ู`num_beams=1`.

```python
>>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed
>>> set_seed(0) # ูู ุฃุฌู ุฅููุงููุฉ ุฅุนุงุฏุฉ ุงูุฅูุชุงุฌ

>>> checkpoint = "openai-community/gpt2-large"
>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)

>>> prompt = "Today was an amazing day because"
>>> inputs = tokenizer(prompt, return_tensors="pt")

>>> outputs = model.generate(**inputs, do_sample=True, num_beams=1, max_new_tokens=100)
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
["Today was an amazing day because we received these wonderful items by the way of a gift shop. The box arrived on a Thursday and I opened it on Monday afternoon to receive the gifts. Both bags featured pieces from all the previous years!\n\nThe box had lots of surprises in it, including some sweet little mini chocolate chips! I don't think I'd eat all of these. This was definitely one of the most expensive presents I have ever got, I actually got most of them for free!\n\nThe first package came"]
```

### ูู ุชุดููุฑ ุงูุจุญุซ ุงูุดุนุงุนู

ุนูู ุนูุณ ุงูุจุญุซ ุงูุดุฑูุ ูุญุชูุธ ูู ุชุดููุฑ ุงูุจุญุซ ุงูุดุนุงุนู ุจุนุฏุฉ ูุฑุถูุงุช ูู ูู ุฎุทูุฉ ุฒูููุฉ ููุฎุชุงุฑ ูู ุงูููุงูุฉ
ุงููุฑุถูุฉ ุงูุชู ููุง ุฃุนูู ุงุญุชูุงู ุฅุฌูุงูู ููุชุณูุณู ุจุฃูููู. ุชุชูุซู ููุฒุฉ ุฐูู ูู ุชุญุฏูุฏ ุชุณูุณูุงุช ุนุงููุฉ ุงูุงุญุชูุงู
ุงูุชู ุชุจุฏุฃ ุจุฑููุฒ ุฃูููุฉ ุฐุงุช ุงุญุชูุงููุฉ ุฃูู ูุงูุชู ุณุชุชุฌุงูููุง ุนูููุฉ ุงูุจุญุซ ุงูุดุฑู.

<a href="https://huggingface.co/spaces/m-ric/beam_search_visualizer" class="flex flex-col justify-center">
    <img style="max-width: 90%; margin: auto;" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/beam_search.png"/>
</a>

ููููู ุชุตูุฑ ููููุฉ ุนูู ูู ุชุดููุฑ ุงูุจุญุซ ุงูุดุนุงุนู ูู [ูุฐุง ุงูุนุฑุถ ุงูุชูุถูุญู ุงูุชูุงุนูู](https://huggingface.co/spaces/m-ric/beam_search_visualizer): ุงูุชุจ ุฌููุชู ุงููุฏุฎูุฉุ ููุนุจ ูุน ุงููุนููุงุช ููุดุงูุฏุฉ ููููุฉ ุชุบููุฑ ุญุฒู ูู ุงูุชุดููุฑ.

ูุชูููู ุงุณุชุฑุงุชูุฌูุฉ ูู ุงูุชุดููุฑ ูุฐูุ ุญุฏุฏ `num_beams` (ุฃู ุนุฏุฏ ุงููุฑุถูุงุช ุงูุชู ูุฌุจ ุชุชุจุนูุง) ุฃูุจุฑ ูู 1.

```python
>>> from transformers import AutoModelForCausalLM, AutoTokenizer
```python
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> prompt = "It is astonishing how one can"
>>> checkpoint = "openai-community/gpt2-medium"

>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
>>> inputs = tokenizer(prompt, return_tensors="pt")

>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)

>>> outputs = model.generate(**inputs, num_beams=5, max_new_tokens=50)
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
['It is astonishing how one can have such a profound impact on the lives of so many people in such a short period of
time."\n\nHe added: "I am very proud of the work I have been able to do in the last few years.\n\n"I have']
```

### ูุนุงููุฉ ุดุนุงุน ูุชุนุฏุฏุฉ ุงูุญุฏูุฏ

ููุง ููุญู ุงูุงุณูุ ุชุฌูุน ุงุณุชุฑุงุชูุฌูุฉ ูู ุงูุชุดููุฑ ูุฐู ุจูู ุงูุจุญุซ ุงูุดุนุงุนู ูุงููุนุงููุฉ ูุชุนุฏุฏุฉ ุงูุญุฏูุฏ. ูุฌุจ ุนููู ุชุญุฏูุฏ
`num_beams` ุฃูุจุฑ ูู 1ุ ูุชุนููู `do_sample=True` ูุงุณุชุฎุฏุงู ุงุณุชุฑุงุชูุฌูุฉ ูู ุงูุชุดููุฑ ูุฐู.

```python
>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, set_seed
>>> set_seed(0) # ูู ุฃุฌู ุฅููุงููุฉ ุฅุนุงุฏุฉ ุงูุฅูุชุงุฌ

>>> prompt = "translate English to German: The house is wonderful."
>>> checkpoint = "google-t5/t5-small"

>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
>>> inputs = tokenizer(prompt, return_tensors="pt")

>>> model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)

>>> outputs = model.generate(**inputs, num_beams=5, do_sample=True)
>>> tokenizer.decode(outputs[0], skip_special_tokens=True)
'Das Haus ist wunderbar.'
```

### ูู ุชุดููุฑ ุงูุจุญุซ ุงูุดุนุงุนู ุงููุชููุน

ุงุณุชุฑุงุชูุฌูุฉ ูู ุชุดููุฑ ุงูุจุญุซ ุงูุดุนุงุนู ุงููุชููุน ูู ุงูุชุฏุงุฏ ูุงุณุชุฑุงุชูุฌูุฉ ุงูุจุญุซ ุงูุดุนุงุนู ุงูุชู ุชุชูุญ ุชูููุฏ ูุฌููุนุฉ ุฃูุซุฑ ุชููุนูุง
ูู ุชุณูุณูุงุช ุงูุดุนุงุน ููุงุฎุชูุงุฑ ูู ุจูููุง. ููุนุฑูุฉ ููููุฉ ุนูููุ ุฑุงุฌุน [ุจุญุซ ุดุนุงุนู ูุชููุน: ูู ุชุดููุฑ ุญููู ูุชููุนุฉ ูู ููุงุฐุฌ ุงูุชุณูุณู ุงูุนุตุจู](https://arxiv.org/pdf/1610.02424.pdf).

ูุฏู ูุฐุง ุงูููุฌ ุซูุงุซุฉ ูุนููุงุช ุฑุฆูุณูุฉ: `num_beams`ุ `num_beam_groups`ุ ู`diversity_penalty`.
ุชุถูู ุนููุจุฉ ุงูุชููุน ุชููุฒ ุงูุฅุฎุฑุงุฌ ุนุจุฑ ุงููุฌููุนุงุชุ ููุชู ุงุณุชุฎุฏุงู ุงูุจุญุซ ุงูุดุนุงุนู ุฏุงุฎู ูู ูุฌููุนุฉ.


```python
>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

>>> checkpoint = "google/pegasus-xsum"
>>> prompt = (
...     "The Permaculture Design Principles are a set of universal design principles "
...     "that can be applied to any location, climate and culture, and they allow us to design "
...     "the most efficient and sustainable human habitation and food production systems. "
...     "Permaculture is a design system that encompasses a wide variety of disciplines, such "
...     "as ecology, landscape design, environmental science and energy conservation, and the "
...     "Permaculture design principles are drawn from these various disciplines. Each individual "
...     "design principle itself embodies a complete conceptual framework based on sound "
...     "scientific principles. When we bring all these separate  principles together, we can "
...     "create a design system that both looks at whole systems, the parts that these systems "
...     "consist of, and how those parts interact with each other to create a complex, dynamic, "
...     "living system. Each design principle serves as a tool that allows us to integrate all "
...     "the separate parts of a design, referred to as elements, into a functional, synergistic, "
...     "whole system, where the elements harmoniously interact and work together in the most "
...     "efficient way possible."
... )

>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
>>> inputs = tokenizer(prompt, return_tensors="pt")

>>> model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)

>>> outputs = model.generate(**inputs, num_beams=5, num_beam_groups=5, max_new_tokens=30, diversity_penalty=1.0)
>>> tokenizer.decode(outputs[0], skip_special_tokens=True)
'The Design Principles are a set of universal design principles that can be applied to any location, climate and
culture, and they allow us to design the'
```
ููุถุญ ูุฐุง ุงูุฏููู ุงููุนููุงุช ุงูุฑุฆูุณูุฉ ุงูุชู ุชููู ุงุณุชุฑุงุชูุฌูุงุช ูู ุงูุชุดููุฑ ุงููุฎุชููุฉ. ููุงู ูุนููุงุช ุฃูุซุฑ ุชูุฏููุง ูู
ุทุฑููุฉ [`generate`]ุ ูุงูุชู ุชููุญู ูุฒูุฏูุง ูู ุงูุชุญูู ูู ุณููู ุทุฑููุฉ [`generate`].

ููุงุทูุงุน ุนูู ุงููุงุฆูุฉ ุงููุงููุฉ ูููุนููุงุช ุงููุชุงุญุฉุ ุฑุงุฌุน [ุชูุซูู API](./main_classes/text_generation.md).

### ูู ุงูุชุดููุฑ ุงูุชุฎูููู

ูู ุงูุชุดููุฑ ุงูุชุฎูููู (ุงููุนุฑูู ุฃูุถูุง ุจุงุณู ูู ุงูุชุดููุฑ ุจูุณุงุนุฏุฉ) ูู ุชุนุฏูู ูุงุณุชุฑุงุชูุฌูุงุช ูู ุงูุชุดููุฑ ุงููุฐููุฑุฉ ุฃุนูุงูุ ูุงูุฐู ูุณุชุฎุฏู
ูููุฐุฌ ูุณุงุนุฏ (ููุถู ุฃู ูููู ุฃุตุบุฑ ุจูุซูุฑ) ุจููุณ ุงููุนุงูุฌ ุงููุบููุ ูุชูููุฏ ุจุนุถ ุงูุฑููุฒ ุงููุฑุดุญุฉ. ุซู ูููู ุงููููุฐุฌ ุงูุฑุฆูุณู
ุจุชุญูู ูู ุงูุฑููุฒ ุงููุฑุดุญุฉ ูู ุชูุฑูุฑ ุชูุฌููู ูุงุญุฏุ ูุงูุฐู ูุณุฑุน ุนูููุฉ ูู ุงูุชุดููุฑ. ุฅุฐุง
`do_sample=True`ุ ูุชู ุงุณุชุฎุฏุงู ุงูุชุญูู ูู ุงูุฑูุฒ ูุน ุฅุนุงุฏุฉ ุงููุนุงููุฉ ุงูููุฏูุฉ ูู
[ูุฑูุฉ ูู ุงูุชุดููุฑ ุงูุชุฎูููู](https://arxiv.org/pdf/2211.17192.pdf).

ุญุงูููุงุ ูุชู ุฏุนู ุงูุจุญุซ ุงูุดุฑู ูุงููุนุงููุฉ ููุท ูุน ูู ุงูุชุดููุฑ ุจูุณุงุนุฏุฉุ ููุง ูุฏุนู ูู ุงูุชุดููุฑ ุจูุณุงุนุฏุฉ ุงูุฅุฏุฎุงูุงุช ุงููุฌูุนุฉ.
ููุนุฑูุฉ ุงููุฒูุฏ ุญูู ูู ุงูุชุดููุฑ ุจูุณุงุนุฏุฉุ ุชุญูู ูู [ูุฐู ุงูุชุฏูููุฉ](https://huggingface.co/blog/assisted-generation).

ูุชูููู ูู ุงูุชุดููุฑ ุจูุณุงุนุฏุฉุ ูู ุจุชุนููู ูุณูุท `assistant_model` ุจูููุฐุฌ.

```python
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> prompt = "Alice and Bob"
>>> checkpoint = "EleutherAI/pythia-1.4b-deduped"
>>> assistant_checkpoint = "EleutherAI/pythia-160m-deduped"

>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
>>> inputs = tokenizer(prompt, return_tensors="pt")

>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)
>>> assistant_model = AutoModelForCausalLM.from_pretrained(assistant_checkpoint)
>>> outputs = model.generate(**inputs, assistant_model=assistant_model)
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
['Alice and Bob are sitting in a bar. Alice is drinking a beer and Bob is drinking a']
```

ุนูุฏ ุงุณุชุฎุฏุงู ูู ุงูุชุดููุฑ ุจูุณุงุนุฏุฉ ูุน ุทุฑู ุงููุนุงููุฉุ ููููู ุงุณุชุฎุฏุงู ูุณูุท `temperature` ููุชุญูู ูู ุงูุนุดูุงุฆูุฉุ
ุชูุงููุง ููุง ูู ุงูุญุงู ูู ุงููุนุงููุฉ ูุชุนุฏุฏุฉ ุงูุญุฏูุฏ. ููุน ุฐููุ ูู ูู ุงูุชุดููุฑ ุจูุณุงุนุฏุฉุ ูุฏ ูุณุงุนุฏ ุชูููู ุฏุฑุฌุฉ ุงูุญุฑุงุฑุฉ ูู ุชุญุณูู ุงููููู.

```python
>>> from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed
>>> set_seed(42) # ูู ุฃุฌู ุฅููุงููุฉ ุฅุนุงุฏุฉ ุงูุฅูุชุงุฌ

>>> prompt = "Alice and Bob"
>>> checkpoint = "EleutherAI/pythia-1.4b-deduped"
>>> assistant_checkpoint = "EleutherAI/pythia-160m-deduped"

>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
>>> inputs = tokenizer(prompt, return_tensors="pt")

>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)
>>> assistant_model = AutoModelForCausalLM.from_pretrained(assistant_checkpoint)
>>> outputs = model.generate(**inputs, assistant_model=assistant_model, do_sample=True, temperature=0.5)
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
['Alice and Bob, a couple of friends of mine, who are both in the same office as']
```

ุจุฏูุงู ูู ุฐููุ ููููู ุฃูุถูุง ุชุนููู `prompt_lookup_num_tokens` ูุชุดุบูู ูู ุงูุชุดููุฑ ุจูุณุงุนุฏุฉ n-gramุ ุจุฏูุงู ูู
ูู ุงูุชุดููุฑ ุจูุณุงุนุฏุฉ ุงูููุงุฐุฌ. ููููู ูุฑุงุกุฉ ุงููุฒูุฏ ุนูู [ููุง](https://twitter.com/joao_gante/status/1747322413006643259).
### ูู ุชุดููุฑ DoLa

**D** ูู ุงูุชุดููุฑ ุนู ุทุฑูู ุชุจุงูู **La** ูู ุชุดููุฑ ุงูุทุจูุงุช (DoLa) ูู ุงุณุชุฑุงุชูุฌูุฉ ูู ุชุดููุฑ ุชุจุงููู ูุชุญุณูู ุงููุงูุนูุฉ ูุงูุญุฏ ูู
ุงููููุณุฉ ูู LLMsุ ููุง ูู ููุถุญ ูู ูุฐู ุงููุฑูุฉ ICLR 2024 [DoLa: ูู ุชุดููุฑ ุงูุทุจูุงุช ุงูุชุจุงููู ูุญุณู ุงููุงูุนูุฉ ูู ููุงุฐุฌ ุงููุบุฉ ุงููุจูุฑุฉ](https://arxiv.org/abs/2309.03883).

ูุชู ุชุญููู DoLa ูู ุฎูุงู ุชุถุฎูู ุงูุงุฎุชูุงูุงุช ูู logits ุงูุชู ุชู ุงูุญุตูู ุนูููุง ูู ุงูุทุจูุงุช ุงูููุงุฆูุฉ
ููุงุจู ุงูุทุจูุงุช ุงูุณุงุจูุฉุ ูุจุงูุชุงูู ุชุถุฎูู ุงููุนุฑูุฉ ุงููุงูุนูุฉ ุงูููุถุนูุฉ ูู ุฌุฒุก ูุนูู ูู ุทุจูุงุช ุงููุญูู.
ูุชู ุชุญููู DoLa ูู ุฎูุงู ุชุถุฎูู ุงูุงุฎุชูุงูุงุช ูู logits ุงูุชู ุชู ุงูุญุตูู ุนูููุง ูู ุงูุทุจูุงุช ุงูููุงุฆูุฉ
ููุงุจู ุงูุทุจูุงุช ุงูุณุงุจูุฉุ ูุจุงูุชุงูู ุชุถุฎูู ุงููุนุฑูุฉ ุงููุงูุนูุฉ ุงูููุถุนูุฉ ูู ุฌุฒุก ูุนูู ูู ุทุจูุงุช ุงููุญูู.

ุงุชุจุน ุงูุฎุทูุชูู ุงูุชุงููุชูู ูุชูุดูุท ูู ุชุดููุฑ DoLa ุนูุฏ ุงุณุชุฏุนุงุก ูุธููุฉ `model.generate`:

1. ูู ุจุชุนููู ูุณูุท `dola_layers`ุ ูุงูุฐู ูููู ุฃู ูููู ุฅูุง ุณูุณูุฉ ุฃู ูุงุฆูุฉ ูู ุงูุฃุนุฏุงุฏ ุงูุตุญูุญุฉ.
    - ุฅุฐุง ุชู ุชุนูููู ุนูู ุณูุณูุฉุ ููููู ุฃู ูููู ุฃุญุฏ `low`ุ `high`.
    - ุฅุฐุง ุชู ุชุนูููู ุนูู ูุงุฆูุฉ ูู ุงูุฃุนุฏุงุฏ ุงูุตุญูุญุฉุ ููุฌุจ ุฃู ูููู ูุงุฆูุฉ ุจูุคุดุฑุงุช ุงูุทุจูุงุช ุจูู 0 ูุงูุนุฏุฏ ุงูุฅุฌูุงูู ููุทุจูุงุช ูู ุงููููุฐุฌ. ุทุจูุฉ 0 ูู ุทุจูุฉ ุชุถููู ุงููููุงุชุ ูุงูุทุจูุฉ 1 ูู ุฃูู ุทุจูุฉ ูุญููุ ูููุฐุง.
2. ูููุชุฑุญ ุชุนููู `repetition_penalty = 1.2` ูุชูููู ุงูุชูุฑุงุฑ ูู ูู ุชุดููุฑ DoLa.

ุฑุงุฌุน ุงูุฃูุซูุฉ ุงูุชุงููุฉ ููู ุชุดููุฑ DoLa ุจุงุณุชุฎุฏุงู ูููุฐุฌ LLaMA-7B ุงููููู ูู 32 ุทุจูุฉ.

```python
>>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("huggyllama/llama-7b")
>>> model = AutoModelForCausalLM.from_pretrained("huggyllama/llama-7b", torch_dtype=torch.float16)
>>> device = 'cuda' if torch.cuda.is_available() else 'cpu'
>>> model.to(device)
>>> set_seed(42)

>>> text = "On what date was the Declaration of Independence officially signed?"
>>> inputs = tokenizer(text, return_tensors="pt").to(device)

# Vanilla greddy decoding
>>> vanilla_output = model.generate(**inputs, do_sample=False, max_new_tokens=50)
>>> tokenizer.batch_decode(vanilla_output[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)
['\nThe Declaration of Independence was signed on July 4, 1776.\nWhat was the date of the signing of the Declaration of Independence?\nThe Declaration of Independence was signed on July 4,']

# DoLa decoding with contrasting higher part of layers (layers 16,18,...,30)
>>> dola_high_output = model.generate(**inputs, do_sample=False, max_new_tokens=50, dola_layers='high')
>>> tokenizer.batch_decode(dola_high_output[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)
['\nJuly 4, 1776, when the Continental Congress voted to separate from Great Britain. The 56 delegates to the Continental Congress signed the Declaration on August 2, 1776.']

# DoLa decoding with contrasting specific layers (layers 28 and 30)
>>> dola_custom_output = model.generate(**inputs, do_sample=False, max_new_tokens=50, dola_layers=[28,30], repetition_penalty=1.2)
>>> tokenizer.batch_decode(dola_custom_output[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)
['\nIt was officially signed on 2 August 1776, when 56 members of the Second Continental Congress, representing the original 13 American colonies, voted unanimously for the resolution for independence. The 2']
```

#### ููู ูุนุงููุงุช 'dola_layers'

ููุซู 'dola_layers' ุทุจูุงุช ุงููุฑุดุญ ูู ุงูุงุฎุชูุงุฑ ุงููุจูุฑ ููุทุจูุฉุ ููุง ูู ููุถุญ ูู ูุฑูุฉ DoLa. ุณุชุชู ููุงุฑูุฉ ุงูุทุจูุฉ ุงููุจูุฑุฉ ุงููุญุฏุฏุฉ ุจุงูุทุจูุฉ ุงูููุงุฆูุฉ.

ูุคุฏู ุชุนููู 'dola_layers' ุฅูู 'low' ุฃู 'high' ุฅูู ุชุญุฏูุฏ ุงูุฌุฒุก ุงูุณููู ุฃู ุงูุนููู ูู ุงูุทุจูุงุช ููููุงุฑูุฉุ ุนูู ุงูุชูุงูู.

- ุจุงููุณุจุฉ ูููุงุฐุฌ 'N-layer' ูุน 'N <= 40' layerุ ูุชู ุงุณุชุฎุฏุงู ุงูุทุจูุงุช ูู 'range(0ุ N // 2ุ 2)' ู'range(N // 2ุ Nุ 2)' ูู 'low' ู 'high' layersุ ุนูู ุงูุชูุงูู.

- ุจุงููุณุจุฉ ููููุงุฐุฌ ุงูุชู ุชุญุชูู ุนูู 'N > 40' layerุ ูุชู ุงุณุชุฎุฏุงู ุงูุทุจูุงุช ูู 'range(0ุ 20ุ 2)' ู'range(N - 20ุ Nุ 2)' ูู 'low' ู 'high' layersุ ุนูู ุงูุชูุงูู.

- ุฅุฐุง ูุงู ูููููุฐุฌ ุชุนูููุงุช ุชูุถูุญูุฉ ูุฑุชุจุทุฉ ุจุงููููุงุชุ ูุฅููุง ูุชุฎุทู ุทุจูุฉ ุงูุชุนูููุงุช ุงูุชูุถูุญูุฉ ูููููุงุช (ุงูุทุจูุฉ 0) ููุจุฏุฃ ูู ุงูุทุจูุฉ ุงูุซุงููุฉุ ูุธุฑูุง ูุฃู ุงูุฎุฑูุฌ ุงููุจูุฑ ูู ุงูุชุนูููุงุช ุงูุชูุถูุญูุฉ ูููููุงุช ุณูุตุจุญ ุฏุงูุฉ ุงููููุฉ.

- ูู ุจุชุนููู 'dola_layers' ุฅูู ูุงุฆูุฉ ูู ุงูุฃุนุฏุงุฏ ุงูุตุญูุญุฉ ูููุฑุณุฉ ุงูุทุจูุงุช ูููุงุฑูุฉ ุงูุทุจูุงุช ุงููุญุฏุฏุฉ ูุฏูููุง. ุนูู ุณุจูู ุงููุซุงูุ ูุคุฏู ุชุนููู 'dola_layers=[28ุ30]' ุฅูู ููุงุฑูุฉ ุงูุทุจูุฉ ุงูููุงุฆูุฉ (ุงูุทุจูุฉ 32) ุจุงูุทุจูุงุช 28 ู30.

ุงูุชุฑุญุช ุงููุฑูุฉ ุฃู ููุงุฑูุฉ ุงูุทุจูุงุช 'ุงูุนุงููุฉ' ูุชุญุณูู ููุงู ุงูุฅุฌุงุจุงุช ุงููุตูุฑุฉ ูุซู TruthfulQAุ ูููุงุฑูุฉ ุงูุทุจูุงุช 'ุงูููุฎูุถุฉ' ูุชุญุณูู ุฌููุน ููุงู ุงูุงุณุชุฏูุงู ุจุงูุฅุฌุงุจุงุช ุงูุทูููุฉ ุงูุฃุฎุฑูุ ูุซู GSM8K ูStrategyQA ูFACTOR ูVicunaQA. ูุง ููุตู ุจุชุทุจูู DoLa ุนูู ุงูููุงุฐุฌ ุงูุฃุตุบุฑ ูุซู GPT-2ุ ููุง ูู ููุถุญ ูู ุงูููุญู N ูู ุงููุฑูุฉ.