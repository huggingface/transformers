# ุงูุชุนููุฏ ุงููุบูู ููููุงุฐุฌ ุฐุงุช ุงูุทูู ุงูุซุงุจุช

[[open-in-colab]]

 ุงูุชุนููุฏ ุงููุบูู (PPL) ูู ูุงุญุฏุฉ ูู ุฃูุซุฑ ุงูููุงููุณ ุดููุนูุง ูุชูููู ููุงุฐุฌ ุงููุบุฉ. ูุจู ุงูุฎูุถ ูู ุงูุชูุงุตููุ ูุฌุจ ุฃู ููุงุญุธ ุฃู ุงููููุงุณ ููุทุจู ุชุญุฏูุฏูุง ุนูู ููุงุฐุฌ ุงููุบุฉ ุงูููุงุณูููุฉ (ููุทูู ุนูููุง ุฃุญูุงููุง ููุงุฐุฌ ุงููุบุฉ ุงูุชููุงุฆูุฉ ุงููุฑุฌุนูุฉ ุฃู ุงูุณุจุจูุฉ) ููู ุบูุฑ ูุญุฏุฏุฉ ุฌูุฏูุง ูููุงุฐุฌ ุงููุบุฉ ุงููููุนุฉ ูุซู BERT (ุฑุงุฌุน [ููุฎุต ุงูููุงุฐุฌ](model_summary)).

ุชูุนุฑููู ุงูุชุนููุฏ ุงููุบูู ุนูู ุฃููุง ุงูุฃุณ ุงูููุฑููุน ููููุฉ ูุชูุณุท ุงูููุบุงุฑูุชู ุงูุงุญุชูุงูู ููุชุชุงููุฉ. ุฅุฐุง ูุงู ูุฏููุง ุชุณูุณู ุฑูุฒู \\(X = (x_0, x_1, \dots, x_t)\\)ุ ูุฅู ุญูุฑุฉ \\(X\\) ููุ

$$\text{PPL}(X) = \exp \left\{ {-\frac{1}{t}\sum_i^t \log p_\theta (x_i|x_{<i}) } \right\}$$

ุญูุซ \\(\log p_\theta (x_i|x_{<i})\\) ูู ุงูููุบุงุฑูุชู ุงูุงุญุชูุงูู ููุฑูุฒ i ุจุดุฑุท ุงูุฑููุฒ ุงูุณุงุจูุฉ \\(x_{<i}\\) ููููุง ููููุฐุฌูุง. ููู ุงููุงุญูุฉ ุงูุจุฏูููุฉุ ูููู ุงุนุชุจุงุฑูุง ุชูููููุง ููุฏุฑุฉ ุงููููุฐุฌ ุนูู ุงูุชูุจุค ุจุงูุชุณุงูู ุจูู ูุฌููุนุฉ ูู ุงูุฑููุฒ ุงููุญุฏุฏุฉ ูู ูุฌููุนุฉ ูู ุงูุจูุงูุงุช. ููู ุงูููู ุงูุฅุดุงุฑุฉ ุฅูู ุฃู ุนูููุฉ ุงูุชูููุฒ ูู ุชุฃุซูุฑ ูุจุงุดุฑูุง ุนูู ุญูุฑุฉ ุงููููุฐุฌุููุฌุจ ูุฑุงุนุงุชูุง ุฏุงุฆููุง ุนูุฏ ููุงุฑูุฉ ุงูููุงุฐุฌ ุงููุฎุชููุฉ.

ููุง ุฃููุง ุชุนุงุฏู ุงูุฃุณ ุงูููุฑููุน ููููุฉ ุงูุงูุชุฑูุจูุง ุงููุชูุงุทุนุฉ ุจูู ุงูุจูุงูุงุช ูุชูุจุคุงุช ุงููููุฐุฌ. ููุฒูุฏ ูู ุงูููู ุญูู ููููู ุงูุชุนููุฏ ุงููุบูู ูุนูุงูุชูุง ุจู Bits Per Character (BPC) ูุถุบุท ุงูุจูุงูุงุชุ ููุฑุฌู ูุฑุงุฌุนุฉ [ุงูุชุฏูููุฉ ุงููููุฏุฉ ุนูู The Gradient](https://thegradient.pub/understanding-evaluation-metrics-for-language-models/).

## ุญุณุงุจ PPL ูุน ุงูููุงุฐุฌ ุฐุงุช ุงูุทูู ุงูุซุงุจุช

ุฅุฐุง ูู ููู ูููุฏูู ุจุญุฌู ุณูุงู ุงููููุฐุฌุ ูุณูููู ุจุชูููู ุงูุชุนููุฏ ุงููุบูู ูููููุฐุฌ ุนู ุทุฑูู ุชุญููู ุงูุชุณูุณู ุชููุงุฆููุง ูุงูุดุฑุท ุนูู ุงูุชุณูุณู ุงููุฑุนู ุงูุณุงุจู ุจุงููุงูู ูู ูู ุฎุทูุฉุ ููุง ูู ููุถุญ ุฃุฏูุงู.

<img width="600" alt="Full decomposition of a sequence with unlimited context length" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/ppl_full.gif"/>

ููู ุนูุฏ ุงูุชุนุงูู ูุน ุงูููุงุฐุฌ ุงูุชูุฑูุจูุฉุ ููุงุฌู ุนุงุฏุฉู ููุฏูุง ุนูู ุนุฏุฏ ุงูุฑููุฒ ุงูุชู ูููู ูููููุฐุฌ ูุนุงูุฌุชูุง. ุนูู ุณุจูู ุงููุซุงูุ ุชุญุชูู ุฃูุจุฑ ูุณุฎุฉ ูู [GPT-2](model_doc/gpt2) ุนูู ุทูู ุซุงุจุช ูุจูุบ 1024 ุฑูุฒูุงุ ูุฐุง ูุง ูููููุง ุญุณุงุจ \\(p_\theta(x_t|x_{<t})\\) ูุจุงุดุฑุฉ ุนูุฏูุง ุชููู \\(t\\) ุฃูุจุฑ ูู 1024.

ุจุฏูุงู ูู ุฐููุ ูุชู ุนุงุฏุฉู ุชูุณูู ุงูุชุณูุณู ุฅูู ุชุณูุณูุงุช ูุฑุนูุฉ ูุณุงููุฉ ูุญุฌู ุงูุฅุฏุฎุงู ุงูุฃูุตู ูููููุฐุฌ. ูุฅุฐุง ูุงู ุญุฌู ุงูุฅุฏุฎุงู ุงูุฃูุตู ูููููุฐุฌ ูู \\(k\\)ุ ูุฅููุง ููุฑุจ ุงุญุชูุงู ุงูุฑูุฒ \\(x_t\\) ุนู ุทุฑูู ุงูุงุดุชูุงู ุงูุดุฑุทู ููุท ุจุงููุณุจุฉ ุฅูู \\(k-1\\) ูู ุงูุฑููุฒ ุงูุชู ุชุณุจูู ุจุฏูุงู ูู ุงูุณูุงู ุจุฃูููู. ูุนูุฏ ุชูููู ุญูุฑุฉ ุงููููุฐุฌ  ูุชุณูุณู ูุงุ ูุฏ ูุจุฏู ูู ุงููุบุฑู ุชูุณูู ุงูุชุณูุณู ุฅูู ุฃุฌุฒุงุก ูููุตูุฉ ูุฌูุน ูุฌููุน ุฏูุงู ุงูููุบุงุฑูุชู ููู ุฌุฒุก ุจุดูู ูุณุชููุ ููู ูุฐุง ุงูุฃุณููุจ ููุณ ุงูุฃูุซู.

<img width="600" alt="Suboptimal PPL not taking advantage of full available context" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/ppl_chunked.gif"/>

ุชุชููุฒ ูุฐู ุงูุทุฑููุฉ ุจุณุฑุนุฉ ุญุณุงุจูุง ูุธุฑูุง ูุฅููุงููุฉ ุญุณุงุจ ุฏุฑุฌุฉ ุงูุชุนููุฏ ุงููุบูู ููู ุฌุฒุก ุจูุณุญ ูุงุญุฏ ููุฃูุงูุ ุฅูุง ุฃููุง ุชูุนุฏู ุชูุฑูุจูุง ุถุนูููุง ูุฏุฑุฌุฉ ุงูุชุนููุฏ ุงููุบูู ุงูููุญูููุฉ ุจุดูู ูุงููุ ูุนุงุฏุฉู ูุง ุชุคุฏู ุฅูู ุฏุฑุฌุฉ ุชุนููุฏ ูุบูู ุฃุนูู (ุฃุณูุฃ) ูุฃู ุงููููุฐุฌ ุณูููู ูุฏูู ุณูุงู ุฃูู ูู ูุนุธู ุฎุทูุงุช ุงูุชูุจุค.

ุจุฏูุงู ูู ุฐููุ ูุฌุจ ุชูููู ุฏุฑุฌุฉ ุงูุชุนููุฏ ุงููุบูู ููููุงุฐุฌ ุฐุงุช ุงูุทูู ุงูุซุงุจุช ุจุงุณุชุฎุฏุงู ุฅุณุชุฑุงุชูุฌูุฉ ุงููุงูุฐุฉ ุงูููุฒููุฉ. ูููุทูู ูุฐุง ุนูู ุชุญุฑูู ูุงูุฐุฉ ุงูุณูุงู ุจุดูู ูุชูุฑุฑ ุจุญูุซ ูููู ูููููุฐุฌ ุณูุงู ุฃูุจุฑ ุนูุฏ ุฅุฌุฑุงุก ูู ุชูุจุค.

<img width="600" alt="Sliding window PPL taking advantage of all available context" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/ppl_sliding.gif"/>

ูุฐุง ุชูุฑูุจ ุฃูุฑุจ ููุชูููู ุงูุญูููู ูุงุญุชูุงููุฉ ุงูุชุณูุณู ูุณูุคุฏู ุนุงุฏุฉู ุฅูู ูุชูุฌุฉ ุฃูุถู.ููู ุงูุฌุงูุจ ุงูุณูุจู ูู ุฃูู ูุชุทูุจ ุชูุฑูุฑูุง ููุฃูุงู ููู ุฑูุฒ ูู ูุฌููุนุฉ ุงูุจูุงูุงุช. ุญู ูุณุท ุนููู ููุงุณุจ ูู ุงุณุชุฎุฏุงู ูุงูุฐุฉ ููุฒููุฉ ุจุฎุทูุฉุ ุจุญูุซ ูุชู ุชุญุฑูู ุงูุณูุงู ุจุฎุทูุงุช ุฃูุจุฑ ุจุฏูุงู ูู ุงูุงูุฒูุงู ุจููุฏุงุฑ 1 ุฑูุฒ ูู ูู ูุฑุฉ. ููุง ูุณูุญ ุจุฅุฌุฑุงุก ุงูุญุณุงุจ ุจุดูู ุฃุณุฑุน ูุน ุฅุนุทุงุก ุงููููุฐุฌ ุณูุงููุง ูุจูุฑูุง ููุชูุจุคุงุช ูู ูู ุฎุทูุฉ.

## ูุซุงู: ุญุณุงุจ ุงูุชุนููุฏ ุงููุบูู ูุน GPT-2 ูู ๐ค Transformers

ุฏุนููุง ููุถุญ ูุฐู ุงูุนูููุฉ ูุน GPT-2.

```python
from transformers import GPT2LMHeadModel, GPT2TokenizerFast

device = "cuda"
model_id = "openai-community/gpt2-large"
model = GPT2LMHeadModel.from_pretrained(model_id).to(device)
tokenizer = GPT2TokenizerFast.from_pretrained(model_id)
```

ุณูููู ุจุชุญููู ูุฌููุนุฉ ุจูุงูุงุช WikiText-2 ูุชูููู ุงูุชุนููุฏ ุงููุบูู ุจุงุณุชุฎุฏุงู ุจุนุถ ุฅุณุชุฑุงุชูุฌูุงุช ูุฎุชููุฉ ุงููุงูุฐุฉ ุงูููุฒููุฉ. ูุธุฑูุง ูุฃู ูุฐู ุงููุฌููุนุฉ ุงูุจูุงูุงุช ุงูุตุบูุฑุฉ ููููู ููุท ุจูุณุญ ูุงุญุฏ ููุท ูููุฌููุนุฉุ ููููููุง ุจุจุณุงุทุฉ ุชุญููู ูุฌููุนุฉ ุงูุจูุงูุงุช ูุชุฑููุฒูุง ุจุงููุงูู ูู ุงูุฐุงูุฑุฉ.

```python
from datasets import load_dataset

test = load_dataset("wikitext", "wikitext-2-raw-v1", split="test")
encodings = tokenizer("\n\n".join(test["text"]), return_tensors="pt")
```

ูุน ๐ค Transformersุ ูููููุง ุจุจุณุงุทุฉ ุชูุฑูุฑ `input_ids` ูู `labels` ุฅูู ูููุฐุฌูุงุ ูุณูุชู ุฅุฑุฌุงุน ูุชูุณุท  ุงุญุชูุงููุฉ ุงูุณุฌู ุงูุณุงูุจ ููู ุฑูุฒ ูุฎุณุงุฑุฉ. ููุน ุฐููุ ูุน ููุฌ ุงููุงูุฐุฉ ุงูููุฒููุฉุ ููุงู ุชุฏุงุฎู ูู ุงูุฑููุฒ ุงูุชู ููุฑุฑูุง ุฅูู ุงููููุฐุฌ ูู ูู ุชูุฑุงุฑ. ูุง ูุฑูุฏ ุชุถููู ุงุญุชูุงููุฉ ุงูุณุฌู ููุฑููุฒ ุงูุชู ูุชุนุงูู ูุนูุง ูุณูุงู ููุท ูู ุฎุณุงุฑุชูุงุ ูุฐุง ูููููุง ุชุนููู ูุฐู ุงูุฃูุฏุงู ุฅูู `-100` ุจุญูุซ ูุชู ุชุฌุงูููุง. ูููุง ููู ูู ูุซุงู ุนูู ููููุฉ ุงูููุงู ุจุฐูู ุจุฎุทูุฉ ุชุจูุบ `512`. ููุฐุง ูุนูู ุฃู ุงููููุฐุฌ ุณูููู ูุฏูู 512 ุฑูุฒูุง ุนูู ุงูุฃูู ููุณูุงู ุนูุฏ ุญุณุงุจ ุงูุงุญุชูุงููุฉ ุงูุดุฑุทูุฉ ูุฃู ุฑูุฒ ูุงุญุฏ (ุจุดุฑุท ุชููุฑ 512 ุฑูุฒูุง ุณุงุจููุง ูุชุงุญูุง ููุงุดุชูุงู).

```python
import torch
from tqdm import tqdm

max_length = model.config.n_positions
stride = 512
seq_len = encodings.input_ids.size(1)

nlls = []
prev_end_loc = 0
for begin_loc in tqdm(range(0, seq_len, stride)):
    end_loc = min(begin_loc + max_length, seq_len)
    trg_len = end_loc - prev_end_loc  # ูุฏ ุชููู ูุฎุชููุฉ ุนู ุงูุฎุทูุฉ ูู ุงูุญููุฉ ุงูุฃุฎูุฑุฉ
    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)
    target_ids = input_ids.clone()
    target_ids[:, :-trg_len] = -100

    with torch.no_grad():
        outputs = model(input_ids, labels=target_ids)

        # ูุชู ุญุณุงุจ ุงูุฎุณุงุฑุฉ ุจุงุณุชุฎุฏุงู CrossEntropyLoss ุงูุฐู ูููู ุจุงููุชูุณุท ุนูู ุงูุชุตูููุงุช ุงูุตุญูุญุฉ
        # ูุงุญุธ ุฃู ุงููููุฐุฌ ูุญุณุจ ุงูุฎุณุงุฑุฉ ุนูู trg_len - 1 ูู ุงูุชุตูููุงุช ููุทุ ูุฃูู ูุชุญูู ุฏุงุฎูููุง ุฅูู ุงููุณุงุฑ ุจูุงุณุทุฉ 1.
        neg_log_likelihood = outputs.loss

    nlls.append(neg_log_likelihood)

    prev_end_loc = end_loc
    if end_loc == seq_len:
        break

ppl = torch.exp(torch.stack(nlls).mean())
```

ูุนุฏ ุชุดุบูู ูุฐุง ูุน ุทูู ุงูุฎุทูุฉ ูุณุงูููุง ูุทูู ุงูุฅุฏุฎุงู ุงูุฃูุตู ูุนุงุฏู ูุงุณุชุฑุงุชูุฌูุฉ ุงููุงูุฐุฉ ุบูุฑ ุงูููุฒููุฉ ูุบูุฑ ุงููุซูู ุงูุชู ูุงูุดูุงูุง ุฃุนูุงู. ููููุง ุตุบุฑุช ุงูุฎุทูุฉุ ุฒุงุฏ ุงูุณูุงู ุงูุฐู ุณูุญุตู ุนููู ุงููููุฐุฌ ูู ุนูู ูู ุชูุจุคุ ููููุง ูุงูุช ุงูุชุนููุฏ ุงููุบูู ุงูููุจูุบ ุนููุง ุฃูุถู ุนุงุฏุฉู.

ุนูุฏูุง ูููู ุจุชุดุบูู ูุง ุณุจู ุจุงุณุชุฎุฏุงู `stride = 1024`ุ ุฃู ุจุฏูู ุชุฏุงุฎูุ ุชููู  ุฏุฑุฌุฉ ุงูุชุนููุฏ ุงููุบูู ุงููุงุชุฌุฉ ูู `19.44`ุ ููู ูุง ููุงุซู `19.93` ุงููุจูุบ ุนููุง ูู ูุฑูุฉ GPT-2. ูู ุฎูุงู ุงุณุชุฎุฏุงู `stride = 512` ูุจุงูุชุงูู ุงุณุชุฎุฏุงู ุฅุณุชุฑุงุชูุฌูุฉ ุงููุงูุฐุฉ ุงูููุฒููุฉุ ููุฎูุถ ูุฐุง ุฅูู `16.45`. ูุฐู ุงููุชูุฌุฉ ููุณุช ููุท ุฃูุถูุ ูููููุง ูุญุณูุจุฉ ุจุทุฑููุฉ ุฃูุฑุจ ุฅูู ุงูุชุญููู ุงูุชููุงุฆู ุงูุญูููู ูุงุญุชูุงููุฉ ุงูุชุณูุณู.