# ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¶Ø®Ù…Ø©

Ø¯ÙØ¹Øª Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¶Ø®Ù…Ø© (LLMs) ØªØ·Ø¨ÙŠÙ‚Ø§Øª ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµØŒ Ù…Ø«Ù„ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© ÙˆØ§Ø³ØªÙƒÙ…Ø§Ù„ Ø§Ù„Ø£ÙƒÙˆØ§Ø¯ØŒ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ§Ù„ÙŠ Ù…Ù† Ø®Ù„Ø§Ù„ Ø¥Ù†ØªØ§Ø¬ Ù†ØµÙˆØµ ØªØ¸Ù‡Ø± Ù…Ø³ØªÙˆÙ‰ Ø¹Ø§Ù„ÙŠÙ‹Ø§ Ù…Ù† Ø§Ù„ÙÙ‡Ù… ÙˆØ§Ù„Ø·Ù„Ø§Ù‚Ø©. ÙˆÙ„ÙƒÙ† Ù…Ø§ ÙŠØ¬Ø¹Ù„ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¶Ø®Ù…Ø© Ù‚ÙˆÙŠØ© - Ø£ÙŠ Ø­Ø¬Ù…Ù‡Ø§ - ÙŠØ·Ø±Ø­ Ø£ÙŠØ¶Ù‹Ø§ ØªØ­Ø¯ÙŠØ§Øª Ù„Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬.

Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ø¨Ø·ÙŠØ¡ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¶Ø®Ù…Ø© ÙŠØ¬Ø¨ Ø£Ù† ØªÙØ³ØªØ¯Ø¹Ù‰ Ø¨Ø´ÙƒÙ„ Ù…ØªÙƒØ±Ø± Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ù…Ø² Ø§Ù„ØªØ§Ù„ÙŠ. ØªØªØ²Ø§ÙŠØ¯ ØªØ³Ù„Ø³Ù„ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ù…Ø¹ ØªÙ‚Ø¯Ù… Ø§Ù„ØªÙˆÙ„ÙŠØ¯ØŒ Ø§Ù„Ø£Ù…Ø± Ø§Ù„Ø°ÙŠ ÙŠØ³ØªØºØ±Ù‚ ÙˆÙ‚ØªÙ‹Ø§ Ø£Ø·ÙˆÙ„ ÙˆØ£Ø·ÙˆÙ„ Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¶Ø®Ù…Ø© Ù„Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§. ØªÙ…ØªÙ„Ùƒ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¶Ø®Ù…Ø© Ø£ÙŠØ¶Ù‹Ø§ Ù…Ù„ÙŠØ§Ø±Ø§Øª Ù…Ù† Ø§Ù„Ù…Ø¹Ù„Ù…Ø§ØªØŒ Ù…Ù…Ø§ ÙŠØ¬Ø¹Ù„ Ù…Ù† Ø§Ù„ØµØ¹Ø¨ ØªØ®Ø²ÙŠÙ† ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø¬Ù…ÙŠØ¹ Ù‡Ø°Ù‡ Ø§Ù„Ø£ÙˆØ²Ø§Ù† ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©.

Ø³ÙŠÙˆØ¶Ø­ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙŠ Ù…ÙƒØªØ¨Ø© Transformers Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¶Ø®Ù…Ø©.

> [!TIP]
> ØªÙˆÙØ± Hugging Face Ø£ÙŠØ¶Ù‹Ø§ [Text Generation Inference (TGI)](https://hf.co/docs/text-generation-inference)ØŒ ÙˆÙ‡ÙŠ Ù…ÙƒØªØ¨Ø© Ù…Ø®ØµØµØ© Ù„Ù†Ø´Ø± ÙˆØ®Ø¯Ù…Ø© Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¶Ø®Ù…Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø© Ù„Ù„ØºØ§ÙŠØ© Ù„Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬. ØªØªØ¶Ù…Ù† Ù…ÙŠØ²Ø§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…ÙˆØ¬Ù‡Ø© Ù„Ù„Ù†Ø´Ø± ØºÙŠØ± Ø§Ù„Ù…Ø¯Ø±Ø¬Ø© ÙÙŠ Ù…ÙƒØªØ¨Ø© TransformersØŒ Ù…Ø«Ù„ Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙ…Ø± Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ© ÙˆÙ…ØªÙˆØ§Ø²ÙŠØ© Ø§Ù„ØªÙ†Ø³ÙˆØ± Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ù…ØªØ¹Ø¯Ø¯ ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª (GPU).

## Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ø«Ø§Ø¨ØªØ© Ù„Ù€ key-value Ùˆ `torch.compile`

Ø£Ø«Ù†Ø§Ø¡ ÙÙƒ Ø§Ù„ØªØ´ÙÙŠØ±ØŒ ÙŠØ­Ø³Ø¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¶Ø®Ù…Ø© Ù‚ÙŠÙ… key-value (kv) Ù„ÙƒÙ„ Ø±Ù…Ø² Ù…Ù† Ø±Ù…ÙˆØ² Ø§Ù„Ù…Ø¯Ø®Ù„Ø§ØªØŒ ÙˆØ¨Ù…Ø§ Ø£Ù†Ù‡ ØªÙ†Ø¨Ø¤ÙŠ Ø°Ø§ØªÙŠÙ‹Ø§ØŒ ÙØ¥Ù†Ù‡ ÙŠØ­Ø³Ø¨ Ù†ÙØ³ Ù‚ÙŠÙ… kv ÙÙŠ ÙƒÙ„ Ù…Ø±Ø© Ù„Ø£Ù† Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆÙ„Ø¯ ÙŠØµØ¨Ø­ Ø§Ù„Ø¢Ù† Ø¬Ø²Ø¡Ù‹Ø§ Ù…Ù† Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª. Ù‡Ø°Ø§ ØºÙŠØ± ÙØ¹Ø§Ù„ Ù„Ø£Ù†Ùƒ ØªÙ‚ÙˆÙ… Ø¨Ø¥Ø¹Ø§Ø¯Ø© Ø­Ø³Ø§Ø¨ Ù†ÙØ³ Ù‚ÙŠÙ… kv ÙÙŠ ÙƒÙ„ Ù…Ø±Ø©.

Ù„ØªØ­Ø³ÙŠÙ† Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù€ kv Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…ÙØ§ØªÙŠØ­ ÙˆØ§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø¥Ø¹Ø§Ø¯Ø© Ø­Ø³Ø§Ø¨Ù‡Ø§ ÙÙŠ ÙƒÙ„ Ù…Ø±Ø©. ÙˆÙ…Ø¹ Ø°Ù„ÙƒØŒ Ù†Ø¸Ø±Ù‹Ø§ Ù„Ø£Ù† Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù€ kv ØªÙ†Ù…Ùˆ Ù…Ø¹ ÙƒÙ„ Ø®Ø·ÙˆØ© Ù…Ù† Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯ ÙˆÙ‡ÙŠ Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ©ØŒ ÙØ¥Ù†Ù‡Ø§ ØªÙ…Ù†Ø¹Ùƒ Ù…Ù† Ø§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† [`torch.compile`](./perf_torch_compile)ØŒ ÙˆÙ‡ÙŠ Ø£Ø¯Ø§Ø© ØªØ­Ø³ÙŠÙ† Ù‚ÙˆÙŠØ© ØªÙ‚ÙˆÙ… Ø¨Ø¯Ù…Ø¬ ÙƒÙˆØ¯ PyTorch ÙÙŠ Ù†ÙˆØ§Ø© Ø³Ø±ÙŠØ¹Ø© ÙˆÙ…Ø­Ø³Ù†Ø©.

ØªØ¹Ø§Ù„Ø¬ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ø«Ø§Ø¨ØªØ© Ù„Ù€ kv Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ®ØµÙŠØµ Ø­Ø¬Ù… Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù€ kv Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ø¥Ù„Ù‰ Ù‚ÙŠÙ…Ø© Ù‚ØµÙˆÙ‰ØŒ Ù…Ù…Ø§ ÙŠØªÙŠØ­ Ù„Ùƒ Ø¯Ù…Ø¬Ù‡Ø§ Ù…Ø¹ `torch.compile` Ù„Ù„ØªØ³Ø±ÙŠØ¹ Ø¨Ù…Ù‚Ø¯Ø§Ø± 4 Ù…Ø±Ø§Øª. Ù‚Ø¯ ÙŠØ®ØªÙ„Ù ØªØ³Ø±ÙŠØ¹Ùƒ Ø§Ø¹ØªÙ…Ø§Ø¯Ù‹Ø§ Ø¹Ù„Ù‰ Ø­Ø¬Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ØªÙ…ØªÙ„Ùƒ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø£ÙƒØ¨Ø± ØªØ³Ø±ÙŠØ¹Ù‹Ø§ Ø£ØµØºØ±) ÙˆØ§Ù„Ø£Ø¬Ù‡Ø²Ø©.

> [!WARNING]
> Ø­Ø§Ù„ÙŠÙ‹Ø§ØŒ ØªØ¯Ø¹Ù… Ù†Ù…Ø§Ø°Ø¬ [Llama](./model_doc/llama2] ÙˆØ¨Ø¹Ø¶ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø£Ø®Ø±Ù‰ ÙÙ‚Ø· Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ø«Ø§Ø¨ØªØ© Ù„Ù€ kv Ùˆ`torch.compile`. ØªØ­Ù‚Ù‚ Ù…Ù† [Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©](https://github.com/huggingface/transformers/issues/28981) Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© ØªÙˆØ§ÙÙ‚ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø©.

Ù‡Ù†Ø§Ùƒ Ø«Ù„Ø§Ø«Ø© Ù†ÙƒÙ‡Ø§Øª Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ø«Ø§Ø¨ØªØ© Ù„Ù€ kvØŒ Ø§Ø¹ØªÙ…Ø§Ø¯Ù‹Ø§ Ø¹Ù„Ù‰ Ù…Ø¯Ù‰ ØªØ¹Ù‚ÙŠØ¯ Ù…Ù‡Ù…ØªÙƒ:
1. Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ: Ù‚Ù… Ø¨Ø¨Ø³Ø§Ø·Ø© Ø¨ØªØ¹ÙŠÙŠÙ† Ø¹Ù„Ø§Ù…Ø© ÙÙŠ `generation_config` (ÙŠÙˆØµÙ‰ Ø¨Ù‡Ø§)Ø›
2. Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù…: Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ ÙƒØ§Ø¦Ù† Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù„ØªÙˆÙ„ÙŠØ¯ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø£Ø¯ÙˆØ§Ø± Ø£Ùˆ Ø­Ù„Ù‚Ø© Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ø®ØµØµØ©Ø›
3. Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù…: Ù‚Ù… Ø¨ØªØ¬Ù…ÙŠØ¹ Ø¯Ø§Ù„Ø© `generate` Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§ ÙÙŠ Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ ÙˆØ§Ø­Ø¯ØŒ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙˆØ¬ÙˆØ¯ Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ ÙˆØ§Ø­Ø¯ Ø°ÙŠ ØµÙ„Ø© Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ùƒ.

Ø­Ø¯Ø¯ Ø¹Ù„Ø§Ù…Ø© Ø§Ù„ØªØ¨ÙˆÙŠØ¨ Ø§Ù„ØµØ­ÙŠØ­Ø© Ø£Ø¯Ù†Ø§Ù‡ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø­ÙˆÙ„ ÙƒÙ„ Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ù†ÙƒÙ‡Ø§Øª.

> [!TIP]
> Ø¨ØºØ¶ Ø§Ù„Ù†Ø¸Ø± Ø¹Ù† Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© Ù…Ø¹ `torch.compile`ØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¬Ù†Ø¨ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„Ø´ÙƒÙ„ Ø¥Ø°Ø§ Ù‚Ù…Øª Ø¨Ù…Ø­Ø§Ø°Ø§Ø© Ø¥Ø¯Ø®Ø§Ù„Ø§Øª Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¶Ø®Ù…Ø© Ø¥Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ø­Ø¯ÙˆØ¯Ø© Ù…Ù† Ø§Ù„Ù‚ÙŠÙ…. Ø¹Ù„Ù… Ø§Ù„ØªÙˆÙƒÙŠØ¯ [`pad_to_multiple_of`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__.pad_to_multiple_of) Ù‡Ùˆ ØµØ¯ÙŠÙ‚Ùƒ!
<hfoptions id="static-kv">
<hfoption id="basic usage: generation_config">

ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¯Ø¹Ù†Ø§ Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ [Gemma](https://hf.co/google/gemma-2b). ÙƒÙ„ Ù…Ø§ Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ÙØ¹Ù„Ù‡ Ù‡Ùˆ:
1. Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø³Ù…Ø© `generation_config` Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØªØ¹ÙŠÙŠÙ† `cache_implementation` Ø¥Ù„Ù‰ "static"Ø›
2. Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ `torch.compile` Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ¬Ù…ÙŠØ¹ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªÙ…Ø±ÙŠØ± Ù„Ù„Ø£Ù…Ø§Ù… Ù…Ø¹ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ø«Ø§Ø¨ØªØ© Ù„Ù€ kv.

ÙˆÙ‡Ø°Ø§ ÙƒÙ„ Ø´ÙŠØ¡!

```py
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"  # Ù„Ù…Ù†Ø¹ Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª Ø§Ù„Ø·ÙˆÙŠÙ„Ø© :)

tokenizer = AutoTokenizer.from_pretrained("google/gemma-2b")
model = AutoModelForCausalLM.from_pretrained("google/gemma-2b", device_map="auto")

model.generation_config.cache_implementation = "static"

model.forward = torch.compile(model.forward, mode="reduce-overhead", fullgraph=True)
input_text = "The theory of special relativity states "
input_ids = tokenizer(input_text, return_tensors="pt").to("cuda")

outputs = model.generate(**input_ids)
print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
['The theory of special relativity states 1. The speed of light is constant in all inertial reference']
```

ØªØ­Øª Ø§Ù„ØºØ·Ø§Ø¡ØŒ Ø³ØªØ­Ø§ÙˆÙ„ Ø¯Ø§Ù„Ø© `generate` Ø¥Ø¹Ø§Ø¯Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙƒØ§Ø¦Ù† Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù†ÙØ³Ù‡ØŒ Ù…Ù…Ø§ ÙŠØ²ÙŠÙ„ Ø§Ù„Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ¬Ù…ÙŠØ¹ ÙÙŠ ÙƒÙ„ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡. ØªØ¬Ù†Ø¨ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ø£Ù…Ø± Ø¨Ø§Ù„Øº Ø§Ù„Ø£Ù‡Ù…ÙŠØ© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ù‚ØµÙ‰ Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† `torch.compile`ØŒ ÙˆÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø¹Ù„Ù‰ Ø¯Ø±Ø§ÙŠØ© Ø¨Ù…Ø§ ÙŠÙ„ÙŠ:
1. Ø¥Ø°Ø§ ØªØºÙŠØ± Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø© Ø£Ùˆ Ø²Ø§Ø¯ Ø·ÙˆÙ„ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„Ø£Ù‚ØµÙ‰ Ø¨ÙŠÙ† Ø§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§ØªØŒ ÙØ³ÙŠØªØ¹ÙŠÙ† Ø¥Ø¹Ø§Ø¯Ø© ØªÙ‡ÙŠØ¦Ø© Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚ØªØŒ Ù…Ù…Ø§ ÙŠØ¤Ø¯ÙŠ Ø¥Ù„Ù‰ ØªØ´ØºÙŠÙ„ ØªØ¬Ù…ÙŠØ¹ Ø¬Ø¯ÙŠØ¯Ø›
2. ØªÙƒÙˆÙ† Ø£ÙˆÙ„Ù‰ Ø§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§Øª Ø§Ù„Ù‚Ù„ÙŠÙ„Ø© Ù„Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø¬Ù…Ø¹Ø© Ø£Ø¨Ø·Ø£ØŒ Ø­ÙŠØ« ÙŠØ¬Ø±ÙŠ ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø¯Ø§Ù„Ø©.

> [!WARNING]
> Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£ÙƒØ«Ø± ØªÙ‚Ø¯Ù…Ù‹Ø§ Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ø«Ø§Ø¨ØªØ©ØŒ Ù…Ø«Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø£Ø¯ÙˆØ§Ø±ØŒ Ù†ÙˆØµÙŠ Ø¨Ø¥Ù†Ø´Ø§Ø¡ ÙƒØ§Ø¦Ù† Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª ÙˆØ§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹Ù‡ Ø®Ø§Ø±Ø¬ [`~GenerationMixin.generate`]. Ø±Ø§Ø¬Ø¹ Ø¹Ù„Ø§Ù…Ø© Ø§Ù„ØªØ¨ÙˆÙŠØ¨ "Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù…".

</hfoption>
<hfoption id="advanced usage: control Static Cache">

ÙŠÙ…ÙƒÙ† ØªÙ…Ø±ÙŠØ± ÙƒØ§Ø¦Ù† [`StaticCache`] Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© [`~GenerationMixin.generate`] Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ Ø¥Ø·Ø§Ø± ÙˆØ³ÙŠØ· `past_key_values`. Ø³ÙŠØ­ØªÙØ¸ ÙƒØ§Ø¦Ù† Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø¨Ù…Ø­ØªÙˆÙŠØ§Øª Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚ØªØŒ Ù„Ø°Ø§ ÙŠÙ…ÙƒÙ†Ùƒ ØªÙ…Ø±ÙŠØ±Ù‡ Ø¥Ù„Ù‰ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø¬Ø¯ÙŠØ¯ Ù„Ù€ [`~GenerationMixin.generate`] Ù„Ù…ÙˆØ§ØµÙ„Ø© Ø§Ù„ØªÙˆÙ„ÙŠØ¯ØŒ ÙƒÙ…Ø§ ØªÙØ¹Ù„ Ù…Ø¹ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ©.

```py
from transformers import AutoTokenizer, AutoModelForCausalLM, StaticCache
import torch
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"  # Ù„Ù…Ù†Ø¹ Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª Ø§Ù„Ø·ÙˆÙŠÙ„Ø© :)

tokenizer = AutoTokenizer.from_pretrained("google/gemma-2b")
model = AutoModelForCausalLM.from_pretrained("google/gemma-2b", device_map="auto")

model.forward = torch.compile(model.forward, mode="reduce-overhead", fullgraph=True)
input_text = "The theory of special relativity states "
input_ids = tokenizer(input_text, return_tensors="pt").to("cuda")
prompt_length = input_ids.input_ids.shape[1]
model.generation_config.max_new_tokens = 16

past_key_values = StaticCache(
    config=model.config,
    max_batch_size=1,
    # Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ®Ø·Ø· Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚ØªØŒ ÙØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø·ÙˆÙ„ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª ÙƒØ¨ÙŠØ± Ø¨Ù…Ø§ ÙŠÙƒÙÙŠ Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø­Ø§Ù„Ø§Øª
max_cache_len=prompt_length+(model.generation_config.max_new_tokens*2),
    device=model.device,
    dtype=model.dtype
)
outputs = model.generate(**input_ids, past_key_values=past_key_values)
print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
['The theory of special relativity states 1. The speed of light is constant in all inertial reference frames. 2']

# Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙˆÙ„Ø¯ ÙˆÙ†ÙØ³ ÙƒØ§Ø¦Ù† Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù„Ø§Ø³ØªÙ…Ø±Ø§Ø± ÙÙŠ Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ù† Ø­ÙŠØ« ØªÙˆÙ‚Ù. Ø§Ø®ØªÙŠØ§Ø±ÙŠÙ‹Ø§ØŒ ÙÙŠ
# Ù…Ø­Ø§Ø¯Ø«Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø£Ø¯ÙˆØ§Ø±ØŒ Ø£Ø¶Ù Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ø¥Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙˆÙ„Ø¯.
new_input_ids = outputs
outputs = model.generate(new_input_ids, past_key_values=past_key_values)
print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
['The theory of special relativity states 1. The speed of light is constant in all inertial reference frames. 2. The speed of light is constant in all inertial reference frames. 3.']
```
> [!TIP]
> Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ±ÙŠØ¯ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†ÙØ³ ÙƒØ§Ø¦Ù† [`StaticCache`] Ø¹Ù„Ù‰ Ù…ÙˆØ¬Ù‡ Ø¬Ø¯ÙŠØ¯ØŒ ÙØªØ£ÙƒØ¯ Ù…Ù† Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ù…Ø­ØªÙˆÙŠØ§ØªÙ‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© `.reset()` Ø¨ÙŠÙ† Ø§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§Øª

Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ±ÙŠØ¯ Ø§Ù„Ø°Ù‡Ø§Ø¨ Ø¥Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø£Ø¹Ù…Ù‚ØŒ ÙÙŠÙ…ÙƒÙ† Ø£ÙŠØ¶Ù‹Ø§ ØªÙ…Ø±ÙŠØ± ÙƒØ§Ø¦Ù† [`StaticCache`] Ø¥Ù„Ù‰ ØªÙ…Ø±ÙŠØ± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø£Ù…Ø§Ù… ÙÙŠ Ø¥Ø·Ø§Ø± ÙˆØ³ÙŠØ· `past_key_values` Ù†ÙØ³Ù‡. Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ù‡ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©ØŒ ÙŠÙ…ÙƒÙ†Ùƒ ÙƒØªØ§Ø¨Ø© Ø¯Ø§Ù„ØªÙƒ Ø§Ù„Ø®Ø§ØµØ© Ù„ÙÙƒ ØªØ´ÙÙŠØ± Ø§Ù„Ø±Ù…Ø² Ø§Ù„ØªØ§Ù„ÙŠ Ù†Ø¸Ø±Ù‹Ø§ Ù„Ù„Ø±Ù…Ø² Ø§Ù„Ø­Ø§Ù„ÙŠ ÙˆØ§Ù„Ù…ÙˆØ¶Ø¹ ÙˆÙ…ÙˆØ¶Ø¹ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…ÙˆÙ„Ø¯Ø© Ø³Ø§Ø¨Ù‚Ù‹Ø§.

```py
from transformers import LlamaTokenizer, LlamaForCausalLM, StaticCache, logging
from transformers.testing_utils import CaptureLogger
import torch

prompts = [
    "Simply put, the theory of relativity states that ",
    "My favorite all time favorite condiment is ketchup.",
]

NUM_TOKENS_TO_GENERATE = 40
torch_device = "cuda"

tokenizer = LlamaTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf", pad_token="</s>", padding_side="right")
model = LlamaForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf", device_map="sequential")
inputs = tokenizer(prompts, return_tensors="pt", padding=True).to(model.device)

def decode_one_tokens(model, cur_token, input_pos, cache_position, past_key_values):
    logits = model(
        cur_token,
        position_ids=input_pos,
        cache_position=cache_position,
        past_key_values=past_key_values,
        return_dict=False,
        use_cache=True
    )[0]
    new_token = torch.argmax(logits[:, -1], dim=-1)[:, None]
    return new_token
```

Ù‡Ù†Ø§Ùƒ Ø¨Ø¹Ø¶ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù„ØªÙŠ ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ Ø§Ù„Ù‚ÙŠØ§Ù… Ø¨Ù‡Ø§ Ù„ØªÙ…ÙƒÙŠÙ† Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ø«Ø§Ø¨ØªØ© Ù„Ù€ kv Ùˆ`torch.compile` Ù…Ø¹ Ø·Ø±ÙŠÙ‚Ø© `StaticCache`:
1. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ [`StaticCache`] Ù‚Ø¨Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬. Ù‡Ù†Ø§Ùƒ ÙŠÙ…ÙƒÙ†Ùƒ ØªÙƒÙˆÙŠÙ† Ù…Ø¹Ù„Ù…Ø§Øª Ù…Ø«Ù„ Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø© Ø§Ù„Ù‚ØµÙˆÙ‰ ÙˆØ·ÙˆÙ„ Ø§Ù„ØªØ³Ù„Ø³Ù„.
2. Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ `torch.compile` Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ¬Ù…ÙŠØ¹ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªÙ…Ø±ÙŠØ± Ù„Ù„Ø£Ù…Ø§Ù… Ù…Ø¹ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ø«Ø§Ø¨ØªØ© Ù„Ù€ kv.
3. Ù‚Ù… Ø¨ØªØ¹ÙŠÙŠÙ† `enable_math=True` ÙÙŠ Ø³ÙŠØ§Ù‚ [torch.backends.cuda.sdp_kernel](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html) Ù„ØªÙ…ÙƒÙŠÙ† ØªÙ†ÙÙŠØ° C++ Ø§Ù„Ø£ØµÙ„ÙŠ Ù„Ù€ PyTorch Ù„Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø¨Ù…Ù†ØªØ¬ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù…Ø­Ø¯Ø¯ Ù„Ø­Ø¬Ù…Ù‡Ø§ Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø£ÙƒØ«Ø±.

```py
batch_size, seq_length = inputs["input_ids"].shape
with torch.no_grad():
    past_key_values = StaticCache(
        config=model.config, max_batch_size=2, max_cache_len=4096, device=torch_device, dtype=model.dtype
    )
    cache_position = torch.arange(seq_length, device=torch_device)
    generated_ids = torch.zeros(
        batch_size, seq_length + NUM_TOKENS_TO_GENERATE + 1, dtype=torch.int, device=torch_device
    )
    generated_ids[:, cache_position] = inputs["input_ids"].to(torch_device).to(torch.int)

    logits = model(
        **inputs, cache_position=cache_position, past_key_values=past_key_values,return_dict=False, use_cache=True
    )[0]
    next_token = torch.argmax(logits[:, -1], dim=-1)[:, None]
    generated_ids[:, seq_length] = next_token[:, 0]

    decode_one_tokens = torch.compile(decode_one_tokens, mode="reduce-overhead", fullgraph=True)
    cache_position = torch.tensor([seq_length + 1], device=torch_device)
    for _ in range(1, NUM_TOKENS_TO_GENERATE):
        with torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True):
            next_token = decode_one_tokens(model, next_token.clone(), None, cache_position, past_key_values)
            generated_ids[:, cache_position] = next_token.int()
        cache_position += 1

text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
text
['Simply put, the theory of relativity states that 1) the speed of light is constant, 2) the speed of light is the same for all observers, and 3) the laws of physics are the same for all observers.',
 'My favorite all time favorite condiment is ketchup. I love it on everything. I love it on my eggs, my fries, my chicken, my burgers, my hot dogs, my sandwiches, my salads, my p']
```

</hfoption>
<hfoption id="advanced usage: end-to-end generate compilation">
</hfoption>
<hfoption id="advanced usage: end-to-end generate compilation">

Ù…Ù† Ø­ÙŠØ« Ø§Ù„ÙƒÙˆØ¯ØŒ ÙØ¥Ù† ØªØ¬Ù…ÙŠØ¹ Ø¯Ø§Ù„Ø© `generate` Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§ Ø£Ø¨Ø³Ø· Ø­ØªÙ‰ Ù…Ù† Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ: Ù‚Ù… Ø¨Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ `torch.compile` Ø¹Ù„Ù‰ `generate` Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø¯Ø§Ù„Ø© Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§. Ù„Ø§ ÙŠÙ„Ø²Ù… ØªØ­Ø¯ÙŠØ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ø«Ø§Ø¨ØªØ©: Ø¹Ù„Ù‰ Ø§Ù„Ø±ØºÙ… Ù…Ù† Ø£Ù†Ù‡Ø§ Ù…ØªÙˆØ§ÙÙ‚Ø©ØŒ Ø¥Ù„Ø§ Ø£Ù† Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ© (Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©) ÙƒØ§Ù†Øª Ø£Ø³Ø±Ø¹ ÙÙŠ Ù…Ù‚Ø§ÙŠÙŠØ³Ù†Ø§.

```py
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"  # Ù„Ù…Ù†Ø¹ Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª Ø§Ù„Ø·ÙˆÙŠÙ„Ø© :)

tokenizer = AutoTokenizer.from_pretrained("google/gemma-2b")
model = AutoModelForCausalLM.from_pretrained("google/gemma-2b", device_map="auto")

model.generate = torch.compile(model.generate, mode="reduce-overhead", fullgraph=True)
input_text = "The theory of special relativity states "
input_ids = tokenizer(input_text, return_tensors="pt").to("cuda")

outputs = model.generate(**input_ids)
print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
['The theory of special relativity states 1. The speed of light is constant in all inertial reference']
```

ÙˆÙ†ØªÙŠØ¬Ø© Ù„Ø°Ù„ÙƒØŒ ÙØ¥Ù†Ù†Ø§ Ù„Ø§ Ù†Ù‚ÙˆÙ… Ø¨ØªØ¬Ù…ÙŠØ¹ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªÙ…Ø±ÙŠØ± Ù„Ù„Ø£Ù…Ø§Ù… Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ ÙØ­Ø³Ø¨ØŒ Ø¨Ù„ Ù†Ù‚ÙˆÙ… Ø£ÙŠØ¶Ù‹Ø§ Ø¨ØªØ¬Ù…ÙŠØ¹ Ø¬Ù…ÙŠØ¹ Ø¹Ù…Ù„ÙŠØ§Øª Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ ÙˆØ¹Ù…Ù„ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø±Ù…ÙˆØ² ÙˆÙ…Ø§ Ø¥Ù„Ù‰ Ø°Ù„Ùƒ. ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ù†Ø§ØªØ¬ Ù„Ù€ `generate` Ø£Ø¨Ø·Ø£ Ù‚Ù„ÙŠÙ„Ø§Ù‹ Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨Ù…Ø«Ø§Ù„ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØŒ ÙˆÙ‚Ø¯ ÙŠÙƒÙˆÙ† Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ Ø§Ù„Ù…Ø¬Ù…Ø¹ Ø£ÙƒØ«Ø± Ù…Ù„Ø§Ø¡Ù…Ø© Ù„Ø£Ø¬Ù‡Ø²Ø© Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© Ø£Ùˆ Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£ÙƒØ«Ø± ØºØ±Ø§Ø¨Ø©. ÙˆÙ…Ø¹ Ø°Ù„ÙƒØŒ Ù‡Ù†Ø§Ùƒ Ø¹ÙŠÙˆØ¨ Ø´Ø¯ÙŠØ¯Ø© ÙÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ø§ Ø§Ù„Ù†Ù‡Ø¬:
1. Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ø£Ø¨Ø·Ø£ Ø¨ÙƒØ«ÙŠØ±Ø›
2. ÙŠØ¬Ø¨ Ø¥Ø¬Ø±Ø§Ø¡ Ø¬Ù…ÙŠØ¹ Ù…Ø¹Ù„Ù…Ø§Øª `generate` Ù…Ù† Ø®Ù„Ø§Ù„ `generation_config`Ø›
3. ÙŠØªÙ… Ù‚Ù…Ø¹ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª ÙˆØ§Ù„Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª - Ù†Ù‚ØªØ±Ø­ Ø¹Ù„ÙŠÙƒ Ø§Ø®ØªØ¨Ø§Ø±Ù‡ Ø£ÙˆÙ„Ø§Ù‹ Ø¨Ø´ÙƒÙ„ ØºÙŠØ± Ù…Ø¬Ù…Ø¹Ø›
4. Ø¹Ù„Ù‰ Ø§Ù„Ø±ØºÙ… Ù…Ù† Ø£Ù†Ù†Ø§ Ù†Ø¹Ù…Ù„ Ø¹Ù„ÙŠÙ‡ØŒ Ø¥Ù„Ø§
## ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡

Ù‡Ù†Ø§Ùƒ Ù…Ø´ÙƒÙ„Ø© Ù…Ø¹Ø±ÙˆÙØ© ÙÙŠ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª ÙˆÙ‡ÙŠ Ø£Ù† Ø¢Ù„ÙŠØ© Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ø§Ù„Ø°Ø§ØªÙŠ ØªÙ†Ù…Ùˆ Ø¨Ø´ÙƒÙ„ ØªØ±Ø¨ÙŠØ¹ÙŠ ÙÙŠ Ø§Ù„Ø­Ø³Ø§Ø¨ ÙˆØ§Ù„Ø°Ø§ÙƒØ±Ø© Ù…Ø¹ Ø¹Ø¯Ø¯ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…Ù…ÙŠØ²Ø© Ù„Ù„Ø¥Ø¯Ø®Ø§Ù„. ÙŠØªÙ… ØªØ¶Ø®ÙŠÙ… Ù‡Ø°Ø§ Ø§Ù„Ù‚ÙŠØ¯ ÙÙ‚Ø· ÙÙŠ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø§Ù„ØªÙŠ ØªØªØ¹Ø§Ù…Ù„ Ù…Ø¹ ØªØ³Ù„Ø³Ù„Ø§Øª Ø£Ø·ÙˆÙ„. Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù‡Ø°Ø§ Ø§Ù„Ø£Ù…Ø±ØŒ Ø¬Ø±Ø¨ FlashAttention2 Ø£Ùˆ Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ù…ÙØ­Ø³ÙÙ‘Ù† Ù„Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ø§Ù„Ù…ÙÙˆØ²ÙÙ‘Ø¹ Ø§Ù„Ù…ÙÙ‚ÙÙŠÙÙ‘Ø¯ Ø§Ù„Ù…ÙÙ‚ÙØ¯ÙÙ‘Ù… ÙÙŠ PyTorchØŒ ÙˆØ§Ù„Ù„Ø°Ø§Ù† ÙŠÙØ¹Ø¯ÙÙ‘Ø§Ù† Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙØ³Ø±ÙÙ‘Ø¹Ø§ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬.

### FlashAttention-2

ÙŠÙ‚Ø³Ù… FlashAttention Ùˆ [FlashAttention-2](./perf_infer_gpu_one#flashattention-2) Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ Ø£ØµØºØ± ÙˆÙŠÙ‚Ù„Ù„ Ø¹Ø¯Ø¯ Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©/Ø§Ù„ÙƒØªØ§Ø¨Ø© Ø§Ù„ÙˆØ³ÙŠØ·Ø© Ø¥Ù„Ù‰ Ø°Ø§ÙƒØ±Ø© GPU Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬. ÙˆÙŠØ­Ø³Ù† FlashAttention-2 Ù…Ù† Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© FlashAttention Ø§Ù„Ø£ØµÙ„ÙŠØ© Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ù…ÙˆØ§Ø²Ø§Ø© Ø£ÙŠØ¶Ù‹Ø§ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¯ Ø·ÙˆÙ„ Ø§Ù„ØªØ³Ù„Ø³Ù„ ÙˆØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ù†ÙÙ‚Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ù„Ø§ØªØµØ§Ù„ ÙˆØ§Ù„ØªØ²Ø§Ù…Ù†.

Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… FlashAttention-2ØŒ Ù‚Ù… Ø¨ØªØ¹ÙŠÙŠÙ† `attn_implementation="flash_attention_2"` ÙÙŠ Ø·Ø±ÙŠÙ‚Ø© [`~PreTrainedModel.from_pretrained`] .

```py
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

quant_config = BitsAndBytesConfig(load_in_8bit=True)
model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-2b",
    quantization_config=quant_config,
    torch_dtype=torch.bfloat16,
    attn_implementation="flash_attention_2",
)
```

### Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ø§Ù„Ù…ÙˆØ²Ø¹ Ø§Ù„Ù…ÙÙ‚ÙÙŠÙÙ‘Ø¯ Ø§Ù„Ù…ÙÙ‚ÙØ¯ÙÙ‘Ù… ÙÙŠ PyTorch

ÙŠØªÙ… ØªÙ…ÙƒÙŠÙ† Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ø§Ù„Ù…ÙˆØ²Ø¹ Ø§Ù„Ù…ÙÙ‚ÙÙŠÙÙ‘Ø¯ Ø§Ù„Ù…ÙÙ‚ÙØ¯ÙÙ‘Ù… ÙÙŠ PyTorch 2.0 Ø§ÙØªØ±Ø§Ø¶ÙŠÙ‹Ø§ ÙˆÙ‡Ùˆ ÙŠØ¯Ø¹Ù… FlashAttention Ùˆ xFormers ÙˆØªÙ†ÙÙŠØ° PyTorch ÙÙŠ C++. ÙŠØ®ØªØ§Ø± Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ø§Ù„Ù…ÙˆØ²Ø¹ Ø§Ù„Ù…ÙÙ‚ÙÙŠÙÙ‘Ø¯ Ø§Ù„Ù…ÙÙ‚ÙØ¯ÙÙ‘Ù… ÙÙŠ PyTorch Ø£ÙƒØ«Ø± Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ø£Ø¯Ø§Ø¡Ù‹ Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ³ØªØ®Ø¯Ù… backend CUDA. ÙˆØ¨Ø§Ù„Ù†Ø³Ø¨Ø© Ø¥Ù„Ù‰ backends Ø§Ù„Ø£Ø®Ø±Ù‰ØŒ ÙØ¥Ù† Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ø§Ù„Ù…ÙˆØ²Ø¹ Ø§Ù„Ù…ÙÙ‚ÙÙŠÙÙ‘Ø¯ Ø§Ù„Ù…ÙÙ‚ÙØ¯ÙÙ‘Ù… ÙÙŠ PyTorch ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù„Ù€ C++.

> [!TIP]
> ÙŠØ¯Ø¹Ù… Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ø§Ù„Ù…ÙˆØ²Ø¹ Ø§Ù„Ù…ÙÙ‚ÙÙŠÙÙ‘Ø¯ Ø§Ù„Ù…ÙÙ‚ÙØ¯ÙÙ‘Ù… ÙÙŠ PyTorch FlashAttention-2 Ø·Ø§Ù„Ù…Ø§ Ø£Ù† Ù„Ø¯ÙŠÙƒ Ø£Ø­Ø¯Ø« Ø¥ØµØ¯Ø§Ø± Ù…Ù† PyTorch Ù…Ø«Ø¨ØªÙ‹Ø§.

Ø§Ø³ØªØ®Ø¯Ù… Ù…Ø¯ÙŠØ± Ø³ÙŠØ§Ù‚ [torch.backends.cuda.sdp_kernel](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html) Ù„ØªÙ…ÙƒÙŠÙ† Ø£Ùˆ ØªØ¹Ø·ÙŠÙ„ Ø£ÙŠ Ù…Ù† Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ø§Ù„Ø«Ù„Ø§Ø« Ø¨Ø´ÙƒÙ„ ØµØ±ÙŠØ­. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ù‚Ù… Ø¨ØªØ¹ÙŠÙŠÙ† `enable_flash=True` Ù„ØªÙ…ÙƒÙŠÙ† FlashAttention.

```py
import torch
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-2b",
    torch_dtype=torch.bfloat16,
)

with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):
    outputs = model.generate(**inputs)
```

## Ø§Ù„ØªÙƒÙ…ÙŠÙ…

ÙŠÙ‚Ù„Ù„ Ø§Ù„ØªÙƒÙ…ÙŠÙ… Ù…Ù† Ø­Ø¬Ù… Ø£ÙˆØ²Ø§Ù† Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ®Ø²ÙŠÙ†Ù‡Ø§ ÙÙŠ Ø¯Ù‚Ø© Ø£Ù‚Ù„. ÙˆÙ‡Ø°Ø§ ÙŠØªØ±Ø¬Ù… Ø¥Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø°Ø§ÙƒØ±Ø© Ø£Ù‚Ù„ ÙˆÙŠØ¬Ø¹Ù„ ØªØ­Ù…ÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ù„Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø£ÙƒØ«Ø± Ø³Ù‡ÙˆÙ„Ø© Ø¥Ø°Ø§ ÙƒÙ†Øª Ù…Ù‚ÙŠØ¯Ù‹Ø§ Ø¨Ø°Ø§ÙƒØ±Ø© GPU Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ. Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…Ø­Ø¯ÙˆØ¯Ù‹Ø§ Ø¨Ù€ GPU Ø§Ù„Ø®Ø§Øµ Ø¨ÙƒØŒ ÙÙ„Ø§ ÙŠÙ„Ø²Ù… Ø¨Ø§Ù„Ø¶Ø±ÙˆØ±Ø© ØªÙƒÙ…ÙŠÙ… Ù†Ù…ÙˆØ°Ø¬Ùƒ Ù„Ø£Ù†Ù‡ Ù‚Ø¯ ÙŠØªÙƒØ¨Ø¯ ØªÙƒÙ„ÙØ© ØµØºÙŠØ±Ø© ÙÙŠ Ø§Ù„ÙƒÙ…ÙˆÙ† (Ø¨Ø§Ø³ØªØ«Ù†Ø§Ø¡ ÙˆØ­Ø¯Ø§Øª AWQ Ùˆ AWQ Ø§Ù„Ù…Ø¯Ù…Ø¬Ø©) Ø¨Ø³Ø¨Ø¨ Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ÙƒÙ…Ù…Ø© ÙˆØ¥Ù„ØºØ§Ø¡ ÙƒÙ…Ù…Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù†.

> [!TIP]
> Ù‡Ù†Ø§Ùƒ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„ØªÙƒÙ…ÙŠÙ… (Ø±Ø§Ø¬Ø¹ Ø¯Ù„ÙŠÙ„ [Quantization](./quantization) Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„) Ø§Ù„Ù…ØªØ§Ø­Ø©ØŒ Ù…Ø«Ù„ Quanto Ùˆ AQLM Ùˆ AWQ Ùˆ AutoGPTQ. Ù„Ø§ ØªØªØ±Ø¯Ø¯ ÙÙŠ ØªØ¬Ø±Ø¨ØªÙ‡Ø§ ÙˆØ´Ø§Ù‡Ø¯ Ø£ÙŠÙ‡Ø§ ÙŠØ¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„ Ù„Ø­Ø§Ù„ØªÙƒ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…ÙŠØ©. Ù†ÙˆØµÙŠ Ø£ÙŠØ¶Ù‹Ø§ Ø¨Ù‚Ø±Ø§Ø¡Ø© Ù…Ù†Ø´ÙˆØ± Ø§Ù„Ù…Ø¯ÙˆÙ†Ø© [Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø© Ø¹Ù„Ù‰ Ù…Ø®Ø·Ø·Ø§Øª Ø§Ù„ØªÙƒÙ…ÙŠÙ… Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø© Ø£ØµÙ„Ø§Ù‹ ÙÙŠ ğŸ¤— Transformers](https://hf.co/blog/overview-quantization-transformers) Ø§Ù„Ø°ÙŠ ÙŠÙ‚Ø§Ø±Ù† AutoGPTQ Ùˆ bitsandbytes.

Ø§Ø³ØªØ®Ø¯Ù… Ø¢Ù„Ø© Ø­Ø§Ø³Ø¨Ø© Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£Ø¯Ù†Ø§Ù‡ Ù„ØªÙ‚Ø¯ÙŠØ± ÙˆÙ…Ù‚Ø§Ø±Ù†Ø© Ù…Ù‚Ø¯Ø§Ø± Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¬Ø±Ø¨ ØªÙ‚Ø¯ÙŠØ± Ù…Ù‚Ø¯Ø§Ø± Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªÙŠ ÙŠØªÙƒÙ„ÙÙ‡Ø§ ØªØ­Ù…ÙŠÙ„ [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1).

<iframe
	src="https://hf-accelerate-model-memory-usage.hf.space"
	frameborder="0"
	width="850"
	height="450"
></iframe>

Ù„ØªØ­Ù…ÙŠÙ„ Mistral-7B-v0.1 Ø¨Ù†ØµÙ Ø§Ù„Ø¯Ù‚Ø©ØŒ Ù‚Ù… Ø¨ØªØ¹ÙŠÙŠÙ† Ù…Ø¹Ù„Ù…Ø© `torch_dtype` ÙÙŠ Ø·Ø±ÙŠÙ‚Ø© [`~transformers.AutoModelForCausalLM.from_pretrained`] Ø¥Ù„Ù‰ `torch.bfloat16`. ÙŠØªØ·Ù„Ø¨ Ù‡Ø°Ø§ 13.74 Ø¬ÙŠØ¬Ø§Ø¨Ø§ÙŠØª Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©.

```py
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-v0.1", torch_dtype=torch.bfloat16, device_map="auto",
)
```


Ù„ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙƒÙ…ÙŠ (8 Ø¨Øª Ø£Ùˆ 4 Ø¨Øª) Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ØŒ Ø¬Ø±Ø¨ [bitsandbytes](https://hf.co/docs/bitsandbytes) ÙˆÙ‚Ù… Ø¨ØªØ¹ÙŠÙŠÙ† Ù…Ø¹Ù„Ù…Ø§Øª "load_in_4bit" Ø£Ùˆ "load_in_8bit" Ø¥Ù„Ù‰ "True". ÙŠØªØ·Ù„Ø¨ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ 8 Ø¨ØªØ§Øª ÙÙ‚Ø· 6.87 Ø¬ÙŠØ¬Ø§Ø¨Ø§ÙŠØª Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©.

```py
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch

quant_config = BitsAndBytesConfig(load_in_8bit=True)
model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-v0.1", quantization_config=quant_config, device_map="auto"
)
```