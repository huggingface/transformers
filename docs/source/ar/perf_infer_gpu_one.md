# Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ­Ø¯Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª (GPU)

ØªØ¹Ø¯ ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª (GPU) Ø§Ù„Ø®ÙŠØ§Ø± Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠ Ù„Ø£Ø¬Ù‡Ø²Ø© Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠØŒ Ø¹Ù„Ù‰ Ø¹ÙƒØ³ ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø±ÙƒØ²ÙŠØ© (CPU)ØŒ Ù„Ø£Ù†Ù‡Ø§ Ù…ÙØ­ÙØ³ÙÙ‘Ù†Ø© Ù„Ø¹Ø±Ø¶ Ù†Ø·Ø§Ù‚ Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ§Ù„ØªÙˆØ§Ø²ÙŠ. ÙˆÙ„Ù…ÙˆØ§ÙƒØ¨Ø© Ø§Ù„Ø£Ø­Ø¬Ø§Ù… Ø§Ù„Ø£ÙƒØ¨Ø± Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø­Ø¯ÙŠØ«Ø© Ø£Ùˆ Ù„ØªØ´ØºÙŠÙ„ Ù‡Ø°Ù‡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙˆØ§Ù„Ù‚Ø¯ÙŠÙ…Ø©ØŒ Ù‡Ù†Ø§Ùƒ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ­Ø¯Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª. ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ØŒ Ø³ØªØªØ¹Ù„Ù… ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… FlashAttention-2 (Ø¢Ù„ÙŠØ© Ø§Ù‡ØªÙ…Ø§Ù… Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© ÙÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©)ØŒ ÙˆBetterTransformer (Ù…Ø³Ø§Ø± Ø³Ø±ÙŠØ¹ Ù„Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø£ØµÙ„ÙŠ ÙÙŠ PyTorch)ØŒ Ùˆbitsandbytes Ù„Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø¥Ù„Ù‰ Ø¯Ù‚Ø© Ø£Ù‚Ù„. ÙˆØ£Ø®ÙŠØ±Ù‹Ø§ØŒ ØªØ¹Ù„Ù… ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… ğŸ¤— Optimum Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ONNX Runtime Ø¹Ù„Ù‰ ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª Nvidia ÙˆAMD.

<Tip>

ØªÙ†Ø·Ø¨Ù‚ Ù…Ø¹Ø¸Ù… Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ù…ÙˆØ¶Ø­Ø© Ù‡Ù†Ø§ Ø£ÙŠØ¶Ù‹Ø§ Ø¹Ù„Ù‰ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©!

</Tip>

## FlashAttention-2

<Tip>

FlashAttention-2 ØªØ¬Ø±ÙŠØ¨ÙŠ ÙˆÙ‚Ø¯ ÙŠØªØºÙŠØ± Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ± ÙÙŠ Ø§Ù„Ø¥ØµØ¯Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©.

</Tip>

[FlashAttention-2](https://huggingface.co/papers/2205.14135) Ù‡Ùˆ ØªÙ†ÙÙŠØ° Ø£Ø³Ø±Ø¹ ÙˆØ£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© Ù„Ø¢Ù„ÙŠØ© Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠØ© Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† Ø£Ù† ØªØ³Ø±Ø¹ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ± Ù…Ù† Ø®Ù„Ø§Ù„:

1. Ù…ÙˆØ§Ø²Ø§Ø© Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø¨Ø´ÙƒÙ„ Ø¥Ø¶Ø§ÙÙŠ Ø¹Ø¨Ø± Ø·ÙˆÙ„ Ø§Ù„ØªØ³Ù„Ø³Ù„
2. ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¹Ù…Ù„ Ø¨ÙŠÙ† Ø®ÙŠÙˆØ· ÙˆØ­Ø¯Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„ØªÙˆØ§ØµÙ„ ÙˆÙ‚Ø±Ø§Ø¡Ø§Øª/ÙƒØªØ§Ø¨Ø§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø´ØªØ±ÙƒØ© Ø¨ÙŠÙ†Ù‡Ø§
ØªØ¯Ø¹Ù… FlashAttention-2 Ø­Ø§Ù„ÙŠÙ‹Ø§ Ø§Ù„Ø¨Ù†Ù‰ Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ© Ø§Ù„ØªØ§Ù„ÙŠØ©:
* [Bark](https://huggingface.co/docs/transformers/model_doc/bark#transformers.BarkModel)
* [Bart](https://huggingface.co/docs/transformers/model_doc/bart#transformers.BartModel)
* [Chameleon](https://huggingface.co/docs/transformers/model_doc/chameleon#transformers.Chameleon)
* [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPModel)
* [Cohere](https://huggingface.co/docs/transformers/model_doc/cohere#transformers.CohereModel)
* [Dbrx](https://huggingface.co/docs/transformers/model_doc/dbrx#transformers.DbrxModel)
* [DistilBert](https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertModel)
* [Gemma](https://huggingface.co/docs/transformers/model_doc/gemma#transformers.GemmaModel)
* [Gemma2](https://huggingface.co/docs/transformers/model_doc/gemma2#transformers.Gemma2Model)
* [GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)
* [GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode#transformers.GPTBigCodeModel)
* [GPTNeo](https://huggingface.co/docs/transformers/model_doc/gpt_neo#transformers.GPTNeoModel)
* [GPTNeoX](https://huggingface.co/docs/transformers/model_doc/gpt_neox#transformers.GPTNeoXModel)
* [GPT-J](https://huggingface.co/docs/transformers/model_doc/gptj#transformers.GPTJModel)
* [Idefics2](https://huggingface.co/docs/transformers/model_doc/idefics2#transformers.Idefics2Model)
* [Falcon](https://huggingface.co/docs/transformers/model_doc/falcon#transformers.FalconModel)
* [JetMoe](https://huggingface.co/docs/transformers/model_doc/jetmoe#transformers.JetMoeModel)
* [Jamba](https://huggingface.co/docs/transformers/model_doc/jamba#transformers.JambaModel)
* [Llama](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaModel)
* [Llava](https://huggingface.co/docs/transformers/model_doc/llava)
* [Llava-NeXT](https://huggingface.co/docs/transformers/model_doc/llava_next)
* [Llava-NeXT-Video](https://huggingface.co/docs/transformers/model_doc/llava_next_video)
* [VipLlava](https://huggingface.co/docs/transformers/model_doc/vipllava)
* [VideoLlava](https://huggingface.co/docs/transformers/model_doc/video_llava)
* [M2M100](https://huggingface.co/docs/transformers/model_doc/m2m_100)
* [MBart](https://huggingface.co/docs/transformers/model_doc/mbart#transformers.MBartModel)
* [Mistral](https://huggingface.co/docs/transformers/model_doc/mistral#transformers.MistralModel)
* [Mixtral](https://huggingface.co/docs/transformers/model_doc/mixtral#transformers.MixtralModel)
* [Musicgen](https://huggingface.co/docs/transformers/model_doc/musicgen#transformers.MusicgenModel)
* [MusicGen Melody](https://huggingface.co/docs/transformers/model_doc/musicgen_melody#transformers.MusicgenMelodyModel)
* [NLLB](https://huggingface.co/docs/transformers/model_doc/nllb)
* [OLMo](https://huggingface.co/docs/transformers/model_doc/olmo#transformers.OlmoModel)
* [OPT](https://huggingface.co/docs/transformers/model_doc/opt#transformers.OPTModel)
* [Phi](https://huggingface.co/docs/transformers/model_doc/phi#transformers.PhiModel)
* [Phi3](https://huggingface.co/docs/transformers/model_doc/phi3#transformers.Phi3Model)
* [SigLIP](https://huggingface.co/docs/transformers/model_doc/siglip)
* [StableLm](https://huggingface.co/docs/transformers/model_doc/stablelm#transformers.StableLmModel)
* [Starcoder2](https://huggingface.co/docs/transformers/model_doc/starcoder2#transformers.Starcoder2Model)
* [Qwen2](https://huggingface.co/docs/transformers/model_doc/qwen2#transformers.Qwen2Model)
* [Qwen2MoE](https://huggingface.co/docs/transformers/model_doc/qwen2_moe#transformers.Qwen2MoeModel)
* [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperModel)
* [Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2#transformers.Wav2Vec2Model)
* [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperModel)
* [Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2#transformers.Wav2Vec2Model)
* [Hubert](https://huggingface.co/docs/transformers/model_doc/hubert#transformers.HubertModel)
* [data2vec_audio](https://huggingface.co/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioModel)
* [Sew](https://huggingface.co/docs/transformers/main/en/model_doc/sew#transformers.SEWModel)
* [UniSpeech](https://huggingface.co/docs/transformers/v4.39.3/en/model_doc/unispeech#transformers.UniSpeechModel)
* [unispeech_sat](https://huggingface.co/docs/transformers/v4.39.3/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel)
ÙŠÙ…ÙƒÙ†Ùƒ Ø·Ù„Ø¨ Ø¥Ø¶Ø§ÙØ© Ø¯Ø¹Ù… FlashAttention-2 Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¢Ø®Ø± Ø¹Ù† Ø·Ø±ÙŠÙ‚ ÙØªØ­ Ù…Ø´ÙƒÙ„Ø© Ø£Ùˆ Ø·Ù„Ø¨ Ø³Ø­Ø¨ Ø¹Ù„Ù‰ GitHub.

Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ø¯Ø¡ØŒ ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª FlashAttention-2.

<hfoptions id="install">
<hfoption id="NVIDIA">

```bash
pip install flash-attn --no-build-isolation
```

Ù†ÙˆØµÙŠ Ø¨Ø´Ø¯Ø© Ø¨Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„Ù‰ [ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„ØªØ«Ø¨ÙŠØª](https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file#installation-and-features) Ø§Ù„Ù…ÙØµÙ„Ø© Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ø­ÙˆÙ„ Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø© ÙˆØ£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª!

</hfoption>
<hfoption id="AMD">

ÙŠØªÙ… Ø¯Ø¹Ù… FlashAttention-2 Ø£ÙŠØ¶Ù‹Ø§ Ø¹Ù„Ù‰ ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª AMDØŒ ÙˆÙŠÙ‚ØªØµØ± Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„Ø­Ø§Ù„ÙŠ Ø¹Ù„Ù‰ **Instinct MI210**ØŒ Ùˆ**Instinct MI250**ØŒ Ùˆ**Instinct MI300**. Ù†ÙˆØµÙŠ Ø¨Ø´Ø¯Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ø§ [Dockerfile](https://github.com/huggingface/optimum-amd/tree/main/docker/transformers-pytorch-amd-gpu-flash/Dockerfile) Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… FlashAttention-2 Ø¹Ù„Ù‰ ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª AMD.

</hfoption>
</hfoptions>

Ù„ØªÙ…ÙƒÙŠÙ† FlashAttention-2ØŒ Ù…Ø±Ø± ÙˆØ³ÙŠØ· `attn_implementation="flash_attention_2"` Ø¥Ù„Ù‰ [`~AutoModelForCausalLM.from_pretrained`]:

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM

model_id = "tiiuae/falcon-7b"
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    attn_implementation="flash_attention_2",
)
```

<Tip>

ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… FlashAttention-2 ÙÙ‚Ø· Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙƒÙˆÙ† Ù†ÙˆØ¹ Ù†Ù…ÙˆØ°Ø¬ "fp16" Ø£Ùˆ "bf16". ØªØ£ÙƒØ¯ Ù…Ù† ØªØ­ÙˆÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø¥Ù„Ù‰ Ø§Ù„Ù†ÙˆØ¹ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ ÙˆØªØ­Ù…ÙŠÙ„Ù‡ Ø¹Ù„Ù‰ Ø¬Ù‡Ø§Ø² Ù…Ø¯Ø¹ÙˆÙ… Ù‚Ø¨Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù… FlashAttention-2.

<br>

ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ ØªØ¹ÙŠÙŠÙ† `use_flash_attention_2=True` Ù„ØªÙ…ÙƒÙŠÙ† FlashAttention-2 ÙˆÙ„ÙƒÙ†Ù‡ Ù…Ù‡Ù…Ù„ Ù„ØµØ§Ù„Ø­ `attn_implementation="flash_attention_2"`.

</Tip>

ÙŠÙ…ÙƒÙ† Ø§Ù„Ø¬Ù…Ø¹ Ø¨ÙŠÙ† FlashAttention-2 ÙˆØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø®Ø±Ù‰ Ù…Ø«Ù„ Ø§Ù„Ø¶Ø¨Ø· Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø²ÙŠØ¯ Ù…Ù† ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¬Ù…Ø¹ Ø¨ÙŠÙ† FlashAttention-2 ÙˆØ§Ù„Ø¶Ø¨Ø· 8-Ø¨Øª Ø£Ùˆ 4-Ø¨Øª:

```py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM

model_id = "tiiuae/falcon-7b"
tokenizer = AutoTokenizer.from_pretrained(model_id)

# ØªØ­Ù…ÙŠÙ„ ÙÙŠ 8 Ø¨Øª
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    load_in_8bit=True,
    attn_implementation="flash_attention_2",
)

# ØªØ­Ù…ÙŠÙ„ ÙÙŠ 4 Ø¨Øª
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    load_in_4bit=True,
    attn_ excentricitÃ©="flash_attention_2"ØŒ
)
```

### ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹

ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„ÙƒØ¨ÙŠØ± Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ØŒ Ø®Ø§ØµØ© Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø°Ø§Øª Ø§Ù„ØªØ³Ù„Ø³Ù„Ø§Øª Ø§Ù„Ø·ÙˆÙŠÙ„Ø©. ÙˆÙ…Ø¹ Ø°Ù„ÙƒØŒ Ù†Ø¸Ø±Ù‹Ø§ Ù„Ø£Ù† FlashAttention-2 Ù„Ø§ ÙŠØ¯Ø¹Ù… Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ù…Ø¹ Ø±Ù…ÙˆØ² Ø§Ù„ØªØ¹Ø¨Ø¦Ø©ØŒ ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ ÙŠØ¯ÙˆÙŠÙ‹Ø§ ØªØ¹Ø¨Ø¦Ø©/Ø¥Ù„ØºØ§Ø¡ ØªØ¹Ø¨Ø¦Ø© Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ù…Ø¬Ù…Ø¹ Ø¹Ù†Ø¯Ù…Ø§ ÙŠØ­ØªÙˆÙŠ Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø¹Ù„Ù‰ Ø±Ù…ÙˆØ² ØªØ¹Ø¨Ø¦Ø©. ÙŠØ¤Ø¯ÙŠ Ù‡Ø°Ø§ Ø¥Ù„Ù‰ ØªØ¨Ø§Ø·Ø¤ ÙƒØ¨ÙŠØ± ÙÙŠ Ø§Ù„Ø£Ø¬ÙŠØ§Ù„ Ø§Ù„Ù…Ø¬Ù…Ø¹Ø© Ù…Ø¹ Ø±Ù…ÙˆØ² Ø§Ù„ØªØ¹Ø¨Ø¦Ø©.

Ù„ØªØ¬Ø§ÙˆØ² Ø°Ù„ÙƒØŒ ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… FlashAttention-2 Ø¨Ø¯ÙˆÙ† Ø±Ù…ÙˆØ² Ø§Ù„ØªØ¹Ø¨Ø¦Ø© ÙÙŠ Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ¹Ø¨Ø¦Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ùˆ [Ø¯Ù…Ø¬ Ø§Ù„ØªØ³Ù„Ø³Ù„Ø§Øª](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py#L516) Ø­ØªÙ‰ Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø·ÙˆÙ„ Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø§Ù„Ø£Ù‚ØµÙ‰).

Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù…Ø±ÙˆØ± Ø§Ù„Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø£Ù…Ø§Ù…ÙŠ Ø§Ù„ÙØ±Ø¯ÙŠ Ø¹Ù„Ù‰ [tiiuae/falcon-7b](https://hf.co/tiiuae/falcon-7b) Ø¨Ø·ÙˆÙ„ ØªØ³Ù„Ø³Ù„ 4096 ÙˆØ£Ø­Ø¬Ø§Ù… Ø¯ÙØ¹Ø§Øª Ù…Ø®ØªÙ„ÙØ© Ø¨Ø¯ÙˆÙ† Ø±Ù…ÙˆØ² Ø§Ù„ØªØ¹Ø¨Ø¦Ø©ØŒ ÙŠÙƒÙˆÙ† ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ Ù‡Ùˆ:

<div style="text-align: center">
<img src="https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/falcon-7b-inference-large-seqlen.png">
</div>

Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù…Ø±ÙˆØ± Ø§Ù„Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø£Ù…Ø§Ù…ÙŠ Ø§Ù„ÙØ±Ø¯ÙŠ Ø¹Ù„Ù‰ [meta-llama/Llama-7b-hf](https://hf.co/meta-llama/Llama-7b-hf) Ø¨Ø·ÙˆÙ„ ØªØ³Ù„Ø³Ù„ 4096 ÙˆØ£Ø­Ø¬Ø§Ù… Ø¯ÙØ¹Ø§Øª Ù…Ø®ØªÙ„ÙØ© Ø¨Ø¯ÙˆÙ† Ø±Ù…ÙˆØ² Ø§Ù„ØªØ¹Ø¨Ø¦Ø©ØŒ ÙŠÙƒÙˆÙ† ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ Ù‡Ùˆ:

<div style="text-align: center">
<img src="https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/llama-7b-inference-large-seqlen.png">
</div>
<div style="text-align-center">
<img src="https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/llama-7b-inference-large-seqlen.png">
</div>

Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù„ØªØ³Ù„Ø³Ù„Ø§Øª Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø±Ù…ÙˆØ² Ø§Ù„ØªØ¹Ø¨Ø¦Ø© (ØªÙˆÙ„ÙŠØ¯ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø±Ù…ÙˆØ² Ø§Ù„ØªØ¹Ø¨Ø¦Ø©)ØŒ ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ Ø¥Ù„ØºØ§Ø¡ ØªØ¹Ø¨Ø¦Ø©/ØªØ¹Ø¨Ø¦Ø© ØªØ³Ù„Ø³Ù„Ø§Øª Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ù„Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­. Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·ÙˆÙ„ ØªØ³Ù„Ø³Ù„ ØµØºÙŠØ± Ù†Ø³Ø¨ÙŠÙ‹Ø§ØŒ ÙŠØ¤Ø¯ÙŠ Ù…Ø±ÙˆØ± Ø§Ù„Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø£Ù…Ø§Ù…ÙŠ Ø§Ù„ÙØ±Ø¯ÙŠ Ø¥Ù„Ù‰ Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¹Ø¨Ø¡ Ù…Ù…Ø§ ÙŠØ¤Ø¯ÙŠ Ø¥Ù„Ù‰ ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¨Ø³ÙŠØ· (ÙÙŠ Ø§Ù„Ù…Ø«Ø§Ù„ Ø£Ø¯Ù†Ø§Ù‡ØŒ ÙŠØªÙ… Ù…Ù„Ø¡ 30% Ù…Ù† Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø¨Ø±Ù…ÙˆØ² Ø§Ù„ØªØ¹Ø¨Ø¦Ø©):

<div style="text-align: center">
<img src="https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/llama-2-small-seqlen-padding.png">
</div>

ÙˆÙ„ÙƒÙ† Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ø£Ø·ÙˆØ§Ù„ Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø§Ù„Ø£ÙƒØ¨Ø±ØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªÙˆÙ‚Ø¹ ÙÙˆØ§Ø¦Ø¯ ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø­ØªÙ‰ Ø£ÙƒØ«Ø± Ù…Ù† Ø°Ù„Ùƒ:

<Tip>

FlashAttention Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© ÙÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©ØŒ Ù…Ù…Ø§ ÙŠØ¹Ù†ÙŠ Ø£Ù†Ù‡ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ Ø£Ø·ÙˆØ§Ù„ ØªØ³Ù„Ø³Ù„ Ø£ÙƒØ¨Ø± Ø¯ÙˆÙ† Ù…ÙˆØ§Ø¬Ù‡Ø© Ù…Ø´ÙƒÙ„Ø§Øª Ù†ÙØ§Ø¯ Ø§Ù„Ø°Ø§ÙƒØ±Ø©. ÙŠÙ…ÙƒÙ†Ùƒ ØªÙ‚Ù„ÙŠÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¨Ù†Ø³Ø¨Ø© ØªØµÙ„ Ø¥Ù„Ù‰ 20 Ù…Ø±Ø© Ù„Ø£Ø·ÙˆØ§Ù„ Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø§Ù„Ø£ÙƒØ¨Ø±. Ø§Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆØ¯Ø¹ [flash-attention](https://github.com/Dao-AILab/flash-attention) Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„.

</Tip>

<div style="text-align: center">
<img src="https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/llama-2-large-seqlen-padding.png">
</div>

## PyTorch scaled dot product attention

ÙŠÙ…ÙƒÙ† Ù„Ù€ PyTorch's [`torch.nn.functional.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html) (SDPA) Ø£ÙŠØ¶Ù‹Ø§ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ FlashAttention ÙˆÙ†ÙˆØ§Ø© Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø§Ù„ÙƒÙØ¤Ø© ÙÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© ØªØ­Øª Ø§Ù„ØºØ·Ø§Ø¡. ÙŠØ¬Ø±ÙŠ Ø­Ø§Ù„ÙŠÙ‹Ø§ Ø¥Ø¶Ø§ÙØ© Ø¯Ø¹Ù… SDPA Ø¨Ø´ÙƒÙ„ Ø£ØµÙ„ÙŠ ÙÙŠ Transformers ÙˆÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ø¨Ø´ÙƒÙ„ Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù„Ù€ `torch>=2.1.1` Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙƒÙˆÙ† Ø§Ù„ØªÙ†ÙÙŠØ° Ù…ØªØ§Ø­Ù‹Ø§. ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ ØªØ¹ÙŠÙŠÙ† `attn_implementation="sdpa"` ÙÙŠ `from_pretrained()` Ù„Ø·Ù„Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù… SDPA Ø¨Ø´ÙƒÙ„ ØµØ±ÙŠØ­.
ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø­Ø§Ù„ÙŠØŒ ÙŠØ¯Ø¹Ù… Transformers Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ ÙˆØ§Ù„ØªØ¯Ø±ÙŠØ¨ SDPA Ù„Ù„Ø¨Ù†Ù‰ Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ© Ø§Ù„ØªØ§Ù„ÙŠØ©:
* [Audio Spectrogram Transformer](https://huggingface.co/docs/transformers/model_doc/audio-spectrogram-transformer#transformers.ASTModel)
* [Bart](https://huggingface.co/docs/transformers/model_doc/bart#transformers.BartModel)
* [Bert](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel)
* [Chameleon](https://huggingface.co/docs/transformers/model_doc/chameleon#transformers.Chameleon)
* [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPModel)
* [Cohere](https://huggingface.co/docs/transformers/model_doc/cohere#transformers.CohereModel)
* [Dbrx](https://huggingface.co/docs/transformers/model_doc/dbrx#transformers.DbrxModel)
* [DeiT](https://huggingface.co/docs/transformers/model_doc/deit#transformers.DeiTModel)
* [Dpr](https://huggingface.co/docs/transformers/model_doc/dpr#transformers.DprReader)
* [Falcon](https://huggingface.co/docs/transformers/model_doc/falcon#transformers.FalconModel)
* [Gemma](https://huggingface.co/docs/transformers/model_doc/gemma#transformers.GemmaModel)
* [Gemma2](https://huggingface.co/docs/transformers/model_doc/gemma2#transformers.Gemma2Model)
* [GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)
* [GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode#transformers.GPTBigCodeModel)
* [GPTNeoX](https://huggingface.co/docs/transformers/model_doc/gpt_neox#transformers.GPTNeoXModel)
* [JetMoe](https://huggingface.co/docs/transformers/model_doc/jetmoe#transformers.JetMoeModel)
* [Jamba](https://huggingface.co/docs/transformers/model_doc/jamba#transformers.JambaModel)
* [Llama](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaModel)
* [OLMo](https://huggingface.co/docs/transformers/model_doc/olmo#transformers.OlmoModel)
* [PaliGemma](https://huggingface.co/docs/transformers/model_doc/paligemma#transformers.PaliGemmaForConditionalGeneration)
* [Phi](https://huggingface.co/docs/transformers/model_doc/phi#transformers.PhiModel)
* [Idefics](https://huggingface.co/docs/transformers/model_doc/idefics#transformers.IdeficsModel)
* [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperModel)
* [Mistral](https://huggingface.co/docs/transformers/model_doc/mistral#transformers.MistralModel)
* [Mixtral](https://huggingface.co/docs/transformers/model_doc/mixtral#transformers.MixtralModel)
* [StableLm](https://huggingface.co/docs/transformers/model_doc/stablelm#transformers.StableLmModel)
* [Starcoder2](https://huggingface.co/docs/transformers/model_doc/starcoder2#transformers.Starcoder2Model)
* [Qwen2](https://huggingface.co/docs/transformers/model_doc/qwen2#transformers.Qwen2Model)
* [Qwen2MoE](https://huggingface.co/docs/transformers/model_doc/qwen2_moe#transformers.Qwen2MoeModel)
* [Musicgen](https://huggingface.co/docs/transformers/model_doc/musicgen#transformers.MusicgenModel)
* [MusicGen Melody](https://huggingface.co/docs/transformers/model_doc/musicgen_melody#transformers.MusicgenMelodyModel)
* [ViT](https://huggingface.co/docs/transformers/model_doc/vit#transformers.ViTModel)
* [ViTHybrid](https://huggingface.co/docs/transformers/model_doc/vit_hybrid#transformers.ViTHybridModel)
* [ViTMAE](https://huggingface.co/docs/transformers/model_doc/vit_mae#transformers.ViTMAEModel)
* [ViTMSN](https://huggingface.co/docs/transformers/model_doc/vit_msn#transformers.ViTMSNModel)
* [VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae#transformers.VideoMAEModell)
* [wav2vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2#transformers.Wav2Vec2Model)
* [Hubert](https://huggingface.co/docs/transformers/model_doc/hubert#transformers.HubertModel)
* [data2vec_audio](https://huggingface.co/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioModel)
* [SigLIP](https://huggingface.co/docs/transformers/model_doc/siglip)
* [Sew](https://huggingface.co/docs/transformers/main/en/model_doc/sew#transformers.SEWModel)
* [UniSpeech](https://huggingface.co/docs/transformers/v4.39.3/en/model_doc/unispeech#transformers.UniSpeechModel)
* [unispeech_sat](https://huggingface.co/docs/transformers/v4.39.3/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel)
* [YOLOS](https://huggingface.co/docs/transformers/model_doc/yolos#transformers.YolosModel)



<Tip>

ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… FlashAttention ÙÙ‚Ø· Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø°Ø§Øª Ø§Ù„Ù†ÙˆØ¹ "fp16" Ø£Ùˆ "bf16" torchØŒ Ù„Ø°Ø§ ØªØ£ÙƒØ¯ Ù…Ù† ØªØ­ÙˆÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø¥Ù„Ù‰ Ø§Ù„Ù†ÙˆØ¹ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ø£ÙˆÙ„Ø§Ù‹. ÙŠÙ…ÙƒÙ† Ù„Ø®Ù„ÙÙŠØ© Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø§Ù„ÙØ¹Ø§Ù„Ø© Ù„Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ù†Ù…Ø§Ø°Ø¬ "fp32".

</Tip>

<Tip>

Ù„Ø§ ØªØ¯Ø¹Ù… SDPA Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ù…Ø¹ÙŠÙ†Ø© Ù…Ù† Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…ØŒ Ù…Ø«Ù„ "head_mask" Ùˆ "output_attentions=True".
ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ ÙŠØ¬Ø¨ Ø£Ù† ØªØ´Ø§Ù‡Ø¯ Ø±Ø³Ø§Ù„Ø© ØªØ­Ø°ÙŠØ± ÙˆØ³Ù†Ù‚ÙˆÙ… Ø¨Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„Ù‰ Ø§Ù„ØªÙ†ÙÙŠØ° (Ø§Ù„Ø£Ø¨Ø·Ø£).

</Tip>

Ø¨Ø´ÙƒÙ„ Ø§ÙØªØ±Ø§Ø¶ÙŠØŒ ØªÙ‚ÙˆÙ… SDPA Ø¨ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ§Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© Ø§Ù„Ù…ØªØ§Ø­Ø©ØŒ ÙˆÙ„ÙƒÙ† ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø®Ù„ÙÙŠØ© Ù…ØªØ§Ø­Ø© ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø¹ÙŠÙ† (Ø§Ù„Ø£Ø¬Ù‡Ø²Ø©ØŒ Ø­Ø¬Ù… Ø§Ù„Ù…Ø´ÙƒÙ„Ø©) Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ["torch.backends.cuda.sdp_kernel"](https://pytorch.org/docs/master/backends.html#torch.backends.cuda.sdp_kernel) ÙƒÙ…Ø¯ÙŠØ± Ø³ÙŠØ§Ù‚:

```diff
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")
model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", torch_dtype=torch.float16).to("cuda")

input_text = "Hello my dog is cute and"
inputs = tokenizer(input_text, return_tensors="pt").to("cuda")

+ with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):
    outputs = model.generate(**inputs)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

Ø¥Ø°Ø§ Ø±Ø£ÙŠØª Ø®Ø·Ø£ Ù…Ø¹ ØªØªØ¨Ø¹ Ø§Ù„Ù…ÙƒØ¯Ø³ Ø£Ø¯Ù†Ø§Ù‡ØŒ ÙØ­Ø§ÙˆÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ù„ÙŠÙ„ÙŠ Ù…Ù† PyTorch Ø§Ù„Ø°ÙŠ Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ù„Ù‡ ØªØºØ·ÙŠØ© Ø£ÙˆØ³Ø¹ Ù„Ù€ FlashAttention:

```bash
RuntimeError: No available kernel. Aborting execution.

# install PyTorch nightly
pip3 install -U --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118
```

## BetterTransformer

<Tip warning={true}>

ÙŠØªÙ… Ù†Ù‚Ù„ Ø¨Ø¹Ø¶ Ù…ÙŠØ²Ø§Øª BetterTransformer Ø¥Ù„Ù‰ Ø£Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ ÙÙŠ Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª Ù…Ø¹ Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù„Ù€ native `torch.nn.scaled_dot_product_attention`. Ù„Ø§ ÙŠØ²Ø§Ù„ Ù„Ø¯Ù‰ BetterTransformer ØªØºØ·ÙŠØ© Ø£ÙˆØ³Ø¹ Ù…Ù† ØªÙƒØ§Ù…Ù„ SDPA ÙÙŠ Ø§Ù„Ù…Ø­ÙˆÙ„Ø§ØªØŒ ÙˆÙ„ÙƒÙ† ÙŠÙ…ÙƒÙ†Ùƒ ØªÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø²ÙŠØ¯ ÙˆØ§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù‡Ù†Ø¯Ø³Ø§Øª Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ© Ø§Ù„ØªÙŠ ØªØ¯Ø¹Ù… SDPA Ø¨Ø´ÙƒÙ„ Ø£ØµÙ„ÙŠ ÙÙŠ Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.

</Tip>

<Tip>

ØªØ­Ù‚Ù‚ Ù…Ù† Ù…Ø¹Ø§ÙŠÙŠØ±Ù†Ø§ Ù…Ø¹ BetterTransformer Ùˆscaled dot product attention ÙÙŠ [Ø§Ù„ØªØ³Ø±ÙŠØ¹ ÙˆØ§Ù„ÙˆÙÙˆØ±Ø§Øª ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø®Ø§Ø±Ø¬ Ø§Ù„ØµÙ†Ø¯ÙˆÙ‚ Ù„Ù†Ù…Ø§Ø°Ø¬ ÙÙƒ ØªØ´ÙÙŠØ± ğŸ¤— Ù…Ø¹ PyTorch 2.0](https://pytorch.org/blog/out-of-the-box-acceleration/) ÙˆØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø²ÙŠØ¯ Ø­ÙˆÙ„ ØªÙ†ÙÙŠØ° fastpath ÙÙŠ [BetterTransformer](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2) Ù…Ù†Ø´ÙˆØ± Ø§Ù„Ù…Ø¯ÙˆÙ†Ø©.

</Tip>

BetterTransformer ÙŠØ³Ø±Ø¹ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ†ÙÙŠØ°Ù‡ Ø§Ù„Ù…ØªØ®ØµØµ ÙÙŠ PyTorch Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…Ø­ÙˆÙ„. Ù‡Ù†Ø§Ùƒ ØªØ­Ø³ÙŠÙ†Ø§Ù† ÙÙŠ ØªÙ†ÙÙŠØ° fastpath:

1. Ø§Ù„Ø§Ù†ØµÙ‡Ø§Ø±ØŒ Ø§Ù„Ø°ÙŠ ÙŠØ¬Ù…Ø¹ Ø¨ÙŠÙ† Ø¹Ø¯Ø© Ø¹Ù…Ù„ÙŠØ§Øª Ù…ØªØªØ§Ù„ÙŠØ© ÙÙŠ "kernel" ÙˆØ§Ø­Ø¯ Ù„ØªÙ‚Ù„ÙŠÙ„ Ø¹Ø¯Ø¯ Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø­Ø³Ø§Ø¨
2. ØªØ®Ø·ÙŠ Ø§Ù„ØªÙØ±Ù‚Ø© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØªØ¹Ø¨Ø¦Ø© Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ø³Ø§Ø¨ ØºÙŠØ± Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠ Ù…Ø¹ Ø§Ù„Ù…ØµÙÙˆÙØ§Øª Ø§Ù„Ù…Ø¶Ù…Ù†Ø©

BetterTransformer ÙŠØ­ÙˆÙ„ Ø£ÙŠØ¶Ù‹Ø§ Ø¬Ù…ÙŠØ¹ Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… [scaled dot product attention (SDPA)](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention)ØŒ Ø§Ù„Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©ØŒ ÙˆÙŠØ³ØªØ¯Ø¹ÙŠ Ù†ÙˆÙ‰ Ù…Ø­Ø³Ù†Ø© Ù…Ø«Ù„ [FlashAttention](https://huggingface.co/papers/2205.14135) ØªØ­Øª ØºØ·Ø§Ø¡ Ø§Ù„Ù…Ø­Ø±Ùƒ.

Ù‚Ø¨Ù„ Ø£Ù† ØªØ¨Ø¯Ø£ØŒ ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ù„Ø¯ÙŠÙƒ ğŸ¤— Optimum [Ù…Ø«Ø¨Øª](https://huggingface.co/docs/optimum/installation).

Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªÙ…ÙƒÙŠÙ† BetterTransformer Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© [`PreTrainedModel.to_bettertransformer`]():

```python
model = model.to_bettertransformer()
```

ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø±Ø¬Ø§Ø¹ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© [`~PreTrainedModel.reverse_bettertransformer`](): ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ø§ Ù‚Ø¨Ù„ Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬Ùƒ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:

```py
model = model.reverse_bettertransformer()
model.save_pretrained ("model_saved")
```

## bitsandbytes

bitsandbytes Ù‡ÙŠ Ù…ÙƒØªØ¨Ø© ØªÙƒÙ…ÙŠÙ… ØªØªØ¶Ù…Ù† Ø¯Ø¹Ù…Ù‹Ø§ Ù„Ù€ 4 Ø¨Øª Ùˆ8 Ø¨Øª. ÙŠÙ‚Ù„Ù„ Ø§Ù„ØªÙƒÙ…ÙŠÙ… Ø­Ø¬Ù… Ù†Ù…ÙˆØ°Ø¬Ùƒ Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ø§Ù„Ø£ØµÙ„ÙŠØŒ Ù…Ù…Ø§ ÙŠØ¬Ø¹Ù„Ù‡ Ø£Ø³Ù‡Ù„ ÙÙŠ ÙˆØ¶Ø¹ Ù†Ù…Ø§Ø°Ø¬ ÙƒØ¨ÙŠØ±Ø© Ø¹Ù„Ù‰ ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª (GPUs) Ø°Ø§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø­Ø¯ÙˆØ¯Ø©.

ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ù„Ø¯ÙŠÙƒ bitsandbytes Ùˆ ğŸ¤— Accelerate Ù…Ø«Ø¨Øª:

```bash
# Ù‡Ø°Ù‡ Ø§Ù„Ø¥ØµØ¯Ø§Ø±Ø§Øª ØªØ¯Ø¹Ù… 8 Ø¨Øª Ùˆ4 Ø¨Øª
pip install bitsandbytes>=0.39.0 accelerate>=0.20.0

# ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª
pip install transformers
```

### 4 Ø¨Øª

Ù„ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ 4 Ø¨Øª Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ù…Ø¹Ù„Ù…Ø© "load_in_4bit". ÙˆØ³ÙŠØ· "device_map" Ø§Ø®ØªÙŠØ§Ø±ÙŠØŒ ÙˆÙ„ÙƒÙ† ÙŠÙÙ†ØµØ­ Ø¨ØªØ¹ÙŠÙŠÙ†Ù‡ Ø¹Ù„Ù‰ "auto" Ù„Ù„Ø³Ù…Ø§Ø­ Ù„Ù€ ğŸ¤— Accelerate Ø¨ØªØ®ØµÙŠØµ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ ÙˆØ¨ÙƒÙØ§Ø¡Ø© Ø¨Ø§Ù„Ù†Ø¸Ø± Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙŠ Ø§Ù„Ø¨ÙŠØ¦Ø©.

```py
from transformers import AutoModelForCausalLM

model_name = "bigscience/bloom-2b5"
model_4bit = AutoModelForCausalLM.from_pretrained(model_nameØŒ device_map="auto"ØŒ load_in_4bit=True)
```

Ù„ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ 4 Ø¨Øª Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª (GPUs) Ù…ØªØ¹Ø¯Ø¯Ø©ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ù…Ù‚Ø¯Ø§Ø± Ø°Ø§ÙƒØ±Ø© GPU Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ ØªØ®ØµÙŠØµÙ‡Ø§ Ù„ÙƒÙ„ GPU. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ù„ØªÙˆØ²ÙŠØ¹ 600 Ù…ÙŠØ¬Ø§Ø¨Ø§ÙŠØª Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¥Ù„Ù‰ ÙˆØ­Ø¯Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª Ø§Ù„Ø£ÙˆÙ„Ù‰ Ùˆ1 Ø¬ÙŠØ¬Ø§Ø¨Ø§ÙŠØª Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¥Ù„Ù‰ ÙˆØ­Ø¯Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª Ø§Ù„Ø«Ø§Ù†ÙŠØ©:

```py
max_memory_mapping = {0: "600MB"ØŒ 1: "1GB"}
model_name = "bigscience/bloom-3b"
model_4bit = AutoModelForCausalLM.from_pretrained(
    model_nameØŒ device_map="auto"ØŒ load_in_4bit=TrueØŒ max_memory=max_memory_mapping
)
```

### 8 Ø¨Øª

<Tip>

Ø¥Ø°Ø§ ÙƒÙ†Øª ÙØ¶ÙˆÙ„ÙŠÙ‹Ø§ ÙˆÙ…Ù‡ØªÙ…Ù‹Ø§ Ø¨Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ø¹Ù† Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙˆØ±Ø§Ø¡ Ø§Ù„ØªÙƒÙ…ÙŠÙ… 8 Ø¨ØªØŒ ÙØ§Ù‚Ø±Ø£ [Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes](https://huggingface.co/blog/hf-bitsandbytes-integration) Ù…Ù†Ø´ÙˆØ± Ø§Ù„Ù…Ø¯ÙˆÙ†Ø©.

</Tip>

Ù„ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ 8 Ø¨Øª Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ù…Ø¹Ù„Ù…Ø© "load_in_8bit". ÙˆØ³ÙŠØ· "device_map" Ø§Ø®ØªÙŠØ§Ø±ÙŠØŒ ÙˆÙ„ÙƒÙ† ÙŠÙÙ†ØµØ­ Ø¨ØªØ¹ÙŠÙŠÙ†Ù‡ Ø¹Ù„Ù‰ "auto" Ù„Ù„Ø³Ù…Ø§Ø­ Ù„Ù€ ğŸ¤— Accelerate Ø¨ØªØ®ØµÙŠØµ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ ÙˆØ¨ÙƒÙØ§Ø¡Ø© Ø¨Ø§Ù„Ù†Ø¸Ø± Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙŠ Ø§Ù„Ø¨ÙŠØ¦Ø©:

```py
from transformers import AutoModelForCausalLMØŒ BitsAndBytesConfig

model_name = "bigscience/bloom-2b5"
model_8bit = AutoModelForCausalLM.from_pretrained(model_nameØŒ quantization_config=BitsAndBytesConfig (load_in_8bit=True))
```

Ø¥Ø°Ø§ ÙƒÙ†Øª ØªÙ‚ÙˆÙ… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ 8 Ø¨Øª Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµØŒ ÙÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© [`~transformers.GenerationMixin.generate`] Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø¯Ø§Ù„Ø© ["Pipeline"] Ø§Ù„ØªÙŠ Ù„Ø§ ÙŠØªÙ… ØªØ­Ø³ÙŠÙ†Ù‡Ø§ Ù„Ù†Ù…Ø§Ø°Ø¬ 8 Ø¨Øª ÙˆØ³ØªÙƒÙˆÙ† Ø£Ø¨Ø·Ø£. Ù„Ø§ ØªØ¯Ø¹Ù… Ø¨Ø¹Ø¶ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§ØªØŒ Ù…Ø«Ù„ Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø§Ù„Ù†ÙˆÙˆÙŠØ©ØŒ Ø¨ÙˆØ§Ø³Ø·Ø© ["Pipeline"] Ù„Ù†Ù…Ø§Ø°Ø¬ 8 Ø¨Øª. ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ Ø£ÙŠØ¶Ù‹Ø§ ÙˆØ¶Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„Ø§Øª Ø¹Ù„Ù‰ Ù†ÙØ³ Ø§Ù„Ø¬Ù‡Ø§Ø² Ù…Ø«Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:

```py
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_name = "bigscience/bloom-2b5"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True))

prompt = "Hello, my llama is cute"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
generated_ids = model.generate(**inputs)
outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
```
Ù„ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ 4 Ø¨Øª Ù„Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¹Ø¯Ø© ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø±Ø³ÙˆÙ…ÙŠØ© (GPUs)ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ù…Ù‚Ø¯Ø§Ø± Ø°Ø§ÙƒØ±Ø© Ø§Ù„ÙˆØµÙˆÙ„ Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠ (RAM) Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ ØªØ®ØµÙŠØµÙ‡Ø§ Ù„ÙƒÙ„ ÙˆØ­Ø¯Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø±Ø³ÙˆÙ…ÙŠØ© (GPU). Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ù„ØªÙˆØ²ÙŠØ¹ 1 Ø¬ÙŠØ¬Ø§Ø¨Ø§ÙŠØª Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¥Ù„Ù‰ ÙˆØ­Ø¯Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø±ÙƒØ²ÙŠØ© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ùˆ2 Ø¬ÙŠØ¬Ø§Ø¨Ø§ÙŠØª Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¥Ù„Ù‰ ÙˆØ­Ø¯Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø±ÙƒØ²ÙŠØ© Ø§Ù„Ø«Ø§Ù†ÙŠØ©:

```py
max_memory_mapping = {0: "1GB", 1: "2GB"}
model_name = "bigscience/bloom-3b"
model_8bit = AutoModelForCausalLM.from_pretrained(
    model_name, device_map="auto", load_in_8bit=True, max_memory=max_memory_mapping
)
```

<Tip>

Ù„Ø§ ØªØªØ±Ø¯Ø¯ ÙÙŠ ØªØ¬Ø±Ø¨Ø© ØªØ´ØºÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ T5 Ø§Ù„Ø°ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ 11 Ù…Ù„ÙŠØ§Ø± Ù…Ø¹Ù„Ù…Ø© Ø£Ùˆ Ù†Ù…ÙˆØ°Ø¬ BLOOM Ø§Ù„Ø°ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ 3 Ù…Ù„ÙŠØ§Ø±Ø§Øª Ù…Ø¹Ù„Ù…Ø© Ù„Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø¹Ù„Ù‰ ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª (GPUs) Ù…Ù† Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ø¬Ø§Ù†ÙŠ ÙÙŠ Google Colab!

</Tip>

## ğŸ¤— Optimum

<Tip>

Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø­ÙˆÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… ORT Ù…Ø¹ ğŸ¤— OptimumØŒ Ø±Ø§Ø¬Ø¹ Ø¯Ù„ÙŠÙ„ÙŠ [ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø¹Ù„Ù‰ ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª (GPUs) Ù…Ù† NVIDIA](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/gpu#accelerated-inference-on-nvidia-gpus) Ùˆ[ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø¹Ù„Ù‰ ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª (GPUs) Ù…Ù† AMD](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/amdgpu#accelerated-inference-on-amd-gpus). ÙŠÙ‚Ø¯Ù… Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù… ÙÙ‚Ø· Ù…Ø«Ø§Ù„Ù‹Ø§ Ù…ÙˆØ¬Ø²Ù‹Ø§ ÙˆØ¨Ø³ÙŠØ·Ù‹Ø§.

</Tip>

ONNX Runtime (ORT) Ù‡Ùˆ Ù…Ø³Ø±Ø¹ Ù†Ù…ÙˆØ°Ø¬ÙŠ ÙŠØ¯Ø¹Ù… Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø§Ù„Ù…Ø¹Ø¬Ù„ Ø¹Ù„Ù‰ ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª (GPUs) Ù…Ù† NvidiaØŒ ÙˆÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª (GPUs) Ù…Ù† AMD Ø§Ù„ØªÙŠ ØªØ³ØªØ®Ø¯Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© ROCm. ÙŠØ³ØªØ®Ø¯Ù… ORT ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ† Ù…Ø«Ù„ Ø¯Ù…Ø¬ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© ÙÙŠ Ø¹Ù‚Ø¯Ø© ÙˆØ§Ø­Ø¯Ø© ÙˆØ·ÙˆÙŠ Ø§Ù„Ø«ÙˆØ§Ø¨Øª Ù„Ø®ÙØ¶ Ø¹Ø¯Ø¯ Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª Ø§Ù„ØªÙŠ ÙŠØªÙ… Ø¥Ø¬Ø±Ø§Ø¤Ù‡Ø§ ÙˆØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬. ÙƒÙ…Ø§ ÙŠÙ‚ÙˆÙ… ORT Ø¨ÙˆØ¶Ø¹ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø£ÙƒØ«Ø± ÙƒØ«Ø§ÙØ© Ø­Ø³Ø§Ø¨ÙŠØ© Ø¹Ù„Ù‰ ÙˆØ­Ø¯Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª (GPU) ÙˆØ§Ù„Ø¨Ø§Ù‚ÙŠ Ø¹Ù„Ù‰ ÙˆØ­Ø¯Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø±ÙƒØ²ÙŠØ© (CPU) Ù„ØªÙˆØ²ÙŠØ¹ Ø¹Ø¨Ø¡ Ø§Ù„Ø¹Ù…Ù„ Ø¨ÙŠÙ† Ø§Ù„Ø¬Ù‡Ø§Ø²ÙŠÙ† Ø¨Ø´ÙƒÙ„ Ø°ÙƒÙŠ.

ØªØ¯Ø¹Ù… Ù…ÙƒØªØ¨Ø© ğŸ¤— Optimum Ø§Ø³ØªØ®Ø¯Ø§Ù… ONNX RuntimeØŒ ÙˆØ§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ ÙÙŠ Ù…ÙƒØªØ¨Ø© ğŸ¤— Transformers. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… [`~optimum.onnxruntime.ORTModel`] Ù„Ù„Ù…Ù‡Ù…Ø© Ø§Ù„ØªÙŠ ØªØ­Ø§ÙˆÙ„ Ø­Ù„Ù‡Ø§ØŒ ÙˆØªØ­Ø¯ÙŠØ¯ Ù…Ø¹Ù„Ù…Ø© `provider` Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† ØªØ¹ÙŠÙŠÙ†Ù‡Ø§ Ø¥Ù…Ø§ Ø¥Ù„Ù‰ [`CUDAExecutionProvider`] Ø£Ùˆ [`ROCMExecutionProvider`] Ø£Ùˆ [`TensorrtExecutionProvider`]. Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ±ÙŠØ¯ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù… ÙŠØªÙ… ØªØµØ¯ÙŠØ±Ù‡ Ø¨Ø¹Ø¯ Ø¥Ù„Ù‰ ONNXØŒ ÙÙŠÙ…ÙƒÙ†Ùƒ ØªØ¹ÙŠÙŠÙ† `export=True` Ù„ØªØ­ÙˆÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ†Ù‚Ù„ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ ONNX:

```py
from optimum.onnxruntime import ORTModelForSequenceClassification

ort_model = ORTModelForSequenceClassification.from_pretrained(
  "distilbert/distilbert-base-uncased-finetuned-sst-2-english",
  export=True,
  provider="CUDAExecutionProvider",
)
```

Ø§Ù„Ø¢Ù† ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬:

```py
from optimum.pipelines import pipeline
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased-finetuned-sst-2-english")

pipeline = pipeline(task="text-classification", model=ort_model, tokenizer=tokenizer, device="cuda:0")
result = pipeline("Both the music and visual were astounding, not to mention the actors performance.")
```

## Ø§Ù„Ø¬Ù…Ø¹ Ø¨ÙŠÙ† Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª

ØºØ§Ù„Ø¨Ù‹Ø§ Ù…Ø§ ÙŠÙƒÙˆÙ† Ù…Ù† Ø§Ù„Ù…Ù…ÙƒÙ† Ø§Ù„Ø¬Ù…Ø¹ Ø¨ÙŠÙ† Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø°ÙƒÙˆØ±Ø© Ø£Ø¹Ù„Ø§Ù‡ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ø£Ø¯Ø§Ø¡ Ø§Ø³ØªÙ†ØªØ§Ø¬ Ù…Ù…ÙƒÙ† Ù„Ù†Ù…ÙˆØ°Ø¬Ùƒ. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ 4 Ø¨ØªØŒ Ø«Ù… ØªÙ…ÙƒÙŠÙ† BetterTransformer Ù…Ø¹ FlashAttention:

```py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ 4 Ø¨Øª
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)

tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")
model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", quantization_config=quantization_config)

# ØªÙ…ÙƒÙŠÙ† BetterTransformer
model = model.to_bettertransformer()

input_text = "Hello my dog is cute and"
inputs = tokenizer(input_text, return_tensors="pt").to("cuda")

# ØªÙ…ÙƒÙŠÙ† FlashAttention
with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):
    outputs = model.generate(**inputs)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```