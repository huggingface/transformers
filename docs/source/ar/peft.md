# ุชุญููู ุงููุญููุงุช ุจุงุณุชุฎุฏุงู ๐ค PEFT

[[open-in-colab]]

ุชููู ุทุฑู [ุงูุชุฏุฑูุจ ุงูุฏููู ุงููุนุงู ููุจุงุฑุงูุชุฑุงุช (PEFT)](https://huggingface.co/blog/peft) ุจุชุฌููุฏ ูุนููุงุช ุงููููุฐุฌ ุงูููุฏุฑุจ ูุณุจููุง ุฃุซูุงุก ุงูุถุจุท ุงูุฏููู ูุฅุถุงูุฉ ุนุฏุฏ ุตุบูุฑ ูู ุงููุนููุงุช ุงููุงุจูุฉ ููุชุฏุฑูุจ (ุงููุญููุงุช) ูููู. ูุชู ุชุฏุฑูุจ ุงููุญููุงุช ูุชุนูู ูุนูููุงุช ุฎุงุตุฉ ุจุงูููุงู. ููุฏ ุซุจุช ุฃู ูุฐุง ุงูููุฌ ูุนุงู ููุบุงูุฉ ูู ุญูุซ ุงูุฐุงูุฑุฉ ูุน ุงูุฎูุงุถ ุงุณุชุฎุฏุงู ุงูููุจููุชุฑ ุฃุซูุงุก ุฅูุชุงุฌ ูุชุงุฆุฌ ูุงุจูุฉ ููููุงุฑูุฉ ูุน ูููุฐุฌ ูุถุจูุท ุฏููููุง ุชูุงููุง.

ุนุงุฏุฉ ูุง ุชููู ุงููุญููุงุช ุงููุฏุฑุจุฉ ุจุงุณุชุฎุฏุงู PEFT ุฃุตุบุฑ ุฃูุถูุง ุจููุฏุงุฑ ุฏุฑุฌุฉ ูู ุญูุซ ุงูุญุฌู ูู ุงููููุฐุฌ ุงููุงููุ ููุง ูุฌุนู ูู ุงูุณูู ูุดุงุฑูุชูุง ูุชุฎุฒูููุง ูุชุญููููุง.

<div class="flex flex-col justify-center">
  <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/PEFT-hub-screenshot.png"/>
  <figcaption class="text-center">ุชุจูุบ ุฃูุฒุงู ุงููุญูู ูุทุฑุงุฒ OPTForCausalLM ุงููุฎุฒู ุนูู Hub ุญูุงูู 6 ููุฌุงุจุงูุช ููุงุฑูุฉ ุจุงูุญุฌู ุงููุงูู ูุฃูุฒุงู ุงููููุฐุฌุ ูุงูุชู ูููู ุฃู ุชููู ุญูุงูู 700 ููุฌุงุจุงูุช.</figcaption>
</div>

ุฅุฐุง ููุช ููุชููุง ุจูุนุฑูุฉ ุงููุฒูุฏ ุนู ููุชุจุฉ ๐ค PEFTุ ูุฑุงุฌุน [ุงููุซุงุฆู](https://huggingface.co/docs/peft/index).

## ุงูุฅุนุฏุงุฏ

ุงุจุฏุฃ ุจุชุซุจูุช ๐ค PEFT:

```bash
pip install peft
```

ุฅุฐุง ููุช ุชุฑูุฏ ุชุฌุฑุจุฉ ุงูููุฒุงุช ุงูุฌุฏูุฏุฉ ุชูุงููุงุ ููุฏ ุชููู ููุชููุง ุจุชุซุจูุช ุงูููุชุจุฉ ูู ุงููุตุฏุฑ:

```bash
pip install git+https://github.com/huggingface/peft.git
```

## ููุงุฐุฌ PEFT ุงููุฏุนููุฉ

ูุฏุนู ๐ค Transformers ุจุดููู ุฃุตูู ุจุนุถ ุทุฑู PEFTุ ููุง ูุนูู ุฃูู ููููู ุชุญููู ุฃูุฒุงู ุงููุญูู ุงููุฎุฒูุฉ ูุญูููุง ุฃู ุนูู Hub ูุชุดุบูููุง ุฃู ุชุฏุฑูุจูุง ุจุจุถุน ุณุทูุฑ ูู ุงูุชุนูููุงุช ุงูุจุฑูุฌูุฉ. ุงูุทุฑู ุงููุฏุนููุฉ ูู:

- [ูุญููุงุช ุงูุฑุชุจุฉ ุงูููุฎูุถุฉ](https://huggingface.co/docs/peft/conceptual_guides/lora)
- [IA3](https://huggingface.co/docs/peft/conceptual_guides/ia3)
- [AdaLoRA](https://arxiv.org/abs/2303.10512)

ุฅุฐุง ููุช ุชุฑูุฏ ุงุณุชุฎุฏุงู ุทุฑู PEFT ุงูุฃุฎุฑูุ ูุซู ุชุนูู ุงููุญุซ ุฃู ุถุจุท ุงููุญุซุ ุฃู ุญูู ููุชุจุฉ ๐ค PEFT ุจุดูู ุนุงูุ ูุฑุฌู ุงูุฑุฌูุน ุฅูู [ุงููุซุงุฆู](https://huggingface.co/docs/peft/index).

## ุชุญููู ูุญูู PEFT

ูุชุญููู ูููุฐุฌ ูุญูู PEFT ูุงุณุชุฎุฏุงูู ูู ๐ค Transformersุ ุชุฃูุฏ ูู ุฃู ูุณุชูุฏุน Hub ุฃู ุงูุฏููู ุงููุญูู ูุญุชูู ุนูู ููู `adapter_config.json` ูุฃูุฒุงู ุงููุญููุ ููุง ูู ููุถุญ ูู ุตูุฑุฉ ุงููุซุงู ุฃุนูุงู. ุจุนุฏ ุฐููุ ููููู ุชุญููู ูููุฐุฌ ูุญูู PEFT ุจุงุณุชุฎุฏุงู ูุฆุฉ `AutoModelFor`. ุนูู ุณุจูู ุงููุซุงูุ ูุชุญููู ูููุฐุฌ ูุญูู PEFT ููููุฐุฌุฉ ุงููุบููุฉ ุงูุณุจุจูุฉ:

1. ุญุฏุฏ ูุนุฑู ูููุฐุฌ PEFT
2. ูุฑุฑู ุฅูู ูุฆุฉ [`AutoModelForCausalLM`]

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

peft_model_id = "ybelkada/opt-350m-lora"
model = AutoModelForCausalLM.from_pretrained(peft_model_id)
```

<Tip>

ููููู ุชุญููู ูุญูู PEFT ุจุงุณุชุฎุฏุงู ูุฆุฉ `AutoModelFor` ุฃู ูุฆุฉ ุงููููุฐุฌ ุงูุฃุณุงุณู ูุซู `OPTForCausalLM` ุฃู `LlamaForCausalLM`.

</Tip>

ููููู ุฃูุถูุง ุชุญููู ูุญูู PEFT ุนู ุทุฑูู ุงุณุชุฏุนุงุก ุทุฑููุฉ `load_adapter`:

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "facebook/opt-350m"
peft_model_id = "ybelkada/opt-350m-lora"

model = AutoModelForCausalLM.from_pretrained(model_id)
model.load_adapter(peft_model_id)
```

ุฑุงุฌุน ูุณู [ูุซุงุฆู API](#transformers.integrations.PeftAdapterMixin) ุฃุฏูุงู ููุฒูุฏ ูู ุงูุชูุงุตูู.

## ุงูุชุญููู ูู 8 ุจุช ุฃู 4 ุจุช

ุฑุงุฌุน ูุณู [ูุซุงุฆู API](#transformers.integrations.PeftAdapterMixin) ุฃุฏูุงู ููุฒูุฏ ูู ุงูุชูุงุตูู.

## ุงูุชุญููู ูู 8 ุจุช ุฃู 4 ุจุช

ุชุฏุนู ุชูุงูู `bitsandbytes` ุฃููุงุน ุจูุงูุงุช ุงูุฏูุฉ 8 ุจุช ู4 ุจุชุ ูุงูุชู ุชููู ูููุฏุฉ ูุชุญููู ุงูููุงุฐุฌ ุงููุจูุฑุฉ ูุฃููุง ุชููุฑ ุงูุฐุงูุฑุฉ (ุฑุงุฌุน ุฏููู ุชูุงูู `bitsandbytes` [guide](./quantization#bitsandbytes-integration) ููุนุฑูุฉ ุงููุฒูุฏ). ุฃุถู `load_in_8bit` ุฃู `load_in_4bit` ุงููุนููุงุช ุฅูู [`~PreTrainedModel.from_pretrained`] ููู ุจุชุนููู `device_map="auto"` ูุชูุฒูุน ุงููููุฐุฌ ุจุดูู ูุนุงู ุนูู ุงูุฃุฌูุฒุฉ ูุฏูู:

```py
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

peft_model_id = "ybelkada/opt-350m-lora"
model = AutoModelForCausalLM.from_pretrained(peft_model_id, quantization_config=BitsAndBytesConfig(load_in_8bit=True))
```

## ุฅุถุงูุฉ ูุญูู ุฌุฏูุฏ

ููููู ุงุณุชุฎุฏุงู [`~peft.PeftModel.add_adapter`] ูุฅุถุงูุฉ ูุญูู ุฌุฏูุฏ ุฅูู ูููุฐุฌ ุจูุญูู ููุฌูุฏ ุทุงููุง ุฃู ุงููุญูู ุงูุฌุฏูุฏ ูู ูู ููุณ ููุน ุงููุญูู ุงูุญุงูู. ุนูู ุณุจูู ุงููุซุงูุ ุฅุฐุง ูุงู ูุฏูู ูุญูู LoRA ููุฌูุฏ ููุญู ุจูููุฐุฌ:

```py
from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
from peft import LoraConfig

model_id = "facebook/opt-350m"
model = AutoModelForCausalLM.from_pretrained(model_id)

lora_config = LoraConfig(
    target_modules=["q_proj", "k_proj"],
    init_lora_weights=False
)

model.add_adapter(lora_config, adapter_name="adapter_1")
```

ูุฅุถุงูุฉ ูุญูู ุฌุฏูุฏ:

```py
# ูู ุจุชุนููู ูุญูู ุฌุฏูุฏ ุจููุณ ุงูุชูููู
model.add_adapter(lora_config, adapter_name="adapter_2")
```

ุงูุขู ููููู ุงุณุชุฎุฏุงู [`~peft.PeftModel.set_adapter`] ูุชุนููู ุงููุญูู ุงูุฐู ุณูุชู ุงุณุชุฎุฏุงูู:

```py
# ุงุณุชุฎุฏู adapter_1
model.set_adapter("adapter_1")
output = model.generate(**inputs)
print(tokenizer.decode(output_disabled[0], skip_special_tokens=True))

# ุงุณุชุฎุฏู adapter_2
model.set_adapter("adapter_2")
output_enabled = model.generate(**inputs)
print(tokenizer.decode(output_enabled[0], skip_special_tokens=True))
```

## ุชูููู ูุชุนุทูู ุงููุญููุงุช

ุจูุฌุฑุฏ ุฅุถุงูุฉ ูุญูู ุฅูู ูููุฐุฌุ ููููู ุชูููู ุฃู ุชุนุทูู ูุญุฏุฉ ุงููุญูู. ูุชูููู ูุญุฏุฉ ุงููุญูู:

```py
from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
from peft import PeftConfig

model_id = "facebook/opt-350m"
adapter_model_id = "ybelkada/opt-350m-lora"
tokenizer = AutoTokenizer.from_pretrained(model_id)
text = "Hello"
inputs = tokenizer(text, return_tensors="pt")

model = AutoModelForCausalLM.from_pretrained(model_id)
peft_config = PeftConfig.from_pretrained(adapter_model_id)

# ูุจุฏุก ุชุดุบููู ุจุฃูุฒุงู ุนุดูุงุฆูุฉ
peft_config.init_lora_weights = False

model.add_adapter(peft_config)
model.enable_adapters()
output = model.generate(**inputs)
```

ูุฅููุงู ุชุดุบูู ูุญุฏุฉ ุงููุญูู:

```py
model.disable_adapters()
output = model.generate(**inputs)
```

## ุชุฏุฑูุจ ูุญูู PEFT

ูุฏุนู ูุญูู PEFT ูุฆุฉ [`Trainer`] ุจุญูุซ ููููู ุชุฏุฑูุจ ูุญูู ูุญุงูุชู ุงูุงุณุชุฎุฏุงู ุงููุญุฏุฏุฉ. ููู ูุชุทูุจ ููุท ุฅุถุงูุฉ ุจุถุน ุณุทูุฑ ุฃุฎุฑู ูู ุงูุชุนูููุงุช ุงูุจุฑูุฌูุฉ. ุนูู ุณุจูู ุงููุซุงูุ ูุชุฏุฑูุจ ูุญูู LoRA:

<Tip>

ุฅุฐุง ูู ุชูู ูุนุชุงุฏูุง ุนูู ุถุจุท ูููุฐุฌ ุฏููู ุจุงุณุชุฎุฏุงู [`Trainer`ุ ูุฑุงุฌุน ุงูุจุฑูุงูุฌ ุงูุชุนูููู](training) ูุถุจุท ูููุฐุฌ ููุฏุฑุจ ูุณุจููุง.

</Tip>

1. ุญุฏุฏ ุชูููู ุงููุญูู ุจุงุณุชุฎุฏุงู ููุน ุงููููุฉ ููุฑุท ุงููุนููุงุช (ุฑุงุฌุน [`~peft.LoraConfig`] ููุฒูุฏ ูู ุงูุชูุงุตูู ุญูู ูุง ุชูุนูู ูุฑุท ุงููุนููุงุช).

```py
from peft import LoraConfig

peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="CAUSAL_LM"ุ
)
```

2. ุฃุถู ุงููุญูู ุฅูู ุงููููุฐุฌ.

```py
model.add_adapter(peft_config)
```

3. ุงูุขู ููููู ุชูุฑูุฑ ุงููููุฐุฌ ุฅูู [`Trainer`]!

```py
trainer = Trainer(model=model, ...)
trainer.train()
```

ูุญูุธ ูุญูู ุงููุฏุฑุจ ูุชุญูููู ูุฑุฉ ุฃุฎุฑู:

```py
model.save_pretrained(save_dir)
model = AutoModelForCausalLM.from_pretrained(save_dir)
```

## ุฅุถุงูุฉ ุทุจูุงุช ูุงุจูุฉ ููุชุฏุฑูุจ ุฅุถุงููุฉ ุฅูู ูุญูู PEFT

```py
model.save_pretrained(save_dir)
model = AutoModelForCausalLM.from_pretrained(save_dir)
```

## ุฅุถุงูุฉ ุทุจูุงุช ูุงุจูุฉ ููุชุฏุฑูุจ ุฅุถุงููุฉ ุฅูู ูุญูู PEFT

ููููู ุฃูุถูุง ุถุจุท ุทุจูุงุช ูุงุจูุฉ ููุชุฏุฑูุจ ุฅุถุงููุฉ ุฃุนูู ูููุฐุฌ ุจู ูุญููุงุช ูุฑููุฉ ุนู ุทุฑูู ุชูุฑูุฑ `modules_to_save` ูู ุชูููู PEFT ุงูุฎุงุต ุจู. ุนูู ุณุจูู ุงููุซุงูุ ุฅุฐุง ููุช ุชุฑูุฏ ุฃูุถูุง ุถุจุท `lm_head` ุฃุนูู ูููุฐุฌ ุจูุญูู LoRA:

```py
from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
from peft import LoraConfig

model_id = "facebook/opt-350m"
model = AutoModelForCausalLM.from_pretrained(model_id)

lora_config = LoraConfig(
    target_modules=["q_proj", "k_proj"],
    modules_to_save=["lm_head"]ุ
)

model.add_adapter(lora_config)
```

## ูุซุงุฆู API

[[autodoc]] integrations.PeftAdapterMixin
    - load_adapter
    - add_adapter
    - set_adapter
    - disable_adapters
    - enable_adapters
    - active_adapters
    - get_adapter_state_dict




<!--
TODO: (@younesbelkada @stevhliu)
-   Link to PEFT docs for further details
-   Trainer
-   8-bit / 4-bit examples ?
-->