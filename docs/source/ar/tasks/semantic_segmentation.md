# ุชุฌุฒุฆุฉ ุงูุตูุฑ

[[open-in-colab]]

<Youtube id="dKE8SIt9C-w"/>

ุชูุตู ููุงุฐุฌ ุชุฌุฒุฆุฉ ุงูุตูุฑ ุงูููุงุทู ุงูุชู ุชุชูุงูู ูุน ููุงุทู ูุฎุชููุฉ ุฐุงุช ุฃูููุฉ ูู ุตูุฑุฉ. ุชุนูู ูุฐู ุงูููุงุฐุฌ ุนู ุทุฑูู ุชุนููู ุชุณููุฉ ููู ุจูุณู. ููุงู ุนุฏุฉ ุฃููุงุน ูู ุงูุชุฌุฒุฆุฉ: ุงูุชุฌุฒุฆุฉ ุงูุฏูุงููุฉุ ูุชุฌุฒุฆุฉ ุงููุซููุ ูุชุฌุฒุฆุฉ ุงููุดูุฏ ุงูููู.

ูู ูุฐุง ุงูุฏูููุ ุณูู:

1. [ุฅููุงุก ูุธุฑุฉ ุนูู ุงูุฃููุงุน ุงููุฎุชููุฉ ูู ุงูุชุฌุฒุฆุฉ](#ุฃููุงุน-ุงูุชุฌุฒุฆุฉ).
2. [ูุฏูู ูุซุงู ุดุงูู ูุถุจุท ุฏููู ููููุฐุฌ ุงูุชุฌุฒุฆุฉ ุงูุฏูุงููุฉ](#ุถุจุท-ูููุฐุฌ-ุจุดูู-ุฏููู-ููุชุฌุฒุฆุฉ).

ูุจู ุฃู ุชุจุฏุฃุ ุชุฃูุฏ ูู ุชุซุจูุช ุฌููุน ุงูููุชุจุงุช ุงูุถุฑูุฑูุฉ:

```py
# ูู ุจุฅูุบุงุก ุงูุชุนููู ูุชุซุจูุช ุงูููุชุจุงุช ุงูุถุฑูุฑูุฉ
!pip install -q datasets transformers evaluate accelerate
```

ูุญู ูุดุฌุนู ุนูู ุชุณุฌูู ุงูุฏุฎูู ุฅูู ุญุณุงุจ Hugging Face ุงูุฎุงุต ุจู ุญุชู ุชุชููู ูู ุชุญููู ููุดุงุฑูุฉ ูููุฐุฌู ูุน ุงููุฌุชูุน. ุนูุฏูุง ููุทูุจ ููู ุฐููุ ุฃุฏุฎู ุฑูุฒู ููุชุณุฌูู:

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## ุฃููุงุน ุงูุชุฌุฒุฆุฉ

ุชุนูู ุงูุชุฌุฒุฆุฉ ุงูุฏูุงููุฉ ุชุณููุฉ ุฃู ูุฆุฉ ููู ุจูุณู ูู ุงูุตูุฑุฉ. ุฏุนููุง ูููู ูุธุฑุฉ ุนูู ุฅุฎุฑุงุฌ ูููุฐุฌ ุงูุชุฌุฒุฆุฉ ุงูุฏูุงููุฉ. ุณูู ูุนูู ููุณ ุงููุฆุฉ ููู ูุซูู ูู ูุงุฆู ูุตุงุฏูู ูู ุตูุฑุฉุ ุนูู ุณุจูู ุงููุซุงูุ ุณูุชู ุชุตููู ุฌููุน ุงููุทุท ุนูู ุฃููุง "ูุทุฉ" ุจุฏูุงู ูู "cat-1"ุ "cat-2".
ูููููุง ุงุณุชุฎุฏุงู ุฎุท ุฃูุงุจูุจ ุชุฌุฒุฆุฉ ุงูุตูุฑ ูู ุงููุญููุงุช ููุชูุจุค ุจุณุฑุนุฉ ุจูููุฐุฌ ุงูุชุฌุฒุฆุฉ ุงูุฏูุงููุฉ. ุฏุนููุง ูููู ูุธุฑุฉ ุนูู ุตูุฑุฉ ุงููุซุงู.

```python
from transformers import pipeline
from PIL import Image
import requests

url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/segmentation_input.jpg"
image = Image.open(requests.get(url, stream=True).raw)
image
```

<div class="flex justify-center">
     <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/segmentation_input.jpg" alt="Segmentation Input"/>
</div>

ุณูุณุชุฎุฏู [nvidia/segformer-b1-finetuned-cityscapes-1024-1024](https://huggingface.co/nvidia/segformer-b1-finetuned-cityscapes-1024-1024).

```python
semantic_segmentation = pipeline("image-segmentation", "nvidia/segformer-b1-finetuned-cityscapes-1024-1024")
results = semantic_segmentation(image)
results
```
```python
semantic_segmentation = pipeline("image-segmentation", "nvidia/segformer-b1-finetuned-cityscapes-1024-1024")
results = semantic_segmentation(image)
results
```

ูุดูู ุฅุฎุฑุงุฌ ุฎุท ุฃูุงุจูุจ ุงูุชุฌุฒุฆุฉ ููุงุนูุง ููู ูุฆุฉ ูุชููุนุฉ.
```bash
[{'score': None,
  'label': 'road',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': None,
  'label': 'sidewalk',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': None,
  'label': 'building',
  'mask': <PIL.Image.Image image mode=L size=612x414>},
 {'score': None,
  'label': 'wall',
  'mask': <PIL.Image.Image image mode=L size=612x414>},
 {'score': None,
  'label': 'pole',
  'mask': <PIL.Image.Image image mode=L size=612x414>},
 {'score': None,
  'label': 'traffic sign',
  'mask': <PIL.Image.Image image mode=L size=612x414>},
 {'score': None,
  'label': 'vegetation',
  'mask': <PIL.Image.Image image mode=L size=612x414>},
 {'score': None,
  'label': 'terrain',
  'mask': <PIL.Image.Image image mode=L size=612x414>},
 {'score': None,
  'label': 'sky',
  'mask': <PIL.Image.Image image mode=L size=612x414>},
 {'score': None,
  'label': 'car',
  'mask': <PIL.Image.Image image mode=L size=612x414>}
]
```

ุนูุฏ ุงููุธุฑ ุฅูู ุงูููุงุน ููุฆุฉ ุงูุณูุงุฑุงุชุ ูููููุง ุฃู ูุฑู ุฃู ูู ุณูุงุฑุฉ ูุตููุฉ ุจููุณ ุงูููุงุน.

```python
results[-1]["mask"]
```
<div class="flex justify-center">
     <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/semantic_segmentation_output.png" alt="Semantic Segmentation Output"/>
</div>

ูู ุชุฌุฒุฆุฉ ุงููุซููุ ุงููุฏู ููุณ ุชุตููู ูู ุจูุณูุ ูููู ุงูุชูุจุค ุจููุงุน ููู **ูุซูู ูุงุฆู** ูู ุตูุฑุฉ ูุนููุฉ. ุฅูู ูุนูู ุจุดูู ูุดุงุจู ุฌุฏูุง ูููุดู ุนู ุงูุฃุดูุงุกุ ุญูุซ ููุฌุฏ ูุฑุจุน ูุญุฏุฏ ููู ูุซููุ ูููุงู ููุงุน ุชุฌุฒุฆุฉ ุจุฏูุงู ูู ุฐูู. ุณูุณุชุฎุฏู [facebook/mask2former-swin-large-cityscapes-instance](https://huggingface.co/facebook/mask2former-swin-large-cityscapes-instance) ููุฐุง.

```python
instance_segmentation = pipeline("image-segmentation", "facebook/mask2former-swin-large-cityscapes-instance")
results = instance_segmentation(image)
results
```

ููุง ุชุฑูู ุฃุฏูุงูุ ููุงู ุงูุนุฏูุฏ ูู ุงูุณูุงุฑุงุช ุงููุตููุฉุ ููุง ุชูุฌุฏ ุชุตูููุงุช ููุจูุณูุงุช ุจุฎูุงู ุงูุจูุณูุงุช ุงูุชู ุชูุชูู ุฅูู ุณูุงุฑุงุช ูุฃุดุฎุงุต.

```bash
[{'score': 0.999944,
  'label': 'car',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.999945,
  'label': 'car',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.999652,
  'label': 'car',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.903529,
  'label': 'person',
  'mask': <PIL.Image.Image image mode=L size=612x415>}
]
```
ุงูุชุญูู ูู ุฅุญุฏู ุฃููุนุฉ ุงูุณูุงุฑุงุช ุฃุฏูุงู.

```python
results[2]["mask"]
```
<div class="flex justify-center">
     <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/instance_segmentation_output.png" alt="Semantic Segmentation Output"/>
</div>

ุชุฏูุฌ ุงูุชุฌุฒุฆุฉ ุงููุดูุฏูุฉ ุงูุชุฌุฒุฆุฉ ุงูุฏูุงููุฉ ูุชุฌุฒุฆุฉ ุงููุซููุ ุญูุซ ูุชู ุชุตููู ูู ุจูุณู ุฅูู ูุฆุฉ ููุซูู ูู ุชูู ุงููุฆุฉุ ูููุงู ุฃููุนุฉ ูุชุนุฏุฏุฉ ููู ูุซูู ูู ูุฆุฉ. ูููููุง ุงุณุชุฎุฏุงู [facebook/mask2former-swin-large-cityscapes-panoptic](https://huggingface.co/facebook/mask2former-swin-large-cityscapes-panoptic) ููุฐุง.

```python
panoptic_segmentation = pipeline("image-segmentation", "facebook/mask2former-swin-large-cityscapes-panoptic")
results = panoptic_segmentation(image)
results
```
ููุง ุชุฑูู ุฃุฏูุงูุ ูุฏููุง ุงููุฒูุฏ ูู ุงููุฆุงุช. ุณูููู ูุงุญููุง ุจุชูุถูุญ ุฃู ูู ุจูุณู ูุตูู ุฅูู ูุงุญุฏุฉ ูู ุงููุฆุงุช.
```bash
[{'score': 0.999981,
  'label': 'car',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.999958,
  'label': 'car',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.99997,
  'label': 'vegetation',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.999575,
  'label': 'pole',
 
...
```

ุฏุนููุง ููุงุฑู ุฌูุจุง ุฅูู ุฌูุจ ูุฌููุน ุฃููุงุน ุงูุชุฌุฒุฆุฉ.

<div class="flex justify-center">
     <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/segmentation-comparison.png" alt="Segmentation Maps Compared"/>
</div>

ุจุนุฏ ุฑุคูุฉ ุฌููุน ุฃููุงุน ุงูุชุฌุฒุฆุฉุ ุฏุนููุง ูุชุนูู ูู ุถุจุท ูููุฐุฌ ุงูุชุฌุฒุฆุฉ ุงูุฏูุงููุฉ.

ุชุดูู ุงูุชุทุจููุงุช ุงููุงูุนูุฉ ุงูุดุงุฆุนุฉ ููุชุฌุฒุฆุฉ ุงูุฏูุงููุฉ ุชุฏุฑูุจ ุงูุณูุงุฑุงุช ุฐุงุชูุฉ ุงูููุงุฏุฉ ุนูู ุงูุชุนุฑู ุนูู ุงููุดุงุฉ ููุนูููุงุช ุงููุฑูุฑ ุงููููุฉุ ูุชุญุฏูุฏ ุงูุฎูุงูุง ูุงูุชุดููุงุช ูู ุงูุตูุฑ ุงูุทุจูุฉุ ููุฑุงูุจุฉ ุงูุชุบูุฑุงุช ุงูุจูุฆูุฉ ูู ุตูุฑ ุงูุฃููุงุฑ ุงูุตูุงุนูุฉ.

## ุถุจุท ูููุฐุฌ ุจุดูู ุฏููู ูุชุฌุฒุฆุฉ

ุณูููู ุงูุขู ุจูุง ููู:

1. ุถุจุท ุฏููู [SegFormer](https://huggingface.co/docs/transformers/main/en/model_doc/segformer#segformer) ุนูู ูุฌููุนุฉ ุงูุจูุงูุงุช [SceneParse150](https://huggingface.co/datasets/scene_parse_150).
2. ุงุณุชุฎุฏุงู ูููุฐุฌู ุงููุถุจูุท ุจุฏูุฉ ููุงุณุชูุชุงุฌ.

<Tip>

ููุนุฑูุฉ ุฌููุน ุงูุชุตูููุงุช ูููุงุท ุงูุชูุชูุด ุงููุชูุงููุฉ ูุน ูุฐู ุงููููุฉุ ููุตู ุจุงูุชุญูู ูู [ุตูุญุฉ ุงููููุฉ](https://huggingface.co/tasks/image-segmentation)

</Tip>


### ุชุญููู ูุฌููุนุฉ ุจูุงูุงุช SceneParse150

ุงุจุฏุฃ ุจุชุญููู ุฌุฒุก ูุฑุนู ุฃุตุบุฑ ูู ูุฌููุนุฉ ุจูุงูุงุช SceneParse150 ูู ููุชุจุฉ Datasets. ุณูุนุทูู ูุฐุง ูุฑุตุฉ ูุชุฌุฑุจุฉ ูุงูุชุฃูุฏ ูู ุฃู ูู ุดูุก ูุนูู ูุจู ูุถุงุก ุงููุฒูุฏ ูู ุงูููุช ูู ุงูุชุฏุฑูุจ ุนูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงููุงููุฉ.

```py
>>> from datasets import load_dataset

>>> ds = load_dataset("scene_parse_150", split="train[:50]")
```

ูู ุจุชูุณูู ูุฌููุนุฉ ุงูุจูุงูุงุช "train" ุฅูู ูุฌููุนุฉ ุชุฏุฑูุจ ูุงุฎุชุจุงุฑ ุจุงุณุชุฎุฏุงู ุทุฑููุฉ [`~datasets.Dataset.train_test_split`]:

```py
>>> ds = ds.train_test_split(test_size=0.2)
>>> train_ds = ds["train"]
>>> test_ds = ds["test"]
```

ุซู ุงูู ูุธุฑุฉ ุนูู ูุซุงู:

```py
>>> train_ds[0]
{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x683 at 0x7F9B0C201F90>,
 'annotation': <PIL.PngImagePlugin.PngImageFile image mode=L size=512x683 at 0x7F9B0C201DD0>,
 'scene_category': 368}

# ุนุฑุถ ุงูุตูุฑุฉ
>>> train_ds[0]["image"]
```

- `image`: ุตูุฑุฉ PIL ูููุดูุฏ.
- `annotation`: ุตูุฑุฉ PIL ูุฎุฑูุทุฉ ุงูุชุฌุฒุฆุฉุ ูุงูุชู ุชุนุฏ ุฃูุถูุง ูุฏู ุงููููุฐุฌ.
- `scene_category`: ูุนุฑู ูุฆุฉ ูุตู ูุฆุฉ ุงููุดูุฏ ูุซู "ุงููุทุจุฎ" ุฃู "ุงูููุชุจ". ูู ูุฐุง ุงูุฏูููุ ุณุชุญุชุงุฌ ููุท ุฅูู `image` ู`annotation`ุ ูููุงููุง ุนุจุงุฑุฉ ุนู ุตูุฑ PIL.

ุณุชุญุชุงุฌ ุฃูุถูุง ุฅูู ุฅูุดุงุก ูุงููุณ ูููู ุจุชุนููู ูุนุฑู ุงูุชุณููุฉ ุฅูู ูุฆุฉ ุงูุชุณููุฉ ูุงูุชู ุณุชููู ูููุฏุฉ ุนูุฏ ุฅุนุฏุงุฏ ุงููููุฐุฌ ูุงุญููุง. ูู ุจุชูุฒูู ุงูุชุนูููุงุช ูู Hub ูุฅูุดุงุก ุงูููุงููุณ `id2label` ู`label2id`:

```py
>>> import json
>>> from pathlib import Path
>>> from huggingface_hub import hf_hub_download

>>> repo_id = "huggingface/label-files"
>>> filename = "ade20k-id2label.json"
>>> id2label = json.loads(Path(hf_hub_download(repo_id, filename, repo_type="dataset")).read_text())
>>> id2label = {int(k): v for k, v in id2label.items()}
>>> label2id = {v: k for k, v in id2label.items()}
>>> num_labels = len(id2label)
```

#### ูุฌููุนุฉ ุจูุงูุงุช ูุฎุตุตุฉ
#### ูุฌููุนุฉ ุจูุงูุงุช ูุฎุตุตุฉ

ููููู ุฃูุถูุง ุฅูุดุงุก ูุงุณุชุฎุฏุงู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจู ุฅุฐุง ููุช ุชูุถู ุงูุชุฏุฑูุจ ุจุงุณุชุฎุฏุงู ุงููุต ุงูุจุฑูุฌู [run_semantic_segmentation.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/semantic-segmentation/run_semantic_segmentation.py) ุจุฏูุงู ูู ูุซูู ุฏูุชุฑ ุงูููุงุญุธุงุช. ูุชุทูุจ ุงููุต ุงูุจุฑูุฌู ูุง ููู:

1. [`~datasets.DatasetDict`] ูุน ุนููุฏูู [`~datasets.Image`]ุ "image" ู"label"

     ```py
     from datasets import Dataset, DatasetDict, Image

     image_paths_train = ["path/to/image_1.jpg/jpg", "path/to/image_2.jpg/jpg", ..., "path/to/image_n.jpg/jpg"]
     label_paths_train = ["path/to/annotation_1.png", "path/to/annotation_2.png", ..., "path/to/annotation_n.png"]

     image_paths_validation = [...]
     label_paths_validation = [...]

     def create_dataset(image_paths, label_paths):
         dataset = Dataset.from_dict({"image": sorted(image_paths),
                                     "label": sorted(label_paths)})
         dataset = dataset.cast_column("image", Image())
         dataset = dataset.cast_column("label", Image())
         return dataset

     # ุงูุฎุทูุฉ 1: ุฅูุดุงุก ูุงุฆูุงุช Dataset
     train_dataset = create_dataset(image_paths_train, label_paths_train)
     validation_dataset = create_dataset(image_paths_validation, label_paths_validation)

     # ุงูุฎุทูุฉ 2: ุฅูุดุงุก DatasetDict
     dataset = DatasetDict({
          "train": train_dataset,
          "validation": validation_dataset,
          }
     )

     # ุงูุฎุทูุฉ 3: ุงูุฏูุน ุฅูู Hub (ููุชุฑุถ ุฃูู ููุช ุจุชุดุบูู ุงูุฃูุฑ huggingface-cli login ูู terminal/notebook)
     dataset.push_to_hub("your-name/dataset-repo")

     # ุจุดูู ุงุฎุชูุงุฑูุ ููููู ุงูุฏูุน ุฅูู ูุณุชูุฏุน ุฎุงุต ุนูู Hub
     # dataset.push_to_hub("name of repo on the hub", private=True)
     ```

2. ูุงููุณ id2label ุงูุฐู ูููู ุจุชุนููู ุฃุนุฏุงุฏ ุตุญูุญุฉ ูููุฆุฉ ุฅูู ุฃุณูุงุก ูุฆุงุชูุง

     ```py
     import json
     # ูุซุงู ุจุณูุท
     id2label = {0: 'cat', 1: 'dog'}
     with open('id2label.json', 'w') as fp:
     json.dump(id2label, fp)
     ```

ููุซุงูุ ุงูู ูุธุฑุฉ ุนูู [ูุฌููุนุฉ ุงูุจูุงูุงุช ูุฐู](https://huggingface.co/datasets/nielsr/ade20k-demo) ุงูุชู ุชู ุฅูุดุงุคูุง ุจุงูุฎุทูุงุช ุงูููุถุญุฉ ุฃุนูุงู.

### ูุนุงูุฌุฉ ูุณุจูุฉ

ุงูุฎุทูุฉ ุงูุชุงููุฉ ูู ุชุญููู ูุนุงูุฌ ุตูุฑ SegFormer ูุฅุนุฏุงุฏ ุงูุตูุฑ ูุงูุชุนูููุงุช ุงูุชูุถูุญูุฉ ูููููุฐุฌ. ุชุณุชุฎุฏู ุจุนุถ ูุฌููุนุงุช ุงูุจูุงูุงุชุ ูุซู ูุฐูุ ุงูููุฑุณ ุงูุตูุฑู ููุฆุฉ ุฎูููุฉ. ููุน ุฐููุ ูุฅู ูุฆุฉ ุงูุฎูููุฉ ููุณุช ูุฏุฑุฌุฉ ุจุงููุนู ูู 150 ูุฆุฉุ ูุฐูู ุณุชุญุชุงุฌ ุฅูู ุชุนููู `do_reduce_labels=True` ูุทุฑุญ ูุงุญุฏ ูู ุฌููุน ุงูุชุณููุงุช. ูุชู ุงุณุชุจุฏุงู ุงูููุฑุณ ุงูุตูุฑู ุจู `255` ุญุชู ูุชู ุชุฌุงููู ุจูุงุณุทุฉ ุฏุงูุฉ ุงูุฎุณุงุฑุฉ SegFormer:

```py
>>> from transformers import AutoImageProcessor

>>> checkpoint = "nvidia/mit-b0"
>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint, do_reduce_labels=True)
```

<frameworkcontent>
<pt>

ูู ุงูุดุงุฆุน ุชุทุจูู ุจุนุถ ุนูููุงุช ุฒูุงุฏุฉ ุงูุจูุงูุงุช ุนูู ูุฌููุนุฉ ุจูุงูุงุช ุงูุตูุฑ ูุฌุนู ุงููููุฐุฌ ุฃูุซุฑ ููุฉ ุถุฏ ุงูุฅูุฑุงุท ูู ุงูุชุฎุตูุต. ูู ูุฐุง ุงูุฏูููุ ุณุชุณุชุฎุฏู ุฏุงูุฉ [`ColorJitter`](https://pytorch.org/vision/stable/generated/torchvision.transforms.ColorJitter.html) ูู [torchvision](https://pytorch.org/vision/stable/index.html) ูุชุบููุฑ ุงูุฎุตุงุฆุต ุงูููููุฉ ููุตูุฑุฉ ุจุดูู ุนุดูุงุฆูุ ูููู ููููู ุฃูุถูุง ุงุณุชุฎุฏุงู ุฃู ููุชุจุฉ ุตูุฑ ุชูุถููุง.

```py
>>> from torchvision.transforms import ColorJitter

>>> jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)
```

ุงูุขู ูู ุจุฅูุดุงุก ุฏุงูุชูู ููุนุงูุฌุฉ ูุณุจูุฉ
ุจุงูุชุฃููุฏุ ุณุฃููู ุจุชุฑุฌูุฉ ุงููุต ูุน ุงุชุจุงุน ุงูุชุนูููุงุช ุงูุชู ูุฏูุชูุง.

```py
>>> training_args = TrainingArguments(
...     output_dir="segformer-b0-scene-parse-150",
...     learning_rate=6e-5,
...     num_train_epochs=50,
...     per_device_train_batch_size=2,
...     per_device_eval_batch_size=2,
...     save_total_limit=3,
...     eval_strategy="steps",
...     save_strategy="steps",
...     save_steps=20,
...     eval_steps=20,
...     logging_steps=1,
...     eval_accumulation_steps=5,
...     remove_unused_columns=False,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=train_ds,
...     eval_dataset=test_ds,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

ุจุนุฏ ุงูุงูุชูุงุก ูู ุงูุชุฏุฑูุจุ ุดุงุฑู ูููุฐุฌู ุนูู Hub ุจุงุณุชุฎุฏุงู ุทุฑููุฉ [~transformers.Trainer.push_to_hub] ุญุชู ูุชููู ุงูุฌููุน ูู ุงุณุชุฎุฏุงู ูููุฐุฌู:

```py
>>> trainer.push_to_hub()
```
</pt>
</frameworkcontent>

<frameworkcontent>
<tf>
<Tip>

ุฅุฐุง ูู ุชูู ูุนุชุงุฏูุง ุนูู ุถุจุท ูููุฐุฌ ุจุงุณุชุฎุฏุงู Kerasุ ูุฑุงุฌุน [ุงูุจุฑูุงูุฌ ุงูุชุนูููู ุงูุฃุณุงุณู](./training#train-a-tensorflow-model-with-keras) ุฃููุงู!

</Tip>

ูุถุจุท ูููุฐุฌ ูู TensorFlowุ ุงุชุจุน ุงูุฎุทูุงุช ุงูุชุงููุฉ:

1. ุญุฏุฏ ูุฑุท ุงููุนููุงุช ุงูุชุฏุฑูุจูุฉุ ููู ุจุฅุนุฏุงุฏ ูุญุณู ูุฌุฏูู ูุนุฏู ุงูุชุนูู.
2. ูู ุจุชูููุฐ ูุซูู ููููุฐุฌ ูุณุจู ุงูุชุฏุฑูุจ.
3. ูู ุจุชุญููู ูุฌููุนุฉ ุจูุงูุงุช ๐ค ุฅูู `tf.data.Dataset`.
4. ูู ุจุชุฌููุน ูููุฐุฌู.
5. ุฃุถู ุงุณุชุฏุนุงุกุงุช ุฑุฌูุน ูุญุณุงุจ ุงูููุงููุณ ูุชุญููู ูููุฐุฌู ุฅูู ๐ค Hub
6. ุงุณุชุฎุฏู ุทุฑููุฉ `fit()` ูุชุดุบูู ุงูุชุฏุฑูุจ.

ุงุจุฏุฃ ุจุชุญุฏูุฏ ูุฑุท ุงููุนููุงุช ูุงููุญุณู ูุฌุฏูู ูุนุฏู ุงูุชุนูู:

```py
>>> from transformers import create_optimizer

>>> batch_size = 2
>>> num_epochs = 50
>>> num_train_steps = len(train_ds) * num_epochs
>>> learning_rate = 6e-5
>>> weight_decay_rate = 0.01

>>> optimizer, lr_schedule = create_optimizer(
...     init_lr=learning_rate,
...     num_train_steps=num_train_steps,
...     weight_decay_rate=weight_decay_rate,
...     num_warmup_steps=0,
... )
```

ุจุนุฏ ุฐููุ ูู ุจุชุญููู SegFormer ูุน [TFAutoModelForSemanticSegmentation] ุฌูุจูุง ุฅูู ุฌูุจ ูุน ุชุนูููุงุช ุงูุชุณููุงุชุ ููู ุจุชุฌููุนูุง ูุน ุงููุญุณู. ูุงุญุธ ุฃู ุฌููุน ููุงุฐุฌ Transformers ุชุญุชูู ุนูู ุฏุงูุฉ ุฎุณุงุฑุฉ ุฐุงุช ุตูุฉ ุจุงููููุฉ ุจุดูู ุงูุชุฑุงุถูุ ูุฐูู ูุง ุชุญุชุงุฌ ุฅูู ุชุญุฏูุฏ ูุงุญุฏุฉ ูุง ูู ุชุฑุบุจ ูู ุฐูู:

```py
>>> from transformers import TFAutoModelForSemanticSegmentation

>>> model = TFAutoModelForSemanticSegmentation.from_pretrained(
...     checkpoint,
...     id2label=id2label,
...     label2id=label2id,
... )
>>> model.compile(optimizer=optimizer)  # ูุง ุชูุฌุฏ ุญุฌุฉ ุงูุฎุณุงุฑุฉ!
```

ูู ุจุชุญููู ูุฌููุนุงุช ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจู ุฅูู ุชูุณูู `tf.data.Dataset` ุจุงุณุชุฎุฏุงู [~datasets.Dataset.to_tf_dataset] ู [DefaultDataCollator]:

```py
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator(return_tensors="tf")

>>> tf_train_dataset = train_ds.to_tf_dataset(
...     columns=["pixel_values", "label"],
...     shuffle=True,
...     batch_size=batch_size,
...     collate_fn=data_collator,
... )

>>> tf_eval_dataset = test_ds.to_tf_dataset(
...     columns=["pixel_values", "label"],
...     shuffle=True,
...     batch_size=batch_size,
...     collate_fn=data_collator,
... )
```

ูุญุณุงุจ ุงูุฏูุฉ ูู ุงูุชููุนุงุช ูุชุญููู ูููุฐุฌู ุฅูู ๐ค Hubุ ุงุณุชุฎุฏู [Keras callbacks](../main_classes/keras_callbacks).
ูุฑุฑ ุฏุงูุฉ `compute_metrics` ุฅูู [KerasMetricCallback]ุ
ูุงุณุชุฎุฏู [PushToHubCallback] ูุชุญููู ุงููููุฐุฌ:

```py
>>> from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback

>>> metric_callback = KerasMetricCallback(
...     metric_fn=compute_metrics, eval_dataset=tf_eval_dataset, batch_size=batch_size, label_cols=["labels"]
... )

>>> push_to_hub_callback = PushToHubCallback(output_dir="scene_segmentation", tokenizer=image_processor)

>>> callbacks = [metric_callback, push_to_hub_callback]
```

ุฃุฎูุฑูุงุ ุฃูุช ูุณุชุนุฏ ูุชุฏุฑูุจ ูููุฐุฌู! ุงุชุตู ุจู `fit()` ุจุงุณุชุฎุฏุงู ูุฌููุนุงุช ุงูุจูุงูุงุช ุงูุชุฏุฑูุจูุฉ ูุงูุชุญูู ูู ุตุญุชูุงุ ูุนุฏุฏ ุงูุนุตูุฑุ
ูุงุณุชุฏุนุงุกุงุช ุงูุฑุฌูุน ุงูุฎุงุตุฉ ุจู ูุถุจุท ุงููููุฐุฌ:
ุฃุฎูุฑูุงุ ุฃูุช ูุณุชุนุฏ ูุชุฏุฑูุจ ูููุฐุฌู! ุงุชุตู ุจู `fit()` ุจุงุณุชุฎุฏุงู ูุฌููุนุงุช ุงูุจูุงูุงุช ุงูุชุฏุฑูุจูุฉ ูุงูุชุญูู ูู ุตุญุชูุงุ ูุนุฏุฏ ุงูุนุตูุฑุ
ูุงุณุชุฏุนุงุกุงุช ุงูุฑุฌูุน ุงูุฎุงุตุฉ ุจู ูุถุจุท ุงููููุฐุฌ:

```py
>>> model.fit(
...     tf_train_dataset,
...     validation_data=tf_eval_dataset,
...     callbacks=callbacks,
...     epochs=num_epochs,
... )
```

ุชูุงูููุง! ููุฏ ุถุจุทุช ูููุฐุฌู ูุดุงุฑูุชู ุนูู ๐ค Hub. ููููู ุงูุขู ุงุณุชุฎุฏุงูู ููุงุณุชูุชุงุฌ!
</tf>
</frameworkcontent>

### ุงูุงุณุชูุชุงุฌ

ุฑุงุฆุนุ ุงูุขู ุจุนุฏ ุฃู ุถุจุทุช ูููุฐุฌูุงุ ููููู ุงุณุชุฎุฏุงูู ููุงุณุชูุชุงุฌ!

ุฃุนุฏ ุชุญููู ูุฌููุนุฉ ุงูุจูุงูุงุช ูุชุญููู ุตูุฑุฉ ููุงุณุชุฏูุงู.

```py
>>> from datasets import load_dataset

>>> ds = load_dataset("scene_parse_150", split="train[:50]")
>>> ds = ds.train_test_split(test_size=0.2)
>>> test_ds = ds["test"]
>>> image = ds["test"][0]["image"]
>>> image
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-image.png" alt="ุตูุฑุฉ ูุบุฑูุฉ ููู"/>
</div>

<frameworkcontent>
<pt>

ุณูุฑู ุงูุขู ููููุฉ ุงูุงุณุชุฏูุงู ุจุฏูู ุฎุท ุฃูุงุจูุจ. ูู ุจูุนุงูุฌุฉ ุงูุตูุฑุฉ ุจูุนุงูุฌ ุงูุตูุฑ ููู ุจูุถุน `pixel_values` ุนูู GPU:

```py
>>> device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # ุงุณุชุฎุฏู GPU ุฅุฐุง ูุงู ูุชุงุญูุงุ ูุฅูุง ุงุณุชุฎุฏู CPU
>>> encoding = image_processor(image, return_tensors="pt")
>>> pixel_values = encoding.pixel_values.to(device)
```

ูุฑุฑ ุงููุฏุฎูุงุช ุฅูู ุงููููุฐุฌ ูุฃุนุฏ ุงูุฎุฑุฌ `logits`:

```py
>>> outputs = model(pixel_values=pixel_values)
>>> logits = outputs.logits.cpu()
```

ุจุนุฏ ุฐููุ ูู ุจุฅุนุงุฏุฉ ุชุญุฌูู logits ุฅูู ุญุฌู ุงูุตูุฑุฉ ุงูุฃุตูู:

```py
>>> upsampled_logits = nn.functional.interpolate(
...     logits,
...     size=image.size[::-1],
...     mode="bilinear",
...     align_corners=False,
... )

>>> pred_seg = upsampled_logits.argmax(dim=1)[0]
```

</pt>
</frameworkcontent>

<frameworkcontent>
<tf>
ูู ุจุชุญููู ูุนุงูุฌ ุงูุตูุฑ ููุนุงูุฌุฉ ุงูุตูุฑุฉ ูุฅุนุงุฏุฉ ุงูุฅุฏุฎุงู ูุชูุงุณู TensorFlow:

```py
>>> from transformers import AutoImageProcessor

>>> image_processor = AutoImageProcessor.from_pretrained("MariaK/scene_segmentation")
>>> inputs = image_processor(image, return_tensors="tf")
```

ูุฑุฑ ุงููุฏุฎูุงุช ุฅูู ุงููููุฐุฌ ูุฃุนุฏ ุงูุฎุฑุฌ `logits`:

```py
>>> from transformers import TFAutoModelForSemanticSegmentation

>>> model = TFAutoModelForSemanticSegmentation.from_pretrained("MariaK/scene_segmentation")
>>> logits = model(**inputs).logits
```

ุจุนุฏ ุฐููุ ูู ุจุฅุนุงุฏุฉ ุชุญุฌูู logits ุฅูู ุญุฌู ุงูุตูุฑุฉ ุงูุฃุตูู ููู ุจุชุทุจูู argmax ุนูู ุงูุจุนุฏ class:
```py
>>> logits = tf.transpose(logits, [0, 2, 3, 1])

>>> upsampled_logits = tf.image.resize(
...     logits,
...     # ูุนูุณ ุดูู `image` ูุฃู `image.size` ูุนูุฏ ุงูุนุฑุถ ูุงูุงุฑุชูุงุน.
...     image.size[::-1],
... )

>>> pred_seg = tf.math.argmax(upsampled_logits, axis=-1)[0]
```

</tf>
</frameworkcontent>

ูุนุฑุถ ุงููุชุงุฆุฌุ ูู ุจุชุญููู ููุญุฉ ุงูุฃููุงู ุงูุฎุงุตุฉ ุจูุฌููุนุฉ ุงูุจูุงูุงุช [dataset color palette](https://github.com/tensorflow/models/blob/3f1ca33afe3c1631b733ea7e40c294273b9e406d/research/deeplab/utils/get_dataset_colormap.py#L51) ูู `ade_palette()` ุงูุชู ุชููู ุจุชุนููู ูู ูุฆุฉ ุฅูู ููู RGB ุงูุฎุงุตุฉ ุจูุง.

```py
def ade_palette():
  return np.asarray([
      [0, 0, 0],
      [120, 120, 120],
      [180, 120, 120],
      [6, 230, 230],
      [80, 50, 50],
      [4, 200, 3],
      [120, 120, 80],
      [140, 140, 140],
      [204, 5, 255],
      [230, 230, 230],
      [4, 250, 7],
      [224, 5, 255],
      [235, 255, 7],
      [150, 5, 61],
      [120, 120, 70],
      [8, 255, 51],
      [255, 6, 82],
      [143, 255, 140],
      [204, 255, 4],
      [255, 51, 7],
      [204, 70, 3],
      [0, 102, 200],
      [61, 230, 250],
      [255, 6, 51],
      [11, 102, 255],
      [255, 7, 71],
      [255, 9, 224],
      [9, 7, 230],
      [220, 220, 220],
      [255, 9, 92],
      [112, 9, 255],
      [8, 255, 214],
      [7, 255, 224],
      [255, 184, 6],
      [10, 255, 71],
      [255, 41, 10],
      [7, 255, 255],
      [224, 255, 8],
      [102, 8, 255],
      [255, 61, 6],
      [255, 194, 7],
      [255, 122, 8],
      [0, 255, 20],
      [255, 8, 41],
      [255, 5, 153],
      [6, 51, 255],
      [235, 12, 255],
      [160, 150, 20],
      [0, 163, 255],
      [140, 140, 140],
      [250, 10, 15],
      [20, 255, 0],
      [31, 255, 0],
      [255, 31, 0],
      [255, 224, 0],
      [153, 255, 0],
      [0, 0, 255],
      [255, 71, 0],
      [0, 235, 255],
      [0, 173, 255],
      [31, 0, 255],
      [11, 200, 200],
      [255, 82, 0],
      [0, 255, 245],
      [0, 61, 255],
      [0, 255, 112],
      [0, 255, 133],
      [255, 0, 0],
      [255, 163, 0],
      [255, 102, 0],
      [194, 255, 0],
      [0, 143, 255],
      [51, 255, 0],
      [0, 82, 255],
      [0, 255, 41],
      [0, 255, 173],
      [10, 0, 255],
      [173, 255, 0],
      [0, 255, 153],
      [255, 92, 0],
      [255, 0, 255],
      [255, 0, 245],
      [255, 0, 102],
      [255, 173, 0],
      [255, 0, 20],
      [255, 184, 184],
      [0, 31, 255],
      [0, 255, 61],
      [0, 71, 255],
      [255, 0, 204],
      [0, 255, 194],
      [0, 255, 82],
      [0, 10, 255],
      [0, 112, 255],
      [51, 0, 255],
      [0, 194, 255],
      [0, 122, 255],
      [0, 255, 163],
      [255, 153, 0],
      [0, 255, 10],
      [255, 112, 0],
      [143, 255, 0],
      [82, 0, 255],
      [163, 255, 0],
      [255, 235, 0],
      [8, 184, 170],
      [133, 0, 255],
      [0, 255, 92],
      [184, 0, 255],
      [255, 0, 31],
      [0, 184, 255],
      [0, 214, 255],
      [255, 0, 112],
      [92, 255, 0],
      [0, 224, 255],
      [112, 224, 255],
      [70, 184, 160],
      [163, 0, 255],
      [153, 0, 255],
      [71, 255, 0],
      [255, 0, 163],
      [255, 204, 0],
      [255, 0, 143],
      [0, 255, 235],
      [133, 255, 0],
      [255, 0, 235],
      [245, 0, 255],
      [255, 0, 122],
      [255, 245, 0],
      [10, 190, 212],
      [214, 255, 0],
      [0, 204, 255],
      [20, 0, 255],
      [255, 255, 0],
      [0, 153, 255],
      [0, 41, 255],
      [0, 255, 204],
      [41, 0, 255],
      [41, 255, 0],
      [173, 0, 255],
      [0, 245, 255],
      [71, 0, 255],
      [122, 0, 255],
      [0, 255, 184],
      [0, 92, 255],
      [184, 255, 0],
      [0, 133, 255],
      [255, 214, 0],
      [25, 194, 194],
      [102, 255, 0],
      [92, 0, 255],
  ])
```

Then you can combine and plot your image and the predicted segmentation map:

```py
>>> import matplotlib.pyplot as plt
>>> import numpy as np

>>> color_seg = np.zeros((pred_seg.shape[0], pred_seg.shape[1], 3), dtype=np.uint8)
>>> palette = np.array(ade_palette())
>>> for label, color in enumerate(palette):
...     color_seg[pred_seg == label, :] = color
>>> color_seg = color_seg[..., ::-1]  # convert to BGR

>>> img = np.array(image) * 0.5 + color_seg * 0.5  # plot the image with the segmentation map
>>> img = img.astype(np.uint8)

>>> plt.figure(figsize=(15, 10))
>>> plt.imshow(img)
>>> plt.show()
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-preds.png" alt="Image of bedroom overlaid with segmentation map"/>
</div>