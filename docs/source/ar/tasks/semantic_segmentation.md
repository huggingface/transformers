<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ± (Image Segmentation)

[[open-in-colab]]

<Youtube id="dKE8SIt9C-w"/>

Ù†Ù…Ø§Ø°Ø¬ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ± ØªÙØµÙ„ Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø© Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø§Ù„Ù…Ø®ØªÙ„ÙØ© ÙÙŠ Ø§Ù„ØµÙˆØ±Ø©. ØªØ¹Ù…Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø¥Ø³Ù†Ø§Ø¯ ØªØµÙ†ÙŠÙ Ù„ÙƒÙ„ Ø¨ÙƒØ³Ù„. ØªÙˆØ¬Ø¯ Ø¹Ø¯Ø© Ø£Ù†ÙˆØ§Ø¹ Ù…Ù† Ø§Ù„ØªÙ‚Ø³ÙŠÙ…: Ø§Ù„ØªØ¬Ø²Ø¦Ø© Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ© (Semantic Segmentation)ØŒ ÙˆØªØ¬Ø²Ø¦Ø© Ø§Ù„Ù…Ø«ÙŠÙ„ (Instance Segmentation)ØŒ ÙˆØ§Ù„ØªØ¬Ø²Ø¦Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø© (Panoptic Segmentation).

ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø³Ù†Ù‚ÙˆÙ… Ø¨Ù…Ø§ ÙŠÙ„ÙŠ:
1. Ø¥Ù„Ù‚Ø§Ø¡ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ù„Ù„ØªÙ‚Ø³ÙŠÙ….
2. ØªÙ‚Ø¯ÙŠÙ… Ù…Ø«Ø§Ù„ ÙƒØ§Ù…Ù„ Ù„Ø¶Ø¨Ø· Ø¯Ù‚ÙŠÙ‚ (Fine-tuning) Ù„Ù„ØªØ¬Ø²Ø¦Ø© Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ©.

Ù‚Ø¨Ù„ Ø£Ù† ØªØ¨Ø¯Ø£ØŒ ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø©:

```py
# uncomment to install the necessary libraries
!pip install -q datasets transformers evaluate accelerate
```

Ù†ÙˆØµÙŠÙƒ Ø¨ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¥Ù„Ù‰ Ø­Ø³Ø§Ø¨Ùƒ Ø¹Ù„Ù‰ Hugging Face Ø­ØªÙ‰ ØªØªÙ…ÙƒÙ† Ù…Ù† Ø±ÙØ¹ ÙˆÙ…Ø´Ø§Ø±ÙƒØ© Ù†Ù…ÙˆØ°Ø¬Ùƒ Ù…Ø¹ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹. Ø¹Ù†Ø¯ Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø©ØŒ Ø£Ø¯Ø®Ù„ Ø§Ù„Ø±Ù…Ø² (token) Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„:

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ØªÙ‚Ø³ÙŠÙ… (Types of Segmentation)

ØªÙØ³Ù†Ø¯ Ø§Ù„ØªØ¬Ø²Ø¦Ø© Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ© ØªØµÙ†ÙŠÙÙ‹Ø§ Ø£Ùˆ ÙØ¦Ø© Ù„ÙƒÙ„ Ø¨ÙƒØ³Ù„ ÙÙŠ Ø§Ù„ØµÙˆØ±Ø©. Ø¯Ø¹Ù†Ø§ Ù†Ù„Ù‚ÙŠ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ù…Ø®Ø±Ø¬Ø§Øª Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ¬Ø²Ø¦Ø© Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ©. Ø³ÙŠÙØ³Ù†Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù†ÙØ³ Ø§Ù„ÙØ¦Ø© Ù„ÙƒÙ„ Ù…Ø«ÙŠÙ„ Ù…Ù† Ø§Ù„ÙƒØ§Ø¦Ù† Ø§Ù„Ø°ÙŠ ÙŠØµØ§Ø¯ÙÙ‡ ÙÙŠ Ø§Ù„ØµÙˆØ±Ø©Ø› Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø³ÙŠØªÙ… ØªØµÙ†ÙŠÙ ÙƒÙ„ Ø§Ù„Ù‚Ø·Ø· ÙƒÙ€ "cat" Ø¨Ø¯Ù„Ù‹Ø§ Ù…Ù† "cat-1"ØŒ "cat-2".
ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù†Ø¨ÙˆØ¨ `image-segmentation` ÙÙŠ Ù…ÙƒØªØ¨Ø© Transformers Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø³Ø±ÙŠØ¹Ù‹Ø§ Ø¨Ù†Ù…ÙˆØ°Ø¬ ØªØ¬Ø²Ø¦Ø© Ø¯Ù„Ø§Ù„ÙŠØ©. Ù„Ù†ØªÙØ­Øµ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø«Ø§Ù„.

```python
from transformers import pipeline
from PIL import Image
import requests

url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/segmentation_input.jpg"
image = Image.open(requests.get(url, stream=True).raw)
image
```

<div class="flex justify-center">
     <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/segmentation_input.jpg" alt="Segmentation Input"/>
</div>

Ø³Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ [nvidia/segformer-b1-finetuned-cityscapes-1024-1024](https://huggingface.co/nvidia/segformer-b1-finetuned-cityscapes-1024-1024).

```python
semantic_segmentation = pipeline("image-segmentation", "nvidia/segformer-b1-finetuned-cityscapes-1024-1024")
results = semantic_segmentation(image)
results
```

ÙŠØªØ¶Ù…Ù† Ø®Ø±Ø¬ Ø£Ù†Ø¨ÙˆØ¨ Ø§Ù„ØªØ¬Ø²Ø¦Ø© Ù‚Ù†Ø§Ø¹Ù‹Ø§ (mask) Ù„ÙƒÙ„ ÙØ¦Ø© Ù…ØªÙ†Ø¨Ø£ Ø¨Ù‡Ø§.
```bash
[{'score': None,
  'label': 'road',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': None,
  'label': 'sidewalk',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': None,
  'label': 'building',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': None,
  'label': 'wall',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': None,
  'label': 'pole',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': None,
  'label': 'traffic sign',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': None,
  'label': 'vegetation',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': None,
  'label': 'terrain',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': None,
  'label': 'sky',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': None,
  'label': 'car',
  'mask': <PIL.Image.Image image mode=L size=612x415>}]
```

Ø¹Ù†Ø¯ Ø§Ù„Ù†Ø¸Ø± Ø¥Ù„Ù‰ Ù‚Ù†Ø§Ø¹ ÙØ¦Ø© Ø§Ù„Ø³ÙŠØ§Ø±Ø©ØŒ ÙŠÙ…ÙƒÙ†Ù†Ø§ Ù…Ù„Ø§Ø­Ø¸Ø© Ø£Ù† ÙƒÙ„ Ø§Ù„Ø³ÙŠØ§Ø±Ø§Øª Ù…ÙØµÙ†Ù‘ÙØ© Ø¨Ù†ÙØ³ Ø§Ù„Ù‚Ù†Ø§Ø¹.

```python
results[-1]["mask"]
```
<div class="flex justify-center">
     <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/semantic_segmentation_output.png" alt="Semantic Segmentation Output"/>
</div>

ÙÙŠ ØªØ¬Ø²Ø¦Ø© Ø§Ù„Ù…Ø«ÙŠÙ„ (Instance Segmentation)ØŒ Ø§Ù„Ù‡Ø¯Ù Ù„ÙŠØ³ ØªØµÙ†ÙŠÙ ÙƒÙ„ Ø¨ÙƒØ³Ù„ØŒ ÙˆØ¥Ù†Ù…Ø§ ØªÙˆÙ‚Ø¹ Ù‚Ù†Ø§Ø¹ Ù„ÙƒÙ„ Ù…Ø«ÙŠÙ„ Ù…Ù† ÙƒØ§Ø¦Ù† Ù…Ø¹ÙŠÙ† ÙÙŠ Ø§Ù„ØµÙˆØ±Ø©. ÙŠØ´Ø¨Ù‡ Ø°Ù„Ùƒ ÙƒØ«ÙŠØ±Ù‹Ø§ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª (Object Detection)ØŒ Ø­ÙŠØ« ÙŠÙˆØ¬Ø¯ ØµÙ†Ø¯ÙˆÙ‚ Ø¥Ø­Ø§Ø·Ø© Ù„ÙƒÙ„ Ù…Ø«ÙŠÙ„ØŒ Ù„ÙƒÙ† Ø¨Ø¯Ù„Ù‹Ø§ Ù…Ù† Ø§Ù„ØµÙ†Ø¯ÙˆÙ‚ ÙŠÙˆØ¬Ø¯ Ù‚Ù†Ø§Ø¹ ØªØ¬Ø²Ø¦Ø©. Ø³Ù†Ø³ØªØ®Ø¯Ù… [facebook/mask2former-swin-large-cityscapes-instance](https://huggingface.co/facebook/mask2former-swin-large-cityscapes-instance) Ù„Ù‡Ø°Ø§ Ø§Ù„ØºØ±Ø¶.

```python
instance_segmentation = pipeline("image-segmentation", "facebook/mask2former-swin-large-cityscapes-instance")
results = instance_segmentation(image)
results
```

ÙƒÙ…Ø§ ØªØ±Ù‰ Ø£Ø¯Ù†Ø§Ù‡ØŒ Ù‡Ù†Ø§Ùƒ Ø¹Ø¯Ø© Ø³ÙŠØ§Ø±Ø§Øª Ù…ÙØµÙ†Ù‘ÙØ©ØŒ ÙˆÙ„Ø§ ÙŠÙˆØ¬Ø¯ ØªØµÙ†ÙŠÙ Ù„Ù„Ø¨ÙƒØ³Ù„Ø§Øª Ø§Ù„Ø£Ø®Ø±Ù‰ Ø¨Ø®Ù„Ø§Ù ØªÙ„Ùƒ Ø§Ù„ØªÙŠ ØªÙ†ØªÙ…ÙŠ Ø¥Ù„Ù‰ Ù…Ø«ÙŠÙ„Ø§Øª "car" Ùˆ"person".

```bash
[{'score': 0.999944,
  'label': 'car',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.999945,
  'label': 'car',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.999652,
  'label': 'car',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.903529,
  'label': 'person',
  'mask': <PIL.Image.Image image mode=L size=612x415>}]
```
ØªÙØ­Ù‘Øµ Ø£Ø­Ø¯ Ø£Ù‚Ù†Ø¹Ø© Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ø¯Ù†Ø§Ù‡.

```python
results[2]["mask"]
```
<div class="flex justify-center">
     <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/instance_segmentation_output.png" alt="Semantic Segmentation Output"/>
</div>

ØªØ¬Ù…Ø¹ Ø§Ù„ØªØ¬Ø²Ø¦Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø© (Panoptic Segmentation) Ø¨ÙŠÙ† Ø§Ù„ØªØ¬Ø²Ø¦Ø© Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ© ÙˆØªØ¬Ø²Ø¦Ø© Ø§Ù„Ù…Ø«ÙŠÙ„ØŒ Ø­ÙŠØ« ÙŠÙØµÙ†Ù‘Ù ÙƒÙ„ Ø¨ÙƒØ³Ù„ Ø¶Ù…Ù† ÙØ¦Ø© ÙˆÙ…Ø«ÙŠÙ„ Ù„ØªÙ„Ùƒ Ø§Ù„ÙØ¦Ø©ØŒ ÙˆØªÙˆØ¬Ø¯ Ø¹Ø¯Ø© Ø£Ù‚Ù†Ø¹Ø© Ù„ÙƒÙ„ Ù…Ø«ÙŠÙ„ Ù…Ù† Ø§Ù„ÙØ¦Ø©. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø§Ø³ØªØ®Ø¯Ø§Ù… [facebook/mask2former-swin-large-cityscapes-panoptic](https://huggingface.co/facebook/mask2former-swin-large-cityscapes-panoptic) Ù„Ù‡Ø°Ø§.

```python
panoptic_segmentation = pipeline("image-segmentation", "facebook/mask2former-swin-large-cityscapes-panoptic")
results = panoptic_segmentation(image)
results
```
ÙƒÙ…Ø§ ØªØ±Ù‰ Ø£Ø¯Ù†Ø§Ù‡ØŒ Ù„Ø¯ÙŠÙ†Ø§ ÙØ¦Ø§Øª Ø£ÙƒØ«Ø±. Ø³Ù†ÙˆØ¶Ù‘Ø­ Ù„Ø§Ø­Ù‚Ù‹Ø§ Ø£Ù† ÙƒÙ„ Ø¨ÙƒØ³Ù„ Ù…ÙØµÙ†Ù‘Ù Ø¶Ù…Ù† Ø¥Ø­Ø¯Ù‰ Ù‡Ø°Ù‡ Ø§Ù„ÙØ¦Ø§Øª.

```bash
[{'score': 0.999981,
  'label': 'car',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.999958,
  'label': 'car',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.99997,
  'label': 'vegetation',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.999575,
  'label': 'pole',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.999958,
  'label': 'building',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.999634,
  'label': 'road',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.996092,
  'label': 'sidewalk',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.999221,
  'label': 'car',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.99987,
  'label': 'sky',
  'mask': <PIL.Image.Image image mode=L size=612x415>}]
```

Ø¯Ø¹Ù†Ø§ Ù†Ø¬Ø±ÙŠ Ù…Ù‚Ø§Ø±Ù†Ø© Ø¬Ù†Ø¨Ù‹Ø§ Ø¥Ù„Ù‰ Ø¬Ù†Ø¨ Ù„Ø¬Ù…ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ØªÙ‚Ø³ÙŠÙ….

<div class="flex justify-center">
     <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/segmentation-comparison.png" alt="Segmentation Maps Compared"/>
</div>

Ø¨Ø¹Ø¯ Ø±Ø¤ÙŠØ© ÙƒÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ØªÙ‚Ø³ÙŠÙ…ØŒ Ø³Ù†ØºÙˆØµ ÙÙŠ ØªÙØ§ØµÙŠÙ„ Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ¬Ø²Ø¦Ø© Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ©.

ØªØ´Ù…Ù„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„ÙˆØ§Ù‚Ø¹ÙŠØ© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ù„Ù„ØªØ¬Ø²Ø¦Ø© Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ© ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø³ÙŠØ§Ø±Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø© Ø¹Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø´Ø§Ø© ÙˆØ§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø±ÙˆØ±ÙŠØ© Ø§Ù„Ù…Ù‡Ù…Ø©ØŒ ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø®Ù„Ø§ÙŠØ§ ÙˆØ§Ù„Ø´Ø°ÙˆØ°Ø§Øª ÙÙŠ Ø§Ù„ØµÙˆØ± Ø§Ù„Ø·Ø¨ÙŠØ©ØŒ ÙˆÙ…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„ØªØºÙŠÙ‘Ø±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦ÙŠØ© Ù…Ù† ØµÙˆØ± Ø§Ù„Ø£Ù‚Ù…Ø§Ø± Ø§Ù„ØµÙ†Ø§Ø¹ÙŠØ©.

## Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªÙ‚Ø³ÙŠÙ… (Fine-tuning a Model for Segmentation)

Ø³Ù†Ù‚ÙˆÙ… Ø§Ù„Ø¢Ù† Ø¨Ù…Ø§ ÙŠÙ„ÙŠ:

1. Ø¥Ø¬Ø±Ø§Ø¡ Ø¶Ø¨Ø· Ø¯Ù‚ÙŠÙ‚ Ù„Ù€ [SegFormer](https://huggingface.co/docs/transformers/main/en/model_doc/segformer#segformer) Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª [SceneParse150](https://huggingface.co/datasets/scene_parse_150).
2. Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…ÙØ¶Ø¨Ø· Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„.

<Tip>

Ù„Ø¹Ø±Ø¶ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙÙ†Ù‰ ÙˆÙ†Ù‚Ø§Ø· Ø§Ù„ØªØ­Ù‚Ù‚ (checkpoints) Ø§Ù„Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù‡Ù…Ø©ØŒ Ù†ÙˆØµÙŠ Ø¨Ø§Ù„Ø§Ø·Ù„Ø§Ø¹ Ø¹Ù„Ù‰ [ØµÙØ­Ø© Ø§Ù„Ù…Ù‡Ù…Ø©](https://huggingface.co/tasks/image-segmentation)

</Tip>


### ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª SceneParse150

Ø§Ø¨Ø¯Ø£ Ø¨ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© ÙØ±Ø¹ÙŠØ© Ø£ØµØºØ± Ù…Ù† SceneParse150 Ù…Ù† Ù…ÙƒØªØ¨Ø© ğŸ¤— Datasets. Ø³ÙŠØ³Ù…Ø­ Ù„Ùƒ Ù‡Ø°Ø§ Ø¨Ø§Ù„ØªØ¬Ø±Ø¨Ø© ÙˆØ§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† ÙƒÙ„ Ø´ÙŠØ¡ ÙŠØ¹Ù…Ù„ Ù‚Ø¨Ù„ Ù‚Ø¶Ø§Ø¡ ÙˆÙ‚Øª Ø£Ø·ÙˆÙ„ ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©.

```py
>>> from datasets import load_dataset

>>> ds = load_dataset("scene_parse_150", split="train[:50]")
```

Ù‚Ø³Ù‘Ù… Ù‚Ø³Ù… `train` ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹ØªÙŠ ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯Ø§Ù„Ø© [`~datasets.Dataset.train_test_split`]:

```py
>>> ds = ds.train_test_split(test_size=0.2)
>>> train_ds = ds["train"]
>>> test_ds = ds["test"]
```

Ø«Ù… Ø£Ù„Ù‚Ù Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ù…Ø«Ø§Ù„:

```py
>>> train_ds[0]
{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x683 at 0x7F9B0C201F90>,
 'annotation': <PIL.PngImagePlugin.PngImageFile image mode=L size=512x683 at 0x7F9B0C201DD0>,
 'scene_category': 368}

# view the image
>>> train_ds[0]["image"]
```

- `image`: ØµÙˆØ±Ø© PIL Ù„Ù„Ù…Ø´Ù‡Ø¯.
- `annotation`: ØµÙˆØ±Ø© PIL Ù„Ø®Ø±ÙŠØ·Ø© Ø§Ù„ØªØ¬Ø²Ø¦Ø©ØŒ ÙˆÙ‡ÙŠ Ø§Ù„Ù‡Ø¯Ù Ù„Ù„Ù†Ù…ÙˆØ°Ø¬.
- `scene_category`: Ù…Ø¹Ø±Ù‘Ù ÙØ¦Ø© ÙŠØµÙ Ù…Ø´Ù‡Ø¯ Ø§Ù„ØµÙˆØ±Ø© Ù…Ø«Ù„ "kitchen" Ø£Ùˆ "office". ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø³ØªØ­ØªØ§Ø¬ ÙÙ‚Ø· Ø¥Ù„Ù‰ `image` Ùˆ`annotation`ØŒ ÙˆÙƒÙ„Ø§Ù‡Ù…Ø§ ØµÙˆØ± PIL.

Ø³ØªØ­ØªØ§Ø¬ Ø£ÙŠØ¶Ù‹Ø§ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ù…ÙˆØ³ ÙŠØ±Ø¨Ø· Ù…Ø¹Ø±Ù‘Ù Ø§Ù„ÙØ¦Ø© Ø¨Ø§Ø³Ù… Ø§Ù„ÙØ¦Ø©ØŒ ÙˆÙ‡Ùˆ Ù…Ø§ Ø³ÙŠÙƒÙˆÙ† Ù…ÙÙŠØ¯Ù‹Ø§ Ø¹Ù†Ø¯ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ø§Ø­Ù‚Ù‹Ø§. Ù†Ø²Ù‘Ù„ Ø§Ù„Ø®Ø±Ø§Ø¦Ø· Ù…Ù† Hub ÙˆØ£Ù†Ø´Ø¦ Ù‚Ø§Ù…ÙˆØ³ÙŠ `id2label` Ùˆ`label2id`:

```py
>>> import json
>>> from pathlib import Path
>>> from huggingface_hub import hf_hub_download

>>> repo_id = "huggingface/label-files"
>>> filename = "ade20k-id2label.json"
>>> id2label = json.loads(Path(hf_hub_download(repo_id, filename, repo_type="dataset")).read_text())
>>> id2label = {int(k): v for k, v in id2label.items()}
>>> label2id = {v: k for k, v in id2label.items()}
>>> num_labels = len(id2label)
```

#### Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø®ØµÙ‘ØµØ© (Custom dataset)

ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø¥Ø°Ø§ ÙƒÙ†Øª ØªÙØ¶Ù‘Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³ÙƒØ±Ø¨Øª [run_semantic_segmentation.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/semantic-segmentation/run_semantic_segmentation.py) Ø¨Ø¯Ù„Ù‹Ø§ Ù…Ù† Ù…ÙÙƒØ±Ø© (notebook). ÙŠØªØ·Ù„Ù‘Ø¨ Ø§Ù„Ø³ÙƒØ±Ø¨Øª Ù…Ø§ ÙŠÙ„ÙŠ:

1. ÙƒØ§Ø¦Ù† [`~datasets.DatasetDict`] ÙŠØ­ØªÙˆÙŠ Ø¹Ù…ÙˆØ¯ÙŠÙ† Ù…Ù† Ù†ÙˆØ¹ [`~datasets.Image`] Ù‡Ù…Ø§ "image" Ùˆ"label"

     ```py
     from datasets import Dataset, DatasetDict, Image

     image_paths_train = ["path/to/image_1.jpg/jpg", "path/to/image_2.jpg/jpg", ..., "path/to/image_n.jpg/jpg"]
     label_paths_train = ["path/to/annotation_1.png", "path/to/annotation_2.png", ..., "path/to/annotation_n.png"]

     image_paths_validation = [...]
     label_paths_validation = [...]

     def create_dataset(image_paths, label_paths):
         dataset = Dataset.from_dict({"image": sorted(image_paths),
                                     "label": sorted(label_paths)})
         dataset = dataset.cast_column("image", Image())
         dataset = dataset.cast_column("label", Image())
         return dataset

     # step 1: create Dataset objects
     train_dataset = create_dataset(image_paths_train, label_paths_train)
     validation_dataset = create_dataset(image_paths_validation, label_paths_validation)

     # step 2: create DatasetDict
     dataset = DatasetDict({
          "train": train_dataset,
          "validation": validation_dataset,
          }
     )

     # step 3: push to Hub (assumes you have ran the hf auth login command in a terminal/notebook)
     dataset.push_to_hub("your-name/dataset-repo")

     # optionally, you can push to a private repo on the Hub
     # dataset.push_to_hub("name of repo on the hub", private=True)
     ```

2. Ù‚Ø§Ù…ÙˆØ³ `id2label` ÙŠØ±Ø¨Ø· Ø§Ù„Ø£Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµØ­ÙŠØ­Ø© Ù„Ù„ÙØ¦Ø§Øª Ø¨Ø£Ø³Ù…Ø§Ø¦Ù‡Ø§

     ```py
     import json
     # simple example
     id2label = {0: 'cat', 1: 'dog'}
     with open('id2label.json', 'w') as fp:
     json.dump(id2label, fp)
     ```

ÙƒÙ…Ø«Ø§Ù„ØŒ Ø·Ø§Ù„Ø¹ [Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‡Ø°Ù‡](https://huggingface.co/datasets/nielsr/ade20k-demo) Ø§Ù„ØªÙŠ Ø£ÙÙ†Ø´Ø¦Øª ÙˆÙÙ‚ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ù…ÙˆØ¶Ù‘Ø­Ø© Ø£Ø¹Ù„Ø§Ù‡.

### Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© (Preprocess)

Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù‡ÙŠ ØªØ­Ù…ÙŠÙ„ Ù…Ø¹Ø§Ù„Ø¬ ØµÙˆØ± SegFormer Ù„Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„ÙˆØ³ÙˆÙ… (annotations) Ù„Ù„Ù†Ù…ÙˆØ°Ø¬. Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ Ù…Ø«Ù„ Ù‡Ø°Ù‡ØŒ ØªØ³ØªØ®Ø¯Ù… Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„ØµÙØ±ÙŠ ÙƒÙØ¦Ø© Ø§Ù„Ø®Ù„ÙÙŠØ©. ÙˆÙ„ÙƒÙ† ÙØ¦Ø© Ø§Ù„Ø®Ù„ÙÙŠØ© Ù„ÙŠØ³Øª Ù…ÙØ¶Ù…Ù‘Ù†Ø© ÙØ¹Ù„ÙŠÙ‹Ø§ Ø¶Ù…Ù† 150 ÙØ¦Ø©ØŒ Ù„Ø°Ø§ Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¶Ø¨Ø· `do_reduce_labels=True` Ù„Ø·Ø±Ø­ ÙˆØ§Ø­Ø¯ Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙˆØ³ÙˆÙ…. ÙŠÙØ³ØªØ¨Ø¯Ù„ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„ØµÙØ±ÙŠ Ø¨Ø§Ù„Ù‚ÙŠÙ…Ø© `255` Ø­ØªÙ‰ ÙŠØªØ¬Ø§Ù‡Ù„Ù‡ Ø¯Ø§Ù„Ù‘Ø© Ø§Ù„Ø®Ø³Ø§Ø±Ø© ÙÙŠ SegFormer:

```py
>>> from transformers import AutoImageProcessor

>>> checkpoint = "nvidia/mit-b0"
>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint, do_reduce_labels=True)
```

<frameworkcontent>
<pt>

Ù…Ù† Ø§Ù„Ø´Ø§Ø¦Ø¹ ØªØ·Ø¨ÙŠÙ‚ Ø¨Ø¹Ø¶ ØªØ¹Ø¸ÙŠÙ…Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Data Augmentations) Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„ØµÙˆØ± Ù„Ø¬Ø¹Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£ÙƒØ«Ø± Ù…ØªØ§Ù†Ø© Ø¶Ø¯ ÙØ±Ø· Ø§Ù„ØªÙƒÙŠÙ‘Ù (overfitting). ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ØŒ Ø³ØªØ³ØªØ®Ø¯Ù… Ø§Ù„Ø¯Ø§Ù„Ø© [`ColorJitter`](https://pytorch.org/vision/stable/generated/torchvision.transforms.ColorJitter.html) Ù…Ù† Ù…ÙƒØªØ¨Ø© [torchvision](https://pytorch.org/vision/stable/index.html) Ù„ØªØºÙŠÙŠØ± Ø®ØµØ§Ø¦Øµ Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ø¹Ø´ÙˆØ§Ø¦ÙŠÙ‹Ø§ØŒ ÙˆÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙŠ Ù…ÙƒØªØ¨Ø© ØµÙˆØ± ØªÙÙØ¶Ù‘Ù„Ù‡Ø§.

```py
>>> from torchvision.transforms import ColorJitter

>>> jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)
```

Ø£Ù†Ø´Ø¦ Ø§Ù„Ø¢Ù† Ø¯Ø§Ù„ØªÙŠÙ† Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ù„Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„ÙˆØ³ÙˆÙ… Ù„Ù„Ù†Ù…ÙˆØ°Ø¬. ØªÙØ­ÙˆÙ‘Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ `pixel_values` ÙˆØ§Ù„ÙˆØ³ÙˆÙ… Ø¥Ù„Ù‰ `labels`. Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ ÙŠÙØ·Ø¨Ù‘Ù‚ `jitter` Ù‚Ø¨Ù„ ØªÙ…Ø±ÙŠØ± Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„ØµÙˆØ±. Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±ØŒ ÙŠÙ‚ÙˆÙ… Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„ØµÙˆØ± Ø¨Ø§Ù„Ù‚Øµ ÙˆØ§Ù„ØªØ·Ø¨ÙŠØ¹ Ù„Ù„ØµÙˆØ± (`images`)ØŒ Ø¨ÙŠÙ†Ù…Ø§ ÙŠÙ‚ÙˆÙ… Ø¨Ù‚Øµ `labels` ÙÙ‚Ø· Ù„Ø£Ù†Ù†Ø§ Ù„Ø§ Ù†Ø·Ø¨Ù‘Ù‚ ØªØ¹Ø¸ÙŠÙ… Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±.

```py
>>> def train_transforms(example_batch):
...     images = [jitter(x) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs


>>> def val_transforms(example_batch):
...     images = [x for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs
```

Ù„ØªØ·Ø¨ÙŠÙ‚ `jitter` Ø¹Ù„Ù‰ ÙƒØ§Ù…Ù„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¯Ø§Ù„Ø© [`~datasets.Dataset.set_transform`] Ù…Ù† ğŸ¤— Datasets. ÙŠØ¬Ø±ÙŠ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¹Ù†Ø¯ Ø§Ù„Ø·Ù„Ø¨ Ù…Ù…Ø§ ÙŠØ¬Ø¹Ù„Ù‡ Ø£Ø³Ø±Ø¹ ÙˆÙŠØ³ØªÙ‡Ù„Ùƒ Ù…Ø³Ø§Ø­Ø© Ù‚Ø±Øµ Ø£Ù‚Ù„:

```py
>>> train_ds.set_transform(train_transforms)
>>> test_ds.set_transform(val_transforms)
```

</pt>
</frameworkcontent>

<frameworkcontent>
<tf>
Ù…Ù† Ø§Ù„Ø´Ø§Ø¦Ø¹ ØªØ·Ø¨ÙŠÙ‚ Ø¨Ø¹Ø¶ ØªØ¹Ø¸ÙŠÙ…Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„ØµÙˆØ± Ù„Ø¬Ø¹Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£ÙƒØ«Ø± Ù…ØªØ§Ù†Ø© Ø¶Ø¯ ÙØ±Ø· Ø§Ù„ØªÙƒÙŠÙ‘Ù.
ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ØŒ Ø³ØªØ³ØªØ®Ø¯Ù… [`tf.image`](https://www.tensorflow.org/api_docs/python/tf/image) Ù„ØªØºÙŠÙŠØ± Ø®ØµØ§Ø¦Øµ Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ø¹Ø´ÙˆØ§Ø¦ÙŠÙ‹Ø§ØŒ ÙˆÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙŠ Ù…ÙƒØªØ¨Ø© ØµÙˆØ± ØªÙÙØ¶Ù‘Ù„Ù‡Ø§.
Ø¹Ø±Ù‘Ù Ø¯Ø§Ù„ØªÙŠ ØªØ­ÙˆÙŠÙ„ Ù…Ù†ÙØµÙ„ØªÙŠÙ†:
- ØªØ­ÙˆÙŠÙ„Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ØªÙŠ ØªØªØ¶Ù…Ù† ØªØ¹Ø¸ÙŠÙ… Ø§Ù„ØµÙˆØ±
- ØªØ­ÙˆÙŠÙ„Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ­Ù‚Ù‚ (validation) Ø§Ù„ØªÙŠ ØªÙ‚ÙˆÙ… ÙÙ‚Ø· Ø¨ØªØ¨Ø¯ÙŠÙ„ ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ØŒ Ù†Ø¸Ø±Ù‹Ø§ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© ÙÙŠ ğŸ¤— Transformers ØªØªÙˆÙ‚Ø¹ ØªØ±ØªÙŠØ¨ Ø§Ù„Ù‚Ù†ÙˆØ§Øª Ø£ÙˆÙ„Ù‹Ø§ (channels-first)

```py
>>> import tensorflow as tf


>>> def aug_transforms(image):
...     image = tf.keras.utils.img_to_array(image)
...     image = tf.image.random_brightness(image, 0.25)
...     image = tf.image.random_contrast(image, 0.5, 2.0)
...     image = tf.image.random_saturation(image, 0.75, 1.25)
...     image = tf.image.random_hue(image, 0.1)
...     image = tf.transpose(image, (2, 0, 1))
...     return image


>>> def transforms(image):
...     image = tf.keras.utils.img_to_array(image)
...     image = tf.transpose(image, (2, 0, 1))
...     return image
```

Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ Ø£Ù†Ø´Ø¦ Ø¯Ø§Ù„ØªÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø³Ø¨Ù‚Ø© Ù„Ø¥Ø¹Ø¯Ø§Ø¯ Ø¯ÙØ¹Ø§Øª Ù…Ù† Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„ÙˆØ³ÙˆÙ… Ù„Ù„Ù†Ù…ÙˆØ°Ø¬. ØªØ·Ø¨Ù‚ Ù‡Ø°Ù‡ Ø§Ù„Ø¯ÙˆØ§Ù„ ØªØ­ÙˆÙŠÙ„Ø§Øª Ø§Ù„ØµÙˆØ± ÙˆØªØ³ØªØ¹Ù…Ù„ `image_processor` Ø§Ù„Ù…ÙØ­Ù…Ù‘Ù„ Ø³Ø§Ø¨Ù‚Ù‹Ø§ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ `pixel_values` ÙˆØ§Ù„ÙˆØ³ÙˆÙ… Ø¥Ù„Ù‰ `labels`. ÙŠØªÙƒÙÙ„ `ImageProcessor` Ø£ÙŠØ¶Ù‹Ø§ Ø¨ØªØºÙŠÙŠØ± Ø§Ù„Ø­Ø¬Ù… (resize) ÙˆØªØ·Ø¨ÙŠØ¹ Ø§Ù„ØµÙˆØ±.

```py
>>> def train_transforms(example_batch):
...     images = [aug_transforms(x.convert("RGB")) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs

>>> def val_transforms(example_batch):
...     images = [transforms(x.convert("RGB")) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs
```

Ù„ØªØ·Ø¨ÙŠÙ‚ ØªØ­ÙˆÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ø¹Ù„Ù‰ ÙƒØ§Ù…Ù„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¯Ø§Ù„Ø© [`~datasets.Dataset.set_transform`] Ù…Ù† ğŸ¤— Datasets. ÙŠØ¬Ø±ÙŠ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¹Ù†Ø¯ Ø§Ù„Ø·Ù„Ø¨ Ù…Ù…Ø§ ÙŠØ¬Ø¹Ù„Ù‡ Ø£Ø³Ø±Ø¹ ÙˆÙŠØ³ØªÙ‡Ù„Ùƒ Ù…Ø³Ø§Ø­Ø© Ù‚Ø±Øµ Ø£Ù‚Ù„:

```py
>>> train_ds.set_transform(train_transforms)
>>> test_ds.set_transform(val_transforms)
```
</tf>
</frameworkcontent>

### Ø§Ù„ØªÙ‚ÙŠÙŠÙ… (Evaluate)

ÙŠÙƒÙˆÙ† ØªØ¶Ù…ÙŠÙ† Ù…Ù‚ÙŠØ§Ø³ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…ÙÙŠØ¯Ù‹Ø§ ØºØ§Ù„Ø¨Ù‹Ø§ Ù„ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬. ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ Ø£Ø³Ù„ÙˆØ¨ ØªÙ‚ÙŠÙŠÙ… Ø¨Ø³Ø±Ø¹Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø© ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index). Ù„Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù‡Ù…Ø©ØŒ Ø­Ù…Ù‘Ù„ Ù…Ù‚ÙŠØ§Ø³ [Ù…ØªÙˆØ³Ø· ØªÙ‚Ø§Ø·Ø¹ Ø§Ù„Ø§ØªØ­Ø§Ø¯](https://huggingface.co/spaces/evaluate-metric/accuracy) (mean Intersection over Union - IoU) (Ø±Ø§Ø¬Ø¹ [Ø§Ù„Ø¬ÙˆÙ„Ø© Ø§Ù„Ø³Ø±ÙŠØ¹Ø©](https://huggingface.co/docs/evaluate/a_quick_tour) Ù„ØªØ¹Ø±Ù Ø§Ù„Ù…Ø²ÙŠØ¯ Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© Ø§Ù„ØªØ­Ù…ÙŠÙ„ ÙˆØ§Ù„Ø­Ø³Ø§Ø¨):

```py
>>> import evaluate

>>> metric = evaluate.load("mean_iou")
```

Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ Ø£Ù†Ø´Ø¦ Ø¯Ø§Ù„Ø© Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`~evaluate.EvaluationModule.compute`]. ÙŠØ¬Ø¨ ØªØ­ÙˆÙŠÙ„ ØªÙ†Ø¨Ø¤Ø§ØªÙƒ Ø¥Ù„Ù‰ `logits` Ø£ÙˆÙ„Ù‹Ø§ØŒ Ø«Ù… Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ÙƒÙŠÙ„Ù‡Ø§ Ù„ØªØ·Ø§Ø¨Ù‚ Ø­Ø¬Ù… Ø§Ù„ÙˆØ³ÙˆÙ… Ù‚Ø¨Ù„ Ø£Ù† ØªØªÙ…ÙƒÙ† Ù…Ù† Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ [`~evaluate.EvaluationModule.compute`]:

<frameworkcontent>
<pt>

```py
>>> import numpy as np
>>> import torch
>>> from torch import nn

>>> def compute_metrics(eval_pred):
...     with torch.no_grad():
...         logits, labels = eval_pred
...         logits_tensor = torch.from_numpy(logits)
...         logits_tensor = nn.functional.interpolate(
...             logits_tensor,
...             size=labels.shape[-2:],
...             mode="bilinear",
...             align_corners=False,
...         ).argmax(dim=1)

...         pred_labels = logits_tensor.detach().cpu().numpy()
...         metrics = metric.compute(
...             predictions=pred_labels,
...             references=labels,
...             num_labels=num_labels,
...             ignore_index=255,
...             reduce_labels=False,
...         )
...         for key, value in metrics.items():
...             if isinstance(value, np.ndarray):
...                 metrics[key] = value.tolist()
...         return metrics
```

</pt>
</frameworkcontent>


<frameworkcontent>
<tf>

```py
>>> def compute_metrics(eval_pred):
...     logits, labels = eval_pred
...     logits = tf.transpose(logits, perm=[0, 2, 3, 1])
...     logits_resized = tf.image.resize(
...         logits,
...         size=tf.shape(labels)[1:],
...         method="bilinear",
...     )

...     pred_labels = tf.argmax(logits_resized, axis=-1)
...     metrics = metric.compute(
...         predictions=pred_labels,
...         references=labels,
...         num_labels=num_labels,
...         ignore_index=-1,
...         reduce_labels=image_processor.do_reduce_labels,
...     )

...     per_category_accuracy = metrics.pop("per_category_accuracy").tolist()
...     per_category_iou = metrics.pop("per_category_iou").tolist()

...     metrics.update({f"accuracy_{id2label[i]}": v for i, v in enumerate(per_category_accuracy)})
...     metrics.update({f"iou_{id2label[i]}": v for i, v in enumerate(per_category_iou)})
...     return {"val_" + k: v for k, v in metrics.items()}
```

</tf>
</frameworkcontent>

Ø§Ù„Ø¢Ù† Ø£ØµØ¨Ø­Øª Ø¯Ø§Ù„Ø© `compute_metrics` Ø¬Ø§Ù‡Ø²Ø©ØŒ ÙˆØ³Ù†Ø¹ÙˆØ¯ Ø¥Ù„ÙŠÙ‡Ø§ Ø¹Ù†Ø¯ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨.

### Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (Train)
<frameworkcontent>
<pt>
<Tip>

Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…Ø¹ØªØ§Ø¯Ù‹Ø§ Ø¹Ù„Ù‰ Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`Trainer`]ØŒ ÙØ£Ù„Ù‚Ù Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ [Ù‡Ù†Ø§](../training#finetune-with-trainer)!

</Tip>

Ø£ØµØ¨Ø­Øª Ø¬Ø§Ù‡Ø²Ù‹Ø§ Ù„Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬! Ø­Ù…Ù‘Ù„ SegFormer Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`AutoModelForSemanticSegmentation`]ØŒ ÙˆÙ…Ø±Ù‘Ø± Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø±Ø¨Ø· Ø¨ÙŠÙ† Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„ÙØ¦Ø§Øª ÙˆØ£Ø³Ù…Ø§Ø¦Ù‡Ø§:

```py
>>> from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer

>>> model = AutoModelForSemanticSegmentation.from_pretrained(checkpoint, id2label=id2label, label2id=label2id)
```

ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø±Ø­Ù„Ø©ØŒ ØªØ¨Ù‚Ù‘Øª Ø«Ù„Ø§Ø« Ø®Ø·ÙˆØ§Øª ÙÙ‚Ø·:

1. Ø­Ø¯Ø¯ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ÙØ§Ø¦Ù‚Ø© ÙÙŠ [`TrainingArguments`]. Ù…Ù† Ø§Ù„Ù…Ù‡Ù… Ø£Ù„Ø§ ØªØ²ÙŠÙ„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© ØºÙŠØ± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© Ù„Ø£Ù† Ø°Ù„Ùƒ Ø³ÙŠØ­Ø°Ù Ø¹Ù…ÙˆØ¯ `image`. Ø¨Ø¯ÙˆÙ† Ø¹Ù…ÙˆØ¯ `image`ØŒ Ù„Ù† ØªØªÙ…ÙƒÙ† Ù…Ù† Ø¥Ù†Ø´Ø§Ø¡ `pixel_values`. Ø§Ø¶Ø¨Ø· `remove_unused_columns=False` Ù„Ù…Ù†Ø¹ Ù‡Ø°Ø§ Ø§Ù„Ø³Ù„ÙˆÙƒ! Ø§Ù„Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø¥Ù„Ø²Ø§Ù…ÙŠ Ø§Ù„Ø¢Ø®Ø± Ù‡Ùˆ `output_dir` Ø§Ù„Ø°ÙŠ ÙŠØ­Ø¯Ø¯ Ù…ÙƒØ§Ù† Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬. Ø³ØªØ¯ÙØ¹ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ Hub Ø¹Ø¨Ø± Ø¶Ø¨Ø· `push_to_hub=True` (ØªØ­ØªØ§Ø¬ Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¥Ù„Ù‰ Hugging Face Ù„Ø±ÙØ¹ Ù†Ù…ÙˆØ°Ø¬Ùƒ). ÙÙŠ Ù†Ù‡Ø§ÙŠØ© ÙƒÙ„ Ø¹Ù‡Ø¯Ø© (epoch)ØŒ Ø³ÙŠÙ‚ÙŠÙ‘Ù… [`Trainer`] Ù…Ù‚ÙŠØ§Ø³ IoU ÙˆÙŠØ­ÙØ¸ Ù†Ù‚Ø·Ø© ØªØ­Ù‚Ù‚ Ø§Ù„ØªØ¯Ø±ÙŠØ¨.
2. Ù…Ø±Ù‘Ø± Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¥Ù„Ù‰ [`Trainer`] Ù…Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆÙ…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„Ù…ÙØ±Ù…Ù‘Ø² (tokenizer) ÙˆØ§Ù„Ù…Ø¬Ù…Ù‘Ø¹ (data collator) ÙˆØ¯Ø§Ù„Ø© `compute_metrics`.
3. Ø§Ø³ØªØ¯Ø¹Ù [`~Trainer.train`] Ù„Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„Ø¶Ø¨Ø· Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬.

```py
>>> training_args = TrainingArguments(
...     output_dir="segformer-b0-scene-parse-150",
...     learning_rate=6e-5,
...     num_train_epochs=50,
...     per_device_train_batch_size=2,
...     per_device_eval_batch_size=2,
...     save_total_limit=3,
...     eval_strategy="steps",
...     save_strategy="steps",
...     save_steps=20,
...     eval_steps=20,
...     logging_steps=1,
...     eval_accumulation_steps=5,
...     remove_unused_columns=False,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=train_ds,
...     eval_dataset=test_ds,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

Ø¨Ø¹Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ Ø´Ø§Ø±Ùƒ Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø¹Ù„Ù‰ Hub Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯Ø§Ù„Ø© [`~transformers.Trainer.push_to_hub`] Ù„ÙŠØµØ¨Ø­ Ù…ØªØ§Ø­Ù‹Ø§ Ù„Ù„Ø¬Ù…ÙŠØ¹:

```py
>>> trainer.push_to_hub()
```
</pt>
</frameworkcontent>

<frameworkcontent>
<tf>
<Tip>

Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…Ø¹ØªØ§Ø¯Ù‹Ø§ Ø¹Ù„Ù‰ Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… KerasØŒ ÙØ§Ø·Ù‘Ù„Ø¹ Ø£ÙˆÙ„Ù‹Ø§ Ø¹Ù„Ù‰ [Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ](./training#train-a-tensorflow-model-with-keras)!

</Tip>

Ù„Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ TensorFlowØŒ Ø§ØªØ¨Ø¹ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:
1. Ø­Ø¯Ø¯ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ÙØ§Ø¦Ù‚Ø©ØŒ ÙˆØ§Ø¶Ø¨Ø· Ø§Ù„Ù…ÙØ­Ø³ÙÙ‘Ù† (optimizer) ÙˆØ¬Ø¯ÙˆÙ„ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù‘Ù….
2. Ø£Ù†Ø´Ø¦ Ù†Ù…ÙˆØ°Ø¬Ù‹Ø§ Ù…ÙØ¯Ø±Ù‘Ø¨Ù‹Ø§ Ù…Ø³Ø¨Ù‚Ù‹Ø§.
3. Ø­ÙˆÙ‘Ù„ Ù…Ø¬Ù…ÙˆØ¹Ø© ğŸ¤— Dataset Ø¥Ù„Ù‰ `tf.data.Dataset`.
4. Ù‚Ù… Ø¨ØªØ¬Ù…ÙŠØ¹ (compile) Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.
5. Ø£Ø¶Ù Ø§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§Øª Ø§Ù„ØªØ±Ø§Ø¬Ø¹ÙŠØ© (callbacks) Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ ÙˆØ±ÙØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ ğŸ¤— Hub.
6. Ø§Ø³ØªØ®Ø¯Ù… `fit()` Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨.

Ø§Ø¨Ø¯Ø£ Ø¨ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© ÙˆØ§Ù„Ù…ÙØ­Ø³ÙÙ‘Ù† ÙˆØ¬Ø¯ÙˆÙ„ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù‘Ù…:

```py
>>> from transformers import create_optimizer

>>> batch_size = 2
>>> num_epochs = 50
>>> num_train_steps = len(train_ds) * num_epochs
>>> learning_rate = 6e-5
>>> weight_decay_rate = 0.01

>>> optimizer, lr_schedule = create_optimizer(
...     init_lr=learning_rate,
...     num_train_steps=num_train_steps,
...     weight_decay_rate=weight_decay_rate,
...     num_warmup_steps=0,
... )
```

Ø«Ù… Ø­Ù…Ù‘Ù„ SegFormer Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`TFAutoModelForSemanticSegmentation`] Ù…Ø¹ Ø®Ø±Ø§Ø¦Ø· Ø§Ù„ÙØ¦Ø§ØªØŒ ÙˆÙ‚Ù… Ø¨ØªØ¬Ù…ÙŠØ¹Ù‡ Ù…Ø¹ Ø§Ù„Ù…ÙØ­Ø³ÙÙ‘Ù†. Ù„Ø§Ø­Ø¸ Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ Transformers ØªÙ…ØªÙ„Ùƒ Ø¯Ø§Ù„Ø© Ø®Ø³Ø§Ø±Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„Ù…Ù‡Ù…Ø©ØŒ Ù„Ø°Ø§ Ù„Ø§ ØªØ­ØªØ§Ø¬ Ù„ØªØ­Ø¯ÙŠØ¯ ÙˆØ§Ø­Ø¯Ø© Ø¥Ù„Ø§ Ø¥Ø°Ø§ Ø±ØºØ¨Øª Ø¨Ø°Ù„Ùƒ:

```py
>>> from transformers import TFAutoModelForSemanticSegmentation

>>> model = TFAutoModelForSemanticSegmentation.from_pretrained(
...     checkpoint,
...     id2label=id2label,
...     label2id=label2id,
... )
>>> model.compile(optimizer=optimizer)  # No loss argument!
```

Ø­ÙˆÙ‘Ù„ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ ØµÙŠØºØ© `tf.data.Dataset` Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`~datasets.Dataset.to_tf_dataset`] Ùˆ[`DefaultDataCollator`]:

```py
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator(return_tensors="tf")

>>> tf_train_dataset = train_ds.to_tf_dataset(
...     columns=["pixel_values", "label"],
...     shuffle=True,
...     batch_size=batch_size,
...     collate_fn=data_collator,
... )

>>> tf_eval_dataset = test_ds.to_tf_dataset(
...     columns=["pixel_values", "label"],
...     shuffle=True,
...     batch_size=batch_size,
...     collate_fn=data_collator,
... )
```

Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ù‚Ù‘Ø© Ù…Ù† Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª ÙˆØ¯ÙØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ ğŸ¤— HubØŒ Ø§Ø³ØªØ®Ø¯Ù… [Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§Øª Keras](../main_classes/keras_callbacks).
Ù…Ø±Ù‘Ø± Ø¯Ø§Ù„Ø© `compute_metrics` Ø¥Ù„Ù‰ [`KerasMetricCallback`],
ÙˆØ§Ø³ØªØ®Ø¯Ù… [`PushToHubCallback`] Ù„Ø±ÙØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:

```py
>>> from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback

>>> metric_callback = KerasMetricCallback(
...     metric_fn=compute_metrics, eval_dataset=tf_eval_dataset, batch_size=batch_size, label_cols=["labels"]
... )

>>> push_to_hub_callback = PushToHubCallback(output_dir="scene_segmentation", tokenizer=image_processor)

>>> callbacks = [metric_callback, push_to_hub_callback]
```

Ø£Ø®ÙŠØ±Ù‹Ø§ØŒ Ø£ØµØ¨Ø­Øª Ø¬Ø§Ù‡Ø²Ù‹Ø§ Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬! Ø§Ø³ØªØ¯Ø¹Ù `fit()` Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ØŒ ÙˆØ¹Ø¯Ø¯ Ø§Ù„Ø¹ÙÙ‡Ø¯ (epochs)ØŒ ÙˆØ§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§Øª Ø§Ù„ØªØ±Ø§Ø¬Ø¹ÙŠØ© Ù„Ø¶Ø¨Ø· Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:

```py
>>> model.fit(
...     tf_train_dataset,
...     validation_data=tf_eval_dataset,
...     callbacks=callbacks,
...     epochs=num_epochs,
... )
```

ØªÙ‡Ø§Ù†ÙŠÙ†Ø§! Ù„Ù‚Ø¯ Ø£Ø¬Ø±ÙŠØª Ø§Ù„Ø¶Ø¨Ø· Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ù„Ù†Ù…ÙˆØ°Ø¬Ùƒ ÙˆØ´Ø§Ø±ÙƒØªÙ‡ Ø¹Ù„Ù‰ ğŸ¤— Hub. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¢Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„!
</tf>
</frameworkcontent>

### Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ (Inference)

Ø±Ø§Ø¦Ø¹ØŒ Ø§Ù„Ø¢Ù† Ø¨Ø¹Ø¯ Ø£Ù† Ø£Ø¬Ø±ÙŠØª Ø§Ù„Ø¶Ø¨Ø· Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„!

Ø£Ø¹Ø¯ ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ­Ù…Ù‘Ù„ ØµÙˆØ±Ø© Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„.

```py
>>> from datasets import load_dataset

>>> ds = load_dataset("scene_parse_150", split="train[:50]")
>>> ds = ds.train_test_split(test_size=0.2)
>>> test_ds = ds["test"]
>>> image = ds["test"][0]["image"]
>>> image
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-image.png" alt="Image of bedroom"/>
</div>

<frameworkcontent>
<pt>

Ø³Ù†Ø±Ù‰ Ø§Ù„Ø¢Ù† ÙƒÙŠÙÙŠØ© Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø¨Ø¯ÙˆÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ù†Ø¨ÙˆØ¨ (pipeline). Ø¹Ø§Ù„Ø¬ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„ØµÙˆØ± ÙˆØ¶Ø¹ `pixel_values` Ø¹Ù„Ù‰ ÙˆØ­Ø¯Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© (GPU/CPU):

```py
>>> from accelerate.test_utils.testing import get_backend
# automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)
>>> device, _, _ = get_backend()
>>> encoding = image_processor(image, return_tensors="pt")
>>> pixel_values = encoding.pixel_values.to(device)
```

Ù…Ø±Ù‘Ø± Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ£Ø±Ø¬ÙØ¹ `logits`:

```py
>>> outputs = model(pixel_values=pixel_values)
>>> logits = outputs.logits.cpu()
```

Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ Ù‚Ù… Ø¨Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ø¬ÙŠÙ… (rescale) `logits` Ø¥Ù„Ù‰ Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠ:

```py
>>> upsampled_logits = nn.functional.interpolate(
...     logits,
...     size=image.size[::-1],
...     mode="bilinear",
...     align_corners=False,
... )

>>> pred_seg = upsampled_logits.argmax(dim=1)[0]
```

</pt>
</frameworkcontent>

<frameworkcontent>
<tf>
Ø­Ù…Ù‘Ù„ Ù…Ø¹Ø§Ù„Ø¬ ØµÙˆØ± Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±Ø© ÙˆØ£Ø±Ø¬ÙØ¹ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ ÙƒÙ…ÙˆØªØ±Ø§Øª TensorFlow:

```py
>>> from transformers import AutoImageProcessor

>>> image_processor = AutoImageProcessor.from_pretrained("MariaK/scene_segmentation")
>>> inputs = image_processor(image, return_tensors="tf")
```

Ù…Ø±Ù‘Ø± Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ£Ø±Ø¬ÙØ¹ `logits`:

```py
>>> from transformers import TFAutoModelForSemanticSegmentation

>>> model = TFAutoModelForSemanticSegmentation.from_pretrained("MariaK/scene_segmentation")
>>> logits = model(**inputs).logits
```

Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ Ø£Ø¹Ø¯ ØªØ­Ø¬ÙŠÙ… `logits` Ø¥Ù„Ù‰ Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠ ÙˆØ·Ø¨Ù‘Ù‚ `argmax` Ø¹Ù„Ù‰ Ø¨ÙØ¹Ø¯ Ø§Ù„ÙØ¦Ø§Øª:
```py
>>> logits = tf.transpose(logits, [0, 2, 3, 1])

>>> upsampled_logits = tf.image.resize(
...     logits,
...     # We reverse the shape of `image` because `image.size` returns width and height.
...     image.size[::-1],
... )

>>> pred_seg = tf.math.argmax(upsampled_logits, axis=-1)[0]
```

</tf>
</frameworkcontent>

Ù„Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨ØµØ±ÙŠÙ‹Ø§ØŒ Ø­Ù…Ù‘Ù„ [Ù„ÙˆØ­Ø© Ø£Ù„ÙˆØ§Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª](https://github.com/tensorflow/models/blob/3f1ca33afe3c1631b733ea7e40c294273b9e406d/research/deeplab/utils/get_dataset_colormap.py#L51) ÙƒØ¯Ø§Ù„Ø© `ade_palette()` Ø§Ù„ØªÙŠ ØªÙØ·Ø§Ø¨Ù‚ ÙƒÙ„ ÙØ¦Ø© Ø¨Ù‚ÙŠÙ… RGB:

```py
def ade_palette():
  return np.asarray([
      [0, 0, 0],
      [120, 120, 120],
      [180, 120, 120],
      [6, 230, 230],
      [80, 50, 50],
      [4, 200, 3],
      [120, 120, 80],
      [140, 140, 140],
      [204, 5, 255],
      [230, 230, 230],
      [4, 250, 7],
      [224, 5, 255],
      [235, 255, 7],
      [150, 5, 61],
      [120, 120, 70],
      [8, 255, 51],
      [255, 6, 82],
      [143, 255, 140],
      [204, 255, 4],
      [255, 51, 7],
      [204, 70, 3],
      [0, 102, 200],
      [61, 230, 250],
      [255, 6, 51],
      [11, 102, 255],
      [255, 7, 71],
      [255, 9, 224],
      [9, 7, 230],
      [220, 220, 220],
      [255, 9, 92],
      [112, 9, 255],
      [8, 255, 214],
      [7, 255, 224],
      [255, 184, 6],
      [10, 255, 71],
      [255, 41, 10],
      [7, 255, 255],
      [224, 255, 8],
      [102, 8, 255],
      [255, 61, 6],
      [255, 194, 7],
      [255, 122, 8],
      [0, 255, 20],
      [255, 8, 41],
      [255, 5, 153],
      [6, 51, 255],
      [235, 12, 255],
      [160, 150, 20],
      [0, 163, 255],
      [140, 140, 140],
      [250, 10, 15],
      [20, 255, 0],
      [31, 255, 0],
      [255, 31, 0],
      [255, 224, 0],
      [153, 255, 0],
      [0, 0, 255],
      [255, 71, 0],
      [0, 235, 255],
      [0, 173, 255],
      [31, 0, 255],
      [11, 200, 200],
      [255, 82, 0],
      [0, 255, 245],
      [0, 61, 255],
      [0, 255, 112],
      [0, 255, 133],
      [255, 0, 0],
      [255, 163, 0],
      [255, 102, 0],
      [194, 255, 0],
      [0, 143, 255],
      [51, 255, 0],
      [0, 82, 255],
      [0, 255, 41],
      [0, 255, 173],
      [10, 0, 255],
      [173, 255, 0],
      [0, 255, 153],
      [255, 92, 0],
      [255, 0, 255],
      [255, 0, 245],
      [255, 0, 102],
      [255, 173, 0],
      [255, 0, 20],
      [255, 184, 184],
      [0, 31, 255],
      [0, 255, 61],
      [0, 71, 255],
      [255, 0, 204],
      [0, 255, 194],
      [0, 255, 82],
      [0, 10, 255],
      [0, 112, 255],
      [51, 0, 255],
      [0, 194, 255],
      [0, 122, 255],
      [0, 255, 163],
      [255, 153, 0],
      [0, 255, 10],
      [255, 112, 0],
      [143, 255, 0],
      [82, 0, 255],
      [163, 255, 0],
      [255, 235, 0],
      [8, 184, 170],
      [133, 0, 255],
      [0, 255, 92],
      [184, 0, 255],
      [255, 0, 31],
      [0, 184, 255],
      [0, 214, 255],
      [255, 0, 112],
      [92, 255, 0],
      [0, 224, 255],
      [112, 224, 255],
      [70, 184, 160],
      [163, 0, 255],
      [153, 0, 255],
      [71, 255, 0],
      [255, 0, 163],
      [255, 204, 0],
      [255, 0, 143],
      [0, 255, 235],
      [133, 255, 0],
      [255, 0, 235],
      [245, 0, 255],
      [255, 0, 122],
      [255, 245, 0],
      [10, 190, 212],
      [214, 255, 0],
      [0, 204, 255],
      [20, 0, 255],
      [255, 255, 0],
      [0, 153, 255],
      [0, 41, 255],
      [0, 255, 204],
      [41, 0, 255],
      [41, 255, 0],
      [173, 0, 255],
      [0, 245, 255],
      [71, 0, 255],
      [122, 0, 255],
      [0, 255, 184],
      [0, 92, 255],
      [184, 255, 0],
      [0, 133, 255],
      [255, 214, 0],
      [25, 194, 194],
      [102, 255, 0],
      [92, 0, 255],
  ])
```

Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ ÙŠÙ…ÙƒÙ†Ùƒ Ø¯Ù…Ø¬ ØµÙˆØ±ØªÙƒ ÙˆØ®Ø±ÙŠØ·Ø© Ø§Ù„ØªØ¬Ø²Ø¦Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© ÙˆØ¹Ø±Ø¶Ù‡Ù…Ø§:

```py
>>> import matplotlib.pyplot as plt
>>> import numpy as np

>>> color_seg = np.zeros((pred_seg.shape[0], pred_seg.shape[1], 3), dtype=np.uint8)
>>> palette = np.array(ade_palette())
>>> for label, color in enumerate(palette):
...     color_seg[pred_seg == label, :] = color
>>> color_seg = color_seg[..., ::-1]  # convert to BGR

>>> img = np.array(image) * 0.5 + color_seg * 0.5  # plot the image with the segmentation map
>>> img = img.astype(np.uint8)

>>> plt.figure(figsize=(15, 10))
>>> plt.imshow(img)
>>> plt.show()
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-preds.png" alt="Image of bedroom overlaid with segmentation map"/>
</div>
