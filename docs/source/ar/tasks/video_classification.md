# ุชุตููู ุงูููุฏูู

[[open-in-colab]]

ุชุตููู ุงูููุฏูู ูู ูููุฉ ุชุนููู ุชุณููุฉ ุฃู ูุฆุฉ ูููุฏูู ูุงูู. ูู ุงููุชููุน ุฃู ูููู ููู ููุฏูู ูุฆุฉ ูุงุญุฏุฉ ููุท. ุชุชููู ููุงุฐุฌ ุชุตููู ุงูููุฏูู ููุฏูู ููุฏุฎูุงุช ูุชุนูุฏ ุชูุจุคูุง ุจุงููุฆุฉ ุงูุชู ููุชูู ุฅูููุง ุงูููุฏูู. ูููู ุงุณุชุฎุฏุงู ูุฐู ุงูููุงุฐุฌ ูุชุตููู ูุญุชูู ุงูููุฏูู. ุฃุญุฏ ุงูุชุทุจููุงุช ุงููุงูุนูุฉ ูุชุตููู ุงูููุฏูู ูู ุงูุชุนุฑู ุนูู ุงูุฅุฌุฑุงุกุงุช/ุงูุฃูุดุทุฉุ ููู ูููุฏ ูุชุทุจููุงุช ุงูููุงูุฉ ุงูุจุฏููุฉ. ููุง ุฃูู ูุณุงุนุฏ ุงูุฃุดุฎุงุต ุถุนุงู ุงูุจุตุฑุ ุฎุงุตุฉ ุนูุฏ ุงูุชููู.

ุณููุถุญ ูุฐุง ุงูุฏููู ููููุฉ:

1. ุถุจุท ูููุฐุฌ [VideoMAE](https://huggingface.co/docs/transformers/main/en/model_doc/videomae) ุนูู ูุฌููุนุฉ ูุฑุนูุฉ ูู ูุฌููุนุฉ ุจูุงูุงุช [UCF101](https://www.crcv.ucf.edu/data/UCF101.php).
2. ุงุณุชุฎุฏุงู ูููุฐุฌู ุงููุถุจูุท ููุชูุจุค.

<Tip>

ููุนุฑูุฉ ุฌููุน ุงูุจูู ูููุงุท ุงููุฑุงูุจุฉ ุงููุชูุงููุฉ ูุน ูุฐู ุงููููุฉุ ููุตู ุจุงูุชุญูู ูู [ุตูุญุฉ ุงููููุฉ](https://huggingface.co/tasks/video-classification).

</Tip>

ูุจู ุงูุจุฏุกุ ุชุฃูุฏ ูู ุชุซุจูุช ุฌููุน ุงูููุชุจุงุช ุงูุถุฑูุฑูุฉ:

```bash
pip install -q pytorchvideo transformers evaluate
```

ุณุชุณุชุฎุฏู [PyTorchVideo](https://pytorchvideo.org/) (ุงููุณูุงุฉ `pytorchvideo`) ููุนุงูุฌุฉ ุงูููุฏูููุงุช ูุฅุนุฏุงุฏูุง.

ูุญู ูุดุฌุนู ุนูู ุชุณุฌูู ุงูุฏุฎูู ุฅูู ุญุณุงุจ Hugging Face ุงูุฎุงุต ุจู ุญุชู ุชุชููู ูู ุชุญููู ูููุฐุฌู ููุดุงุฑูุชู ูุน ุงููุฌุชูุน. ุนูุฏูุง ููุทูุจ ููู ุฐููุ ุฃุฏุฎู ุฑูุฒู ููุชุณุฌูู:

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## ุชุญููู ูุฌููุนุฉ ุจูุงูุงุช UCF101

ุงุจุฏุฃ ุจุชุญููู ูุฌููุนุฉ ูุฑุนูุฉ ูู [ูุฌููุนุฉ ุจูุงูุงุช UCF-101](https://www.crcv.ucf.edu/data/UCF101.php). ุณูุนุทูู ูุฐุง ูุฑุตุฉ ููุชุฌุฑุจุฉ ูุงูุชุฃูุฏ ูู ุฃู ูู ุดูุก ูุนูู ูุจู ูุถุงุก ุงููุฒูุฏ ูู ุงูููุช ูู ุงูุชุฏุฑูุจ ุนูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงููุงููุฉ.

```py
>>> from huggingface_hub import hf_hub_download

>>> hf_dataset_identifier = "sayakpaul/ucf101-subset"
>>> filename = "UCF101_subset.tar.gz"
>>> file_path = hf_hub_download(repo_id=hf_dataset_identifier, filename=filename, repo_type="dataset")
```

ุจุนุฏ ุชูุฒูู ุงููุฌููุนุฉ ุงููุฑุนูุฉุ ุชุญุชุงุฌ ุฅูู ุงุณุชุฎุฑุงุฌ ุงูุฃุฑุดูู ุงููุถุบูุท:

```py
>>> import tarfile

>>> with tarfile.open(file_path) as t:
...     t.extractall(".")
```

ุจุดูู ุนุงูุ ูุชู ุชูุธูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุนูู ุงููุญู ุงูุชุงูู:

```bash
UCF101_subset/
    train/
        BandMarching/
            video_1.mp4
            video_2.mp4
            ...
        Archery
            video_1.mp4
            video_2.mp4
            ...
        ...
    val/
        BandMarching/
            video_1.mp4
            video_2.mp4
            ...
        Archery
            video_1.mp4
            video_2.mp4
            ...
        ...
    test/
        BandMarching/
            video_1.mp4
            video_2.mp4
            ...
        Archery
            video_1.mp4
            video_2.mp4
            ...
        ...
```

ุจุนุฏ ุฐููุ ููููู ุญุณุงุจ ุนุฏุฏ ููุงุทุน ุงูููุฏูู ุงูุฅุฌูุงููุฉ.

```py
>>> import pathlib
>>> dataset_root_path = "UCF101_subset"
>>> dataset_root_path = pathlib.Path(dataset_root_path)
```

```py
>>> video_count_train = len(list(dataset_root_path.glob("train/*/*.avi")))
>>> video_count_val = len(list(dataset_root_
path.glob("val/*/*.avi")))
>>> video_count_test = len(list(dataset_root_path.glob("test/*/*.avi")))
>>> video_total = video_count_train + video_count_val + video_count_test
>>> print(f"Total videos: {video_total}")
```

```py
>>> all_video_file_paths = (
...     list(dataset_root_path.glob("train/*/*.avi"))
...     + list(dataset_root_path.glob("val/*/*.avi"))
...     + list(dataset_root_path.glob("test/*/*.avi"))
... )
>>> all_video_file_paths[:5]
```

ุชุธูุฑ ูุณุงุฑุงุช ุงูููุฏูู (ุงูููุฑุชุจุฉ) ุนูู ุงููุญู ุงูุชุงูู:

```bash
...
'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c04.avi',
'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c06.avi',
'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi',
'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c02.avi',
'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c06.avi'
...
```

ุณุชูุงุญุธ ุฃู ููุงู ููุงุทุน ููุฏูู ุชูุชูู ุฅูู ููุณ ุงููุฌููุนุฉ/ุงููุดูุฏ ุญูุซ ุชูุดูุฑ ุงููุฌููุนุฉ ุฅูู "g" ูู ูุณุงุฑุงุช ูููุงุช ุงูููุฏูู. ุนูู ุณุจูู ุงููุซุงูุ `v_ApplyEyeMakeup_g07_c04.avi`  ู  `v_ApplyEyeMakeup_g07_c06.avi` .

ุจุงููุณุจุฉ ูุนูููุงุช ุงูุชูุณูู ูุงูุชุญูู ูู ุงูุตุญุฉุ ูุง ุชุฑูุฏ ุฃู ุชููู ูุฏูู ููุงุทุน ููุฏูู ูู ููุณ ุงููุฌููุนุฉ/ุงููุดูุฏ ูููุน [ุชุณุฑุจ ุงูุจูุงูุงุช](https://www.kaggle.com/code/alexisbcook/data-leakage). ุชุฃุฎุฐ ุงููุฌููุนุฉ ุงููุฑุนูุฉ ุงูุชู ุชุณุชุฎุฏููุง ูู ูุฐุง ุงูุจุฑูุงูุฌ ุงูุชุนูููู ูุฐู ุงููุนูููุงุช ูู ุงูุงุนุชุจุงุฑ.

ุจุนุฏ ุฐููุ ุณุชููู ุจุงุณุชูุชุงุฌ ูุฌููุนุฉ ุงูุนูุงูุงุช ุงูููุฌูุฏุฉ ูู ูุฌููุนุฉ ุงูุจูุงูุงุช. ููุง ุณุชููู ุจุฅูุดุงุก ูุงููุณูู ุณููููุงู ูููุฏูู ุนูุฏ ุชููุฆุฉ ุงููููุฐุฌ:

* `label2id`: ูููู ุจุชุนููู ุฃุณูุงุก ุงููุฆุงุช ุฅูู ุฃุนุฏุงุฏ ุตุญูุญุฉ.
* `id2label`: ูููู ุจุชุนููู ุงูุฃุนุฏุงุฏ ุงูุตุญูุญุฉ ุฅูู ุฃุณูุงุก ุงููุฆุงุช.

```py
>>> class_labels = sorted({str(path).split("/")[2] for path in all_video_file_paths})
>>> label2id = {label: i for i, label in enumerate(class_labels)}
>>> id2label = {i: label for label, i in label2id.items()}

>>> print(f"Unique classes: {list(label2id.keys())}.")

# Unique classesุฉ: ['ApplyEyeMakeup'ุ 'ApplyLipstick'ุ 'Archery'ุ 'BabyCrawling'ุ 'BalanceBeam'ุ 'BandMarching'ุ 'BaseballPitch'ุ 'Basketball'ุ 'BasketballDunk'ุ 'BenchPress'].
```

ููุงู 10 ูุฆุงุช ูุฑูุฏุฉ. ููู ูุฆุฉุ ููุงู 30 ููุทุน ููุฏูู ูู ูุฌููุนุฉ ุงูุชุฏุฑูุจ.

## ุชุญููู ูููุฐุฌ ูุถุจุท ุฏููู

ูู ุจุชููุฆุฉ ูููุฐุฌ ุชุตููู ููุฏูู ูู ููุทุฉ ุชูุชูุด ููุฏุฑุจุฉ ูุณุจููุง ููุนุงูุฌ ุงูุตูุฑ ุงููุฑุชุจุท ุจูุง. ูุญุชูู ูุดูุฑ ุงููููุฐุฌ ุนูู ูุนููุงุช ููุฏุฑุจุฉ ูุณุจููุงุ ูุฑุฃุณ ุงูุชุตููู ููููุฃ ุจุดูู ุนุดูุงุฆู. ุณูููู ูุนุงูุฌ ุงูุตูุฑ ูููุฏูุง ุนูุฏ ูุชุงุจุฉ ุฎุท ุฃูุงุจูุจ ุงููุนุงูุฌุฉ ุงููุณุจูุฉ ููุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจูุง.

```py
>>> from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification

>>> model_ckpt = "MCG-NJU/videomae-base"
>>> image_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)
>>> model = VideoMAEForVideoClassification.from_pretrained(
...     model_ckpt,
...     label2id=label2id,
...     id2label=id2label,
...     ignore_mismatched_sizes=True,  # ูู ุจุชูููุฑ ูุฐุง ูู ุญุงูุฉ ููุช ุชุฎุทุท ูุถุจุท ุฏููู ูููุทุฉ ุชูุชูุด ููุฏุฑุจุฉ ุจุงููุนู
... )
```

ุจูููุง ูุชู ุชุญููู ุงููููุฐุฌุ ูุฏ ุชูุงุญุธ ุงูุชุญุฐูุฑ ุงูุชุงูู:

```bash
Some weights of the model checkpoint at MCG-NJU/videomae-base were not used when initializing VideoMAEForVideoClassification: [..., 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.key.weight']
- This IS expected if you are initializing VideoMAEForVideoClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing VideoMAEForVideoClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
```

ููุฎุจุฑูุง ุงูุชุญุฐูุฑ ุฃููุง ูููู ุจุญุฐู ุจุนุถ ุงูุฃูุฒุงู (ูุซู ุฃูุฒุงู ูุงูุญูุงุฒ ุทุจูุฉ "ุงููุตูู") ูุชููุฆุฉ ุจุนุถ ุงูุฃูุฒุงู ุงูุฃุฎุฑู ุจุดูู ุนุดูุงุฆู (ุฃูุฒุงู ูุงูุญูุงุฒ ุทุจูุฉ "ูุตูู" ุฌุฏูุฏุฉ). ูุฐุง ูุชููุน ูู ูุฐู ุงูุญุงูุฉุ ูุฃููุง ูููู ุจุฅุถุงูุฉ ุฑุฃุณ ุฌุฏูุฏ ูุง ุชุชููุฑ ูู ุฃูุฒุงู ููุฏุฑุจุฉ ูุณุจููุงุ ูุฐุง ูุญุฐุฑูุง ุงูุจุฑูุงูุฌ ูู ุฃูู ูุฌุจ ุนูููุง ุถุจุท ุงููููุฐุฌ ุฏููููุง ูุจู ุงุณุชุฎุฏุงูู ููุชูุจุคุ ููู ูุง ุณูููู ุจู ุจุงูุถุจุท.

**ููุงุญุธุฉ**: ุฃู [ูุฐู ุงูููุทุฉ](https://huggingface.co/MCG-NJU/videomae-base-finetuned-kinetics) ุชุคุฏู ุฅูู ุฃุฏุงุก ุฃูุถู ูู ูุฐู ุงููููุฉ ูุฃู ููุทุฉ ุงูุชูุชูุด ุชู ุงูุญุตูู ุนูููุง ุนู ุทุฑูู ุงูุถุจุท ุงูุฏููู ุนูู ูููุฉ ุฃุณูู ูุฌุฑู ููุงุซูุฉ ุฐุงุช ุชุฏุงุฎู ูุจูุฑ ูู ุงููุทุงู. ููููู ุงูุชุญูู ูู [ูุฐู ุงูููุทุฉ](https://huggingface.co/sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset) ูุงูุชู ุชู ุงูุญุตูู ุนูููุง ุนู ุทุฑูู ุงูุถุจุท ุงูุฏููู ูู `MCG-NJU/videomae-base-finetuned-kinetics`.

## ุฅุนุฏุงุฏ ูุฌููุนุงุช ุงูุจูุงูุงุช ููุชุฏุฑูุจ

ููุนุงูุฌุฉ ููุงุทุน ุงูููุฏูู ูุณุจููุงุ ุณุชุณุชุฎุฏู ููุชุจุฉ [PyTorchVideo](https://pytorchvideo.org/). ุงุจุฏุฃ ุจุงุณุชูุฑุงุฏ ุงูุชุจุนูุงุช ุงูุชู ูุญุชุงุฌูุง.

```py
>>> import pytorchvideo.data

>>> from pytorchvideo.transforms import (
...     ApplyTransformToKey,
...     Normalize,
...     RandomShortSideScale,
...     RemoveKey,
...     ShortSideScale,
...     UniformTemporalSubsample,
... )

>>> from torchvision.transforms import (
...     Compose,
...     Lambda,
...     RandomCrop,
...     RandomHorizontalFlip,
...     Resize,
... )
```

ุจุงููุณุจุฉ ูุชุญูููุงุช ูุฌููุนุฉ ุจูุงูุงุช ุงูุชุฏุฑูุจุ ุงุณุชุฎุฏู ูุฒูุฌูุง ูู ุงูุงุณุชุนูุงู ุงูุฒููู ุงูููุญุฏุ ูุชุทุจูุน ุงูุจูุณูุ ูุงูุชูุทูุน ุงูุนุดูุงุฆูุ ูุงูุงูุนูุงุณ ุงูุฃููู ุงูุนุดูุงุฆู. ุจุงููุณุจุฉ ูุชุญูููุงุช ูุฌููุนุฉ ุจูุงูุงุช ุงูุชุญูู ูู ุงูุตุญุฉ ูุงูุชููููุ ุงุญุชูุธ ุจููุณ ุชุณูุณู ุงูุชุญููุงุช ุจุงุณุชุซูุงุก ุงูุชูุทูุน ุงูุนุดูุงุฆู ูุงูุงูุนูุงุณ ุงูุฃููู. ููุฒูุฏ ูู ุงููุนูููุงุช ุญูู ุชูุงุตูู ูุฐู ุงูุชุญููุงุชุ ุฑุงุฌุน [ุงููุซุงุฆู ุงูุฑุณููุฉ ูู PyTorchVideo](https://pytorchvideo.org).

ุงุณุชุฎุฏู `image_processor` ุงููุฑุชุจุท ุจุงููููุฐุฌ ุงูููุฏุฑุจ ูุณุจููุง ููุญุตูู ุนูู ุงููุนูููุงุช ุงูุชุงููุฉ:

* ูุชูุณุท ุงูุงูุญุฑุงู ุงููุนูุงุฑู ููุตูุฑุฉ ุงูุชู ุณูุชู ุชุทุจูุนูุง ูุนูุง ุจูุณู ุฅุทุงุฑ ุงูููุฏูู.
* ุงูุฏูุฉ ุงูููุงููุฉ ุงูุชู ุณูุชู ุชุบููุฑ ุญุฌู ุฅุทุงุฑุงุช ุงูููุฏูู ุฅูููุง.

ุงุจุฏุฃ ุจุชุญุฏูุฏ ุจุนุถ ุงูุซูุงุจุช.

```py
>>> mean = image_processor.image_mean
>>> std = image_processor.image_std
>>> if "shortest_edge" in image_processor.size:
...     height = width = image_processor.size["shortest_edge"]
>>> else:
...     height = image_processor.size["height"]
...     width = image_processor.size["width"]
>>> resize_to = (height, width)

>>> num_frames_to_sample = model.config.num_frames
>>> sample_rate = 4
>>> fps = 30
>>> clip_duration = num_frames_to_sample * sample_rate / fps
```

ุงูุขูุ ูู ุจุชุนุฑูู ุชุญูููุงุช ูุฌููุนุฉ ุงูุจูุงูุงุช ุงููุญุฏุฏุฉ ููุฌููุนุงุช ุงูุจูุงูุงุช ุนูู ุงูุชูุงูู. ุจุฏุกูุง ูู ูุฌููุนุฉ ุงูุชุฏุฑูุจ:

```py
>>> train_transform = Compose(
...     [
...         ApplyTransformToKey(
...             key="video",
...             transform=Compose(
...                 [
...                     UniformTemporalSubsample(num_frames_to_sample),
...                     Lambda(lambda x: x / 255.0),
...                     Normalize(mean, std),
...                     RandomShortSideScale(min_size=256, max_size=320),
...                     RandomCrop(resize_to),
...                     RandomHorizontalFlip(p=0.5),
...                 ]
...             ),
...         ),
...     ]
... )

>>> train_dataset = pytorchvideo.data.Ucf101(
...     data_path=os.path.join(dataset_root_path, "train"),
...     clip_sampler=pytorchvideo.data.make_clip_sampler("random", clip_duration),
...     decode_audio=False,
...     transform=train_transform,
... )
```

ูููู ุชุทุจูู ููุณ ุชุณูุณู ุณูุฑ ุงูุนูู ุนูู ูุฌููุนุงุช ุงูุชุญูู ูู ุงูุตุญุฉ ูุงูุชูููู:

```py
>>> val_transform = Compose(
...     [
...         ApplyTransformToKey(
...             key="video",
...             transform=Compose(
...                 [
...                     UniformTemporalSubsample(num_frames_to_sample),
...                     Lambda(lambda x: x / 255.0),
...                     Normalize(mean, std),
...                     Resize(resize_to),
...                 ]
...             ),
...         ),
...     ]
... )

>>> val_dataset = pytorchvideo.data.Ucf101(
...     data_path=os.path.min(dataset_root_path, "val"),
...     clip_sampler=pytorchvideo.data.make_clip_sampler("uniform", clip_duration),
...     decode_audio=False,
...     transform=val_transform,
... )

>>> test_dataset = pytorchvideo.data.Ucf101(
...     data_path=os.path.join(dataset_root_path, "test"),
...     clip_sampler=pytorchvideo.data.make_clip_sampler("uniform", clip_duration),
...     decode_audio=False,
...     transform=val_transform,
... )
```

**ููุงุญุธุฉ**: ุชู ุฃุฎุฐ ุฎุทูุท ุฃูุงุจูุจ ูุฌููุนุฉ ุงูุจูุงูุงุช ุฃุนูุงู ูู [ูุซุงู PyTorchVideo ุงูุฑุณูู](https://pytorchvideo.org/docs/tutorial_classification#dataset). ูุญู ูุณุชุฎุฏู ุงูุฏุงูุฉ [`pytorchvideo.data.Ucf101()`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.Ucf101) ูุฃููุง ูุตููุฉ ุฎุตูุตูุง ููุฌููุนุฉ ุจูุงูุงุช UCF-101. ูู ุงูุฃุณุงุณุ ูุฅูู ูุนูุฏ ูุงุฆู [`pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.LabeledVideoDataset). ุชุนุฏ ูุฆุฉ `LabeledVideoDataset` ุงููุฆุฉ ุงูุฃุณุงุณูุฉ ูุฌููุน ููุงุทุน ุงูููุฏูู ูู ูุฌููุนุฉ ุจูุงูุงุช PyTorchVideo. ูุฐููุ ุฅุฐุง ููุช ุชุฑูุฏ ุงุณุชุฎุฏุงู ูุฌููุนุฉ ุจูุงูุงุช ูุฎุตุตุฉ ุบูุฑ ูุฏุนููุฉ ุงูุชุฑุงุถููุง ุจูุงุณุทุฉ PyTorchVideoุ ูููููู ุชูุณูุน ูุฆุฉ `LabeledVideoDataset` ููููุง ูุฐูู. ุฑุงุฌุน ูุซุงุฆู [API ููุจูุงูุงุช](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html) ููุนุฑูุฉ ุงููุฒูุฏ. ุฃูุถูุงุ ุฅุฐุง ูุงูุช ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจู ุชุชุจุน ุจููุฉ ููุงุซูุฉ (ููุง ูู ููุถุญ ุฃุนูุงู)ุ ูุฅู ุงุณุชุฎุฏุงู `pytorchvideo.data.Ucf101()` ูุฌุจ ุฃู ูุนูู ุจุดูู ุฌูุฏ.

ููููู ุงููุตูู ุฅูู ูุณูุท `num_videos` ููุนุฑูุฉ ุนุฏุฏ ููุงุทุน ุงูููุฏูู ูู ูุฌููุนุฉ ุงูุจูุงูุงุช.

```py
>>> print(train_dataset.num_videos, val_dataset.num_videos, test_dataset.num_videos)
# (300ุ 30ุ 75)
```

## ุชุตูุฑ ุงูููุฏูู ุงููุนุงูุฌ ูุณุจููุง ููุชุตุญูุญ ุงูุฃูุถู

```py
>>> import imageio
>>> import numpy as np
>>> from IPython.display import Image

>>> def unnormalize_img(img):
...     """Un-normalizes the image pixels."""
...     img = (img * std) + mean
...     img = (img * 255).astype("uint8")
...     return img.clip(0, 255)

>>> def create_gif(video_tensor, filename="sample.gif"):
...     """Prepares a GIF from a video tensor.
...     
...     The video tensor is expected to have the following shape:
...     (num_frames, num_channels, height, width).
...     """
...     frames = []
...     for video_frame in video_tensor:
...         frame_unnormalized = unnormalize_img(video_frame.permute(1, 2, 0).numpy())
...         frames.append(frame_unnormalized)
...     kargs = {"duration": 0.25}
...     imageio.mimsave(filename, frames, "GIF", **kargs)
...     return filename

>>> def display_gif(video_tensor, gif_name="sample.gif"):
...     """Prepares and displays a GIF from a video tensor."""
...     video_tensor = video_tensor.permute(1, 0, 2, 3)
...     gif_filename = create_gif(video_tensor, gif_name)
...     return Image(filename=gif_filename)

>>> sample_video = next(iter(train_dataset))
>>> video_tensor = sample_video["video"]
>>> display_gif(video_tensor)
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_gif.gif" alt="Person playing basketball"/>
</div>

## ุชุฏุฑูุจ ุงููููุฐุฌ 

ุงุณุชูุฏ ูู [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer) ูู  ๐ค Transformers ูุชุฏุฑูุจ ุงููููุฐุฌ. ูุชููุฆุฉ ูุซูู ูู `Trainer`ุ ุชุญุชุงุฌ ุฅูู ุชุญุฏูุฏ ุชูููู ุงูุชุฏุฑูุจ ูููุงููุณ ุงูุชูููู. ูุงูุฃูู ูู ุฐูู ูู [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments)ุ ููู ูุฆุฉ ุชุญุชูู ุนูู ุฌููุน ุงูุณูุงุช ูุชูููู ุงูุชุฏุฑูุจ. ูุชุทูุจ ุงุณู ูุฌูุฏ ุงูุฅุฎุฑุงุฌุ ูุงูุฐู ุณูุชู ุงุณุชุฎุฏุงูู ูุญูุธ ููุงุท ุงูุชูุชูุด ูููููุฐุฌ. ููุง ูุณุงุนุฏ ูู ูุฒุงููุฉ ุฌููุน ุงููุนูููุงุช ูู ูุณุชูุฏุน ุงููููุฐุฌ ุนูู ๐ค Hub.

ูุนุธู ุงูุญุฌุฌ ุงูุชุฏุฑูุจูุฉ ูุงุถุญุฉุ ูููู ููุงู ูุงุญุฏุฉ ูููุฉ ุฌุฏูุง ููุง ูู `remove_unused_columns=False`. ุณูุคุฏู ูุฐุง ุฅูู ุฅุณูุงุท ุฃู ููุฒุงุช ูุง ูุณุชุฎุฏููุง ุงูุฏุงูุฉ call ุงูุฎุงุตุฉ ุจุงููููุฐุฌ. ุจุดูู ุงูุชุฑุงุถูุ ูููู True ูุฃูู ูู ุงููุซุงูู ุนุงุฏุฉู ุฅุณูุงุท ุฃุนูุฏุฉ ุงูููุฒุงุช ุบูุฑ ุงููุณุชุฎุฏูุฉุ ููุง ูุฌุนู ูู ุงูุณูู ุชูููู ุงูุฅุฏุฎุงูุงุช ูู ุฏุงูุฉ ุงูุงุณุชุฏุนุงุก ุงูุฎุงุตุฉ ุจุงููููุฐุฌ. ููููุ ูู ูุฐู ุงูุญุงูุฉุ ูุฃูุช ุจุญุงุฌุฉ ุฅูู ุงูููุฒุงุช ุบูุฑ ุงููุณุชุฎุฏูุฉ ('video' ุนูู ูุฌู ุงูุฎุตูุต) ูู ุฃุฌู ุฅูุดุงุก 'pixel_values' (ููู ููุชุงุญ ุฅูุฒุงูู ูุชููุนู ูููุฐุฌูุง ูู ุฅุฏุฎุงูุงุชู).


```py 
>>> from transformers import TrainingArgumentsุ Trainer

>>> model_name = model_ckpt.split("/")[-1]
>>> new_model_name = f"{model_name}-finetuned-ucf101-subset"
>>> num_epochs = 4

>>> args = TrainingArguments(
...     new_model_nameุ
...     remove_unused_columns=Falseุ
...     eval_strategy="epoch"ุ
...     save_strategy="epoch"ุ
...     learning_rate=5e-5ุ
...     per_device_train_batch_size=batch_sizeุ
...     per_device_eval_batch_size=batch_sizeุ
...     warmup_ratio=0.1ุ
...     logging_steps=10ุ
...     load_best_model_at_end=Trueุ
...     metric_for_best_model="accuracy"ุ
...     push_to_hub=Trueุ
...     max_steps=(train_dataset.num_videos // batch_size) * num_epochsุ
... )
```

ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุชู ุชู ุฅุฑุฌุงุนูุง ุจูุงุณุทุฉ `pytorchvideo.data.Ucf101()` ูุง ุชููุฐ ุทุฑููุฉ `__len__`. ูุฐููุ ูุฌุจ ุนูููุง ุชุญุฏูุฏ `max_steps` ุนูุฏ ุฅูุดุงุก ูุซูู ูู `TrainingArguments`. 

ุจุนุฏ ุฐููุ ุชุญุชุงุฌ ุฅูู ุชุญุฏูุฏ ุฏุงูุฉ ูุญุณุงุจ ุงูููุงููุณ ูู ุงูุชููุนุงุชุ ูุงูุชู ุณุชุณุชุฎุฏู `metric` ุงูุชู ุณุชููู ุจุชุญููููุง ุงูุขู. ุงููุนุงูุฌุฉ ุงููุณุจูุฉ ุงููุญูุฏุฉ ุงูุชู ูุฌุจ ุนููู ุงูููุงู ุจูุง ูู ุฃุฎุฐ argmax ูู logits ุงููุชููุนุฉ:

```py
import evaluate

metric = evaluate.load("accuracy")


def compute_metrics(eval_pred):
    predictions = np.argmax(eval_pred.predictions, axis=1)
    return metric.compute(predictions=predictions, references=eval_pred.label_ids)
```

**ููุงุญุธุฉ ุญูู ุงูุชูููู**:

ูู [ูุฑูุฉ VideoMAE](https://arxiv.org/abs/2203.12602)ุ ูุณุชุฎุฏู ุงููุคูููู ุงุณุชุฑุงุชูุฌูุฉ ุงูุชูููู ุงูุชุงููุฉ. ุญูุซ ูููููู ุจุชูููู ุงููููุฐุฌ ุนูู ุนุฏุฉ ููุงุทุน ูู ููุงุทุน ุงูููุฏูู ุงูุงุฎุชุจุงุฑูุฉ ูุชุทุจูู ุชูุทูุนุงุช ูุฎุชููุฉ ุนูู ุชูู ุงูููุงุทุน ูุงูุฅุจูุงุบ ุนู ุงููุชูุฌุฉ ุงูุฅุฌูุงููุฉ. ููุน ุฐููุ ุญุฑุตูุง ุนูู ุงูุจุณุงุทุฉ ูุงูุฅูุฌุงุฒุ ูุง ูุฃุฎุฐ ุฐูู ูู ุงูุงุนุชุจุงุฑ ูู ูุฐุง ุงูุจุฑูุงูุฌ ุงูุชุนูููู.

ูู ุฃูุถูุง ุจุชุนุฑูู `collate_fn`ุ ูุงูุชู ุณุชูุณุชุฎุฏู ูุฏูุฌ ุงูุฃูุซูุฉ ูู ูุฌููุนุงุช. ุชุชููู ูู ูุฌููุนุฉ ูู ููุชุงุญููุ ูููุง `pixel_values` ู`labels`.

```py
>>> def collate_fn(examples):
    # permute to (num_frames, num_channels, height, width)
    pixel_values = torch.stack(
        [example["video"].permute(1, 0, 2, 3) for example in examples]
    )
    labels = torch.tensor([example["label"] for example in examples])
    return {"pixel_values": pixel_values, "labels": labels}
```

ุจุนุฏ ุฐููุ ูู ุจุจุณุงุทุฉ ุจุชูุฑูุฑ ูู ูุฐุง ุจุงูุฅุถุงูุฉ ุฅูู ูุฌููุนุงุช ุงูุจูุงูุงุช ุฅูู `Trainer`:

```py
>>> trainer = Trainer(
    model,
    args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=image_processor,
    compute_metrics=compute_metrics,
    data_collator=collate_fn,
)
```

ูุฏ ุชุชุณุงุกู ุนู ุณุจุจ ุชูุฑูุฑ `image_processor` ูู tokenizer ุนูู ุงูุฑุบู ูู ุฃูู ููุช ุจูุนุงูุฌุฉ ุงูุจูุงูุงุช ุจุงููุนู. ูุฐุง ููุท ููุชุฃูุฏ ูู ุฃู ููู ุชูููู ูุนุงูุฌ ุงูุตูุฑ (ุงููุฎุฒู ุจุชูุณูู JSON) ุณูุชู ุชุญูููู ุฃูุถูุง ุฅูู ุงููุณุชูุฏุน ุนูู Hub.

ุงูุขูุ ูููู ุจุถุจุท ูููุฐุฌูุง ุนู ุทุฑูู ุงุณุชุฏุนุงุก ุทุฑููุฉ `train`:

```py
>>> train_results = trainer.train()
```

ุจูุฌุฑุฏ ุงูุชูุงู ุงูุชุฏุฑูุจุ ุดุงุฑู ูููุฐุฌู ุนูู Hub ุจุงุณุชุฎุฏุงู ุทุฑููุฉ [`~transformers.Trainer.push_to_hub`] ุญุชู ูุชููู ุงูุฌููุน ูู ุงุณุชุฎุฏุงู ูููุฐุฌู:

```py
>>> trainer.push_to_hub()
```

## ุงูุงุณุชูุชุงุฌ

ุฑุงุฆุนุ ุงูุขู ุจุนุฏ ุฃู ุถุจุทุช ูููุฐุฌูุงุ ููููู ุงุณุชุฎุฏุงูู ููุงุณุชูุชุงุฌ!

ูู ุจุชุญููู ููุทุน ููุฏูู ููุงุณุชูุชุงุฌ:

```py
>>> sample_test_video = next(iter(test_dataset))
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_gif_two.gif" alt="ูุฑู ุชูุนุจ ูุฑุฉ ุงูุณูุฉ"/>
</div>

ุฃุจุณุท ุทุฑููุฉ ูุชุฌุฑุจุฉ ูููุฐุฌู ุงููุถุจูุท ููุงุณุชูุชุงุฌ ูู ุงุณุชุฎุฏุงูู ูู [`pipeline`](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.VideoClassificationPipeline). ูู ุจุชูููุฐ ุนูููุฉ ุฃูุงุจูุจ ูุชุตููู ุงูููุฏูู ุจุงุณุชุฎุฏุงู ูููุฐุฌูุ ููุฑุฑ ุงูููุฏูู ุฅููู:

```py
>>> from transformers import pipeline

>>> video_cls = pipeline(model="my_awesome_video_cls_model")
>>> video_cls("https://huggingface.co/datasets/sayakpaul/ucf101-subset/resolve/main/v_BasketballDunk_g14_c06.avi")
[{'score': 0.9272987842559814, 'label': 'BasketballDunk'},
 {'score': 0.017777055501937866, 'label': 'BabyCrawling'},
 {'score': 0.01663011871278286, 'label': 'BalanceBeam'},
 {'score': 0.009560945443809032, 'label': 'BandMarching'},
 {'score': 0.0068979403004050255, 'label': 'BaseballPitch'}]
```

ููููู ุฃูุถูุง ูุญุงูุงุฉ ูุชุงุฆุฌ ุงูุฃูุงุจูุจ ูุฏูููุง ุฅุฐุง ุฃุฑุฏุช.

```py
>>> def run_inference(model, video):
    # (num_frames, num_channels, height, width)
    perumuted_sample_test_video = video.permute(1, 0, 2, 3)
    inputs = {
        "pixel_values": perumuted_sample_test_video.unsqueeze(0),
        "labels": torch.tensor(
            [sample_test_video["label"]]
        ),  # ูููู ุชุฎุทู ูุฐุง ุฅุฐุง ูู ุชูู ูุฏูู ุชุณููุงุช ูุชุงุญุฉ.
    }

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    inputs = {k: v.to(device) for k, v in inputs.items()}
    model = model.to(device)

    # forward pass
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits

    return logits
```

ุงูุขูุ ูู ุจุชูุฑูุฑ ุฅุฏุฎุงูู ุฅูู ุงููููุฐุฌ ูุฅุฑุฌุงุน `logits`:

```py
>>> logits = run_inference(trained_model, sample_test_video["video"])
```

ุจุนุฏ ูู ุชุดููุฑ `logits`ุ ูุญุตู ุนูู:

```py
>>> predicted_class_idx = logits.argmax(-1).item()
>>> print("Predicted class:", model.config.id2label[predicted_class_idx])
# Predicted class: BasketballDunk
```