<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

โ๏ธ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# ุงูุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉ ูู ุงููุณุชูุฏุงุช (Document Question Answering)

[[open-in-colab]]

ุงูุฅุฌุงุจุฉ ุนูู ุฃุณุฆูุฉ ุงููุณุชูุฏุงุชุ ูุชูุนุฑู ุฃูุถูุง ุจุงุณู "ุงูุฅุฌุงุจุฉ ุงูุจุตุฑูุฉ ุนูู ุฃุณุฆูุฉ ุงููุณุชูุฏุงุช"ุ ูู ูููุฉ ุชุชุถูู ุชูุฏูู ุฅุฌุงุจุงุช ุนู ุฃุณุฆูุฉ ุชูุทุฑุญ ุญูู ุตูุฑ ูุณุชูุฏูุฉ. ุนุงุฏุฉู ูุง ูููู ุฅุฏุฎุงู ุงูููุงุฐุฌ ุงูุฏุงุนูุฉ ููุฐู ุงููููุฉ ูุฒูุฌูุง ูู ุตูุฑุฉ ูุณุคุงูุ ุจูููุง ูููู ุงูุฅุฎุฑุงุฌ ุฅุฌุงุจุฉ ุจุงููุบุฉ ุงูุทุจูุนูุฉ. ุชุณุชููุฏ ูุฐู ุงูููุงุฐุฌ ูู ุนุฏุฉ ุฃููุงุท (modalities)ุ ุจูุง ูู ุฐูู ุงููุตุ ูููุงูุน ุงููููุงุช (ุงูุตูุงุฏูู ุงูููุญุฏููุฏุฉ bounding boxes)ุ ูุงูุตูุฑุฉ ููุณูุง.

ููุถูุญ ูุฐุง ุงูุฏููู ููููุฉ:

- ุถุจุท [LayoutLMv2](../model_doc/layoutlmv2) ุนูู ุจูุงูุงุช [DocVQA](https://huggingface.co/datasets/nielsr/docvqa_1200_examples_donut).
- ุงุณุชุฎุฏุงู ูููุฐุฌู ุงููุถุจูุท ููุงุณุชุฏูุงู.

<Tip>

ูุงุทููุงุน ุนูู ุฌููุน ุงูุจููู ูููุงุท ุงูุชุญูู ุงููุชูุงููุฉ ูุน ูุฐู ุงููููุฉุ ููุตุญ ุจุฒูุงุฑุฉ [ุตูุญุฉ ุงููููุฉ](https://huggingface.co/tasks/image-to-text)

</Tip>

ูุนุงูุฌ LayoutLMv2 ูููุฉ ุงูุฅุฌุงุจุฉ ุนูู ุฃุณุฆูุฉ ุงููุณุชูุฏุงุช ุจุฅุถุงูุฉ ุฑุฃุณ (head) ุฎุงุต ุจุงูุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉ ุฃุนูู ุงูุญุงูุงุช ุงููุฎููุฉ ุงูููุงุฆูุฉ ููุฑููุฒ (tokens)ุ ููุชูุจุค ุจููุงูุน ุฑููุฒ ุงูุจุฏุงูุฉ ูุงูููุงูุฉ ููุฅุฌุงุจุฉ. ูุจูุนูู ุขุฎุฑุ ุชูุนุงูู ุงููุดููุฉ ุนูู ุฃููุง ุฅุฌุงุจุฉ ุงุณุชุฎุฑุงุฌูุฉ: ุจุงููุธุฑ ุฅูู ุงูุณูุงูุ ุงุณุชุฎุฑุฌ ุงููุนูููุฉ ุงูุชู ุชุฌูุจ ุนู ุงูุณุคุงู. ูุฃุชู ุงูุณูุงู ูู ูุฎุฑุฌุงุช ูุญุฑู OCRุ ูููุง ูุณุชุฎุฏู Tesseract ูู Google.

ูุจู ุงูุจุฏุกุ ุชุฃูุฏ ูู ุชุซุจูุช ุฌููุน ุงูููุชุจุงุช ุงููุงุฒูุฉ. ูุนุชูุฏ LayoutLMv2 ุนูู detectron2 ูtorchvision ูtesseract.

```bash
pip install -q transformers datasets
```

```bash
pip install 'git+https://github.com/facebookresearch/detectron2.git'
pip install torchvision
```

```bash
sudo apt install tesseract-ocr
pip install -q pytesseract
```

ุจุนุฏ ุชุซุจูุช ุฌููุน ุงูุชุจุนูุงุชุ ุฃุนุฏ ุชุดุบูู ุจูุฆุชู ุงูุชูููุฐูุฉ.

ูุดุฌูุนู ุนูู ูุดุงุฑูุฉ ูููุฐุฌู ูุน ุงููุฌุชูุน. ุณุฌูู ุงูุฏุฎูู ุฅูู ุญุณุงุจู ุนูู Hugging Face ูุฑูุนู ุฅูู ๐ค Hub. ุนูุฏ ุงููุทุงูุจุฉุ ุฃุฏุฎู ุฑูุฒ ุงููุตูู ุงูุฎุงุต ุจู:

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

ููุนุฑูู ุจุนุถ ุงููุชุบูุฑุงุช ุงูุนุงูุฉ.

```py
>>> model_checkpoint = "microsoft/layoutlmv2-base-uncased"
>>> batch_size = 4
```

## ุชุญููู ุงูุจูุงูุงุช

ูุณุชุฎุฏู ูุฐุง ุงูุฏููู ุนููุฉ ุตุบูุฑุฉ ูู DocVQA ุงูููุนุงูุฌุฉ ูุณุจููุง ูุงููุชููุฑุฉ ุนูู ๐ค Hub. ุฅุฐุง ุฑุบุจุช ูู ุงุณุชุฎุฏุงู ูุฌููุนุฉ DocVQA ูุงููุฉุ ููููู ุงูุชุณุฌูู ูุชูุฒูููุง ูู [ุงูุตูุญุฉ ุงูุฑุฆูุณูุฉ ูู DocVQA](https://rrc.cvc.uab.es/?ch=17). ุฅุฐุง ูุนูุช ุฐููุ ููุชุงุจุนุฉ ูุฐุง ุงูุฏููู ุงุทูุน ุนูู [ููููุฉ ุชุญููู ุงููููุงุช ุฅูู ูุฌููุนุฉ ุจูุงูุงุช ๐ค](https://huggingface.co/docs/datasets/loading#local-and-remote-files).

```py
>>> from datasets import load_dataset

>>> dataset = load_dataset("nielsr/docvqa_1200_examples")
>>> dataset
DatasetDict({
    train: Dataset({
        features: ['id', 'image', 'query', 'answers', 'words', 'bounding_boxes', 'answer'],
        num_rows: 1000
    })
    test: Dataset({
        features: ['id', 'image', 'query', 'answers', 'words', 'bounding_boxes', 'answer'],
        num_rows: 200
    })
})
```

ููุง ุชุฑูุ ุชููุณู ูุฌููุนุฉ ุงูุจูุงูุงุช ุจุงููุนู ุฅูู ุชุฏุฑูุจู (train) ูุงุฎุชุจุงุฑู (test). ุฃููู ูุธุฑุฉ ุนูู ูุซุงู ุนุดูุงุฆู ููุชุนุฑูู ุนูู ุงูููุฒุงุช.

```py
>>> dataset["train"].features
```

ููุซู ูู ุญูู ููุง ููู:
* `id`: ูุนุฑูู ุงููุซุงู
* `image`: ูุงุฆู PIL.Image.Image ูุญุชูู ุนูู ุตูุฑุฉ ุงููุณุชูุฏ
* `query`: ุงูุณุคุงู (ูุฏ ูุญุชูู ุนูู ุชุฑุฌูุงุช ูุชุนุฏุฏุฉุ ูุซู `en`)
* `answers`: ูุงุฆูุฉ ุจุงูุฅุฌุงุจุงุช ุงูุตุญูุญุฉ ุงูุชู ูุฏูููุง ุงููููููููู ุงูุจุดุฑููู
* `words` ู`bounding_boxes`: ูุชุงุฆุฌ OCRุ ูุงูุชู ูู ูุณุชุฎุฏููุง ููุง
* `answer`: ุฅุฌุงุจุฉ ุทุงุจูุชููุง ุฃุฏุงุฉ ูุฎุชููุฉ ููู ูุณุชุฎุฏููุง ููุง

ูููุจูู ุนูู ุงูุฃุณุฆูุฉ ุงูุฅูุฌููุฒูุฉ ููุทุ ูููุณูุท ุงูุญูู `answer` ุงูุฐู ูุจุฏู ุฃูู ูุญุชูู ุนูู ุชูุจุคุงุช ูููุฐุฌ ุขุฎุฑ. ุณูุฃุฎุฐ ุฃูุถูุง ุฃูู ุฅุฌุงุจุฉ ูู ูุฌููุนุฉ ุงูุฅุฌุงุจุงุช ุงูุชู ูููุฑูุง ุงูููููููู. ุจุฏูููุง ุนู ุฐููุ ููููู ุฃุฎุฐ ุนููุฉ ุนุดูุงุฆูุฉ.

```py
>>> def keep_english_drop_prediction(example):
...     new_example = {}
...     new_example["id"] = example["id"]
...     new_example["image"] = example["image"]
...     new_example["question"] = example["query"]["en"]
...     new_example["answers"] = example["answers"]["en"]
...     new_example["words"] = example["words"]
...     new_example["bounding_boxes"] = example["bounding_boxes"]
...     return new_example

>>> updated_dataset = dataset.map(keep_english_drop_prediction)
```

ูุฏ ูุฑุบุจ ุงูุจุนุถ ุจุชุตููุฉ ุงูุฃูุซูุฉ ุงูุชู ุชุชุฌุงูุฒ ุญุฏ ุทูู ุงูุฅุฏุฎุงู. ูุณุชุฎุฏู LayoutLMv2 ุญุฏูุง ุฃูุตู ุจุทูู 512 ุฑูุฒูุง ูููุต ูุน ูุฌููุน ุงููููุงุชุ ูุฐุง ุณููุณููุท ุงูุฃูุซูุฉ ุงูุฃุทูู.

```py
>>> updated_dataset = updated_dataset.filter(lambda x: len(x["words"]) + len(x["question"].split()) < 512)
```

ูู ูุฐู ุงููุฑุญูุฉุ ูููุฒูู ููุฒุงุช OCR ูู ูุฐู ุงูุนูููุฉ. ุฅุฐ ุฅููุง ูุฎุตุตุฉ ูุถุจุท ูููุฐุฌ ุขุฎุฑุ ูุณุชุชุทูุจ ูุนุงูุฌุฉ ุฅุถุงููุฉ ูุชูุงุฆู ูุชุทูุจุงุช ุฅุฏุฎุงู ูููุฐุฌูุง. ุณูุญุณุจูุง ุนุจุฑ ุชุทุจูู OCR ุจุฃููุณูุง ุถูู ูุณุงุฑ ุงููุนุงูุฌุฉ (preprocessing) ุงูุฎุงุต ุจูุง ุจุงุณุชุฎุฏุงู Tesseract.

ูุจู ุชุฌููุฒ ุงูุจูุงูุงุชุ ูุญุชุงุฌ ุฅูู ุชุฌููุฒ ุงููุนุงููุฌ (processor) ุงูููุงุณุจ ูููููุฐุฌ. ุชุฌูุน [`LayoutLMv2Processor`] ุฏุงุฎูููุง ุจูู ูุนุงูุฌ ุงูุตูุฑ (image processor) ููุนุงูุฌุฉ ุจูุงูุงุช ุงูุตูุฑ ูููุฑูููุฒ ุงููุต (tokenizer) ูุชุฑููุฒ ุงููุต.

```py
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained(model_checkpoint)
```

### ูุนุงูุฌุฉ ุตูุฑ ุงููุณุชูุฏุงุช ูุณุจููุง

ุณููุชุจ ุฏูุงููุง ูุณุงุนุฏุฉ ูุชุทุจูู OCR ุนูู ุงูุตูุฑ ูุงุณุชุฎุฑุงุฌ ุงููููุงุช ูุงูุตูุงุฏูู ุงููุญูุทุฉ (bounding boxes) ุจุงุณุชุฎุฏุงู pytesseract.

```py
>>> import pytesseract
>>> from pytesseract import Output
>>> from PIL import Image

>>> def get_ocr_words_and_boxes(batch):
...     images = batch["image"]
...     words = []
...     boxes = []
...     for image in images:
...         data = pytesseract.image_to_data(image.convert("RGB"), output_type=Output.DICT)
...         words.append(data["text"])  # ูููุงุช ุนูู ูุณุชูู ุงููููุฉ
...         # ุงูุตูุงุฏูู: x, y, w, h -> ุญููููุง ุฅูู ูุธุงู LayoutLMv2 [0, 1000]
...         image_width, image_height = image.size
...         normalized_boxes = []
...         for x, y, w, h in zip(data["left"], data["top"], data["width"], data["height"]):
...             x0, y0, x1, y1 = x, y, x + w, y + h
...             normalized_boxes.append([
...                 int(1000 * x0 / image_width),
...                 int(1000 * y0 / image_height),
...                 int(1000 * x1 / image_width),
...                 int(1000 * y1 / image_height),
...             ])
...         boxes.append(normalized_boxes)
...     return {"words": words, "boxes": boxes}

>>> dataset_with_ocr = updated_dataset.map(get_ocr_words_and_boxes, batched=True, batch_size=2)
```

### ูุนุงูุฌุฉ ุงูุจูุงูุงุช ุงููุตูุฉ ูุณุจููุง

ูุญููู ุงูุขู ุงููููุงุช ูุงูุตูุงุฏูู ุงูุชู ุญุตููุง ุนูููุง ูู ุงูุฎุทูุฉ ุงูุณุงุจูุฉ ุฅูู `input_ids` ู`attention_mask` ู`token_type_ids` ู`bbox` ุนูู ูุณุชูู ุงูุฑููุฒ (token-level). ุณูุญุชุงุฌ ุฅูู `tokenizer` ูู ุงููุนุงูุฌ ููุนุงูุฌุฉ ุงููุต.

```py
>>> tokenizer = processor.tokenizer
```

ุณููุชุจ ุฏุงูุฉ ูุณุงุนุฏุฉ ููุนุซูุฑ ุนูู ูููุน ุงูุฅุฌุงุจุฉ (ูุงุฆูุฉ ูููุงุช) ุถูู ูุงุฆูุฉ ูููุงุช ุงููุซุงู.

```py
>>> def subfinder(words_list, answer_list):
...     matches = []
...     start_indices = []
...     end_indices = []
...     for idx, i in enumerate(range(len(words_list))):
...         if words_list[i] == answer_list[0] and words_list[i : i + len(answer_list)] == answer_list:
...             matches.append(words_list[i : i + len(answer_list)])
...             start_indices.append(i)
...             end_indices.append(i + len(answer_list) - 1)
...     if len(matches) > 0:
...         return matches[0], start_indices[0], end_indices[0]
...     else:
...         return None, 0, 0
```

ููุฌุฑูุจูุง ุนูู ูุซุงู ูุงุญุฏ.

```py
>>> example = dataset_with_ocr["train"][0]
>>> words = [word.lower() for word in example["words"]]
>>> answer = [word.lower() for word in example["answers"][0].split()]
>>> answer, word_idx_start, word_idx_end = subfinder(words, answer)

>>> print("Question: ", example["question"])
>>> print("Words:", words)
>>> print("Answer: ", example["answer"])
>>> print("start_index", word_idx_start)
>>> print("end_index", word_idx_end)
```

ุจุนุฏ ุชุฑููุฒ ุงูุฃูุซูุฉ ุณุชุจุฏู ูุงูุชุงูู:

```py
>>> encoding = tokenizer(example["question"], example["words"], example["boxes"])
>>> tokenizer.decode(encoding["input_ids"])
```

ูููุชุจ ุฏุงูุฉ ูุชุฑููุฒ ุฏูุนุฉ ูู ุงูุฃูุซูุฉ ูุฅูุชุงุฌ ููุงุถุน ุงูุจุฏุงูุฉ ูุงูููุงูุฉ ููุฅุฌุงุจุฉ ุจูู ูุซุงู.

```py
>>> def encode_dataset(batch, max_length=512):
...     questions = batch["question"]
...     words = batch["words"]
...     boxes = batch["boxes"]
...     encoding = tokenizer(questions, words, boxes, max_length=max_length, padding="max_length", truncation=True)
...     start_positions = []
...     end_positions = []
...
...     # ุญููุฉ ุนูู ุงูุฃูุซูุฉ ูู ุงูุฏูุนุฉ
...     for i in range(len(questions)):
...         cls_index = encoding["input_ids"][i].index(tokenizer.cls_token_id)
...
...         # ุงุจุญุซ ุนู ูููุน ุงูุฅุฌุงุจุฉ ุถูู ูููุงุช ุงููุซุงู
...         words_example = [word.lower() for word in words[i]]
...         answer = words_example  # placeholder ุฅู ูุฒู
...         # ูุจุฏุฆููุง: ุญุงูู ูุทุงุจูุฉ ุฃูู ุฅุฌุงุจุฉ ููููุฑุฉ
...         ans_tokens = batch["answers"][i][0].lower().split()
...         _, word_idx_start, word_idx_end = subfinder(words_example, ans_tokens)
...
...         # ุญููู ููุงุถุน ุงููููุงุช ุฅูู ููุงุถุน ุงูุฑููุฒ ุนุจุฑ `word_ids`
...         word_ids = encoding.word_ids(i)
...         token_start_index = 0
...         while token_start_index < len(word_ids) and word_ids[token_start_index] != word_idx_start:
...             token_start_index += 1
...         token_end_index = len(word_ids) - 1
...         while token_end_index >= 0 and word_ids[token_end_index] != word_idx_end:
...             token_end_index -= 1
...
...         # ุฅู ูู ูุฌุฏูุงุ ุงุฌุนู ุงูุฅุฌุงุจุฉ ูู CLS
...         if token_start_index >= len(word_ids) or token_end_index < 0:
...             start_positions.append(cls_index)
...             end_positions.append(cls_index)
...         else:
...             start_positions.append(token_start_index)
...             end_positions.append(token_end_index)
...
...     encoding["start_positions"] = start_positions
...     encoding["end_positions"] = end_positions
...     return encoding
```

ุทุจูู ุงูุชุฑููุฒ ุนูู ูุณูู ุงูุชุฏุฑูุจ ูุงูุงุฎุชุจุงุฑ.

```py
>>> encoded_train_dataset = dataset_with_ocr["train"].map(
...     encode_dataset, batched=True, batch_size=2, remove_columns=dataset_with_ocr["train"].column_names
... )
>>> encoded_test_dataset = dataset_with_ocr["test"].map(
...     encode_dataset, batched=True, batch_size=2, remove_columns=dataset_with_ocr["test"].column_names
... )
```

ููุทููุน ุนูู ููุฒุงุช ูุฌููุนุฉ ุงูุจูุงูุงุช ุจุนุฏ ุงูุชุฑููุฒ:

```py
>>> encoded_train_dataset.features
```

## ุงูุชุฏุฑูุจ

ุชูุงูููุง! ููุฏ ุชุฌุงูุฒุช ุฃุตุนุจ ุฌุฒุก ูู ูุฐุง ุงูุฏููู ูุฃุตุจุญุช ุฌุงูุฒูุง ูุชุฏุฑูุจ ูููุฐุฌู. ูุชุถูู ุงูุชุฏุฑูุจ ุงูุฎุทูุงุช ุงูุชุงููุฉ:

- ุชุญููู ุงููููุฐุฌ ุงูููุงุณุจ ูููููุฉ.
- ุชุญุฏูุฏ ูุนุงููุงุช ุงูุชุฏุฑูุจ.
- ุชุฑุชูุจ ูุฌููุน ุงูุจูุงูุงุช (data collator).
- ุงุณุชุฏุนุงุก [`Trainer.train`].

ุฃูููุงุ ููุญููู ุงููููุฐุฌ.

```py
>>> from transformers import AutoModelForDocumentQuestionAnswering

>>> model = AutoModelForDocumentQuestionAnswering.from_pretrained(model_checkpoint)
```

ูู [`TrainingArguments`] ุงุณุชุฎุฏู `output_dir` ูุชุญุฏูุฏ ููุงู ุญูุธ ุงููููุฐุฌุ ูุงุถุจุท ุงููุงูุจุฑ-ุจุงุฑุงูุชุฑุฒ ููุง ุชุฑุงู ููุงุณุจูุง. ุฅุฐุง ุฑุบุจุช ูู ูุดุงุฑูุฉ ูููุฐุฌู ูุน ุงููุฌุชูุนุ ุงุถุจุท `push_to_hub` ุฅูู `True` (ูุฌุจ ุฃู ุชููู ูุณุฌูู ุงูุฏุฎูู ุฅูู Hugging Face). ูู ูุฐู ุงูุญุงูุฉ ุณูููู `output_dir` ุฃูุถูุง ุงุณู ูุณุชูุฏุน ุงููููุฐุฌ ุญูุซ ุณุชูุฑูุน ููุงุท ุงูุชุญูู.

```py
>>> from transformers import TrainingArguments

>>> # ุงุณุชุจุฏู ูุฐุง ุจูุนุฑูู ุงููุณุชูุฏุน ุงูุฎุงุต ุจู
>>> repo_id = "MariaK/layoutlmv2-base-uncased_finetuned_docvqa"

>>> training_args = TrainingArguments(
...     output_dir=repo_id,
...     per_device_train_batch_size=4,
...     num_train_epochs=20,
...     save_steps=200,
...     logging_steps=50,
...     eval_strategy="steps",
...     learning_rate=5e-5,
...     save_total_limit=2,
...     remove_unused_columns=False,
...     push_to_hub=True,
... )
```

ุนุฑูู ูุฌููุน ุจูุงูุงุช ุจุณูุทูุง ูุฌูุน ุงูุฃูุซูุฉ ูู ุฏูุนุงุช.

```py
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator()
```

ุฃุฎูุฑูุงุ ููุฌูุน ูู ุดูุก ููุณุชุฏุนู [`~Trainer.train`]:

```py
>>> from transformers import Trainer

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     data_collator=data_collator,
...     train_dataset=encoded_train_dataset,
...     eval_dataset=encoded_test_dataset,
...     processing_class=processor,
... )

>>> trainer.train()
```

ูุฅุถุงูุฉ ุงููููุฐุฌ ุงูููุงุฆู ุฅูู ๐ค Hubุ ุฃูุดุฆ ุจุทุงูุฉ ูููุฐุฌ ูุงุณุชุฏุนู `push_to_hub`:

```py
>>> trainer.create_model_card()
>>> trainer.push_to_hub()
```

## ุงูุงุณุชุฏูุงู

ุงูุขู ุจุนุฏ ุฃู ุถุจุทุช ูููุฐุฌ LayoutLMv2 ูุฑูุนุชู ุฅูู ๐ค Hubุ ููููู ุงุณุชุฎุฏุงูู ููุงุณุชุฏูุงู. ุฃุจุณุท ุทุฑููุฉ ูู ุงุณุชุฎุฏุงูู ุถูู [`Pipeline`].

ููุฃุฎุฐ ูุซุงููุง:
```py
>>> example = dataset["test"][2]
>>> question = example["query"]["en"]
>>> image = example["image"]
>>> print(question)
>>> print(example["answers"])
'Who is โpresidingโ TRRF GENERAL SESSION (PART 1)?'
['TRRF Vice President', 'lee a. waller']
```

ุจุนุฏูุงุ ุฃูุดุฆ ุจุงูุจูุงูู ููุฅุฌุงุจุฉ ุนูู ุฃุณุฆูุฉ ุงููุณุชูุฏุงุช ุจุงุณุชุฎุฏุงู ูููุฐุฌูุ ููุฑูุฑ ุงูุตูุฑุฉ + ุงูุณุคุงู ุฅููู.

```py
>>> from transformers import pipeline

>>> qa_pipeline = pipeline("document-question-answering", model="MariaK/layoutlmv2-base-uncased_finetuned_docvqa")
>>> qa_pipeline(image, question)
[{'score': 0.9949808120727539,
  'answer': 'Lee A. Waller',
  'start': 55,
  'end': 57}]
```

ููููู ุฃูุถูุง ุฅุนุงุฏุฉ ุชูููุฐ ุฎุทูุงุช ุงูุจุงูุจูุงูู ูุฏูููุง ุฅุฐุง ุฑุบุจุช:
1. ุฎุฐ ุตูุฑุฉ ูุณุคุงููุงุ ูุฌููุฒููุง ูููููุฐุฌ ุจุงุณุชุฎุฏุงู ุงููุนุงูุฌ.
2. ูุฑูุฑ ุงููุฏุฎูุงุช ุนุจุฑ ุงููููุฐุฌ.
3. ููุฑุฌุน ุงููููุฐุฌ `start_logits` ู`end_logits`ุ ูุชุดูุฑ ุฅูู ุฑูุฒ ุจุฏุงูุฉ ุงูุฅุฌุงุจุฉ ูุฑูุฒ ููุงูุชูุงุ ูููุงููุง ุจุงูุดูู (batch_size, sequence_length).
4. ุทุจูู argmax ุนูู ุงูุจูุนุฏ ุงูุฃุฎูุฑ ููู ูู `start_logits` ู`end_logits` ููุญุตูู ุนูู `start_idx` ู`end_idx` ุงููุชูุจุฃ ุจููุง.
5. ูู ุชุฑููุฒ ุงูุฅุฌุงุจุฉ ุจุงุณุชุฎุฏุงู tokenizer.

```py
>>> import torch
>>> from transformers import AutoProcessor
>>> from transformers import AutoModelForDocumentQuestionAnswering

>>> processor = AutoProcessor.from_pretrained("MariaK/layoutlmv2-base-uncased_finetuned_docvqa")
>>> model = AutoModelForDocumentQuestionAnswering.from_pretrained("MariaK/layoutlmv2-base-uncased_finetuned_docvqa")

>>> with torch.no_grad():
...     encoding = processor(image.convert("RGB"), question, return_tensors="pt")
...     outputs = model(**encoding)
...     start_logits = outputs.start_logits
...     end_logits = outputs.end_logits
...     predicted_start_idx = start_logits.argmax(-1).item()
...     predicted_end_idx = end_logits.argmax(-1).item()

>>> processor.tokenizer.decode(encoding.input_ids.squeeze()[predicted_start_idx : predicted_end_idx + 1])
'lee a. waller'
```
