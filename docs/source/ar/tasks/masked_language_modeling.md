# Masked language modeling

[[open-in-colab]]

<Youtube id="mqElG5QJWUg"/>

ÙŠØªÙ… ÙÙŠ Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…Ù‚Ù†Ø¹Ø© Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ù…Ø² Ù…Ù‚Ù†Ø¹ ÙÙŠ ØªØ³Ù„Ø³Ù„ØŒ ÙˆÙŠÙ…ÙƒÙ† Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø¨Ø§Ù„Ø±Ù…ÙˆØ² Ø«Ù†Ø§Ø¦ÙŠØ© Ø§Ù„Ø§ØªØ¬Ø§Ù‡. ÙˆÙ‡Ø°Ø§ ÙŠØ¹Ù†ÙŠ Ø£Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ø¯ÙŠÙ‡ Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ù„ÙˆØµÙˆÙ„ Ø§Ù„ÙƒØ§Ù…Ù„ Ø¥Ù„Ù‰ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„ÙŠØ³Ø§Ø± ÙˆØ§Ù„ÙŠÙ…ÙŠÙ†. ØªØ¹Ø¯ Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…Ù‚Ù†Ø¹Ø© Ø±Ø§Ø¦Ø¹Ø© Ù„Ù„Ù…Ù‡Ø§Ù… Ø§Ù„ØªÙŠ ØªØªØ·Ù„Ø¨ ÙÙ‡Ù…Ù‹Ø§ Ø³ÙŠØ§Ù‚ÙŠÙ‹Ø§ Ø¬ÙŠØ¯Ù‹Ø§ Ù„ØªØ³Ù„Ø³Ù„ ÙƒØ§Ù…Ù„. BERT Ù‡Ùˆ Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…Ù‚Ù†Ø¹.

Ø³ÙŠÙˆØ¶Ø­ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ù„Ùƒ ÙƒÙŠÙÙŠØ©:

1. Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ [DistilRoBERTa](https://huggingface.co/distilbert/distilroberta-base) Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© ÙØ±Ø¹ÙŠØ© [r/askscience](https://www.reddit.com/r/askscience/) Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª [ELI5](https://huggingface.co/datasets/eli5).
2. Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ù„Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬.

<Tip>

Ù„Ù…Ø´Ø§Ù‡Ø¯Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØµÙ…ÙŠÙ…Ø§Øª ÙˆÙ†Ù‚Ø§Ø· Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù‡Ù…Ø©ØŒ Ù†ÙˆØµÙŠ Ø¨Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† [ØµÙØ­Ø© Ø§Ù„Ù…Ù‡Ù…Ø©](https://huggingface.co/tasks/fill-mask)

</Tip>

Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ø¯Ø¡ØŒ ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ©:

```bash
pip install transformers datasets evaluate
```

Ù†Ø­Ù† Ù†Ø´Ø¬Ø¹Ùƒ Ø¹Ù„Ù‰ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¥Ù„Ù‰ Ø­Ø³Ø§Ø¨ Hugging Face Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ø­ØªÙ‰ ØªØªÙ…ÙƒÙ† Ù…Ù† ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ ÙˆÙ…Ø´Ø§Ø±ÙƒØªÙ‡ Ù…Ø¹ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹. Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙØ·Ù„Ø¨ Ù…Ù†Ùƒ Ø°Ù„ÙƒØŒ Ø£Ø¯Ø®Ù„ Ø±Ù…Ø²Ùƒ Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„:

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ELI5

Ø§Ø¨Ø¯Ø£ Ø¨ØªØ­Ù…ÙŠÙ„ Ø£ÙˆÙ„ 5000 Ù…Ø«Ø§Ù„ Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª [ELI5-Category](https://huggingface.co/datasets/eli5_category) Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø© Datasets ğŸ¤—. Ø³ÙŠØªÙŠØ­ Ù„Ùƒ Ù‡Ø°Ø§ ÙØ±ØµØ© Ø§Ù„ØªØ¬Ø±Ø¨Ø© ÙˆØ§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† ÙƒÙ„ Ø´ÙŠØ¡ ÙŠØ¹Ù…Ù„ Ù‚Ø¨Ù„ Ù‚Ø¶Ø§Ø¡ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ÙˆÙ‚Øª ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø©.

```py
>>> from datasets import load_dataset

>>> eli5 = load_dataset("eli5_category", split="train[:5000]")
```

Ù‚Ø³ÙÙ‘Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª ÙØ±Ø¹ÙŠØ© Ù„Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© [`~datasets.Dataset.train_test_split`]:

```py
>>> eli5 = eli5.train_test_split(test_size=0.2)
```

Ø«Ù… Ø§Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ù…Ø«Ø§Ù„:
```py
>>> eli5 = eli5.train_test_split(test_size=0.2)
```

Ø«Ù… Ø§Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ù…Ø«Ø§Ù„:

```py
>>> eli5["train"][0]
{'q_id': '7h191n',
 'title': 'What does the tax bill that was passed today mean? How will it affect Americans in each tax bracket?',
 'selftext': '',
 'category': 'Economics',
 'subreddit': 'explainlikeimfive',
 'answers': {'a_id': ['dqnds8l', 'dqnd1jl', 'dqng3i1', 'dqnku5x'],
  'text': ["The tax bill is 500 pages long and there were a lot of changes still going on right to the end. It's not just an adjustment to the income tax brackets, it's a whole bunch of changes. As such there is no good answer to your question. The big take aways are: - Big reduction in corporate income tax rate will make large companies very happy. - Pass through rate change will make certain styles of business (law firms, hedge funds) extremely happy - Income tax changes are moderate, and are set to expire (though it's the kind of thing that might just always get re-applied without being made permanent) - People in high tax states (California, New York) lose out, and many of them will end up with their taxes raised.",
   'None yet. It has to be reconciled with a vastly different house bill and then passed again.',
   'Also: does this apply to 2017 taxes? Or does it start with 2018 taxes?',
   'This article explains both the House and senate bills, including the proposed changes to your income taxes based on your income level. URL_0'],
  'score': [21, 19, 5, 3],
  'text_urls': [[],
   [],
   [],
   ['https://www.investopedia.com/news/trumps-tax-reform-what-can-be-done/']]},
 'title_urls': ['url'],
 'selftext_urls': ['url']}
```

Ø¹Ù„Ù‰ Ø§Ù„Ø±ØºÙ… Ù…Ù† Ø£Ù† Ù‡Ø°Ø§ Ù‚Ø¯ ÙŠØ¨Ø¯Ùˆ ÙƒØ«ÙŠØ±Ù‹Ø§ØŒ Ø¥Ù„Ø§ Ø£Ù†Ùƒ Ù…Ù‡ØªÙ… Ø­Ù‚Ù‹Ø§ Ø¨Ø­Ù‚Ù„ "Ø§Ù„Ù†Øµ". Ù…Ø§ Ù‡Ùˆ Ø±Ø§Ø¦Ø¹ ÙÙŠ Ù…Ù‡Ø§Ù… Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºØ© Ù‡Ùˆ Ø£Ù†Ùƒ Ù„Ø§ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¹Ù„Ø§Ù…Ø§Øª ØªØµÙ†ÙŠÙ (ØªÙØ¹Ø±Ù Ø£ÙŠØ¶Ù‹Ø§ Ø¨Ø§Ø³Ù… Ø§Ù„Ù…Ù‡Ù…Ø© ØºÙŠØ± Ø§Ù„Ø®Ø§Ø¶Ø¹Ø© Ù„Ù„Ø¥Ø´Ø±Ø§Ù) Ù„Ø£Ù† Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù‡ÙŠ Ø§Ù„ØªØµÙ†ÙŠÙ.

## Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø³Ø¨Ù‚Ø©

<Youtube id="8PmhEIXhBvI"/>

Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…Ù‚Ù†Ø¹Ø©ØŒ ØªØªÙ…Ø«Ù„ Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ© ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø¨Ø±Ù†Ø§Ù…Ø¬ ØªØ´ÙÙŠØ± DistilRoBERTa Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø­Ù‚Ù„ "Ø§Ù„Ù†Øµ" Ø§Ù„ÙØ±Ø¹ÙŠ:

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilroberta-base")
```

Ø³ØªÙ„Ø§Ø­Ø¸ Ù…Ù† Ø§Ù„Ù…Ø«Ø§Ù„ Ø£Ø¹Ù„Ø§Ù‡ØŒ Ø£Ù† Ø­Ù‚Ù„ "Ø§Ù„Ù†Øµ" Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ù„ÙØ¹Ù„ Ø¯Ø§Ø®Ù„ "Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª". ÙˆÙ‡Ø°Ø§ ÙŠØ¹Ù†ÙŠ Ø£Ù†Ùƒ Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø­Ù‚Ù„ "Ø§Ù„Ù†Øµ" Ù…Ù† Ù‡ÙŠÙƒÙ„Ù‡ Ø§Ù„Ù…Ø¶Ù…Ù† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© ["flatten"](https://huggingface.co/docs/datasets/process#flatten):
```py
>>> eli5 = eli5.flatten()
>>> eli5["train"][0]
{'q_id': '7h191n',
 'title': 'What does the tax bill that was passed today mean? How will it affect Americans in each tax bracket?',
 'selftext': '',
 'category': 'Economics',
 'subreddit': 'explainlikeimfive',
 'answers.a_id': ['dqnds8l', 'dqnd1jl', 'dqng3i1', 'dqnku5x'],
 'answers.text': ["The tax bill is 500 pages long and there were a lot of changes still going on right to the end. It's not just an adjustment to the income tax brackets, it's a whole bunch of changes. As such there is no good answer to your question. The big take aways are: - Big reduction in corporate income tax rate will make large companies very happy. - Pass through rate change will make certain styles of business (law firms, hedge funds) extremely happy - Income tax changes are moderate, and are set to expire (though it's the kind of thing that might just always get re-applied without being made permanent) - People in high tax states (California, New York) lose out, and many of them will end up with their taxes raised.",
  'None yet. It has to be reconciled with a vastly different house bill and then passed again.',
  'Also: does this apply to 2017 taxes? Or does it start with 2018 taxes?',
  'This article explains both the House and senate bills, including the proposed changes to your income taxes based on your income level. URL_0'],
 'answers.score': [21, 19, 5, 3],
 'answers.text_urls': [[],
  [],
  [],
  ['https://www.investopedia.com/news/trumps-tax-reform-what-can-be-done/']],
 'title_urls': ['url'],
 'selftext_urls': ['url']}
```

ÙƒÙ„ Ø­Ù‚Ù„ ÙØ±Ø¹ÙŠ Ù‡Ùˆ Ø§Ù„Ø¢Ù† Ø¹Ù…ÙˆØ¯ Ù…Ù†ÙØµÙ„ ÙƒÙ…Ø§ Ù‡Ùˆ Ù…ÙˆØ¶Ø­ Ø¨Ø§Ù„Ø¨Ø§Ø¯Ø¦Ø© "Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª"ØŒ ÙˆØ­Ù‚Ù„ "Ø§Ù„Ù†Øµ" Ù‡Ùˆ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¢Ù†. Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† ØªØ´ÙÙŠØ± ÙƒÙ„ Ø¬Ù…Ù„Ø© Ø¨Ø´ÙƒÙ„ Ù…Ù†ÙØµÙ„ØŒ Ù‚Ù… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¥Ù„Ù‰ Ø³Ù„Ø³Ù„Ø© Ø¨Ø­ÙŠØ« ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ÙÙŠØ±Ù‡Ø§ Ø¨Ø´ÙƒÙ„ Ù…Ø´ØªØ±Ùƒ.

Ù‡Ø°Ù‡ Ù‡ÙŠ Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ù„Ø¯Ù…Ø¬ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø³Ù„Ø§Ø³Ù„ Ù„ÙƒÙ„ Ù…Ø«Ø§Ù„ ÙˆØªØ´ÙÙŠØ± Ø§Ù„Ù†ØªÙŠØ¬Ø©:

```py
>>> def preprocess_function(examples):
...     return tokenizer([" ".join(x) for x in examples["answers.text"]])
```

Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ù‡Ø°Ù‡ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø·Ø±ÙŠÙ‚Ø© [`~datasets.Dataset.map`] ÙÙŠ Ù…ÙƒØªØ¨Ø© Datasets ğŸ¤—. ÙŠÙ…ÙƒÙ†Ùƒ ØªØ³Ø±ÙŠØ¹ ÙˆØ¸ÙŠÙØ© "map" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ¹ÙŠÙŠÙ† "batched=True" Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ù†Ø§ØµØ± Ù…ØªØ¹Ø¯Ø¯Ø© Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ ÙˆÙ‚Øª ÙˆØ§Ø­Ø¯ØŒ ÙˆØ²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… "num_proc". Ø§Ø­Ø°Ù Ø£ÙŠ Ø£Ø¹Ù…Ø¯Ø© Ù„Ø§ ØªØ­ØªØ§Ø¬ Ø¥Ù„ÙŠÙ‡Ø§:

```py
>>> tokenized_eli5 = eli5.map(
...     preprocess_function,
...     batched=True,
...     num_proc=4,
...     remove_columns=eli5["train"].column_names,
... )
```

ØªØ­ØªÙˆÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‡Ø°Ù‡ Ø¹Ù„Ù‰ ØªØ³Ù„Ø³Ù„Ø§Øª Ø§Ù„Ø±Ù…ÙˆØ²ØŒ ÙˆÙ„ÙƒÙ† Ø¨Ø¹Ø¶Ù‡Ø§ Ø£Ø·ÙˆÙ„ Ù…Ù† Ø·ÙˆÙ„ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬.

Ø§Ù„Ø¢Ù† ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ© Ù„:
- Ø¯Ù…Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ³Ù„Ø³Ù„Ø§Øª
- ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØªØ³Ù„Ø³Ù„Ø§Øª Ø§Ù„Ù…Ø¯Ù…Ø¬Ø© Ø¥Ù„Ù‰ Ù‚Ø·Ø¹ Ø£Ù‚ØµØ± Ù…Ø­Ø¯Ø¯Ø© Ø¨ÙˆØ§Ø³Ø·Ø© "block_size"ØŒ ÙˆØ§Ù„ØªÙŠ ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø£Ù‚ØµØ± Ù…Ù† Ø·ÙˆÙ„ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø£Ù‚ØµÙ‰ ÙˆÙ‚ØµÙŠØ±Ø© Ø¨Ø¯Ø±Ø¬Ø© ÙƒØ§ÙÙŠØ© Ù„Ø°Ø§ÙƒØ±Ø© GPU. 

```py
>>> block_size = 128


>>> def group_texts(examples):
...     # Concatenate all texts.
...     concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
...     total_length = len(concatenated_examples[list(examples.keys())[0]])
...     # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can
...     # customize this part to your needs.
...     if total_length >= block_size:
...         total_length = (total_length // block_size) * block_size
...     # Split by chunks of block_size.
...     result = {
...         k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
...         for k, t in concatenated_examples.items()
...     }
...     return result
```

Ù‚Ù… Ø¨ØªØ·Ø¨ÙŠÙ‚ Ø¯Ø§Ù„Ø© "group_texts" Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§:

```py
>>> lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)
```
Ù‚Ù… Ø¨ØªØ·Ø¨ÙŠÙ‚ Ø¯Ø§Ù„Ø© "group_texts" Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§:

```py
>>> lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)
```

Ø§Ù„Ø¢Ù† Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø¯ÙØ¹Ø© Ù…Ù† Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`DataCollatorForLanguageModeling`]. Ù…Ù† Ø§Ù„Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© *Ø­Ø´Ùˆ* Ø§Ù„Ø¬Ù…Ù„ Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠÙ‹Ø§ Ø¥Ù„Ù‰ Ø£Ø·ÙˆÙ„ Ø·ÙˆÙ„ ÙÙŠ Ø¯ÙØ¹Ø© Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¬Ù…ÙŠØ¹ØŒ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø­Ø´Ùˆ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§ Ø¥Ù„Ù‰ Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ø£Ù‚ØµÙ‰.

<frameworkcontent>
<pt>

Ø§Ø³ØªØ®Ø¯Ù… Ø±Ù…Ø² Ù†Ù‡Ø§ÙŠØ© Ø§Ù„ØªØ³Ù„Ø³Ù„ ÙƒØ±Ù…ÙˆØ² Ø­Ø´Ùˆ ÙˆØ­Ø¯Ø¯ "mlm_probability" Ù„Ø¥Ø®ÙØ§Ø¡ Ø§Ù„Ø±Ù…ÙˆØ² Ø¹Ø´ÙˆØ§Ø¦ÙŠÙ‹Ø§ ÙƒÙ„Ù…Ø§ Ù‚Ù…Øª Ø¨Ø§Ù„ØªÙ†Ù‚Ù„ Ø®Ù„Ø§Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:

```py
>>> from transformers import DataCollatorForLanguageModeling

>>> tokenizer.pad_token = tokenizer.eos_token
>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)
```
</pt>
<tf>

Ø§Ø³ØªØ®Ø¯Ù… Ø±Ù…Ø² Ù†Ù‡Ø§ÙŠØ© Ø§Ù„ØªØ³Ù„Ø³Ù„ ÙƒØ±Ù…ÙˆØ² Ø­Ø´Ùˆ ÙˆØ­Ø¯Ø¯ "mlm_probability" Ù„Ø¥Ø®ÙØ§Ø¡ Ø§Ù„Ø±Ù…ÙˆØ² Ø¹Ø´ÙˆØ§Ø¦ÙŠÙ‹Ø§ ÙƒÙ„Ù…Ø§ Ù‚Ù…Øª Ø¨Ø§Ù„ØªÙ†Ù‚Ù„ Ø®Ù„Ø§Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:

```py
>>> from transformers import DataCollatorForLanguageModeling

>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors="tf")
```
</tf>
</frameworkcontent>

## ØªØ¯Ø±ÙŠØ¨

<frameworkcontent>
<pt>
<Tip>

Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ø¹Ù„Ù‰ Ø¯Ø±Ø§ÙŠØ© Ø¨Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`Trainer`ØŒ ÙØ±Ø§Ø¬Ø¹ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù‡Ù†Ø§](../training#train-with-pytorch-trainer)

</Tip>

Ø£Ù†Øª Ø§Ù„Ø¢Ù† Ù…Ø³ØªØ¹Ø¯ Ù„Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬Ùƒ! Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ DistilRoBERTa Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`AutoModelForMaskedLM`]:

```py
>>> from transformers import AutoModelForMaskedLM

>>> model = AutoModelForMaskedLM.from_pretrained("distilbert/distilroberta-base")
```

ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø±Ø­Ù„Ø©ØŒ Ù‡Ù†Ø§Ùƒ Ø«Ù„Ø§Ø« Ø®Ø·ÙˆØ§Øª ÙÙ‚Ø·:

1. Ø­Ø¯Ø¯ ÙØ±Ø· Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ ÙÙŠ [`TrainingArguments`]. Ø§Ù„Ù…Ø¹Ù„Ù…Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ø§Ù„ÙˆØ­ÙŠØ¯Ø© Ù‡ÙŠ "output_dir" ÙˆØ§Ù„ØªÙŠ ØªØ­Ø¯Ø¯ Ù…ÙƒØ§Ù† Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬Ùƒ. ÙŠÙ…ÙƒÙ†Ùƒ Ø¯ÙØ¹ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ Hub Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ¹ÙŠÙŠÙ† "push_to_hub=True" (ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ù…Ø³Ø¬Ù„Ø§Ù‹ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¥Ù„Ù‰ Hugging Face Ù„ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ).
2. Ù…Ø±Ø± ÙØ±Ø· Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¥Ù„Ù‰ [`Trainer`] Ø¬Ù†Ø¨Ù‹Ø§ Ø¥Ù„Ù‰ Ø¬Ù†Ø¨ Ù…Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆÙ…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙ…Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.
3. Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ [`~Trainer.train`] Ù„Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬Ùƒ.

```py
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_eli5_mlm_model",
...     eval_strategy="epoch"ØŒ
...     learning_rate=2e-5,
...     num_train_epochs=3,
...     weight_decay=0.01,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=lm_dataset["train"],
...     eval_dataset=lm_dataset["test"],
...     data_collator=data_collator,
... )

>>> trainer.train()
```

Ø¨Ù…Ø¬Ø±Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø·Ø±ÙŠÙ‚Ø© [`~transformers.Trainer.evaluate`] Ù„ØªÙ‚ÙŠÙŠÙ… Ù†Ù…ÙˆØ°Ø¬Ùƒ ÙˆØ§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø­Ø¯ÙˆØ«Ù‡:

```py
>>> import math

>>> eval_results = trainer.evaluate()
>>> print(f"Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
Perplexity: 8.76
```

Ø«Ù… Ø´Ø§Ø±Ùƒ Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø¹Ù„Ù‰ Hub Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© [`~transformers.Trainer.push_to_hub`] Ø­ØªÙ‰ ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ø¬Ù…ÙŠØ¹ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬Ùƒ:

```py
>>> trainer.push_to_hub()
```
</pt>
<tf>
<Tip>

Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ø¹Ù„Ù‰ Ø¯Ø±Ø§ÙŠØ© Ø¨Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… KerasØŒ ÙØ±Ø§Ø¬Ø¹ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù‡Ù†Ø§ [../training#train-a-tensorflow-model-with-keras]!

</Tip>
Ù„Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ TensorFlowØŒ Ø§Ø¨Ø¯Ø£ Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ Ø¯Ø§Ù„Ø© Ù…Ø­Ø³Ù† ÙˆÙ…Ø¹Ø¯Ù„ ØªØ¹Ù„Ù… ÙˆØ¬Ø¯ÙˆÙ„ Ø²Ù…Ù†ÙŠ ÙˆÙ…ÙØ±Ø· Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨:

```py
>>> from transformers import create_optimizer, AdamWeightDecay

>>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)
```

Ø«Ù… ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ DistilRoBERTa Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`TFAutoModelForMaskedLM`]:

```py
>>> from transformers import TFAutoModelForMaskedLM

>>> model = TFAutoModelForMaskedLM. from_pretrained("distilbert/distilroberta-base")
```

Ù‚Ù… Ø¨ØªØ­ÙˆÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ "tf.data.Dataset" Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:

```py
>>> tf_train_set = model.prepare_tf_dataset(
...     lm_dataset["train"],
...     shuffle=True,
...     batch_size=16,
...     collate_fn=data_collator,
... )
```py
>>> tf_train_set = model.prepare_tf_dataset(
...     lm_dataset["train"],
...     shuffle=True,
...     batch_size=16,
...     collate_fn=data_collator,
... )

>>> tf_test_set = model.prepare_tf_dataset(
...     lm_dataset["test"],
...     shuffle=False,
...     batch_size=16,
...     collate_fn=data_collator,
... )
```

Ù‚Ù… Ø¨ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ["compile"](https://keras.io/api/models/model_training_apis/#compile-method). Ù„Ø§Ø­Ø¸ Ø£Ù† Ø¬Ù…ÙŠØ¹ Ù†Ù…Ø§Ø°Ø¬ Transformers ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¯Ø§Ù„Ø© Ø®Ø³Ø§Ø±Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø°Ø§Øª ØµÙ„Ø© Ø¨Ø§Ù„Ù…Ù‡Ù…Ø©ØŒ Ù„Ø°Ù„Ùƒ Ù„Ø§ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ ÙˆØ§Ø­Ø¯Ø© Ù…Ø§ Ù„Ù… ØªØ±ØºØ¨ ÙÙŠ Ø°Ù„Ùƒ:

```py
>>> import tensorflow as tf

>>> model.compile(optimizer=optimizer) # Ù„Ø§ ØªÙˆØ¬Ø¯ ÙˆØ³ÙŠØ·Ø© Ø¯Ø§Ù„Ø© Ø§Ù„Ø®Ø³Ø§Ø±Ø©!
```

ÙŠÙ…ÙƒÙ† Ø§Ù„Ù‚ÙŠØ§Ù… Ø¨Ø°Ù„Ùƒ Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ÙƒØ§Ù† Ø§Ù„Ø°ÙŠ Ø³ØªØ¯ÙØ¹ ÙÙŠÙ‡ Ù†Ù…ÙˆØ°Ø¬Ùƒ ÙˆØ¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ ÙÙŠ [`~transformers.PushToHubCallback`]:

```py
>>> from transformers.keras_callbacks import PushToHubCallback

>>> callback = PushToHubCallback(
...     output_dir="my_awesome_eli5_mlm_model"ØŒ
...     tokenizer=tokenizer,
... )
```

Ø£Ø®ÙŠØ±Ù‹Ø§ØŒ Ø£Ù†Øª Ù…Ø³ØªØ¹Ø¯ Ù„Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬Ùƒ! Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ ["fit"](https://keras.io/api/models/model_training_apis/#fit-method) Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ÙŠØ© ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­ØªÙ‡Ø§ØŒ ÙˆØ¹Ø¯Ø¯ Ø§Ù„Ø¹ØµÙˆØ±