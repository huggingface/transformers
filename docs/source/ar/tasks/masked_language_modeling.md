<!--Copyright 2022 The HuggingFace Team. All rights reserved.
Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.
-->

# Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…Ù‚Ù†Ø¹Ø© (Masked language modeling)

[[open-in-colab]]

<Youtube id="mqElG5QJWUg"/>

ØªØªÙ†Ø¨Ø£ Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…Ù‚Ù†Ø¹Ø© Ø¨Ø±Ù…Ø² Ù…Ù‚Ù†Ø¹ ÙÙŠ ØªØ³Ù„Ø³Ù„ØŒ ÙˆÙŠÙ…ÙƒÙ† Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ø±Ù…ÙˆØ² Ø¨Ø´ÙƒÙ„ Ø«Ù†Ø§Ø¦ÙŠ Ø§Ù„Ø§ØªØ¬Ø§Ù‡. Ù‡Ø°Ø§
ÙŠØ¹Ù†ÙŠ Ø£Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ø¯ÙŠÙ‡ Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ù„ÙˆØµÙˆÙ„ Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„ÙŠØ³Ø§Ø± ÙˆØ§Ù„ÙŠÙ…ÙŠÙ†. ØªØ¹Ø¯ Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…Ù‚Ù†Ø¹Ø© Ù…Ù…ØªØ§Ø²Ø© Ù„Ù„Ù…Ù‡Ø§Ù… Ø§Ù„ØªÙŠ
ØªØªØ·Ù„Ø¨ ÙÙ‡Ù…Ù‹Ø§ Ø³ÙŠØ§Ù‚ÙŠÙ‹Ø§ Ø¬ÙŠØ¯Ù‹Ø§ Ù„ØªØ³Ù„Ø³Ù„ ÙƒØ§Ù…Ù„. BERT Ù‡Ùˆ Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ù†Ù…ÙˆØ°Ø¬ Ù„ØºØ© Ù…Ù‚Ù†Ø¹.

Ø³ÙŠÙˆØ¶Ø­ Ù„Ùƒ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ ÙƒÙŠÙÙŠØ©:

1. ØªÙƒÙŠÙŠÙ [DistilRoBERTa](https://huggingface.co/distilbert/distilroberta-base) Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© ÙØ±Ø¹ÙŠØ© [r/askscience](https://www.reddit.com/r/askscience/) Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª [ELI5](https://huggingface.co/datasets/eli5).
2. Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„.

<Tip>

Ù„Ù…Ø¹Ø±ÙØ© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨Ù†Ù‰ ÙˆØ§Ù„Ù†Ø³Ø® Ø§Ù„Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù‡Ù…Ø©ØŒ Ù†ÙˆØµÙŠ Ø¨Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† [ØµÙØ­Ø© Ø§Ù„Ù…Ù‡Ù…Ø©](https://huggingface.co/tasks/fill-mask)

</Tip>

Ù‚Ø¨Ù„ Ø£Ù† ØªØ¨Ø¯Ø£ØŒ ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ©:

```bash
pip install transformers datasets evaluate
```

Ù†Ø­Ù† Ù†Ø´Ø¬Ø¹Ùƒ Ø¹Ù„Ù‰ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¥Ù„Ù‰ Ø­Ø³Ø§Ø¨ Hugging Face Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ø­ØªÙ‰ ØªØªÙ…ÙƒÙ† Ù…Ù† ØªØ­Ù…ÙŠÙ„ ÙˆÙ…Ø´Ø§Ø±ÙƒØ© Ù†Ù…ÙˆØ°Ø¬Ùƒ Ù…Ø¹ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹. Ø¹Ù†Ø¯Ù…Ø§ ØªØªÙ… Ù…Ø·Ø§Ù„Ø¨ØªÙƒØŒ Ø£Ø¯Ø®Ù„ Ø±Ù…Ø²Ùƒ Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„:

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ELI5

Ø§Ø¨Ø¯Ø£ Ø¨ØªØ­Ù…ÙŠÙ„ Ø£ÙˆÙ„ 5000 Ù…Ø«Ø§Ù„ Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª [ELI5-Category](https://huggingface.co/datasets/eli5_category) Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø© ğŸ¤— Datasets. Ø³ÙŠØ¹Ø·ÙŠÙƒ Ù‡Ø°Ø§ ÙØ±ØµØ© Ù„Ù„ØªØ¬Ø±Ø¨Ø© ÙˆØ§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† ÙƒÙ„ Ø´ÙŠØ¡ ÙŠØ¹Ù…Ù„ Ù‚Ø¨Ù„ Ù‚Ø¶Ø§Ø¡ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ÙˆÙ‚Øª ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø©.

```py
>>> from datasets import load_dataset

>>> eli5 = load_dataset("eli5_category", split="train[:5000]")
```

Ù‚Ù… Ø¨ØªÙ‚Ø³ÙŠÙ… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª `train` Ø¥Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹ØªÙŠ ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯Ø§Ù„Ø© [`~datasets.Dataset.train_test_split`]:

```py
>>> eli5 = eli5.train_test_split(test_size=0.2)
```

Ø«Ù… Ø£Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ù…Ø«Ø§Ù„:

```py
>>> eli5["train"][0]
{'q_id': '7h191n',
 'title': 'What does the tax bill that was passed today mean? How will it affect Americans in each tax bracket?',
 'selftext': '',
 'category': 'Economics',
 'subreddit': 'explainlikeimfive',
 'answers': {'a_id': ['dqnds8l', 'dqnd1jl', 'dqng3i1', 'dqnku5x'],
  'text': ["The tax bill is 500 pages long and there were a lot of changes still going on right to the end. It's not just an adjustment to the income tax brackets, it's a whole bunch of changes. As such there is no good answer to your question. The big take aways are: - Big reduction in corporate income tax rate will make large companies very happy. - Pass through rate change will make certain styles of business (law firms, hedge funds) extremely happy - Income tax changes are moderate, and are set to expire (though it's the kind of thing that might just always get re-applied without being made permanent) - People in high tax states (California, New York) lose out, and many of them will end up with their taxes raised.",
   'None yet. It has to be reconciled with a vastly different house bill and then passed again.',
   'Also: does this apply to 2017 taxes? Or does it start with 2018 taxes?',
   'This article explains both the House and senate bills, including the proposed changes to your income taxes based on your income level. URL_0'],
  'score': [21, 19, 5, 3],
  'text_urls': [[],
   [],
   [],
   ['https://www.investopedia.com/news/trumps-tax-reform-what-can-be-done/']]},
 'title_urls': ['url'],
 'selftext_urls': ['url']}
```

Ø¹Ù„Ù‰ Ø§Ù„Ø±ØºÙ… Ù…Ù† Ø£Ù† Ù‡Ø°Ø§ Ù‚Ø¯ ÙŠØ¨Ø¯Ùˆ ÙƒØ«ÙŠØ±Ù‹Ø§ØŒ Ø¥Ù„Ø§ Ø£Ù†Ùƒ Ù…Ù‡ØªÙ… Ø­Ù‚Ù‹Ø§ Ø¨Ø­Ù‚Ù„ `text`. Ù…Ø§ Ù‡Ùˆ Ø±Ø§Ø¦Ø¹ Ø­ÙˆÙ„ Ù…Ù‡Ø§Ù… Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºØ© Ù‡Ùˆ Ø£Ù†Ùƒ Ù„Ø§ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ³Ù…ÙŠØ§Øª (ØªÙØ¹Ø±Ù Ø£ÙŠØ¶Ù‹Ø§ Ø¨Ø§Ø³Ù… Ø§Ù„Ù…Ù‡Ù…Ø© ØºÙŠØ± Ø§Ù„Ø®Ø§Ø¶Ø¹Ø© Ù„Ù„Ø¥Ø´Ø±Ø§Ù) Ù„Ø£Ù† Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© *Ù‡ÙŠ* Ø§Ù„ØªØ³Ù…ÙŠØ©.

## Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø³Ø¨Ù‚Ø© (Preprocess)

<Youtube id="8PmhEIXhBvI"/>

Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…Ù‚Ù†Ø¹Ø©ØŒ ÙØ¥Ù† Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù‡ÙŠ ØªØ­Ù…ÙŠÙ„ Ù…Ø¹Ø§Ù„Ø¬ DistilRoBERTa Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø­Ù‚Ù„ `text` Ø§Ù„ÙØ±Ø¹ÙŠ:

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilroberta-base")
```

Ø³ØªÙ„Ø§Ø­Ø¸ Ù…Ù† Ø§Ù„Ù…Ø«Ø§Ù„ Ø£Ø¹Ù„Ø§Ù‡ØŒ Ø£Ù† Ø­Ù‚Ù„ `text` Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ù„ÙØ¹Ù„ Ø¯Ø§Ø®Ù„ `answers`. Ù‡Ø°Ø§ ÙŠØ¹Ù†ÙŠ Ø£Ù†Ùƒ Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø­Ù‚Ù„ `text` Ø§Ù„ÙØ±Ø¹ÙŠ Ù…Ù† Ø¨Ù†ÙŠØªÙ‡ Ø§Ù„Ù…Ø¶Ù…Ù†Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯Ø§Ù„Ø© [`flatten`](https://huggingface.co/docs/datasets/process#flatten):

```py
>>> eli5 = eli5.flatten()
>>> eli5["train"][0]
{'q_id': '7h191n',
 'title': 'What does the tax bill that was passed today mean? How will it affect Americans in each tax bracket?',
 'selftext': '',
 'category': 'Economics',
 'subreddit': 'explainlikeimfive',
 'answers.a_id': ['dqnds8l', 'dqnd1jl', 'dqng3i1', 'dqnku5x'],
 'answers.text': ["The tax bill is 500 pages long and there were a lot of changes still going on right to the end. It's not just an adjustment to the income tax brackets, it's a whole bunch of changes. As such there is no good answer to your question. The big take aways are: - Big reduction in corporate income tax rate will make large companies very happy. - Pass through rate change will make certain styles of business (law firms, hedge funds) extremely happy - Income tax changes are moderate, and are set to expire (though it's the kind of thing that might just always get re-applied without being made permanent) - People in high tax states (California, New York) lose out, and many of them will end up with their taxes raised.",
  'None yet. It has to be reconciled with a vastly different house bill and then passed again.',
  'Also: does this apply to 2017 taxes? Or does it start with 2018 taxes?',
  'This article explains both the House and senate bills, including the proposed changes to your income taxes based on your income level. URL_0'],
 'answers.score': [21, 19, 5, 3],
 'answers.text_urls': [[],
  [],
  [],
  ['https://www.investopedia.com/news/trumps-tax-reform-what-can-be-done/']],
 'title_urls': ['url'],
 'selftext_urls': ['url']}
```

ÙƒÙ„ Ø­Ù‚Ù„ ÙØ±Ø¹ÙŠ Ù‡Ùˆ Ø§Ù„Ø¢Ù† Ø¹Ù…ÙˆØ¯ Ù…Ù†ÙØµÙ„ ÙƒÙ…Ø§ Ù‡Ùˆ Ù…ÙˆØ¶Ø­ Ø¨ÙˆØ§Ø³Ø·Ø© Ø¨Ø§Ø¯Ø¦Ø© `answers`ØŒ ÙˆØ­Ù‚Ù„ `text` Ù‡Ùˆ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¢Ù†. Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù†
Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ Ø¬Ù…Ù„Ø© Ø¨Ø´ÙƒÙ„ Ù…Ù†ÙØµÙ„ØŒ Ù‚Ù… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¥Ù„Ù‰ Ø³Ù„Ø³Ù„Ø© Ø­ØªÙ‰ ØªØªÙ…ÙƒÙ† Ù…Ù† Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§ Ø¨Ø´ÙƒÙ„ Ù…Ø´ØªØ±Ùƒ.

Ù‡Ù†Ø§ Ø£ÙˆÙ„ Ø¯Ø§Ù„Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø³Ø¨Ù‚Ø© Ù„Ø±Ø¨Ø· Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø³Ù„Ø§Ø³Ù„ Ù„ÙƒÙ„ Ù…Ø«Ø§Ù„ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØªÙŠØ¬Ø©:

```py
>>> def preprocess_function(examples):
...     return tokenizer([" ".join(x) for x in examples["answers.text"]])
```

Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¯Ø§Ù„Ø© ğŸ¤— Datasets [`~datasets.Dataset.map`]. ÙŠÙ…ÙƒÙ†Ùƒ ØªØ³Ø±ÙŠØ¹ Ø¯Ø§Ù„Ø© `map` Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ¹ÙŠÙŠÙ† `batched=True` Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ø¯Ø© Ø¹Ù†Ø§ØµØ± ÙÙŠ ÙˆÙ‚Øª ÙˆØ§Ø­Ø¯ØŒ ÙˆØ²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `num_proc`. Ø§Ø­Ø°Ù Ø£ÙŠ Ø£Ø¹Ù…Ø¯Ø© ØºÙŠØ± Ø¶Ø±ÙˆØ±ÙŠØ©:

```py
>>> tokenized_eli5 = eli5.map(
...     preprocess_function,
...     batched=True,
...     num_proc=4,
...     remove_columns=eli5["train"].column_names,
... )
```


ØªØ­ØªÙˆÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‡Ø°Ù‡ Ø¹Ù„Ù‰ ØªØ³Ù„Ø³Ù„Ø§Øª Ø±Ù…Ø²ÙŠØ©ØŒ ÙˆÙ„ÙƒÙ† Ø¨Ø¹Ø¶Ù‡Ø§ Ø£Ø·ÙˆÙ„ Ù…Ù† Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ù„Ù„Ù†Ù…ÙˆØ°Ø¬.

ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¢Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¯Ø§Ù„Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø³Ø¨Ù‚Ø© Ø«Ø§Ù†ÙŠØ© Ù„Ù€:
- ØªØ¬Ù…ÙŠØ¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ³Ù„Ø³Ù„Ø§Øª
- ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØªØ³Ù„Ø³Ù„Ø§Øª Ø§Ù„Ù…Ø¬Ù…Ù‘Ø¹Ø© Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ Ø£Ù‚ØµØ± Ù…Ø­Ø¯Ø¯Ø© Ø¨Ù€ `block_size`ØŒ ÙˆØ§Ù„ØªÙŠ ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø£Ù‚ØµØ± Ù…Ù† Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø·ÙˆÙ„ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª ÙˆÙ…Ù†Ø§Ø³Ø¨Ø© Ù„Ø°Ø§ÙƒØ±Ø© GPU.

```py
>>> block_size = 128

>>> def group_texts(examples):
...     # ØªØ¬Ù…ÙŠØ¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØµÙˆØµ.
...     concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
...     total_length = len(concatenated_examples[list(examples.keys())[0]])
...     # Ù†ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…ØªØ¨Ù‚ÙŠ Ø§Ù„ØµØºÙŠØ±ØŒ ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø­Ø´Ùˆ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠØ¯Ø¹Ù…Ù‡ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¥Ø³Ù‚Ø§Ø·ØŒ ÙŠÙ…ÙƒÙ†Ùƒ
...     # ØªØ®ØµÙŠØµ Ù‡Ø°Ø§ Ø§Ù„Ø¬Ø²Ø¡ Ø­Ø³Ø¨ Ø§Ø­ØªÙŠØ§Ø¬Ø§ØªÙƒ.
...     if total_length >= block_size:
...         total_length = (total_length // block_size) * block_size
...     # ØªÙ‚Ø³ÙŠÙ…Ù‡Ø§ Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ Ø¨Ø­Ø¬Ù… block_size.
...     result = {
...         k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
...         for k, t in concatenated_examples.items()
...     }
...     return result
```

Ø·Ø¨Ù‚ Ø¯Ø§Ù„Ø© `group_texts` Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§:

```py
>>> lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)
```

Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø¯ÙØ¹Ø© Ù…Ù† Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`DataCollatorForLanguageModeling`]. Ù…Ù† Ø§Ù„Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© Ø£Ù† ØªÙ‚ÙˆÙ… Ø¨Ù€ *Ø§Ù„Ø­Ø´Ùˆ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ* Ù„Ù„Ø¬Ù…Ù„ Ø¥Ù„Ù‰ Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ø£Ø·ÙˆÙ„ ÙÙŠ Ø§Ù„Ø¯ÙØ¹Ø© Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¬Ù…ÙŠØ¹ØŒ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø­Ø´Ùˆ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§ Ø¥Ù„Ù‰ Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ø£Ù‚ØµÙ‰.

<frameworkcontent>
<pt>

Ø§Ø³ØªØ®Ø¯Ù… Ø±Ù…Ø² Ù†Ù‡Ø§ÙŠØ© Ø§Ù„ØªØ³Ù„Ø³Ù„ ÙƒØ±Ù…Ø² Ø§Ù„Ø­Ø´Ùˆ ÙˆØ­Ø¯Ø¯ `mlm_probability` Ù„Ø­Ø¬Ø¨ Ø§Ù„Ø±Ù…ÙˆØ² Ø¹Ø´ÙˆØ§Ø¦ÙŠØ§Ù‹ ÙƒÙ„ Ù…Ø±Ø© ØªÙƒØ±Ø± ÙÙŠÙ‡Ø§ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:

```py
>>> from transformers import DataCollatorForLanguageModeling

>>> tokenizer.pad_token = tokenizer.eos_token
>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)
```
</pt>
<tf>

Ø§Ø³ØªØ®Ø¯Ù… Ø±Ù…Ø² Ù†Ù‡Ø§ÙŠØ© Ø§Ù„ØªØ³Ù„Ø³Ù„ ÙƒØ±Ù…Ø² Ø§Ù„Ø­Ø´Ùˆ ÙˆØ­Ø¯Ø¯ `mlm_probability` Ù„Ø­Ø¬Ø¨ Ø§Ù„Ø±Ù…ÙˆØ² Ø¹Ø´ÙˆØ§Ø¦ÙŠØ§Ù‹ ÙƒÙ„ Ù…Ø±Ø© ØªÙƒØ±Ø± ÙÙŠÙ‡Ø§ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:

```py
>>> from transformers import DataCollatorForLanguageModeling

>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors="tf")
```
</tf>
</frameworkcontent>

## Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (Train)

<frameworkcontent>
<pt>

<Tip>

Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ø¹Ù„Ù‰ Ø¯Ø±Ø§ÙŠØ© Ø¨ØªØ¹Ø¯ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`Trainer`], Ø£Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ [Ù‡Ù†Ø§](../training#train-with-pytorch-trainer)!

</Tip>

Ø£Ù†Øª Ù…Ø³ØªØ¹Ø¯ Ø§Ù„Ø¢Ù† Ù„Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬Ùƒ! Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ DistilRoBERTa Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`AutoModelForMaskedLM`]:

```py
>>> from transformers import AutoModelForMaskedLM

>>> model = AutoModelForMaskedLM.from_pretrained("distilbert/distilroberta-base")
```

ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø±Ø­Ù„Ø©ØŒ ØªØ¨Ù‚Ù‰ Ø«Ù„Ø§Ø« Ø®Ø·ÙˆØ§Øª ÙÙ‚Ø·:

1. Ø­Ø¯Ø¯ Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ ÙÙŠ [`TrainingArguments`]. Ø§Ù„Ù…Ø¹Ù„Ù…Ø© Ø§Ù„ÙˆØ­ÙŠØ¯Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù‡ÙŠ `output_dir` ÙˆØ§Ù„ØªÙŠ ØªØ­Ø¯Ø¯ Ù…ÙƒØ§Ù† Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬Ùƒ. Ø³ØªÙ‚ÙˆÙ… Ø¨Ø¯ÙØ¹ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ Hub Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ¹ÙŠÙŠÙ† `push_to_hub=True` (ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ù…Ø³Ø¬Ù„Ø§Ù‹ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¥Ù„Ù‰ Hugging Face Ù„ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ).
2. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¥Ù„Ù‰ [`Trainer`] Ù…Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙˆÙ…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙˆÙ…Ø¬Ù…Ù‘Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.
3. Ù‚Ù… Ø¨Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ [`~Trainer.train`] Ù„ØªØ¹Ø¯ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ.

```py
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_eli5_mlm_model",
...     eval_strategy="epoch",
...     learning_rate=2e-5,
...     num_train_epochs=3,
...     weight_decay=0.01,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=lm_dataset["train"],
...     eval_dataset=lm_dataset["test"],
...     data_collator=data_collator,
...     tokenizer=tokenizer,
... )

>>> trainer.train()
```

Ø¨Ù…Ø¬Ø±Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø·Ø±ÙŠÙ‚Ø© [`~transformers.Trainer.evaluate`] Ù„ØªÙ‚ÙŠÙŠÙ… Ù†Ù…ÙˆØ°Ø¬Ùƒ ÙˆØ§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­ÙŠØ±ØªÙ‡:

```py
>>> import math

>>> eval_results = trainer.evaluate()
>>> print(f"Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
Perplexity: 8.76
```

Ø«Ù… Ø´Ø§Ø±Ùƒ Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø¹Ù„Ù‰ Hub Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© [`~transformers.Trainer.push_to_hub`] Ø­ØªÙ‰ ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ø¬Ù…ÙŠØ¹ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬Ùƒ:

```py
>>> trainer.push_to_hub()
```
</pt>
<tf>
<Tip>

Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ø¹Ù„Ù‰ Ø¯Ø±Ø§ÙŠØ© Ø¨ØªØ¹Ø¯ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… KerasØŒ Ø£Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ [Ù‡Ù†Ø§](../training#train-a-tensorflow-model-with-keras)!

</Tip>
Ù„ØªØ¹Ø¯ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ TensorFlowØŒ Ø§Ø¨Ø¯Ø£ Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ Ø¯Ø§Ù„Ø© Ù…Ø­Ø³Ù†ØŒ ÙˆØ¬Ø¯ÙˆÙ„ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…ØŒ ÙˆØ¨Ø¹Ø¶ Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨:

```py
>>> from transformers import create_optimizer, AdamWeightDecay

>>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)
```

Ø«Ù… ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ DistilRoBERTa Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`TFAutoModelForMaskedLM`]:

```py
>>> from transformers import TFAutoModelForMaskedLM

>>> model = TFAutoModelForMaskedLM.from_pretrained("distilbert/distilroberta-base")
```

Ù‚Ù… Ø¨ØªØ­ÙˆÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§ØªÙƒ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ `tf.data.Dataset` Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:

```py
>>> tf_train_set = model.prepare_tf_dataset(
...     lm_dataset["train"],
...     shuffle=True,
...     batch_size=16,
...     collate_fn=data_collator,
... )

>>> tf_test_set = model.prepare_tf_dataset(
...     lm_dataset["test"],
...     shuffle=False,
...     batch_size=16,
...     collate_fn=data_collator,
... )
```

Ù‚Ù… Ø¨ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`compile`](https://keras.io/api/models/model_training_apis/#compile-method). Ù„Ø§Ø­Ø¸ Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ Transformers Ù„Ø¯ÙŠÙ‡Ø§ Ø¬Ù…ÙŠØ¹Ù‡Ø§ Ø¯Ø§Ù„Ø© Ø®Ø³Ø§Ø±Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø°Ø§Øª ØµÙ„Ø© Ø¨Ø§Ù„Ù…Ù‡Ù…Ø©ØŒ Ù„Ø°Ù„Ùƒ Ù„Ø§ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ ÙˆØ§Ø­Ø¯Ø© Ù…Ø§ Ù„Ù… ØªÙƒÙ† ØªØ±ÙŠØ¯ Ø°Ù„Ùƒ:

```py
>>> import tensorflow as tf

>>> model.compile(optimizer=optimizer)  # Ù„Ø§ ØªÙˆØ¬Ø¯ Ø­Ø¬Ø© Ù„Ù„Ø®Ø³Ø§Ø±Ø©!
```

ÙŠÙ…ÙƒÙ† Ø§Ù„Ù‚ÙŠØ§Ù… Ø¨Ø°Ù„Ùƒ Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ­Ø¯ÙŠØ¯ Ù…ÙƒØ§Ù† Ø¯ÙØ¹ Ù†Ù…ÙˆØ°Ø¬Ùƒ ÙˆÙ…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø±Ù…ÙˆØ² ÙÙŠ [`~transformers.PushToHubCallback`]:

```py
>>> from transformers.keras_callbacks import PushToHubCallback

>>> callback = PushToHubCallback(
...     output_dir="my_awesome_eli5_mlm_model",
...     tokenizer=tokenizer,
... )
```

Ø£Ø®ÙŠØ±Ø§Ù‹ØŒ Ø£Ù†Øª Ù…Ø³ØªØ¹Ø¯ Ù„Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬Ùƒ! Ù‚Ù… Ø¨Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ØŒ ÙˆØ¹Ø¯Ø¯ Ø§Ù„Ø¹ØµÙˆØ±ØŒ ÙˆØ§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:

```py
>>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=[callback])
```

Ø¨Ù…Ø¬Ø±Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¥Ù„Ù‰ Hub Ø­ØªÙ‰ ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ø¬Ù…ÙŠØ¹ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡!
</tf>
</frameworkcontent>

<Tip>

Ù„Ù…Ø«Ø§Ù„ Ø£ÙƒØ«Ø± ØªÙØµÙŠÙ„Ø§Ù‹ Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© ØªØ¹Ø¯ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºÙˆÙŠØ© Ø§Ù„Ù…Ù‚Ù†Ø¹Ø©ØŒ Ø£Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¯ÙØªØ± Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„
[Ø¯ÙØªØ± PyTorch](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)
Ø£Ùˆ [Ø¯ÙØªØ± TensorFlow](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb).

</Tip>

## Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„

Ø±Ø§Ø¦Ø¹ØŒ Ø§Ù„Ø¢Ù† Ø¨Ø¹Ø¯ Ø£Ù† Ù‚Ù…Øª Ø¨ØªØ¹Ø¯ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„!

ÙÙƒØ± ÙÙŠ Ø¨Ø¹Ø¶ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£Ù† ÙŠÙ…Ù„Ø£ Ø§Ù„ÙØ±Ø§Øº Ø¨Ù‡Ø§ØŒ ÙˆØ§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø±Ù…Ø² Ø§Ù„Ø®Ø§Øµ `<mask>` Ù„Ù„Ø¥Ø´Ø§Ø±Ø© Ø¥Ù„Ù‰ Ø§Ù„ÙØ±Ø§Øº:

```py
>>> text = "The Milky Way is a <mask> galaxy."
```

Ø£Ø¨Ø³Ø· Ø·Ø±ÙŠÙ‚Ø© Ù„ØªØ¬Ø±Ø¨Ø© Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø§Ù„Ù…Ø¹Ø¯Ù„ Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ù‡ÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙÙŠ [`pipeline`]. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Ù„Ù€ `pipeline` Ù„Ù…Ù„Ø¡ Ø§Ù„ÙØ±Ø§Øº Ù…Ø¹ Ù†Ù…ÙˆØ°Ø¬ÙƒØŒ ÙˆÙ…Ø±Ø± Ù†ØµÙƒ Ø¥Ù„ÙŠÙ‡. Ø¥Ø°Ø§ Ø£Ø±Ø¯ØªØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ù„Ù…Ø© `top_k` Ù„ØªØ­Ø¯ÙŠØ¯ Ø¹Ø¯Ø¯ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ Ø¥Ø±Ø¬Ø§Ø¹Ù‡Ø§:

```py
>>> from transformers import pipeline

>>> mask_filler = pipeline("fill-mask", "username/my_awesome_eli5_mlm_model")
>>> mask_filler(text, top_k=3)
[{'score': 0.5150994658470154,
  'token': 21300,
  'token_str': ' spiral',
  'sequence': 'The Milky Way is a spiral galaxy.'},
 {'score': 0.07087188959121704,
  'token': 2232,
  'token_str': ' massive',
  'sequence': 'The Milky Way is a massive galaxy.'},
 {'score': 0.06434620916843414,
  'token': 650,
  'token_str': ' small',
  'sequence': 'The Milky Way is a small galaxy.'}]
```

<frameworkcontent>
<pt>
Ù‚Ù… Ø¨ØªØ¬Ø²Ø¦Ø© Ø§Ù„Ù†Øµ ÙˆØ¥Ø±Ø¬Ø§Ø¹ `input_ids` ÙƒÙ…ØªØ¬Ù‡Ø§Øª PyTorch. Ø³ØªØ­ØªØ§Ø¬ Ø£ÙŠØ¶Ù‹Ø§ Ø¥Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ Ù…ÙˆØ¶Ø¹ Ø±Ù…Ø² `<mask>`:

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("username/my_awesome_eli5_mlm_model")
>>> inputs = tokenizer(text, return_tensors="pt")
>>> mask_token_index = torch.where(inputs["input_ids"] == tokenizer.mask_token_id)[1]
```

Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ¥Ø±Ø¬Ø§Ø¹ `logits` Ù„Ù„Ø±Ù…Ø² Ø§Ù„Ù…Ù‚Ù†Ø¹:

```py
>>> from transformers import AutoModelForMaskedLM

>>> model = AutoModelForMaskedLM.from_pretrained("username/my_awesome_eli5_mlm_model")
>>> logits = model(**inputs).logits
>>> mask_token_logits = logits[0, mask_token_index, :]
```

Ø«Ù… Ù‚Ù… Ø¨Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ù…Ù‚Ù†Ø¹Ø© Ø°Ø§Øª Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„Ø£Ø¹Ù„Ù‰ ÙˆØ·Ø¨Ø§Ø¹ØªÙ‡Ø§:

```py
>>> top_3_tokens = torch.topk(mask_token_logits, 3, dim=1).indices[0].tolist()

>>> for token in top_3_tokens:
...     print(text.replace(tokenizer.mask_token, tokenizer.decode([token])))
The Milky Way is a spiral galaxy.
The Milky Way is a massive galaxy.
The Milky Way is a small galaxy.
```
</pt>
<tf>
Ù‚Ù… Ø¨ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø±Ù…ÙˆØ² ÙˆØ¥Ø±Ø¬Ø§Ø¹ `input_ids` ÙƒÙ€ TensorFlow tensors. Ø³ØªØ­ØªØ§Ø¬ Ø£ÙŠØ¶Ù‹Ø§ Ø¥Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ Ù…ÙˆØ¶Ø¹ Ø±Ù…Ø² `<mask>`:

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("username/my_awesome_eli5_mlm_model")
>>> inputs = tokenizer(text, return_tensors="tf")
>>> mask_token_index = tf.where(inputs["input_ids"] == tokenizer.mask_token_id)[0, 1]
```

Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ¥Ø±Ø¬Ø§Ø¹ `logits` Ù„Ù„Ø±Ù…Ø² Ø§Ù„Ù…Ù‚Ù†Ø¹:

```py
>>> from transformers import TFAutoModelForMaskedLM

>>> model = TFAutoModelForMaskedLM.from_pretrained("username/my_awesome_eli5_mlm_model")
>>> logits = model(**inputs).logits
>>> mask_token_logits = logits[0, mask_token_index, :]
```

Ø«Ù… Ù‚Ù… Ø¨Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ù…Ù‚Ù†Ø¹Ø© Ø°Ø§Øª Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„Ø£Ø¹Ù„Ù‰ ÙˆØ·Ø¨Ø§Ø¹ØªÙ‡Ø§:

```py
>>> top_3_tokens = tf.math.top_k(mask_token_logits, 3).indices.numpy()

>>> for token in top_3_tokens:
...     print(text.replace(tokenizer.mask_token, tokenizer.decode([token])))
The Milky Way is a spiral galaxy.
The Milky Way is a massive galaxy.
The Milky Way is a small galaxy.
```
</tf>
</frameworkcontent>