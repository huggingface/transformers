# Ø§Ù„ØªØ±Ø¬Ù…Ø©

[[open-in-colab]]

<Youtube id="1JvfrvZgi6c"/>

ØªØ¹Ø¯ Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø¥Ø­Ø¯Ù‰ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ†Ùƒ ØµÙŠØ§ØºØªÙ‡Ø§ ÙƒÙ…Ø´ÙƒÙ„Ø© ØªØ³Ù„Ø³Ù„ Ø¥Ù„Ù‰ ØªØ³Ù„Ø³Ù„ØŒ ÙˆÙ‡Ùˆ Ø¥Ø·Ø§Ø± Ø¹Ù…Ù„ Ù‚ÙˆÙŠ Ù„Ø¥Ø±Ø¬Ø§Ø¹ Ø¨Ø¹Ø¶ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª Ù…Ù† Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ØŒ Ù…Ø«Ù„ Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø£Ùˆ Ø§Ù„Ù…Ù„Ø®Øµ. ÙˆØªÙØ³ØªØ®Ø¯Ù… Ø£Ù†Ø¸Ù…Ø© Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø¹Ø§Ø¯Ø© Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†ØµÙˆØµ Ø¨ÙŠÙ† Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©ØŒ ÙˆÙ„ÙƒÙ† ÙŠÙ…ÙƒÙ† Ø£ÙŠØ¶Ù‹Ø§ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ù„Ù„ÙƒÙ„Ø§Ù… Ø£Ùˆ Ø¨Ø¹Ø¶ Ø§Ù„Ù…Ø²Ø¬ Ø¨ÙŠÙ†Ù‡Ù…Ø§ Ù…Ø«Ù„ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø£Ùˆ Ø§Ù„ÙƒÙ„Ø§Ù… Ø¥Ù„Ù‰ Ù†Øµ.

Ø³ÙŠÙˆØ¶Ø­ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ ÙƒÙŠÙÙŠØ©:

1. Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ T5 Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙØ±Ø¹ÙŠ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ Ø§Ù„ÙØ±Ù†Ø³ÙŠ Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª OPUS Books Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†Øµ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ Ø¥Ù„Ù‰ Ø§Ù„ÙØ±Ù†Ø³ÙŠØ©.
2. Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ù„Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬.

<Tip>

Ù„Ø¹Ø±Ø¶ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨Ù†ÙŠØ§Øª ÙˆÙ†Ù‚Ø§Ø· Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù‡Ù…Ø©ØŒ Ù†ÙˆØµÙŠ Ø¨Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† [ØµÙØ­Ø© Ø§Ù„Ù…Ù‡Ù…Ø©](https://huggingface.co/tasks/translation).

</Tip>

Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ø¯Ø¡ØŒ ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ©:

```bash
pip install transformers datasets evaluate sacrebleu
```

Ù†Ø´Ø¬Ø¹Ùƒ Ø¹Ù„Ù‰ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¥Ù„Ù‰ Ø­Ø³Ø§Ø¨ Hugging Face Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ø­ØªÙ‰ ØªØªÙ…ÙƒÙ† Ù…Ù† ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ ÙˆÙ…Ø´Ø§Ø±ÙƒØªÙ‡ Ù…Ø¹ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹. Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙØ·Ù„Ø¨ Ù…Ù†Ùƒ Ø°Ù„ÙƒØŒ Ø£Ø¯Ø®Ù„ Ø±Ù…Ø²Ùƒ Ù„Ù„ØªØ³Ø¬ÙŠÙ„:

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª OPUS Books

Ø§Ø¨Ø¯Ø£ Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙØ±Ø¹ÙŠ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ Ø§Ù„ÙØ±Ù†Ø³ÙŠ Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª [OPUS Books](https://huggingface.co/datasets/opus_books) Ù…Ù† Ù…ÙƒØªØ¨Ø© Datasets ğŸ¤—:

```py
>>> from datasets import load_dataset

>>> books = load_dataset("opus_books", "en-fr")
```

Ù‚Ø³ÙÙ‘Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© [`~datasets.Dataset.train_test_split`]:

```py
>>> books = books["train"].train_test_split(test_size=0.2)
```

Ø«Ù… Ø§Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ù…Ø«Ø§Ù„:

```py
>>> books["train"][0]
{'id': '90560',
 'translation': {'en': 'But this lofty plateau measured only a few fathoms, and soon we reentered Our Element.',
  'fr': 'Mais ce plateau Ã©levÃ© ne mesurait que quelques toises, et bientÃ´t nous fÃ»mes rentrÃ©s dans notre Ã©lÃ©ment.'}}
```

`translation`: ØªØ±Ø¬Ù…Ø© Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© ÙˆÙØ±Ù†Ø³ÙŠØ© Ù„Ù„Ù†Øµ.

## Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø³Ø¨Ù‚Ø©

<Youtube id="XAR8jnZZuUs"/>

Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù‡ÙŠ ØªØ­Ù…ÙŠÙ„ Ø¨Ø±Ù†Ø§Ù…Ø¬ ØªØ´ÙÙŠØ± T5 Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø²ÙˆØ§Ø¬ Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© ÙˆØ§Ù„ÙØ±Ù†Ø³ÙŠØ©:

```py
>>> from transformers import AutoTokenizer

>>> checkpoint = "google-t5/t5-small"
>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

ÙŠØ¬Ø¨ Ø£Ù† ØªÙ‚ÙˆÙ… Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ Ø¥Ù†Ø´Ø§Ø¦Ù‡Ø§ Ø¨Ù…Ø§ ÙŠÙ„ÙŠ:

1. Ø¥Ø¶Ø§ÙØ© Ø¨Ø§Ø¯Ø¦Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙˆØ¬Ù‡ Ø­ØªÙ‰ ÙŠØ¹Ø±Ù T5 Ø£Ù† Ù‡Ø°Ù‡ Ù…Ù‡Ù…Ø© ØªØ±Ø¬Ù…Ø©. ØªØªØ·Ù„Ø¨ Ø¨Ø¹Ø¶ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù‚Ø§Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ù…Ù‡Ø§Ù… NLP Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø© Ø¨Ù…Ù‡Ø§Ù… Ù…Ø­Ø¯Ø¯Ø©.
2. Ù‚Ù… Ø¨ØªØ´ÙÙŠØ± Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ (Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©) ÙˆØ§Ù„Ù‡Ø¯Ù (Ø§Ù„ÙØ±Ù†Ø³ÙŠØ©) Ø¨Ø´ÙƒÙ„ Ù…Ù†ÙØµÙ„ Ù„Ø£Ù†Ù‡ Ù„Ø§ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ÙÙŠØ± Ø§Ù„Ù†Øµ Ø§Ù„ÙØ±Ù†Ø³ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ø±Ù†Ø§Ù…Ø¬ ØªØ´ÙÙŠØ± ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ø¹Ù„Ù‰ Ù…ÙØ±Ø¯Ø§Øª Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©.
3. Ø§Ù‚Ø·Ø¹ Ø§Ù„ØªØ³Ù„Ø³Ù„Ø§Øª Ø¨Ø­ÙŠØ« Ù„Ø§ ØªÙƒÙˆÙ† Ø£Ø·ÙˆÙ„ Ù…Ù† Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ø£Ù‚ØµÙ‰ Ø§Ù„Ù…Ø­Ø¯Ø¯ Ø¨ÙˆØ§Ø³Ø·Ø© Ù…Ø¹Ù„Ù…Ø© "max_length".

```py
>>> source_lang = "en"
>>> target_lang = "fr"
>>> prefix = "translate English to French: "
```py
>>> source_lang = "en"
>>> target_lang = "fr"
>>> prefix = "translate English to French: "


>>> def preprocess_function(examples):
...     inputs = [prefix + example[source_lang] for example in examples["translation"]]
...     targets = [example[target_lang] for example in examples["translation"]]
...     model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)
...     return model_inputs
```

Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø·Ø±ÙŠÙ‚Ø© [`~datasets.Dataset.map`] ÙÙŠ Ù…ÙƒØªØ¨Ø© Datasets ğŸ¤—. ÙŠÙ…ÙƒÙ†Ùƒ ØªØ³Ø±ÙŠØ¹ ÙˆØ¸ÙŠÙØ© "map" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ¹ÙŠÙŠÙ† "batched=True" Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ù†Ø§ØµØ± Ù…ØªØ¹Ø¯Ø¯Ø© Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ ÙˆÙ‚Øª ÙˆØ§Ø­Ø¯:

```py
>>> tokenized_books = books.map(preprocess_function, batched=True)
```

Ø§Ù„Ø¢Ù† Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø¯ÙØ¹Ø© Ù…Ù† Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`DataCollatorForSeq2Seq`]. Ù…Ù† Ø§Ù„Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© *ØªØ¹Ø¨Ø¦Ø©* Ø§Ù„Ø¬Ù…Ù„ Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠÙ‹Ø§ Ø¥Ù„Ù‰ Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ø£Ø·ÙˆÙ„ ÙÙŠ Ø¯ÙØ¹Ø© Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¬Ù…ÙŠØ¹ØŒ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† ØªØ¹Ø¨Ø¦Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§ Ø¥Ù„Ù‰ Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ø£Ù‚ØµÙ‰.

<frameworkcontent>
<pt>
  
```py
>>> from transformers import DataCollatorForSeq2Seq

>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)
```
</pt>
<tf>

```py
>>> from transformers import DataCollatorForSeq2Seq

>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors="tf")
```
</tf>
</frameworkcontent>

## ØªÙ‚ÙŠÙŠÙ…

ØºØ§Ù„Ø¨Ù‹Ø§ Ù…Ø§ ÙŠÙƒÙˆÙ† ØªØ¶Ù…ÙŠÙ† Ù…Ù‚ÙŠØ§Ø³ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…ÙÙŠØ¯Ù‹Ø§ Ù„ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬Ùƒ. ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ Ø·Ø±ÙŠÙ‚Ø© ØªÙ‚ÙŠÙŠÙ… Ø¨Ø³Ø±Ø¹Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø© ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index). Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù‡Ù…Ø©ØŒ Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù…Ù‚ÙŠØ§Ø³ [SacreBLEU](https://huggingface.co/spaces/evaluate-metric/sacrebleu) (Ø±Ø§Ø¬Ø¹ Ø¯Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø®Ø§Øµ Ø¨Ù€ ğŸ¤— Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© ØªØ­Ù…ÙŠÙ„ ÙˆØ­Ø³Ø§Ø¨ Ù…Ù‚ÙŠØ§Ø³):

```py
>>> import evaluate

>>> metric = evaluate.load("sacrebleu")
```

Ø«Ù… Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø¯Ø§Ù„Ø© ØªÙ…Ø±Ø± ØªÙ†Ø¨Ø¤Ø§ØªÙƒ ÙˆØªØµÙ†ÙŠÙØ§ØªÙƒ Ø¥Ù„Ù‰ [`~evaluate.EvaluationModule.compute`] Ù„Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© SacreBLEU:

```py
>>> import numpy as np


>>> def postprocess_text(predsØŒ labels):
...     preds = [pred.strip() for pred in preds]
...     labels = [[label.strip()] for label in labels]

...     return preds, labels


>>> def compute_metrics(eval_preds):
...     preds, labels = eval_preds
...     if isinstance(preds, tuple):
...         preds = preds[0]
...     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

...     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
...     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

...     decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

...     result = metric.compute(predictions=decoded_preds, references=decoded_labels)
...     result = {"bleu": result["score"]}

...     prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
...     result["gen_len"] = np.mean(prediction_lens)
...     result = {k: round(v, 4) for k, v in result.items()}
...     return result
```

Ø¯Ø§Ù„ØªÙƒ `compute_metrics` Ø¬Ø§Ù‡Ø²Ø© Ø§Ù„Ø¢Ù†ØŒ ÙˆØ³ØªØ¹ÙˆØ¯ Ø¥Ù„ÙŠÙ‡Ø§ Ø¹Ù†Ø¯ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨.

## ØªØ¯Ø±ÙŠØ¨

<frameworkcontent>
<pt>
<Tip>

Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ø¹Ù„Ù‰ Ø¯Ø±Ø§ÙŠØ© Ø¨Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`Trainer`ØŒ ÙØ±Ø§Ø¬Ø¹ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ [here](../training#train-with-pytorch-trainer)!

</Tip>

Ø£Ù†Øª Ù…Ø³ØªØ¹Ø¯ Ø§Ù„Ø¢Ù† Ù„Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬Ùƒ! Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ T5 Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`AutoModelForSeq2SeqLM`]:

```py
>>> from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer

>>> model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)
```

ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø±Ø­Ù„Ø©ØŒ Ù‡Ù†Ø§Ùƒ Ø«Ù„Ø§Ø« Ø®Ø·ÙˆØ§Øª ÙÙ‚Ø·:

1. Ø­Ø¯Ø¯ ÙØ±Ø· Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ ÙÙŠ [`Seq2SeqTrainingArguments`]. Ø§Ù„Ù…Ø¹Ù„Ù…Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ø§Ù„ÙˆØ­ÙŠØ¯Ø© Ù‡ÙŠ `output_dir` Ø§Ù„ØªÙŠ ØªØ­Ø¯Ø¯ Ù…ÙƒØ§Ù† Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬Ùƒ. Ø³ØªÙ‚ÙˆÙ… Ø¨Ø§Ù„Ø¯ÙØ¹ Ø¥Ù„Ù‰ Hub Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ¹ÙŠÙŠÙ† `push_to_hub=True` (ÙŠØ¬Ø¨ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¥Ù„Ù‰ Hugging Face Ù„ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ). ÙÙŠ Ù†Ù‡Ø§ÙŠØ© ÙƒÙ„ Ø­Ù‚Ø¨Ø©ØŒ Ø³ÙŠÙ‚ÙˆÙ… [`Trainer`] Ø¨ØªÙ‚ÙŠÙŠÙ… Ù…Ù‚ÙŠØ§Ø³ SacreBLEU ÙˆØ­ÙØ¸ Ù†Ù‚Ø·Ø© Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ÙŠØ©.
2. Ù…Ø±Ø± Ø­Ø¬Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¥Ù„Ù‰ [`Seq2SeqTrainer`] Ø¬Ù†Ø¨Ù‹Ø§ Ø¥Ù„Ù‰ Ø¬Ù†Ø¨ Ù…Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆÙ…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ±Ù…ÙŠØ² ÙˆÙ…Ø¬Ù…ÙÙ‘Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙˆØ¸ÙŠÙØ© `compute_metrics`.
3. Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ [`~Trainer.train`] Ù„Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø¨Ø´ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚.

```py
>>> training_args = Seq2SeqTrainingArguments(
...     output_dir="my_awesome_opus_books_model"ØŒ
...     eval_strategy="epoch"ØŒ
...     learning_rate=2e-5ØŒ
...     per_device_train_batch_size=16ØŒ
...     per_device_eval_batch_size=16ØŒ
...     weight_decay=0.01ØŒ
...     save_total_limit=3ØŒ
...     num_train_epochs=2ØŒ
...     predict_with_generate=TrueØŒ
...     fp16=TrueØŒ
...     push_to_hub=TrueØŒ
... )

>>> trainer = Seq2SeqTrainer(
...     model=modelØŒ
...     args=training_argsØŒ
...     train_dataset=tokenized_books["train"]ØŒ
...     eval_dataset=tokenized_books["test"]ØŒ
...     tokenizer=tokenizerØŒ
...     data_collator=data_collatorØŒ
...     compute_metrics=compute_metricsØŒ
... )

>>> trainer.train()
```

Ø¨Ù…Ø¬Ø±Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ Ø´Ø§Ø±Ùƒ Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø¹Ù„Ù‰ Hub Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© [`~transformers.Trainer.push_to_hub`] Ø­ØªÙ‰ ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ø¬Ù…ÙŠØ¹ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬Ùƒ:

```py
>>> trainer.push_to_hub()
```
</pt>
<tf>
<Tip>

Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ø¹Ù„Ù‰ Ø¯Ø±Ø§ÙŠØ© Ø¨Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… KerasØŒ ÙØ±Ø§Ø¬Ø¹ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ [here](../training#train-a-tensorflow-model-with-keras)!

</Tip>
Ù„Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ø¯Ù‚ÙŠÙ‚ ÙÙŠ TensorFlowØŒ Ø§Ø¨Ø¯Ø£ Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ Ø¯Ø§Ù„Ø© Ù…Ø­Ø³Ù† ÙˆÙ…Ø¹Ø¯Ù„ ØªØ¹Ù„Ù… ÙˆØ¨Ø¹Ø¶ ÙØ±Ø· Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨:

```py
>>> from transformers import AdamWeightDecay

>>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)
```

Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ T5 Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`TFAutoModelForSeq2SeqLM`]:

```py
>>> from transformers import TFAutoModelForSeq2SeqLM

>>> model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)
```

Ù‚Ù… Ø¨ØªØ­ÙˆÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ `tf.data.Dataset` Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:

```py
>>> tf_train_set = model.prepare_tf_dataset(
...     tokenized_books["train"]ØŒ
...     shuffle=TrueØŒ
...     batch_size=16ØŒ
...     collate_fn=data_collatorØŒ
... )

>>> tf_test_set = model.prepare_tf_dataset(
...     tokenized_books["test"]ØŒ
...     shuffle=FalseØŒ
...     batch_size=16ØŒ
...     collate_fn=data_collatorØŒ
... )
```

Ù‚Ù… Ø¨ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`compile`](https://keras.io/api/models/model_training_apis/#compile-method). Ù„Ø§Ø­Ø¸ Ø£Ù† Ø¬Ù…ÙŠØ¹ Ù†Ù…Ø§Ø°Ø¬ Transformers Ø¨Ù‡Ø§ Ø¯Ø§Ù„Ø© Ø®Ø³Ø§Ø±Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø°Ø§Øª ØµÙ„Ø© Ø¨Ø§Ù„Ù…Ù‡Ù…Ø©ØŒ Ù„Ø°Ù„Ùƒ Ù„Ø§ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ ÙˆØ§Ø­Ø¯Ø© Ù…Ø§ Ù„Ù… ØªØ±ØºØ¨ ÙÙŠ Ø°Ù„Ùƒ:

```py
>>> import tensorflow as tf

>>> model.compile(optimizer=optimizer) # Ù„Ø§ ØªÙˆØ¬Ø¯ Ø­Ø¬Ø© Ø§Ù„Ø®Ø³Ø§Ø±Ø©!
```

Ø§Ù„Ø£Ù…Ø±Ø§Ù† Ø§Ù„Ø£Ø®ÙŠØ±Ø§Ù† Ø§Ù„Ù„Ø°Ø§Ù† ÙŠØ¬Ø¨ Ø¥Ø¹Ø¯Ø§Ø¯Ù‡Ù…Ø§ Ù‚Ø¨Ù„ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù‡Ù…Ø§ Ø­Ø³Ø§Ø¨ Ù…Ù‚ÙŠØ§Ø³ SacreBLEU Ù…Ù† Ø§Ù„ØªÙ†Ø¨Ø¤Ø§ØªØŒ ÙˆØªÙˆÙÙŠØ± Ø·Ø±ÙŠÙ‚Ø© Ù„Ø¯ÙØ¹ Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø¥Ù„Ù‰ Hub. ÙŠØªÙ… Ø°Ù„Ùƒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [Keras callbacks](../main_classes/keras_callbacks).

Ù…Ø±Ø± Ø¯Ø§Ù„ØªÙƒ `compute_metrics` Ø¥Ù„Ù‰ [`~transformers.KerasMetricCallback`]:

```py
>>> from transformers.keras_callbacks import KerasMetricCallback

>>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)
```

Ø­Ø¯Ø¯ Ø§Ù„Ù…ÙƒØ§Ù† Ø§Ù„Ø°ÙŠ Ø³ØªØ¯ÙØ¹ ÙÙŠÙ‡ Ù†Ù…ÙˆØ°Ø¬Ùƒ ÙˆØ¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ ÙÙŠ [`~transformers.PushToHubCallback`]:

```py
>>> from transformers.keras_callbacks import PushToHubCallback

>>> push_to_hub_callback = PushToHubCallback(
...     output_dir="my_awesome_opus_books_model"ØŒ
...     tokenizer=tokenizerØŒ
... )
```

Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ Ù‚Ù… Ø¨ØªØ¬Ù…ÙŠØ¹ Ù…ÙƒØ§Ù„Ù…Ø§ØªÙƒ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰:

```py
>>> callbacks = [metric_callback, push_to_hub_callback]
```
Ø«Ù… Ù‚Ù… Ø¨ØªØ¬Ù…ÙŠØ¹ Ù…ÙƒØ§Ù„Ù…Ø§ØªÙƒ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰:

```py
>>> callbacks = [metric_callback, push_to_hub_callback]
```

Ø£Ø®ÙŠØ±Ù‹Ø§ØŒ Ø£Ù†Øª Ù…Ø³ØªØ¹Ø¯ Ù„Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬Ùƒ! Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ÙŠØ© ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­ØªÙ‡Ø§ØŒ ÙˆØ¹Ø¯Ø¯ Ø§Ù„Ø¹ØµÙˆØ±ØŒ ÙˆÙ…ÙƒØ§Ù„Ù…Ø§ØªÙƒ Ù„Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬Ùƒ:

```py
>>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=callbacks)
```

Ø¨Ù…Ø¬Ø±Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¥Ù„Ù‰ Hub Ø­ØªÙ‰ ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ø¬Ù…ÙŠØ¹ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡!
</tf>
</frameworkcontent>

<Tip>

Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø«Ø§Ù„ Ø£ÙƒØ«Ø± Ø´Ù…ÙˆÙ„Ø§Ù‹ Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ±Ø¬Ù…Ø©ØŒ Ø±Ø§Ø¬Ø¹ Ø§Ù„Ø¯ÙØªØ± Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
[Ø¯ÙØªØ± Ù…Ù„Ø§Ø­Ø¸Ø§Øª PyTorch](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb)
Ø£Ùˆ [Ø¯ÙØªØ± Ù…Ù„Ø§Ø­Ø¸Ø§Øª TensorFlow](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb).

</Tip>

## Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬

Ø±Ø§Ø¦Ø¹ØŒ Ø§Ù„Ø¢Ù† Ø¨Ø¹Ø¯ Ø£Ù† Ø¶Ø¨Ø·Øª Ù†Ù…ÙˆØ°Ø¬Ù‹Ø§ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬!

ÙÙƒØ± ÙÙŠ Ø¨Ø¹Ø¶ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØªÙŠ ØªØ±ØºØ¨ ÙÙŠ ØªØ±Ø¬Ù…ØªÙ‡Ø§ Ø¥Ù„Ù‰ Ù„ØºØ© Ø£Ø®Ø±Ù‰. Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù€ T5ØŒ ÙŠØ¬Ø¨ Ø¥Ø¶Ø§ÙØ© Ø¨Ø§Ø¯Ø¦Ø© Ø¥Ù„Ù‰ Ø¥Ø¯Ø®Ø§Ù„Ùƒ ÙˆÙÙ‚Ù‹Ø§ Ù„Ù„Ù…Ù‡Ù…Ø© Ø§Ù„ØªÙŠ ØªØ¹Ù…Ù„ Ø¹Ù„ÙŠÙ‡Ø§. Ù„Ù„ØªØ±Ø¬Ù…Ø© Ù…Ù† Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„ÙØ±Ù†Ø³ÙŠØ©ØŒ ÙŠØ¬Ø¨ Ø¥Ø¶Ø§ÙØ© Ø¨Ø§Ø¯Ø¦Ø© Ø¥Ù„Ù‰ Ø¥Ø¯Ø®Ø§Ù„Ùƒ ÙƒÙ…Ø§ Ù‡Ùˆ Ù…ÙˆØ¶Ø­ Ø£Ø¯Ù†Ø§Ù‡:

```py
>>> text = "translate English to French: Legumes share resources with nitrogen-fixing bacteria."
```

Ø£Ø¨Ø³Ø· Ø·Ø±ÙŠÙ‚Ø© Ù„ØªØ¬Ø±Ø¨Ø© Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ù„Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ù‡ÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙÙŠ [`pipeline`]. Ù‚Ù… Ø¨ØªÙ†ÙÙŠØ° Ù…Ø«ÙŠÙ„ Ù„Ù€ `pipeline` Ù„Ù„ØªØ±Ø¬Ù…Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ÙƒØŒ ÙˆÙ…Ø±Ø± Ù†ØµÙƒ Ø¥Ù„ÙŠÙ‡:

```py
>>> from transformers import pipeline

# ØªØºÙŠÙŠØ± `xx` Ø¥Ù„Ù‰ Ù„ØºØ© Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ùˆ`yy` Ø¥Ù„Ù‰ Ù„ØºØ© Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©.
# Ø£Ù…Ø«Ù„Ø©: "en" Ù„Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©ØŒ "fr" Ù„Ù„ÙØ±Ù†Ø³ÙŠØ©ØŒ "de" Ù„Ù„Ø£Ù„Ù…Ø§Ù†ÙŠØ©ØŒ "es" Ù„Ù„Ø¥Ø³Ø¨Ø§Ù†ÙŠØ©ØŒ "zh" Ù„Ù„ØµÙŠÙ†ÙŠØ©ØŒ Ø¥Ù„Ø®Ø› translation_en_to_fr ÙŠØªØ±Ø¬Ù… Ù…Ù† Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„ÙØ±Ù†Ø³ÙŠØ©
# ÙŠÙ…ÙƒÙ†Ùƒ Ø¹Ø±Ø¶ Ø¬Ù…ÙŠØ¹ Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„Ù„ØºØ§Øª Ù‡Ù†Ø§ - https://huggingface.co/languages
>>> translator = pipeline("translation_xx_to_yy"ØŒ model="my_awesome_opus_books_model")
>>> translator(text)
[{'translation_text': 'Legumes partagent des ressources avec des bactÃ©ries azotantes.'}]
```

ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ Ø¥Ø¹Ø§Ø¯Ø© Ø¥Ù†ØªØ§Ø¬ Ù†ØªØ§Ø¦Ø¬ `pipeline` ÙŠØ¯ÙˆÙŠÙ‹Ø§:

<frameworkcontent>
<pt>
Ù‚Ù… Ø¨ØªØ±Ù…ÙŠØ² Ø§Ù„Ù†Øµ ÙˆØ¥Ø±Ø¬Ø§Ø¹ `input_ids` ÙƒØ±Ù…ÙˆØ² PyTorch:

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("my_awesome_opus_books_model")
>>> inputs = tokenizer(text, return_tensors="pt").input_ids
```

Ø§Ø³ØªØ®Ø¯Ù… Ø·Ø±ÙŠÙ‚Ø© [`~generation.GenerationMixin.generate`] Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ±Ø¬Ù…Ø©. Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø­ÙˆÙ„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø®ØªÙ„ÙØ© ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ù„Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„ØªÙˆÙ„ÙŠØ¯ØŒ Ø±Ø§Ø¬Ø¹ ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª [Text Generation](../main_classes/text_generation).

```py
>>> from transformers import AutoModelForSeq2SeqLM

>>> model = AutoModelForSeq2SeqLM.from_pretrained("my_awesome_opus_books_model")
>>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)
```
Ù‚Ù… Ø¨ÙÙƒ ØªØ´ÙÙŠØ± Ø±Ù…ÙˆØ² Ø§Ù„Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„Ù…ÙˆÙ„Ø¯Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø¥Ù„Ù‰ Ù†Øµ:

```py
>>> tokenizer.decode(outputs[0]ØŒ skip_special_tokens=True)
"Les lignÃ©es partagent des ressources avec des bactÃ©ries enfixant l'azote."
```
</pt>
<tf>
Ù‚Ù… Ø¨Ø±Ù…Ø² Ø§Ù„Ù†Øµ ÙˆØ¥Ø±Ø¬Ø§Ø¹ `input_ids` ÙƒØ±Ù…ÙˆØ² TensorFlow:

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("my_awesome_opus_books_model")
>>> inputs = tokenizer(textØŒ return_tensors="tf").input_ids
```

Ø§Ø³ØªØ®Ø¯Ù… Ø·Ø±ÙŠÙ‚Ø© [`~transformers.generation_tf_utils.TFGenerationMixin.generate`] Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ±Ø¬Ù…Ø©. Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø­ÙˆÙ„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø®ØªÙ„ÙØ© ÙˆÙ…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„Ø¥Ù†Ø´Ø§Ø¡ØŒ Ø±Ø§Ø¬Ø¹ ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª [Text Generation](../main_classes/text_generation).

```py
>>> from transformers import TFAutoModelForSeq2SeqLM

>>> model = TFAutoModelForSeq2SeqLM.from_pretrained("my_awesome_opus_books_model")
>>> outputs = model.generate(inputsØŒ max_new_tokens=40ØŒ do_sample=TrueØŒ top_k=30ØŒ top_p=0.95)
```

Ù‚Ù… Ø¨ÙÙƒ ØªØ´ÙÙŠØ± Ø±Ù…ÙˆØ² Ø§Ù„Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„Ù…ÙˆÙ„Ø¯Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø¥Ù„Ù‰ Ù†Øµ:

```py
>>> tokenizer.decode(outputs[0]ØŒ skip_special_tokens=True)
"Les lugumes partagent les ressources avec des bactÃ©ries fixatrices d'azote."
```
</tf>
</frameworkcontent>