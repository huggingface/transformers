# ØªÙˆÙ„ÙŠØ¯ ØªØ¹Ù„ÙŠÙ‚Ø§Øª ØªÙˆØ¶ÙŠØ­ÙŠØ© Ù„Ù„ØµÙˆØ±

[[open-in-colab]]

ØªØªÙ…Ø«Ù„ Ù…Ù‡Ù…Ø© ØªÙˆÙ„ÙŠØ¯ ØªØ¹Ù„ÙŠÙ‚Ø§Øª ØªÙˆØ¶ÙŠØ­ÙŠØ© Ù„Ù„ØµÙˆØ± ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨ØªØ¹Ù„ÙŠÙ‚ ØªÙˆØ¶ÙŠØ­ÙŠ Ù„ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©. ÙˆØªØ´Ù…Ù„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„ÙˆØ§Ù‚Ø¹ÙŠØ© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ù„Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù‡Ù…Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ø¶Ø¹Ø§Ù Ø§Ù„Ø¨ØµØ± Ø¹Ù„Ù‰ Ø§Ù„ØªÙ†Ù‚Ù„ ÙÙŠ Ù…Ø®ØªÙ„Ù Ø§Ù„Ù…ÙˆØ§Ù‚Ù. ÙˆØ¨Ø§Ù„ØªØ§Ù„ÙŠØŒ ØªØ³Ø§Ø¹Ø¯ ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„ØµÙˆØ± Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠØ© Ø¹Ù„Ù‰ ØªØ­Ø³ÙŠÙ† Ø¥Ù…ÙƒØ§Ù†ÙŠØ© ÙˆØµÙˆÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù„Ù„Ø£Ø´Ø®Ø§Øµ Ù…Ù† Ø®Ù„Ø§Ù„ ÙˆØµÙ Ø§Ù„ØµÙˆØ± Ù„Ù‡Ù….

Ø³ÙŠÙˆØ¶Ø­ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ ÙƒÙŠÙÙŠØ©:

* Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ ØªÙˆÙ„ÙŠØ¯ ØªØ¹Ù„ÙŠÙ‚Ø§Øª ØªÙˆØ¶ÙŠØ­ÙŠØ© Ù„Ù„ØµÙˆØ± Ø¨Ø¯Ù‚Ø©.
* Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¶Ø¨ÙˆØ· Ø¨Ø¯Ù‚Ø© Ù„Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬.

Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ø¯Ø¡ØŒ ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ©:

```bash
pip install transformers datasets evaluate -q
pip install jiwer -q
```

Ù†Ø´Ø¬Ø¹Ùƒ Ø¹Ù„Ù‰ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¥Ù„Ù‰ Ø­Ø³Ø§Ø¨Ùƒ ÙÙŠ Hugging Face Ø­ØªÙ‰ ØªØªÙ…ÙƒÙ† Ù…Ù† ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ ÙˆÙ…Ø´Ø§Ø±ÙƒØªÙ‡ Ù…Ø¹ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹. Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙØ·Ù„Ø¨ Ù…Ù†Ùƒ Ø°Ù„ÙƒØŒ Ø£Ø¯Ø®Ù„ Ø±Ù…Ø²Ùƒ Ù„Ù„ØªÙˆØ«ÙŠÙ‚:


```python
from huggingface_hub import notebook_login

notebook_login()
```

## ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª PokÃ©mon BLIP captions

Ø§Ø³ØªØ®Ø¯Ù… Ù…ÙƒØªØ¨Ø© Dataset Ù…Ù† ğŸ¤— Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ØªØªÙƒÙˆÙ† Ù…Ù† Ø£Ø²ÙˆØ§Ø¬ {Ø§Ù„ØµÙˆØ±Ø©-Ø§Ù„ØªØ¹Ù„ÙŠÙ‚ Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠ}. Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø®Ø§ØµØ© Ø¨Ùƒ Ù„ØªÙˆÙ„ÙŠØ¯ ØªØ¹Ù„ÙŠÙ‚Ø§Øª ØªÙˆØ¶ÙŠØ­ÙŠØ© Ù„Ù„ØµÙˆØ± ÙÙŠ PyTorchØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§ØªØ¨Ø§Ø¹ [Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/GIT/Fine_tune_GIT_on_an_image_captioning_dataset.ipynb).


```python
from datasets import load_dataset

ds = load_dataset("lambdalabs/pokemon-blip-captions")
ds
```
```bash
DatasetDict({
    train: Dataset({
        features: ['image', 'text'],
        num_rows: 833
    })
})
```

ØªØ­ØªÙˆÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù„Ù‰ Ù…ÙŠØ²ØªÙŠÙ†ØŒ `image` Ùˆ`text`.

<Tip>

ØªØ­ØªÙˆÙŠ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„ØµÙˆØ± Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠØ© Ø¹Ù„Ù‰ ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ù„ÙƒÙ„ ØµÙˆØ±Ø©. ÙˆÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø§ØªØŒ ØªØªÙ…Ø«Ù„ Ø¥Ø­Ø¯Ù‰ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© ÙÙŠ Ø£Ø®Ø° Ø¹ÙŠÙ†Ø© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù…Ù† Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ù…Ù† Ø¨ÙŠÙ† Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨.

</Tip>

Ù‚Ù… Ø¨ØªÙ‚Ø³ÙŠÙ… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ÙŠØ© Ø¥Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© [`~datasets.Dataset.train_test_split`]:


```python
ds = ds["train"].train_test_split(test_size=0.1)
train_ds = ds["train"]
test_ds = ds["test"]
```

Ø¯Ø¹ÙˆÙ†Ø§ Ù†Ø¹Ø±Ø¶ Ø¨Ø¹Ø¶ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨.


```python
from textwrap import wrap
import matplotlib.pyplot as plt
import numpy as np


def plot_images(images, captions):
    plt.figure(figsize=(20, 20))
    for i in range(len(images)):
        ax = plt.subplot(1, len(images), i + 1)
        caption = captions[i]
        caption = "\n".join(wrap(caption, 12))
        pltMultiplier = plt.title(caption)
        plt.imshow(images[i])
        plt.axis("off")


sample_images_to_visualize = [np.array(train_ds[i]["image"]) for i in range(5)]
sample_captions = [train_ds[i]["text"] for i in range(5)]
plot_images(sample_images_to_visualize, sample_captions)
```
    
<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_training_images_image_cap.png" alt="Sample training images"/>
</div>

## Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø³Ø¨Ù‚Ù‹Ø§

Ù†Ø¸Ø±Ù‹Ø§ Ù„Ø£Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù‡Ø§ ÙˆØ³ÙŠÙ„ØªØ§Ù† (Ø§Ù„ØµÙˆØ±Ø© ÙˆØ§Ù„Ù†Øµ)ØŒ ÙØ¥Ù† Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ø³ØªÙ‚ÙˆÙ… Ø¨Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠØ©.

Ù„Ù„Ù‚ÙŠØ§Ù… Ø¨Ø°Ù„ÙƒØŒ Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ ÙØ¦Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ Ø³ØªÙ‚ÙˆÙ… Ø¨Ø¶Ø¨Ø·Ù‡ Ø¨Ø¯Ù‚Ø©.

```python
from transformers import AutoProcessor

checkpoint = "microsoft/git-base"
processor = AutoProcessor.from_pretrained(checkpoint)
```
```python
from transformers import AutoProcessor

checkpoint = "microsoft/git-base"
processor = AutoProcessor.from_pretrained(checkpoint)
```

Ø³ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø¯Ø§Ø®Ù„ÙŠÙ‹Ø§ Ø¨Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±Ø© (Ø§Ù„ØªÙŠ ØªØ´Ù…Ù„ ØªØºÙŠÙŠØ± Ø§Ù„Ø­Ø¬Ù…ØŒ ÙˆØªØ¯Ø±Ø¬ Ø§Ù„Ø¨ÙƒØ³Ù„) ÙˆØ±Ù…ÙˆØ² Ø§Ù„ØªØ¹Ù„ÙŠÙ‚ Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠ.

```python
def transforms(example_batch):
    images = [x for x in example_batch["image"]]
    captions = [x for x in example_batch["text"]]
    inputs = processor(images=images, text=captions, padding="max_length")
    inputs.update({"labels": inputs["input_ids"]})
    return inputs


train_ds.set_transform(transforms)
test_ds.set_transform(transforms)
```

Ù…Ø¹ ØªØ­Ø¶ÙŠØ± Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¢Ù† Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø¶Ø¨Ø· Ø§Ù„Ø¯Ù‚ÙŠÙ‚.

## ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø£Ø³Ø§Ø³ÙŠ

Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ ["microsoft/git-base"](https://huggingface.co/microsoft/git-base) ÙÙŠ ÙƒØ§Ø¦Ù† [`AutoModelForCausalLM`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM).


```python
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(checkpoint)
```

## Ø§Ù„ØªÙ‚ÙŠÙŠÙ…

ØªÙÙ‚ÙŠÙÙ‘Ù… Ù†Ù…Ø§Ø°Ø¬ ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„ØµÙˆØ± Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠØ© Ø¹Ø§Ø¯Ø©Ù‹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [Rouge Score](https://huggingface.co/spaces/evaluate-metric/rouge) Ø£Ùˆ [Word Error Rate](https://huggingface.co/spaces/evaluate-metric/wer). ÙˆÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ØŒ Ø³ØªØ³ØªØ®Ø¯Ù… Ù…Ø¹Ø¯Ù„ Ø®Ø·Ø£ Ø§Ù„ÙƒÙ„Ù…Ø§Øª (WER).

Ù†Ø³ØªØ®Ø¯Ù… Ù…ÙƒØªØ¨Ø© ğŸ¤— Evaluate Ù„Ù„Ù‚ÙŠØ§Ù… Ø¨Ø°Ù„Ùƒ. ÙˆÙ„Ù„Ø§Ø·Ù„Ø§Ø¹ Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙŠÙˆØ¯ Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø© ÙˆØ§Ù„Ù…Ø´ÙƒÙ„Ø§Øª Ø§Ù„Ø£Ø®Ø±Ù‰ Ù„Ù…Ø¹Ø¯Ù„ WERØŒ Ø±Ø§Ø¬Ø¹ [Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„](https://huggingface.co/spaces/evaluate-metric/wer).


```python
from evaluate import load
import torch

wer = load("wer")


def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predicted = logits.argmax(-1)
    decoded_labels = processor.batch_decode(labels, skip_special_tokens=True)
    decoded_predictions = processor.batch_decode(predicted, skip_special_tokens=True)
    wer_score = wer.compute(predictions=decoded_predictions, references=decoded_labels)
    return {"wer_score": wer_score}
```

## Ø§Ù„ØªØ¯Ø±ÙŠØ¨!

Ø§Ù„Ø¢Ù†ØŒ Ø£Ù†Øª Ù…Ø³ØªØ¹Ø¯ Ù„Ø¨Ø¯Ø¡ Ø§Ù„Ø¶Ø¨Ø· Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬. Ø³ØªØ³ØªØ®Ø¯Ù… ğŸ¤— [`Trainer`] Ù„Ù‡Ø°Ø§ Ø§Ù„ØºØ±Ø¶.

Ø£ÙˆÙ„Ø§Ù‹ØŒ Ø­Ø¯Ø¯ ÙˆØ³Ø§Ø¦Ø· Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`TrainingArguments`].


```python
from transformers import TrainingArguments, Trainer

model_name = checkpoint.split("/")[1]

training_args = TrainingArguments(
    output_dir=f"{model_name}-pokemon"ØŒ
    learning_rate=5e-5ØŒ
    num_train_epochs=50ØŒ
    fp16=TrueØŒ
    per_device_train_batch_size=32ØŒ
    per_device_eval_batch_size=32ØŒ
    gradient_accumulation_steps=2ØŒ
    save_total_limit=3ØŒ
    eval_strategy="steps"ØŒ
    eval_steps=50ØŒ
    save_strategy="steps"ØŒ
    save_steps=50ØŒ
    logging_steps=50ØŒ
    remove_unused_columns=FalseØŒ
    push_to_hub=TrueØŒ
    label_names=["labels"]ØŒ
    load_best_model_at_end=TrueØŒ
)
```

Ø«Ù… Ù…Ø±Ø±Ù‡Ø§ Ø¥Ù„Ù‰ Ø¬Ø§Ù†Ø¨ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ ğŸ¤— Trainer.

```python
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=test_ds,
    compute_metrics=compute_metrics,
)
```

Ù„Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ Ù…Ø§ Ø¹Ù„ÙŠÙƒ Ø³ÙˆÙ‰ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ [`~Trainer.train`] Ø¹Ù„Ù‰ ÙƒØ§Ø¦Ù† [`Trainer`].

```python 
trainer.train()
```

ÙŠØ¬Ø¨ Ø£Ù† ØªØ´Ø§Ù‡Ø¯ Ø§Ù†Ø®ÙØ§Ø¶ Ø®Ø³Ø§Ø±Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø³Ù„Ø§Ø³Ø© Ù…Ø¹ ØªÙ‚Ø¯Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨.

Ø¨Ù…Ø¬Ø±Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ Ø´Ø§Ø±Ùƒ Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø¹Ù„Ù‰ Hub Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© [`~Trainer.push_to_hub`] Ø­ØªÙ‰ ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ø¬Ù…ÙŠØ¹ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬Ùƒ:


```python
trainer.push_to_hub()
```

## Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬

Ø®Ø° Ø¹ÙŠÙ†Ø© ØµÙˆØ±Ø© Ù…Ù† `test_ds` Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.


```python
from PIL import Image
import requests

url = "https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/pokemon.png"
image = Image.open(requests.get(url, stream=True).raw)
image
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/test_image_image_cap.png" alt="Test image"/>
</div>
    
Ù‚Ù… Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙˆØ±Ø© Ù„Ù„Ù†Ù…ÙˆØ°Ø¬.

```python
device = "cuda" if torch.cuda.is_available() else "cpu"

inputs = processor(images=image, return_tensors="pt").to(device)
pixel_values = inputs.pixel_values
```
```python
device = "cuda" if torch.cuda.is_available() else "cpu"

inputs = processor(images=image, return_tensors="pt").to(device)
pixel_values = inputs.pixel_values
```

Ø§Ø³ØªØ¯Ø¹Ù [`generate`] ÙˆÙÙƒ ØªØ´ÙÙŠØ± Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª.

```python
generated_ids = model.generate(pixel_values=pixel_values, max_length=50)
generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(generated_caption)
```
```bash
a drawing of a pink and blue pokemon
```

ÙŠØ¨Ø¯Ùˆ Ø£Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¶Ø¨ÙˆØ· Ø¨Ø¯Ù‚Ø© Ù‚Ø¯ ÙˆÙ„ÙÙ‘Ø¯ ØªØ¹Ù„ÙŠÙ‚Ù‹Ø§ ØªÙˆØ¶ÙŠØ­ÙŠÙ‹Ø§ Ø¬ÙŠØ¯Ù‹Ø§!