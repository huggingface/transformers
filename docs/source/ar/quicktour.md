# ุฌููุฉ ุณุฑูุนุฉ

[[open-in-colab]]

ุงุจุฏุฃ ุจุงุณุชุฎุฏุงู ููุชุจุฉ ๐ค Transformers! ุณูุงุก ููุช ูุทูุฑูุง ุฃู ูุณุชุฎุฏููุง ุนุงุฏููุงุ ุณุชุณุงุนุฏู ูุฐู ุงูุฌููุฉ ุงูุณุฑูุนุฉ ุนูู ุงูุจุฏุก ูุณุชูุธูุฑ ูู ููููุฉ ุงุณุชุฎุฏุงู [`pipeline`] ููุงุณุชูุชุงุฌุ ูุชุญููู ูููุฐุฌ ููุฏุฑุจ ูุณุจููุง ููุนุงูุฌ ููุณุจู ูุน [AutoClass](./model_doc/auto)ุ ูุชุฏุฑูุจ ูููุฐุฌ ุจุณุฑุนุฉ ุจุงุณุชุฎุฏุงู PyTorch ุฃู TensorFlow. ุฅุฐุง ููุช ูุจุชุฏุฆูุงุ ููุตู ุจุงูุงุทูุงุน ุนูู ุฏุฑูุณูุง ุฃู [ุงูุฏูุฑุฉ](https://huggingface.co/course/chapter1/1) ููุญุตูู ุนูู ุดุฑุญ ุฃูุซุฑ ุชุนูููุง ููููุงููู ุงูุชู ุชู ุชูุฏูููุง ููุง.

ูุจู ุงูุจุฏุกุ ุชุฃูุฏ ูู ุชุซุจูุช ุฌููุน ุงูููุชุจุงุช ุงูุถุฑูุฑูุฉ:

```bash
!pip install transformers datasets evaluate accelerate
```

ุณุชุญุชุงุฌ ุฃูุถูุง ุฅูู ุชุซุจูุช ุฅุทุงุฑ ุนูู ุงูุชุนูู ุงูุขูู ุงูููุถู ูุฏูู:

<frameworkcontent>
<pt>

```bash
pip install torch
```
</pt>
<tf>

```bash
pip install tensorflow
```
</tf>
</frameworkcontent>

## ุฎุท ุงูุฃูุงุจูุจ

<Youtube id="tiZFewofSLM"/>

ููุซู [`pipeline`] ุฃุณูู ูุฃุณุฑุน ุทุฑููุฉ ูุงุณุชุฎุฏุงู ูููุฐุฌ ููุฏุฑุจ ูุณุจููุง ููุงุณุชูุชุงุฌ. ููููู ุงุณุชุฎุฏุงู [`pipeline`] ุฌุงูุฒูุง ููุนุฏูุฏ ูู ุงูููุงู ุนุจุฑ ุทุฑุงุฆู ูุฎุชููุฉุ ูุงูุชู ูุธูุฑ ุจุนุถูุง ูู ุงูุฌุฏูู ุฃุฏูุงู:

<Tip>

ููุงุทูุงุน ุนูู ุงููุงุฆูุฉ ุงููุงููุฉ ููููุงู ุงููุชุงุญุฉุ ุฑุงุฌุน [ูุฑุฌุน ูุงุฌูุฉ ุจุฑูุฌุฉ ุงูุชุทุจููุงุช ุงูุฎุงุตุฉ ุจุฎุท ุงูุฃูุงุจูุจ](./main_classes/pipelines).

</Tip>


| **ุงููููุฉ**                     | **ุงููุตู**                                                                                              | **ุงูุทุฑููุฉ**    | **ูุนุฑู ุฎุท ุงูุฃูุงุจูุจ**                       |
|------------------------------|--------------------------------------------------------------------------------------------------------------|-----------------|-----------------------------------------------|
| ุชุตููู ุงููุต          | ุชุนููู ุชุณููุฉ ุฅูู ุชุณูุณู ูุต ูุนูู                                                                   | NLP             | pipeline(task=โsentiment-analysisโ)           |
| ุชูููุฏ ุงููุต              | ุชูููุฏ ูุต ุจูุงุกู ุนูู ููุฌู ูุนูู                                                                                 | NLP             | pipeline(task=โtext-generationโ)              |
| ุชูุฎูุต                | ุชูููุฏ ููุฎุต ูุชุณูุณู ูุต ุฃู ูุณุชูุฏ                                                         | NLP             | pipeline(task=โsummarizationโ)                |
| ุชุตููู ุงูุตูุฑ         | ุชุนููู ุชุณููุฉ ูุตูุฑุฉ ูุนููุฉ                                                                                   | ุฑุคูุฉ ุญุงุณูุจูุฉ | pipeline(task=โimage-classificationโ)         |
| ุชุฌุฒุฆุฉ ุงูุตูุฑุฉ           | ุชุนููู ุชุณููุฉ ููู ุจูุณู ูุฑุฏู ูู ุงูุตูุฑุฉ (ูุฏุนู ุงูุชุฌุฒุฆุฉ ุงูุฏูุงููุฉุ ูุงููุฌููุฉุ ูุชุฌุฒุฆุฉ ูุซููุงุช) | ุฑุคูุฉ ุญุงุณูุจูุฉ | pipeline(task=โimage-segmentationโ)           |
| ุงูุชุดุงู ุงูุฃุดูุงุก             | ุงูุชูุจุค ุจุญุฏูุฏ ุงูุฃุดูุงุก ููุฆุงุชูุง ูู ุตูุฑุฉ ูุนููุฉ                                                | ุฑุคูุฉ ุญุงุณูุจูุฉ | pipeline(task=โobject-detectionโ)             |
| ุชุตููู ุงูุตูุช         | ุชุนููู ุชุณููุฉ ูุจูุงูุงุช ุตูุชูุฉ ูุนููุฉ                                                                            | ุตูุชู           | pipeline(task=โaudio-classificationโ)         |
| ุงูุชุนุฑู ุนูู ุงูููุงู ุงูุชููุงุฆู | ูุณุฎ ุงูููุงู ุฅูู ูุต                                                                                  | ุตูุชู           | pipeline(task=โautomatic-speech-recognitionโ) |
| ุงูุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉ ุงูุจุตุฑูุฉ    | ุงูุฅุฌุงุจุฉ ุนูู ุณุคุงู ุญูู ุงูุตูุฑุฉุ ูุน ุฅุนุทุงุก ุตูุฑุฉ ูุณุคุงู                                             | ูุชุนุฏุฏ ุงููุณุงุฆุท      | pipeline(task=โvqaโ)                          |
| ุงูุฅุฌุงุจุฉ ุนูู ุฃุณุฆูุฉ ุงููุณุชูุฏุงุช  | ุงูุฅุฌุงุจุฉ ุนูู ุณุคุงู ุญูู ุงููุณุชูุฏุ ูุน ุฅุนุทุงุก ูุณุชูุฏ ูุณุคุงู                                        | ูุชุนุฏุฏ ุงููุณุงุฆุท      | pipeline(task="document-question-answering")  |
| ูุชุงุจุฉ ุชุนููู ุนูู ุงูุตูุฑุฉ             | ุฅูุดุงุก ุชุนููู ุนูู ุตูุฑุฉ ูุนููุฉ                                                                         | ูุชุนุฏุฏ ุงููุณุงุฆุท      | pipeline(task="image-to-text")                |

ุงุจุฏุฃ ุจุฅูุดุงุก ูุซูู ูู [`pipeline`] ูุชุญุฏูุฏ ุงููููุฉ ุงูุชู ุชุฑูุฏ ุงุณุชุฎุฏุงูู ููุง. ูู ูุฐุง ุงูุฏูููุ ุณุชุณุชุฎุฏู ุฎุท ุงูุฃูุงุจูุจ ููุชุญููู ุงููุตู ููููุฐุฌ:

```py
>>> from transformers import pipeline

>>> classifier = pipeline("sentiment-analysis")
```

ูููู [`pipeline`] ุจุชูุฒูู ูุชุฎุฒูู ูุณุฎุฉ ุงุญุชูุงุทูุฉ ูู ูููุฐุฌ ุงูุชุฑุงุถู [ููุฏุฑุจ ูุณุจููุง](https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english) ููุนุงูุฌ ููุชุญููู ุงููุตู. ุงูุขู ููููู ุงุณุชุฎุฏุงู `classifier` ุนูู ุงููุต ุงููุณุชูุฏู:

```py
>>> classifier("We are very happy to show you the ๐ค Transformers library.")
[{'label': 'POSITIVE', 'score': 0.9998}]
```

ุฅุฐุง ูุงู ูุฏูู ุฃูุซุฑ ูู ุฅุฏุฎุงู ูุงุญุฏุ ูู ุจุชูุฑูุฑ ุฅุฏุฎุงูุงุชู ููุงุฆูุฉ ุฅูู [`pipeline`] ูุฅุฑุฌุงุน ูุงุฆูุฉ ูู ุงูููุงููุณ:

```py
>>> results = classifier(["We are very happy to show you the ๐ค Transformers library.", "We hope you don't hate it."])
>>> for result in results:
...     print(f"label: {result['label']}, with score: {round(result['score'], 4)}")
label: POSITIVE, with score: 0.9998
label: NEGATIVE, with score: 0.5309
```
ูููู ูุฎุท ุงูุฃูุงุจูุจ ุฃูุถูุง ุฃู ูุชููู ุฎูุงู ูุฌููุนุฉ ุจูุงูุงุช ูุงููุฉ ูุฃู ูููุฉ ุชุฑูุฏูุง. ููุซุงู ุนูู ุฐููุ ุฏุนูุง ูุฎุชุงุฑ ุงูุชุนุฑู ุนูู ุงูููุงู ุงูุชููุงุฆู ููููุฉ ููุง:

```py
>>> import torch
>>> from transformers import pipeline

>>> speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h")
```

ูู ุจุชุญููู ูุฌููุนุฉ ุจูุงูุงุช ุตูุชูุฉ (ุฑุงุฌุน ุฏููู ุงูุจุฏุก ุงูุณุฑูุน ูู ๐ค Datasets [Quick Start](https://huggingface.co/docs/datasets/quickstart#audio) ููุญุตูู ุนูู ูุฒูุฏ ูู ุงูุชูุงุตูู) ุงูุชู ุชุฑูุฏ ุงูุชููู ุฎูุงููุง. ุนูู ุณุจูู ุงููุซุงูุ ูู ุจุชุญููู ูุฌููุนุฉ ุจูุงูุงุช [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14):

```py
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")  # doctest: +IGNORE_RESULT
```

ูุฌุจ ุงูุชุฃูุฏ ูู ุฃู ูุนุฏู ุฃุฎุฐ ุงูุนููุงุช ููุฌููุนุฉ ุงูุจูุงูุงุช ูุชุทุงุจู ูุน ูุนุฏู ุฃุฎุฐ ุงูุนููุงุช ุงูุฐู ุชู ุชุฏุฑูุจ [`facebook/wav2vec2-base-960h`](https://huggingface.co/facebook/wav2vec2-base-960h) ุนููู:

```py
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))
```

ูุชู ุชุญููู ุงููููุงุช ุงูุตูุชูุฉ ูุฅุนุงุฏุฉ ุฃุฎุฐ ุงูุนููุงุช ุชููุงุฆููุง ุนูุฏ ุงุณุชุฏุนุงุก ุงูุนููุฏ "audio".
ุงุณุชุฎุฑุฌ ุงููุตูููุงุช ุงูููุฌูุฉ ุงูุฎุงู ูู ุฃูู 4 ุนููุงุช ููุฑุฑูุง ููุงุฆูุฉ ุฅูู ุฎุท ุงูุฃูุงุจูุจ:

```py
>>> result = speech_recognizer(dataset[:4]["audio"])
>>> print([d["text"] for d in result])
['I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT', "FONDERING HOW I'D SET UP A JOIN TO HELL T WITH MY WIFE AND WHERE THE AP MIGHT BE", "I I'D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I'M NOT SEEING THE OPTION TO DO IT ON THE APSO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AN I'M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS", 'HOW DO I FURN A JOINA COUT']
```

ุจุงููุณุจุฉ ููุฌููุนุงุช ุงูุจูุงูุงุช ุงูุฃูุจุฑ ุญูุซ ุชููู ุงูุฅุฏุฎุงูุงุช ูุจูุฑุฉ (ููุง ูู ุงูุญุงู ูู ุงูููุงู ุฃู ุงูุฑุคูุฉ)ุ ุณุชุฑุบุจ ูู ุชูุฑูุฑ ูููุฏ ุจุฏูุงู ูู ูุงุฆูุฉ ูุชุญููู ุฌููุน ุงูุฅุฏุฎุงูุงุช ูู ุงูุฐุงูุฑุฉ. ุฑุงุฌุน [ูุฑุฌุน ูุงุฌูุฉ ุจุฑูุฌุฉ ุงูุชุทุจููุงุช ุงูุฎุงุตุฉ ุจุฎุท ุงูุฃูุงุจูุจ](./main_classes/pipelines) ููุญุตูู ุนูู ูุฒูุฏ ูู ุงููุนูููุงุช.

### ุงุณุชุฎุฏุงู ูููุฐุฌ ููุนุงูุฌ ุขุฎุฑูู ูู ุฎุท ุงูุฃูุงุจูุจ

ูููู ูุฎุท ุงูุฃูุงุจูุจ [`pipeline`] ุงุณุชูุนุงุจ ุฃู ูููุฐุฌ ูู [Hub](https://huggingface.co/models)ุ ููุง ูุฌุนูู ุณูู ุงูุชููู ูุน ุญุงูุงุช ุงูุงุณุชุฎุฏุงู ุงูุฃุฎุฑู. ุนูู ุณุจูู ุงููุซุงูุ ุฅุฐุง ููุช ุชุฑูุฏ ูููุฐุฌูุง ูุงุฏุฑูุง ุนูู ุงูุชุนุงูู ูุน ุงููุต ุงููุฑูุณูุ ูุงุณุชุฎุฏู ุงูุนูุงูุงุช ุนูู Hub ูุชุตููุฉ ูููุฐุฌ ููุงุณุจ. ุชุนูุฏ ุงููุชูุฌุฉ ุงูุฃููู ุงููุฑุดุญุฉ ูููุฐุฌ BERT ูุชุนุฏุฏ ุงููุบุงุช [BERT model](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment) ุงูุฐู ุชู ุถุจุทู ูุณุจููุง ููุชุญููู ุงููุตู ูุงูุฐู ููููู ุงุณุชุฎุฏุงูู ูููุต ุงููุฑูุณู:

```py
>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
```

<frameworkcontent>
<pt>
ุงุณุชุฎุฏู [`AutoModelForSequenceClassification`] ู [`AutoTokenizer`] ูุชุญููู ุงููููุฐุฌ ุงูููุฏุฑุจ ูุณุจููุง ููุนุงูุฌุชู ุงููุฑุชุจุท ุจู (ูุฒูุฏ ูู ุงููุนูููุงุช ุญูู `AutoClass` ูู ุงููุณู ุงูุชุงูู):

```py
>>> from transformers import AutoTokenizer, AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained(model_name)
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```
</pt>
<tf>
ุงุณุชุฎุฏู [`TFAutoModelForSequenceClassification`] ู [`AutoTokenizer`] ูุชุญููู ุงููููุฐุฌ ุงูููุฏุฑุจ ูุณุจููุง ููุนุงูุฌุชู ุงููุฑุชุจุท ุจู (ูุฒูุฏ ูู ุงููุนูููุงุช ุญูู `TFAutoClass` ูู ุงููุณู ุงูุชุงูู):

```py
>>> from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

>>> model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```
</tf>
</frameworkcontent>

ุญุฏุฏ ุงููููุฐุฌ ูุงููุนุงูุฌ ูู [`pipeline`]. ุงูุขู ููููู ุชุทุจูู `classifier` ุนูู ุงููุต ุงููุฑูุณู:

```py
>>> classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
>>> classifier("Nous sommes trรจs heureux de vous prรฉsenter la bibliothรจque ๐ค Transformers.")
[{'label': '5 stars', 'score': 0.7273}]
```
ุฅุฐุง ูู ุชุชููู ูู ุงูุนุซูุฑ ุนูู ูููุฐุฌ ูุญุงูุชู ุงูุงุณุชุฎุฏุงููุฉุ ูุณุชุญุชุงุฌ ุฅูู ุถุจุท ูููุฐุฌ ููุฏุฑุจ ูุณุจููุง ุนูู ุจูุงูุงุชู. ุงุทูุน ุนูู [ุฏููู ุงูุถุจุท ุงูุฏููู](./training) ููุชุนุฑู ุนูู ููููุฉ ุงูููุงู ุจุฐูู. ูุฃุฎูุฑูุงุ ุจุนุฏ ุถุจุท ูููุฐุฌู ุงูููุฏุฑุจ ูุณุจููุงุ ูุฑุฌู ูุฑุงุนุงุฉ [ุงููุดุงุฑูุฉ](./model_sharing) ุจุงููููุฐุฌ ูุน ุงููุฌุชูุน ุนูู Hub ูุฏููุฑุทุฉ ุงูุชุนูู ุงูุขูู ููุฌููุน! ๐ค

## AutoClass

<Youtube id="AhChOFRegn4"/>

ูู ุงูุฎูููุฉุ ุชุนูู ูุฆุงุช [`AutoModelForSequenceClassification`] ู [`AutoTokenizer`] ูุนูุง ูุชุดุบูู ุฎุท ุงูุฃูุงุจูุจ ุงูุฐู ุงุณุชุฎุฏูุชู ุฃุนูุงู. ุชุนุชุจุฑ [AutoClass](./model_doc/auto) ุงุฎุชุตุงุฑูุง ูููู ุชููุงุฆููุง ุจุงุณุชุฑุฏุงุฏ ุจููุฉ ูููุฐุฌ ููุฏุฑุจ ูุณุจููุง ูู ุงุณูู ุฃู ูุณุงุฑู. ูู ูุง ุนููู ูุนูู ูู ุชุญุฏูุฏ ูุฆุฉ `AutoClass` ุงูููุงุณุจุฉ ููููุชู ููุฆุฉ ุงููุนุงูุฌุฉ ุงููุฑุชุจุทุฉ ุจูุง.

ููุนุฏ ุฅูู ุงููุซุงู ูู ุงููุณู ุงูุณุงุจู ูููุฑู ููู ููููู ุงุณุชุฎุฏุงู `AutoClass` ูุชูุฑุงุฑ ูุชุงุฆุฌ ุฎุท ุงูุฃูุงุจูุจ.

### AutoTokenizer

ูุชููู ุงููุนุงูุฌ ูุณุคูููุฉ ูุนุงูุฌุฉ ุงููุต ุฅูู ูุตูููุฉ ูู ุงูุฃุฑูุงู ูุฅุฏุฎุงูุงุช ููููุฐุฌ ูุนูู. ููุงู ููุงุนุฏ ูุชุนุฏุฏุฉ ุชุญูู ุนูููุฉ ุงููุนุงูุฌุฉุ ุจูุง ูู ุฐูู ููููุฉ ุชูุณูู ูููุฉ ููุง ูู ุงููุณุชูู ุงูุฐู ูุฌุจ ุฃู ุชููุณู ููู ุงููููุงุช (ุชุนุฑู ุนูู ุงููุฒูุฏ ุญูู ุงููุนุงูุฌุฉ ูู [ููุฎุต ุงููุนุงูุฌ](./tokenizer_summary)). ุฃูู ุดูุก ูุฌุจ ุชุฐูุฑู ูู ุฃูู ุชุญุชุงุฌ ุฅูู ุฅูุดุงุก ูุซูู ูููุนุงูุฌ ุจููุณ ุงุณู ุงููููุฐุฌ ูุถูุงู ุงุณุชุฎุฏุงูู ูููุงุนุฏ ุงููุนุงูุฌุฉ ููุณูุง ุงูุชู ุชู ุชุฏุฑูุจ ุงููููุฐุฌ ุนูููุง.

ูู ุจุชุญููู ูุนุงูุฌ ุจุงุณุชุฎุฏุงู [`AutoTokenizer`]:

```py
>>> from transformers import AutoTokenizer

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```

ูุฑุฑ ูุตู ุฅูู ุงููุนุงูุฌ:

```py
>>> encoding = tokenizer("We are very happy to show you the ๐ค Transformers library.")
>>> print(encoding)
{'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

ูุนูุฏ ุงููุนุงูุฌ ูุงููุณูุง ูุญุชูู ุนูู:

* [input_ids](./glossary#input-ids): ุงูุชูุซููุงุช ุงูุฑูููุฉ ูุฑููุฒู.
* [attention_mask](./glossary#attention-mask): ุชุดูุฑ ุฅูู ุงูุฑููุฒ ุงูุชู ูุฌุจ ุงูุงูุชูุงู ุจูุง.

ูููู ูููุนุงูุฌ ุฃูุถูุง ูุจูู ูุงุฆูุฉ ูู ุงูุฅุฏุฎุงูุงุชุ ูุชูุณูู ุงููุต ูุฅููุงูู ูุฅุฑุฌุงุน ุฏูุนุฉ ุฐุงุช ุทูู ููุญุฏ:

<frameworkcontent>
<pt>

```py
>>> pt_batch = tokenizer(
...     ["We are very happy to show you the ๐ค Transformers library.", "We hope you don't hate it."],
...     padding=True,
...     truncation=True,
...     max_length=512,
...     return_tensors="pt",
... )
```
</pt>
<tf>

```py
>>> tf_batch = tokenizer(
...     ["We are very happy to show you the ๐ค Transformers library.", "We hope you don't hate it."],
...     padding=True,
...     truncation=True,
...     max_length=512,
...     return_tensors="tf",
... )
```
</tf>
</frameworkcontent>

<Tip>

ุงุทูุน ุนูู [ุงูุฏููู ุงูุชูููุฏู ูููุนุงูุฌุฉ ุงููุณุจูุฉ](./preprocessing) ููุญุตูู ุนูู ูุฒูุฏ ูู ุงูุชูุงุตูู ุญูู ุงููุนุงูุฌุฉุ ูููููุฉ ุงุณุชุฎุฏุงู [`AutoImageProcessor`] ู [`AutoFeatureExtractor`] ู [`AutoProcessor`] ููุนุงูุฌุฉ ุงูุตูุฑ ูุงูุตูุช ูุงูุฅุฏุฎุงูุงุช ูุชุนุฏุฏุฉ ุงููุณุงุฆุท.

</Tip>

### AutoModel

<frameworkcontent>
<pt>
ุชูุฏู ููุชุจุฉ ๐ค Transformers ุทุฑููุฉ ุจุณูุทุฉ ูููุญุฏุฉ ูุชุญููู ูุซููุงุช ููุฏุฑุจุฉ ูุณุจููุง. ููุฐุง ูุนูู ุฃูู ููููู ุชุญููู [`AutoModel`] ููุง ูู ููุช ุชููู ุจุชุญููู [`AutoTokenizer`]. ุงููุฑู ุงููุญูุฏ ูู ุงุฎุชูุงุฑ ูุฆุฉ [`AutoModel`] ุงูููุงุณุจุฉ ูููููุฉ. ุจุงููุณุจุฉ ูุชุตููู ุงููุต (ุฃู ุงูุชุณูุณู)ุ ูุฌุจ ุนููู ุชุญููู [`AutoModelForSequenceClassification`]:

```py
>>> from transformers import AutoModelForSequenceClassification

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
```

<Tip>

ุฑุงุฌุน [ููุฎุต ุงููููุฉ](./task_summary) ููุงุทูุงุน ุนูู ุงูููุงู ุงูุชู ุชุฏุนููุง ูุฆุฉ [`AutoModel`].

</Tip>

ุงูุขู ูู ุจุชูุฑูุฑ ุฏูุนุฉ ุงูุฅุฏุฎุงูุงุช ุงูููุนุงูุฌุฉ ูุณุจููุง ูุจุงุดุฑุฉ ุฅูู ุงููููุฐุฌ. ูู ูุง ุนููู ูุนูู ูู ูู ุชุนุจุฆุฉ ุงููุงููุณ ุนู ุทุฑูู ุฅุถุงูุฉ `**`:

## ุชุฏุฑูุจ ุงููููุฐุฌ

ุงูุขูุ ูุฑุฑ ุฏูุนุฉ ุงููุฏุฎูุงุช ุงููุนุงูุฌุฉ ูุณุจููุง ูุจุงุดุฑุฉ ุฅูู ุงููููุฐุฌ. ูุง ุนููู ุณูู ูู ุชุนุจุฆุฉ ุงููุงููุณ ุนู ุทุฑูู ุฅุถุงูุฉ `**`:

```py
>>> pt_outputs = pt_model(**pt_batch)
```

ูููู ุงููููุฐุฌ ุจุฅุฎุฑุงุฌ ุงูุชูุดูุทุงุช ุงูููุงุฆูุฉ ูู ุณูุฉ `logits`. ุทุจู ุฏุงูุฉ softmax ุนูู `logits` ูุงุณุชุฑุฏุงุฏ ุงูุงุญุชูุงูุงุช:

```py
>>> from torch import nn

>>> pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
>>> print(pt_predictions)
tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],
        [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)
```
</pt>
<tf>
ูููุฑ ๐ค Transformers ุทุฑููุฉ ุจุณูุทุฉ ูููุญุฏุฉ ูุชุญููู ูุซููุงุช ููุฏุฑุจุฉ ูุณุจููุง. ููุฐุง ูุนูู ุฃูู ููููู ุชุญููู [`TFAutoModel`] ูุซู ุชุญููู [`AutoTokenizer`]. ูุงููุฑู ุงููุญูุฏ ูู ุชุญุฏูุฏ [`TFAutoModel`] ุงูุตุญูุญ ูููููุฉ. ููุชุตููู ุงููุตู (ุฃู ุงูุชุณูุณูู)ุ ูุฌุจ ุชุญููู [`TFAutoModelForSequenceClassification`]:

```py
>>> from transformers import TFAutoModelForSequenceClassification

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
```

<Tip>

ุฑุงุฌุน [ููุฎุต ุงูููุงู](./task_summary) ููููุงู ุงููุฏุนููุฉ ุจูุงุณุทุฉ ูุฆุฉ [`AutoModel`].

</Tip>

ุงูุขูุ ูุฑุฑ ุฏูุนุฉ ุงููุฏุฎูุงุช ุงููุนุงูุฌุฉ ูุณุจููุง ูุจุงุดุฑุฉ ุฅูู ุงููููุฐุฌ. ููููู ุชูุฑูุฑ ุงููุตูููุงุช ููุง ูู:

```py
>>> tf_outputs = tf_model(tf_batch)
```

ูููู ุงููููุฐุฌ ุจุฅุฎุฑุงุฌ ุงูุชูุดูุทุงุช ุงูููุงุฆูุฉ ูู ุณูุฉ `logits`. ุทุจู ุฏุงูุฉ softmax ุนูู `logits` ูุงุณุชุฑุฏุงุฏ ุงูุงุญุชูุงูุงุช:

```py
>>> import tensorflow as tf

>>> tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)
>>> tf_predictions  # doctest: +IGNORE_RESULT
```
</tf>
</frameworkcontent>

<Tip>

ุชุฎุฑุฌ ุฌููุน ููุงุฐุฌ ๐ค Transformers (PyTorch ุฃู TensorFlow) ุงููุตูููุงุช *ูุจู* ุฏุงูุฉ ุงูุชูุดูุท ุงูููุงุฆูุฉ (ูุซู softmax) ูุฃู ุฏุงูุฉ ุงูุชูุดูุท ุงูููุงุฆูุฉ ุบุงูุจูุง ูุง ุชููู ูุฏูุฌุฉ ูุน ุงูุฎุณุงุฑุฉ. ูุฎุฑุฌุงุช ุงููููุฐุฌ ุนุจุงุฑุฉ ุนู ูุฆุงุช ุจูุงูุงุช ุฎุงุตุฉุ ูุฐูู ูุชู ุงุณุชููุงู ุณูุงุชูุง ุชููุงุฆููุง ูู IDE. ูุชุชุตุฑู ูุฎุฑุฌุงุช ุงููููุฐุฌ ูุซู ุฒูุฌ ูุฑุชุจ ุฃู ูุงููุณ (ููููู ุงูููุฑุณุฉ ุจุงุณุชุฎุฏุงู ุนุฏุฏ ุตุญูุญ ุฃู ุดุฑูุญุฉ ุฃู ุณูุณูุฉ)ุ ููู ูุฐู ุงูุญุงูุฉุ ูุชู ุชุฌุงูู ุงูุณูุงุช ุงูุชู ุชููู None.

</Tip>

### ุญูุธ ุงููููุฐุฌ

<frameworkcontent>
<pt>
ุจูุฌุฑุฏ ุถุจุท ูููุฐุฌูุ ููููู ุญูุธู ูุน ุจุฑูุงูุฌ ุงูุชุฑููุฒ ุงูุฎุงุต ุจู ุจุงุณุชุฎุฏุงู [`PreTrainedModel.save_pretrained`]:

```py
>>> pt_save_directory = "./pt_save_pretrained"
>>> tokenizer.save_pretrained(pt_save_directory)  # doctest: +IGNORE_RESULT
>>> pt_model.save_pretrained(pt_save_directory)
```

ุนูุฏูุง ุชููู ูุณุชุนุฏูุง ูุงุณุชุฎุฏุงู ุงููููุฐุฌ ูุฑุฉ ุฃุฎุฑูุ ุฃุนุฏ ุชุญูููู ุจุงุณุชุฎุฏุงู [`PreTrainedModel.from_pretrained`]:

```py
>>> pt_model = AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained")
```
</pt>
<tf>
ุจูุฌุฑุฏ ุถุจุท ูููุฐุฌูุ ููููู ุญูุธู ูุน ุจุฑูุงูุฌ ุงูุชุฑููุฒ ุงูุฎุงุต ุจู ุจุงุณุชุฎุฏุงู [`TFPreTrainedModel.save_pretrained`]:

```py
>>> tf_save_directory = "./tf_save_pretrained"
>>> tokenizer.save_pretrained(tf_save_directory)  # doctest: +IGNORE_RESULT
>>> tf_model.save_pretrained(tf_save_directory)
```

ุนูุฏูุง ุชููู ูุณุชุนุฏูุง ูุงุณุชุฎุฏุงู ุงููููุฐุฌ ูุฑุฉ ุฃุฎุฑูุ ุฃุนุฏ ุชุญูููู ุจุงุณุชุฎุฏุงู [`TFPreTrainedModel.from_pretrained`]:

```py
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained("./tf_save_pretrained")
```
</tf>
</frameworkcontent>

ูู ุงูููุฒุงุช ุงูุฑุงุฆุนุฉ ูู ๐ค Transformers ุงููุฏุฑุฉ ุนูู ุญูุธ ูููุฐุฌ ูุฅุนุงุฏุฉ ุชุญูููู ููููุฐุฌ PyTorch ุฃู TensorFlow. ูููู ุฃู ูุญูู ูุนุงูู `from_pt` ุฃู `from_tf` ุงููููุฐุฌ ูู ุฅุทุงุฑ ุนูู ุฅูู ุขุฎุฑ:

<frameworkcontent>
<pt>

```py
>>> from transformers import AutoModel

>>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
>>> pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)
```
</pt>
<tf>

```py
>>> from transformers import TFAutoModel

>>> tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)
```
</tf>
</frameworkcontent>


## ุจูุงุก ูููุฐุฌ ูุฎุตุต

ููููู ุชุนุฏูู ูุฆุฉ ุชูููู ุงููููุฐุฌ ูุชุบููุฑ ููููุฉ ุจูุงุก ุงููููุฐุฌ. ูุญุฏุฏ ุงูุชูููู ุณูุงุช ุงููููุฐุฌุ ูุซู ุนุฏุฏ ุงูุทุจูุงุช ุงููุฎููุฉ ุฃู ุฑุคูุณ ุงูุงูุชูุงู. ุชุจุฏุฃ ูู ุงูุตูุฑ ุนูุฏ ุชููุฆุฉ ูููุฐุฌ ูู ูุฆุฉ ุชูููู ูุฎุตุต. ูุชู ุชููุฆุฉ ุณูุงุช ุงููููุฐุฌ ุจุดูู ุนุดูุงุฆูุ ููุฌุจ ุชุฏุฑูุจ ุงููููุฐุฌ ูุจู ุงุณุชุฎุฏุงูู ููุญุตูู ุนูู ูุชุงุฆุฌ ุฐุงุช ูุนูู.

ุงุจุฏุฃ ุจุงุณุชูุฑุงุฏ [`AutoConfig`]. ุซู ูู ุจุชุญููู ุงููููุฐุฌ ุงูููุฏุฑุจ ูุณุจููุง ุงูุฐู ุชุฑูุฏ ุชุนุฏููู. ุถูู [`AutoConfig.from_pretrained`]. ููููู ุชุญุฏูุฏ ุงูุณูุฉ ุงูุชู ุชุฑูุฏ ุชุบููุฑูุงุ ูุซู ุนุฏุฏ ุฑุคูุณ ุงูุงูุชูุงู:

```py
>>> from transformers import AutoConfig

>>> my_config = AutoConfig.from_pretrained("distilbert/distilbert-base-uncased", n_heads=12)
```

<frameworkcontent>
<pt>
ูู ุจุฅูุดุงุก ูููุฐุฌ ูู ุชููููู ุงููุฎุตุต ุจุงุณุชุฎุฏุงู [`AutoModel.from_config`]:

```py
>>> from transformers import AutoModel

>>> my_model = AutoModel.from_config(my_config)
```
</pt>
<tf>
ูู ุจุฅูุดุงุก ูููุฐุฌ ูู ุชููููู ุงููุฎุตุต ุจุงุณุชุฎุฏุงู [`TFAutoModel.from_config`]:

```py
>>> from transformers import TFAutoModel

>>> my_model = TFAutoModel.from_config(my_config)
```
</tf>
</frameworkcontent>

ุงูู ูุธุฑุฉ ุนูู ุฏููู [ุฅูุดุงุก ุจููุฉ ูุฎุตุตุฉ](./create_a_model) ููุฒูุฏ ูู ุงููุนูููุงุช ุญูู ุจูุงุก ุงูุชููููุงุช ุงููุฎุตุตุฉ.

## ุงููุฏุฑุจ - ุญููุฉ ุชุฏุฑูุจ ูุญุณูุฉ ูู PyTorch

ุฌููุน ุงูููุงุฐุฌ ุนุจุงุฑุฉ ุนู [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) ููุงุณูุฉุ ูุฐุง ููููู ุงุณุชุฎุฏุงููุง ูู ุฃู ุญููุฉ ุชุฏุฑูุจ ูููุฐุฌูุฉ. ูู ุญูู ููููู ูุชุงุจุฉ ุญููุฉ ุงูุชุฏุฑูุจ ุงูุฎุงุตุฉ ุจูุ ูููุฑ ๐ค Transformers ูุฆุฉ [`Trainer`] ูู PyTorchุ ูุงูุชู ุชุญุชูู ุนูู ุญููุฉ ุงูุชุฏุฑูุจ ุงูุฃุณุงุณูุฉ ูุชุถูู ูุธุงุฆู ุฅุถุงููุฉ ูููุฒุงุช ูุซู ุงูุชุฏุฑูุจ ุงูููุฒุนุ ูุงูุฏูุฉ ุงููุฎุชูุทุฉุ ูุงููุฒูุฏ.

ููููุง ููููุชูุ ุณุชููู ุนุงุฏุฉู ุจุชูุฑูุฑ ุงููุนููุงุช ุงูุชุงููุฉ ุฅูู [`Trainer`]:

1. ุณุชุจุฏุฃ ุจู [`PreTrainedModel`] ุฃู [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module):

   ```py
   >>> from transformers import AutoModelForSequenceClassification

   >>> model = AutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
   ```

2. ุชุญุชูู [`TrainingArguments`] ุนูู ูุฑุท ูุนููุงุช ุงููููุฐุฌ ุงูุชู ููููู ุชุบููุฑูุง ูุซู ูุนุฏู ุงูุชุนููุ ูุญุฌู ุงูุฏูุนุฉุ ูุนุฏุฏ ุงูุนุตูุฑ ุงูุชู ูุฌุจ ุงูุชุฏุฑูุจ ุนูููุง. ูุชู ุงุณุชุฎุฏุงู ุงูููู ุงูุงูุชุฑุงุถูุฉ ุฅุฐุง ูู ุชุญุฏุฏ ุฃู ุญุฌุฌ ุชุฏุฑูุจ:

   ```py
   >>> from transformers import TrainingArguments

   >>> training_args = TrainingArguments(
   ...     output_dir="path/to/save/folder/",
   ...     learning_rate=2e-5,
   ...     per_device_train_batch_size=8,
   ...     per_device_eval_batch_size=8,
   ...     num_train_epochs=2,
   ... )
   ```

3. ูู ุจุชุญููู ูุฆุฉ ูุนุงูุฌุฉ ูุณุจูุฉ ูุซู ุจุฑูุงูุฌ ุงูุชุฑููุฒุ ุฃู ูุนุงูุฌ ุงูุตูุฑุ ุฃู ูุณุชุฎุฑุฌ ุงูููุฒุงุชุ ุฃู ุงููุนุงูุฌ:

   ```py
   >>> from transformers import AutoTokenizer

   >>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
   ```

4. ูู ุจุชุญููู ูุฌููุนุฉ ุจูุงูุงุช:

   ```py
   >>> from datasets import load_dataset

   >>> dataset = load_dataset("rotten_tomatoes")  # doctest: +IGNORE_RESULT
   ```

5. ูู ุจุฅูุดุงุก ุฏุงูุฉ ูุชุฑููุฒ ูุฌููุนุฉ ุงูุจูุงูุงุช:

   ```py
   >>> def tokenize_dataset(dataset):
   ...     return tokenizer(dataset["text"])
   ```

   ุซู ูู ุจุชุทุจููู ุนูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุจุฃููููุง ุจุงุณุชุฎุฏุงู [`~datasets.Dataset.map`]:

   ```py
   >>> dataset = dataset.map(tokenize_dataset, batched=True)
   ```

6. [`DataCollatorWithPadding`] ูุฅูุดุงุก ุฏูุนุฉ ูู ุงูุฃูุซูุฉ ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจู:

   ```py
   >>> from transformers import DataCollatorWithPadding

   >>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
   ```

ุงูุขู ูู ุจุชุฌููุน ุฌููุน ูุฐู ุงููุฆุงุช ูู [`Trainer`]:

```py
>>> from transformers import Trainer

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=dataset["train"],
...     eval_dataset=dataset["test"],
...     tokenizer=tokenizer,
...     data_collator=data_collator,
... )  # doctest: +SKIP
```
ุนูุฏูุง ุชููู ูุณุชุนุฏูุงุ ุงุชุตู ุจู [`~Trainer.train`] ูุจุฏุก ุงูุชุฏุฑูุจ:

```py
>>> trainer.train()  # doctest: +SKIP
```

<Tip>

ุจุงููุณุจุฉ ููููุงู - ูุซู ุงูุชุฑุฌูุฉ ุฃู ุชูุฎูุต - ุงูุชู ุชุณุชุฎุฏู ูููุฐุฌ ุชุณูุณู ุฅูู ุชุณูุณูุ ุงุณุชุฎุฏู ูุฆุงุช [`Seq2SeqTrainer`] ู [`Seq2SeqTrainingArguments`] ุจุฏูุงู ูู ุฐูู.

</Tip>

ููููู ุชุฎุตูุต ุณููู ุญููุฉ ุงูุชุฏุฑูุจ ุนู ุทุฑูู ุฅูุดุงุก ูุฆุฉ ูุฑุนูุฉ ูู ุงูุทุฑู ุฏุงุฎู [`Trainer`]. ูุณูุญ ูู ุฐูู ุจุชุฎุตูุต ููุฒุงุช ูุซู ุฏุงูุฉ ุงูุฎุณุงุฑุฉุ ูุงููุญุณูุ ูุงููุฌุฏูู. ุฑุงุฌุน ูุฑุฌุน [`Trainer`] ููุชุนุฑู ุนูู ุงูุทุฑู ุงูุชู ูููู ุฅูุดุงุก ูุฆุงุช ูุฑุนูุฉ ูููุง.

ูุงูุทุฑููุฉ ุงูุฃุฎุฑู ูุชุฎุตูุต ุญููุฉ ุงูุชุฏุฑูุจ ูู ุจุงุณุชุฎุฏุงู [ุงููุณุชุฏุนูุงุช](./main_classes/callback). ููููู ุงุณุชุฎุฏุงู ุงููุณุชุฏุนูุงุช ููุงูุฏูุงุฌ ูุน ุงูููุชุจุงุช ุงูุฃุฎุฑู ููุญุต ุญููุฉ ุงูุชุฏุฑูุจ ููุฅุจูุงุบ ุนู ุงูุชูุฏู ุฃู ุฅููุงู ุงูุชุฏุฑูุจ ูุจูุฑูุง. ูุง ุชุนุฏู ุงููุณุชุฏุนูุงุช ุฃู ุดูุก ูู ุญููุฉ ุงูุชุฏุฑูุจ ููุณูุง. ูุชุฎุตูุต ุดูุก ูุซู ุฏุงูุฉ ุงูุฎุณุงุฑุฉุ ุชุญุชุงุฌ ุฅูู ุฅูุดุงุก ูุฆุฉ ูุฑุนูุฉ ูู [`Trainer`] ุจุฏูุงู ูู ุฐูู.

## ุงูุชุฏุฑูุจ ุจุงุณุชุฎุฏุงู TensorFlow

ุฌููุน ุงูููุงุฐุฌ ุนุจุงุฑุฉ ุนู [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) ููุงุณูุฉุ ูุฐุง ูููู ุชุฏุฑูุจูุง ูู TensorFlow ุจุงุณุชุฎุฏุงู ูุงุฌูุฉ ุจุฑูุฌุฉ ุชุทุจููุงุช Keras. ูููุฑ ๐ค Transformers ุทุฑููุฉ [`~TFPreTrainedModel.prepare_tf_dataset`] ูุชุญููู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจู ุจุณูููุฉ ูู `tf.data.Dataset` ุญุชู ุชุชููู ูู ุงูุจุฏุก ูู ุงูุชุฏุฑูุจ ุนูู ุงูููุฑ ุจุงุณุชุฎุฏุงู ุทุฑู `compile` ู`fit` ูู Keras.

1. ุณุชุจุฏุฃ ุจู [`TFPreTrainedModel`] ุฃู [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model):

   ```py
   >>> from transformers import TFAutoModelForSequenceClassification

   >>> model = TFAutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
   ```

2. ูู ุจุชุญููู ูุฆุฉ ูุนุงูุฌุฉ ูุณุจูุฉ ูุซู ุจุฑูุงูุฌ ุงูุชุฑููุฒุ ุฃู ูุนุงูุฌ ุงูุตูุฑุ ุฃู ูุณุชุฎุฑุฌ ุงูููุฒุงุชุ ุฃู ุงููุนุงูุฌ:

   ```py
   >>> from transformers import AutoTokenizer

   >>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
   ```

3. ูู ุจุฅูุดุงุก ุฏุงูุฉ ูุชุฑููุฒ ูุฌููุนุฉ ุงูุจูุงูุงุช:

   ```py
   >>> def tokenize_dataset(dataset):
   ...     return tokenizer(dataset["text"])  # doctest: +SKIP
   ```

4. ูู ุจุชุทุจูู ุจุฑูุงูุฌ ุงูุชุฑููุฒ ุนูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุจุฃููููุง ุจุงุณุชุฎุฏุงู [`~datasets.Dataset.map`] ุซู ูุฑุฑ ูุฌููุนุฉ ุงูุจูุงูุงุช ูุจุฑูุงูุฌ ุงูุชุฑููุฒ ุฅูู [`~TFPreTrainedModel.prepare_tf_dataset`]. ููููู ุฃูุถูุง ุชุบููุฑ ุญุฌู ุงูุฏูุนุฉ ูุฎูุท ูุฌููุนุฉ ุงูุจูุงูุงุช ููุง ุฅุฐุง ุฃุฑุฏุช:

   ```py
   >>> dataset = dataset.map(tokenize_dataset)  # doctest: +SKIP
   >>> tf_dataset = model.prepare_tf_dataset(
   ...     dataset["train"], batch_size=16, shuffle=True, tokenizer=tokenizer
   ... )  # doctest: +SKIP
   ```

5. ุนูุฏูุง ุชููู ูุณุชุนุฏูุงุ ููููู ุงุณุชุฏุนุงุก `compile` ู`fit` ูุจุฏุก ุงูุชุฏุฑูุจ. ูุงุญุธ ุฃู ุฌููุน ููุงุฐุฌ Transformers ูุฏููุง ุฏุงูุฉ ุฎุณุงุฑุฉ ุฐุงุช ุตูุฉ ุจุงููููุฉ ุจุดูู ุงูุชุฑุงุถูุ ูุฐุง ูุฃูุช ูุณุช ุจุญุงุฌุฉ ุฅูู ุชุญุฏูุฏ ูุงุญุฏุฉ ูุง ูู ุชุฑุบุจ ูู ุฐูู:

   ```py
   >>> from tensorflow.keras.optimizers import Adam

   >>> model.compile(optimizer='adam')  # ูุง ุชูุฌุฏ ูุณูุทุฉ ุฏุงูุฉ ุงูุฎุณุงุฑุฉ!
   >>> model.fit(tf_dataset)  # doctest: +SKIP
   ```

## ูุงุฐุง ุจุนุฏุ

ุงูุขู ุจุนุฏ ุฃู ุฃูููุช ุงูุฌููุฉ ุงูุณุฑูุนุฉ ูู ๐ค Transformersุ ุฑุงุฌุน ุฃุฏูุฉูุง ูุชุนุฑู ุนูู ููููุฉ ุงูููุงู ุจุฃุดูุงุก ุฃูุซุฑ ุชุญุฏูุฏูุง ูุซู ูุชุงุจุฉ ูููุฐุฌ ูุฎุตุตุ ูุถุจุท ูููุฐุฌ ููููุฉุ ูููููุฉ ุชุฏุฑูุจ ูููุฐุฌ ุจุงุณุชุฎุฏุงู ูุต ุจุฑูุฌู. ุฅุฐุง ููุช ููุชููุง ุจูุนุฑูุฉ ุงููุฒูุฏ ุนู ุงูููุงููู ุงูุฃุณุงุณูุฉ ูู ๐ค Transformersุ ูุงุญุตู ุนูู ููุฌุงู ูู ุงููููุฉ ูุงุทูุน ุนูู ุฃุฏูุฉ ุงูููุงููู ุงูุฎุงุตุฉ ุจูุง!