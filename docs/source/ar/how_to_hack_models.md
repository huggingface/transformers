# ููููุฉ ุชุนุฏูู ุฃู ูููุฐุฌ ูู ููุงุฐุฌ Transformers

ุชููุฑ ููุชุจุฉ [๐ค Transformers](https://github.com/huggingface/transformers) ูุฌููุนุฉ ูู ุงูููุงุฐุฌ ุงููุณุจูุฉ ุงูุชุฏุฑูุจ ูุงูุฃุฏูุงุช ููุนุงูุฌุฉ ุงููุบุงุช ุงูุทุจูุนูุฉุ ูุงูุฑุคูุฉุ ููุง ุฅูู ุฐูู. ุนูู ุงูุฑุบู ูู ุฃู ูุฐู ุงูููุงุฐุฌ ุชุบุทู ูุฌููุนุฉ ูุงุณุนุฉ ูู ุงูุชุทุจููุงุชุ ููุฏ ุชูุงุฌู ุญุงูุงุช ุงุณุชุฎุฏุงู ูุง ุชุฏุนููุง ุงูููุชุจุฉ ุจุดูู ุงูุชุฑุงุถู. ููููู ููุชุฎุตูุต ุฃู ููุชุญ ุฅููุงููุงุช ุฌุฏูุฏุฉุ ูุซู ุฅุถุงูุฉ ุทุจูุงุช ุฌุฏูุฏุฉุ ุฃู ุชุนุฏูู ุงูุจููุฉ ุงููุนูุงุฑูุฉุ ุฃู ุชุญุณูู ุขููุงุช ุงูุงูุชุจุงู. ุณูููุถุญ ูู ูุฐุง ุงูุฏููู ููููุฉ ุชุนุฏูู ููุงุฐุฌ Transformers ุงูููุฌูุฏุฉ ูุชูุจูุฉ ุงุญุชูุงุฌุงุชู ุงููุญุฏุฏุฉ. ุงูุดูุก ุงูุฑุงุฆุน ูู ุฃูู ูุณุช ุจุญุงุฌุฉ ุฅูู ุงูุฎุฑูุฌ ูู ุฅุทุงุฑ ุนูู Transformers ูุฅุฌุฑุงุก ูุฐู ุงูุชุบููุฑุงุช. ู ููููู ุชุนุฏูู ุงูููุงุฐุฌ ูุจุงุดุฑุฉู ูู Transformers ูุงูุงุณุชูุงุฏุฉ ูู ุงูููุฒุงุช ูุซู [ูุงุฌูุฉ ุจุฑูุฌุฉ ุงูุชุทุจููุงุช Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer)ุ ู [PreTrainedModel](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel)ุ ูุงูุถุจุท ุงูุฏููู ุงููุนุงู ุจุงุณุชุฎุฏุงู ุฃุฏูุงุช ูุซู [PEFT](https://huggingface.co/docs/peft/index).

ุณูุฑุดุฏู ูู ูุฐุง ุงูุฏููู  ูููููุฉ ุชุฎุตูุต ููุงุฐุฌ Transformers ุงูููุฌูุฏุฉ ูุชูุจูุฉ ูุชุทูุจุงุชูุ ุฏูู ููุฏุงู ูุฒุงูุง ุงูุฅุทุงุฑ. ุณุชุชุนูู ููููุฉ:

- ุชุนุฏูู ุจููุฉ ูููุฐุฌ ูุง ูู ุฎูุงู ุชุบููุฑ ุขููุฉ ุงูุงูุชุจุงู ุงูุฎุงุตุฉ ุจู.
- ุชุทุจูู ุชูููุงุช ูุซู Low-Rank Adaptation (LoRA) ุนูู ููููุงุช ูููุฐุฌ ูุญุฏุฏุฉ.

ูุญู ูุดุฌุนู ุนูู ุงููุณุงููุฉ ุจุงุฎุชุฑุงูุงุชู ุงูุฎุงุตุฉ ููุดุงุฑูุชูุง ููุง ูุน ุงููุฌุชูุน1

## ูุซุงู: ุชุนุฏูู ุขููุฉ ุงูุงูุชุจุงู ูู ูููุฐุฌ Segment Anything (SAM)

ูููุฐุฌ **Segment Anything (SAM)** ูู ูููุฐุฌ ุฑุงุฆุฏ ูู ูุฌุงู ุชุฌุฒุฆุฉ ุงูุตูุฑ. ูู ุชูููุฐู ุงูุงูุชุฑุงุถูุ ูุณุชุฎุฏู SAM ุฅุณูุงุทูุง ูุฌูุนูุง ููุงุณุชุนูุงู ูุงูููุชุงุญ ูุงููููุฉ (`qkv`) ูู ุขููุฉ ุงูุงูุชุจุงู ุงูุฎุงุตุฉ ุจู. ููุน ุฐููุ ูุฏ ุชุฑุบุจ ูู ุถุจุท ููููุงุช ูุญุฏุฏุฉ ููุท ูู ุขููุฉ ุงูุงูุชุจุงูุ ูุซู ุฅุณูุงุทุงุช ุงูุงุณุชุนูุงู (`q`) ูุงููููุฉ (`v`)ุ ูุชูููู ุนุฏุฏ ุงููุนููุงุช ุงููุงุจูุฉ ููุชุฏุฑูุจ ูุงูููุงุฑุฏ ุงูุญุณุงุจูุฉ ุงููุทููุจุฉ.

### ุงูุฏุงูุน

ูู ุฎูุงู ุชูุณูู ุงูุฅุณูุงุท ุงููุฌูุน `qkv` ุฅูู ุฅุณูุงุทุงุช ูููุตูุฉ `q` ู `k` ู `v`ุ ููููู ุชุทุจูู ุชูููุงุช ูุซู **LoRA** (Low-Rank Adaptation) ุนูู ุฅุณูุงุทู `q` ู `v` ููุท. ูุณูุญ ูู ูุฐุง ุจูุง ููู:

- ุถุจุท ุนุฏุฏ ุฃูู ูู ุงููุนููุงุชุ ููุง ูููู ูู ุงูุนุจุก ุงูุญุณุงุจู.
- ุชุญููู ุฃุฏุงุก ุฃูุถู ูู ุฎูุงู ุงูุชุฑููุฒ ุนูู ููููุงุช ูุญุฏุฏุฉ.
- ุชุฌุฑุจุฉ ุงุณุชุฑุงุชูุฌูุงุช ุชุนุฏูู ูุฎุชููุฉ ูู ุขููุฉ ุงูุงูุชุจุงู.

### ุงูุชูููุฐ

#### **ุงูุฎุทูุฉ 1: ุฅูุดุงุก ูุฆุฉ ุงูุชูุงู ูุฎุตุตุฉ**

ุจุนุฏ ุฐููุ ูู ุจุฅูุดุงุก ูุฆุฉ ูุฑุนูุฉ ูู ูุฆุฉ `SamVisionAttention` ุงูุฃุตููุฉ ูุนุฏููุง ูุชุถู ุฅุณูุงุทุงุช `q` ู `k` ู `v` ูููุตูุฉ.

```python
import torch
import torch.nn as nn
from transformers.models.sam.modeling_sam import SamVisionAttention

class SamVisionAttentionSplit(SamVisionAttention, nn.Module):
    def __init__(self, config, window_size):
        super().__init__(config, window_size)
        del self.qkv
        # ุฅุณูุงุทุงุช ูููุตูุฉ q ู k ู v
        self.q = nn.Linear(config.hidden_size, config.hidden_size, bias=config.qkv_bias)
        self.k = nn.Linear(config.hidden_size, config.hidden_size, bias=config.qkv_bias)
        self.v = nn.Linear(config.hidden_size, config.hidden_size, bias=config.qkv_bias)
        self._register_load_state_dict_pre_hook(self.split_q_k_v_load_hook)

    def split_q_k_v_load_hook(self, state_dict, prefix, *args):
        keys_to_delete = []
        for key in list(state_dict.keys()):
            if "qkv." in key:
                # ุชูุณูู q ู k ู v ูู ุงูุฅุณูุงุท ุงููุฌูุน
                q, k, v = state_dict[key].chunk(3, dim=0)
                # ุงุณุชุจุฏุงู ุงูุฅุณูุงุทุงุช ุงููุฑุฏูุฉ q ู k ู v
                state_dict[key.replace("qkv.", "q.")] = q
                state_dict[key.replace("qkv.", "k.")] = k
                state_dict[key.replace("qkv.", "v.")] = v
                # ูุถุน ุนูุงูุฉ ุนูู ููุชุงุญ qkv ุงููุฏูู ููุญุฐู
                keys_to_delete.append(key)
        
        # ุญุฐู ููุงุชูุญ qkv ุงููุฏููุฉ
        for key in keys_to_delete:
            del state_dict[key]

    def forward(self, hidden_states: torch.Tensor, output_attentions=False) -> torch.Tensor:
        batch_size, height, width, _ = hidden_states.shape
        qkv_shapes = (batch_size *  self.num_attention_heads,  height * width, -1)
        query = self.q(hidden_states).reshape((batch_size,  height * width,self.num_attention_heads, -1)).permute(0,2,1,3).reshape(qkv_shapes)
        key = self.k(hidden_states).reshape((batch_size,  height * width,self.num_attention_heads, -1)).permute(0,2,1,3).reshape(qkv_shapes)
        value = self.v(hidden_states).reshape((batch_size,  height * width,self.num_attention_heads, -1)).permute(0,2,1,3).reshape(qkv_shapes)

        attn_weights = (query * self.scale) @ key.transpose(-2, -1)

        if self.use_rel_pos:
            attn_weights = self.add_decomposed_rel_pos(
                attn_weights, query, self.rel_pos_h, self.rel_pos_w, (height, width), (height, width)
            )

        attn_weights = torch.nn.functional.softmax(attn_weights, dtype=torch.float32, dim=-1).to(query.dtype)
        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)
        attn_output = (attn_probs @ value).reshape(batch_size, self.num_attention_heads, height, width, -1)
        attn_output = attn_output.permute(0, 2, 3, 1, 4).reshape(batch_size, height, width, -1)
        attn_output = self.proj(attn_output)

        if output_attentions:
            outputs = (attn_output, attn_weights)
        else:
            outputs = (attn_output, None)
        return outputs
```

**ุงูุดุฑุญ:**

- **ุงูุฅุณูุงุทุงุช ุงููููุตูุฉ:** ูุชู ุฅุฒุงูุฉ ุงูุฅุณูุงุท ุงูููุฌูุน `qkv`ุ ูุฅูุดุงุก ุฅุณูุงุทุงุช ุฎุทูุฉ ูููุตูุฉ `q` ู `k` ู `v`.
- **ุฏุงูุฉ ุงุณุชุฏุนุงุก  ุชุญููู ุงูุฃูุฒุงู:** ุชููู ุทุฑููุฉ `_split_qkv_load_hook` ุจุชูุณูู ุฃูุฒุงู `qkv` ุงููุณุจูุฉ ุงูุชุฏุฑูุจ ุฅูู ุฃูุฒุงู `q` ู `k` ู `v` ูููุตูุฉ ุนูุฏ ุชุญููู ุงููููุฐุฌ. ูุถูู ูุฐุง ุงูุชูุงูู ูุน ุฃู ูููุฐุฌ ูุณุจู ุงูุชุฏุฑูุจ.
- **ุงูุชูููุฐ ุงูุฃูุงูู:** ูุชู ุญุณุงุจ ุงูุงุณุชุนูุงูุงุช ูุงูููุงุชูุญ ูุงูููู ุจุดูู ูููุตูุ ูุชุณุชูุฑ ุขููุฉ ุงูุงูุชุจุงู ูุงููุนุชุงุฏ.

#### **ุงูุฎุทูุฉ 2: ุงุณุชุจุฏุงู ูุฆุฉ ุงูุงูุชุจุงู ุงูุฃุตููุฉ**

ุงุณุชุจุฏู ูุฆุฉ `SamVisionAttention` ุงูุฃุตููุฉ ุจูุฆุชู ุงููุฎุตุตุฉ ุจุญูุซ ูุณุชุฎุฏู ุงููููุฐุฌ ุขููุฉ ุงูุงูุชุจุงู ุงููุนุฏูุฉ.

```python
from transformers import SamModel
from transformers.models.sam import modeling_sam

# ุงุณุชุจุฏุงู ูุฆุฉ ุงูุงูุชูุงู ูู ูุญุฏุฉ ููุทูุฉ modeling_sam
modeling_sam.SamVisionAttention = SamVisionAttentionSplit

# ุชุญููู ูููุฐุฌ SAM ุงููุณุจู ุงูุชุฏุฑูุจ
model = SamModel.from_pretrained("facebook/sam-vit-base")
```

**ุงูุดุฑุญ:**

- **ุงุณุชุจุฏุงู ุงููุฆุฉ:** ูู ุฎูุงู ุชุนููู ูุฆุชู ุงููุฎุตุตุฉ ุฅูู `modeling_sam.SamVisionAttention`ุ ูุฅู ุฃู ุญุงูุงุช ูู ูุฆุฉ `SamVisionAttention` ูู ุงููููุฐุฌ ุณุชุณุชุฎุฏู ุงููุณุฎุฉ ุงููุนุฏูุฉ. ูุจุงูุชุงููุ ุนูุฏ ุงุณุชุฏุนุงุก `SamModel`ุ ุณูุชู ุงุณุชุฎุฏุงู `SamVisionAttentionSplit` ุงููุญุฏุฏุฉ ุญุฏูุซูุง.
- **ุชุญููู ุงููููุฐุฌ:** ูุชู ุชุญููู ุงููููุฐุฌ ุจุงุณุชุฎุฏุงู `from_pretrained`ุ ููุชู ุฏูุฌ ุขููุฉ ุงูุงูุชุจุงู ุงููุฎุตุตุฉ.

#### **ุงูุฎุทูุฉ 3: ุชุทุจูู LoRA ุนูู ุฅุณูุงุทุงุช ูุญุฏุฏุฉ**

ูุน ูุฌูุฏ ุฅุณูุงุทุงุช `q` ู `k` ู `v` ูููุตูุฉุ ููููู ุงูุขู ุชุทุจูู LoRA ุนูู ููููุงุช ูุญุฏุฏุฉุ ูุซู ุฅุณูุงุทุงุช `q` ู `v`.

```python
from peft import LoraConfig, get_peft_model

config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q", "v"],  # ุชุทุจูู LoRA ุนูู ุฅุณูุงุทุงุช q ู v
    lora_dropout=0.1,
    task_type="mask-generation"
)

# ุชุทุจูู LoRA ุนูู ุงููููุฐุฌ
model = get_peft_model(model, config)
```

**ุงูุดุฑุญ:**

- **ุชูููู LoRA:** ุชุญุฏุฏ `LoraConfig` ุงููุฑุชุจุฉ `r`ุ ูุนุงูู ุงูููุงุณ `lora_alpha`ุ ูุงููุญุฏุงุช ุงููุณุชูุฏูุฉ (`"q"` ู `"v"`)ุ ููุนุฏู ุงูุชุฎููุ ูููุน ุงููููุฉ.
- **ุชุทุจูู LoRA:** ุชููู ุฏุงูุฉ `get_peft_model` ุจุชุทุจูู LoRA ุนูู ุงููุญุฏุงุช ุงููุญุฏุฏุฉ ูู ุงููููุฐุฌ.
- **ุชูููู ุงููุนููุงุช:** ูู ุฎูุงู ุงูุชุฑููุฒ ุนูู `q` ู `v`ุ ูุฅูู ุชููู ุนุฏุฏ ุงููุนููุงุช ุงููุงุจูุฉ ููุชุฏุฑูุจุ ููุง ูุคุฏู ุฅูู ุชุณุฑูุน ุงูุชุฏุฑูุจ ูุชูููู ุงุณุชุฎุฏุงู ุงูุฐุงูุฑุฉ.

#### **ุงูุฎุทูุฉ 4: ุงูุชุญูู ูู ุนุฏุฏ ุงููุนููุงุช ุงููุงุจูุฉ ููุชุฏุฑูุจ**

ูู ุงูุณูู ุงูุชุญูู ูู ุนุฏุฏ ุงููุนููุงุช ุงููุงุจูุฉ ููุชุฏุฑูุจ ููุนุฑูุฉ ุชุฃุซูุฑ ุชุนุฏููู.

```python
model.print_trainable_parameters()
```

**ุงููุงุชุฌ ุงููุชููุน:**

```
ุนุฏุฏ ุงููุนููุงุช ุงููุงุจูุฉ ููุชุฏุฑูุจ: 608,256 || ุฌููุน ุงููุนููุงุช: 94,343,728 || ูุณุจุฉ ุงููุนููุงุช ุงููุงุจูุฉ ููุชุฏุฑูุจ: 0.6447
ุนุฏุฏ ุงููุนููุงุช ุงููุงุจูุฉ ููุชุฏุฑูุจ: 912,384 || ุฌููุน ุงููุนููุงุช: 94,647,856 || ูุณุจุฉ ุงููุนููุงุช ุงููุงุจูุฉ ููุชุฏุฑูุจ: 0.9640 # ูุน k
```

## ุงููุณุงููุฉ ุจุงุจุฏุงุนุงุชู ุงูุฎุงุตุฉ

ูููู ูุชุนุฏูู ุงูููุงุฐุฌ ุงููุณุจูุฉ ุงูุชุฏุฑูุจ ุฃู ููุชุญ ุขูุงููุง ุฌุฏูุฏุฉ ููุจุญุซ ูุงูุชุทุจูู. ูู ุฎูุงู ููู ูุชุนุฏูู ุงูุขููุงุช ุงูุฏุงุฎููุฉ ููููุงุฐุฌ ูุซู SAMุ ููููู ุชุฎุตูุตูุง ูุชูุจูุฉ ุงุญุชูุงุฌุงุชู ุงููุญุฏุฏุฉุ ูุชุญุณูู ุงูุฃุฏุงุกุ ูุชุฌุฑุจุฉ ุฃููุงุฑ ุฌุฏูุฏุฉ.

ุฅุฐุง ููุช ุจุชุทููุฑ ุชุนุฏู๏ปปุชู ุงูุฎุงุตุฉ ูููุงุฐุฌ Transformers ูุชุฑุบุจ ูู ูุดุงุฑูุชูุงุ ูููุฑ ูู ุงููุณุงููุฉ ูู ูุฐู ุงููุซููุฉ.

- **ุฅูุดุงุก ุทูุจ ุณุญุจ (Pull Request):** ุดุงุฑู ุชุบููุฑุงุชู ูุชุญุณููุงุชู ูู ุงูุชุนูููุงุช ุงูุจุฑูุฌูุฉ ูุจุงุดุฑุฉ ูู ุงููุณุชูุฏุน.
- **ูุชุงุจุฉ ุงูุชูุซูู:** ูุฏู ุชูุณูุฑุงุช ูุฃูุซูุฉ ูุงุถุญุฉ ูุชุนุฏููุงุชู.
- **ุงูุชูุงุนู ูุน ุงููุฌุชูุน:** ูุงูุด ุฃููุงุฑู ูุงุญุตู ุนูู ุชุนูููุงุช ูู ุงููุทูุฑูู ูุงูุจุงุญุซูู ุงูุขุฎุฑูู ูู ุฎูุงู ูุชุญ ูุดููุฉ.
