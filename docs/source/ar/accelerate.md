Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¹Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ ðŸ¤— Transformers Ø¨Ø³Ù‡ÙˆÙ„Ø© Ø¹Ù„Ù‰ Ø£ÙŠ Ù†ÙˆØ¹ Ù…Ù† Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…ÙˆØ²Ø¹Ø©ØŒ Ø³ÙˆØ§Ø¡ ÙƒØ§Ù† Ø°Ù„Ùƒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ­Ø¯Ø§Øª GPU Ù…ØªØ¹Ø¯Ø¯Ø© Ø¹Ù„Ù‰ Ø¬Ù‡Ø§Ø² ÙˆØ§Ø­Ø¯ Ø£Ùˆ ÙˆØ­Ø¯Ø§Øª GPU Ù…ØªØ¹Ø¯Ø¯Ø© Ø¹Ø¨Ø± Ø£Ø¬Ù‡Ø²Ø© Ù…ØªØ¹Ø¯Ø¯Ø©ØŒ Ø£Ù†Ø´Ø£Ù†Ø§ ÙÙŠ Hugging Face Ù…ÙƒØªØ¨Ø© [ðŸ¤— Accelerate](https://huggingface.co/docs/accelerate). ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØŒ ØªØ¹Ù„Ù… ÙƒÙŠÙÙŠØ© ØªØ®ØµÙŠØµ Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø£ØµÙ„ÙŠØ© ÙÙŠ PyTorch Ù„ØªÙ…ÙƒÙŠÙ† Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙÙŠ Ø¨ÙŠØ¦Ø© Ù…ÙˆØ²Ø¹Ø©.

## Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯

Ø§Ø¨Ø¯Ø£ Ø¨ØªØ«Ø¨ÙŠØª ðŸ¤— Accelerate:

```bash
pip install accelerate
```

Ø«Ù… Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ ÙˆØ¥Ù†Ø´Ø§Ø¡ ÙƒØ§Ø¦Ù† [`~accelerate.Accelerator`]. Ø³ÙŠÙ‚ÙˆÙ… [`~accelerate.Accelerator`] ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¨Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ù†ÙˆØ¹ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ÙˆØ²Ø¹ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ ÙˆØªÙ‡ÙŠØ¦Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨. Ù„Ø§ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ÙˆØ¶Ø¹ Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø¹Ù„Ù‰ Ø¬Ù‡Ø§Ø² Ø¨Ø´ÙƒÙ„ ØµØ±ÙŠØ­.

```py
>>> from accelerate import Accelerator

>>> accelerator = Accelerator()
```

## Ø§Ù„Ø§Ø³ØªØ¹Ø¯Ø§Ø¯ Ù„Ù„ØªØ³Ø±ÙŠØ¹

Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù‡ÙŠ ØªÙ…Ø±ÙŠØ± Ø¬Ù…ÙŠØ¹ ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ø¥Ù„Ù‰ Ø·Ø±ÙŠÙ‚Ø© [`~accelerate.Accelerator.prepare`]. ÙˆÙŠØ´Ù…Ù„ Ø°Ù„Ùƒ DataLoaders Ù„Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ…ØŒ ÙˆÙ†Ù…ÙˆØ°Ø¬Ù‹Ø§ ÙˆÙ…ÙØ­ÙŽØ³ÙÙ‘Ù†Ù‹Ø§:

```py
>>> train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
...     train_dataloader, eval_dataloader, model, optimizer
... )
```

## Ø§Ù„Ø®Ù„ÙÙŠ

Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø£Ø®ÙŠØ±Ø© Ù‡ÙŠ Ø§Ø³ØªØ¨Ø¯Ø§Ù„ `loss.backward()` Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ÙŠ ÙÙŠ Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø¨Ø£Ø³Ù„ÙˆØ¨ [`~accelerate.Accelerator.backward`] ÙÙŠ ðŸ¤— Accelerate:

```py
>>> for epoch in range(num_epochs):
...     for batch in train_dataloader:
...         outputs = model(**batch)
...         loss = outputs.loss
...         accelerator.backward(loss)

...         optimizer.step()
...         lr_scheduler.step()
...         optimizer.zero_grad()
...         progress_bar.update(1)
```

ÙƒÙ…Ø§ ÙŠÙ…ÙƒÙ†Ùƒ Ø£Ù† ØªØ±Ù‰ ÙÙŠ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„ØªØ§Ù„ÙŠØŒ ÙØ£Ù†Øª Ø¨Ø­Ø§Ø¬Ø© ÙÙ‚Ø· Ø¥Ù„Ù‰ Ø¥Ø¶Ø§ÙØ© Ø£Ø±Ø¨Ø¹Ø© Ø£Ø³Ø·Ø± Ù…Ù† Ø§Ù„ÙƒÙˆØ¯ Ø¥Ù„Ù‰ Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ù„ØªÙ…ÙƒÙŠÙ† Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ÙˆØ²Ø¹!

```diff
+ from accelerate import Accelerator
  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

+ accelerator = Accelerator()

  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
  optimizer = AdamW(model.parameters(), lr=3e-5)

- device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
- model.to(device)

+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
+     train_dataloader, eval_dataloader, model, optimizer
+ )

  num_epochs = 3
  num_training_steps = num_epochs * len(train_dataloader)
  lr_scheduler = get_scheduler(
      "linear",
      optimizer=optimizer,
      num_warmup_steps=0,
      num_training_steps=num_training_steps
  )

  progress_bar = tqdm(range(num_training_steps))

  model.train()
  for epoch in range(num_epochs):
      for batch in train_dataloader:
-         batch = {k: v.to(device) for k, v in batch.items()}
          outputs = model(**batch)
          loss = outputs.loss
-         loss.backward()
+         accelerator.backward(loss)
optimizer.step()
          lr_scheduler.step()
          optimizer.zero_grad()
          progress_bar.update(1)
```

## ØªØ¯Ø±ÙŠØ¨

Ø¨Ù…Ø¬Ø±Ø¯ Ø¥Ø¶Ø§ÙØ© Ø£Ø³Ø·Ø± Ø§Ù„ÙƒÙˆØ¯ Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©ØŒ Ù‚Ù… Ø¨ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ ÙÙŠ Ø£Ø­Ø¯ Ø§Ù„Ù†ØµÙˆØµ Ø£Ùˆ Ø§Ù„Ø¯ÙØ§ØªØ± Ù…Ø«Ù„ Colaboratory.

### Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Øµ Ø¨Ø±Ù…Ø¬ÙŠ

Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ´ØºÙ„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ù…Ù† Ù†Øµ Ø¨Ø±Ù…Ø¬ÙŠØŒ ÙÙ‚Ù… Ø¨ØªØ´ØºÙŠÙ„ Ø§Ù„Ø£Ù…Ø± Ø§Ù„ØªØ§Ù„ÙŠ Ù„Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ­ÙØ¸ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ†:

```bash
accelerate config
```

Ø«Ù… Ù‚Ù… Ø¨ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù…:

```bash
accelerate launch train.py
```

### Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¯ÙØªØ± Ù…Ù„Ø§Ø­Ø¸Ø§Øª

ÙŠÙ…ÙƒÙ† Ø£ÙŠØ¶Ù‹Ø§ ØªØ´ØºÙŠÙ„ ðŸ¤— Accelerate ÙÙŠ Ø¯ÙØªØ± Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ®Ø·Ø· Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…ÙŠØ§Øª (TPUs) ÙÙŠ Colaboratory. Ù‚Ù… Ø¨ØªØºÙ„ÙŠÙ ÙƒÙ„ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„ Ø¹Ù† Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙÙŠ Ø¯Ø§Ù„Ø©ØŒ ÙˆÙ…Ø±Ø±Ù‡Ø§ Ø¥Ù„Ù‰ [`~accelerate.notebook_launcher`]:

```py
>>> from accelerate import notebook_launcher

>>> notebook_launcher(training_function)
```

Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø­ÙˆÙ„ ðŸ¤— Accelerate ÙˆÙ…ÙŠØ²Ø§ØªÙ‡ Ø§Ù„ØºÙ†ÙŠØ©ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„Ù‰ [Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚](https://huggingface.co/docs/accelerate).