# Tiktoken ูุงูุชูุงุนู ูุน Transformers

ูุชู ุฏูุฌ ุฏุนู ูููุงุช ูููุฐุฌ tiktoken ุจุณูุงุณุฉ ูู ๐ค transformers ุนูุฏ ุชุญููู ุงูููุงุฐุฌ
`from_pretrained` ูุน ููู `tokenizer.model` tiktoken ุนูู Hubุ ูุงูุฐู ูุชู ุชุญูููู ุชููุงุฆููุง ุฅูู [ุงููุญูู ุงููุบูู ุงูุณุฑูุน](https://huggingface.co/docs/transformers/main/en/main_classes/tokenizer#transformers.PythonBackend).

### ุงูููุงุฐุฌ ุงููุนุฑููุฉ ุงูุชู ุชู ุฅุตุฏุงุฑูุง ูุน `tiktoken.model`:
	- gpt2
	- llama3

## ูุซุงู ุนูู ุงูุงุณุชุฎุฏุงู

ูู ุฃุฌู ุชุญููู ูููุงุช `tiktoken` ูู `transformers`ุ ุชุฃูุฏ ูู ุฃู ููู `tokenizer.model` ูู ููู tiktoken ูุณูุชู ุชุญูููู ุชููุงุฆููุง ุนูุฏ ุงูุชุญููู `from_pretrained`. ุฅููู ููููุฉ ุชุญููู ูุฌุฒูุก ูุบูู ููููุฐุฌุ ูุงูุฐู
ูููู ุชุญูููู ูู ููุณ ุงูููู ุจุงูุถุจุท:

```py
from transformers import AutoTokenizer

model_id = "meta-llama/Meta-Llama-3-8B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_id, subfolder="original")
```
## ุฅูุดุงุก ูุฌุฒูุก ูุบูู tiktoken

ูุง ูุญุชูู ููู `tokenizer.model` ุนูู ุฃู ูุนูููุงุช ุญูู ุงูุฑููุฒ ุฃู ุงูุฃููุงุท ุงูุฅุถุงููุฉ. ุฅุฐุง ูุงูุช ูุฐู ุงูุฃููุฑ ูููุฉุ ูู ุจุชุญููู ุงููุญูู ุงููุบูู ุฅูู `tokenizer.json`ุ ููู ุงูุชูุณูู ุงูููุงุณุจ ูู [`PythonBackend`].

ูู ุจุชูููุฏ ููู `tokenizer.model` ุจุงุณุชุฎุฏุงู [tiktoken.get_encoding](https://github.com/openai/tiktoken/blob/63527649963def8c759b0f91f2eb69a40934e468/tiktoken/registry.py#L63) ุซู ูู ุจุชุญูููู ุฅูู `tokenizer.json` ุจุงุณุชุฎุฏุงู [`convert_tiktoken_to_fast`].

```py

from transformers.integrations.tiktoken import convert_tiktoken_to_fast
from tiktoken import get_encoding

# ููููู ุชุญููู ุชุฑููุฒู ุงููุฎุตุต ุฃู ุงูุชุฑููุฒ ุงูุฐู ุชููุฑู OpenAI
encoding = get_encoding("gpt2")
convert_tiktoken_to_fast(encoding, "config/save/dir")
```

ูุชู ุญูุธ ููู `tokenizer.json` ุงููุงุชุฌ ูู ุงูุฏููู ุงููุญุฏุฏ ููููู ุชุญูููู ุจุงุณุชุฎุฏุงู [`PythonBackend`].

```py
tokenizer = PythonBackend.from_pretrained("config/save/dir")
```
