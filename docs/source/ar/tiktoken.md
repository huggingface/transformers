# Tiktoken ÙˆØ§Ù„ØªÙØ§Ø¹Ù„ Ù…Ø¹ Transformers

ÙŠØªÙ… Ø¯Ù…Ø¬ Ø¯Ø¹Ù… Ù…Ù„ÙØ§Øª Ù†Ù…ÙˆØ°Ø¬ tiktoken Ø¨Ø³Ù„Ø§Ø³Ø© ÙÙŠ ğŸ¤— transformers Ø¹Ù†Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
`from_pretrained` Ù…Ø¹ Ù…Ù„Ù `tokenizer.model` tiktoken Ø¹Ù„Ù‰ HubØŒ ÙˆØ§Ù„Ø°ÙŠ ÙŠØªÙ… ØªØ­ÙˆÙŠÙ„Ù‡ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¥Ù„Ù‰ [Ø§Ù„Ù…Ø­Ù„Ù„ Ø§Ù„Ù„ØºÙˆÙŠ Ø§Ù„Ø³Ø±ÙŠØ¹](https://huggingface.co/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast).

### Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ© Ø§Ù„ØªÙŠ ØªÙ… Ø¥ØµØ¯Ø§Ø±Ù‡Ø§ Ù…Ø¹ `tiktoken.model`:
	- gpt2
	- llama3

## Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…

Ù…Ù† Ø£Ø¬Ù„ ØªØ­Ù…ÙŠÙ„ Ù…Ù„ÙØ§Øª `tiktoken` ÙÙŠ `transformers`ØŒ ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ù…Ù„Ù `tokenizer.model` Ù‡Ùˆ Ù…Ù„Ù tiktoken ÙˆØ³ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¹Ù†Ø¯ Ø§Ù„ØªØ­Ù…ÙŠÙ„ `from_pretrained`. Ø¥Ù„ÙŠÙƒ ÙƒÙŠÙÙŠØ© ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ø²Ù‰Ø¡ Ù„ØºÙˆÙŠ ÙˆÙ†Ù…ÙˆØ°Ø¬ØŒ ÙˆØ§Ù„Ø°ÙŠ
ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„Ù‡ Ù…Ù† Ù†ÙØ³ Ø§Ù„Ù…Ù„Ù Ø¨Ø§Ù„Ø¶Ø¨Ø·:

```py
from transformers import AutoTokenizer

model_id = "meta-llama/Meta-Llama-3-8B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_id, subfolder="original")
```
## Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ø²Ù‰Ø¡ Ù„ØºÙˆÙŠ tiktoken

Ù„Ø§ ÙŠØ­ØªÙˆÙŠ Ù…Ù„Ù `tokenizer.model` Ø¹Ù„Ù‰ Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø­ÙˆÙ„ Ø§Ù„Ø±Ù…ÙˆØ² Ø£Ùˆ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ©. Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù‡Ø°Ù‡ Ø§Ù„Ø£Ù…ÙˆØ± Ù…Ù‡Ù…Ø©ØŒ Ù‚Ù… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ø­Ù„Ù„ Ø§Ù„Ù„ØºÙˆÙŠ Ø¥Ù„Ù‰ `tokenizer.json`ØŒ ÙˆÙ‡Ùˆ Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ù„Ù€ [`PreTrainedTokenizerFast`].

Ù‚Ù… Ø¨ØªÙˆÙ„ÙŠØ¯ Ù…Ù„Ù `tokenizer.model` Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [tiktoken.get_encoding](https://github.com/openai/tiktoken/blob/63527649963def8c759b0f91f2eb69a40934e468/tiktoken/registry.py#L63) Ø«Ù… Ù‚Ù… Ø¨ØªØ­ÙˆÙŠÙ„Ù‡ Ø¥Ù„Ù‰ `tokenizer.json` Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`convert_tiktoken_to_fast`].

```py

from transformers.integrations.tiktoken import convert_tiktoken_to_fast
from tiktoken import get_encoding

# ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ ØªØ±Ù…ÙŠØ²Ùƒ Ø§Ù„Ù…Ø®ØµØµ Ø£Ùˆ Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ø°ÙŠ ØªÙˆÙØ±Ù‡ OpenAI
encoding = get_encoding("gpt2")
convert_tiktoken_to_fast(encoding, "config/save/dir")
```

ÙŠØªÙ… Ø­ÙØ¸ Ù…Ù„Ù `tokenizer.json` Ø§Ù„Ù†Ø§ØªØ¬ ÙÙŠ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­Ø¯Ø¯ ÙˆÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`PreTrainedTokenizerFast`].

```py
tokenizer = PreTrainedTokenizerFast.from_pretrained("config/save/dir")
```
