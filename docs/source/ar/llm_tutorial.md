# ุงูุชูููุฏ ุจุงุณุชุฎุฏุงู ููุงุฐุฌ ุงููุบุงุช ุงููุจูุฑุฉ (LLMs)

[[open-in-colab]]

ุชุนุฏ LLMsุ ุฃู ููุงุฐุฌ ุงููุบุฉ ุงููุจูุฑุฉุ ุงููููู ุงูุฑุฆูุณู ูุฑุงุก ุชูููุฏ ุงููุตูุต. ูุจุงุฎุชุตุงุฑุ ุชุชููู ูู ููุงุฐุฌ ูุญูู ูุจูุฑุฉ ูุณุจูุฉ ุงูุชุฏุฑูุจ ุชู ุชุฏุฑูุจูุง ููุชูุจุค ุจุงููููุฉ ุงูุชุงููุฉ (ุฃูุ ุจุดูู ุฃูุซุฑ ุฏูุฉุ ุงูุฑูุฒ ุงููุบูู) ุจุงููุธุฑ ุฅูู ูุต ูุนูู. ูุธุฑูุง ูุฃููุง ุชุชูุจุฃ ุจุฑูุฒ ูุงุญุฏ ูู ูู ูุฑุฉุ ูุฌุจ ุนููู ุงูููุงู ุจุดูุก ุฃูุซุฑ ุชุนููุฏูุง ูุชูููุฏ ุฌูู ุฌุฏูุฏุฉ ุจุฎูุงู ูุฌุฑุฏ ุงุณุชุฏุนุงุก ุงููููุฐุฌ - ูุฌุจ ุนููู ุฅุฌุฑุงุก ุงูุชูููุฏ ุงูุชููุงุฆู.

ุงูุชูููุฏ ุงูุชููุงุฆู ูู ุฅุฌุฑุงุก ููุช ุงูุงุณุชุฏูุงู ุงูุฐู ูุชุถูู ุงุณุชุฏุนุงุก ุงููููุฐุฌ ุจุดูู ูุชูุฑุฑ ุจุงุณุชุฎุฏุงู ูุฎุฑุฌุงุชู ุงูุฎุงุตุฉุ ุจุงููุธุฑ ุฅูู ุจุนุถ ุงููุฏุฎูุงุช ุงูุฃูููุฉ. ูู ๐ค Transformersุ ูุชู ุงูุชุนุงูู ูุน ูุฐุง ุจูุงุณุทุฉ ุฏุงูุฉ [`~generation.GenerationMixin.generate`]ุ ูุงูุชู ุชุชููุฑ ูุฌููุน ุงูููุงุฐุฌ ุฐุงุช ุงููุฏุฑุงุช ุงูุชูููุฏูุฉ.

ุณููุถุญ ูุฐุง ุงูุจุฑูุงูุฌ ุงูุชุนูููู ููููุฉ:

* ุชุชูููุฏ ูุต ุจุงุณุชุฎุฏุงู ูููุฐุฌ ุงููุบุงุช ุงููุจูุฑุฉ (LLM)
* ุชุฌูุจ ุงููููุน ูู ุงูุฃุฎุทุงุก ุงูุดุงุฆุนุฉ
* ุงูุฎุทูุงุช ุงูุชุงููุฉ ููุณุงุนุฏุชู ูู ุงูุงุณุชูุงุฏุฉ ุงููุตูู ูู LLM ุงูุฎุงุต ุจู

ูุจู ุงูุจุฏุกุ ุชุฃูุฏ ูู ุชุซุจูุช ุฌููุน ุงูููุชุจุงุช ุงูุถุฑูุฑูุฉ:

```bash
pip install transformers bitsandbytes>=0.39.0 -q
```

## ุชูููุฏ ุงููุต

ูุฃุฎุฐ ูููุฐุฌ ุงููุบุฉ ุงููุฏุฑุจ ูู [ููุฐุฌุฉ ุงููุบุฉ ุงูุณุจุจูุฉ](tasks/language_modeling) ูุฃุฎุฐ ุชุณูุณููุง ูู ุฑููุฒ ูุตูุฉ ููุฏุฎู ููุนูุฏ ุชูุฒูุน ุงูุงุญุชูุงููุฉ ููุฑูุฒ ุงูุชุงูู.

<!-- [GIF 1 -- FWD PASS] -->
<figure class="image table text-center m-0 w-full">
    <video
        style="max-width: 90%; margin: auto;"
        autoplay loop muted playsinline
        src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_1_1080p.mov"
    ></video>
    <figcaption>"ุงูุชูุจุค ุจุงููููุฉ ุงูุชุงููุฉ ููููุฐุฌ ุงููุบุฉ (LLM)"</figcaption>
</figure>

ููุงู ุฌุงูุจ ุจุงูุบ ุงูุฃูููุฉ ูู ุงูุชูููุฏ ุงูุชููุงุฆู ุจุงุณุชุฎุฏุงู LLMs ููู ููููุฉ ุงุฎุชูุงุฑ ุงูุฑูุฒ ุงูุชุงูู ูู ุชูุฒูุน ุงูุงุญุชูุงููุฉ ูุฐุง. ูู ุดูุก ูุณููุญ ุจู ูู ูุฐู ุงูุฎุทูุฉ ุทุงููุง ุฃูู ุชูุชูู ุจุฑูุฒ ููุชูุฑุงุฑ ุงูุชุงูู. ููุฐุง ูุนูู ุฃูู ูููู ุฃู ูููู ุจุณูุทูุง ูุซู ุงุฎุชูุงุฑ ุงูุฑูุฒ ุงูุฃูุซุฑ ุงุญุชูุงููุง ูู ุชูุฒูุน ุงูุงุญุชูุงููุฉ ุฃู ูุนูุฏูุง ูุซู ุชุทุจูู ุนุดุฑุงุช ุงูุชุญููุงุช ูุจู ุฃุฎุฐ ุงูุนููุงุช ูู ุงูุชูุฒูุน ุงููุงุชุฌ.

<!-- [GIF 2 -- TEXT GENERATION] -->
<figure class="image table text-center m-0 w-full">
    <video
        style="max-width: 90%; margin: auto;"
        autoplay loop muted playsinline
        src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_2_1080p.mov"
    ></video>
    <figcaption>"ุงูุชูููุฏ ุงูุชููุงุฆู ุงููุชุณูุณู"</figcaption>
</figure>

ุชุชูุฑุฑ ุงูุนูููุฉ ุงูููุถุญุฉ ุฃุนูุงู ุจุดูู ุชูุฑุงุฑู ุญุชู ูุชู ุงููุตูู ุฅูู ุดุฑุท ุงูุชููู. ูู ุงููุถุน ุงููุซุงููุ ูุญุฏุฏ ุงููููุฐุฌ ุดุฑุท ุงูุชูููุ ูุงูุฐู ูุฌุจ ุฃู ูุชุนูู ุนูุฏ ุฅุฎุฑุงุฌ ุฑูุฒ ููุงูุฉ ุงูุชุณูุณู (`EOS`). ุฅุฐุง ูู ููู ุงูุฃูุฑ ูุฐููุ ูุชููู ุงูุชูููุฏ ุนูุฏ ุงููุตูู ุฅูู ุทูู ุฃูุตู ูุญุฏุฏ ูุณุจููุง.

ูู ุงูุถุฑูุฑู ุฅุนุฏุงุฏ ุฎุทูุฉ ุงุฎุชูุงุฑ ุงูุฑูุฒ ูุดุฑุท ุงูุชููู ุจุดูู ุตุญูุญ ูุฌุนู ูููุฐุฌู ูุชุตุฑู ููุง ุชุชููุน ูู ูููุชู. ูููุฐุง ุงูุณุจุจ ูุฏููุง [`~generation.GenerationConfig`] ููู ูุฑุชุจุท ุจูู ูููุฐุฌุ ูุงูุฐู ูุญุชูู ุนูู ูุนููุฉ ุชูููุฏูุฉ ุงูุชุฑุงุถูุฉ ุฌูุฏุฉ ููุชู ุชุญูููู ุฌูุจูุง ุฅูู ุฌูุจ ูุน ูููุฐุฌู.

ุฏุนูุง ูุชุญุฏุซ ุนู ุงูููุฏ!


<Tip>

ุฅุฐุง ููุช ููุชููุง ุจุงูุงุณุชุฎุฏุงู ุงูุฃุณุงุณู ูู LLMุ ูุฅู ูุงุฌูุฉ [`Pipeline`](pipeline_tutorial) ุนุงููุฉ ุงููุณุชูู ูู ููุทุฉ ุงูุทูุงู ุฑุงุฆุนุฉ. ููุน ุฐููุ ุบุงูุจูุง ูุง ุชุชุทูุจ LLMs ููุฒุงุช ูุชูุฏูุฉ ูุซู ุงูุชูููู ูุงูุชุญูู ุงูุฏููู ูู ุฎุทูุฉ ุงุฎุชูุงุฑ ุงูุฑูุฒุ ูุงูุชู ูุชู ุชูููุฐูุง ุจุดูู ุฃูุถู ูู ุฎูุงู [`~generation.GenerationMixin.generate`]. ุงูุชูููุฏ ุงูุชููุงุฆู ุจุงุณุชุฎุฏุงู LLMs  ูุณุชููู ุงููุซูุฑ ูู ุงูููุงุฑุฏุฏ ููุฌุจ ุชูููุฐู ุนูู ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณููุงุช ููุญุตูู ุนูู ุฃุฏุงุก ูุงูู.

</Tip>

ุฃููุงูุ ุชุญุชุงุฌ ุฅูู ุชุญููู ุงููููุฐุฌ.

```py
>>> from transformers import AutoModelForCausalLM

>>> model = AutoModelForCausalLM.from_pretrained(
...     "mistralai/Mistral-7B-v0.1", device_map="auto", load_in_4bit=True
... )
```

ุณุชูุงุญุธ ูุฌูุฏ ูุนุงูููู ูู ุงูุงุณุชุฏุนุงุก `from_pretrained`:

 - `device_map` ูุถูู ุงูุชูุงู ุงููููุฐุฌ ุฅูู ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณููุงุช (GPU) ุงูุฎุงุตุฉ ุจู
 - `load_in_4bit` ูุทุจู [4-bit dynamic quantization](main_classes/quantization) ูุฎูุถ ูุชุทูุจุงุช ุงูููุงุฑุฏ ุจุดูู ูุจูุฑ

ููุงู ุทุฑู ุฃุฎุฑู ูุชููุฆุฉ ูููุฐุฌุ ูููู ูุฐุง ุฎุท ุฃุณุงุณ ุฌูุฏ ููุจุฏุก ุจุงุณุชุฎุฏุงู LLM.

ุจุนุฏ ุฐููุ ุชุญุชุงุฌ ุฅูู ูุนุงูุฌุฉ ุฅุฏุฎุงู ุงููุต ุงูุฎุงุต ุจู ุจุงุณุชุฎุฏุงู [ููุฌุฒูุฆ ุงููุบูู](tokenizer_summary).

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1", padding_side="left")
>>> model_inputs = tokenizer(["A list of colors: red, blue"], return_tensors="pt").to("cuda")
```

ูุญุชูู ูุชุบูุฑ `model_inputs` ุนูู ุงููุต ุงููุฏุฎู ุจุนุฏ ุชูุณููู ุฅูู ูุญุฏุงุช ูุบููุฉ (tokens)ุ ุจุงูุฅุถุงูุฉ ุฅูู ููุงุน ุงูุงูุชุจุงู. ูู ุญูู ุฃู [`~generation.GenerationMixin.generate`] ุชุจุฐู ูุตุงุฑู ุฌูุฏูุง ูุงุณุชูุชุงุฌ ููุงุน ุงูุงูุชุจุงู ุนูุฏูุง ูุง ูุชู ุชูุฑูุฑูุ ููุตู ุจุชูุฑูุฑู ูููุง ุฃููู ุฐูู ููุญุตูู ุนูู ูุชุงุฆุฌ ูุซุงููุฉ.

ุจุนุฏ ุชูุณูู ุงููุฏุฎูุงุช ุฅูู ูุญุฏุงุช ูุบููุฉุ ููููู ุงุณุชุฏุนุงุก ุงูุฏุงูุฉ [`~generation.GenerationMixin.generate`] ูุฅุฑุฌุงุน ุงููุญุฏุงุช ุงููุบููุฉ ุงููุงุชุฌุฉ. ูุฌุจ ุจุนุฏ ุฐูู ุชุญููู ุงููุญุฏุงุช ุงููููุฏุฉ ุฅูู ูุต ูุจู ุทุจุงุนุชู.

```py
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'A list of colors: red, blue, green, yellow, orange, purple, pink,'
```

ุฃุฎูุฑูุงุ ููุณ ุนููู ูุนุงูุฌุฉ ุงููุชุชุงููุงุช ุงููุงุญุฏุฉ ุชูู ุงูุฃุฎุฑู! ููููู ูุนุงูุฌุฉ ูุฌููุนุฉ ูู ุงููุฏุฎูุงุช ุฏูุนุฉ ูุงุญุฏุฉุ ูุงูุชู ุณุชุนูู ุนูู ุชุญุณูู ุงูุฅูุชุงุฌูุฉ ุจุดูู ูุจูุฑ ุจุชูููุฉ ุตุบูุฑุฉ ูู ุฒูู ุงูุงุณุชุฌุงุจุฉ ูุงุณุชููุงู ุงูุฐุงูุฑ. ูู ูุง ุนููู ุงูุชุฃูุฏ ููู ูู  ุชุนุจุฆุฉ ุงููุฏุฎูุงุช ุจุดูู ุตุญูุญ (ุงููุฒูุฏ ุญูู ุฐูู ุฃุฏูุงู).

```py
>>> tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default
>>> model_inputs = tokenizer(
...     ["A list of colors: red, blue", "Portugal is"], return_tensors="pt", padding=True
... ).to("cuda")
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
['A list of colors: red, blue, green, yellow, orange, purple, pink,',
'Portugal is a country in southwestern Europe, on the Iber']
```

ููุฐุง ูู ุดูุก! ูู ุจุถุน ุณุทูุฑ ูู ุงูุชุนูููุงุช ุงูุจุฑูุฌูุฉุ ููููู ุชุณุฎูุฑ ููุฉ LLM.

## ุงูุฃุฎุทุงุก ุงูุดุงุฆุนุฉ

ููุงู ุงูุนุฏูุฏ ูู [ุงุณุชุฑุงุชูุฌูุงุช ุงูุชูููุฏ](generation_strategies)ุ ููู ุจุนุถ ุงูุฃุญูุงู ูุฏ ูุง ุชููู ุงูููู ุงูุงูุชุฑุงุถูุฉ ููุงุณุจุฉ ูุญุงูุชู ุงูุงุณุชุฎุฏุงู. ุฅุฐุง ูู ุชูู ุงูุฅุฎุฑุงุฌ ุงูุฎุงุตุฉ ุจู ูุชูุงููุฉ ูุน ูุง ุชุชููุนูุ ููุฏ ูููุง ุจุฅูุดุงุก ูุงุฆูุฉ ุจุฃูุซุฑ ุงูุฃุฎุทุงุก ุงูุดุงุฆุนุฉ ูููููุฉ ุชุฌูุจูุง.

```py
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")
>>> tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default
>>> model = AutoModelForCausalLM.from_pretrained(
...     "mistralai/Mistral-7B-v0.1", device_map="auto", load_in_4bit=True
... )
```

### ุงูุฅุฎุฑุงุฌ ุงููููุฏ ูุตูุฑ ุฌุฏูุง/ุทููู ุฌุฏูุง

ุฅุฐุง ูู ูุชู ุชุญุฏูุฏ ุงูุนุฏุฏ ุงูุฃูุตู ููุฑููุฒ ูู [`~generation.GenerationConfig`] ุงููููุ `generate` ูุนูุฏ ูุง ูุตู ุฅูู 20 ุฑูุฒูุง ุจุดูู ุงูุชุฑุงุถู. ููุตู ุจุดุฏุฉ ุจุชุนููู `max_new_tokens` ูุฏูููุง ูู ููุงููุฉ `generate` ููุชุญูู ูู ุงูุนุฏุฏ ุงูุฃูุตู ูู ุงูุฑููุฒ ุงูุฌุฏูุฏุฉ ุงูุชู ูููู ุฃู ูุนูุฏูุง. ุถุน ูู ุงุนุชุจุงุฑู ุฃู LLMs (ุจุดูู ุฃูุซุฑ ุฏูุฉุ [ููุงุฐุฌ ูู ุงูุชุดููุฑ ููุท](https://huggingface.co/learn/nlp-course/chapter1/6ุfw=pt)) ุชุนูุฏ ุฃูุถูุง ุงููุฏุฎูุงุช ุงูุฃุตููุฉ ูุฌุฒุก ูู ุงููุงุชุฌ.
```py
>>> model_inputs = tokenizer(["A sequence of numbers: 1, 2"], return_tensors="pt").to("cuda")

>>> # By default, the output will contain up to 20 tokens
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'A sequence of numbers: 1, 2, 3, 4, 5'

>>> # Setting `max_new_tokens` allows you to control the maximum length
>>> generated_ids = model.generate(**model_inputs, max_new_tokens=50)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'A sequence of numbers: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,'
```

### ูุถุน ุงูุชูููุฏ ุงูุงูุชุฑุงุถู

ุจุดูู ุงูุชุฑุงุถูุ ููุง ูู ูุชู ุชุญุฏูุฏู ูู [`~generation.GenerationConfig`] ุงููููุ `generate` ูุญุฏุฏ ุงููููุฉ ุงูุฃูุซุฑ ุงุญุชูุงููุง ูู ูู ุฎุทูุฉ ูู ุฎุทูุงุช ุนูููุฉ ุงูุชูููุฏ (ููุฐุง ููุนุฑู ุจุงูุชุดููุฑ ุงูุฌุดุน). ุงุนุชูุงุฏูุง ุนูู ูููุชูุ ูุฏ ูููู ูุฐุง ุบูุฑ ูุฑุบูุจ ูููุ ุชุณุชููุฏ ุงูููุงู ุงูุฅุจุฏุงุนูุฉ ูุซู ุจุฑุงูุฌ ุงูุฏุฑุฏุดุฉ ุฃู ูุชุงุจุฉ ููุงู ุณุชููุฏ ูู ุฃุณููุจ ุงูุนููุฉ ุงูุนุดูุงุฆูุฉ ูู ุงุฎุชูุงุฑ ุงููููุงุชุ ุชูู ูุงุญูุฉ ุฃุฎุฑูุ ูุฅู ุงูููุงู ุงูุชู ุชุนุชูุฏ ุนูู ูุฏุฎูุงุช ูุญุฏุฏุฉ  ูุซู ุชุญููู ุงูุตูุช ุฅูู ูุต ุฃู ุงูุชุฑุฌู ูู ูู ุงูุชุดููุฑ ุงูุฌุดุน. ูู ุจุชูุนูู ุฃุณููุจ ุงูุนููุงุช ุงูุนุดูุงุฆูุฉ ุจุงุณุชุฎุฏุงู `do_sample=True`ุ ูููููู ูุนุฑูุฉ ุงููุฒูุฏ ุญูู ูุฐุง ุงูููุถูุน ูู [ุชุฏูููุฉ ุงููุฏููุฉ](https://huggingface.co/blog/how-to-generate).

```py
>>> # Set seed or reproducibility -- you don't need this unless you want full reproducibility
>>> from transformers import set_seed
>>> set_seed(42)

>>> model_inputs = tokenizer(["I am a cat."], return_tensors="pt").to("cuda")

>>> # LLM + greedy decoding = repetitive, boring output
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'I am a cat. I am a cat. I am a cat. I am a cat'

>>> # With sampling, the output becomes more creative!
>>> generated_ids = model.generate(**model_inputs, do_sample=True)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'I am a cat.  Specifically, I am an indoor-only cat.  I'
```

### ูุดููุฉ ุญุดู ุงููุฏุฎูุงุช ูู ุงูุงุชุฌุงุฉ ุงูุฎุทุฃ

LLMs ูู [ูุนูุงุฑูุงุช ูู ุงูุชุดููุฑ ููุท](https://huggingface.co/learn/nlp-course/chapter1/6ุfw=pt)ุ ููุง ูุนูู ุฃููุง ุชุณุชูุฑ ูู ุงูุชูุฑุงุฑ ุนูู ููุฌู ุงูุฅุฏุฎุงู ุงูุฎุงุต ุจู. ูุฅู ุฌููุน ุงููุฏุฎูุงุช ูุฌุจ ุฃู ุชููู ุจููุณ ุงูุทูู. ูุญู ูุฐู ุงููุณุฃูุฉุ ูุชู ุฅุถุงูุฉ ุฑููุฒ ุญุดู ุฅูู ุงููุฏุฎูุงุช ุงูุฃูุตุฑ. ูุธุฑูุง ูุฃู LLMs  ูุง ุชููู ุงูุชูุงููุง ูุฑููุฒ ุงูุญุดู ูุฐูุ ุฐููุ ูุฌุจ ุชุญุฏูุฏ ุงูุฌุฒุก ุงูููู ูู ุงููุฏุฎู ุงูุฐู ูุฌุจ ุฃู ูุฑูุฒ ุนููู ุงููููุฐุฌุ ููุฐุง ูุชู ุนู ุทุฑูู ูุง ูุณูู ุจู "ููุงุน ุงูุงูุชุจุงู". ูุฌุจ ุฃู ูููู ุงูุญุดู ูู ุจุฏุงูุฉ ุงููุฏุฎู (ุงูุญุดู ูู ุงููุณุงุฑ)ุ ูููุณ ูู ููุงูุชู.

```py
>>> # The tokenizer initialized above has right-padding active by default: the 1st sequence,
>>> # which is shorter, has padding on the right side. Generation fails to capture the logic.
>>> model_inputs = tokenizer(
...     ["1, 2, 3", "A, B, C, D, E"], padding=True, return_tensors="pt"
... ).to("cuda")
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'1, 2, 33333333333'

>>> # With left-padding, it works as expected!
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1", padding_side="left")
>>> tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default
>>> model_inputs = tokenizer(
...     ["1, 2, 3", "A, B, C, D, E"], padding=True, return_tensors="pt"
... ).to("cuda")
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'1, 2, 3, 4, 5, 6,'
```

### ููุฌู ุบูุฑ ุตุญูุญ

ุชุชููุน ุจุนุถ ููุงุฐุฌ ุงููุบุงุช ุงููุจูุฑุฉ ุนูู ุตูุบุฉ ูุญุฏุฏุฉ ูููุฏุฎูุงุช  ููุนูู ุจุดูู ุตุญูุญ. ุฅุฐุง ูู ูุชู ุงุชุจุงุน ูุฐู ุงูุตูุบุฉุ ูุฅู ุฃุฏุงุก ุงููููุฐุฌ ูุชุฃุซุฑ ุณูุจูุง: ููู ูุฐุง ุงูุชุฏููุฑ ูุฏ ูุง ูููู ูุงุถุญูุง ููุนูุงู. ุชุชููุฑ ูุนูููุงุช ุฅุถุงููุฉ ุญูู ุงูุชูุฌููุ ุจูุง ูู ุฐูู ุงูููุงุฐุฌ ูุงูููุงู ุงูุชู ุชุญุชุงุฌ ุฅูู ุชูุฎู ุงูุญุฐุฑุ ูู [ุงูุฏููู](tasks/prompting). ุฏุนูุง ูุฑู ูุซุงูุงู ุจุงุณุชุฎุฏุงู LLM ููุฏุฑุฏุดุฉุ ูุงูุฐู ูุณุชุฎุฏู [ูุงูุจ ุงูุฏุฑุฏุดุฉ](chat_templating):
```python
>>> tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/zephyr-7b-alpha")
>>> model = AutoModelForCausalLM.from_pretrained(
...     "HuggingFaceH4/zephyr-7b-alpha", device_map="auto", load_in_4bit=True
... )
>>> set_seed(0)
>>> prompt = """How many helicopters can a human eat in one sitting? Reply as a thug."""
>>> model_inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
>>> input_length = model_inputs.input_ids.shape[1]
>>> generated_ids = model.generate(**model_inputs, max_new_tokens=20)
>>> print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])
"I'm not a thug, but i can tell you that a human cannot eat"
>>> # Oh no, it did not follow our instruction to reply as a thug! Let's see what happens when we write
>>> # a better prompt and use the right template for this model (through `tokenizer.apply_chat_template`)

>>> set_seed(0)
>>> messages = [
...     {
...         "role": "system",
...         "content": "You are a friendly chatbot who always responds in the style of a thug",
...     },
...     {"role": "user", "content": "How many helicopters can a human eat in one sitting?"},
... ]
>>> model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt").to("cuda")
>>> input_length = model_inputs.shape[1]
>>> generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=20)
>>> print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])
'None, you thug. How bout you try to focus on more useful questions?'
>>> # As we can see, it followed a proper thug style ๐
```

## ููุงุฑุฏ ุฅุถุงููุฉ

ูู ุญูู ุฃู ุนูููุฉ ุงูุชูููุฏ ุงูุชููุงุฆู ุจุณูุทุฉ ูุณุจููุงุ ูุฅู ุงูุงุณุชูุงุฏุฉ ุงููุตูู ูู LLM ุงูุฎุงุต ุจู ูููู ุฃู ุชููู ูููุฉ ุตุนุจุฉ ูุฃู ููุงู ุงูุนุฏูุฏ ูู ุงูุฃุฌุฒุงุก ุงููุชุญุฑูุฉ. ููุฎุทูุงุช ุงูุชุงููุฉ ููุณุงุนุฏุชู ูู ุงูุบูุต ุจุดูู ุฃุนูู ูู ุงุณุชุฎุฏุงู LLM ููููู:

### ุงุณุชุฎุฏุงูุงุช ูุชูุฏูุฉ ููุชูููุฏ ูู ููุงุฐุฌ ุงููุบุงุช ุงููุจูุฑุฉ

1. ุฏููู ุญูู ููููุฉ [ุงูุชุญูู ูู ุทุฑู ุงูุชูููุฏ ุงููุฎุชููุฉ](generation_strategies)ุ ูููููุฉ ุฅุนุฏุงุฏ ููู ุชูููู ุงูุชูููุฏุ ูููููุฉ ุจุซ ุงููุงุชุฌุ
2. [ุชุณุฑูุน ุชูููุฏ ุงููุต](llm_optims)ุ
3.[ููุงูุจ ููุฌูุงุช ููุฏุฑุฏุดุฉ LLMs](chat_
4. [ุฏููู ุชุตููู ุงูููุฌู](tasks/prompting);
5. ูุฑุฌุน ูุงุฌูุฉ ุจุฑูุฌุฉ ุงูุชุทุจููุงุช (API)  [`~generation.GenerationConfig`], [`~generation.GenerationMixin.generate`], ู  [generate-related classes](internal/generation_utils). ูุงูุนุฏูุฏ ูู ุงููุฆุงุช ุงูุฃุฎุฑู ุงููุฑุชุจุทุฉ ุจุนูููุฉ ุงูุชูููุฏ.!

### ููุญุงุช ุตุฏุงุฑุฉ ููุงุฐุฌ ุงููุบุงุช ุงููุจูุฑุฉ
1. ููุญุฉ ุตุฏุงุฑุฉ ููุงุฐุฌ ุงููุบุงุช ุงููุจูุฑุฉ ุงูููุชูุญุฉ ุงููุตุฏุฑ (Open LLM Leaderboard): ุชุฑูุฒ ุนูู ุฌูุฏุฉ ุงูููุงุฐุฌ ููุชูุญุฉ ุงููุตุฏุฑ [ุฑุงุจุท ููุญุฉ ุงูุตุฏุงุฑุฉ](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).
2. ููุญุฉ ุตุฏุงุฑุฉ ุฃุฏุงุก ููุงุฐุฌ ุงููุบุงุช ุงููุจูุฑุฉ ุงูููุชูุญุฉ ุงููุตุฏุฑ (Open LLM-Perf Leaderboard): ุชุฑูุฒ ุนูู ุฅูุชุงุฌูุฉ ููุงุฐุฌ ุงููุบุงุช ุงููุจูุฑุฉ [ุฑุงุจุท ููุญุฉ ุงูุตุฏุงุฑุฉ](https://huggingface.co/spaces/optimum/llm-perf-leaderboard).

### ุฒูู ุงูุงุณุชุฌุงุจุฉ ูุงูุฅูุชุงุฌูุฉ ูุงุณุชููุงู ุงูุฐุงูุฑุฉ
1. ุฏููู ุชุญุณูู ููุงุฐุฌ ุงููุบุงุช ุงููุจูุฑุฉ ูู ุญูุซ ุงูุณุฑุนุฉ ูุงูุฐุงูุฑุฉ: ุฏููู ุชุญุณูู ููุงุฐุฌ ุงููุบุงุช ุงููุจูุฑุฉ.
2. ุงูุชูููู (Quantization): ุฏููู ุญูู ุชูููุฉ ุงูุชูููู ุงูุชูููู ูุซู ุชูููุชู bitsandbytes ู autogptqุ ูุงูุชู ุชูุถุญ ููููุฉ ุชูููู ูุชุทูุจุงุช ุงูุฐุงูุฑุฉ ุจุดูู ูุจูุฑ.

### ููุชุจุงุช ูุฑุชุจุทุฉ
1. [`optimum`](https://github.com/huggingface/optimum), ุงูุชุฏุงุฏ ูููุชุจุฉ Transformers ูุนูู ุนูู ุชุญุณูู ุงูุฃุฏุงุก ูุฃุฌูุฒุฉ ูุนููุฉ.
2. [`outlines`](https://github.com/outlines-dev/outlines), ููุชุจุฉ ููุชุญูู ูู ุชูููุฏ ุงููุตูุต (ุนูู ุณุจูู ุงููุซุงูุ ูุชูููุฏ ูููุงุช JSON).
3. [`SynCode`](https://github.com/uiuc-focal-lab/syncode), ููุชุจุฉ ููุชูููุฏ ุงูููุฌู ุจููุงุนุฏ ุงููุบุฉ ุงูุฎุงููุฉ ูู ุงูุณูุงู (ุนูู ุณุจูู ุงููุซุงูุ JSONุ SQLุ Python).
4. [`text-generation-inference`](https://github.com/huggingface/text-generation-inference), ุฎุงุฏู ุฌุงูุฒ ููุฅูุชุงุฌ ูููุงุฐุฌ ุงููุบุงุช ุงููุจูุฑุฉ.
5. [`text-generation-webui`](https://github.com/oobabooga/text-generation-webui), ูุงุฌูุฉ ูุณุชุฎุฏู ูุชูููุฏ ุงููุตูุต. ย 
