# Mistral

## ูุธุฑุฉ ุนุงูุฉ
ุชู ุชูุฏูู ููุณุชุฑุงู ูู [ูุฐู ุงูุชุฏูููุฉ](https://mistral.ai/news/announcing-mistral-7b/) ุจูุงุณุทุฉ ุฃูุจุฑุช ุฌูุงูุฌุ ูุฃููุณูุฏุฑ ุณุงุจููุฑููุฒุ ูุฃุฑุซุฑ ูููุดุ ููุฑูุณ ุจุงูููุฑุฏุ ูุฏูููุฏุฑุง ุณููุบ ุชุดุงุจููุชุ ูุฏููุฌู ุฏู ูุงุณ ูุงุณุงุณุ ููููุฑูุงู ุจุฑูุณุงูุฏุ ูุฌูุงูุง ูููุฌููุ ูุฌููู ูุงุจูุ ูููููู ุฑููุงุฑุฏ ูุงููุ ูููุณูู ุณููููููุ ููุงุฑู-ุขู ูุงุดูุ ูุจููุฑ ุณุชููุ ูุชูููู ูู ุณูุงูุ ูุชูุจู ูุงูุฑูุ ูุชููุงุณ ูุงูุฌุ ูุชูููุชู ูุงูุฑูุงุ ูููููุงู ุฅู ุณูุฏ.

ุชููู ููุฏูุฉ ุงูุชุฏูููุฉ:

> *ููุชุฎุฑ ูุฑูู Mistral AI ุจุชูุฏูู Mistral 7Bุ ุฃููู ูููุฐุฌ ูุบูู ุญุชู ุงูุขู ุจุญุฌูู.*

ููุณุชุฑุงู-7B ูู ุฃูู ูููุฐุฌ ูุบูู ูุจูุฑ (LLM) ุฃุตุฏุฑู [mistral.ai](https://mistral.ai/).

### ุงูุชูุงุตูู ุงููุนูุงุฑูุฉ

ููุณุชุฑุงู-7B ูู ูุญูู ูู ุชุดููุฑ ููุท ูุน ุฎูุงุฑุงุช ุงูุชุตููู ุงููุนูุงุฑู ุงูุชุงููุฉ:

- Sliding Window Attention - ุชู ุชุฏุฑูุจู ุจุงุณุชุฎุฏุงู ุทูู ุณูุงู 8k ูุญุฌู ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ุงูุซุงุจุชุ ูุน ูุทุงู ุงูุชูุงู ูุธุฑู ูุจูุบ 128K ุฑูุฒูุง

- GQA (Grouped Query Attention) - ูุณูุญ ุจุฅุฌุฑุงุก ุงุณุชุฏูุงู ุฃุณุฑุน ูุญุฌู ุฐุงูุฑุฉ ุชุฎุฒูู ูุคูุช ุฃูู.

- Byte-fallback BPE tokenizer - ูุถูู ุนุฏู ุชุนููู ุงูุฃุญุฑู ูุทูููุง ุฅูู ุฑููุฒ ุฎุงุฑุฌ ุงูููุฑุฏุงุช.

ููุญุตูู ุนูู ูุฒูุฏ ูู ุงูุชูุงุตููุ ูุฑุฌู ุงูุฑุฌูุน ุฅูู [ุชุฏูููุฉ ุงูุฅุตุฏุงุฑ](https://mistral.ai/news/announcing-mistral-7b/).

### ุงูุชุฑุฎูุต

ุชู ุฅุตุฏุงุฑ `Mistral-7B` ุจููุฌุจ ุชุฑุฎูุต Apache 2.0.

## ูุตุงุฆุญ ุงูุงุณุชุฎุฏุงู

ุฃุตุฏุฑ ูุฑูู ููุณุชุฑุงู 3 ููุงุท ุชูุชูุด:

- ูููุฐุฌ ุฃุณุงุณูุ [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)ุ ุชู ุชุฏุฑูุจู ูุณุจููุง ููุชูุจุค ุจุงูุฑูุฒ ุงูุชุงูู ุนูู ุจูุงูุงุช ุจุญุฌู ุงูุฅูุชุฑูุช.

- ูููุฐุฌ ุถุจุท ุงูุชุนูููุงุชุ [Mistral-7B-Instruct-v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)ุ ููู ุงููููุฐุฌ ุงูุฃุณุงุณู ุงูุฐู ุชู ุชุญุณููู ูุฃุบุฑุงุถ ุงูุฏุฑุฏุดุฉ ุจุงุณุชุฎุฏุงู ุงูุถุจุท ุงูุฏููู ุงูููุดุฑู (SFT) ูุงูุชุญุณูู ุงููุจุงุดุฑ ููุฃูุถููุงุช (DPO).

- ูููุฐุฌ ุถุจุท ุชุนูููุงุช ูุญุณูุ [Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)ุ ูุงูุฐู ูุญุณู ุงูุฅุตุฏุงุฑ 1.

ูููู ุงุณุชุฎุฏุงู ุงููููุฐุฌ ุงูุฃุณุงุณู ุนูู ุงููุญู ุงูุชุงูู:

```python
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-v0.1", device_map="auto")
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")

>>> prompt = "My favourite condiment is"

>>> model_inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
>>> model.to(device)

>>> generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)
>>> tokenizer.batch_decode(generated_ids)[0]
"My favourite condiment is to ..."
```

ูููู ุงุณุชุฎุฏุงู ูููุฐุฌ ุถุจุท ุงูุชุนูููุงุช ุนูู ุงููุญู ุงูุชุงูู:

```python
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2", device_map="auto")
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2")

>>> messages = [
...     {"role": "user", "content": "What is your favourite condiment?"},
...     {"role": "assistant", "content": "Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!"},
...     {"role": "user", "content": "Do you have mayonnaise recipes?"}
... ]

>>> model_inputs = tokenizer.apply_chat_template(messages, return_tensors="pt").to("cuda")

>>> generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)
>>> tokenizer.batch_decode(generated_ids)[0]
"Mayonnaise can be made as follows: (...)"
```

ููุง ูู ููุถุญุ ูุชุทูุจ ูููุฐุฌ ุถุจุท ุงูุชุนูููุงุช [ูุงูุจ ุฏุฑุฏุดุฉ](../chat_templating) ููุชุฃูุฏ ูู ุฅุนุฏุงุฏ ุงููุฏุฎูุงุช ุจุชูุณูู ุตุญูุญ.

## ุชุณุฑูุน ููุณุชุฑุงู ุจุงุณุชุฎุฏุงู Flash Attention

ุชูุถุญ ููุชุทูุงุช ุงูุดูุฑุฉ ุฃุนูุงู ุงูุงุณุชุฏูุงู ุจุฏูู ุฃู ุญูู ููุชุญุณูู. ููุน ุฐููุ ูููู ูููุฑุก ุชุณุฑูุน ุงููููุฐุฌ ุจุดูู ูุจูุฑ ูู ุฎูุงู ุงูุงุณุชูุงุฏุฉ ูู [Flash Attention](../perf_train_gpu_one.md#flash-attention-2)ุ ููู ุชูููุฐ ุฃุณุฑุน ูุขููุฉ ุงูุงูุชูุงู ุงููุณุชุฎุฏูุฉ ุฏุงุฎู ุงููููุฐุฌ.

ุฃููุงูุ ุชุฃูุฏ ูู ุชุซุจูุช ุฃุญุฏุซ ุฅุตุฏุงุฑ ูู Flash Attention 2 ูุชุถููู ููุฒุฉ ูุงูุฐุฉ ุงูุงูุฒูุงู.

```bash
pip install -U flash-attn --no-build-isolation
```

ุชุฃูุฏ ุฃูุถูุง ูู ุฃู ูุฏูู ุฃุฌูุฒุฉ ูุชูุงููุฉ ูุน Flash-Attention 2. ุงูุฑุฃ ุงููุฒูุฏ ุนููุง ูู ุงููุซุงุฆู ุงูุฑุณููุฉ ูู [ูุณุชูุฏุน ุงูุงูุชูุงู ุจุงููููุถ](https://github.com/Dao-AILab/flash-attention). ุชุฃูุฏ ุฃูุถูุง ูู ุชุญููู ูููุฐุฌู ูู ูุตู ุงูุฏูุฉ (ุนูู ุณุจูู ุงููุซุงู `torch.float16`)

ููุชุญููู ูุชุดุบูู ูููุฐุฌ ุจุงุณุชุฎุฏุงู Flash Attention-2ุ ุฑุงุฌุน ุงูููุชุทู ุฃุฏูุงู:

```python
>>> import torch
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-v0.1", torch_dtype=torch.float16, attn_implementation="flash_attention_2", device_map="auto")
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")

>>> prompt = "My favourite condiment is"

>>> model_inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
>>> model.to(device)

>>> generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)
>>> tokenizer.batch_decode(generated_ids)[0]
"My favourite condiment is to (...)"
```

### ุชุณุฑูุน ูุชููุน

ูููุง ููู ุฑุณู ุจูุงูู ููุชุณุฑูุน ุงููุชููุน ุงูุฐู ููุงุฑู ููุช ุงูุงุณุชุฏูุงู ุงูููู ุจูู ุงูุชูููุฐ ุงูุฃุตูู ูู ุงููุญููุงุช ุจุงุณุชุฎุฏุงู ููุทุฉ ุชูุชูุด `mistralai/Mistral-7B-v0.1` ูุฅุตุฏุงุฑ Flash Attention 2 ูู ุงููููุฐุฌ.

<div style="text-align: center">
<img src="https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/mistral-7b-inference-large-seqlen.png">
</div>

### ูุงูุฐุฉ ุงูุฒูุงู ุงูุงูุชูุงู

ูุฏุนู ุงูุชูููุฐ ุงูุญุงูู ุขููุฉ ุงูุชูุงู ูุงูุฐุฉ ุงูุงูุฒูุงู ูุฅุฏุงุฑุฉ ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ุงููุนุงูุฉ ูู ุญูุซ ุงูุฐุงูุฑุฉ.

ูุชูููู ุงูุชูุงู ูุงูุฐุฉ ุงูุงูุฒูุงูุ ุชุฃูุฏ ููุท ูู ูุฌูุฏ ุฅุตุฏุงุฑ `flash-attn` ูุชูุงูู ูุน ุงูุชูุงู ูุงูุฐุฉ ุงูุงูุฒูุงู (`>=2.3.0`).

ูุณุชุฎุฏู ูููุฐุฌ Flash Attention-2 ุฃูุถูุง ุขููุฉ ุชูุทูุน ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ุงูุฃูุซุฑ ููุงุกุฉ ูู ุญูุซ ุงูุฐุงูุฑุฉ - ููููุง ููุชูููุฐ ุงูุฑุณูู ููููุฐุฌ ููุณุชุฑุงู ุงูุฐู ูุณุชุฎุฏู ุขููุฉ ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ุงููุชุฏุงููุฉุ ูุญุชูุธ ุจุญุฌู ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ุซุงุจุชูุง (`self.config.sliding_window`)ุ ููุฏุนู ุงูุชูููุฏ ุงููุฌูุน ููุท ูู `padding_side="left"` ููุณุชุฎุฏู ุงูููุถุน ุงููุทูู ููุฑูุฒ ุงูุญุงูู ูุญุณุงุจ ุงูุชุถููู ุงูููุถุนู.

## ุชูููู ุญุฌู ููุณุชุฑุงู ุจุงุณุชุฎุฏุงู ุงูุชูููู

ูุธุฑูุง ูุฃู ูููุฐุฌ ููุณุชุฑุงู ูุญุชูู ุนูู 7 ูููุงุฑุงุช ูุนููุฉุ ูุณูุญุชุงุฌ ุฐูู ุฅูู ุญูุงูู 14 ุฌูุฌุงุจุงูุช ูู ุฐุงูุฑุฉ GPU RAM ูู ุงูุฏูุฉ ุงููุตููุฉ (float16)ุ ุญูุซ ูุชู ุชุฎุฒูู ูู ูุนููุฉ ูู 2 ุจุงูุช. ููุน ุฐููุ ูููู ุชูููู ุญุฌู ุงููููุฐุฌ ุจุงุณุชุฎุฏุงู [ุงูุชูููู](../quantization.md). ุฅุฐุง ุชู ุชูููู ุงููููุฐุฌ ุฅูู 4 ุจุชุงุช (ุฃู ูุตู ุจุงูุช ููู ูุนููุฉ)ุ ููุฐุง ูุชุทูุจ ููุท ุญูุงูู 3.5 ุฌูุฌุงุจุงูุช ูู ุฐุงูุฑุฉ ุงููุตูู ุงูุนุดูุงุฆู.

ุฅู ุชูููู ูููุฐุฌ ุจุณูุท ูุซู ุชูุฑูุฑ `quantization_config` ุฅูู ุงููููุฐุฌ. ุฃุฏูุงูุ ุณูุณุชููุฏ ูู ุชูููู BitsAndyBytes (ูููู ุฑุงุฌุน [ูุฐู ุงูุตูุญุฉ](../quantization.md) ูุฃุณุงููุจ ุงูุชูููู ุงูุฃุฎุฑู):

```python
>>> import torch
>>> from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

>>> # specify how to quantize the model
>>> quantization_config = BitsAndBytesConfig(
...         load_in_4bit=True,
...         bnb_4bit_quant_type="nf4",
...         bnb_4bit_compute_dtype="torch.float16",
... )

>>> model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2", quantization_config=True, device_map="auto")
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2")

>>> prompt = "My favourite condiment is"

>>> messages = [
...     {"role": "user", "content": "What is your favourite condiment?"},
...     {"role": "assistant", "content": "Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!"},
...     {"role": "user", "content": "Do you have mayonnaise recipes?"}
... ]

>>> model_inputs = tokenizer.apply_chat_template(messages, return_tensors="pt").to("cuda")

>>> generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)
>>> tokenizer.batch_decode(generated_ids)[0]
"The expected output"
```

ุชูุช ุงููุณุงููุฉ ุจูุฐุง ุงููููุฐุฌ ูู ูุจู [ูููุณ ุจููุงุฏุฉ](https://huggingface.co/ybelkada) ู[ุขุฑุซุฑ ุฒููุฑ](https://huggingface.co/ArthurZ).

ูููู ุงูุนุซูุฑ ุนูู ุงูููุฏ ุงูุฃุตูู [ููุง](https://github.com/mistralai/mistral-src).

## ุงูููุงุฑุฏ

ูุงุฆูุฉ ุจููุงุฑุฏ Hugging Face ุงูุฑุณููุฉ ูููุงุฑุฏ ุงููุฌุชูุน (ูุดุงุฑ ุฅูููุง ุจู ๐) ููุณุงุนุฏุชู ูู ุงูุจุฏุก ุจุงุณุชุฎุฏุงู ููุณุชุฑุงู. ุฅุฐุง ููุช ููุชููุง ุจุชูุฏูู ููุฑุฏ ูุฅุฏุฑุงุฌู ููุงุ ููุฑุฌู ูุชุญ ุทูุจ ุณุญุจ ูุณูุฑุงุฌุนู! ูุฌุจ ุฃู ููุถุญ ุงูููุฑุฏ ุจุดูู ูุซุงูู ุดูุก ุฌุฏูุฏ ุจุฏูุงู ูู ุชูุฑุงุฑ ููุฑุฏ ููุฌูุฏ.

<PipelineTag pipeline="text-generation"/>

- ูููู ุงูุนุซูุฑ ุนูู ุฏูุชุฑ ููุงุญุธุงุช ุชูุถูุญู ูุฃุฏุงุก ุงูุถุจุท ุงูุฏููู ุงูููุดุฑู (SFT) ูู Mistral-7B [ููุง](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Mistral/Supervised_fine_tuning_(SFT)_of_an_LLM_using_Hugging_Face_tooling.ipynb). ๐

- [ุชุฏูููุฉ](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl) ุญูู ููููุฉ ุถุจุท ุฏูุฉ LLMs ูู ุนุงู 2024 ุจุงุณุชุฎุฏุงู ุฃุฏูุงุช Hugging Face. ๐

- ูุชุถูู [ุฏููู ุงููุญุงุฐุงุฉ](https://github.com/huggingface/alignment-handbook) ูู Hugging Face ูุตูุตูุง ููุตูุงุช ูุฃุฏุงุก ุงูุถุจุท ุงูุฏููู ุงูููุดุฑู (SFT) ูุงูุชุญุณูู ุงููุจุงุดุฑ ููุฃูุถููุงุช ุจุงุณุชุฎุฏุงู Mistral-7B. ูุชุถูู ูุฐุง ุงููุตูุต ููุถุจุท ุงูุฏููู ุงููุงููุ ูQLoRa ุนูู GPU ูุงุญุฏ ุจุงูุฅุถุงูุฉ ุฅูู ุงูุถุจุท ุงูุฏููู ูุชุนุฏุฏ GPU.

- [ุฏููู ููุงู ููุฐุฌุฉ ุงููุบุฉ ุงูุณุจุจูุฉ](../tasks/language_modeling)

## MistralConfig

[[autodoc]] MistralConfig

## MistralModel

[[autodoc]] MistralModel

- forward

## MistralForCausalLM

[[autodoc]] MistralForCausalLM

- forward

## MistralForSequenceClassification

[[autodoc]] MistralForSequenceClassification

- forward

## MistralForTokenClassification

[[autodoc]] MistralForTokenClassification

- forward

## FlaxMistralModel

[[autodoc]] FlaxMistralModel

- __call__

## FlaxMistralForCausalLM

[[autodoc]] FlaxMistralForCausalLM

- __call__

## TFMistralModel

[[autodoc]] TFMistralModel

- call

## TFMistralForCausalLM

[[autodoc]] TFMistralForCausalLM

- call

## TFMistralForSequenceClassification

[[autodoc]] TFMistralForSequenceClassification

- call