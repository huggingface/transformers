# T5

<div class="flex flex-wrap space-x-1">
<a href="https://huggingface.co/models?filter=t5">
<img alt="Models" src="https://img.shields.io/badge/All_model_pages-t5-blueviolet">
</a>
<a href="https://huggingface.co/spaces/docs-demos/t5-base">
<img alt="Spaces" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue">
</a>
<a href="https://huggingface.co/papers/1910.10683">
<img alt="Paper page" src="https://img.shields.io/badge/Paper%20page-1910.10683-green">
</a>
</div>

## ูุธุฑุฉ ุนุงูุฉ
ุชู ุชูุฏูู ูููุฐุฌ T5 ูู ุจุญุซ [ุงุณุชูุดุงู ุญุฏูุฏ ุชุนูู ุงูููู ูุน ูุญูู ุงููุต ุงูููุญุฏ ุฅูู ุงููุต](https://arxiv.org/pdf/1910.10683.pdf) ุจูุงุณุทุฉ [ูููู ุฑุงููู](https://huggingface.co/craffel)ุ ูููุงู ุดุงุฒูุฑุ ู[ุขุฏู ุฑูุจุฑุชุณ](https://huggingface.co/adarob)ุ ููุงุซุฑูู ููุ ูุดุงุฑุงู ูุงุฑุงูุฌุ
ูุงููู ูุงุชููุงุ ููุงููู ุชุดูุ ููู ููุ ู[ุจูุชุฑ ุฌูู ููู](https://huggingface.co/peterjliu).

ููุฎุต ุงูุจุญุซ ูู ุงูุชุงูู:

> "ุฃุตุจุญ ุชุนูู ุงููููุ ุญูุซ ูุชู ุชุฏุฑูุจ ุงููููุฐุฌ ูุณุจููุง ุฃููุงู ุนูู ูููุฉ ุบููุฉ ุจุงูุจูุงูุงุช ูุจู ุถุจุท ุฏูุชู ุนูู ูููุฉ ุฃุณูู ุงูุจุซุ ุชูููุฉ ูููุฉ ูู ูุนุงูุฌุฉ ุงููุบุงุช ุงูุทุจูุนูุฉ (NLP). ููุฏ ุฃุฏุช ูุนุงููุฉ ุชุนูู ุงูููู ุฅูู ุธููุฑ ูุฌููุนุฉ ูุชููุนุฉ ูู ุงูุฃุณุงููุจ ูุงููููุฌูุงุช ูุงูููุงุฑุณุงุช. ูู ูุฐู ุงููุฑูุฉุ ูุณุชูุดู ูุฌุงู ุชูููุงุช ุชุนูู ุงูููู ููุนุงูุฌุฉ ุงููุบุงุช ุงูุทุจูุนูุฉ ูู ุฎูุงู ุชูุฏูู ุฅุทุงุฑ ุนูู ููุญุฏ ูุญูู ูู ูุดููุฉ ูุบููุฉ ุฅูู ุชูุณูู ูุตู. ุชูุงุฑู ุฏุฑุงุณุชูุง ุงููููุฌูุฉ ุฃูุฏุงู ุงูุชุฏุฑูุจ ุงููุณุจู ูุงูุจููุงุช ูุงููุฌููุนุงุช ุบูุฑ ุงููุนููุฉ ูููุฌ ุงูููู ูุนูุงูู ุฃุฎุฑู ูู ุนุดุฑุงุช ููุงู ููู ุงููุบุฉ. ูู ุฎูุงู ุงูุฌูุน ุจูู ุงูุฃููุงุฑ ุงููุณุชูุงุฏุฉ ูู ุงุณุชูุดุงููุง ูุน ุงููุทุงู ู"ูุฌููุนุชูุง ุงูุถุฎูุฉ ุงููุธููุฉ ุงููุณุชุฎุฑุฌุฉ ูู ุงูููุจ"ุ ูุญูู ูุชุงุฆุฌ ูุชูุฏูุฉ ูู ุงูุนุฏูุฏ ูู ุงููุนุงููุฑ ุงููุฑุฌุนูุฉ ุงูุชู ุชุบุทู ุงูููุฎุตุ ูุงูุฃุณุฆูุฉ ูุงูุฃุฌูุจุฉุ ูุชุตููู ุงููุตูุตุ ูุงููุฒูุฏ. ููุชุณููู ุงูุนูู ุงููุณุชูุจูู ูู ุชุนูู ุงูููู ููุนุงูุฌุฉ ุงููุบุงุช ุงูุทุจูุนูุฉุ ูููู ุจุฅุทูุงู ูุฌููุนุชูุง ูุทุฑุงุฒูุง ุงูููุฏุฑุจ ูุณุจููุง ูุดูุฑุฉ ุงูุจุฑูุงูุฌ."

ูููู ุงูุนุซูุฑ ุนูู ุฌููุน ููุงุท ุงูุชูุชูุด ุนูู [ุงููุฑูุฒ](https://huggingface.co/models?search=t5).

ุชูุช ุงููุณุงููุฉ ุจูุฐุง ุงููููุฐุฌ ูู ูุจู [thomwolf](https://huggingface.co/thomwolf). ูููู ุงูุนุซูุฑ ุนูู ุงูุดูุฑุฉ ุงูุฃุตููุฉ [ููุง](https://github.com/google-research/text-to-text-transfer-transformer).

## ูุตุงุฆุญ ุงูุงุณุชุฎุฏุงู

- T5 ูู ูููุฐุฌ ุชุฑููุฒ ูู ุชุฑููุฒ ููุฏุฑุจ ูุณุจููุง ุนูู ูุฒูุฌ ูู ุงูููุงู ูุชุนุฏุฏุฉ ุงูููุงู ุบูุฑ ุงูุฎุงุถุนุฉ ููุฅุดุฑุงู ูุงูุฎุงุถุนุฉ ููุฅุดุฑุงูุ ููุชู ุชุญููู ูู ูููุฉ ูููุง ุฅูู ุชูุณูู ูุตู. ูุนูู T5 ุจุดูู ุฌูุฏ ูู ูุฌููุนุฉ ูุชููุนุฉ ูู ุงูููุงู ุฎุงุฑุฌ ุงูุตูุฏูู ุนู ุทุฑูู ุฅุถุงูุฉ ุจุงุฏุฆุฉ ูุฎุชููุฉ ุฅูู ุงูุฅุฏุฎุงู ุงูููุงุจู ููู ูููุฉุ ุนูู ุณุจูู ุงููุซุงูุ ููุชุฑุฌูุฉ: *ุชุฑุฌูุฉ ูู ุงูุฅูุฌููุฒูุฉ ุฅูู ุงูุฃููุงููุฉ: ...*ุ
ููุชูุฎูุต: *ุชูุฎูุต: ...*.

- ูุชุถูู ุงูุชุฏุฑูุจ ุงููุณุจู ูููุง ูู ุงูุชุฏุฑูุจ ุงูุฎุงุถุน ููุฅุดุฑุงู ูุงูุชุฏุฑูุจ ุงูุฐุงุชู ุงูุฎุงุถุน ููุฅุดุฑุงู. ูุชู ุฅุฌุฑุงุก ุงูุชุฏุฑูุจ ุงูุฎุงุถุน ููุฅุดุฑุงู ุนูู ุงูููุงู ุงูุฃุณุงุณูุฉ ุจูุงุณุทุฉ ูุนุงููุฑ GLUE ูSuperGLUE (ุนู ุทุฑูู ุชุญููููุง ุฅูู ููุงู ูุตูุฉ ููุง ูู ููุถุญ ุฃุนูุงู).

- ูุณุชุฎุฏู ุงูุชุฏุฑูุจ ุงูุฐุงุชู ุงูุฎุงุถุน ููุฅุดุฑุงู ุงูุฑููุฒ ุงููููุฒุฉ ุงููุนุทูุฉุ ุนู ุทุฑูู ุฅุฒุงูุฉ 15% ูู ุงูุฑููุฒ ุจุดูู ุนุดูุงุฆู ูุงุณุชุจุฏุงููุง ุจุฑููุฒ ูููุฒุฉ ูุฑุฏูุฉ (ุฅุฐุง ุชู ูุถุน ุนูุงูุฉ ุนูู ุนุฏุฉ ุฑููุฒ ูุชุชุงููุฉ ููุฅุฒุงูุฉุ ูุชู ุงุณุชุจุฏุงู ุงููุฌููุนุฉ ุจุฃููููุง ุจุฑูุฒ ูููุฒ ูุงุญุฏ). ุฅุฏุฎุงู ุงูุชุฑููุฒ ูู ุงูุฌููุฉ ุงููุนุทูุฉุ ูุฅุฏุฎุงู ูู ุงูุชุฑููุฒ ูู ุงูุฌููุฉ ุงูุฃุตููุฉ ูุงููุฏู ูู ุงูุฑููุฒ ุงููููุฒุฉ ุงูุชู ุชู ุฅุณูุงุทูุง ูุญุฏุฏุฉ ุจุฑููุฒูุง ุงููููุฒุฉ.

- ูุณุชุฎุฏู T5 ุชุถูููุงุช ููุงุณูุฉ ูุณุจูุฉ. ูููู ุฅุฌุฑุงุก ุชุฑููุฒ ุงูุฅุฏุฎุงู ุงููุงุฑุบ ุนูู ุงููุณุงุฑ ูุนูู ุงููููู.

- ุฑุงุฌุน ุฃูุณุงู [ุงูุชุฏุฑูุจ](#training)ุ ู[ุงูุงุณุชูุชุงุฌ](#inference)ุ ู[ุงูููุงุฑุฏ](#resources) ุฃุฏูุงู ููุญุตูู ุนูู ุฌููุน ุงูุชูุงุตูู ุงููุชุนููุฉ ุจุงูุงุณุชุฎุฏุงู.

ูุฃุชู T5 ุจุฃุญุฌุงู ูุฎุชููุฉ:

- [google-t5/t5-small](https://huggingface.co/google-t5/t5-small)
- [google-t5/t5-base](https://huggingface.co/google-t5/t5-base)
- [google-t5/t5-large](https://huggingface.co/google-t5/t5-large)
- [google-t5/t5-3b](https://huggingface.co/google-t5/t5-3b)
- [google-t5/t5-11b](https://huggingface.co/google-t5/t5-11b).

ุจูุงุกู ุนูู ูููุฐุฌ T5 ุงูุฃุตููุ ุฃุตุฏุฑุช Google ุจุนุถ ุงูุฃุนูุงู ุงููุงุญูุฉ:

- **T5v1.1**: T5v1.1 ูู ุฅุตุฏุงุฑ ูุญุณู ูู T5 ูุน ุจุนุถ ุงูุชุนุฏููุงุช ุงููุนูุงุฑูุฉุ ููู ููุฏุฑุจ ูุณุจููุง ุนูู C4 ููุท ุฏูู
ูุฒุฌ ุงูููุงู ุงูุฎุงุถุนุฉ ููุฅุดุฑุงู. ุฑุงุฌุน ูุซุงุฆู T5v1.1 ุงูููุฌูุฏุฉ [ููุง](t5v1.1).

- **mT5**: mT5 ูู ูููุฐุฌ T5 ูุชุนุฏุฏ ุงููุบุงุช. ููู ููุฏุฑุจ ูุณุจููุง ุนูู ูุฌููุนุฉ ุจูุงูุงุช mC4ุ ูุงูุชู ุชุดูู 101 ูุบุฉ. ุฑุงุฌุน
ูุซุงุฆู mT5 ุงูููุฌูุฏุฉ [ููุง](mt5).

- **byT5**: byT5 ูู ูููุฐุฌ T5 ููุฏุฑุจ ูุณุจููุง ุนูู ุชุณูุณูุงุช ุงูุจุงูุช ุจุฏูุงู ูู ุชุณูุณูุงุช ุงูุฑููุฒ ุงููุฑุนูุฉ SentencePiece. ุฑุงุฌุน
ูุซุงุฆู byT5 ุงูููุฌูุฏุฉ [ููุง](byt5).

- **UL2**: UL2 ูู ูููุฐุฌ ูุดุงุจู ูู T5 ููุฏุฑุจ ูุณุจููุง ุนูู ุฃูุฏุงู ุฅุฒุงูุฉ ุงูุถูุถุงุก ุงููุฎุชููุฉ

- **Flan-T5**: Flan ูู ุทุฑููุฉ ุชุฏุฑูุจ ูุณุจู ุชุนุชูุฏ ุนูู ุงููุทุงูุจุงุช. Flan-T5 ูู ููุงุฐุฌ T5 ูุฏุฑุจุฉ ุนูู ูุฌููุนุฉ Flan ูู
ูุฌููุนุงุช ุงูุจูุงูุงุช ุงูุชู ุชุดูู: `taskmaster2`ุ `djaym7/wiki_dialog`ุ `deepmind/code_contests`ุ `lambada`ุ `gsm8k`ุ `aqua_rat`ุ `esnli`ุ `quasc`ุ ู`qed`.

- **FLan-UL2**: ูููุฐุฌ UL2 ูุถุจูุท ุงูุฏูุฉ ุจุงุณุชุฎุฏุงู ุถุจุท ุฏูุฉ ุงููุทุงูุจุฉ "Flan" ููุฌููุนุฉ ุงูุจูุงูุงุช.

- **UMT5**: UmT5 ูู ูููุฐุฌ T5 ูุชุนุฏุฏ ุงููุบุงุช ููุฏุฑุจ ุนูู ูุฌููุนุฉ mC4 ูุชุนุฏุฏุฉ ุงููุบุงุช ุงููุญุณูุฉ ูุงููุญุฏุซุฉุ 29 ุชุฑููููู ุญุฑู ุนุจุฑ 107 ูุบุฉุ ุจุงุณุชุฎุฏุงู ุทุฑููุฉ ุฃุฎุฐ ุงูุนููุงุช ุงูุฌุฏูุฏุฉุ UniMax. ุฑุงุฌุน
ูุซุงุฆู mT5 ุงูููุฌูุฏุฉ [ููุง](umt5).
## ุงูุชุฏุฑูุจ

T5 ูู ูููุฐุฌ ุชุฑููุฒ ููู ุชุฑููุฒ ูุญูู ุฌููุน ูุดููุงุช ูุนุงูุฌุฉ ุงููุบุงุช ุงูุทุจูุนูุฉ ุฅูู ุชูุณูู ูุต ุฅูู ูุต. ูุชู ุชุฏุฑูุจู ุจุงุณุชุฎุฏุงู ุทุฑููุฉ "teacher forcing". ููุฐุง ูุนูู ุฃูู ุจุงููุณุจุฉ ููุชุฏุฑูุจุ ูุญุชุงุฌ ุฏุงุฆููุง ุฅูู ุชุณูุณู ุฅุฏุฎุงู ูุชุณูุณู ูุฏู ูุทุงุจู. ูุชู ุชุบุฐูุฉ ุชุณูุณู ุงูุฅุฏุฎุงู ุฅูู ุงููููุฐุฌ ุจุงุณุชุฎุฏุงู "input_ids". ูุชู ุชุญููู ุชุณูุณู ุงููุฏู ุฅูู ุงูููููุ ุฃู ูุชู ุฅุถุงูุฉ ุฑูุฒ ุชุณูุณู ุงูุจุฏุงูุฉ ูู ุงูุจุฏุงูุฉ ููุชู ุชุบุฐูุชู ุฅูู ูู ุงูุชุฑููุฒ ุจุงุณุชุฎุฏุงู "decoder_input_ids". ููู ุฃุณููุจ "teacher forcing"ุ ูุชู ุจุนุฏ ุฐูู ุฅุถุงูุฉ ุฑูุฒ "EOS" ุฅูู ุชุณูุณู ุงููุฏูุ ูุงูุฐู ูุชูุงูู ูุน "labels". ููุชู ุงุณุชุฎุฏุงู ุฑูุฒ "PAD" ููุง ูุฑููุฒ ุชุณูุณู ุงูุจุฏุงูุฉ. ูููู ุชุฏุฑูุจ T5 / ุถุจุทู ุงูุฏููู ุจุฃุณููุจ ุฅุดุฑุงูู ูุบูุฑ ุฅุดุฑุงูู ุนูู ุญุฏ ุณูุงุก.

ููููู ุงุณุชุฎุฏุงู [T5ForConditionalGeneration] (ุฃู ุงููุชุบูุฑ Tensorflow/Flax)ุ ูุงูุฐู ูุชุถูู ุฑุฃุณ ููุฐุฌุฉ ุงููุบุฉ ุฃุนูู ูู ุงูุชุฑููุฒ.

- ุงูุชุฏุฑูุจ ุบูุฑ ุงูุฅุดุฑุงูู ูุฅุฒุงูุฉ ุงูุชุดููุด

ูู ูุฐุง ุงูุฅุนุฏุงุฏุ ูุชู ููุงุน ุฃุฌุฒุงุก ูู ุชุณูุณู ุงูุฅุฏุฎุงู ุจูุงุณุทุฉ ูุง ูุณูู ุฑููุฒ ุงูุงุณุชุทูุงุน (ุฃู ุฑููุฒ ุงูููุงุน ุงููุฑูุฏุฉ) ููุชู ุชุดููู ุชุณูุณู ุงูุฅุฎุฑุงุฌ ูุชูุตูู ูุฑููุฒ ุงูุงุณุชุทูุงุน ููุณูุง ูุฑููุฒ ุงูููุงุน "ุงูุญููููุฉ". ููุซู ูู ุฑูุฒ ุงุณุชุทูุงุน ุฑูุฒ ููุงุน ูุฑูุฏ ููุฐู ุงูุฌููุฉ ููุฌุจ ุฃู ูุจุฏุฃ ุจู "<extra_id_0>"ุ "<extra_id_1>"ุ ... ุญุชู "<extra_id_99>". ูุงูุชุฑุงุถูุ ุชุชููุฑ 100 ุฑููุฒ ุงุณุชุทูุงุน ูู [T5Tokenizer].

ุนูู ุณุจูู ุงููุซุงูุ ูุฌุจ ูุนุงูุฌุฉ ุงูุฌููุฉ "The cute dog walks in the park" ูุน ุงูุฃููุนุฉ ุงูููุถูุนุฉ ุนูู "cute dog" ู "the" ุนูู ุงููุญู ุงูุชุงูู:

```python
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration

>>> tokenizer = T5Tokenizer.from_pretrained("google-t5/t5-small")
>>> model = T5ForConditionalGeneration.from_pretrained("google-t5/t5-small")

>>> input_ids = tokenizer("The <extra_id_0> walks in <extra_id_1> park", return_tensors="pt").input_ids
>>> labels = tokenizer("<extra_id_0> cute dog <extra_id_1> the <extra_id_2>", return_tensors="pt").input_ids

>>> # ุชููู ุฏุงูุฉ ุงูุชูุฏูู ุชููุงุฆููุง ุจุฅูุดุงุก decoder_input_ids ุงูุตุญูุญุฉ
>>> loss = model(input_ids=input_ids, labels=labels).loss
>>> loss.item()
3.7837
```

ุฅุฐุง ููุช ููุชููุง ุจุงูุชุฏุฑูุจ ุงููุณุจู ูู T5 ุนูู ูุฌููุนุฉ ุจูุงูุงุช ุฌุฏูุฏุฉุ ุชุญูู ูู [run_t5_mlm_flax.py](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling) script ูู ุฏููู ุงูุฃูุซูุฉ.

- ุงูุชุฏุฑูุจ ุงูุฅุดุฑุงูู

ูู ูุฐุง ุงูุฅุนุฏุงุฏุ ูููู ุชุณูุณู ุงูุฅุฏุฎุงู ูุงูุฅุฎุฑุงุฌ ุนุจุงุฑุฉ ุนู ุฑุณู ุฎุฑุงุฆุท ููุงุณู ููุฅุฏุฎุงู ูุงูุฅุฎุฑุงุฌ ุชุณูุณู ุฅูู ุชุณูุณู. ูููุชุฑุถ ุฃููุง ูุฑูุฏ ุถุจุท ุงููููุฐุฌ ุงูุฏููู ููุชุฑุฌูุฉ ุนูู ุณุจูู ุงููุซุงูุ ููุฏููุง ูุซุงู ุชุฏุฑูุจ: ุชุณูุณู ุงูุฅุฏุฎุงู "The house is wonderful." ูุชุณูุณู ุงูุฅุฎุฑุงุฌ "Das Haus ist wunderbar."ุ ููุฌุจ ุฅุนุฏุงุฏููุง ูููููุฐุฌ ุนูู ุงููุญู ุงูุชุงูู:

```python
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration

>>> tokenizer = T5Tokenizer.from_pretrained("google-t5/t5-small")
>>> model = T5ForConditionalGeneration.from_pretrained("google-t5/t5-small")

>>> input_ids = tokenizer("translate English to German: The house is wonderful.", return_tensors="pt").input_ids
>>> labels = tokenizer("Das Haus ist wunderbar.", return_tensors="pt").input_ids

>>> # ุชููู ุฏุงูุฉ ุงูุชูุฏูู ุชููุงุฆููุง ุจุฅูุดุงุก decoder_input_ids ุงูุตุญูุญุฉ
>>> loss = model(input_ids=input_ids, labels=labels).loss
>>> loss.item()
0.2542
```

ููุง ุชุฑููุ ูุญุชุงุฌ ุงููููุฐุฌ ุฅูู ุฅุฏุฎุงููู ููุท ูุญุณุงุจ ุงูุฎุณุงุฑุฉ: `input_ids` (ุงูุชู ูู `input_ids` ูุชุณูุณู ุงูุฅุฏุฎุงู ุงููุดูุฑ) ู`labels` (ุงูุชู ูู `input_ids` ูุชุณูุณู ุงููุฏู ุงููุดูุฑ). ุณูููู ุงููููุฐุฌ ุชููุงุฆููุง ุจุฅูุดุงุก `decoder_input_ids` ุจูุงุกู ุนูู `labels`ุ ุนู ุทุฑูู ุชุญููููุง ููุถุน ูุงุญุฏ ุฅูู ุงููููู ูุฅุถุงูุฉ `config.decoder_start_token_id`ุ ูุงูุฐู ูุณุงูู 0 ูู T5 (ุฃู ูุนุฑู ุฑูุฒ ุงูุญุดู). ูุงุญุธ ุฃูุถูุง ุจุงุฏุฆุฉ ุงููููุฉ: ูููู ุจุฅุถุงูุฉ ุจุงุฏุฆุฉ ุฅูู ุชุณูุณู ุงูุฅุฏุฎุงู ุจู 'translate English to German: ' ูุจู ุชุดููุฑู. ุณูุณุงุนุฏ ูุฐุง ูู ุชุญุณูู ุงูุฃุฏุงุกุ ุญูุซ ุชู ุงุณุชุฎุฏุงู ุจุงุฏุฆุฉ ุงููููุฉ ูุฐู ุฃุซูุงุก ุงูุชุฏุฑูุจ ุงููุณุจู ูู T5.

ููุน ุฐููุ ูุฅู ุงููุซุงู ุฃุนูุงู ููุถุญ ูุซุงู ุชุฏุฑูุจ ูุงุญุฏ ููุท. ูู ุงูููุงุฑุณุฉ ุงูุนูููุฉุ ูุชู ุชุฏุฑูุจ ููุงุฐุฌ ุงูุชุนูู ุงูุนููู ูู ุฏูุนุงุช. ููุฐุง ูุนูู ุฃูู ูุฌุจ ุนูููุง ุฅุถุงูุฉ ุฃูุซูุฉ / ุงูุชุทุงุนูุง ุฅูู ููุณ ุงูุทูู. ุจุงููุณุจุฉ ูููุงุฐุฌ ุงูุชุฑููุฒ ููู ุงูุชุฑููุฒุ ูุชู ุนุงุฏุฉู ุชุญุฏูุฏ `max_source_length` ู`max_target_length`ุ ูุงูุชู ุชุญุฏุฏ ุงูุทูู ุงูุฃูุตู ูุชุณูุณูุงุช ุงูุฅุฏุฎุงู ูุงูุฅุฎุฑุงุฌ ุนูู ุงูุชูุงูู (ูุฅูุง ูุชู ุงูุชุทุงุนูุง). ูุฌุจ ุถุจุท ูุฐู ุงูููู ุจุนูุงูุฉ ุงุนุชูุงุฏูุง ุนูู ุงููููุฉ.

ุจุงูุฅุถุงูุฉ ุฅูู ุฐููุ ูุฌุจ ุงูุชุฃูุฏ ูู ุฃู ูุนุฑูุงุช ุฑููุฒ ุงูุญุดู ูู `labels` ูุง ุชุคุฎุฐ ูู ุงูุงุนุชุจุงุฑ ุจูุงุณุทุฉ ุฏุงูุฉ ุงูุฎุณุงุฑุฉ. ูู PyTorch ูTensorflowุ ูููู ุงูููุงู ุจุฐูู ุนู ุทุฑูู ุงุณุชุจุฏุงููุง ุจู -100ุ ููู `ignore_index` ูู `CrossEntropyLoss`. ูู Flaxุ ููููู ุงุณุชุฎุฏุงู `decoder_attention_mask` ูุชุฌุงูู ุงูุฑููุฒ ุงููุถุงูุฉ ูู ุงูุฎุณุงุฑุฉ (ุฑุงุฌุน [ูุฎุทูุท ููุฎุต Flax](https://github.com/huggingface/transformers/tree/main/examples/flax/summarization) ููุญุตูู ุนูู ุงูุชูุงุตูู). ููุง ูููู ุจุชูุฑูุฑ `attention_mask` ูุฅุฏุฎุงู ุฅุถุงูู ุฅูู ุงููููุฐุฌุ ูุงูุฐู ูุชุฃูุฏ ูู ุชุฌุงูู ุฑููุฒ ุงูุญุดู ููุฅุฏุฎุงูุงุช. ููุถุญ ูุซุงู ุงูููุฏ ุฃุฏูุงู ูู ูุฐุง.

```python
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration
>>> import torch

>>> tokenizer = T5Tokenizer.from_pretrained("google-t5/t5-small")
>>> model = T5ForConditionalGeneration.from_pretrained("google-t5/t5-small")

>>> # ูุงุชุงู ุงููุนููุชุงู ุงูุฎุงุตุชุงู ุจุงูููุงู
>>> max_source_length = 512
>>> max_target_length = 128

>>> # ูููุชุฑุถ ุฃู ูุฏููุง ูุซุงููู ุชุฏุฑูุจ ุงูุชุงูููู:
>>> input_sequence_1 = "Welcome to NYC"
>>> output_sequence_1 = "Bienvenue ร NYC"

>>> input_sequence_2 = "HuggingFace is a company"
>>> output_sequence_2 = "HuggingFace est une entreprise"

>>> # ุชุดููุฑ ุงูุฅุฏุฎุงูุงุช
>>> task_prefix = "translate English to French: "
>>> input_sequences = [input_sequence_1, input_sequence_2]

>>> encoding = tokenizer(
...     [task_prefix + sequence for sequence in input_sequences],
...     padding="longest"ุ
...     max_length=max_source_lengthุ
...     truncation=Trueุ
...     return_tensors="pt"ุ
... )

>>> input_ids, attention_mask = encoding.input_ids, encoding.attention_mask

>>> # ุชุดููุฑ ุงูุฃูุฏุงู
>>> target_encoding = tokenizer(
...     [output_sequence_1, output_sequence_2],
...     padding="longest"ุ
...     max_length=max_target_lengthุ
...     truncation=Trueุ
...     return_tensors="pt"ุ
... )
>>> labels = target_encoding.input_ids

>>> # ุงุณุชุจุฏู ูุนุฑูุงุช ุฑููุฒ ุงูุญุดู ูู ุงูุนูุงูุงุช ุจู -100 ุญุชู ูุชู ุชุฌุงูููุง ุจูุงุณุทุฉ ุงูุฎุณุงุฑุฉ
>>> labels[labels == tokenizer.pad_token_id] = -100

>>> # ุชูุฑูุฑ ุฅูู ุงูุฃูุงู
>>> loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss
>>> loss.item()
0.188
```

ูุตุงุฆุญ ุชุฏุฑูุจ ุฅุถุงููุฉ:

- ุชุญุชุงุฌ ููุงุฐุฌ T5 ุฅูู ูุนุฏู ุชุนูู ุฃุนูู ููููุงู ูู ุงูุงูุชุฑุงุถู ุงููุญุฏุฏ ูู `Trainer` ุนูุฏ ุงุณุชุฎุฏุงู ูุญุณู AdamW. ุนุงุฏุฉู ูุง ุชุนูู ุงูููู 1e-4 ู3e-4 ุจุดูู ุฌูุฏ ููุนุธู ุงููุดููุงุช (ุงูุชุตูููุ ูุงูุชูุฎูุตุ ูุงูุชุฑุฌูุฉุ ูุงูุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉุ ูุชูููุฏ ุงูุฃุณุฆูุฉ). ูุงุญุธ ุฃู T5 ุชู ุชุฏุฑูุจู ูุณุจููุง ุจุงุณุชุฎุฏุงู ูุญุณู AdaFactor.

ููููุง [ููุฐุง ุงูููุดูุฑ ูู ุงูููุชุฏู](https://discuss.huggingface.co/t/t5-finetuning-tips/684)ุ ูุฅู ุจุงุฏุฆุงุช ุงูููุงู ูููุฉ ุนูุฏ (1) ุฅุฌุฑุงุก ุงูุชุฏุฑูุจ ูุชุนุฏุฏ ุงูููุงู (2) ูููุชู ูุดุงุจูุฉ ุฃู ูุฑุชุจุทุฉ ุจูููุฉ ุฅุดุฑุงููุฉ ุชู ุงุณุชุฎุฏุงููุง ูู ูุฒูุฌ ุงูุชุฏุฑูุจ ุงููุณุจู ูู T5 (ุฑุงุฌุน ุงูุชุฐููู D ูู [ุงููุฑูุฉ](https://arxiv.org/pdf/1910.10683.pdf) ููุญุตูู ุนูู ุจุงุฏุฆุงุช ุงูููุงู ุงููุณุชุฎุฏูุฉ).

ุฅุฐุง ููุช ุชุชุฏุฑุจ ุนูู TPUุ ููู ุงููุณุชุญุณู ุฅุถุงูุฉ ุฃูุซูุฉ ูุฌููุนุฉ ุงูุจูุงูุงุช ุจุฃููููุง ุฅูู ููุณ ุงูุทูู ุฃู ุงุณุชุฎุฏุงู "pad_to_multiple_of" ููุญุตูู ุนูู ุนุฏุฏ ุตุบูุฑ ูู ุฃุญุฌุงู ุงูุฏูุงุก ุงููุญุฏุฏุฉ ูุณุจููุง ูุชูุงุณุจ ุฌููุน ุงูุฃูุซูุฉ. ูุง ูููุตุญ ุจุฅุถุงูุฉ ุงูุฏูุนุงุช ุฏููุงูููููุง ุฅูู ุฃุทูู ูุซุงู ุนูู TPU ูุฃูู ูุคุฏู ุฅูู ุฅุนุงุฏุฉ ุชุฌููุน ููู ุดูู ุฏูุนุฉ ูุชู ููุงุฌูุชู ุฃุซูุงุก ุงูุชุฏุฑูุจุ ููุง ูุจุทุฆ ุงูุชุฏุฑูุจ ุจุดูู ูุจูุฑ. (ุฅุถุงูุฉ ููุท ุฅูู ุฃุทูู ูุซุงู ูู ุฏูุนุฉ) ูุคุฏู ุฅูู ุชุฏุฑูุจ ุจุทูุก ููุบุงูุฉ ุนูู TPU.

## ุงูุงุณุชูุชุงุฌ

ูู ููุช ุงูุงุณุชูุชุงุฌุ ูููุตุญ ุจุงุณุชุฎุฏุงู [`~generation.GenerationMixin.generate`]. ุชูุชู ูุฐู ุงูุทุฑููุฉ ุจุชุดููุฑ ุงูุฅุฏุฎุงู ูุชุบุฐูุฉ ุงูุญุงูุงุช ุงููุฎููุฉ ุงููุดูุฑุฉ ุนุจุฑ ุทุจูุงุช ุงูุงูุชูุงู ุงููุชูุงุทุน ุฅูู ูู ุงูุชุฑููุฒ ูุชูููุฏ ุฅุฎุฑุงุฌ ูู ุงูุชุฑููุฒ ุชููุงุฆููุง. ุชุญูู ูู [ูุฐู ุงูุชุฏูููุฉ](https://huggingface.co/blog/how-to-generate) ููุนุฑูุฉ ุฌููุน ุงูุชูุงุตูู ุญูู ุชูููุฏ ุงููุต ุจุงุณุชุฎุฏุงู Transformers.

ููุงู ุฃูุถูุง [ูุฐู ุงูุชุฏูููุฉ](https://huggingface.co/blog/encoder-decoder#encoder-decoder) ุงูุชู ุชุดุฑุญ ููููุฉ ุนูู ุงูุชูููุฏ ุจุดูู ุนุงู ูู ููุงุฐุฌ ุงูุชุฑููุฒ ููู ุงูุชุฑููุฒ.

```python
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration

>>> tokenizer = T5Tokenizer.from_pretrained("google-t5/t5-small")
>>> model = T5ForConditionalGeneration.from_pretrained("google-t5/t5-small")

>>> input_ids = tokenizer("translate English to German: The house is wonderful.", return_tensors="pt").input_ids
>>> outputs = model.generate(input_ids)
>>> print(tokenizer.decode(outputs[0], skip_special_tokens=True))
Das Haus ist wunderbar.
```

ูุงุญุธ ุฃู T5 ูุณุชุฎุฏู `pad_token_id` ูู `decoder_start_token_id`ุ ูุฐุง ุนูุฏ ุฅุฌุฑุงุก ุงูุชูููุฏ ุจุฏูู ุงุณุชุฎุฏุงู [`~generation.GenerationMixin.generate`]ุ ุชุฃูุฏ ูู ุจุฏุฆู ุจู `pad_token_id`.

ููุถุญ ุงููุซุงู ุฃุนูุงู ูุซุงููุง ูุงุญุฏูุง ููุท. ููููู ุฃูุถูุง ุฅุฌุฑุงุก ุงูุงุณุชุฏูุงู ุนูู ุฏูุนุงุชุ ููุง ููู:

```python
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration

>>> tokenizer = T5Tokenizer.from_pretrained("google-t5/t5-small")
>>> model = T5ForConditionalGeneration.from_pretrained("google-t5/t5-small")

>>> task_prefix = "translate English to German: "
>>> # ุงุณุชุฎุฏุงู ุฌูู ุฐุงุช ุฃุทูุงู ูุฎุชููุฉ ูุงุฎุชุจุงุฑ ุงูุฏูุนุงุช
>>> sentences = ["The house is wonderful.", "I like to work in NYC."]

>>> inputs = tokenizer([task_prefix + sentence for sentence in sentences], return_tensors="pt", padding=True)

>>> output_sequences = model.generate(
...     input_ids=inputs["input_ids"],
...     attention_mask=inputs["attention_mask"],
...     do_sample=False,  # ุชุนุทูู ุงูุนููุงุช ูุงุฎุชุจุงุฑ ูุง ุฅุฐุง ูุงู ุงูุฏูุนุงุช ุชุคุซุฑ ุนูู ุงูุฅุฎุฑุงุฌ
... )

>>> print(tokenizer.batch_decode(output_sequences, skip_special_tokens=True))
['Das Haus ist wunderbar.', 'Ich arbeite gerne in NYC.']
```

ูุธุฑูุง ูุฃู T5 ุชู ุชุฏุฑูุจู ุนูู ูุฏู ุฅุฒุงูุฉ ุงูุชุดููุด ูู ุงููุทุงูุ ููููู ุงุณุชุฎุฏุงูู ููุชูุจุค ุจุฑููุฒ ุงูุงุณุชุทูุงุน (ุงูุฑููุฒ ุงููููุนุฉ) ุฃุซูุงุก ุงูุงุณุชุฏูุงู. ุซู ูุชู ูุถุน ุงูุฑููุฒ ุงููุชููุนุฉ ุจูู ุฑููุฒ ุงูุงุณุชุทูุงุน.

```python
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration

>>> tokenizer = T5Tokenizer.from_pretrained("google-t5/t5-small")
>>> model = T5ForConditionalGeneration.from_pretrained("google-t5/t5-small")

>>> input_ids = tokenizer("The <extra_id_0> walks in <extra_id_1> park", return_tensors="pt").input_ids

>>> sequence_ids = model.generate(input_ids)
>>> sequences = tokenizer.batch_decode(sequence_ids)
>>> sequences
['<pad> <extra_id_0> park offers <extra_id_1> the <extra_id_2> park.</s>']
```

## ุงูุฃุฏุงุก

ุฅุฐุง ููุช ุชุฑูุฏ ุฃุฏุงุก ุชุฏุฑูุจ ูุงุณุชุฏูุงู ุฃุณุฑุนุ ููู ุจุชุซุจูุช [NVIDIA APEX](https://github.com/NVIDIA/apex#quick-start) ููุนุงูุฌุงุช NVIDIA GPUุ ุฃู [ROCm APEX](https://github.com/ROCmSoftwarePlatform/apex) ููุนุงูุฌุงุช AMD GPUุ ุซู ุณูุณุชุฎุฏู ุงููููุฐุฌ ุชููุงุฆููุง `apex.normalization.FusedRMSNorm` ุจุฏูุงู ูู `T5LayerNorm`. ูุณุชุฎุฏู ุงูุณุงุจู ููุงุฉ ูุฏูุฌุฉ ูุญุณูุฉ ููู ุฃุณุฑุน ุนุฏุฉ ูุฑุงุช ูู ุงูุฃุฎูุฑ.
## ุงูููุงุฑุฏ

ูุงุฆูุฉ ุจููุงุฑุฏ Hugging Face ุงูุฑุณููุฉ ูููุงุฑุฏ ุงููุฌุชูุน (ูุดุงุฑ ุฅูููุง ุจุงูุฑูุฒ ๐) ููุณุงุนุฏุชู ูู ุงูุจุฏุก ูุน T5. ุฅุฐุง ููุช ููุชููุง ุจุชูุฏูู ููุฑุฏ ูุฅุฏุฑุงุฌู ููุงุ ููุฑุฌู ูุชุญ ุทูุจ ุณุญุจ Pull Request ูุณูููู ุจูุฑุงุฌุนุชู! ูููุถู ุฃู ููุธูุฑ ุงูููุฑุฏ ุดูุฆูุง ุฌุฏูุฏูุง ุจุฏูุงู ูู ุชูุฑุงุฑ ููุฑุฏ ููุฌูุฏ.

<PipelineTag pipeline="text-classification"/>

- ุฏูุชุฑ ููุงุญุธุงุช ุญูู ููููุฉ [ุถุจุท T5 ุงูุฏููู ููุชุตููู ูุงูุงุฎุชูุงุฑ ุงููุชุนุฏุฏ](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb).

- ุฏูุชุฑ ููุงุญุธุงุช ุญูู ููููุฉ [ุถุจุท T5 ุงูุฏููู ูุงุณุชุฎุฑุงุฌ ุงููุดุงุนุฑ ูู ุงููุต](https://colab.research.google.com/github/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb). ๐

<PipelineTag pipeline="token-classification"/>

- ุฏูุชุฑ ููุงุญุธุงุช ุญูู ููููุฉ [ุถุจุท T5 ุงูุฏููู ููุชุนุฑู ุนูู ุงูููุงูุงุช ุงููุณูุงุฉ](https://colab.research.google.com/drive/1obr78FY_cBmWY5ODViCmzdY6O1KB65Vc?usp=sharing). ๐

<PipelineTag pipeline="text-generation"/>

- ุฏูุชุฑ ููุงุญุธุงุช ุญูู [ุถุจุท CodeT5 ุงูุฏููู ูุชูููุฏ ุงูุชูุซููุงุช ูู ููุฏ Ruby](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/T5/Fine_tune_CodeT5_for_generating_docstrings_from_Ruby_code.ipynb).

<PipelineTag pipeline="summarization"/>

- ุฏูุชุฑ ููุงุญุธุงุช ูุถุจุท [T5-base-dutch ุงูุฏููู ูุฃุฏุงุก ุงูููุฎุต ุจุงููุบุฉ ุงูููููุฏูุฉ ุนูู ูุญุฏุฉ ูุนุงูุฌุฉ Tensor Processing Unit (TPU)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/T5/Fine_tuning_Dutch_T5_base_on_CNN_Daily_Mail_for_summarization_(on_TPU_using_HuggingFace_Accelerate).ipynb).

- ุฏูุชุฑ ููุงุญุธุงุช ุญูู ููููุฉ [ุถุจุท T5 ุงูุฏููู ููุชูุฎูุต ูู PyTorch ูุชุชุจุน ุงูุชุฌุงุฑุจ ุจุงุณุชุฎุฏุงู WandB](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb#scrollTo=OKRpFvYhBauC). ๐

- ููุดูุฑ ูุฏููุฉ ุญูู [ุงูุชุฏุฑูุจ ุงูููุฒุน: ุชุฏุฑูุจ BART/T5 ููุชูุฎูุต ุจุงุณุชุฎุฏุงู ๐ค Transformers ูAmazon SageMaker](https://huggingface.co/blog/sagemaker-distributed-training-seq2seq).

- [`T5ForConditionalGeneration`] ูุฏุนูู ุจูุงุณุทุฉ [ุณูุฑูุจุช ุงููุซุงู](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization) ู[ุฏูุชุฑ ุงูููุงุญุธุงุช](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb).

- [`TFT5ForConditionalGeneration`] ูุฏุนูู ุจูุงุณุทุฉ [ุณูุฑูุจุช ุงููุซุงู](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/summarization) ู[ุฏูุชุฑ ุงูููุงุญุธุงุช](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb).

- [`FlaxT5ForConditionalGeneration`] ูุฏุนูู ุจูุงุณุทุฉ [ุณูุฑูุจุช ุงููุซุงู](https://github.com/huggingface/transformers/tree/main/examples/flax/summarization).

- ูุตู [ุงูุชูุฎูุต](https://huggingface.co/course/chapter7/5?fw=pt#summarization) ูู ุฏูุฑุฉ ๐ค Hugging Face.

- [ุฏููู ูููุฉ ุงูุชูุฎูุต](../tasks/summarization)

<PipelineTag pipeline="fill-mask"/>

- [`FlaxT5ForConditionalGeneration`] ูุฏุนูู ุจูุงุณุทุฉ [ุณูุฑูุจุช ุงููุซุงู](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#t5-like-span-masked-language-modeling) ูุชุฏุฑูุจ T5 ุจูุฏู ูููุฐุฌ ุงููุบุฉ ุงููููุนุฉ. ููุถุญ ุงูุณูุฑุจุช ุฃูุถูุง ููููุฉ ุชุฏุฑูุจ ูุญูู T5. [`FlaxT5ForConditionalGeneration`] ูุฏุนูู ุฃูุถูุง ุจูุงุณุทุฉ ูุฐุง [ุฏูุชุฑ ุงูููุงุญุธุงุช](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked_language_modeling_flax.ipynb).

<PipelineTag pipeline="translation"/>

- [`T5ForConditionalGeneration`] ูุฏุนูู ุจูุงุณุทุฉ [ุณูุฑูุจุช ุงููุซุงู](https://github.com/huggingface/transformers/tree/main/examples/pytorch/translation) ู[ุฏูุชุฑ ุงูููุงุญุธุงุช](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb).

- [`TFT5ForConditionalGeneration`] ูุฏุนูู ุจูุงุณุทุฉ [ุณูุฑูุจุช ุงููุซุงู](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/translation) ู[ุฏูุชุฑ ุงูููุงุญุธุงุช](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb).

- [ุฏููู ูููุฉ ุงูุชุฑุฌูุฉ](../tasks/translation)

<PipelineTag pipeline="question-answering"/>

- ุฏูุชุฑ ููุงุญุธุงุช ุญูู ููููุฉ [ุถุจุท T5 ุงูุฏููู ููุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉ ุจุงุณุชุฎุฏุงู TensorFlow 2](https://colab.research.google.com/github/snapthat/TF-T5-text-to-text/blob/master/snapthatT5/notebooks/TF-T5-Datasets%20Training.ipynb). ๐

- ุฏูุชุฑ ููุงุญุธุงุช ุญูู ููููุฉ [ุถุจุท T5 ุงูุฏููู ููุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉ ุนูู ูุญุฏุฉ ูุนุงูุฌุฉ Tensor Processing Unit (TPU)](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb#scrollTo=QLGiFCDqvuil).

๐ **ุงููุดุฑ**

- ููุดูุฑ ูุฏููุฉ ุญูู ููููุฉ ูุดุฑ [T5 11B ููุงุณุชูุชุงุฌ ุจุฃูู ูู 500 ุฏููุงุฑ](https://www.philschmid.de/deploy-t5-11b).

## T5Config

[[autodoc]] T5Config

## T5Tokenizer

[[autodoc]] T5Tokenizer

- build_inputs_with_special_tokens

- get_special_tokens_mask

- create_token_type_ids_from_sequences

- save_vocabulary

## T5TokenizerFast

[[autodoc]] T5TokenizerFast

<frameworkcontent>

<pt>

## T5Model

[[autodoc]] T5Model

- forward

## T5ForConditionalGeneration

[[autodoc]] T5ForConditionalGeneration

- forward

## T5EncoderModel

[[autodoc]] T5EncoderModel

- forward

## T5ForSequenceClassification

[[autodoc]] T5ForSequenceClassification

- forward

## T5ForTokenClassification

[[autodoc]] T5ForTokenClassification

- forward

## T5ForQuestionAnswering

[[autodoc]] T5ForQuestionAnswering

- forward

</pt>

<tf>

## TFT5Model

[[autodoc]] TFT5Model

- call

## TFT5ForConditionalGeneration

[[autodoc]] TFT5ForConditionalGeneration

- call

## TFT5EncoderModel

[[autodoc]] TFT5EncoderModel

- call

</tf>

<jax>

## FlaxT5Model

[[autodoc]] FlaxT5Model

- __call__

- encode

- decode

## FlaxT5ForConditionalGeneration

[[autodoc]] FlaxT5ForConditionalGeneration

- __call__

- encode

- decode

## FlaxT5EncoderModel

[[autodoc]] FlaxT5EncoderModel

- __call__

</jax>

</frameworkcontent>