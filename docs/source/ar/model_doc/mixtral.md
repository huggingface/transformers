# Mixtral

## ูุธุฑุฉ ุนุงูุฉ
ุชู ุชูุฏูู Mixtral-8x7B ูู [ููุดูุฑ ูุฏููุฉ Mixtral of Experts](https://mistral.ai/news/mixtral-of-experts/) ุจูุงุณุทุฉ Albert Jiangุ ูAlexandre Sablayrollesุ ูArthur Menschุ ูChris Bamfordุ ูDevendra Singh Chaplotุ ูDiego de las Casasุ ูFlorian Bressandุ ูGianna Lengyelุ ูGuillaume Lampleุ ูLรฉlio Renard Lavaudุ ูLucile Saulnierุ ูMarie-Anne Lachauxุ ูPierre Stockุ ูTeven Le Scaoุ ูThibaut Lavrilุ ูThomas Wangุ ูTimothรฉe Lacroixุ ูWilliam El Sayed.

ุชููู ููุฏูุฉ ููุดูุฑ ุงููุฏููุฉ:

> "ุงููููุ ููุฎุฑ ุงููุฑูู ุจุฅุทูุงู Mixtral 8x7Bุ ููู ูููุฐุฌ ุนุงูู ุงูุฌูุฏุฉ ูู ููุงุฐุฌ ุงููุฒุฌ ุงููุงุฏุฑ ููุฎุจุฑุงุก (SMoE) ุฐู ุฃูุฒุงู ููุชูุญุฉ. ูุฑุฎุต ุจููุฌุจ Apache 2.0. ูุชููู Mixtral ุนูู Llama 2 70B ูู ูุนุธู ุงููุนุงููุฑ ุงููุฑุฌุนูุฉ ุจุณุฑุนุฉ ุงุณุชุฏูุงู ุฃุณุฑุน 6 ูุฑุงุช. ุฅูู ุฃููู ูููุฐุฌ ุฐู ูุฒู ููุชูุญ ุจุฑุฎุตุฉ ูุณููุญุฉุ ูุฃูุถู ูููุฐุฌ ุจุดูู ุนุงู ูููุง ูุชุนูู ุจุงูููุงูุถุงุช ุจูู ุงูุชูููุฉ ูุงูุฃุฏุงุก. ูุนูู ูุฌู ุงูุฎุตูุตุ ูุฅูู ูุทุงุจู ุฃุฏุงุก GPT3.5 ุฃู ูุชููู ุนููู ูู ูุนุธู ุงููุนุงููุฑ ุงููุฑุฌุนูุฉ ุงูููุงุณูุฉ."

Mixtral-8x7B ูู ุซุงูู ูููุฐุฌ ูุบุฉ ูุจูุฑ (LLM) ุฃุตุฏุฑุชู [mistral.ai](https://mistral.ai/)ุ ุจุนุฏ [Mistral-7B](mistral).

### ุงูุชูุงุตูู ุงููุนูุงุฑูุฉ
Mixtral-8x7B ูู ูุญูู ูู ุชุดููุฑ ููุท ูุน ุฎูุงุฑุงุช ุงูุชุตููู ุงููุนูุงุฑู ุงูุชุงููุฉ:

- Mixtral ูู ูููุฐุฌ ูุฒูุฌ ูู ุงูุฎุจุฑุงุก (MoE) ูุน 8 ุฎุจุฑุงุก ููู ุดุจูุฉ ุนุตุจูุฉ ูุชุนุฏุฏุฉ ุงูุทุจูุงุช (MLP)ุ ุจุฅุฌูุงูู 45 ูููุงุฑ ูุนููุฉ. ููุฒูุฏ ูู ุงููุนูููุงุช ุญูู ุงููุฒุฌ ูู ุงูุฎุจุฑุงุกุ ูุฑุฌู ุงูุฑุฌูุน ุฅูู [ููุดูุฑ ุงููุฏููุฉ](https://huggingface.co/blog/moe).

- ุนูู ุงูุฑุบู ูู ุฃู ุงููููุฐุฌ ูุญุชูู ุนูู 45 ูููุงุฑ ูุนููุฉุ ุฅูุง ุฃู ุงูููุจููุชุฑ ุงููุทููุจ ูุฅุฌุฑุงุก ุชูุฑูุฑ ุฃูุงูู ูุงุญุฏ ูู ููุณู ุงููุทููุจ ููููุฐุฌ ูุน 14 ูููุงุฑ ูุนููุฉ. ููุฑุฌุน ุฐูู ุฅูู ุฃูู ุนูู ุงูุฑุบู ูู ุถุฑูุฑุฉ ุชุญููู ูู ุฎุจูุฑ ูู ุฐุงูุฑุฉ ุงููุตูู ุงูุนุดูุงุฆู (ูุชุทูุจุงุช ุฐุงูุฑุฉ ุงููุตูู ุงูุนุดูุงุฆู ูุซู 70B)ุ ูุชู ุฅุฑุณุงู ูู ุฑูุฒ ูู ุงูุฑููุฒ ุงููุฎููุฉ ูุฑุชูู (ุงูุชุตููู ุงูุฃุนูู 2) ูุจุงูุชุงูู ูุฅู ุงูููุจููุชุฑ (ุงูุนูููุฉ ุงููุทููุจุฉ ูู ูู ุนูููุฉ ุญุณุงุจูุฉ ุฃูุงููุฉ) ูู ูุฌุฑุฏ 2 X sequence_length.

ุชูุงุตูู ุงูุชูููุฐ ุงูุชุงููุฉ ูุดุชุฑูุฉ ูุน ุงููููุฐุฌ ุงูุฃูู ูู Mistral AI [Mistral-7B](mistral):

- ูุงูุฐุฉ ุงูุฒูุงู ุงูุงูุชูุงู - ูุฏุฑุจุฉ ุจุทูู ุณูุงู 8 ูููู ุจุงูุช ูุญุฌู ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ุซุงุจุชุ ูุน ูุทุงู ุงูุชูุงู ูุธุฑู ูุจูุบ 128 ุฃูู ุฑูุฒ.

- GQA (ูุฌููุนุฉ Query Attention) - ุชุณูุญ ุจุงุณุชุฏูุงู ุฃุณุฑุน ูุญุฌู ุฐุงูุฑุฉ ุชุฎุฒูู ูุคูุช ุฃูู.

- ูุญุฑู Byte-fallback BPE - ูุถูู ุนุฏู ุชุนููู ุงูุฃุญุฑู ูุทูููุง ุฅูู ุฑููุฒ ุฎุงุฑุฌ ุงูููุฑุฏุงุช.

ููุญุตูู ุนูู ูุฒูุฏ ูู ุงูุชูุงุตููุ ูุฑุฌู ุงูุฑุฌูุน ุฅูู [ููุดูุฑ ุงููุฏููุฉ](https://mistral.ai/news/mixtral-of-experts/).

### ุงูุชุฑุฎูุต
ุชู ุฅุตุฏุงุฑ `Mixtral-8x7B` ุจููุฌุจ ุชุฑุฎูุต Apache 2.0.

## ูุตุงุฆุญ ุงูุงุณุชุฎุฏุงู
ุฃุตุฏุฑ ูุฑูู Mistral ููุทุชู ุชูุชูุด:

- ูููุฐุฌ ุฃุณุงุณูุ [Mixtral-8x7B-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)ุ ุชู ุชุฏุฑูุจู ูุณุจููุง ููุชูุจุค ุจุงูุฑูุฒ ุงูุชุงูู ุนูู ุจูุงูุงุช ุนูู ูุทุงู ุงูุฅูุชุฑูุช.

- ูููุฐุฌ ุถุจุท ุงูุชุนูููุงุชุ [Mixtral-8x7B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)ุ ููู ุงููููุฐุฌ ุงูุฃุณุงุณู ุงูุฐู ุชู ุชุญุณููู ูุฃุบุฑุงุถ ุงูุฏุฑุฏุดุฉ ุจุงุณุชุฎุฏุงู ุงูุถุจุท ุงูุฏููู ุงูุฎุงุถุน ููุฅุดุฑุงู (SFT) ูุงูุชุญุณูู ุงููุจุงุดุฑ ููุฃูุถููุงุช (DPO).

ูููู ุงุณุชุฎุฏุงู ุงููููุฐุฌ ุงูุฃุณุงุณู ุนูู ุงููุญู ุงูุชุงูู:

```python
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> model = AutoModelForCausalLM.from_pretrained("mistralai/Mixtral-8x7B-v0.1", device_map="auto")
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mixtral-8x7B-v0.1")

>>> prompt = "My favourite condiment is"

>>> model_inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
>>> model.to(device)

>>> generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)
>>> tokenizer.batch_decode(generated_ids)[0]
"My favourite condiment is to ..."
```

ูููู ุงุณุชุฎุฏุงู ูููุฐุฌ ุถุจุท ุงูุชุนูููุงุช ุนูู ุงููุญู ุงูุชุงูู:

```python
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> model = AutoModelForCausalLM.from_pretrained("mistralai/Mixtral-8x7B-Instruct-v0.1", device_map="auto")
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mixtral-8x7B-Instruct-v0.1")

>>> messages = [
...     {"role": "user", "content": "What is your favourite condiment?"},
...     {"role": "assistant", "content": "Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!"},
...     {"role": "user", "content": "Do you have mayonnaise recipes?"}
... ]

>>> model_inputs = tokenizer.apply_chat_template(messages, return_tensors="pt").to("cuda")

>>> generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)
>>> tokenizer.batch_decode(generated_ids)[0]
"Mayonnaise can be made as follows: (...)"
```

ููุง ูู ููุถุญุ ูุชุทูุจ ุงููููุฐุฌ ุงููุถุจูุท ููุชุนูููุงุช [ูุงูุจ ุฏุฑุฏุดุฉ](../chat_templating) ููุชุฃูุฏ ูู ุฅุนุฏุงุฏ ุงูุฅุฏุฎุงูุงุช ุจุชูุณูู ุตุญูุญ.

## ุชุณุฑูุน Mixtral ุจุงุณุชุฎุฏุงู Flash Attention
ุชูุถุญ ููุชุทูุงุช ุงูุชุนูููุงุช ุงูุจุฑูุฌูุฉ ุฃุนูุงู ุงูุงุณุชุฏูุงู ุจุฏูู ุฃู ุญูู ููุชุญุณูู. ููุน ุฐููุ ูููู ูููุฑุก ุฃู ูุณุฑุน ุจุดูู ูุจูุฑ ูู ุงููููุฐุฌ ูู ุฎูุงู ุงูุงุณุชูุงุฏุฉ ูู [Flash Attention](../perf_train_gpu_one.md#flash-attention-2)ุ ููู ุชูููุฐ ุฃุณุฑุน ูุขููุฉ ุงูุงูุชูุงู ุงููุณุชุฎุฏูุฉ ุฏุงุฎู ุงููููุฐุฌ.

ุฃููุงูุ ุชุฃูุฏ ูู ุชุซุจูุช ุฃุญุฏุซ ุฅุตุฏุงุฑ ูู Flash Attention 2 ูุชุถููู ููุฒุฉ ูุงูุฐุฉ ุงูุงูุฒูุงู.

```bash
pip install -U flash-attn --no-build-isolation
```

ุชุฃูุฏ ุฃูุถูุง ูู ุฃู ูุฏูู ุฃุฌูุฒุฉ ูุชูุงููุฉ ูุน Flash-Attention 2. ุงูุฑุฃ ุงููุฒูุฏ ุนููุง ูู ุงููุซุงุฆู ุงูุฑุณููุฉ ูู [ูุณุชูุฏุน ุงูุงูุชูุงู ุจุงููููุถ](https://github.com/Dao-AILab/flash-attention). ุชุฃูุฏ ุฃูุถูุง ูู ุชุญููู ูููุฐุฌู ูู ูุตู ุงูุฏูุฉ (ุนูู ุณุจูู ุงููุซุงู `torch.float16`).

ููุชุญููู ูุชุดุบูู ูููุฐุฌ ุจุงุณุชุฎุฏุงู Flash Attention-2ุ ุฑุงุฌุน ุงูููุชุทู ุฃุฏูุงู:

```python
>>> import torch
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> model = AutoModelForCausalLM.from_pretrained("mistralai/Mixtral-8x7B-v0.1", torch_dtype=torch.float16, attn_implementation="flash_attention_2", device_map="auto")
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mixtral-8x7B-v0.1")

>>> prompt = "My favourite condiment is"

>>> model_inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
>>> model.to(device)

>>> generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)
>>> tokenizer.batch_decode(generated_ids)[0]
"The expected output"
```

### ุชุณุฑูุน ูุชููุน
ูููุง ููู ุฑุณู ุจูุงูู ููุชุณุฑูุน ุงููุชููุน ุงูุฐู ููุงุฑู ููุช ุงูุงุณุชุฏูุงู ุงูููู ุจูู ุงูุชูููุฐ ุงูุฃุตูู ูู ุงููุญููุงุช ุจุงุณุชุฎุฏุงู ููุทุฉ ุชูุชูุด `mistralai/Mixtral-8x7B-v0.1` ูุฅุตุฏุงุฑ Flash Attention 2 ูู ุงููููุฐุฌ.

<div style="text-align: center">
<img src="https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/mixtral-7b-inference-large-seqlen.png">
</div>

### ูุงูุฐุฉ ุงูุฒูุงู ุงูุงูุชูุงู
ุชุฏุนู ุงูุชูููุฐ ุงูุญุงูู ุขููุฉ ุงูุชูุงู ูุงูุฐุฉ ุงูุงูุฒูุงู ูุฅุฏุงุฑุฉ ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ุงููุนุงูุฉ ูู ุญูุซ ุงูุฐุงูุฑุฉ.

ูุชูููู ุงูุชูุงู ูุงูุฐุฉ ุงูุงูุฒูุงูุ ุชุฃูุฏ ููุท ูู ูุฌูุฏ ุฅุตุฏุงุฑ `flash-attn` ูุชูุงูู ูุน ุงูุชูุงู ูุงูุฐุฉ ุงูุงูุฒูุงู (`>=2.3.0`).

ูุณุชุฎุฏู ูููุฐุฌ Flash Attention-2 ุฃูุถูุง ุขููุฉ ุชูุทูุน ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ุงูุฃูุซุฑ ููุงุกุฉ ูู ุญูุซ ุงูุฐุงูุฑุฉ - ููุง ูู ููุตู ุจู ููููุง ููุชูููุฐ ุงูุฑุณูู ููููุฐุฌ Mistral ุงูุฐู ูุณุชุฎุฏู ุขููุฉ ุงูุชุฎุฒูู ุงููุคูุช ุงููุชุฏุงููุ ูุญุงูุธ ุนูู ุญุฌู ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ุซุงุจุชูุง (`self.config.sliding_window`)ุ ููุฏุนู ุงูุชูููุฏ ุงููุฌูุน ููุท ูู `padding_side="left"` ููุณุชุฎุฏู ุงูููุถุน ุงููุทูู ููุฑูุฒ ุงูุญุงูู ูุญุณุงุจ ุงูุชุถููู ุงูููุถุนู.

## ุชูููู ุญุฌู Mixtral ุจุงุณุชุฎุฏุงู ุงูุชูููู
ูุธุฑูุง ูุฃู ูููุฐุฌ Mixtral ูุญุชูู ุนูู 45 ูููุงุฑ ูุนููุฉุ ูุณูุญุชุงุฌ ุฐูู ุฅูู ุญูุงูู 90 ุฌูุฌุงุจุงูุช ูู ุฐุงูุฑุฉ ุงููุตูู ุงูุนุดูุงุฆู ููุฑุณูููุงุช ูู ูุตู ุงูุฏูุฉ (float16)ุ ุญูุซ ูุชู ุชุฎุฒูู ูู ูุนููุฉ ูู ุจุงูุชูู. ููุน ุฐููุ ููููู ุชูููู ุญุฌู ุงููููุฐุฌ ุจุงุณุชุฎุฏุงู [ุงูุชูููู](../quantization.md). ุฅุฐุง ุชู ุชูููู ุงููููุฐุฌ ุฅูู 4 ุจุชุงุช (ุฃู ูุตู ุจุงูุช ููู ูุนููุฉ)ุ ูุฅู A100 ูุงุญุฏ ุจุฐุงูุฑุฉ ูุตูู ุนุดูุงุฆู ุณุนุฉ 40 ุฌูุฌุงุจุงูุช ูููู ูุชูุงุณุจ ุงููููุฐุฌ ุจุงููุงููุ ููู ูุฐู ุงูุญุงูุฉุ ุชููู ููุงู ุญุงุฌุฉ ุฅูู ุญูุงูู 27 ุฌูุฌุงุจุงูุช ููุท ูู ุฐุงูุฑุฉ ุงููุตูู ุงูุนุดูุงุฆู.

ุฅู ุชูููู ูููุฐุฌ ุจุณูุท ูุซู ุชูุฑูุฑ `quantization_config` ุฅูู ุงููููุฐุฌ. ุฃุฏูุงูุ ุณูุณุชููุฏ ูู ุชูููู BitsAndyBytes (ูููู ุฑุงุฌุน [ูุฐู ุงูุตูุญุฉ](../quantization.md) ูุฃุณุงููุจ ุงูุชูููู ุงูุฃุฎุฑู):

```python
>>> import torch
>>> from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

>>> # specify how to quantize the model
>>> quantization_config = BitsAndBytesConfig(
...         load_in_4bit=True,
...         bnb_4bit_quant_type="nf4",
...         bnb_4bit_compute_dtype="torch.float16",
... )

>>> model = AutoModelForCausalLM.from_pretrained("mistralai/Mixtral-8x7B-Instruct-v0.1", quantization_config=True, device_map="auto")
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mixtral-8x7B-Instruct-v0.1")

>>> prompt = "My favourite condiment is"

>>> messages = [
...     {"role": "user", "content": "What is your favourite condiment?"},
...     {"role": "assistant", "content": "Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!"},
...     {"role": "user", "content": "Do you have mayonnaise recipes?"}
... ]

>>> model_inputs = tokenizer.apply_chat_template(messages, return_tensors="pt").to("cuda")

>>> generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)
>>> tokenizer.batch_decode(generated_ids)[0]
"The expected output"
```

ุชูุช ุงููุณุงููุฉ ุจูุฐุง ุงููููุฐุฌ ูู ูุจู [Younes Belkada](https://huggingface.co/ybelkada) ู[Arthur Zucker](https://huggingface.co/ArthurZ).

ูููู ุงูุนุซูุฑ ุนูู ุงูููุฏ ุงูุฃุตูู [ููุง](https://github.com/mistralai/mistral-src).

## ุงูููุงุฑุฏ
ูุงุฆูุฉ ุจููุงุฑุฏ Hugging Face ุงูุฑุณููุฉ ูููุงุฑุฏ ุงููุฌุชูุน (ุงููุดุงุฑ ุฅูููุง ุจู ๐) ููุณุงุนุฏุชู ูู ุงูุจุฏุก ุจุงุณุชุฎุฏุงู Mixtral. ุฅุฐุง ููุช ููุชููุง ุจุชูุฏูู ููุฑุฏ ูุฅุฏุฑุงุฌู ููุงุ ููุฑุฌู ูุชุญ ุทูุจ ุณุญุจ ูุณูุฑุงุฌุนู! ูุฌุจ ุฃู ููุถุญ ุงูููุฑุฏ ุจุดูู ูุซุงูู ุดูุฆูุง ุฌุฏูุฏูุง ุจุฏูุงู ูู ุชูุฑุงุฑ ููุฑุฏ ููุฌูุฏ.

<PipelineTag pipeline="text-generation"/>

- ูููู ุงูุนุซูุฑ ุนูู ุฏูุชุฑ ููุงุญุธุงุช ุชูุถูุญู ูุฅุฌุฑุงุก ุงูุถุจุท ุงูุฏููู ุงูุฎุงุถุน ููุฅุดุฑุงู (SFT) ูู Mixtral-8x7B [ููุง](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Mistral/Supervised_fine_tuning_(SFT)_of_an_LLM_using_Hugging_Face_tooling.ipynb). ๐

- ููุดูุฑ [ูุฏููุฉ](https://medium.com/@prakharsaxena11111/finetuning-mixtral-7bx8-6071b0ebf114) ุญูู ุงูุถุจุท ุงูุฏููู ูู Mixtral-8x7B ุจุงุณุชุฎุฏุงู PEFT. ๐

- ูุชุถูู [ุฏููู ุงููุญุงุฐุงุฉ](https://github.com/huggingface/alignment-handbook) ูู Hugging Face ูุตูุตูุง ููุตูุงุช ูุฃุฏุงุก ุงูุถุจุท ุงูุฏููู ุงูุฎุงุถุน ููุฅุดุฑุงู (SFT) ูุงูุชุญุณูู ุงููุจุงุดุฑ ููุฃูุถููุงุช ุจุงุณุชุฎุฏุงู Mistral-7B. ููุดูู ุฐูู ูุตูุต ุงูุถุจุท ุงูุฏููู ุงููุงููุ ูQLoRa ุนูู ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณููุงุช (GPU) ูุงุญุฏุฉ ุจุงูุฅุถุงูุฉ ุฅูู ุงูุถุจุท ุงูุฏููู ูุชุนุฏุฏ ูุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณููุงุช (GPU).

- [ูููุฉ ููุฐุฌุฉ ุงููุบุฉ ุงูุณุจุจูุฉ](../tasks/language_modeling)

## MixtralConfig
[[autodoc]] MixtralConfig

## MixtralModel
[[autodoc]] MixtralModel

- forward

## MixtralForCausalLM
[[autodoc]] MixtralForCausalLM

- forward

## MixtralForSequenceClassification
[[autodoc]] MixtralForSequenceClassification

- forward

## MixtralForTokenClassification
[[autodoc]] MixtralForTokenClassification

- forward