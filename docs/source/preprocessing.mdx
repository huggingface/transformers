<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Preprocess

[[open-in-colab]]

Before you can use your data in a model, you need to process the data into a format the model can understand. A model does not understand raw text, images or audio. These inputs need to be converted into numbers and assembled into tensors. In this tutorial, you will:

* Preprocess textual data with a tokenizer.
* Preprocess image or audio data with a feature extractor.
* Preprocess data for a multimodal task with a processor.

## NLP

<Youtube id="Yffk5aydLzg"/>

The main tool for processing textual data is a [tokenizer](main_classes/tokenizer). A tokenizer begins by splitting text into _tokens_ according to a set of rules. The _tokens_ are converted into numbers, which are used to build tensors as input to a model. Any additional inputs required by a model are also added by the tokenizer.

<Tip>

If you plan on using a pretrained model, it's important to use the associated pretrained tokenizer. This ensures the text is split the same way as the pretraining corpus, and uses the same corresponding tokens-to-index (usually referrred to as the _vocab_) during pretraining.

</Tip>

You can quickly get started by loading a pretrained tokenizer with the ['AutoTokenizer'] class. This downloads the _vocab_ used during pretraining or fine-tuning a model.

### Tokenize

Load a pretrained tokenizer with the [`AutoTokenizer.from_pretrained`] method:

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

Then you can pass your sentence to the tokenizer:

```py
>>> encoded_input = tokenizer("Do not meddle in the affairs of wizards, for they are subtle and quick to anger.")
>>> print(encoded_input)
{'input_ids': [101, 2079, 2025, 19960, 10362, 1999, 1996, 3821, 1997, 16657, 1010, 2005, 2027, 2024, 11259, 1998, 4248, 2000, 4963, 1012, 102], 
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

The tokenizer returns a dictionary with three important itmes:

* [input_ids](glossary#input-ids) are the indices corresponding to each token in the sentence.
* [attention_mask](glossary#attention-mask) indicates whether a token should be attended to or not.
* [token_type_ids](glossary#token-type-ids) identifies which sequence a token belongs to when there is more than one sequence.

You can decode the `input_ids` to return the original input:

```py
>>> tokenizer.decode(encoded_input["input_ids"])
'[CLS] Do not meddle in the affairs of wizards, for they are subtle and quick to anger. [SEP]'
```

As you can see, the tokenizer added two special tokens - `CLS` and `SEP` (classifier and separator) - to the sentence. Not all models need
special tokens, but if they do, the tokenizer will automatically add them for you.

If you have several sentences you want to process, you can do this efficiently by sending the sentences as a list to the tokenizer:

```py
>>> batch_sentences = ["The Road goes ever on and on", "Down from the door where it began", "Now far ahead the Road has gone"]
>>> encoded_inputs = tokenizer(batch_sentences)
>>> print(encoded_inputs)
{'input_ids': [[101, 1996, 2346, 3632, 2412, 2006, 1998, 2006, 102], 
               [101, 2091, 2013, 1996, 2341, 2073, 2009, 2211, 102], 
               [101, 2085, 2521, 3805, 1996, 2346, 2038, 2908, 102]], 
 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0], 
                    [0, 0, 0, 0, 0, 0, 0, 0, 0], 
                    [0, 0, 0, 0, 0, 0, 0, 0, 0]], 
 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1], 
                    [1, 1, 1, 1, 1, 1, 1, 1, 1], 
                    [1, 1, 1, 1, 1, 1, 1, 1, 1]]}
```

### Pad

This brings us to an important topic. When you process a batch of sentences, they aren't always the same length. This is a problem because tensors, the input to the model, need to have a uniform shape. Padding is a strategy for ensuring tensors are rectangular by adding a special _padding token_ to sentences with fewer tokens.

Set the `padding` parameter to `True` to pad the shorter sequences in the batch to match the longest sequence:

```py
>>> batch_sentences = ["Pursuing it with eager feet", "Until it joins some larger way"]
>>> encoded_input = tokenizer(batch_sentences, padding=True)
>>> print(encoded_input)
{'input_ids': [[101, 11828, 2009, 2007, 9461, 2519, 102, 0], 
               [101, 2127, 2009, 9794, 2070, 3469, 2126, 102]], 
 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], 
                    [0, 0, 0, 0, 0, 0, 0, 0]], 
 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0], 
                    [1, 1, 1, 1, 1, 1, 1, 1]]}
```

Notice the tokenizer padded the first sentence with a `0` because it is shorter!

### Truncation

On the other end of the spectrum, sometimes a sequence may be too long for a model to handle. In this case, you will need to truncate the sequence to a shorter length.

Set the `truncation` parameter to `True` to truncate a sequence to the maximum length accepted by the model:

```py
>>> batch_sentences = ["Pursuing it with eager feet", "Until it joins some larger way"]
>>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True)
>>> print(encoded_input)
{'input_ids': [[101, 11828, 2009, 2007, 9461, 2519, 102, 0], 
               [101, 2127, 2009, 9794, 2070, 3469, 2126, 102]], 
 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], 
                    [0, 0, 0, 0, 0, 0, 0, 0]], 
 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0], 
                    [1, 1, 1, 1, 1, 1, 1, 1]]}
```

### Build tensors

Finally, you want the tokenizer to return the actual tensors that are fed to the model.

Set the `return_tensors` parameter to either `pt` for PyTorch, or `tf` for TensorFlow:

```py
>>> batch = ["Pursuing it with eager feet", "Until it joins some larger way"]
>>> encoded_input = tokenizer(batch, padding=True, truncation=True, return_tensors='pt')
>>> print(encoded_input)
{'input_ids': tensor([[  101, 11828,  2009,  2007,  9461,  2519,   102,     0],
                      [  101,  2127,  2009,  9794,  2070,  3469,  2126,   102]]), 
 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0],
                           [0, 0, 0, 0, 0, 0, 0, 0]]), 
 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0],
                           [1, 1, 1, 1, 1, 1, 1, 1]])}
===PT-TF-SPLIT===
>>> batch = ["Pursuing it with eager feet", "Until it joins some larger way"]
>>> encoded_input = tokenizer(batch, padding=True, truncation=True, return_tensors="tf")
>>> print(encoded_input)
{'input_ids': <tf.Tensor: shape=(2, 8), dtype=int32, numpy=
array([[  101, 11828,  2009,  2007,  9461,  2519,   102,     0],
       [  101,  2127,  2009,  9794,  2070,  3469,  2126,   102]],
      dtype=int32)>, 
 'token_type_ids': <tf.Tensor: shape=(2, 8), dtype=int32, numpy=
array([[0, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 
 'attention_mask': <tf.Tensor: shape=(2, 8), dtype=int32, numpy=
array([[1, 1, 1, 1, 1, 1, 1, 0],
       [1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}
```

### Preprocess pairs of sentences

<Youtube id="0u3ioSwev3s"/>

Sometimes the task requires your model accept a pair of sentences as input, such as question-answering tasks. In this case, you want to supply your sentences as two arguments instead of a list. As you just saw in the previous section, providing your sentences in a list will be interpreted as a batch of two single sentences.

```py
>>> encoded_input = tokenizer("Where am I, and what is the time?' he said aloud to the ceiling.", "In the house of Elrond, and it is ten o'clock in the morning,' said a voice.")
>>> print(encoded_input)
{'input_ids': [101, 2073, 2572, 1045, 1010, 1998, 2054, 2003, 1996, 2051, 1029, 1005, 2002, 2056, 12575, 2000, 1996, 5894, 1012, 102, 1999, 1996, 2160, 1997, 3449, 4948, 2094, 1010, 1998, 2009, 2003, 2702, 1051, 1005, 5119, 1999, 1996, 2851, 1010, 1005, 2056, 1037, 2376, 1012, 102], 
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

You can see the [token_type_ids](glossary#token-type-ids) indicate to the model which tokens correspond to the first sentence, and which tokens correspond to the second sentence.

Decoding the `input_ids` will show that special tokens have been added:

```py
>>> tokenizer.decode(encoded_input["input_ids"])
'[CLS] Where am I, and what is the time?'he said aloud to the ceiling. [SEP] In the house of Elrond, and it is ten o'clock in the morning,'said a voice. [SEP]'
```

When you have a list of pairs of sequences you want to process, create two lists for your tokenizer:

```py
>>> batch_sentences = ["Hello I'm a single sentence", "And another sentence", "And the very very last one"]
>>> batch_of_second_sentences = [
...     "I'm a sentence that goes with the first sentence",
...     "And I should be encoded with the second sentence",
...     "And I go with the very last one",
... ]
>>> encoded_inputs = tokenizer(batch_sentences, batch_of_second_sentences)
>>> print(encoded_inputs)
{'input_ids': [[101, 8667, 146, 112, 182, 170, 1423, 5650, 102, 146, 112, 182, 170, 5650, 1115, 2947, 1114, 1103, 1148, 5650, 102], 
               [101, 1262, 1330, 5650, 102, 1262, 146, 1431, 1129, 12544, 1114, 1103, 1248, 5650, 102], 
               [101, 1262, 1103, 1304, 1304, 1314, 1141, 102, 1262, 146, 1301, 1114, 1103, 1304, 1314, 1141, 102]], 
'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 
                   [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 
                   [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 
'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 
                   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 
                   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}
```

Decoding the `input_ids` will show that each sentence is correctly separated and matched with their sentence pair:

```py
>>> for ids in encoded_inputs["input_ids"]:
...     print(tokenizer.decode(ids))
[CLS] Hello I'm a single sentence [SEP] I'm a sentence that goes with the first sentence [SEP]
[CLS] And another sentence [SEP] And I should be encoded with the second sentence [SEP]
[CLS] And the very very last one [SEP] And I go with the very last one [SEP]
```

Lastly, you can apply a padding and truncation strategy, and return the tensors directly with the following:

```py
>>> batch = tokenizer(batch_sentences, batch_of_second_sentences, padding=True, truncation=True, return_tensors="pt")
===PT-TF-SPLIT===
>>> batch = tokenizer(batch_sentences, batch_of_second_sentences, padding=True, truncation=True, return_tensors="tf")
```

### Pre-tokenized inputs

The tokenizer also accepts pre-tokenized inputs. This is particularly useful for tasks like [named entity recognition (NER)](task_summary#named-entity-recognition) or [part-of-speech tagging (POS tagging)](https://en.wikipedia.org/wiki/Part-of-speech_tagging).

<Tip warning={true}>

Pre-tokenized does not mean the inputs are already tokenized. It only means the inputs are already split into words. You still need to tokenize the inputs!

</Tip>

When dealing with pre-tokenized inputs, set `is_split_into_words=True`:

```py
>>> encoded_input = tokenizer(["In", "a", "hole", "in", "the", "ground", "there", "lived", "a", "hobbit"], is_split_into_words=True)
>>> print(encoded_input)
{'input_ids': [101, 1999, 1037, 4920, 1999, 1996, 2598, 2045, 2973, 1037, 7570, 10322, 4183, 102], 
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

Note that the tokenizer still adds the special token ids (if applicable) unless you pass `add_special_tokens=False`.

Encode a batch of sentences like this:

```py
>>> batch_sentences = [
    ["Hello", "I'm", "a", "single", "sentence"],
    ["And", "another", "sentence"],
    ["And", "the", "very", "very", "last", "one"],
]
>>> encoded_inputs = tokenizer(batch_sentences, is_split_into_words=True)
```

Or a batch of sentence pairs like this:

```py
>>> batch_of_second_sentences = [
    ["I'm", "a", "sentence", "that", "goes", "with", "the", "first", "sentence"],
    ["And", "I", "should", "be", "encoded", "with", "the", "second", "sentence"],
    ["And", "I", "go", "with", "the", "very", "last", "one"],
]
>>> encoded_inputs = tokenizer(batch_sentences, batch_of_second_sentences, is_split_into_words=True)
```

Pad, truncate, and return the tensors like before:

```py
>>> batch = tokenizer(
...    batch_sentences,
...     batch_of_second_sentences,
...     is_split_into_words=True,
...     padding=True,
...     truncation=True,
...     return_tensors="pt",
)
===PT-TF-SPLIT===
>>> batch = tokenizer(
...     batch_sentences,
...     batch_of_second_sentences,
...     is_split_into_words=True,
...     padding=True,
...     truncation=True,
...     return_tensors="tf",
)
```

## Audio

Audio inputs are preprocessed differently than textual inputs, but the end goal remains the same: create numerical sequences the model can understand. A [feature extractor](main_classes/feature_extractor) will help you with this. Before you begin, install ðŸ¤— Datasets to load an audio dataset to experiment with.

```bash
pip install datasets
```

Load the keyword spotting task from the [SUPERB](https://huggingface.co/datasets/superb) benchmark (see the ðŸ¤— [Datasets tutorial](https://huggingface.co/docs/datasets/load_hub.html) for more information):

```py
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("superb", "ks")
```

Access the first element of the `audio` column to take a look at the input. Calling the `audio` column will automatically load and resample the audio file:

```py
>>> dataset["train"][0]["audio"]
{'array': array([ 0.        ,  0.        ,  0.        , ..., -0.00592041,
        -0.00405884, -0.00253296], dtype=float32),
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/05734a36d88019a09725c20cc024e1c4e7982e37d7d55c0c1ca1742ea1cdd47f/_background_noise_/doing_the_dishes.wav',
 'sampling_rate': 16000}
```

This returns three items:

* `array` is the speech signal, or the input format the model expects.
* `path` points to the location of the audio file.
* `sampling_rate` refers to how many data points in the speech signal are measured per second.

### Resample

For this tutorial, you will use the [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base) model. As you can see from the model card, the Wav2Vec2 model is pretrained on 16kHz sampled speech audio. It is important your audio data sampling rate matches the sampling rate of the dataset used to pretrain the model. If your data's sampling rate isn't the same, then you will need to resample your audio data. For example, if your audio data has a sampling rate of 48kHz, then you will need to downsample to 16kHz.

1. Use ðŸ¤— Datasets `cast_column` method to downsample the sampling rate to 16kHz:

```py
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=16_000))
```

2. Now when you load the audio file, it will be automatically resampled to 16kHz:

```py
>>> dataset["train"][0]["audio"]
```

### Feature extractor

The next step is to load a feature extractor to normalize the input.

Load the feature extractor with the [`AutoFeatureExtractor.from_pretrained`] method:

```py
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base")
```

Pass the audio `array` to the feature extractor. We also recommend you add the `sampling_rate` argument in the feature extractor in order to better debug any silent errors that may occur.

```py
>>> audio_input = [dataset["train"][0]["audio"]["array"]]
>>> feature_extractor(audio_input, sampling_rate=16000)
{'input_values': [array([ 0.00045439,  0.00045439,  0.00045439, ..., -0.1578519 , -0.10807519, -0.06727459], dtype=float32)]}
```

Just like the tokenizer, you can also apply a padding or truncation strategy to handle short and long sequences in a batch:

```py
>>> feature_extractor(audio_input, sampling_rate=16000, padding=True, max_length=160000, truncation=True)
```

## Vision

A feature extractor can also be used to process images for vision tasks. Once again, the goal is to convert the raw image into a batch of tensors for the model.

Let's load the [food101](https://huggingface.co/datasets/food101) dataset for this tutorial. Use ðŸ¤— Datasets `split` parameter to load only a small sample of the training split since the dataset is quite large:

```py
>>> from datasets import load_dataset

>>> dataset = load_dataset("food101", split='train[:100]')
```

Next, take a look at the image with ðŸ¤— Datasets [`Image`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=image#datasets.Image) feature:

```py
>>> dataset[0]["image"]
```

### Feature extractor

Load the feature extractor with the [`AutoFeatureExtractor.from_pretrained`] method:

```py
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("google/vit-base-patch16-224")
```

### Data augmentation

For vision tasks, it is common to add some type of data augmentation to the images as a part of preprocessing. You can add augmentations with any library you'd like, but in this tutorial, you will use torchvision's [`transforms`](https://pytorch.org/vision/stable/transforms.html) module.

1. Normalize the image and use `Compose` to chain some transforms together:

```py
>>> from torchvision.transforms import CenterCrop, Compose, Normalize, RandomHorizontalFlip, RandomResizedCrop, Resize, ToTensor

>>> normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)
>>> _transforms = Compose(
        [
            RandomResizedCrop(feature_extractor.size),
            RandomHorizontalFlip(),
            ToTensor(),
            normalize,
        ]
    )
```

2. The model accepts [`pixel_values`](model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderModel.forward.pixel_values) as it's input which is generated by the feature extractor. Create a function that generates the `pixel_values` from the transforms you want to apply:

```py
>>> def transforms(examples):
...    examples['pixel_values'] = [_transforms(image.convert("RGB")) for image in examples['image']]
...    return examples
```

3. Then use ðŸ¤— Datasets [`set_transform`](https://huggingface.co/docs/datasets/process.html#format-transform) to apply the transforms on-the-fly:

```py
>>> dataset.set_transform(transforms)
```

4. Now when you access the image, you will notice the feature extractor has added the model input `pixel_values`:

```py
>>> dataset[0]["image"]
{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=384x512 at 0x7F1A7B0630D0>,
 'label': 6,
 'pixel_values': tensor([[[ 0.0353,  0.0745,  0.1216,  ..., -0.9922, -0.9922, -0.9922],
          [-0.0196,  0.0667,  0.1294,  ..., -0.9765, -0.9843, -0.9922],
          [ 0.0196,  0.0824,  0.1137,  ..., -0.9765, -0.9686, -0.8667],
          ...,
          [ 0.0275,  0.0745,  0.0510,  ..., -0.1137, -0.1216, -0.0824],
          [ 0.0667,  0.0824,  0.0667,  ..., -0.0588, -0.0745, -0.0980],
          [ 0.0353,  0.0353,  0.0431,  ..., -0.0039, -0.0039, -0.0588]],
 
         [[ 0.2078,  0.2471,  0.2863,  ..., -0.9451, -0.9373, -0.9451],
          [ 0.1608,  0.2471,  0.3098,  ..., -0.9373, -0.9451, -0.9373],
          [ 0.2078,  0.2706,  0.3020,  ..., -0.9608, -0.9373, -0.8275],
          ...,
          [-0.0353,  0.0118, -0.0039,  ..., -0.2392, -0.2471, -0.2078],
          [ 0.0196,  0.0353,  0.0196,  ..., -0.1843, -0.2000, -0.2235],
          [-0.0118, -0.0039, -0.0039,  ..., -0.0980, -0.0980, -0.1529]],
 
         [[ 0.3961,  0.4431,  0.4980,  ..., -0.9216, -0.9137, -0.9216],
          [ 0.3569,  0.4510,  0.5216,  ..., -0.9059, -0.9137, -0.9137],
          [ 0.4118,  0.4745,  0.5216,  ..., -0.9137, -0.8902, -0.7804],
          ...,
          [-0.2314, -0.1922, -0.2078,  ..., -0.4196, -0.4275, -0.3882],
          [-0.1843, -0.1686, -0.2000,  ..., -0.3647, -0.3804, -0.4039],
          [-0.1922, -0.1922, -0.1922,  ..., -0.2941, -0.2863, -0.3412]]])}
```

## Multimodal

Finally, for multimodal tasks you will use a combination of everything you've learned so far. In this multimodal preprocessing tutorial, you will focus on a automatic speech recognition (ASR) task. This means you will need a:

* Feature extractor to preprocess the audio data.
* Tokenizer to process the text.

Let's use the [`TIMIT`](https://huggingface.co/datasets/timit_asr) dataset:

```py
>>> from datasets import load_dataset

>>> dataset = load_dataset("timit_asr", split="test")
```

For this tutorial, you are mainly interested in the `audio` and `text` column. Remove all the other columns:

```py
>>> dataset = dataset.map(remove_columns=["file", "phonetic_detail", "word_detail", "dialect_region", "sentence_type", "speaker_id", "id"])
```

Now let's take a look at the `audio` and `text` columns:

```py
>>> dataset[0]["audio"]
{'array': array([ 2.4414062e-04, -3.0517578e-05,  3.0517578e-05, ...,
         6.1035156e-05,  9.1552734e-05,  6.1035156e-05], dtype=float32),
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/404950a46da14eac65eb4e2a8317b1372fb3971d980d91d5d5b221275b1fd7e0/data/TEST/DR4/MGMM0/SX139.WAV',
 'sampling_rate': 16000}

>>> dataset[0]["text"]
'The bungalow was pleasantly situated near the shore.'
```

### Processor

A processor combines the feature extractor and tokenizer. Load a processor with the [`AutoProcessor.from_pretrained] method:

```py
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")
```

1. Create a function to process the audio data to `input_values`, and to tokenize the text to `labels`. These are your inputs to the model:

```py
>>> def prepare_dataset(example):
...    audio = example["audio"]
...
...    example["input_values"] = processor(audio["array"], sampling_rate=16000)
...    
...    with processor.as_target_processor():
...            example["labels"] = processor(example["text"]).input_ids
...    return example
```

2. Apply `prepare_dataset` to your example, and you will see the processor has added `input_values` and `labels`:

```py
>>> prepare_dataset(dataset[0])
```

Awesome, you should now be able to preprocess data for any modality! In the next tutorial, you will learn how to train a model on the preprocessed data.

## Everything you always wanted to know about padding and truncation

We have seen the commands that will work for most cases (pad your batch to the length of the maximum sentence and
truncate to the maximum length the model can accept). However, the API supports more strategies if you need them. The
three arguments you need to know for this are `padding`, `truncation` and `max_length`.

- `padding` controls the padding. It can be a boolean or a string which should be:

  - `True` or `'longest'` to pad to the longest sequence in the batch (doing no padding if you only provide
    a single sequence).
  - `'max_length'` to pad to a length specified by the `max_length` argument or the maximum length accepted
    by the model if no `max_length` is provided (`max_length=None`). If you only provide a single sequence,
    padding will still be applied to it.
  - `False` or `'do_not_pad'` to not pad the sequences. As we have seen before, this is the default
    behavior.

- `truncation` controls the truncation. It can be a boolean or a string which should be:

  - `True` or `'longest_first'` truncate to a maximum length specified by the `max_length` argument or
    the maximum length accepted by the model if no `max_length` is provided (`max_length=None`). This will
    truncate token by token, removing a token from the longest sequence in the pair until the proper length is
    reached.
  - `'only_second'` truncate to a maximum length specified by the `max_length` argument or the maximum
    length accepted by the model if no `max_length` is provided (`max_length=None`). This will only truncate
    the second sentence of a pair if a pair of sequence (or a batch of pairs of sequences) is provided.
  - `'only_first'` truncate to a maximum length specified by the `max_length` argument or the maximum
    length accepted by the model if no `max_length` is provided (`max_length=None`). This will only truncate
    the first sentence of a pair if a pair of sequence (or a batch of pairs of sequences) is provided.
  - `False` or `'do_not_truncate'` to not truncate the sequences. As we have seen before, this is the
    default behavior.

- `max_length` to control the length of the padding/truncation. It can be an integer or `None`, in which case
  it will default to the maximum length the model can accept. If the model has no specific maximum input length,
  truncation/padding to `max_length` is deactivated.

Here is a table summarizing the recommend way to setup padding and truncation. If you use pair of inputs sequence in
any of the following examples, you can replace `truncation=True` by a `STRATEGY` selected in
`['only_first', 'only_second', 'longest_first']`, i.e. `truncation='only_second'` or `truncation= 'longest_first'` to control how both sequence in the pair are truncated as detailed before.

| Truncation                           | Padding                           | Instruction                                                                                 |
|--------------------------------------|-----------------------------------|---------------------------------------------------------------------------------------------|
| no truncation                        | no padding                        | `tokenizer(batch_sentences)`                                                           |
|                                      | padding to max sequence in batch  | `tokenizer(batch_sentences, padding=True)` or                                          |
|                                      |                                   | `tokenizer(batch_sentences, padding='longest')`                                        |
|                                      | padding to max model input length | `tokenizer(batch_sentences, padding='max_length')`                                     |
|                                      | padding to specific length        | `tokenizer(batch_sentences, padding='max_length', max_length=42)`                      |
| truncation to max model input length | no padding                        | `tokenizer(batch_sentences, truncation=True)` or                                       |
|                                      |                                   | `tokenizer(batch_sentences, truncation=STRATEGY)`                                      |
|                                      | padding to max sequence in batch  | `tokenizer(batch_sentences, padding=True, truncation=True)` or                         |
|                                      |                                   | `tokenizer(batch_sentences, padding=True, truncation=STRATEGY)`                        |
|                                      | padding to max model input length | `tokenizer(batch_sentences, padding='max_length', truncation=True)` or                 |
|                                      |                                   | `tokenizer(batch_sentences, padding='max_length', truncation=STRATEGY)`                |
|                                      | padding to specific length        | Not possible                                                                                |
| truncation to specific length        | no padding                        | `tokenizer(batch_sentences, truncation=True, max_length=42)` or                        |
|                                      |                                   | `tokenizer(batch_sentences, truncation=STRATEGY, max_length=42)`                       |
|                                      | padding to max sequence in batch  | `tokenizer(batch_sentences, padding=True, truncation=True, max_length=42)` or          |
|                                      |                                   | `tokenizer(batch_sentences, padding=True, truncation=STRATEGY, max_length=42)`         |
|                                      | padding to max model input length | Not possible                                                                                |
|                                      | padding to specific length        | `tokenizer(batch_sentences, padding='max_length', truncation=True, max_length=42)` or  |
|                                      |                                   | `tokenizer(batch_sentences, padding='max_length', truncation=STRATEGY, max_length=42)` |
