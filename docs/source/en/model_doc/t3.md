# T3

## Overview

T3 (Token-To-Token) is a text-to-speech model that converts text tokens to speech tokens using a transformer-based language model architecture. It was introduced as part of the [chatterbox](https://github.com/resemble-ai/chatterbox) TTS pipeline and serves as the first stage in converting text to speech.

The model uses a LLaMA transformer backbone (520M parameters) to perform sequence-to-sequence translation from text tokens to speech tokens. These speech tokens can then be decoded by models like S3Gen to produce mel spectrograms and waveforms.

## Model Architecture

T3 consists of several key components:

1. **Text Token Embeddings**: Learnable embeddings for text tokens
2. **Speech Token Embeddings**: Learnable embeddings for speech tokens
3. **LLaMA Transformer Backbone**: Core transformer model for sequence modeling
4. **Conditioning System**: Optional style and speaker conditioning
   - Voice Encoder: Extracts speaker embeddings from reference audio
   - Perceiver Resampler: Projects conditioning features to fixed-length prompts
5. **Output Projection**: Projects hidden states to speech token vocabulary

### Key Features

- **Language Model Approach**: Uses auto-regressive generation like GPT
- **Speaker Conditioning**: Optional voice encoder for voice cloning
- **Style Conditioning**: Optional style vectors for controlling prosody
- **Multilingual Support**: Can be configured for English-only or multilingual use
- **Flexible Architecture**: Based on LLaMA with customizable layers and attention heads

## Usage

### Basic Text-to-Speech Token Generation

```python
from transformers import T3Model, T3Config
from transformers.models.t3.modeling_t3 import T3Cond
import torch

# Load model
model = T3Model.from_pretrained("ResembleAI/t3")
model = model.to("cuda")  # or "cpu"
model.eval()

# Prepare text tokens (from your tokenizer)
text_tokens = torch.tensor([255, 45, 12, 89, 34, 0])  # Example: [start, tokens..., stop]

# Create minimal conditioning (speaker embedding required)
speaker_emb = torch.randn(1, 256)  # Dummy speaker embedding
t3_cond = T3Cond(speaker_emb=speaker_emb)

# Generate speech tokens
speech_tokens = model.inference(
    t3_cond=t3_cond,
    text_tokens=text_tokens,
    max_new_tokens=500,
    temperature=0.8,
    top_p=0.95,
    cfg_weight=0.0,  # No classifier-free guidance
)

print(f"Generated {speech_tokens.shape[1]} speech tokens")
```

### Voice-Conditioned Generation

```python
import numpy as np
import torchaudio
from transformers.models.t3.modeling_t3 import T3Cond

# Load reference audio for voice conditioning
ref_wav, ref_sr = torchaudio.load("reference.wav")
ref_audio = ref_wav.squeeze().numpy()

# Extract speaker embedding using voice encoder
speaker_embed = model.voice_encoder.embeds_from_wavs(
    wavs=[ref_audio],
    sample_rate=ref_sr
)
speaker_emb = torch.from_numpy(speaker_embed).to(model.device)

# Create conditioning with speaker embedding
t3_cond = T3Cond(speaker_emb=speaker_emb)

# Generate speech tokens with voice conditioning
speech_tokens = model.inference(
    t3_cond=t3_cond,
    text_tokens=text_tokens,
    max_new_tokens=500,
    temperature=0.8,
    cfg_weight=0.5,
)
```

### Advanced: Full Conditioning with Prompts

```python
from transformers.models.t3.modeling_t3 import T3Cond

# Extract speaker embedding
speaker_emb = torch.from_numpy(
    model.voice_encoder.embeds_from_wavs([ref_audio], sample_rate=16000)
).to(model.device)

# Optional: Add speech prompt tokens for style conditioning
cond_prompt_speech_tokens = torch.randint(0, 6561, (1, 150)).to(model.device)

# Optional: Add emotion/exaggeration control
emotion_adv = torch.tensor([[[0.5]]]).to(model.device)  # 0.0 to 1.0

# Create full conditioning
t3_cond = T3Cond(
    speaker_emb=speaker_emb,
    cond_prompt_speech_tokens=cond_prompt_speech_tokens,
    emotion_adv=emotion_adv,
)

# Generate with full conditioning
speech_tokens = model.inference(
    t3_cond=t3_cond,
    text_tokens=text_tokens,
    max_new_tokens=1000,
    temperature=0.8,
    top_p=0.95,
    min_p=0.05,
    repetition_penalty=1.2,
    cfg_weight=0.5,
)
```


## Model Details

### Configuration

The model can be configured via [`T3Config`]:

```python
from transformers import T3Config

# English-only configuration
config = T3Config.english_only()

# Multilingual configuration
config = T3Config.multilingual()

# Custom configuration
config = T3Config(
    text_tokens_dict_size=704,  # English-only vocab
    speech_tokens_dict_size=8194,
    hidden_size=1024,
    num_hidden_layers=30,
    num_attention_heads=16,
    use_perceiver_resampler=True,
    speaker_embed_size=256,
)
```

### Input Requirements

For `inference()` method:
- **t3_cond** (T3Cond): Conditioning object containing:
  - `speaker_emb`: Float tensor of shape `(batch, 256)` - required
  - `cond_prompt_speech_tokens`: Integer tensor of shape `(batch, prompt_len)` - optional
  - `emotion_adv`: Float tensor of shape `(batch, 1, 1)` - optional (0.0 to 1.0)
  - `clap_emb`: Not implemented
- **text_tokens**: Integer tensor of shape `(text_length,)` or `(batch, text_length)` with values in range `[0, text_vocab_size-1]`
  - Should include start token (255) at beginning and stop token (0) at end
- **initial_speech_tokens**: Integer tensor for prompt (optional, defaults to start token)

Generation parameters:
- **max_new_tokens** (int): Maximum speech tokens to generate
- **temperature** (float, default 0.8): Sampling temperature
- **top_p** (float, default 0.95): Top-p (nucleus) sampling
- **min_p** (float, default 0.05): Minimum probability threshold
- **repetition_penalty** (float, default 1.2): Penalty for repeating tokens
- **cfg_weight** (float, default 0.5): Classifier-free guidance weight (0.0 to disable)

### Output

From `inference()`:
- **Speech Tokens**: Integer tensor of shape `(batch, generated_length)` containing generated speech tokens


### Special Tokens

- Text tokens:
  - Start: 255
  - Stop: 0
- Speech tokens:
  - Valid range: [0, 6560]
  - Start: 6561
  - Stop: 6562

### Voice Encoder

The built-in voice encoder extracts speaker embeddings from audio:

```python
import numpy as np
import torchaudio

# Load audio (will be resampled to 16kHz internally)
ref_wav, ref_sr = torchaudio.load("reference.wav")
ref_audio = ref_wav.squeeze().numpy()

# Extract speaker embedding (supports batch of waveforms)
speaker_embeds = model.voice_encoder.embeds_from_wavs(
    wavs=[ref_audio],  # List of numpy arrays
    sample_rate=ref_sr,
    overlap=0.5,  # Overlap for mel partials
    rate=1.3,  # Sampling rate for partials
    batch_size=32,  # Batch size for processing
)

print(f"Speaker embedding shape: {speaker_embeds.shape}")  # (1, 256)
```

The voice encoder:
- Automatically resamples audio to 16kHz
- Trims silence using voice activity detection
- Extracts mel spectrograms (40 mel bins)
- Processes mel in overlapping windows
- Returns L2-normalized speaker embeddings

## Architecture Details

### LLaMA Backbone

T3 uses a modified LLaMA architecture:
- 30 transformer layers
- 16 attention heads
- 1024 hidden dimensions
- 4096 intermediate dimensions
- RoPE (Rotary Position Embeddings) with extended context
- SwiGLU activation functions

### Perceiver Resampler

When enabled, the perceiver resampler:
- Takes variable-length conditioning features
- Projects to fixed number of latent tokens (default: 32)
- Uses cross-attention to compress information
- Outputs conditioning prompts prepended to text tokens

## Limitations

- Inference-only model (no training support in this implementation)
- Requires corresponding text tokenizer for text input (not included in model)
- Speech tokens must be decoded by separate model (e.g., S3Gen) to produce audio
- English-only model trained primarily on English data; use multilingual config for other languages
- Reference audio quality affects conditioning quality
- Classifier-free guidance (CFG) requires duplicating text tokens (handled internally)
- Alignment stream analyzer only works with multilingual configuration

## Citation

```bibtex
@misc{chatterbox2025,
  title={Chatterbox: High-Quality Text-to-Speech Synthesis},
  author={Resemble AI},
  year={2025},
  publisher={GitHub},
  url={https://github.com/resemble-ai/chatterbox}
}
```

## T3Cond

The `T3Cond` dataclass is used to pass conditioning information to the T3 model:

```python
from transformers.models.t3.modeling_t3 import T3Cond
import torch

# Minimal conditioning (speaker only)
t3_cond = T3Cond(
    speaker_emb=torch.randn(1, 256),  # Required: speaker embedding
)

# Full conditioning
t3_cond = T3Cond(
    speaker_emb=torch.randn(1, 256),  # Required: speaker embedding from voice encoder
    cond_prompt_speech_tokens=torch.randint(0, 6561, (1, 150)),  # Optional: speech prompt tokens
    emotion_adv=torch.tensor([[[0.5]]]),  # Optional: emotion/exaggeration (0.0-1.0)
    clap_emb=None,  # Not implemented
)

# Move to device
t3_cond = t3_cond.to(device="cuda")
```

**Fields:**
- `speaker_emb` (Tensor, required): Speaker embedding from voice encoder, shape `(batch, 256)`
- `cond_prompt_speech_tokens` (Tensor, optional): Speech tokens for style conditioning, shape `(batch, prompt_len)`
- `cond_prompt_speech_emb` (Tensor, optional): Pre-computed embeddings for prompts (computed internally if not provided)
- `emotion_adv` (Tensor, optional): Emotion/exaggeration control, shape `(batch, 1, 1)`, range [0.0, 1.0]
- `clap_emb` (Tensor, optional): CLAP embeddings (not implemented)

## T3Config

[[autodoc]] T3Config
    - english_only
    - multilingual

## T3Model

[[autodoc]] T3Model
    - inference

