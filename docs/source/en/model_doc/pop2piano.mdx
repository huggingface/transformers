<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Pop2Piano

## Overview

The Pop2Piano model was proposed in [Pop2Piano : Pop Audio-based Piano Cover Generation](https://arxiv.org/abs/2211.00895) by Jongho Choi, Kyogu Lee.

The abstract from the paper is the following:

*The piano cover of pop music is widely enjoyed by people. How-ever,
the generation task of the pop piano cover is still understudied.
This is partly due to the lack of synchronized {Pop, Piano
Cover} data pairs, which made it challenging to apply the latest
data-intensive deep learning-based methods. To leverage the power
of the data-driven approach, we make a large amount of paired and
synchronized {Pop, Piano Cover} data using an automated
pipeline. In this paper, we present Pop2Piano, a Transformer net-
work that generates piano covers given waveforms of pop music. To
the best of our knowledge, this is the first model to directly generate
a piano cover from pop audio without melody and chord extraction
modules. We show that Pop2Piano trained with our dataset can
generate plausible piano covers.*


Tips:

1. Pop2Piano is an Encoder-Decoder based model like T5.
2. Pop2Piano can be used to generate midi-audio files for a given audio sequence. This HuggingFace implementation allows to save midi_output as well as stereo-mix output of the audio sequence.
3. Choosing different composers in `Pop2PianoForConditionalGeneration.generate()` can lead to variety of different results.
4. Please note that  HuggingFace implementation of Pop2Piano(both Pop2PianoForConditionalGeneration and Pop2PianoFeatureExtractor) can only work with one raw_audio sequence at a time. So if you want to process multiple files, please feed them one by one.  

This model was contributed by [Susnato Dhar](https://huggingface.co/susnato).
The original code can be found [here](https://github.com/sweetcocoa/pop2piano).

Example:
```
import librosa
from transformers import Pop2PianoFeatureExtractor, Pop2PianoForConditionalGeneration, Pop2PianoTokenizer

raw_audio, sr = librosa.load("audio.mp3", sr=44100)
model = Pop2PianoForConditionalGeneration.from_pretrained("susnato/pop2piano_dev")
feature_extractor = Pop2PianoFeatureExtractor.from_pretrained("susnato/pop2piano_dev")
tokenizer = Pop2PianoTokenizer.from_pretrained("susnato/pop2piano_dev")

model.eval()

feature_extractor_outputs = fe(raw_audio=raw_audio, audio_sr=sr, return_tensors="pt")
model_outputs = model.generate(feature_extractor_outputs, composer="composer1")

opt_postprocess = tokenizer(relative_tokens=model_outputs,
                           beatsteps=feature_extractor_outputs["beatsteps"],
                           ext_beatstep=feature_extractor_outputs["ext_beatstep"],
                           raw_audio=raw_audio,
                           sampling_rate=sr,
                           save_path="./Music/Outputs/",
                           audio_file_name="filename",
                           save_midi=True
                )
```


## Pop2PianoConfig

[[autodoc]] Pop2PianoConfig

## Pop2PianoFeatureExtractor

[[autodoc]] Pop2PianoFeatureExtractor
    - __call__

## Pop2PianoForConditionalGeneration

[[autodoc]] Pop2PianoForConditionalGeneration
    - forward
    - generate

## Pop2PianoTokenizer

[[autodoc]] Pop2PianoTokenizer
    - __call__

