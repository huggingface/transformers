<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Ce que ü§ó Transformers peut faire

ü§ó Transformers est une biblioth√®que de mod√®les pr√©entra√Æn√©s √† la pointe de la technologie pour les t√¢ches de traitement du langage naturel (NLP), de vision par ordinateur et de traitement audio et de la parole. Non seulement la biblioth√®que contient des mod√®les Transformer, mais elle inclut √©galement des mod√®les non-Transformer comme des r√©seaux convolutionnels modernes pour les t√¢ches de vision par ordinateur. Si vous regardez certains des produits grand public les plus populaires aujourd'hui, comme les smartphones, les applications et les t√©l√©viseurs, il est probable qu'une technologie d'apprentissage profond soit derri√®re. Vous souhaitez supprimer un objet de fond d'une photo prise avec votre smartphone ? C'est un exemple de t√¢che de segmentation panoptique (ne vous inqui√©tez pas si vous ne savez pas encore ce que cela signifie, nous le d√©crirons dans les sections suivantes !).

Cette page fournit un aper√ßu des diff√©rentes t√¢ches de traitement de la parole et de l'audio, de vision par ordinateur et de NLP qui peuvent √™tre r√©solues avec la biblioth√®que ü§ó Transformers en seulement trois lignes de code !

## Audio

Les t√¢ches de traitement audio et de la parole sont l√©g√®rement diff√©rentes des autres modalit√©s principalement parce que l'audio en tant que donn√©e d'entr√©e est un signal continu. Contrairement au texte, un signal audio brut ne peut pas discr√©tis√© de la mani√®re dont une phrase peut √™tre divis√©e en mots. Pour contourner cela, le signal audio brut est g√©n√©ralement √©chantillonn√© √† intervalles r√©guliers. Si vous prenez plus d'√©chantillons dans un intervalle, le taux d'√©chantillonnage est plus √©lev√© et l'audio ressemble davantage √† la source audio originale.

Les approches pr√©c√©dentes pr√©traitaient l'audio pour en extraire des caract√©ristiques utiles. Il est maintenant plus courant de commencer les t√¢ches de traitement audio et de la parole en donnant directement le signal audio brut √† un encodeur de caract√©ristiques pour extraire une repr√©sentation de l'audio. Cela correspond √†l'√©tape de pr√©traitement et permet au mod√®le d'apprendre les caract√©ristiques les plus essentielles du signal.

### Classification audio

La classification audio est une t√¢che qui consiste √† attribuer une classe, parmi un ensemble de classes pr√©d√©fini, √† un audio. La classification audio englobe de nombreuses applications sp√©cifiques, dont certaines incluent :

* la classification d'environnements sonores : attribuer une classe (cat√©gorie) √† l'audio pour indiquer l'environnement associ√©, tel que "bureau", "plage" ou "stade". 
* la d√©tection d'√©v√©nements sonores : √©tiqueter l'audio avec une √©tiquette d'√©v√©nement sonore ("klaxon de voiture", "appel de baleine", "verre bris√©")
* l'identification d'√©l√©ments sonores : attribuer des tags (*√©tiquettes* en fran√ßais) √† l'audio pour marquer des sons sp√©cifiques, comme "chant des oiseaux" ou "identification du locuteur lors d'une r√©union".
* la classification musicale : attribuer un genre √† la musique, comme "metal", "hip-hop" ou "country".

```py
>>> from transformers import pipeline

>>> classifier = pipeline(task="audio-classification", model="superb/hubert-base-superb-er")
>>> preds = classifier("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> preds
[{'score': 0.4532, 'label': 'hap'},
 {'score': 0.3622, 'label': 'sad'},
 {'score': 0.0943, 'label': 'neu'},
 {'score': 0.0903, 'label': 'ang'}]
```

### Reconnaissance vocale

La reconnaissance vocale (*Automatic Speech Recognition* ou ASR en anglais) transcrit la parole en texte. C'est l'une des t√¢ches audio les plus courantes en partie parce que la parole est une forme de communication la plus naturelle pour nous, humains. Aujourd'hui, les syst√®mes ASR sont int√©gr√©s dans des produits technologiques "intelligents" comme les enceintes, les t√©l√©phones et les voitures. Il est d√©sormais possible de demander √† nos assistants virtuels de jouer de la musique, de d√©finir des rappels et de nous indiquer la m√©t√©o.

Mais l'un des principaux d√©fis auxquels les architectures Transformer contribuent √† r√©soudre est celui des langues √† faibles ressources, c'est-√†-dire des langues pour lesquelles il existe peu de donn√©es √©tiquet√©es. En pr√©entra√Ænant sur de grandes quantit√©s de donn√©es vocales d'un autre language plus ou moins similaire, le r√©glage fin (*fine-tuning* en anglais) du mod√®le avec seulement une heure de donn√©es vocales √©tiquet√©es dans une langue √† faibles ressources peut tout de m√™me produire des r√©sultats de haute qualit√© compar√©s aux syst√®mes ASR pr√©c√©dents entra√Æn√©s sur 100 fois plus de donn√©es √©tiquet√©es.

```py
>>> from transformers import pipeline

>>> transcriber = pipeline(task="automatic-speech-recognition", model="openai/whisper-small")
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
```

## Vision par ordinateur

L'une des premi√®res r√©ussites en vision par ordinateur a √©t√© la reconnaissance des num√©ros de code postal √† l'aide d'un [r√©seau de neurones convolutionnel (CNN)](glossary#convolution). Une image est compos√©e de pixels, chacun ayant une valeur num√©rique, ce qui permet de repr√©senter facilement une image sous forme de matrice de valeurs de pixels. Chaque combinaison de valeurs de pixels correspond aux couleurs d'une image.

Il existe deux approches principales pour r√©soudre les t√¢ches de vision par ordinateur :

1. Utiliser des convolutions pour apprendre les caract√©ristiques hi√©rarchiques d'une image, des d√©tails de bas niveau aux √©l√©ments abstraits de plus haut niveau.
2. Diviser l'image en morceaux (*patches* en anglais) et utiliser un Transformer pour apprendre progressivement comment chaque morceau est li√© aux autres pour former l'image compl√®te. Contrairement √† l'approche ascendante des CNNs, cette m√©thode ressemble √† un processus o√π l'on d√©marre avec une image floue pour ensuite la mettre au point petit √† petit. 

### Classification d'images

La classification d'images consiste √† attribuer une classe, parmi un ensemble de classes pr√©d√©fini, √† toute une image. Comme pour la plupart des t√¢ches de classification, les cas d'utilisation pratiques sont nombreux, notamment :

- Sant√© : classification d'images m√©dicales pour d√©tecter des maladies ou surveiller l'√©tat de sant√© des patients.
- Environnement : classification d'images satellites pour suivre la d√©forestation, aider √† la gestion des terres ou d√©tecter les incendies de for√™t.
- Agriculture : classification d'images de cultures pour surveiller la sant√© des plantes ou des images satellites pour analyser l'utilisation des terres.
- √âcologie : classification d'images d'esp√®ces animales ou v√©g√©tales pour suivre les populations fauniques ou les esp√®ces menac√©es.

```py
>>> from transformers import pipeline

>>> classifier = pipeline(task="image-classification")
>>> preds = classifier(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> print(*preds, sep="\n")
{'score': 0.4335, 'label': 'lynx, catamount'}
{'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}
{'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}
{'score': 0.0239, 'label': 'Egyptian cat'}
{'score': 0.0229, 'label': 'tiger cat'}
```

### D√©tection d'objets

La d√©tection d'objets, √† la diff√©rence de la classification d'images, identifie plusieurs objets dans une image ainsi que leurs positions, g√©n√©ralement d√©finies par des bo√Ætes englobantes (*bounding boxes* en anglais). Voici quelques exemples d'applications :

- V√©hicules autonomes : d√©tection des objets de la circulation, tels que les v√©hicules, pi√©tons et feux de signalisation.
- T√©l√©d√©tection : surveillance des catastrophes, planification urbaine et pr√©visions m√©t√©orologiques.
- D√©tection de d√©fauts : identification des fissures ou dommages structurels dans les b√¢timents, ainsi que des d√©fauts de fabrication.

```py
>>> from transformers import pipeline

>>> detector = pipeline(task="object-detection")
>>> preds = detector(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"], "box": pred["box"]} for pred in preds]
>>> preds
[{'score': 0.9865,
  'label': 'cat',
  'box': {'xmin': 178, 'ymin': 154, 'xmax': 882, 'ymax': 598}}]
```

### Segmentation d'images

La segmentation d'images est une t√¢che qui consiste √† attribuer une classe √† chaque pixel d'une image, ce qui la rend plus pr√©cise que la d√©tection d'objets, qui se limite aux bo√Ætes englobantes (*bounding boxes* en anglais). Elle permet ainsi de d√©tecter les objets √† la pr√©cision du pixel. Il existe plusieurs types de segmentation d'images :

- Segmentation d'instances : en plus de classifier un objet, elle identifie chaque instance distincte d'un m√™me objet (par exemple, "chien-1", "chien-2").
- Segmentation panoptique : combine segmentation s√©mantique et segmentation d'instances, attribuant √† chaque pixel une classe s√©mantique **et** une instance sp√©cifique.

Ces techniques sont utiles pour les v√©hicules autonomes, qui doivent cartographier leur environnement pixel par pixel pour naviguer en toute s√©curit√© autour des pi√©tons et des v√©hicules. Elles sont √©galement pr√©cieuses en imagerie m√©dicale, o√π la pr√©cision au niveau des pixels permet de d√©tecter des anomalies cellulaires ou des caract√©ristiques d'organes. Dans le commerce en ligne, la segmentation est utilis√©e pour des essayages virtuels de v√™tements ou des exp√©riences de r√©alit√© augment√©e, en superposant des objets virtuels sur des images du monde r√©el via la cam√©ra.

```py
>>> from transformers import pipeline

>>> segmenter = pipeline(task="image-segmentation")
>>> preds = segmenter(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> print(*preds, sep="\n")
{'score': 0.9879, 'label': 'LABEL_184'}
{'score': 0.9973, 'label': 'snow'}
{'score': 0.9972, 'label': 'cat'}
```

### Estimation de la profondeur

L'estimation de la profondeur consiste √† pr√©dire la distance de chaque pixel d'une image par rapport √† la cam√©ra. Cette t√¢che est cruciale pour comprendre et reconstruire des sc√®nes r√©elles. Par exemple, pour les voitures autonomes, il est essentiel de d√©terminer la distance des objets tels que les pi√©tons, les panneaux de signalisation et les autres v√©hicules pour √©viter les collisions. L'estimation de la profondeur permet √©galement de cr√©er des mod√®les 3D √† partir d'images 2D, ce qui est utile pour g√©n√©rer des repr√©sentations d√©taill√©es de structures biologiques ou de b√¢timents.

Il existe deux principales approches pour estimer la profondeur :

- St√©r√©o : la profondeur est estim√©e en comparant deux images d'une m√™me sc√®ne prises sous des angles l√©g√®rement diff√©rents.
- Monoculaire : la profondeur est estim√©e √† partir d'une seule image.

```py
>>> from transformers import pipeline

>>> depth_estimator = pipeline(task="depth-estimation")
>>> preds = depth_estimator(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
```

## Traitement du langage naturel

Les t√¢ches de traitement du langage naturel (*Natural Language Processing* ou *NLP* en anglais) sont courantes car le texte est une forme naturelle de communication pour nous. Pour qu'un mod√®le puisse traiter le texte, celui-ci doit √™tre *tokenis√©*, c'est-√†-dire divis√© en mots ou sous-mots appel√©s "*tokens*", puis converti en nombres. Ainsi, une s√©quence de texte peut √™tre repr√©sent√©e comme une s√©quence de nombres, qui peut ensuite √™tre utilis√©e comme donn√©es d'entr√©e pour un mod√®le afin de r√©soudre diverses t√¢ches  de traitement du langage naturel.

### Classification de texte

La classification de texte attribue une classe √† une s√©quence de texte (au niveau d'une phrase, d'un paragraphe ou d'un document) √† partir d'un ensemble de classes pr√©d√©fini. Voici quelques applications pratiques :

- **Analyse des sentiments** : √©tiqueter le texte avec une polarit√© telle que `positive` ou `n√©gative`, ce qui aide √† la prise de d√©cision dans des domaines comme la politique, la finance et le marketing.
- **Classification de contenu** : organiser et filtrer les informations en attribuant des *tags* sur des sujets sp√©cifiques, comme `m√©t√©o`, `sports` ou `finance`, dans les flux d'actualit√©s et les r√©seaux sociaux.

```py
>>> from transformers import pipeline

>>> classifier = pipeline(task="sentiment-analysis")
>>> preds = classifier("Hugging Face is the best thing since sliced bread!")
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> preds
[{'score': 0.9991, 'label': 'POSITIVE'}]
```

### Classification des tokens

Dans les t√¢ches de traitement du language naturel, le texte est d'abord pr√©trait√© en le s√©parant en mots ou sous-mots individuels, appel√©s *[tokens](glossary#token)*. La classification des tokens attribue une classe √† chaque token √† partir d'un ensemble de classes pr√©d√©fini.

Voici deux types courants de classification des tokens :

- **Reconnaissance d'entit√©s nomm√©es (*Named Entity Recognition* ou *NER* en anglais)** : √©tiqueter un token selon une cat√©gorie d'entit√©, telle qu'organisation, personne, lieu ou date. La NER est particuli√®rement utilis√©e dans les contextes biom√©dicaux pour identifier des g√®nes, des prot√©ines et des noms de m√©dicaments.
- **√âtiquetage des parties du discours (*Part of Speech* ou *POS* en anglais)** : √©tiqueter un token en fonction de sa partie du discours, comme nom, verbe ou adjectif. Le POS est utile pour les syst√®mes de traduction afin de comprendre comment deux mots identiques peuvent avoir des r√¥les grammaticaux diff√©rents (par exemple, "banque" comme nom versus "banque" comme verbe).

```py
>>> from transformers import pipeline

>>> classifier = pipeline(task="ner")
>>> preds = classifier("Hugging Face is a French company based in New York City.")
>>> preds = [
...     {
...         "entity": pred["entity"],
...         "score": round(pred["score"], 4),
...         "index": pred["index"],
...         "word": pred["word"],
...         "start": pred["start"],
...         "end": pred["end"],
...     }
...     for pred in preds
... ]
>>> print(*preds, sep="\n")
{'entity': 'I-ORG', 'score': 0.9968, 'index': 1, 'word': 'Hu', 'start': 0, 'end': 2}
{'entity': 'I-ORG', 'score': 0.9293, 'index': 2, 'word': '##gging', 'start': 2, 'end': 7}
{'entity': 'I-ORG', 'score': 0.9763, 'index': 3, 'word': 'Face', 'start': 8, 'end': 12}
{'entity': 'I-MISC', 'score': 0.9983, 'index': 6, 'word': 'French', 'start': 18, 'end': 24}
{'entity': 'I-LOC', 'score': 0.999, 'index': 10, 'word': 'New', 'start': 42, 'end': 45}
{'entity': 'I-LOC', 'score': 0.9987, 'index': 11, 'word': 'York', 'start': 46, 'end': 50}
{'entity': 'I-LOC', 'score': 0.9992, 'index': 12, 'word': 'City', 'start': 51, 'end': 55}
```

### R√©ponse √† des questions - (*Question Answering*)

La r√©ponse √† des questions (*Question Answering* ou *QA* en anglais) est une t√¢che de traitement du language naturel qui consiste √† fournir une r√©ponse √† une question, parfois avec l'aide d'un contexte (domaine ouvert) et d'autres fois sans contexte (domaine ferm√©). Cette t√¢che intervient lorsqu'on interroge un assistant virtuel, par exemple pour savoir si un restaurant est ouvert. Elle est √©galement utilis√©e pour le support client, technique, et pour aider les moteurs de recherche √† fournir des informations pertinentes.

Il existe deux types courants de r√©ponse √† des questions :

- **Extractive** : pour une question donn√©e et un contexte fourni, la r√©ponse est extraite directement du texte du contexte par le mod√®le.
- **Abstractive** : pour une question donn√©e et un contexte, la r√©ponse est g√©n√©r√©e √† partir du contexte. Cette approche utilise le [`Text2TextGenerationPipeline`] plut√¥t que le [`QuestionAnsweringPipeline`] montr√© ci-dessous.


```py
>>> from transformers import pipeline

>>> question_answerer = pipeline(task="question-answering")
>>> preds = question_answerer(
...     question="What is the name of the repository?",
...     context="The name of the repository is huggingface/transformers",
... )
>>> print(
...     f"score: {round(preds['score'], 4)}, start: {preds['start']}, end: {preds['end']}, answer: {preds['answer']}"
... )
score: 0.9327, start: 30, end: 54, answer: huggingface/transformers
```

### R√©sum√© de texte - (*Summarization*)

Le r√©sum√© de text consiste √† cr√©er une version plus courte d'un texte tout en conservant l'essentiel du sens du document original. C'est une t√¢che de s√©quence √† s√©quence qui produit un texte plus condens√© √† partir du texte initial. Cette technique est utile pour aider les lecteurs √† saisir rapidement les points cl√©s de longs documents, comme les projets de loi, les documents juridiques et financiers, les brevets, et les articles scientifiques.

Il existe deux types courants de summarization :

- **Extractive** : identifier et extraire les phrases les plus importantes du texte original.
- **Abstractive** : g√©n√©rer un r√©sum√© qui peut inclure des mots nouveaux non pr√©sents dans le texte d'origine. Le [`SummarizationPipeline`] utilise l'approche abstractive.

```py
>>> from transformers import pipeline

>>> summarizer = pipeline(task="summarization")
>>> summarizer(
...     "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles."
... )
[{'summary_text': ' The Transformer is the first sequence transduction model based entirely on attention . It replaces the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention . For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .'}]
```

### Traduction

La traduction convertit un texte d'une langue √† une autre. Elle facilite la communication entre personnes de diff√©rentes langues, permet de toucher des audiences plus larges et peut aussi servir d'outil d'apprentissage pour ceux qui apprennent une nouvelle langue. Comme le r√©sum√© de texte, la traduction est une t√¢che de s√©quence √† s√©quence, o√π le mod√®le re√ßoit une s√©quence d'entr√©e (un texte est ici vu comme une s√©quence de mots, ou plus pr√©cis√©ment de tokens) et produit une s√©quence de sortie dans la langue cible.

Initialement, les mod√®les de traduction √©taient principalement monolingues, mais il y a eu r√©cemment un int√©r√™t croissant pour les mod√®les multilingues capables de traduire entre plusieurs paires de langues.

```py
>>> from transformers import pipeline

>>> text = "translate English to French: Hugging Face is a community-based open-source platform for machine learning."
>>> translator = pipeline(task="translation", model="google-t5/t5-small")
>>> translator(text)
[{'translation_text': "Hugging Face est une tribune communautaire de l'apprentissage des machines."}]
```

### Mod√©lisation du langage

La mod√©lisation du langage consiste √† pr√©dire un mot dans un texte. Cette t√¢che est devenue tr√®s populaire en traitement du language naturel, car un mod√®le de langage pr√©entra√Æn√© sur cette t√¢che peut ensuite √™tre ajust√© (*finetuned*) pour accomplir de nombreuses autres t√¢ches. R√©cemment, les grands mod√®les de langage (LLMs) ont suscit√© beaucoup d'int√©r√™t pour leur capacit√© √† apprendre avec peu ou pas de donn√©es sp√©cifiques √† une t√¢che, ce qui leur permet de r√©soudre des probl√®mes pour lesquels ils n'ont pas √©t√© explicitement entra√Æn√©s. Ces mod√®les peuvent g√©n√©rer du texte fluide et convaincant, bien qu'il soit important de v√©rifier leur pr√©cision.

Il existe deux types de mod√©lisation du langage :

- **Causale** : le mod√®le pr√©dit le token suivant dans une s√©quence, avec les tokens futurs masqu√©s.

    ```py
    >>> from transformers import pipeline

    >>> prompt = "Hugging Face is a community-based open-source platform for machine learning."
    >>> generator = pipeline(task="text-generation")
    >>> generator(prompt)  # doctest: +SKIP
    ```

- **Masqu√©e** : le mod√®le pr√©dit un token masqu√© dans une s√©quence en ayant acc√®s √† tous les autres tokens de la s√©quence (pass√© et futur).

    ```py
    >>> text = "Hugging Face is a community-based open-source <mask> for machine learning."
    >>> fill_mask = pipeline(task="fill-mask")
    >>> preds = fill_mask(text, top_k=1)
    >>> preds = [
    ...     {
    ...         "score": round(pred["score"], 4),
    ...         "token": pred["token"],
    ...         "token_str": pred["token_str"],
    ...         "sequence": pred["sequence"],
    ...     }
    ...     for pred in preds
    ... ]
    >>> preds
    [{'score': 0.2236,
      'token': 1761,
      'token_str': ' platform',
      'sequence': 'Hugging Face is a community-based open-source platform for machine learning.'}]
    ```

## Multimodal

Les t√¢ches multimodales n√©cessitent qu'un mod√®le traite plusieurs types de donn√©es (texte, image, audio, vid√©o) pour r√©soudre un probl√®me sp√©cifique. Par exemple, la g√©n√©ration de l√©gendes pour les images est une t√¢che multimodale o√π le mod√®le prend une image en entr√©e et produit une s√©quence de texte d√©crivant l'image ou ses propri√©t√©s.

Bien que les mod√®les multimodaux traitent divers types de donn√©es, ils convertissent toutes ces donn√©es en *embeddings* (vecteurs ou listes de nombres contenant des informations significatives). Pour des t√¢ches comme la g√©n√©ration de l√©gendes pour les images, le mod√®le apprend les relations entre les *embeddings* d'images et ceux de texte.

### R√©ponse √† des questions sur des documents - (*Document Question Answering*)

La r√©ponse √† des questions sur des documents consiste √† r√©pondre √† des questions en langage naturel en utilisant un document comme r√©f√©rence. Contrairement √† la r√©ponse √† des questions au niveau des tokens, qui prend du texte en entr√©e, cette t√¢che prend une image d'un document ainsi qu'une question concernant ce document, et fournit une r√©ponse. Elle est utile pour analyser des donn√©es structur√©es et extraire des informations cl√©es. Par exemple, √† partir d'un re√ßu, on peut extraire des informations telles que le montant total et le change d√ª.

```py
>>> from transformers import pipeline
>>> from PIL import Image
>>> import requests

>>> url = "https://huggingface.co/datasets/hf-internal-testing/example-documents/resolve/main/jpeg_images/2.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> doc_question_answerer = pipeline("document-question-answering", model="magorshunov/layoutlm-invoices")
>>> preds = doc_question_answerer(
...     question="What is the total amount?",
...     image=image,
... )
>>> preds
[{'score': 0.8531, 'answer': '17,000', 'start': 4, 'end': 4}]
```

En esp√©rant que cette page vous ait donn√© plus d'informations sur les diff√©rents types de t√¢ches dans chaque modalit√© et l'importance pratique de chacune d'elles. Dans la [section suivante](tasks_explained), vous d√©couvrirez **comment** ü§ó Transformers fonctionne pour r√©soudre ces t√¢ches.