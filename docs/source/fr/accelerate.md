<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# EntraÃ®nement distribuÃ© avec ðŸ¤— Accelerate

Comme les modÃ¨les deviennent plus gros, le parallÃ©lisme est devenu une stratÃ©gie pour entraÃ®ner des modÃ¨les plus grands sur du matÃ©riel aux capacitÃ©s limitÃ©es et accÃ©lÃ©rer la vitesse d'entraÃ®nement de plusieurs ordres de grandeur. Hugging Face fournit la librairie [ðŸ¤— Accelerate](https://huggingface.co/docs/accelerate) pour aider les utilisateurs Ã  entraÃ®ner facilement un modÃ¨le ðŸ¤— Transformers sur n'importe quel type de configuration distribuÃ©e, qu'il s'agisse de plusieurs GPU sur une machine ou de plusieurs GPU sur plusieurs machines. Dans ce tutoriel, vous apprenez Ã  personnaliser votre boucle d'entraÃ®nement avec PyTorch pour permettre l'entraÃ®nement dans un environnement distribuÃ©.

## Configuration

Commencez par installer ðŸ¤— Accelerate :

```bash
pip install accelerate
```

Ensuite, importez et crÃ©ez un objet [`~accelerate.Accelerator`]. L'objet [`~accelerate.Accelerator`] dÃ©tectera automatiquement votre type de configuration distribuÃ©e et initialisera tous les composants nÃ©cessaires Ã  l'entraÃ®nement. Vous n'avez pas besoin de placer explicitement votre modÃ¨le sur une carte graphique ou CPU.

```py
>>> from accelerate import Accelerator

>>> accelerator = Accelerator()
```

## PrÃ©paration pour l'accÃ©lÃ©ration

L'Ã©tape suivante consiste Ã  passer tous les objets d'entraÃ®nement pertinents Ã  la mÃ©thode [`~accelerate.Accelerator.prepare`]. Cela inclut les DataLoaders pour l'entraÃ®nement et l'Ã©valuation, un modÃ¨le et un optimiseur :

```py
>>> train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
...     train_dataloader, eval_dataloader, model, optimizer
... )
```

## Retropropagation

La derniÃ¨re Ã©tape consiste Ã  remplacer `loss.backward()` dans votre boucle d'entraÃ®nement par la fonction [`~accelerate.Accelerator.backward`] de ðŸ¤— Accelerate :

```py
>>> for epoch in range(num_epochs):
...     for batch in train_dataloader:
...         outputs = model(**batch)
...         loss = outputs.loss
...         accelerator.backward(loss)

...         optimizer.step()
...         lr_scheduler.step()
...         optimizer.zero_grad()
...         progress_bar.update(1)
```

Comme vous pouvez le voir dans le code dessous, vous avez seulement besoin d'ajouter quatre lignes de code Ã  votre boucle d'entraÃ®nement pour activer l'entraÃ®nement distribuÃ© !

```diff
+ from accelerate import Accelerator
  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

+ accelerator = Accelerator()

  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
  optimizer = AdamW(model.parameters(), lr=3e-5)

- device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
- model.to(device)

+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
+     train_dataloader, eval_dataloader, model, optimizer
+ )

  num_epochs = 3
  num_training_steps = num_epochs * len(train_dataloader)
  lr_scheduler = get_scheduler(
      "linear",
      optimizer=optimizer,
      num_warmup_steps=0,
      num_training_steps=num_training_steps
  )

  progress_bar = tqdm(range(num_training_steps))

  model.train()
  for epoch in range(num_epochs):
      for batch in train_dataloader:
-         batch = {k: v.to(device) for k, v in batch.items()}
          outputs = model(**batch)
          loss = outputs.loss
-         loss.backward()
+         accelerator.backward(loss)

          optimizer.step()
          lr_scheduler.step()
          optimizer.zero_grad()
          progress_bar.update(1)
```

## EntraÃ®nement

Une fois que vous avez ajoutÃ© les lignes de code nÃ©cessaires, vous pouvez lancer votre entraÃ®nement avec un script ou un notebook comme Colaboratory.

### EntraÃ®nement avec un script

Si votre entraÃ®nement est lancÃ© avec un script, vous devez exÃ©cuter la commande suivante pour crÃ©er et enregistrer un fichier de configuration :

```bash
accelerate config
```

Puis, vous lancez l'entraÃ®nement avec la commande suivante :

```bash
accelerate launch train.py
```

### EntraÃ®nement avec un notebook

ðŸ¤— Accelerate peut aussi etre utilisÃ© dans un notebook si vous prÃ©voyez d'utiliser les TPUs de Colaboratory. CrÃ©ez une fonction contenant le code responsable de l'entraÃ®nement, et passez-la Ã  [`~accelerate.notebook_launcher`]:

```py
>>> from accelerate import notebook_launcher

>>> notebook_launcher(training_function)
```

Pour plus d'informations sur ðŸ¤— Accelerate et ses fonctionnalitÃ©s, consultez la [documentation](https://huggingface.co/docs/accelerate).
