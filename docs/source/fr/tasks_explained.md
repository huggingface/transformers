<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Comment ü§ó Transformers r√©sout ces t√¢ches

Dans [Ce que ü§ó Transformers peut faire](task_summary), vous avez d√©couvert les t√¢ches de traitement du langage naturel (NLP), de traitement de la parole et de l'audio, de vision par ordinateur, ainsi que certaines de leurs applications importantes. Cette page se penche sur la mani√®re dont les mod√®les r√©solvent ces t√¢ches et explique les processus en arri√®re-plan. Bien que diff√©rents mod√®les puissent utiliser diverses techniques ou approches innovantes, les mod√®les Transformer suivent g√©n√©ralement une id√©e commune. Gr√¢ce √† leur architecture flexible, la plupart des mod√®les sont bas√©s sur un encodeur, un d√©codeur ou une combinaison encodeur-d√©codeur. En plus des mod√®les Transformer, notre biblioth√®que comprend √©galement des r√©seaux de neurones convolutifs (CNN), qui restent utilis√©s pour les t√¢ches de vision par ordinateur. Nous expliquerons aussi le fonctionnement d'un CNN moderne.

Voici comment diff√©rents mod√®les r√©solvent des t√¢ches sp√©cifiques :

- [Wav2Vec2](model_doc/wav2vec2) pour la classification audio et la reconnaissance vocale (*ASR* en anglais)
- [Vision Transformer (ViT)](model_doc/vit) et [ConvNeXT](model_doc/convnext) pour la classification d'images
- [DETR](model_doc/detr) pour la d√©tection d'objets
- [Mask2Former](model_doc/mask2former) pour la segmentation d'images
- [GLPN](model_doc/glpn) pour l'estimation de la profondeur
- [BERT](model_doc/bert) pour les t√¢ches de traitement du language naturel telles que la classification de texte, la classification des tokens et la r√©ponse √† des questions utilisant un encodeur
- [GPT2](model_doc/gpt2) pour les t√¢ches de traitement du language naturel telles que la g√©n√©ration de texte utilisant un d√©codeur
- [BART](model_doc/bart) pour les t√¢ches de traitement du language naturel telles que le r√©sum√© de texte et la traduction utilisant un encodeur-d√©codeur

<Tip>

Avant de poursuivre, il est utile d'avoir quelques connaissances de base sur l'architecture des Transformers. Comprendre le fonctionnement des encodeurs, des d√©codeurs et du m√©canisme d'attention vous aidera √† saisir comment les diff√©rents mod√®les Transformer fonctionnent. Si vous d√©butez ou avez besoin d'un rappel, consultez notre [cours](https://huggingface.co/course/chapter1/4?fw=pt) pour plus d'informations !

</Tip>

## Paroles et audio

[Wav2Vec2](model_doc/wav2vec2) est un mod√®le auto-supervis√© qui est pr√©entra√Æn√© sur des donn√©es de parole non √©tiquet√©es et ajust√© sur des donn√©es √©tiquet√©es pour des t√¢ches telles que la classification audio et la reconnaissance vocale (ASR).

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/wav2vec2_architecture.png"/>
</div>

Ce mod√®le comporte quatre composants principaux :

1. **Encodeur de caract√©ristiques** (*feature encoder*): Il prend le signal audio brut, le normalise pour avoir une moyenne nulle et une variance unitaire, et le convertit en une s√©quence de vecteurs de caract√©ristiques, chacun repr√©sentant une dur√©e de 20 ms.

2. **Module de quantification** (*quantization module*): Les vecteurs de caract√©ristiques sont pass√©s √† ce module pour apprendre des unit√©s de parole discr√®tes. Chaque vecteur est associ√© √† un *codebook* (une collection de mots-cl√©s), et l'unit√© de parole la plus repr√©sentative est s√©lectionn√©e parmi celles du codebook et transmise au mod√®le.

3. **R√©seau de contexte** (*context network*): Environ la moiti√© des vecteurs de caract√©ristiques sont masqu√©s al√©atoirement. Les vecteurs masqu√©s sont ensuite envoy√©s √† un *r√©seau de contexte*, qui est un encodeur qui ajoute des embeddings positionnels relatifs.

4. **T√¢che contrastive** (*contrastive task*): Le r√©seau de contexte est pr√©entra√Æn√© avec une t√¢che contrastive. Le mod√®le doit pr√©dire la v√©ritable unit√© de parole quantifi√©e √† partir de la pr√©diction masqu√©e parmi un ensemble de fausses, ce qui pousse le mod√®le √† trouver l'unit√© de parole quantifi√©e la plus proche de la pr√©diction.

Une fois pr√©entra√Æn√©, wav2vec2 peut √™tre ajust√© sur vos propres donn√©es pour des t√¢ches comme la classification audio ou la reconnaissance automatique de la parole !

### Classification audio

Pour utiliser le mod√®le pr√©entra√Æn√© pour la classification audio, ajoutez une t√™te de classification de s√©quence au-dessus du mod√®le Wav2Vec2 de base. Cette t√™te de classification est une couche lin√©aire qui re√ßoit les √©tats cach√©s (*hidden states*) de l'encodeur. Ces √©tats cach√©s, qui repr√©sentent les caract√©ristiques apprises de chaque trame audio, peuvent avoir des longueurs variables. Pour obtenir un vecteur de longueur fixe, les √©tats cach√©s sont d'abord regroup√©s, puis transform√©s en logits correspondant aux √©tiquettes de classe. La perte d'entropie crois√©e est calcul√©e entre les logits et la cible pour d√©terminer la classe la plus probable.

Pr√™t √† vous lancer dans la classification audio ? Consultez notre [guide complet de classification audio](tasks/audio_classification) pour apprendre √† ajuster Wav2Vec2 et √† l'utiliser pour l'inf√©rence !

### Reconnaissance vocale

Pour utiliser le mod√®le pr√©entra√Æn√© pour la reconnaissance vocale, ajoutez une t√™te de mod√©lisation du langage au-dessus du mod√®le Wav2Vec2 de base pour la [classification temporelle connexionniste (CTC)](glossary#connectionist-temporal-classification-ctc). Cette t√™te de mod√©lisation du langage est une couche lin√©aire qui prend les √©tats cach√©s (*hidden states*) de l'encodeur et les convertit en logits. Chaque logit correspond √† une classe de token (le nombre de tokens provient du vocabulaire de la t√¢che). La perte CTC est calcul√©e entre les logits et les cibles (*targets*) pour identifier la s√©quence de tokens la plus probable, qui est ensuite d√©cod√©e en transcription.

Pr√™t √† vous lancer dans la reconnaissance automatique de la parole ? Consultez notre [guide complet de reconnaissance automatique de la parole](tasks/asr) pour apprendre √† ajuster Wav2Vec2 et √† l'utiliser pour l'inf√©rence !

## Vision par ordinateur

Il existe deux fa√ßons d'aborder les t√¢ches de vision par ordinateur :

1. **Diviser une image en une s√©quence de patches** et les traiter en parall√®le avec un Transformer.
2. **Utiliser un CNN moderne**, comme [ConvNeXT](model_doc/convnext), qui repose sur des couches convolutionnelles mais adopte des conceptions de r√©seau modernes.

<Tip>

Une troisi√®me approche combine les Transformers avec des convolutions (par exemple, [Convolutional Vision Transformer](model_doc/cvt) ou [LeViT](model_doc/levit)). Nous ne discuterons pas de ces approches ici, car elles m√©langent simplement les deux approches que nous examinons.

</Tip>

ViT et ConvNeXT sont couramment utilis√©s pour la classification d'images. Pour d'autres t√¢ches de vision par ordinateur comme la d√©tection d'objets, la segmentation et l'estimation de la profondeur, nous examinerons respectivement DETR, Mask2Former et GLPN, qui sont mieux adapt√©s √† ces t√¢ches.

### Classification d'images

ViT et ConvNeXT peuvent tous deux √™tre utilis√©s pour la classification d'images ; la principale diff√©rence r√©side dans leurs approches : ViT utilise un m√©canisme d'attention tandis que ConvNeXT repose sur des convolutions.

#### Transformer

[ViT](model_doc/vit) remplace enti√®rement les convolutions par une architecture Transformer pure. Si vous √™tes d√©j√† familiaris√© avec le Transformer original, vous trouverez que ViT suit des principes similaires, mais adapt√©s pour traiter les images comme des s√©quences de patches.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vit_architecture.jpg"/>
</div>

Le principal changement introduit par ViT concerne la fa√ßon dont les images sont fournies √† un Transformer :

1. **Tokenisation des images** : L'image est divis√©e en patches carr√©s non chevauchants, chacun √©tant transform√© en un vecteur ou *embedding de patch*. Ces embeddings de patch sont g√©n√©r√©s √† partir d'une couche convolutionnelle 2D pour adapter les dimensions d'entr√©e (par exemple, 768 valeurs pour chaque embedding de patch). Si vous avez une image de 224x224 pixels, elle peut √™tre divis√©e en 196 patches de 16x16 pixels. Ainsi, une image est "tokenis√©e" en une s√©quence de patches.

2. **Token `[CLS]`** : Un *embedding apprenables* sp√©cial, appel√© token `[CLS]`, est ajout√© au d√©but des embeddings de patch, similaire √† BERT. L'√©tat cach√© final du token `[CLS]` est utilis√© comme entr√©e pour la t√™te de classification attach√©e, tandis que les autres sorties sont ignor√©es. Ce token aide le mod√®le √† encoder une repr√©sentation globale de l'image.

3. **Embeddings de position** : Pour que le mod√®le comprenne l'ordre des patches, des *embeddings de position* sont ajout√©s aux embeddings de patch. Ces embeddings de position, √©galement apprenables et de la m√™me taille que les embeddings de patch, permettent au mod√®le de saisir la structure spatiale de l'image.

4. **Classification** : Les embeddings, enrichis des embeddings de position, sont ensuite trait√©s par l'encodeur Transformer. La sortie associ√©e au token `[CLS]` est pass√©e √† une t√™te de perceptron multicouche (MLP) pour la classification. La t√™te MLP convertit cette sortie en logits pour chaque √©tiquette de classe, et la perte d'entropie crois√©e est calcul√©e pour d√©terminer la classe la plus probable.

Pr√™t √† vous essayer √† la classification d'images ? Consultez notre [guide complet de classification d'images](tasks/image_classification) pour apprendre √† ajuster ViT et √† l'utiliser pour l'inf√©rence !

#### CNN

<Tip>

Cette section explique bri√®vement les convolutions, mais il serait utile d'avoir une compr√©hension pr√©alable de la fa√ßon dont elles modifient la forme et la taille d'une image. Si vous n'√™tes pas familier avec les convolutions, consultez le [chapitre sur les r√©seaux de neurones convolutionnels](https://github.com/fastai/fastbook/blob/master/13_convolutions.ipynb) du livre fastai !

</Tip>

[ConvNeXT](model_doc/convnext) est une architecture CNN qui adopte des conceptions de r√©seau modernes pour am√©liorer les performances. Cependant, les convolutions restent au c≈ìur du mod√®le. D'un point de vue g√©n√©ral, une [convolution](glossary#convolution) est une op√©ration o√π une matrice plus petite (*noyau*) est multipli√©e par une petite fen√™tre de pixels de l'image. Elle calcule certaines caract√©ristiques √† partir de cette fen√™tre, comme une texture particuli√®re ou la courbure d'une ligne. Ensuite, elle se d√©place vers la fen√™tre suivante de pixels ; la distance parcourue par la convolution est appel√©e le *stride*.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/convolution.gif"/>
</div>

<small>Une convolution de base sans padding ni stride, tir√©e de <a href="https://huggingface.co/papers/1603.07285">Un guide des calculs de convolution pour l'apprentissage profond.</a></small>

Vous pouvez alimenter la sortie d'une couche convolutionnelle √† une autre couche convolutionnelle. √Ä chaque couche successive, le r√©seau apprend des caract√©ristiques de plus en plus complexes et abstraites, telles que des objets sp√©cifiques comme des hot-dogs ou des fus√©es. Entre les couches convolutionnelles, il est courant d'ajouter des couches de pooling pour r√©duire la dimensionnalit√© et rendre le mod√®le plus robuste aux variations de position des caract√©ristiques.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/convnext_architecture.png"/>
</div>

ConvNeXT modernise un CNN de cinq mani√®res :

1. **Modification du nombre de blocs** : ConvNeXT utilise une approche similaire √† ViT en "patchifiant" l'image avec un stride plus grand et une taille de noyau correspondante, divisant ainsi l'image en patches non chevauchants.

2. **Couche de goulot d'√©tranglement** (*bottleneck layer*) : Cette couche r√©duit puis restaure le nombre de canaux pour acc√©l√©rer les convolutions 1x1, permettant une plus grande profondeur du r√©seau. Un goulot d'√©tranglement invers√© augmente d'abord le nombre de canaux avant de les r√©duire, optimisant ainsi l'utilisation de la m√©moire.

3. **Convolution en profondeur** (*depthwise convolution*): Remplace la convolution 3x3 traditionnelle par une convolution appliqu√©e √† chaque canal d'entr√©e s√©par√©ment, am√©liorant ainsi la largeur du r√©seau et ses performances.

4. **Augmentation de la taille du noyau** : ConvNeXT utilise un noyau de 7x7 pour imiter le champ r√©ceptif global de ViT, ce qui permet de capturer des informations sur une plus grande partie de l'image.

5. **Changements de conception des couches** : Le mod√®le adopte des modifications inspir√©es des Transformers, telles que moins de couches d'activation et de normalisation, l'utilisation de GELU au lieu de ReLU, et LayerNorm plut√¥t que BatchNorm.

La sortie des blocs de convolution est ensuite pass√©e √† une t√™te de classification, qui convertit les sorties en logits et calcule la perte d'entropie crois√©e pour d√©terminer l'√©tiquette la plus probable.

### Object detection

[DETR](model_doc/detr), *DEtection TRansformer*, est un mod√®le de d√©tection d'objets de bout en bout qui combine un CNN avec un encodeur-d√©codeur Transformer.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/detr_architecture.png"/>
</div>

D√©composons le fonctionnement de DETR (DEtection TRansformer) pour la d√©tection d'objets :

1. **Extraction des caract√©ristiques avec le CNN** : Un CNN pr√©entra√Æn√©, appel√© *backbone*, prend une image et g√©n√®re une carte de caract√©ristiques (*feature map*) √† basse r√©solution. Une convolution 1x1 est ensuite appliqu√©e pour r√©duire la dimensionnalit√© et cr√©er une nouvelle carte de caract√©ristiques qui repr√©sente des abstractions de plus haut niveau de l'image. Cette derni√®re est ensuite aplatie en une s√©quence de vecteurs de caract√©ristiques, qui sont combin√©s avec des embeddings positionnels.

2. **Traitement avec l'encodeur et le d√©codeur** : Les vecteurs de caract√©ristiques sont pass√©s √† l'encodeur, qui apprend les repr√©sentations de l'image avec ses couches d'attention. Les √©tats cach√©s de l'encodeur sont ensuite combin√©s avec des *objects queries* dans le d√©codeur. Ces *objects queries* sont des embeddings appris qui se concentrent sur diff√©rentes r√©gions de l'image et sont mis √† jour √† chaque couche d'attention. Les √©tats cach√©s du d√©codeur sont utilis√©s pour pr√©dire les coordonn√©es de la bo√Æte englobante (*bounding box*) et le label de la classe pour chaque objet query, ou `pas d'objet` si aucun objet n'est d√©tect√©.

3. **Perte de correspondance bipartite** : Lors de l'entra√Ænement, DETR utilise une *perte de correspondance bipartite* pour comparer un nombre fixe de pr√©dictions avec un ensemble fixe de labels de v√©rit√© terrain. Si le nombre de labels de v√©rit√© terrain est inf√©rieur au nombre de *N* labels, ils sont compl√©t√©s avec une classe `pas d'objet`. Cette fonction de perte encourage DETR √† trouver une correspondance un √† un entre les pr√©dictions et les labels de v√©rit√© terrain. Si les bo√Ætes englobantes ou les labels de classe ne sont pas corrects, une perte est encourue. De m√™me, si DETR pr√©dit un objet inexistant, il est p√©nalis√©. Cela encourage DETR √† trouver d'autres objets dans l'image au lieu de se concentrer sur un seul objet tr√®s pro√©minent.

Une t√™te de d√©tection d'objets est ajout√©e au-dessus de DETR pour trouver le label de la classe et les coordonn√©es de la bo√Æte englobante. Cette t√™te de d√©tection d'objets comprend deux composants : une couche lin√©aire pour transformer les √©tats cach√©s du d√©codeur en logits sur les labels de classe, et un MLP pour pr√©dire la bo√Æte englobante.

Pr√™t √† essayer la d√©tection d'objets ? Consultez notre guide complet sur la [d√©tection d'objets](tasks/object_detection) pour apprendre √† affiner DETR et √† l'utiliser pour l'inf√©rence !

### Segmentation d'image

[Mask2Former](model_doc/mask2former) est une architecture polyvalente con√ßue pour traiter tous les types de t√¢ches de segmentation d'image. Contrairement aux mod√®les de segmentation traditionnels, qui sont g√©n√©ralement sp√©cialis√©s dans des sous-t√¢ches sp√©cifiques comme la segmentation d'instances, s√©mantique ou panoptique, Mask2Former aborde chaque t√¢che comme un probl√®me de *classification de masques*. Cette approche regroupe les pixels en *N* segments et pr√©dit pour chaque image *N* masques ainsi que leur √©tiquette de classe correspondante. Dans cette section, nous vous expliquerons le fonctionnement de Mask2Former et vous aurez la possibilit√© d'effectuer un r√©glage fin (*fine-tuning*) de SegFormer √† la fin.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/mask2former_architecture.png"/>
</div>

Il y a trois composants principaux dans Mask2Former :

1. Un [backbone Swin](model_doc/swin) qui prend une image en entr√©e et g√©n√®re une carte de caract√©ristiques (*feature map*) √† basse r√©solution apr√®s trois convolutions successives de 3x3.

2. Cette carte de caract√©ristiques est ensuite envoy√©e √† un *d√©codeur de pixels*, qui augmente progressivement la r√©solution des caract√©ristiques pour obtenir des embeddings par pixel en haute r√©solution. Le d√©codeur de pixels produit des caract√©ristiques multi-√©chelles, comprenant des r√©solutions de 1/32, 1/16, et 1/8 de l'image originale.

3. Les cartes de caract√©ristiques √† diff√©rentes √©chelles sont successivement trait√©es par une couche de d√©codeur Transformer, permettant de capturer les petits objets √† partir des caract√©ristiques haute r√©solution. Le point central de Mask2Former est le m√©canisme de *masquage d'attention* dans le d√©codeur. Contrairement √† l'attention crois√©e, qui peut se concentrer sur l'ensemble de l'image, l'attention masqu√©e se focalise uniquement sur certaines zones sp√©cifiques. Cette approche est plus rapide et am√©liore les performances en permettant au mod√®le de se concentrer sur les d√©tails locaux de l'image.

4. √Ä l'instar de [DETR](tasks_explained#object-detection), Mask2Former utilise √©galement des requ√™tes d'objet apprises, qu'il combine avec les caract√©ristiques de l'image du d√©codeur de pixels pour faire des pr√©dictions globales (c'est-√†-dire, `√©tiquette de classe`, `pr√©diction de masque`). Les √©tats cach√©s du d√©codeur sont pass√©s dans une couche lin√©aire pour √™tre transform√©s en logits correspondant aux √©tiquettes de classe. La perte d'entropie crois√©e est alors calcul√©e entre les logits et l'√©tiquette de classe pour d√©terminer la plus probable.

   Les pr√©dictions de masque sont g√©n√©r√©es en combinant les embeddings de pixels avec les √©tats cach√©s finaux du d√©codeur. La perte d'entropie crois√©e sigmo√Øde et la perte de Dice sont calcul√©es entre les logits et le masque de v√©rit√© terrain pour d√©terminer le masque le plus probable.

Pr√™t √† vous lancer dans la d√©tection d'objets ? Consultez notre [guide complet sur la segmentation d'image](tasks/semantic_segmentation) pour apprendre √† affiner SegFormer et l'utiliser pour l'inf√©rence !

### Estimation de la profondeur

[GLPN](model_doc/glpn), *Global-Local Path Network*, est un Transformer pour l'estimation de profondeur qui combine un encodeur [SegFormer](model_doc/segformer) avec un d√©codeur l√©ger.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/glpn_architecture.jpg"/>
</div>
1. Comme avec ViT, une image est divis√©e en une s√©quence de patches, mais ces patches sont plus petits. Cette approche est particuli√®rement adapt√©e aux t√¢ches de pr√©diction dense telles que la segmentation ou l'estimation de profondeur. Les patches d'image sont transform√©s en embeddings (voir la section [classification d'image](#image-classification) pour plus de d√©tails sur la cr√©ation des embeddings), puis envoy√©s √† l'encodeur.

2. L'encodeur traite les embeddings de patches √† travers plusieurs blocs d'encodeur. Chaque bloc comprend des couches d'attention et de Mix-FFN, con√ßues pour fournir des informations positionnelles. √Ä la fin de chaque bloc, une couche de *fusion de patches* cr√©e des repr√©sentations hi√©rarchiques. Les caract√©ristiques des groupes de patches voisins sont concat√©n√©es, et une couche lin√©aire est appliqu√©e pour r√©duire le nombre de patches √† une r√©solution de 1/4. Ce processus est r√©p√©t√© dans les blocs suivants jusqu'√† obtenir des caract√©ristiques d'image avec des r√©solutions de 1/8, 1/16, et 1/32.

3. Un d√©codeur l√©ger prend la derni√®re carte de caract√©ristiques (√† l'√©chelle 1/32) de l'encodeur et l'agrandit √† l'√©chelle 1/16. Ensuite, cette caract√©ristique passe par un module de *Fusion de Caract√©ristiques S√©lective (SFF)*, qui s√©lectionne et combine les caract√©ristiques locales et globales √† partir d'une carte d'attention pour chaque caract√©ristique, puis l'agrandit √† 1/8. Ce processus est r√©p√©t√© jusqu'√† ce que les caract√©ristiques d√©cod√©es aient la m√™me taille que l'image originale. La sortie est ensuite trait√©e par deux couches de convolution, suivies d'une activation sigmo√Øde pour pr√©dire la profondeur de chaque pixel.

## Traitement du langage naturel

Le Transformer a √©t√© initialement con√ßu pour la traduction automatique, et depuis, il est devenu pratiquement l'architecture par d√©faut pour r√©soudre toutes les t√¢ches de traitement du langage naturel (NLP). Certaines t√¢ches se pr√™tent bien √† la structure d'encodeur du Transformer, tandis que d'autres sont mieux adapt√©es au d√©codeur. D'autres t√¢ches encore utilisent √† la fois la structure encodeur-d√©codeur du Transformer.

### Classification de texte

[BERT](model_doc/bert) est un mod√®le bas√© uniquement sur l'encodeur, qui a √©t√© le premier √† int√©grer efficacement la bidirectionnalit√© profonde pour obtenir des repr√©sentations plus riches du texte en tenant compte des mots en amont et en aval.

1. BERT utilise la tokenisation [WordPiece](tokenizer_summary#wordpiece) pour g√©n√©rer des embeddings de tokens √† partir du texte. Pour diff√©rencier une seule phrase d'une paire de phrases, un token sp√©cial `[SEP]` est ajout√©. De plus, un token sp√©cial `[CLS]` est plac√© au d√©but de chaque s√©quence de texte. La sortie finale associ√©e au token `[CLS]` est utilis√©e comme entr√©e pour la t√™te de classification des t√¢ches. BERT ajoute √©galement un embedding de segment pour indiquer si un token appartient √† la premi√®re ou √† la deuxi√®me phrase dans une paire.

2. BERT est pr√©entra√Æn√© avec deux objectifs : le masquage de mots (masked language modeling) et la pr√©diction de la phrase suivante. Pour le masquage de mots, un pourcentage des tokens d'entr√©e est masqu√© al√©atoirement, et le mod√®le doit pr√©dire ces mots. Cela permet de surmonter le probl√®me de la bidirectionnalit√©, o√π le mod√®le pourrait autrement tricher en voyant tous les mots et en "pr√©dire" le mot suivant. Les √©tats cach√©s finaux des tokens masqu√©s sont pass√©s √† un r√©seau feedforward avec une fonction softmax sur le vocabulaire pour pr√©dire le mot masqu√©.

   Le deuxi√®me objectif de pr√©entra√Ænement est la pr√©diction de la phrase suivante. Le mod√®le doit d√©terminer si la phrase B suit la phrase A. Dans la moiti√© des cas, la phrase B est la phrase suivante, et dans l'autre moiti√©, elle est al√©atoire. Cette pr√©diction (phrase suivante ou non) est envoy√©e √† un r√©seau feedforward avec une softmax sur les deux classes (`IsNext` et `NotNext`).

3. Les embeddings d'entr√©e sont trait√©s par plusieurs couches d'encodeur pour produire des √©tats cach√©s finaux.

Pour utiliser le mod√®le pr√©entra√Æn√© pour la classification de texte, ajoutez une t√™te de classification de s√©quence au-dessus du mod√®le BERT de base. Cette t√™te est une couche lin√©aire qui prend les √©tats cach√©s finaux et les transforme en logits. La perte d'entropie crois√©e est ensuite calcul√©e entre les logits et les cibles pour d√©terminer l'√©tiquette la plus probable.

Pr√™t √† essayer la classification de texte ? Consultez notre [guide complet sur la classification de texte](tasks/sequence_classification) pour apprendre √† effectuer un r√©glagle fin (*fine-tuning*) de DistilBERT et l'utiliser pour l'inf√©rence !

### Classification de tokens

Pour utiliser BERT dans des t√¢ches de classification de tokens, comme la reconnaissance d'entit√©s nomm√©es (NER), ajoutez une t√™te de classification de tokens au-dessus du mod√®le BERT de base. Cette t√™te est une couche lin√©aire qui prend les √©tats cach√©s finaux et les transforme en logits. La perte d'entropie crois√©e est ensuite calcul√©e entre les logits et les labels de chaque token pour d√©terminer l'√©tiquette la plus probable.

Pr√™t √† essayer la classification de tokens ? Consultez notre [guide complet sur la classification de tokens](tasks/token_classification) pour d√©couvrir comment effectuer un r√©glagle fin (*fine-tuning*) de DistilBERT et l'utiliser pour l'inf√©rence !

### R√©ponse aux questions - (*Question Answering*)

Pour utiliser BERT pour la r√©ponse aux questions, ajoutez une t√™te de classification de span au-dessus du mod√®le BERT de base. Cette t√™te est une couche lin√©aire qui transforme les √©tats cach√©s finaux en logits pour les positions de d√©but et de fin du `span` correspondant √† la r√©ponse. La perte d'entropie crois√©e est calcul√©e entre les logits et les positions r√©elles pour d√©terminer le span de texte le plus probable en tant que r√©ponse.

Pr√™t √† essayer la r√©ponse aux questions ? Consultez notre [guide complet sur la r√©ponse aux questions](tasks/question_answering) pour d√©couvrir comment effectuer un r√©glagle fin (*fine-tuning*) de DistilBERT et l'utiliser pour l'inf√©rence !

<Tip>

üí° Une fois BERT pr√©entra√Æn√©, il est incroyablement facile de l‚Äôadapter √† diverses t√¢ches ! Il vous suffit d‚Äôajouter une t√™te sp√©cifique au mod√®le pr√©entra√Æn√© pour transformer les √©tats cach√©s en la sortie souhait√©e.

</Tip>

### G√©n√©ration de texte

[GPT-2](model_doc/gpt2) est un mod√®le bas√© uniquement sur le d√©codeur, pr√©entra√Æn√© sur une grande quantit√© de texte. Il peut g√©n√©rer du texte convaincant (bien que parfois inexact !) √† partir d'une invite et accomplir d'autres t√¢ches de NLP, comme la r√©ponse aux questions, m√™me s'il n'a pas √©t√© sp√©cifiquement entra√Æn√© pour ces t√¢ches.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gpt2_architecture.png"/>
</div>

1. GPT-2 utilise le [byte pair encoding (BPE)](tokenizer_summary#bytepair-encoding-bpe) pour tokeniser les mots et g√©n√©rer des embeddings de tokens. Des encodages positionnels sont ajout√©s pour indiquer la position de chaque token dans la s√©quence. Les embeddings d'entr√©e passent √† travers plusieurs blocs de d√©codeur pour produire des √©tats cach√©s finaux. Chaque bloc de d√©codeur utilise une couche d'*attention masqu√©e*, ce qui signifie que GPT-2 ne peut pas se concentrer sur les tokens futurs et est uniquement autoris√© √† se focaliser sur les tokens √† gauche dans le texte. Cela diff√®re du token [`mask`] de BERT, car ici, dans l'attention masqu√©e, un masque d'attention est utilis√© pour attribuer un score de `0` aux tokens futurs.

2. La sortie du d√©codeur est ensuite envoy√©e √† une t√™te de mod√©lisation du langage, qui effectue une transformation lin√©aire pour convertir les √©tats cach√©s en logits. L'√©tiquette est le token suivant dans la s√©quence, obtenue en d√©calant les logits vers la droite d'une position. La perte d'entropie crois√©e est calcul√©e entre les logits d√©cal√©s et les √©tiquettes pour d√©terminer le token suivant le plus probable.

L'objectif de pr√©entra√Ænement de GPT-2 est bas√© sur la [mod√©lisation du langage causale](glossary#causal-language-modeling), qui consiste √† pr√©dire le mot suivant dans une s√©quence. Cette approche rend GPT-2 particuli√®rement efficace pour les t√¢ches de g√©n√©ration de texte.

Pr√™t √† essayer la g√©n√©ration de texte ? Consultez notre [guide complet sur la mod√©lisation du langage causale](tasks/language_modeling#causal-language-modeling) pour d√©couvrir comment effectuer un r√©glagle fin (*fine-tuning*) de DistilGPT-2 et l'utiliser pour l'inf√©rence !

<Tip>

Pour plus d'informations sur la g√©n√©ration de texte, consultez le guide sur les [strat√©gies de g√©n√©ration de texte](generation_strategies) !

</Tip>

### R√©sum√© de texte

Les mod√®les encodeur-d√©codeur tels que [BART](model_doc/bart) et [T5](model_doc/t5) sont con√ßus pour les t√¢ches de r√©sum√© en mode s√©quence-√†-s√©quence. Dans cette section, nous expliquerons le fonctionnement de BART, puis vous aurez l'occasion de d√©couvrir comment r√©aliser un r√©glagle fin (*fine-tuning*) de T5.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bart_architecture.png"/>
</div>

1. L'architecture de l'encodeur de BART est tr√®s similaire √† celle de BERT, acceptant des embeddings de tokens et des embeddings positionnels du texte. BART est pr√©entra√Æn√© en corrompant l'entr√©e et en la reconstruisant avec le d√©codeur. Contrairement √† d'autres encodeurs utilisant des strat√©gies de corruption sp√©cifiques, BART peut appliquer divers types de corruption, parmi lesquelles la strat√©gie de *text infilling* est la plus efficace. Dans le text infilling, plusieurs segments de texte sont remplac√©s par un **seul** token [`mask`]. Cette approche est cruciale car elle force le mod√®le √† pr√©dire les tokens masqu√©s et √† estimer le nombre de tokens manquants. Les embeddings d'entr√©e et les spans masqu√©s sont pass√©s √† l'encodeur pour produire des √©tats cach√©s finaux. Contrairement √† BERT, BART ne comporte pas de r√©seau feedforward final pour pr√©dire un mot.

2. La sortie de l'encodeur est transmise au d√©codeur, qui doit pr√©dire √† la fois les tokens masqu√©s et les tokens non corrompus. Ce contexte suppl√©mentaire aide le d√©codeur √† restaurer le texte original. La sortie du d√©codeur est ensuite envoy√©e √† une t√™te de mod√©lisation du langage, qui transforme les √©tats cach√©s en logits. La perte d'entropie crois√©e est calcul√©e entre les logits et l'√©tiquette, qui est simplement le token d√©cal√© vers la droite.

Pr√™t √† essayer le r√©sum√© ? Consultez notre [guide complet sur le r√©sum√©](tasks/summarization) pour apprendre √† effectuer un r√©glage fin (*fine-tuning*) de T5 et l'utiliser pour l'inf√©rence !

<Tip>

Pour plus d'informations sur la g√©n√©ration de texte, consultez le guide sur les [strat√©gies de g√©n√©ration de texte](generation_strategies) !

</Tip>

### Traduction

La traduction est un autre exemple de t√¢che s√©quence-√†-s√©quence, ce qui signifie qu'un mod√®le encodeur-d√©codeur comme [BART](model_doc/bart) ou [T5](model_doc/t5) peut √™tre utilis√© pour cette t√¢che. Nous expliquerons ici comment BART fonctionne pour la traduction, puis vous pourrez d√©couvrir comment affiner T5.

BART adapte le mod√®le √† la traduction en ajoutant un encodeur s√©par√©, initialis√© al√©atoirement, pour mapper la langue source en une entr√©e qui peut √™tre d√©cod√©e dans la langue cible. Les embeddings de cet encodeur sont ensuite pass√©s √† l'encodeur pr√©entra√Æn√© au lieu des embeddings de mots originaux. L'encodeur source est entra√Æn√© en mettant √† jour l'encodeur source, les embeddings positionnels et les embeddings d'entr√©e avec la perte d'entropie crois√©e provenant de la sortie du mod√®le. Les param√®tres du mod√®le sont fig√©s lors de cette premi√®re √©tape, et tous les param√®tres du mod√®le sont entra√Æn√©s ensemble lors de la deuxi√®me √©tape.

BART a √©t√© suivi par une version multilingue, mBART, qui est sp√©cifiquement con√ßue pour la traduction et pr√©entra√Æn√©e sur de nombreuses langues diff√©rentes.

Pr√™t √† essayer la traduction ? Consultez notre [guide complet sur la traduction](tasks/translation) pour apprendre √† affiner T5 et l'utiliser pour l'inf√©rence !

<Tip>

Pour plus d'informations sur la g√©n√©ration de texte, consultez le guide sur les [strat√©gies de g√©n√©ration de texte](generation_strategies) !

</Tip>
