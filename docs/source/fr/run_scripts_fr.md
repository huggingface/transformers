<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Entra√Æner avec un script

En plus des [notebooks](./notebooks) de ü§ó Transformers, il existe √©galement des exemples de scripts d√©montrant comment entra√Æner un mod√®le pour une t√¢che avec [PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch), [TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow) ou [JAX/Flax](https://github.com/huggingface/transformers/tree/main/examples/flax).


Vous trouverez √©galement des scripts que nous avons utilis√© dans nos [projets de recherche](https://github.com/huggingface/transformers/tree/main/examples/research_projects) et des [exemples "legacy"](https://github.com/huggingface/transformers/tree/main/examples/legacy) qui sont des contributions de la communaut√©. Ces scripts ne sont pas activement maintenus et n√©cessitent une version sp√©cifique de ü§ó Transformers qui sera probablement incompatible avec la derni√®re version de la librairie.

Les exemples de scripts ne sont pas cens√©s fonctionner imm√©diatement pour chaque probl√®me, et il se peut que vous ayez besoin d'adapter le script au probl√®me que vous essayez de r√©soudre. Pour vous aider dans cette t√¢che, la plupart des scripts exposent enti√®rement la mani√®re dont les donn√©es sont pr√©trait√©es, vous permettant de les modifier selon vos besoins.

Pour toute fonctionnalit√© que vous souhaitez impl√©menter dans un script d'exemple, veuillez en discuter sur le [forum](https://discuss.huggingface.co/) ou dans une [issue](https://github.com/huggingface/transformers/issues) avant de soumettre une Pull Request. Bien que nous acceptions les corrections de bugs, il est peu probable que nous fusionnions une Pull Request (op√©ration "merge" dans Git) ajoutant plus de fonctionnalit√©s au d√©triment de la lisibilit√©.

Ce guide vous montrera comment ex√©cuter un script d'entra√Ænement de r√©sum√© en exemple avec [PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization) et [TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/summarization). Tous les exemples sont cens√©s fonctionner avec les deux frameworks, sauf indication contraire.

## Configuration

Pour ex√©cuter avec succ√®s la derni√®re version des scripts d'exemple, vous devez **installer ü§ó Transformers √† partir du code source** dans un nouvel environnement virtuel :

```bash
git clone https://github.com/huggingface/transformers
cd transformers
pip install .
```

Pour les versions plus anciennes des exemples de scripts, cliquez sur le bouton ci-dessous :

<details>
  <summary>Exemples pour les anciennes versions de Transformers ü§ó</summary>
	<ul>
		<li><a href="https://github.com/huggingface/transformers/tree/v4.5.1/examples">v4.5.1</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v4.4.2/examples">v4.4.2</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v4.3.3/examples">v4.3.3</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v4.2.2/examples">v4.2.2</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v4.1.1/examples">v4.1.1</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v4.0.1/examples">v4.0.1</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v3.5.1/examples">v3.5.1</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v3.4.0/examples">v3.4.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v3.3.1/examples">v3.3.1</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v3.2.0/examples">v3.2.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v3.1.0/examples">v3.1.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v3.0.2/examples">v3.0.2</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.11.0/examples">v2.11.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.10.0/examples">v2.10.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.9.1/examples">v2.9.1</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.8.0/examples">v2.8.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.7.0/examples">v2.7.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.6.0/examples">v2.6.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.5.1/examples">v2.5.1</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.4.0/examples">v2.4.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.3.0/examples">v2.3.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.2.0/examples">v2.2.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.1.0/examples">v2.1.1</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.0.0/examples">v2.0.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v1.2.0/examples">v1.2.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v1.1.0/examples">v1.1.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v1.0.0/examples">v1.0.0</a></li>
	</ul>
</details>

Ensuite, changez votre clone actuel de  ü§ó Transformers pour une version sp√©cifique, comme par exemple v3.5.1 :

```bash
git checkout tags/v3.5.1
```

Apr√®s avoir configur√© la bonne version de la librairie, acc√©dez au dossier d'exemple de votre choix et installez les pr√©requis sp√©cifiques √† l'exemple.

```bash
pip install -r requirements.txt
```

## Ex√©cuter un script

<frameworkcontent>
<pt>

Le script d'exemple t√©l√©charge et pr√©traite un jeu de donn√©es √† partir de la biblioth√®que ü§ó [Datasets](https://huggingface.co/docs/datasets/). Ensuite, le script affine un ensemble de donn√©es √† l'aide de [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) sur une architecture qui prend en charge la t√¢che de r√©sum√©. L'exemple suivant montre comment ajuster le mod√®le [T5-small](https://huggingface.co/google-t5/t5-small) sur les donn√©es [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail). Le mod√®le T5 n√©cessite un argument suppl√©mentaire `source_prefix` en raison de la fa√ßon dont il a √©t√© entra√Æn√©. Cette invite permet √† T5 de savoir qu'il s'agit d'une t√¢che de r√©sum√©.

```bash
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```
</pt>
<tf>

Le script d'exemple t√©l√©charge et pr√©traite un jeu de donn√©es √† partir de la biblioth√®que  ü§ó [Datasets](https://huggingface.co/docs/datasets/). Ensuite, le script ajuste un mod√®le √† l'aide de Keras sur une architecture qui prend en charge la t√¢che de r√©sum√©. L'exemple suivant montre comment ajuster le mod√®le [T5-small](https://huggingface.co/google-t5/t5-small) sur le jeu de donn√©es [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail). Le mod√®le T5 n√©cessite un argument suppl√©mentaire source_prefix en raison de la fa√ßon dont il a √©t√© entra√Æn√©. Cette invite permet √† T5 de savoir qu'il s'agit d'une t√¢che de r√©sum√©.

```bash
python examples/tensorflow/summarization/run_summarization.py  \
    --model_name_or_path google-t5/t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --output_dir /tmp/tst-summarization  \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 16 \
    --num_train_epochs 3 \
    --do_train \
    --do_eval
```
</tf>
</frameworkcontent>

## Entra√Ænement distribu√© et pr√©cision mixte

[Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) prend en charge l'entra√Ænement distribu√© et la pr√©cision mixte, ce qui signifie que vous pouvez √©galement les utiliser dans un script. Pour activer ces deux fonctionnalit√©s :

- Ajoutez l'argument fp16 pour activer la pr√©cision mixte.
- D√©finissez le nombre de GPU √† utiliser avec l'argument `nproc_per_node`.

```bash
torchrun \
    --nproc_per_node 8 pytorch/summarization/run_summarization.py \
    --fp16 \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

Les scripts TensorFlow utilisent une Strategie en Miroir [`MirroredStrategy`](https://www.tensorflow.org/guide/distributed_training#mirroredstrategy) pour l'entra√Ænement distribu√©, et vous n'avez pas besoin d'ajouter d'arguments suppl√©mentaires au script d'entra√Ænement. Le script TensorFlow utilisera plusieurs GPU par d√©faut s'ils sont disponibles.

## Ex√©cuter un script sur un TPU 

<frameworkcontent>
<pt>

Les unit√©s de traitement de tenseurs (UTT) (TPU) sont sp√©cialement con√ßues pour acc√©l√©rer les performances. PyTorch prend en charge les TPU avec le compilateur de deep learning [XLA](https://www.tensorflow.org/xla). Pour utiliser un TPU, lancez le script xla_spawn.py et utilisez l'argument num_cores pour d√©finir le nombre de c≈ìurs TPU que vous souhaitez utilise

```bash
python xla_spawn.py --num_cores 8 \
    summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```
</pt>
<tf>
Les scripts TensorFlow utilisent une [`TPUStrategy`](https://www.tensorflow.org/guide/distributed_training#tpustrategy) pour l'entra√Ænement sur TPU. Pour utiliser un TPU, passez le nom de la ressource TPU √† l'argument tpu.

```bash
python run_summarization.py  \
    --tpu name_of_tpu_resource \
    --model_name_or_path google-t5/t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --output_dir /tmp/tst-summarization  \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 16 \
    --num_train_epochs 3 \
    --do_train \
    --do_eval
```
</tf>
</frameworkcontent>

## Ex√©cuter un script avec ü§ó Accelerate 

ü§ó [Accelerate](https://huggingface.co/docs/accelerate) est une biblioth√®que uniquement pour PyTorch qui offre une m√©thode unifi√©e pour entra√Æner un mod√®le sur plusieurs types de configurations (CPU uniquement, plusieurs GPU, TPU) tout en maintenant une visibilit√© compl√®te sur la boucle d'entra√Ænement PyTorch. Assurez-vous que vous avez install√© ü§ó Accelerate si ce n'est pas d√©j√† le cas.

> Note : Comme Accelerate est en d√©veloppement rapide, la version git d'accelerate doit √™tre install√©e pour ex√©cuter les scripts.
```bash
pip install git+https://github.com/huggingface/accelerate
```

Au lieu du script `run_summarization.py`, vous devez utiliser le script `run_summarization_no_trainer.py`. Les scripts compatibles avec ü§ó Accelerate auront un fichier `task_no_trainer.py` dans le dossier. Commencez par ex√©cuter la commande suivante pour cr√©er et enregistrer un fichier de configuration.

```bash
accelerate config
```

Testez votre configuration pour vous assurer qu'elle est correctement configur√©e :

```bash
accelerate test
```

Maintenant, vous √™tes pr√™t √† lancer l'entra√Ænement :

```bash
accelerate launch run_summarization_no_trainer.py \
    --model_name_or_path google-t5/t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir ~/tmp/tst-summarization
```

## Utiliser un jeu de donn√©es personnalis√© 

Le script de r√©sum√© prend en charge les jeux de donn√©es personnalis√©s tant qu'ils sont au format CSV ou JSON Line. Lorsque vous utilisez votre propre jeu de donn√©es, vous devez sp√©cifier plusieurs arguments suppl√©mentaires :

- `train_file` et `validation_file` sp√©cifient le chemin vers vos fichiers d'entra√Ænement et de validation.
- `text_column` est le texte d'entr√©e √† r√©sumer.
- `summary_column` est le texte cible √† produire.

Un exemple de script de r√©sum√© utilisant un ensemble de donn√©es personnalis√© ressemblerait √† ceci :

```bash
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --train_file path_to_csv_or_jsonlines_file \
    --validation_file path_to_csv_or_jsonlines_file \
    --text_column text_column_name \
    --summary_column summary_column_name \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --overwrite_output_dir \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --predict_with_generate
```

## Tester un script
Il est souvent judicieux d'ex√©cuter votre script sur un plus petit nombre d'exemples de jeu de donn√©es pour s'assurer que tout fonctionne comme pr√©vu avant de s'engager sur un jeu de donn√©es complet qui pourrait prendre des heures √† traiter. Utilisez les arguments suivants pour tronquer le jeu de donn√©es √† un nombre maximal d'√©chantillons :

- `max_train_samples`
- `max_eval_samples`
- `max_predict_samples`

```bash
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --max_train_samples 50 \
    --max_eval_samples 50 \
    --max_predict_samples 50 \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

Tous les scripts d'exemple ne prennent pas en charge l'argument `max_predict_samples`. Si vous n'√™tes pas s√ªr que votre script prenne en charge cet argument, ajoutez l'argument `-h` pour v√©rifier.

```bash
examples/pytorch/summarization/run_summarization.py -h
```

## Reprendre l'entra√Ænement √† partir d'un point de contr√¥le

Une autre option utile est de reprendre l'entra√Ænement √† partir d'un point de contr√¥le pr√©c√©dent. Cela vous permettra de reprendre l√† o√π vous vous √©tiez arr√™t√© sans recommencer si votre entra√Ænement est interrompu. Il existe deux m√©thodes pour reprendre l'entra√Ænement √† partir d'un point de contr√¥le.

La premi√®re m√©thode utilise l'argument `output_dir previous_output_dir` pour reprendre l'entra√Ænement √† partir du dernier point de contr√¥le stock√© dans `output_dir`. Dans ce cas, vous devez supprimer l'argument `overwrite_output_dir`.

```bash
python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --output_dir previous_output_dir \
    --predict_with_generate
```

La seconde m√©thode utilise l'argument `resume_from_checkpoint path_to_specific_checkpoint` pour reprendre l'entra√Ænement √† partir d'un dossier de point de contr√¥le sp√©cifique.

```bash
python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --resume_from_checkpoint path_to_specific_checkpoint \
    --predict_with_generate
```

## Partage ton mod√®le

Tous les scripts peuvent t√©l√©charger votre mod√®le final sur le Model Hub. Assurez-vous que vous √™tes connect√© √† Hugging Face avant de commencer :

```bash
huggingface-cli login
```

Ensuite, ajoutez l'argument `push_to_hub` au script. Cet argument cr√©era un d√©p√¥t avec votre nom d'utilisateur Hugging Face et le nom du dossier sp√©cifi√© dans `output_dir`.


Pour donner un nom sp√©cifique √† votre d√©p√¥t, utilisez l'argument `push_to_hub_model_id` pour l'ajouter. Le d√©p√¥t sera automatiquement list√© sous votre namespace. 

L'exemple suivant montre comment t√©l√©charger un mod√®le avec un nom de d√©p√¥t sp√©cifique :

```bash
python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --push_to_hub \
    --push_to_hub_model_id finetuned-t5-cnn_dailymail \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```