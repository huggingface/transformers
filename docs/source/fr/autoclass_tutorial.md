<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Chargement d'instances pr√©-entra√Æn√©es avec une AutoClass

Avec autant d'architectures Transformer diff√©rentes, il peut √™tre difficile d'en cr√©er une pour votre ensemble de poids (aussi appel√©s "weights" ou "checkpoint" en anglais). Dans l'id√©e de cr√©er une librairie facile, simple et flexible √† utiliser, ü§ó Transformers fournit une `AutoClass` qui inf√®re et charge automatiquement l'architecture correcte √† partir d'un ensemble de poids donn√©. La fonction `from_pretrained()` vous permet de charger rapidement un mod√®le pr√©-entra√Æn√© pour n'importe quelle architecture afin que vous n'ayez pas √† consacrer du temps et des ressources √† l'entra√Ænement d'un mod√®le √† partir de z√©ro. Produire un tel code ind√©pendant d'un ensemble de poids signifie que si votre code fonctionne pour un ensemble de poids, il fonctionnera avec un autre ensemble - tant qu'il a √©t√© entra√Æn√© pour une t√¢che similaire - m√™me si l'architecture est diff√©rente.

<Tip>

Rappel, l'architecture fait r√©f√©rence au squelette du mod√®le et l'ensemble de poids contient les poids pour une architecture donn√©e. Par exemple, [BERT](https://huggingface.co/google-bert/bert-base-uncased) est une architecture, tandis que `google-bert/bert-base-uncased` est un ensemble de poids. Le terme mod√®le est g√©n√©ral et peut signifier soit architecture soit ensemble de poids.

</Tip>

Dans ce tutoriel, vous apprendrez √†:

  * Charger un tokenizer pr√©-entra√Æn√©.
  * Charger un processeur d'image pr√©-entra√Æn√©.
  * Charger un extracteur de caract√©ristiques pr√©-entra√Æn√©.
  * Charger un processeur pr√©-entra√Æn√©.
  * Charger un mod√®le pr√©-entra√Æn√©.

## AutoTokenizer

Quasiment toutes les t√¢ches de traitement du langage (NLP) commencent avec un tokenizer. Un tokenizer convertit votre texte initial dans un format qui peut √™tre trait√© par le mod√®le.

Chargez un tokenizer avec [`AutoTokenizer.from_pretrained`]:

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
```

Puis, transformez votre texte initial comme montr√© ci-dessous:

```py
>>> sequence = "In a hole in the ground there lived a hobbit."
>>> print(tokenizer(sequence))
{'input_ids': [101, 1999, 1037, 4920, 1999, 1996, 2598, 2045, 2973, 1037, 7570, 10322, 4183, 1012, 102], 
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

## AutoImageProcessor

Pour les t√¢ches de vision, un processeur d'image traite l'image pour la formater correctment.

```py
>>> from transformers import AutoImageProcessor

>>> image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
```

## AutoBackbone

<div style="text-align: center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Swin%20Stages.png">
    <figcaption class="mt-2 text-center text-sm text-gray-500">Un backbone Swin avec plusieurs √©tapes pour produire une carte de caract√©ristiques.</figcaption>
</div>

[`AutoBackbone`] vous permet d'utiliser des mod√®les pr√©-entra√Æn√©s comme backbones pour obtenir des cartes de caract√©ristiques √† partir de diff√©rentes √©tapes du backbone. Vous devez sp√©cifier l'un des param√®tres suivants dans [`~PretrainedConfig.from_pretrained`] :

* `out_indices` est l'index de la couche dont vous souhaitez obtenir la carte de caract√©ristiques
* `out_features` est le nom de la couche dont vous souhaitez obtenir la carte de caract√©ristiques

Ces param√®tres peuvent √™tre utilis√©s de mani√®re interchangeable, mais si vous utilisez les deux, assurez-vous qu'ils sont align√©s l'un avec l'autre ! Si vous ne passez aucun de ces param√®tres, le backbone renvoie la carte de caract√©ristiques de la derni√®re couche.

<div style="text-align: center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Swin%20Stage%201.png">
    <figcaption class="mt-2 text-center text-sm text-gray-500">Une carte de caract√©ristiques de la premi√®re √©tape du backbone. La partition de patch fait r√©f√©rence √† la tige du mod√®le.</figcaption>
</div>

Par exemple, dans le diagramme ci-dessus, pour renvoyer la carte de caract√©ristiques de la premi√®re √©tape du backbone Swin, vous pouvez d√©finir `out_indices=(1,)` :

```py
>>> from transformers import AutoImageProcessor, AutoBackbone
>>> import torch
>>> from PIL import Image
>>> import requests
>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> processor = AutoImageProcessor.from_pretrained("microsoft/swin-tiny-patch4-window7-224")
>>> model = AutoBackbone.from_pretrained("microsoft/swin-tiny-patch4-window7-224", out_indices=(1,))

>>> inputs = processor(image, return_tensors="pt")
>>> outputs = model(**inputs)
>>> feature_maps = outputs.feature_maps
```

Vous pouvez maintenant acc√©der √† l'objet `feature_maps` de la premi√®re √©tape du backbone :


```py
>>> list(feature_maps[0].shape)
[1, 96, 56, 56]
```

## AutoFeatureExtractor

Pour les t√¢ches audio, un extracteur de caract√©ristiques (aussi appel√©s "features" en anglais) traite le signal audio pour le formater correctement.

Chargez un extracteur de caract√©ristiques avec [`AutoFeatureExtractor.from_pretrained`]:

```py
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained(
...     "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition"
... )
```

## AutoProcessor

Les t√¢ches multimodales n√©cessitent un processeur qui combine deux types d'outils de pr√©traitement. Par exemple, le mod√®le [LayoutLMV2](model_doc/layoutlmv2) n√©cessite un processeur d'image pour traiter les images et un tokenizer pour traiter le texte ; un processeur combine les deux.

Chargez un processeur avec [`AutoProcessor.from_pretrained`]:

```py
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("microsoft/layoutlmv2-base-uncased")
```

## AutoModel

Enfin, les classes `AutoModelFor` vous permettent de charger un mod√®le pr√©-entra√Æn√© pour une t√¢che donn√©e (voir [ici](model_doc/auto) pour une liste compl√®te des t√¢ches disponibles). Par exemple, chargez un mod√®le pour la classification de s√©quence avec [`AutoModelForSequenceClassification.from_pretrained`]:

```py
>>> from transformers import AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

R√©utilisez facilement le m√™me ensemble de poids pour charger une architecture pour une t√¢che diff√©rente :

```py
>>> from transformers import AutoModelForTokenClassification

>>> model = AutoModelForTokenClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

<Tip warning={true}>

Pour les mod√®les PyTorch, la fonction `from_pretrained()` utilise `torch.load()` qui utilise `pickle` en interne et est connu pour √™tre non s√©curis√©. En g√©n√©ral, ne chargez jamais un mod√®le qui pourrait provenir d'une source non fiable, ou qui pourrait avoir √©t√© alt√©r√©. Ce risque de s√©curit√© est partiellement att√©nu√© pour les mod√®les h√©berg√©s publiquement sur le Hugging Face Hub, qui sont [scann√©s pour les logiciels malveillants](https://huggingface.co/docs/hub/security-malware) √† chaque modification. Consultez la [documentation du Hub](https://huggingface.co/docs/hub/security) pour conna√Ætre les meilleures pratiques comme la [v√©rification des modifications sign√©es](https://huggingface.co/docs/hub/security-gpg#signing-commits-with-gpg) avec GPG.

Les points de contr√¥le TensorFlow et Flax ne sont pas concern√©s, et peuvent √™tre charg√©s dans des architectures PyTorch en utilisant les arguments `from_tf` et `from_flax` de la fonction `from_pretrained` pour contourner ce probl√®me.

</Tip>

En g√©n√©ral, nous recommandons d'utiliser les classes `AutoTokenizer` et `AutoModelFor` pour charger des instances pr√©-entra√Æn√©es de tokenizers et mod√®les respectivement. Cela vous permettra de charger la bonne architecture √† chaque fois. Dans le prochain [tutoriel](preprocessing), vous apprenez √† utiliser un tokenizer, processeur d'image, extracteur de caract√©ristiques et processeur pour pr√©-traiter un jeu de donn√©es pour le fine-tuning.
