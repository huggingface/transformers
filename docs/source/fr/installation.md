<!---
Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

# Installation

Installez ü§ó Transformers pour n'importe quelle librairie d'apprentissage profond avec laquelle vous avez l'habitude de travaillez, configurez votre cache et configurez ü§ó Transformers pour un usage hors ligne (facultatif).

ü§ó Transformers est test√© avec Python 3.6+, PyTorch 1.1.0+, TensorFlow 2.0+ et Flax.
Consulter les instructions d'installation ci-dessous pour la librairie d'apprentissage profond que vous utilisez:

  * Instructions d'installation pour [PyTorch](https://pytorch.org/get-started/locally/).
  * Instructions d'installation pour [TensorFlow 2.0](https://www.tensorflow.org/install/pip).
  * Instructions d'installation pour [Flax](https://flax.readthedocs.io/en/latest/).

## Installation avec pip

Vous devriez installer ü§ó Transformers dans un [environnement virtuel](https://docs.python.org/3/library/venv.html).
Si vous n'√™tes pas √† l'aise avec les environnements virtuels, consultez ce [guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).
Utiliser un environnement virtuel permet de facilement g√©rer diff√©rents projets et d'√©viter des erreurs de compatibilit√© entre les diff√©rentes d√©pendances.

Commencez par cr√©er un environnement virtuel dans l'espace de travail de votre projet :

```bash
python -m venv .env
```

Activez l'environnement virtuel. Sur Linux ou MacOs :

```bash
source .env/bin/activate
```

Activez l'environnement virtuel sur Windows :

```bash
.env/Scripts/activate
```

Maintenant, ü§ó Transformers peut √™tre install√© avec la commande suivante :

```bash
pip install transformers
```

Pour une utilisation avec CPU seulement, ü§ó Transformers et la librairie d'apprentissage profond de votre choix peuvent √™tre install√©s en une seule ligne.
Par exemple, installez ü§ó Transformers et PyTorch avec la commande suivante :

```bash
pip install 'transformers[torch]'
```

ü§ó Transformers et TensorFlow 2.0 :

```bash
pip install 'transformers[tf-cpu]'
```

<Tip warning={true}>

Pour les architectures mac M1 / ARM

Vous devez installer les outils suivants avant d'installer TensorFLow 2.0

```bash
brew install cmake
brew install pkg-config
```

</Tip>

ü§ó Transformers et Flax :

```bash
pip install 'transformers[flax]'
```

V√©rifiez que ü§ó Transformers a bien √©t√© install√© avec la commande suivante. La commande va t√©l√©charger un mod√®le pr√©-entra√Æn√© :

```bash
python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"
```

Le label et score sont ensuite affich√©s :

```bash
[{'label': 'POSITIVE', 'score': 0.9998704791069031}]
```

## Installation depuis le code source

Installez ü§ó Transformers depuis le code source avec la commande suivante :

```bash
pip install git+https://github.com/huggingface/transformers
```

Cette commande installe la version depuis la branche `main` au lieu de la derni√®re version stable. La version de la branche `main` est utile pour avoir les derniers d√©veloppements. Par exemple, si un bug a √©t√© r√©solu depuis la derni√®re version stable mais n'a pas encore √©t√© publi√© officiellement. Cependant, cela veut aussi dire que la version de la branche `main` n'est pas toujours stable. Nous nous effor√ßons de maintenir la version de la branche `main` op√©rationnelle, et la plupart des probl√®mes sont g√©n√©ralement r√©solus en l'espace de quelques heures ou d'un jour. Si vous recontrez un probl√®me, n'h√©sitez pas √† cr√©er une [Issue](https://github.com/huggingface/transformers/issues) pour que l'on puisse trouver une solution au plus vite !

V√©rifiez que ü§ó Transformers a bien √©t√© install√© avec la commande suivante :

```bash
python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))"
```

## Installation modifiable

Vous aurez besoin d'une installation modifiable si vous le souhaitez :

  * Utiliser la version de la branche `main` du code source.
  * Contribuer √† ü§ó Transformers et vouler tester vos modifications du code source.

Clonez le projet et installez ü§ó Transformers avec les commandes suivantes :

```bash
git clone https://github.com/huggingface/transformers.git
cd transformers
pip install -e .
```

Ces commandes cr√©ent des liens entre le dossier o√π le projet a √©t√© clon√© et les chemins de vos librairies Python. Python regardera maintenant dans le dossier que vous avez clon√© en plus des dossiers o√π sont install√©es vos autres librairies. Par exemple, si vos librairies Python sont install√©es dans `~/anaconda3/envs/main/lib/python3.7/site-packages/`, Python cherchera aussi dans le dossier o√π vous avez clon√© : `~/transformers/`.

<Tip warning={true}>

Vous devez garder le dossier `transformers` si vous voulez continuer d'utiliser la librairie.

</Tip>

Maintenant, vous pouvez facilement mettre √† jour votre clone avec la derni√®re version de ü§ó Transformers en utilisant la commande suivante :

```bash
cd ~/transformers/
git pull
```

Votre environnement Python utilisera la version de la branche `main` lors de la prochaine ex√©cution.

## Installation avec conda

Installation via le canal `conda-forge` de conda :

```bash
conda install conda-forge::transformers
```

## Configuration du cache

Les mod√®les pr√©-entra√Æn√©s sont t√©l√©charg√©s et mis en cache localement dans le dossier suivant : `~/.cache/huggingface/hub`. C'est le dossier par d√©faut donn√© par la variable d'environnement `TRANSFORMERS_CACHE`. Sur Windows, le dossier par d√©faut est `C:\Users\nom_utilisateur\.cache\huggingface\hub`. Vous pouvez modifier les variables d'environnement indiqu√©es ci-dessous - par ordre de priorit√© - pour sp√©cifier un dossier de cache diff√©rent :

1. Variable d'environnement (par d√©faut) : `HF_HUB_CACHE` ou `TRANSFORMERS_CACHE`.
2. Variable d'environnement : `HF_HOME`.
3. Variable d'environnement : `XDG_CACHE_HOME` + `/huggingface`.

<Tip>

ü§ó Transformers utilisera les variables d'environnement `PYTORCH_TRANSFORMERS_CACHE` ou `PYTORCH_PRETRAINED_BERT_CACHE` si vous utilisez une version pr√©c√©dente de cette librairie et avez d√©fini ces variables d'environnement, sauf si vous sp√©cifiez la variable d'environnement `TRANSFORMERS_CACHE`.

</Tip>

## Mode hors ligne

ü§ó Transformers peut fonctionner dans un environnement cloisonn√© ou hors ligne en n'utilisant que des fichiers locaux. D√©finissez la variable d'environnement `HF_HUB_OFFLINE=1` pour activer ce mode.

<Tip>

Ajoutez [ü§ó Datasets](https://huggingface.co/docs/datasets/) √† votre processus d'entra√Ænement hors ligne en d√©finissant la variable d'environnement `HF_DATASETS_OFFLINE=1`.

</Tip>

```bash
HF_DATASETS_OFFLINE=1 HF_HUB_OFFLINE=1 \
python examples/pytorch/translation/run_translation.py --model_name_or_path google-t5/t5-small --dataset_name wmt16 --dataset_config ro-en ...
```

Le script devrait maintenant s'ex√©cuter sans rester en attente ou attendre une expiration, car il n'essaiera pas de t√©l√©charger des mod√®le sur le Hub.

Vous pouvez aussi √©viter de t√©l√©charger un mod√®le √† chaque appel de la fonction [`~PreTrainedModel.from_pretrained`] en utilisant le param√®tre [local_files_only]. Seuls les fichiers locaux sont charg√©s lorsque ce param√®tre est activ√© (c.-√†-d. `local_files_only=True`) :

```py
from transformers import T5Model

model = T5Model.from_pretrained("./path/to/local/directory", local_files_only=True)
```

### R√©cup√©rer des mod√®les et des tokenizers pour une utilisation hors ligne

Une autre option pour utiliser ü§ó Transformers hors ligne est de t√©l√©charger les fichiers √† l'avance, puis d'utiliser les chemins locaux lorsque vous en avez besoin en mode hors ligne. Il existe trois fa√ßons de faire cela :

  * T√©l√©chargez un fichier via l'interface utilisateur sur le [Model Hub](https://huggingface.co/models) en cliquant sur l'ic√¥ne ‚Üì.

    ![download-icon](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/download-icon.png)

  * Utilisez les fonctions [`PreTrainedModel.from_pretrained`] et [`PreTrainedModel.save_pretrained`] :

    1. T√©l√©chargez vos fichiers √† l'avance avec [`PreTrainedModel.from_pretrained`]:

    ```py
    >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

    >>> tokenizer = AutoTokenizer.from_pretrained("bigscience/T0_3B")
    >>> model = AutoModelForSeq2SeqLM.from_pretrained("bigscience/T0_3B")
    ```

    2. Sauvegardez les fichiers dans un dossier de votre choix avec [`PreTrainedModel.save_pretrained`]:

    ```py
    >>> tokenizer.save_pretrained("./your/path/bigscience_t0")
    >>> model.save_pretrained("./your/path/bigscience_t0")
    ```

    3. Maintenant, lorsque vous √™tes hors ligne, rechargez vos fichiers avec [`PreTrainedModel.from_pretrained`] depuis le dossier o√π vous les avez sauvegard√©s :

    ```py
    >>> tokenizer = AutoTokenizer.from_pretrained("./your/path/bigscience_t0")
    >>> model = AutoModel.from_pretrained("./your/path/bigscience_t0")
    ```

  * T√©l√©chargez des fichiers de mani√®re automatique avec la librairie [huggingface_hub](https://github.com/huggingface/huggingface_hub/tree/main/src/huggingface_hub) :

    1. Installez la librairie `huggingface_hub`  dans votre environnement virtuel :

    ```bash
    python -m pip install huggingface_hub
    ```

    2. Utilisez la fonction [`hf_hub_download`](https://huggingface.co/docs/hub/adding-a-library#download-files-from-the-hub) pour t√©l√©charger un fichier vers un chemin de votre choix.  Par exemple, la commande suivante t√©l√©charge le fichier `config.json` du mod√®le [T0](https://huggingface.co/bigscience/T0_3B) vers le chemin de votre choix :

    ```py
    >>> from huggingface_hub import hf_hub_download

    >>> hf_hub_download(repo_id="bigscience/T0_3B", filename="config.json", cache_dir="./your/path/bigscience_t0")
    ```

Une fois que votre fichier est t√©l√©charg√© et cach√© localement, sp√©cifiez son chemin local pour le charger et l'utiliser :

```py
>>> from transformers import AutoConfig

>>> config = AutoConfig.from_pretrained("./your/path/bigscience_t0/config.json")
```

<Tip>

Consultez la section [How to download files from the Hub (Comment t√©l√©charger des fichiers depuis le Hub)](https://huggingface.co/docs/hub/how-to-downstream) pour plus de d√©tails sur le t√©l√©chargement de fichiers stock√©s sur le Hub.

</Tip>
