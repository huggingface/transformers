<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Visite rapide

[[open-in-colab]]

Soyez op√©rationnel avec ü§ó Transformers ! Que vous soyez un d√©veloppeur ou un utilisateur lambda, cette visite rapide vous aidera √† d√©marrer et vous montrera comment utiliser le [`pipeline`] pour l'inf√©rence, charger un mod√®le pr√©-entra√Æn√© et un pr√©processeur avec une [AutoClass](./model_doc/auto), et entra√Æner rapidement un mod√®le avec PyTorch ou TensorFlow. Si vous √™tes un d√©butant, nous vous recommandons de consulter nos tutoriels ou notre [cours](https://huggingface.co/course/chapter1/1) suivant pour des explications plus approfondies des concepts pr√©sent√©s ici.

Avant de commencer, assurez-vous que vous avez install√© toutes les biblioth√®ques n√©cessaires :

```bash
!pip install transformers datasets
```

Vous aurez aussi besoin d'installer votre biblioth√®que d'apprentissage profond favorite :

<frameworkcontent>
<pt>
```bash
pip install torch
```
</pt>
<tf>
```bash
pip install tensorflow
```
</tf>
</frameworkcontent>

## Pipeline

<Youtube id="tiZFewofSLM"/>

Le [`pipeline`] est le moyen le plus simple d'utiliser un mod√®le pr√©-entra√Æn√© pour l'inf√©rence. Vous pouvez utiliser le [`pipeline`] pr√™t √† l'emploi pour de nombreuses t√¢ches dans diff√©rentes modalit√©s. Consultez le tableau ci-dessous pour conna√Ætre les t√¢ches prises en charge :

| **T√¢che**                     | **Description**                                                                                              | **Modalit√©**        | **Identifiant du pipeline**                   |
|------------------------------|--------------------------------------------------------------------------------------------------------------|----------------------|-----------------------------------------------|
| Classification de texte      | Attribue une cat√©gorie √† une s√©quence de texte donn√©e                                                        | Texte                | pipeline(task="sentiment-analysis")           |
| G√©n√©ration de texte          | G√©n√®re du texte √† partir d'une consigne donn√©e                                                               | Texte                | pipeline(task="text-generation")              |
| Reconnaissance de token nomm√©      | Attribue une cat√©gorie √† chaque token dans une s√©quence (personnes, organisation, localisation, etc.)                            | Texte                | pipeline(task="ner")                          |
| Question r√©ponse             | Extrait une r√©ponse du texte en fonction du contexte et d'une question                                       | Texte                | pipeline(task="question-answering")           |
| Pr√©diction de token masqu√©                    | Pr√©dit correctement le token masqu√© dans une s√©quence                                                               | Texte                | pipeline(task="fill-mask")                    |
| G√©n√©ration de r√©sum√©                | G√©n√®re un r√©sum√© d'une s√©quence de texte donn√©e ou d'un document                                                         | Texte                | pipeline(task="summarization")                |
| Traduction                  | Traduit du texte d'un langage √† un autre                                                                      | Texte                | pipeline(task="translation")                  |
| Classification d'image       | Attribue une cat√©gorie √† une image                                                                           | Image                | pipeline(task="image-classification")         |
| Segmentation d'image           | Attribue une cat√©gorie √† chaque pixel d'une image (supporte la segmentation s√©mantique, panoptique et d'instance) | Image                | pipeline(task="image-segmentation")           |
| D√©tection d'objects             | Pr√©dit les d√©limitations et cat√©gories d'objects dans une image                                                | Image                | pipeline(task="object-detection")             |
| Classification d'audio       | Attribue une cat√©gorie √† un fichier audio                                                                    | Audio                | pipeline(task="audio-classification")         |
| Reconnaissance automatique de la parole | Extrait le discours d'un fichier audio en texte                                                                  | Audio                | pipeline(task="automatic-speech-recognition") |
| Question r√©ponse visuels    | Etant donn√©es une image et une question, r√©pond correctement √† une question sur l'image                                   | Modalit√©s multiples  | pipeline(task="vqa")                          |

Commencez par cr√©er une instance de [`pipeline`] et sp√©cifiez la t√¢che pour laquelle vous souhaitez l'utiliser. Vous pouvez utiliser le [`pipeline`] pour n'importe laquelle des t√¢ches mentionn√©es dans le tableau pr√©c√©dent. Pour obtenir une liste compl√®te des t√¢ches prises en charge, consultez la documentation de l'[API pipeline](./main_classes/pipelines). Dans ce guide, nous utiliserons le [`pipeline`] pour l'analyse des sentiments √† titre d'exemple :

```py
>>> from transformers import pipeline

>>> classifier = pipeline("sentiment-analysis")
```

Le [`pipeline`] t√©l√©charge et stocke en cache un [mod√®le pr√©-entra√Æn√©](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) et un tokenizer par d√©faut pour l'analyse des sentiments. Vous pouvez maintenant utiliser le `classifier` sur le texte de votre choix :

```py
>>> classifier("We are very happy to show you the ü§ó Transformers library.")
[{'label': 'POSITIVE', 'score': 0.9998}]
```

Si vous voulez classifier plus qu'un texte, donnez une liste de textes au [`pipeline`] pour obtenir une liste de dictionnaires en retour :

```py
>>> results = classifier(["We are very happy to show you the ü§ó Transformers library.", "We hope you don't hate it."])
>>> for result in results:
...     print(f"label: {result['label']}, avec le score de: {round(result['score'], 4)}")
label: POSITIVE, avec le score de: 0.9998
label: NEGATIVE, avec le score de: 0.5309
```

Le [`pipeline`] peut aussi it√©rer sur un jeu de donn√©es entier pour n'importe quelle t√¢che. Prenons par exemple la reconnaissance automatique de la parole :

```py
>>> import torch
>>> from transformers import pipeline

>>> speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h")
```

Chargez un jeu de donn√©es audio (voir le ü§ó Datasets [Quick Start](https://huggingface.co/docs/datasets/quickstart#audio) pour plus de d√©tails) sur lequel vous souhaitez it√©rer. Pour cet example, nous chargons le jeu de donn√©es [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) :

```py
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")  # doctest: +IGNORE_RESULT
```

Vous devez vous assurer que le taux d'√©chantillonnage de l'ensemble de donn√©es correspond au taux d'√©chantillonnage sur lequel [`facebook/wav2vec2-base-960h`](https://huggingface.co/facebook/wav2vec2-base-960h) a √©t√© entra√Æn√© :

```py
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))
```

Les fichiers audio sont automatiquement charg√©s et r√©√©chantillonn√©s lors de l'appel de la colonne `"audio"`.
Extrayez les tableaux de formes d'ondes brutes des quatre premiers √©chantillons et passez-les comme une liste au pipeline :

```py
>>> result = speech_recognizer(dataset[:4]["audio"])
>>> print([d["text"] for d in result])
['I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT', "FODING HOW I'D SET UP A JOIN TO HET WITH MY WIFE AND WHERE THE AP MIGHT BE", "I I'D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I'M NOT SEEING THE OPTION TO DO IT ON THE AP SO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AND I'M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS", 'HOW DO I THURN A JOIN A COUNT']
```

Pour les ensembles de donn√©es plus importants o√π les entr√©es sont volumineuses (comme dans les domaines de la parole ou de la vision), utilisez plut√¥t un g√©n√©rateur au lieu d'une liste pour charger toutes les entr√©es en m√©moire. Pour plus d'informations, consultez la documentation de l'[API pipeline](./main_classes/pipelines).

### Utiliser une autre mod√®le et tokenizer dans le pipeline

Le [`pipeline`] peut √™tre utilis√© avec n'importe quel mod√®le du [Hub](https://huggingface.co/models), ce qui permet d'adapter facilement le [`pipeline`] √† d'autres cas d'utilisation. Par exemple, si vous souhaitez un mod√®le capable de traiter du texte fran√ßais, utilisez les filtres du Hub pour trouver un mod√®le appropri√©. Le premier r√©sultat renvoie un [mod√®le BERT](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment) multilingue finetun√© pour l'analyse des sentiments que vous pouvez utiliser pour le texte fran√ßais :

```py
>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
```

<frameworkcontent>
<pt>
Utilisez [`AutoModelForSequenceClassification`] et [`AutoTokenizer`] pour charger le mod√®le pr√©-entra√Æn√© et le tokenizer adapt√© (plus de d√©tails sur une `AutoClass` dans la section suivante) :

```py
>>> from transformers import AutoTokenizer, AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained(model_name)
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```
</pt>
<tf>
Utilisez [`TFAutoModelForSequenceClassification`] et [`AutoTokenizer`] pour charger le mod√®le pr√©-entra√Æn√© et le tokenizer adapt√© (plus de d√©tails sur une `TFAutoClass` dans la section suivante) :

```py
>>> from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

>>> model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```
</tf>
</frameworkcontent>

Specifiez le mod√®le et le tokenizer dans le [`pipeline`], et utilisez le `classifier` sur le texte en fran√ßais :

```py
>>> classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
>>> classifier("Nous sommes tr√®s heureux de vous pr√©senter la biblioth√®que ü§ó Transformers.")
[{'label': '5 stars', 'score': 0.7273}]
```

Si vous ne parvenez pas √† trouver un mod√®le adapt√© √† votre cas d'utilisation, vous devrez finetuner un mod√®le pr√©-entra√Æn√© sur vos donn√©es. Jetez un coup d'≈ìil √† notre [tutoriel sur le finetuning](./training) pour apprendre comment faire. Enfin, apr√®s avoir finetun√© votre mod√®le pr√©-entra√Æn√©, pensez √† [partager](./model_sharing) le mod√®le avec la communaut√© sur le Hub afin de d√©mocratiser l'apprentissage automatique pour tous ! ü§ó

## AutoClass

<Youtube id="AhChOFRegn4"/>

Les classes [`AutoModelForSequenceClassification`] et [`AutoTokenizer`] fonctionnent ensemble pour cr√©er un [`pipeline`] comme celui que vous avez utilis√© ci-dessus. Une [AutoClass](./model_doc/auto) est un raccourci qui r√©cup√®re automatiquement l'architecture d'un mod√®le pr√©-entra√Æn√© √† partir de son nom ou de son emplacement. Il vous suffit de s√©lectionner l'`AutoClass` appropri√©e √† votre t√¢che et la classe de pr√©traitement qui lui est associ√©e. 

Reprenons l'exemple de la section pr√©c√©dente et voyons comment vous pouvez utiliser l'`AutoClass` pour reproduire les r√©sultats du [`pipeline`].

### AutoTokenizer

Un tokenizer est charg√© de pr√©traiter le texte pour en faire un tableau de chiffres qui servira d'entr√©e √† un mod√®le. De nombreuses r√®gles r√©gissent le processus de tokenisation, notamment la mani√®re de diviser un mot et le niveau auquel les mots doivent √™tre divis√©s (pour en savoir plus sur la tokenisation, consultez le [r√©sum√©](./tokenizer_summary)). La chose la plus importante √† retenir est que vous devez instancier un tokenizer avec le m√™me nom de mod√®le pour vous assurer que vous utilisez les m√™mes r√®gles de tokenisation que celles avec lesquelles un mod√®le a √©t√© pr√©-entra√Æn√©.

Chargez un tokenizer avec [`AutoTokenizer`] :

```py
>>> from transformers import AutoTokenizer

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```

Passez votre texte au tokenizer :

```py
>>> encoding = tokenizer("We are very happy to show you the ü§ó Transformers library.")
>>> print(encoding)
{'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

Le tokenizer retourne un dictionnaire contenant :

* [input_ids](./glossary#input-ids): la repr√©sentation num√©rique des tokens.
* [attention_mask](.glossary#attention-mask): indique quels tokens doivent faire l'objet d'une attention particuli√®re (plus particuli√®rement les tokens de remplissage).

Un tokenizer peut √©galement accepter une liste de textes, et remplir et tronquer le texte pour retourner un √©chantillon de longueur uniforme :

<frameworkcontent>
<pt>
```py
>>> pt_batch = tokenizer(
...     ["We are very happy to show you the ü§ó Transformers library.", "We hope you don't hate it."],
...     padding=True,
...     truncation=True,
...     max_length=512,
...     return_tensors="pt",
... )
```
</pt>
<tf>
```py
>>> tf_batch = tokenizer(
...     ["We are very happy to show you the ü§ó Transformers library.", "We hope you don't hate it."],
...     padding=True,
...     truncation=True,
...     max_length=512,
...     return_tensors="tf",
... )
```
</tf>
</frameworkcontent>

<Tip>

Consultez le tutoriel [pr√©traitement](./preprocessing) pour plus de d√©tails sur la tokenisation, et sur la mani√®re d'utiliser un [`AutoImageProcessor`], un [`AutoFeatureExtractor`] et un [`AutoProcessor`] pour pr√©traiter les images, l'audio et les contenus multimodaux.

</Tip>

### AutoModel

<frameworkcontent>
<pt>
ü§ó Transformers fournit un moyen simple et unifi√© de charger des instances pr√©-entra√Æn√©es. Cela signifie que vous pouvez charger un [`AutoModel`] comme vous chargeriez un [`AutoTokenizer`]. La seule diff√©rence est de s√©lectionner l'[`AutoModel`] appropri√© pour la t√¢che. Pour une classification de texte (ou de s√©quence de textes), vous devez charger [`AutoModelForSequenceClassification`] :

```py
>>> from transformers import AutoModelForSequenceClassification

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
```

<Tip>

Voir le [r√©sum√© de la t√¢che](./task_summary) pour v√©rifier si elle est prise en charge par une classe [`AutoModel`].

</Tip>

Maintenant, passez votre √©chantillon d'entr√©es pr√©trait√©es directement au mod√®le. Il vous suffit de d√©compresser le dictionnaire en ajoutant `**` :

```py
>>> pt_outputs = pt_model(**pt_batch)
```

Le mod√®le produit les activations finales dans l'attribut `logits`. Appliquez la fonction softmax aux `logits` pour r√©cup√©rer les probabilit√©s :

```py
>>> from torch import nn

>>> pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
>>> print(pt_predictions)
tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],
        [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)
```
</pt>
<tf>
ü§ó Transformers fournit un moyen simple et unifi√© de charger des instances pr√©-entra√Æn√©s. Cela signifie que vous pouvez charger un [`TFAutoModel`] comme vous chargeriez un [`AutoTokenizer`]. La seule diff√©rence est de s√©lectionner le [`TFAutoModel`] appropri√© pour la t√¢che. Pour une classification de texte (ou de s√©quence de textes), vous devez charger [`TFAutoModelForSequenceClassification`] :

```py
>>> from transformers import TFAutoModelForSequenceClassification

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
```

<Tip>

Voir le [r√©sum√© de la t√¢che](./task_summary) pour v√©rifier si elle est prise en charge par une classe [`AutoModel`].

</Tip>

Passez maintenant votre √©chantillon d'entr√©es pr√©trait√©es directement au mod√®le en passant les cl√©s du dictionnaire directement aux tensors :

```py
>>> tf_outputs = tf_model(tf_batch)
```

Le mod√®le produit les activations finales dans l'attribut `logits`. Appliquez la fonction softmax aux `logits` pour r√©cup√©rer les probabilit√©s :

```py
>>> import tensorflow as tf

>>> tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)
>>> tf_predictions  # doctest: +IGNORE_RESULT
```
</tf>
</frameworkcontent>

<Tip>

Tous les mod√®les ü§ó Transformers (PyTorch ou TensorFlow) produisent les tensors *avant* la fonction d'activation finale (comme softmax) car la fonction d'activation finale est souvent fusionn√©e avec le calcul de la perte. Les structures produites par le mod√®le sont des classes de donn√©es sp√©ciales, de sorte que leurs attributs sont autocompl√©t√©s dans un environnement de d√©veloppement. Les structures produites par le mod√®le se comportent comme un tuple ou un dictionnaire (vous pouvez les indexer avec un entier, une tranche ou une cha√Æne), auquel cas les attributs qui sont None sont ignor√©s.

</Tip>

### Sauvegarder un mod√®le

<frameworkcontent>
<pt>
Une fois que votre mod√®le est finetun√©, vous pouvez le sauvegarder avec son tokenizer en utilisant [`PreTrainedModel.save_pretrained`] :

```py
>>> pt_save_directory = "./pt_save_pretrained"
>>> tokenizer.save_pretrained(pt_save_directory)  # doctest: +IGNORE_RESULT
>>> pt_model.save_pretrained(pt_save_directory)
```

Lorsque vous voulez r√©utiliser le mod√®le, rechargez-le avec [`PreTrainedModel.from_pretrained`] :

```py
>>> pt_model = AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained")
```
</pt>
<tf>
Une fois que votre mod√®le est finetun√©, vous pouvez le sauvegarder avec son tokenizer en utilisant [`TFPreTrainedModel.save_pretrained`] :

```py
>>> tf_save_directory = "./tf_save_pretrained"
>>> tokenizer.save_pretrained(tf_save_directory)  # doctest: +IGNORE_RESULT
>>> tf_model.save_pretrained(tf_save_directory)
```

Lorsque vous voulez r√©utiliser le mod√®le, rechargez-le avec [`TFPreTrainedModel.from_pretrained`] :

```py
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained("./tf_save_pretrained")
```
</tf>
</frameworkcontent>

Une fonctionnalit√© particuli√®rement cool ü§ó Transformers est la possibilit√© d'enregistrer un mod√®le et de le recharger en tant que mod√®le PyTorch ou TensorFlow. Le param√®tre `from_pt` ou `from_tf` permet de convertir le mod√®le d'un framework √† l'autre :

<frameworkcontent>
<pt>
```py
>>> from transformers import AutoModel

>>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
>>> pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)
```
</pt>
<tf>
```py
>>> from transformers import TFAutoModel

>>> tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)
```
</tf>
</frameworkcontent>

## Constructions de mod√®les personnalis√©s

Vous pouvez modifier la configuration du mod√®le pour changer la fa√ßon dont un mod√®le est construit. La configuration sp√©cifie les attributs d'un mod√®le, tels que le nombre de couches ou de t√™tes d'attention. Vous partez de z√©ro lorsque vous initialisez un mod√®le √† partir d'une configuration personnalis√©e. Les attributs du mod√®le sont initialis√©s de mani√®re al√©atoire et vous devrez entra√Æner le mod√®le avant de pouvoir l'utiliser pour obtenir des r√©sultats significatifs.

Commencez par importer [`AutoConfig`], puis chargez le mod√®le pr√©-entra√Æn√© que vous voulez modifier. Dans [`AutoConfig.from_pretrained`], vous pouvez sp√©cifier l'attribut que vous souhaitez modifier, tel que le nombre de t√™tes d'attention :

```py
>>> from transformers import AutoConfig

>>> my_config = AutoConfig.from_pretrained("distilbert-base-uncased", n_heads=12)
```

<frameworkcontent>
<pt>
Cr√©ez un mod√®le personnalis√© √† partir de votre configuration avec [`AutoModel.from_config`] :

```py
>>> from transformers import AutoModel

>>> my_model = AutoModel.from_config(my_config)
```
</pt>
<tf>
Cr√©ez un mod√®le personnalis√© √† partir de votre configuration avec [`TFAutoModel.from_config`] :

```py
>>> from transformers import TFAutoModel

>>> my_model = TFAutoModel.from_config(my_config)
```
</tf>
</frameworkcontent>

Consultez le guide [Cr√©er une architecture personnalis√©e](./create_a_model) pour plus d'informations sur la cr√©ation de configurations personnalis√©es.

## Trainer - une boucle d'entra√Ænement optimis√©e par PyTorch

Tous les mod√®les sont des [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) standard, vous pouvez donc les utiliser dans n'importe quelle boucle d'entra√Ænement typique. Bien que vous puissiez √©crire votre propre boucle d'entra√Ænement, ü§ó Transformers fournit une classe [`Trainer`] pour PyTorch, qui contient la boucle d'entra√Ænement de base et ajoute des fonctionnalit√©s suppl√©mentaires comme l'entra√Ænement distribu√©, la pr√©cision mixte, et plus encore.

En fonction de votre t√¢che, vous passerez g√©n√©ralement les param√®tres suivants √† [`Trainer`] :

1. Un [`PreTrainedModel`] ou un [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module):

   ```py
   >>> from transformers import AutoModelForSequenceClassification

   >>> model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")
   ```

2. [`TrainingArguments`] contient les hyperparam√®tres du mod√®le que vous pouvez changer comme le taux d'apprentissage, la taille due l'√©chantillon, et le nombre d'√©poques pour s'entra√Æner. Les valeurs par d√©faut sont utilis√©es si vous ne sp√©cifiez pas d'hyperparam√®tres d'apprentissage :

   ```py
   >>> from transformers import TrainingArguments

   >>> training_args = TrainingArguments(
   ...     output_dir="path/to/save/folder/",
   ...     learning_rate=2e-5,
   ...     per_device_train_batch_size=8,
   ...     per_device_eval_batch_size=8,
   ...     num_train_epochs=2,
   ... )
   ```

3. Une classe de pr√©traitement comme un tokenizer, un processeur d'images ou un extracteur de caract√©ristiques :

   ```py
   >>> from transformers import AutoTokenizer

   >>> tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
   ```

4. Chargez un jeu de donn√©es :

   ```py
   >>> from datasets import load_dataset

   >>> dataset = load_dataset("rotten_tomatoes")  # doctest: +IGNORE_RESULT
   ```

5. Cr√©ez une fonction qui transforme le texte du jeu de donn√©es en token :

   ```py
   >>> def tokenize_dataset(dataset):
   ...     return tokenizer(dataset["text"])
   ```

   Puis appliquez-la √† l'int√©gralit√© du jeu de donn√©es avec [`~datasets.Dataset.map`]:

   ```py
   >>> dataset = dataset.map(tokenize_dataset, batched=True)
   ```

6. Un [`DataCollatorWithPadding`] pour cr√©er un √©chantillon d'exemples √† partir de votre jeu de donn√©es :

   ```py
   >>> from transformers import DataCollatorWithPadding

   >>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
   ```

Maintenant, rassemblez tous ces √©l√©ments dans un [`Trainer`] :

```py
>>> from transformers import Trainer

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=dataset["train"],
...     eval_dataset=dataset["test"],
...     tokenizer=tokenizer,
...     data_collator=data_collator,
... )  # doctest: +SKIP
```

Une fois que vous √™tes pr√™t, appelez la fonction [`~Trainer.train`] pour commencer l'entra√Ænement :

```py
>>> trainer.train()  # doctest: +SKIP
```

<Tip>

Pour les t√¢ches - comme la traduction ou la g√©n√©ration de r√©sum√© - qui utilisent un mod√®le s√©quence √† s√©quence, utilisez plut√¥t les classes [`Seq2SeqTrainer`] et [`Seq2SeqTrainingArguments`].

</Tip>

Vous pouvez personnaliser le comportement de la boucle d'apprentissage en red√©finissant les m√©thodes √† l'int√©rieur de [`Trainer`]. Cela vous permet de personnaliser des caract√©ristiques telles que la fonction de perte, l'optimiseur et le planificateur. Consultez la documentation de [`Trainer`] pour savoir quelles m√©thodes peuvent √™tre red√©finies. 

L'autre moyen de personnaliser la boucle d'apprentissage est d'utiliser les [Callbacks](./main_classes/callbacks). Vous pouvez utiliser les callbacks pour int√©grer d'autres biblioth√®ques et inspecter la boucle d'apprentissage afin de suivre la progression ou d'arr√™ter l'apprentissage plus t√¥t. Les callbacks ne modifient rien dans la boucle d'apprentissage elle-m√™me. Pour personnaliser quelque chose comme la fonction de perte, vous devez red√©finir le [`Trainer`] √† la place.

## Entra√Ænement avec TensorFlow

Tous les mod√®les sont des mod√®les standard [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) afin qu'ils puissent √™tre entra√Æn√©s avec TensorFlow avec l'API [Keras](https://keras.io/). ü§ó Transformers fournit la fonction [`~TFPreTrainedModel.prepare_tf_dataset`] pour charger facilement votre jeu de donn√©es comme un `tf.data.Dataset` afin que vous puissiez commencer l'entra√Ænement imm√©diatement avec les fonctions [`compile`](https://keras.io/api/models/model_training_apis/#compile-method) et [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) de Keras.

1. Vous commencez avec un mod√®le [`TFPreTrainedModel`] ou [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) :

   ```py
   >>> from transformers import TFAutoModelForSequenceClassification

   >>> model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")
   ```

2. Une classe de pr√©traitement comme un tokenizer, un processeur d'images ou un extracteur de caract√©ristiques :

   ```py
   >>> from transformers import AutoTokenizer

   >>> tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
   ```

3. Cr√©ez une fonction qui transforme le texte du jeu de donn√©es en token :

   ```py
   >>> def tokenize_dataset(dataset):
   ...     return tokenizer(dataset["text"])  # doctest: +SKIP
   ```

4. Appliquez le tokenizer √† l'ensemble du jeu de donn√©es avec [`~datasets.Dataset.map`] et passez ensuite le jeu de donn√©es et le tokenizer √† [`~TFPreTrainedModel.prepare_tf_dataset`]. Vous pouvez √©galement modifier la taille de l'√©chantillon et m√©langer le jeu de donn√©es ici si vous le souhaitez :

   ```py
   >>> dataset = dataset.map(tokenize_dataset)  # doctest: +SKIP
   >>> tf_dataset = model.prepare_tf_dataset(
   ...     dataset, batch_size=16, shuffle=True, tokenizer=tokenizer
   ... )  # doctest: +SKIP
   ```

5. Une fois que vous √™tes pr√™t, appelez les fonctions `compile` et `fit` pour commencer l'entra√Ænement :

   ```py
   >>> from tensorflow.keras.optimizers import Adam

   >>> model.compile(optimizer=Adam(3e-5))
   >>> model.fit(dataset)  # doctest: +SKIP
   ```

## Et apr√®s ?

Maintenant que vous avez termin√© la visite rapide de ü§ó Transformers, consultez nos guides et apprenez √† faire des choses plus sp√©cifiques comme cr√©er un mod√®le personnalis√©, finetuner un mod√®le pour une t√¢che, et comment entra√Æner un mod√®le avec un script. Si vous souhaitez en savoir plus sur les concepts fondamentaux de ü§ó Transformers, jetez un ≈ìil √† nos guides conceptuels !