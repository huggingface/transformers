<!--‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.
-->

# Pipelines pour l'inf√©rence

L'objet [`pipeline`] rend simple l'utilisation de n'importe quel mod√®le du [Hub](https://huggingface.co/models) pour l'inf√©rence sur n'importe quelle langue, t√¢ches de vision par ordinateur, d'audio et multimodales. M√™me si vous n'avez pas d'exp√©rience avec une modalit√© sp√©cifique ou si vous n'√™tes pas familier avec le code ci-dessous des mod√®les, vous pouvez toujours les utiliser pour l'inf√©rence avec la [`pipeline`] ! Ce tutoriel vous apprendra √† :

* Utiliser un [`pipeline`] pour l'inf√©rence.
* Utiliser un tokenizer ou mod√®le sp√©cifique.
* Utiliser un [`pipeline`] pour des t√¢ches audio, de vision et multimodales.

<Tip>

Consultez la documentation du [`pipeline`] pour une liste compl√®te des t√¢ches prises en charge et des param√®tres disponibles.

</Tip>

## Utilisation du pipeline

Bien que chaque t√¢che ait son propre [`pipeline`], il est plus simple d'utiliser le [`pipeline`] g√©n√©rale qui inclut tous les pipelines sp√©cifiques aux diff√©rentes t√¢ches. Cette approche charge automatiquement un mod√®le par d√©faut et une classe de pr√©traitement adapt√©e √† votre t√¢che, simplifiant ainsi votre utilisation. Prenons l'exemple de l'utilisation du [`pipeline`] pour la reconnaissance automatique de la parole (ASR) ou de la transcription de la parole en texte.

1. Commencez par cr√©er un [`pipeline`] et sp√©cifiez la t√¢che d'inf√©rence :

```py
>>> from transformers import pipeline

>>> transcriber = pipeline(task="automatic-speech-recognition")
```

2. Passez votre entr√©e au [`pipeline`]. Dans le cas de la reconnaissance vocale, il s'agit d'un fichier audio :

```py
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': 'I HAVE A DREAM BUT ONE DAY THIS NATION WILL RISE UP LIVE UP THE TRUE MEANING OF ITS TREES'}
```

Pas le r√©sultat que vous aviez en t√™te ? Consultez certains des [mod√®les de reconnaissance vocale automatique les plus t√©l√©charg√©s](https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=trending) 
sur le Hub pour voir si vous pouvez obtenir une meilleure transcription.

Essayons le mod√®le [Whisper large-v2](https://huggingface.co/openai/whisper-large) de OpenAI. Whisper a √©t√© publi√© 2 ans apr√®s Wav2Vec2 et a √©t√© entra√Æn√© sur pr√®s de 10 fois plus de donn√©es. En tant que tel, il surpasse Wav2Vec2 sur la plupart des benchmarks en aval. Il a √©galement l'avantage suppl√©mentaire de pr√©dire la ponctuation et la casse, ce qui n'est pas possible avec Wav2Vec2.

Essayons-le ici pour voir comment il fonctionne :

```py
>>> transcriber = pipeline(model="openai/whisper-large-v2")
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
```

Maintenant, ce r√©sultat semble plus pr√©cis ! Pour une comparaison approfondie entre Wav2Vec2 et Whisper, consultez le [cours Audio Transformers](https://huggingface.co/learn/audio-course/chapter5/asr_models).
Nous vous encourageons vraiment √† consulter le Hub pour des mod√®les dans diff√©rentes langues, des mod√®les sp√©cialis√©s dans votre domaine, et plus encore.
Vous pouvez consulter et comparer les r√©sultats des mod√®les directement depuis votre navigateur sur le Hub pour voir s'ils conviennent ou g√®rent mieux les cas particuliers que d'autres.
Et si vous ne trouvez pas de mod√®le pour votre cas d'utilisation, vous pouvez toujours commencer √† [entra√Æner](training) le v√¥tre !

Si vous avez plusieurs entr√©es, vous pouvez passer votre entr√©e sous forme de liste :

```py
transcriber(
    [
        "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac",
        "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac",
    ]
)
```

Les pipelines sont excellents pour l'exp√©rimentation car passer d'un mod√®le √† un autre est trivial ; cependant, il existe des moyens de les optimiser pour des charges de travail plus importantes que l'exp√©rimentation. Consultez les guides suivants qui expliquent comment it√©rer sur des ensembles de donn√©es complets ou utiliser des pipelines dans un serveur web :
de la documentation :
* [Utilisation des pipelines sur un ensemble de donn√©es](#using-pipelines-on-a-dataset)
* [Utilisation des pipelines pour un serveur web](./pipeline_webserver)

## Param√®tres

[`pipeline`] prend en charge de nombreux param√®tres ; certains sont sp√©cifiques √† la t√¢che et d'autres sont g√©n√©raux pour tous les pipelines.
En g√©n√©ral, vous pouvez sp√©cifier les param√®tres o√π vous le souhaitez :

```py
transcriber = pipeline(model="openai/whisper-large-v2", my_parameter=1)

out = transcriber(...)  # This will use `my_parameter=1`.
out = transcriber(..., my_parameter=2)  # This will override and use `my_parameter=2`.
out = transcriber(...)  # This will go back to using `my_parameter=1`.
```

Voyons 3 param√®tres importants :

### Device

Si vous utilisez `device=n`, le pipeline met automatiquement le mod√®le sur l'appareil sp√©cifi√©.
Cela fonctionnera que vous utilisiez PyTorch ou Tensorflow.

```py
transcriber = pipeline(model="openai/whisper-large-v2", device=0)
```

Si le mod√®le est trop grand pour un seul GPU et que vous utilisez PyTorch, vous pouvez d√©finir `device_map="auto"` pour d√©terminer automatiquement comment charger et stocker les poids du mod√®le. L'utilisation de l'argument `device_map` n√©cessite le package ü§ó [Accelerate](https://huggingface.co/docs/accelerate) :

```bash
pip install --upgrade accelerate
```

Le code suivant charge et stocke automatiquement les poids du mod√®le sur plusieurs appareils :

```py
transcriber = pipeline(model="openai/whisper-large-v2", device_map="auto")
```

Notez que si `device_map="auto"` est pass√©, il n'est pas n√©cessaire d'ajouter l'argument `device=device` lors de l'instanciation de votre `pipeline` car vous pourriez rencontrer des comportements inattendus !

### Batch size

Par d√©faut, les pipelines ne feront pas d'inf√©rence en batch pour des raisons expliqu√©es en d√©tail [ici](https://huggingface.co/docs/transformers/main_classes/pipelines#pipeline-batching). La raison est que le batching n'est pas n√©cessairement plus rapide, et peut en fait √™tre beaucoup plus lent dans certains cas.

Mais si cela fonctionne dans votre cas d'utilisation, vous pouvez utiliser :

```py
transcriber = pipeline(model="openai/whisper-large-v2", device=0, batch_size=2)
audio_filenames = [f"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/{i}.flac" for i in range(1, 5)]
texts = transcriber(audio_filenames)
```

Cela ex√©cute le pipeline sur les 4 fichiers audio fournis, mais les passera par batch de 2 au mod√®le (qui est sur un GPU, o√π le batching est plus susceptible d'aider) sans n√©cessiter de code suppl√©mentaire de votre part. 
La sortie doit toujours correspondre √† ce que vous auriez re√ßu sans batching. Il s'agit uniquement d'un moyen de vous aider √† obtenir plus de vitesse avec un pipeline.

Les pipelines peuvent √©galement att√©nuer certaines des complexit√©s du batching car, pour certains pipelines, un seul √©l√©ment (comme un long fichier audio) doit √™tre divis√© en plusieurs parties pour √™tre trait√© par un mod√®le. Le pipeline effectue ce [*batching par morceaux*](./main_classes/pipelines#pipeline-chunk-batching) pour vous.

### Param√®tres sp√©cifiques √† la t√¢che

Toutes les t√¢ches fournissent des param√®tres sp√©cifiques √† la t√¢che qui permettent une flexibilit√© et des options suppl√©mentaires pour vous aider √† accomplir votre travail.
Par exemple, la m√©thode [`transformers.AutomaticSpeechRecognitionPipeline.__call__`] dispose d'un param√®tre `return_timestamps` qui semble prometteur pour le sous-titrage des vid√©os :

```py
>>> transcriber = pipeline(model="openai/whisper-large-v2", return_timestamps=True)
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.', 'chunks': [{'timestamp': (0.0, 11.88), 'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its'}, {'timestamp': (11.88, 12.38), 'text': ' creed.'}]}
```

Comme vous pouvez le voir, le mod√®le a inf√©r√© le texte et a √©galement indiqu√© **quand** les diff√©rentes phrases ont √©t√© prononc√©es.

Il existe de nombreux param√®tres disponibles pour chaque t√¢che, alors consultez la r√©f√©rence API de chaque t√¢che pour voir ce que vous pouvez ajuster !
Par exemple, le [`~transformers.AutomaticSpeechRecognitionPipeline`] dispose d'un param√®tre `chunk_length_s` qui est utile pour travailler sur des fichiers audio tr√®s longs (par exemple, le sous-titrage de films entiers ou de vid√©os d'une heure) qu'un mod√®le ne peut g√©n√©ralement pas g√©rer seul :

```python
>>> transcriber = pipeline(model="openai/whisper-large-v2", chunk_length_s=30)
>>> transcriber("https://huggingface.co/datasets/reach-vb/random-audios/resolve/main/ted_60.wav")
{'text': " So in college, I was a government major, which means I had to write a lot of papers. Now, when a normal student writes a paper, they might spread the work out a little like this. So, you know. You get started maybe a little slowly, but you get enough done in the first week that with some heavier days later on, everything gets done and things stay civil. And I would want to do that like that. That would be the plan. I would have it all ready to go, but then actually the paper would come along, and then I would kind of do this. And that would happen every single paper. But then came my 90-page senior thesis, a paper you're supposed to spend a year on. I knew for a paper like that, my normal workflow was not an option, it was way too big a project. So I planned things out and I decided I kind of had to go something like this. This is how the year would go. So I'd start off light and I'd bump it up"}
```

Si vous ne trouvez pas un param√®tre qui vous aiderait vraiment, n'h√©sitez pas √† [le demander](https://github.com/huggingface/transformers/issues/new?assignees=&labels=feature&template=feature-request.yml) !

## Utilisation des pipelines sur un ensemble de donn√©es

Le pipeline peut √©galement ex√©cuter des inf√©rences sur un grand ensemble de donn√©es. Le moyen le plus simple que nous recommandons pour cela est d'utiliser un it√©rateur :

```py
def data():
    for i in range(1000):
        yield f"My example {i}"


pipe = pipeline(model="openai-community/gpt2", device=0)
generated_characters = 0
for out in pipe(data()):
    generated_characters += len(out[0]["generated_text"])
```


L'it√©rateur `data()` g√©n√®re chaque r√©sultat, et le pipeline reconna√Æt automatiquement que l'entr√©e est it√©rable et commencera √† r√©cup√©rer les donn√©es tout en continuant √† les traiter sur le GPU (cela utilise [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) sous le capot).
C'est important car vous n'avez pas besoin d'allouer de m√©moire pour l'ensemble de donn√©es complet et vous pouvez alimenter le GPU aussi rapidement que possible.

√âtant donn√© que le lotissement pourrait acc√©l√©rer les choses, il peut √™tre utile d'essayer de r√©gler le param√®tre `batch_size` ici.

La fa√ßon la plus simple d'it√©rer sur un ensemble de donn√©es est d'en charger un depuis ü§ó [Datasets](https://github.com/huggingface/datasets) :

```py
# KeyDataset is a util that will just output the item we're interested in.
from transformers.pipelines.pt_utils import KeyDataset
from datasets import load_dataset

pipe = pipeline(model="hf-internal-testing/tiny-random-wav2vec2", device=0)
dataset = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation[:10]")

for out in pipe(KeyDataset(dataset, "audio")):
    print(out)
```

## Utilisation des pipelines pour un serveur web

<Tip>
Cr√©er un moteur d'inf√©rence est un sujet complexe qui m√©rite sa propre page.
</Tip>

[Lien](./pipeline_webserver)

## Pipeline de vision

Utiliser un [`pipeline`] pour les t√¢ches de vision est pratiquement identique.

Sp√©cifiez votre t√¢che et passez votre image au classificateur. L'image peut √™tre un lien, un chemin local ou une image encod√©e en base64. Par exemple, quelle esp√®ce de chat est montr√©e ci-dessous ?

![pipeline-cat-chonk](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg)

```py
>>> from transformers import pipeline

>>> vision_classifier = pipeline(model="google/vit-base-patch16-224")
>>> preds = vision_classifier(
...     images="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> preds
[{'score': 0.4335, 'label': 'lynx, catamount'}, {'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.0239, 'label': 'Egyptian cat'}, {'score': 0.0229, 'label': 'tiger cat'}]
```


## Pipeline de texte

Utiliser un [`pipeline`] pour les t√¢ches de NLP est pratiquement identique.

```py
>>> from transformers import pipeline

>>> # This model is a `zero-shot-classification` model.
>>> # It will classify text, except you are free to choose any label you might imagine
>>> classifier = pipeline(model="facebook/bart-large-mnli")
>>> classifier(
...     "I have a problem with my iphone that needs to be resolved asap!!",
...     candidate_labels=["urgent", "not urgent", "phone", "tablet", "computer"],
... )
{'sequence': 'I have a problem with my iphone that needs to be resolved asap!!', 'labels': ['urgent', 'phone', 'computer', 'not urgent', 'tablet'], 'scores': [0.504, 0.479, 0.013, 0.003, 0.002]}
```


## Pipeline multimodal

Le [`pipeline`] prend en charge plus d'une modalit√©. Par exemple, une t√¢che de r√©ponse √† des questions visuelles (VQA) combine texte et image. N'h√©sitez pas √† utiliser n'importe quel lien d'image que vous aimez et une question que vous souhaitez poser √† propos de l'image. L'image peut √™tre une URL ou un chemin local vers l'image.

Par exemple, si vous utilisez cette [image de facture](https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png) :

```py
>>> from transformers import pipeline

>>> vqa = pipeline(model="impira/layoutlm-document-qa")
>>> output = vqa(
...     image="https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png",
...     question="What is the invoice number?",
... )
>>> output[0]["score"] = round(output[0]["score"], 3)
>>> output
[{'score': 0.425, 'answer': 'us-001', 'start': 16, 'end': 16}]
```

<Tip>

Pour ex√©cuter l'exemple ci-dessus, vous devez avoir [`pytesseract`](https://pypi.org/project/pytesseract/) install√© en plus de ü§ó Transformers :

```bash
sudo apt install -y tesseract-ocr
pip install pytesseract
```

</Tip>

## Utilisation de `pipeline` sur de grands mod√®les avec ü§ó `accelerate` :

Vous pouvez facilement ex√©cuter `pipeline` sur de grands mod√®les en utilisant ü§ó `accelerate` ! Assurez-vous d'abord d'avoir install√© `accelerate` avec `pip install accelerate`. 

Chargez d'abord votre mod√®le en utilisant `device_map="auto"` ! Nous utiliserons `facebook/opt-1.3b` pour notre exemple.

```py
# pip install accelerate
import torch
from transformers import pipeline

pipe = pipeline(model="facebook/opt-1.3b", torch_dtype=torch.bfloat16, device_map="auto")
output = pipe("This is a cool example!", do_sample=True, top_p=0.95)
```
Vous pouvez √©galement passer des mod√®les charg√©s en 8 bits si vous installez `bitsandbytes` et ajoutez l'argument `load_in_8bit=True`
Notez que vous pouvez remplacer le point de contr√¥le par n'importe quel mod√®le.

```py
# pip install accelerate bitsandbytes
import torch
from transformers import pipeline

pipe = pipeline(model="facebook/opt-1.3b", device_map="auto", model_kwargs={"load_in_8bit": True})
output = pipe("This is a cool example!", do_sample=True, top_p=0.95)
```

## Cr√©ation de d√©monstrations web √† partir de pipelines avec `gradio`

Hugging Face prenant en charge le chargement de grands mod√®les, comme BLOOM.
Les pipelines sont automatiquement pris en charge dans [Gradio](https://github.com/gradio-app/gradio/), une biblioth√®que qui facilite la cr√©ation d'applications d'apprentissage automatique belles et conviviales sur le web. Tout d'abord, assurez-vous que Gradio est install√© :

```
pip install gradio
```

Ensuite, vous pouvez cr√©er une d√©monstration web autour d'un pipeline de classification d'images (ou tout autre pipeline) en une seule ligne de code en appelant la fonction [`Interface.from_pipeline`](https://www.gradio.app/docs/interface#interface-from-pipeline) de Gradio pour lancer le pipeline. Cela cr√©e une interface intuitive de glisser-d√©poser dans votre navigateur :

```py
from transformers import pipeline
import gradio as gr

pipe = pipeline("image-classification", model="google/vit-base-patch16-224")

gr.Interface.from_pipeline(pipe).launch()
```

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/panda-classification.png)


Par d√©faut, la d√©monstration web s'ex√©cute sur un serveur local. Si vous souhaitez la partager avec d'autres, vous pouvez g√©n√©rer un lien public temporaire en d√©finissant `share=True` dans `launch()`. Vous pouvez √©galement h√©berger votre d√©monstration sur [Hugging Face Spaces](https://huggingface.co/spaces) pour obtenir un lien permanent.