<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# í…ìŠ¤íŠ¸ ìƒì„± ì „ëµ

í…ìŠ¤íŠ¸ ìƒì„±ì€ ì˜¤í”ˆì—”ë“œ í…ìŠ¤íŠ¸ ìƒì„±, ìš”ì•½, ë²ˆì—­ ë“± ì—¬ëŸ¬ NLP ì‘ì—…ì— í•„ìˆ˜ì ì…ë‹ˆë‹¤. ë˜í•œ ìŒì„±ì—ì„œ í…ìŠ¤íŠ¸ë¡œ, ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” ë“±ì˜ ë‹¤ì–‘í•œ í˜¼í•© ëª¨ë‹¬ë¦¬í‹° ì‘ìš© í”„ë¡œê·¸ë¨ì—ì„œë„ ì—­í• ì„ í•©ë‹ˆë‹¤. í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆëŠ” ì¼ë¶€ ëª¨ë¸ì—ëŠ” GPT2, XLNet, OpenAI GPT, CTRL, TransformerXL, XLM, Bart, T5, GIT, Whisperê°€ í¬í•¨ë©ë‹ˆë‹¤.

[`~transformers.generation_utils.GenerationMixin.generate`] ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ë¥¸ ì‘ì—…ì— ëŒ€í•œ í…ìŠ¤íŠ¸ ì¶œë ¥ì„ ìƒì„±í•˜ëŠ” ëª‡ ê°€ì§€ ì˜ˆì œë¥¼ í™•ì¸í•´ë³´ì„¸ìš”:
* [í…ìŠ¤íŠ¸ ìš”ì•½](./tasks/summarization#inference)
* [ì´ë¯¸ì§€ ìº¡ì…˜](./model_doc/git#transformers.GitForCausalLM.forward.example)
* [ì˜¤ë””ì˜¤ ì „ì‚¬](./model_doc/whisper#transformers.WhisperForConditionalGeneration.forward.example)

`generate` ë©”ì„œë“œì˜ ì…ë ¥ì€ ëª¨ë¸ì˜ ëª¨ë‹¬ë¦¬í‹°ì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤. AutoTokenizerë‚˜ AutoProcessorì™€ ê°™ì€ ëª¨ë¸ì˜ ì „ì²˜ë¦¬ê¸° í´ë˜ìŠ¤ì—ì„œ ë°˜í™˜ë©ë‹ˆë‹¤. ëª¨ë¸ì˜ ì „ì²˜ë¦¬ê¸°ê°€ ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ì…ë ¥ì„ ìƒì„±í•˜ëŠ” ê²½ìš°, ëª¨ë“  ì…ë ¥ì„ generate()ì— ì „ë‹¬í•˜ì„¸ìš”. ê° ëª¨ë¸ì˜ ì „ì²˜ë¦¬ê¸°ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ í•´ë‹¹ ëª¨ë¸ì˜ ë¬¸ì„œì—ì„œ ì•Œì•„ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ ì¶œë ¥ í† í°ì„ ì„ íƒí•˜ëŠ” ê³¼ì •ì„ ë””ì½”ë”©ì´ë¼ê³  í•©ë‹ˆë‹¤. `generate()` ë©”ì„œë“œê°€ ì‚¬ìš©í•  ë””ì½”ë”© ì „ëµì„ ì‚¬ìš©ì ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë””ì½”ë”© ì „ëµì„ ìˆ˜ì •í•´ë„ í›ˆë ¨ ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ì˜ ê°’ì€ ë³€ê²½ë˜ì§€ ì•Šì§€ë§Œ, ìƒì„±ëœ ì¶œë ¥ì˜ í’ˆì§ˆì— ëˆˆì— ë„ëŠ” ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ í…ìŠ¤íŠ¸ì˜ ë°˜ë³µì„ ì¤„ì´ê³  ë” ì¼ê´€ëœ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.

ì´ ì•ˆë‚´ì„œì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë‚´ìš©ì„ ì„¤ëª…í•©ë‹ˆë‹¤:
* ê¸°ë³¸ í…ìŠ¤íŠ¸ ìƒì„± êµ¬ì„±
* ì¼ë°˜ì ì¸ ë””ì½”ë”© ì „ëµ ë° ì£¼ìš” ë§¤ê°œë³€ìˆ˜
* ğŸ¤— Hubì˜ ì„¸ë¶€ íŠœë‹ëœ ëª¨ë¸ê³¼ ì‚¬ìš©ì ì§€ì • ìƒì„± êµ¬ì„± ì €ì¥ ë° ê³µìœ 

## ê¸°ë³¸ í…ìŠ¤íŠ¸ ìƒì„± êµ¬ì„±

ëª¨ë¸ì˜ ë””ì½”ë”© ì „ëµì€ ìƒì„± êµ¬ì„±ì— ì •ì˜ë©ë‹ˆë‹¤. [`íŒŒì´í”„ë¼ì¸`] ë‚´ì—ì„œ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ì¶”ë¡ ì— ì‚¬ìš©í•  ë•Œ, ëª¨ë¸ì€ ë‚´ë¶€ì ìœ¼ë¡œ `PreTrainedModel.generate()` ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ë©° ê¸°ë³¸ ìƒì„± êµ¬ì„±ì„ ì ìš©í•©ë‹ˆë‹¤. ê¸°ë³¸ êµ¬ì„±ì€ ëª¨ë¸ê³¼ ê´€ë ¨ëœ ì‚¬ìš©ì ì •ì˜ êµ¬ì„±ì´ ì €ì¥ë˜ì§€ ì•Šì€ ê²½ìš°ì—ë„ ì‚¬ìš©ë©ë‹ˆë‹¤.

ëª¨ë¸ì„ ëª…ì‹œì ìœ¼ë¡œ ë¡œë“œí•˜ëŠ” ê²½ìš° `model.generation_config`ì„ í†µí•´ í•´ë‹¹ ëª¨ë¸ê³¼ í•¨ê»˜ ì œê³µë˜ëŠ” ìƒì„± êµ¬ì„±ì„ ê²€ì‚¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
>>> from transformers import AutoModelForCausalLM

>>> model = AutoModelForCausalLM.from_pretrained("distilgpt2")
>>> model.generation_config
GenerationConfig {
    "_from_model_config": true,
    "bos_token_id": 50256,
    "eos_token_id": 50256,
    "transformers_version": "4.26.0.dev0"
}
```

`model.generation_config`ë¥¼ ì¶œë ¥í•˜ë©´ ê¸°ë³¸ ìƒì„± êµ¬ì„±ê³¼ ë‹¤ë¥¸ ê°’ë§Œ ë‚˜ì—´ë˜ë©°, ê¸°ë³¸ ê°’ì€ ë‚˜ì—´ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

ê¸°ë³¸ ìƒì„± êµ¬ì„±ì€ ì¶œë ¥ì˜ í¬ê¸°ë¥¼ ì œí•œí•˜ì—¬ ì…ë ¥ í”„ë¡¬í”„íŠ¸ì™€ í•¨ê»˜ ìµœëŒ€ 20ê°œì˜ í† í°ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê¸°ë³¸ ë””ì½”ë”© ì „ëµì€ íƒìš•ì ì¸ íƒìƒ‰(greedy search)ìœ¼ë¡œ, ë‹¤ìŒ í† í°ìœ¼ë¡œ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°–ëŠ” í† í°ì„ ì„ íƒí•˜ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë””ì½”ë”© ì „ëµì…ë‹ˆë‹¤. ë§ì€ ì‘ì—…ê³¼ ì‘ì€ ì¶œë ¥ í¬ê¸°ì—ëŠ” ì˜ ì‘ë™í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê¸´ ì¶œë ¥ì„ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©í•  ê²½ìš°, íƒìš•ì ì¸ íƒìƒ‰ì€ ë°˜ë³µì ì¸ ê²°ê³¼ë¥¼ ìƒì„±í•˜ê¸° ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## í…ìŠ¤íŠ¸ ìƒì„± ì‚¬ìš©ì ì •ì˜

[`generate`] ë©”ì„œë“œì— ë§¤ê°œë³€ìˆ˜ì™€ í•´ë‹¹ ê°’ë“¤ì„ ì§ì ‘ ì „ë‹¬í•˜ì—¬ `generation_config`ë¥¼ ì¬ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
>>> my_model.generate(**inputs, num_beams=4, do_sample=True)
```

ê¸°ë³¸ ë””ì½”ë”© ì „ëµì´ ì‘ì—…ì— ëŒ€í•´ ëŒ€ë¶€ë¶„ ì‘ë™í•˜ë”ë¼ë„ ëª‡ ê°€ì§€ ì‚¬í•­ì„ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì¡°ì •ë˜ëŠ” ë§¤ê°œë³€ìˆ˜ ì¤‘ ì¼ë¶€ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

- `max_new_tokens`: ìƒì„±í•  í† í°ì˜ ìµœëŒ€ ê°œìˆ˜ì…ë‹ˆë‹¤. ë‹¤ì‹œ ë§í•´, í”„ë¡¬í”„íŠ¸ì˜ í† í°ì„ í¬í•¨í•˜ì§€ ì•ŠëŠ” ì¶œë ¥ ì‹œí€€ìŠ¤ì˜ í¬ê¸°ì…ë‹ˆë‹¤.
- `num_beams`: 1ë³´ë‹¤ í° ë¹”ì˜ ìˆ˜ë¥¼ ì§€ì •í•¨ìœ¼ë¡œì¨ íƒìš•ì ì¸ íƒìƒ‰ì—ì„œ ë¹” íƒìƒ‰ìœ¼ë¡œ ì „í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ì „ëµì€ ê° ì‹œê°„ ë‹¨ê³„ì—ì„œ ì—¬ëŸ¬ ê°€ì„¤ì„ í‰ê°€í•˜ê³  ìµœì¢…ì ìœ¼ë¡œ ì „ì²´ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ì „ë°˜ì ì¸ í™•ë¥ ì´ ê°€ì¥ ë†’ì€ ê°€ì„¤ì„ ì„ íƒí•©ë‹ˆë‹¤. ì´ëŠ” ì´ˆê¸° í† í°ì˜ í™•ë¥ ì´ ë‚®ì€ ìƒí™©ì—ì„œ ì‹œì‘í•˜ëŠ” ë†’ì€ í™•ë¥ ì˜ ì‹œí€€ìŠ¤ë¥¼ ì‹ë³„í•˜ëŠ” ì¥ì ì´ ìˆìœ¼ë©°, íƒìš•ì ì¸ íƒìƒ‰ì—ì„œ ë¬´ì‹œë˜ì—ˆì„ ê²ƒì…ë‹ˆë‹¤.
- `do_sample`: `True`ë¡œ ì„¤ì •í•˜ë©´ ì´ ë§¤ê°œë³€ìˆ˜ëŠ” ë‹¤í•­ ë¶„í¬ ìƒ˜í”Œë§, ë¹” íƒìƒ‰ ë‹¤í•­ ë¶„í¬ ìƒ˜í”Œë§, Top-K ìƒ˜í”Œë§ ë° Top-p ìƒ˜í”Œë§ê³¼ ê°™ì€ ë””ì½”ë”© ì „ëµì„ í™œì„±í™”í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì „ëµì€ ì „ì²´ ì–´íœ˜ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ì—ì„œ ë‹¤ìŒ í† í°ì„ ì„ íƒí•©ë‹ˆë‹¤.
- `num_return_sequences`: ê° ì…ë ¥ì— ëŒ€í•´ ë°˜í™˜í•  ì‹œí€€ìŠ¤ í›„ë³´ì˜ ìˆ˜ì…ë‹ˆë‹¤. ì´ ì˜µì…˜ì€ ë¹” íƒìƒ‰ê³¼ ìƒ˜í”Œë§ê³¼ ê°™ì€ ë‹¤ì¤‘ ì‹œí€€ìŠ¤ í›„ë³´ë¥¼ ì§€ì›í•˜ëŠ” ë””ì½”ë”© ì „ëµì—ë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íƒìš•ì ì¸ íƒìƒ‰ê³¼ ëŒ€ì¡°ì ì¸ íƒìƒ‰ê³¼ ê°™ì€ ë””ì½”ë”© ì „ëµì€ ë‹¨ì¼ ì¶œë ¥ ì‹œí€€ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

## ì‚¬ìš©ì ì •ì˜ ë””ì½”ë”© ì „ëµì„ ëª¨ë¸ê³¼ í•¨ê»˜ ì €ì¥

íŠ¹ì • ìƒì„± êµ¬ì„±ì„ ê°–ëŠ” íŠœë‹ëœ ëª¨ë¸ì„ ê³µìœ í•˜ë ¤ë©´ ë‹¤ìŒì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
* [`GenerationConfig`] í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
* ë””ì½”ë”© ì „ëµ ë§¤ê°œë³€ìˆ˜ ì§€ì •
* [`GenerationConfig.save_pretrained`]ì„ ì‚¬ìš©í•˜ì—¬ ìƒì„± êµ¬ì„± ì €ì¥ (config_file_name ë§¤ê°œë³€ìˆ˜ë¥¼ ë¹„ì›Œë‘ëŠ” ê²ƒì„ ìŠì§€ ë§ˆì„¸ìš”)
* `push_to_hub`ë¥¼ `True`ë¡œ ì„¤ì •í•˜ì—¬ êµ¬ì„±ì„ ëª¨ë¸ ë¦¬í¬ì— ì—…ë¡œë“œ

```python
>>> from transformers import AutoModelForCausalLM, GenerationConfig

>>> model = AutoModelForCausalLM.from_pretrained("my_account/my_model")
>>> generation_config = GenerationConfig(
...     max_new_tokens=50, do_sample=True, top_k=50, eos_token_id=model.config.eos_token_id
... )
>>> generation_config.save_pretrained("my_account/my_model", push_to_hub=True)
```

[`GenerationConfig.save_pretrained`]ì—ì„œ `config_file_name` ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ìƒì„± êµ¬ì„±ì„ ë‹¨ì¼ ë””ë ‰í† ë¦¬ì— ì €ì¥í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. [`GenerationConfig.from_pretrained`]ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‚˜ì¤‘ì— ì´ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë‹¨ì¼ ëª¨ë¸ì— ëŒ€í•´ ì—¬ëŸ¬ ìƒì„± êµ¬ì„±ì„ ì €ì¥í•˜ë ¤ëŠ” ê²½ìš° ìœ ìš©í•©ë‹ˆë‹¤ (ì˜ˆ: ìƒ˜í”Œë§ê³¼ í•¨ê»˜ ì°½ì˜ì ì¸ í…ìŠ¤íŠ¸ ìƒì„±ì„ ìœ„í•œ êµ¬ì„±ê³¼ ë¹” íƒìƒ‰ì„ ìœ„í•œ êµ¬ì„±). ëª¨ë¸ì— êµ¬ì„± íŒŒì¼ì„ ì¶”ê°€í•˜ë ¤ë©´ ì˜¬ë°”ë¥¸ Hub ê¶Œí•œì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

```python
>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig

>>> tokenizer = AutoTokenizer.from_pretrained("t5-small")
>>> model = AutoModelForSeq2SeqLM.from_pretrained("t5-small")

>>> translation_generation_config = GenerationConfig(
...     num_beams=4,
...     early_stopping=True,
...     decoder_start_token_id=0,
...     eos_token_id=model.config.eos_token_id,
...     pad_token=model.config.pad_token_id,
... )

>>> translation_generation_config.save_pretrained("t5-small", "translation_generation_config.json", push_to_hub=True)

>>> # You could then use the named generation config file to parameterize generation
>>> generation_config = GenerationConfig.from_pretrained("t5-small", "translation_generation_config.json")
>>> inputs = tokenizer("translate English to French: Configuration files are easy to use!", return_tensors="pt")
>>> outputs = model.generate(**inputs, generation_config=generation_config)
>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
['Les fichiers de configuration sont faciles Ã  utiliser !']
```

## ìŠ¤íŠ¸ë¦¬ë°

`generate()`ëŠ” `streamer` ì…ë ¥ì„ í†µí•´ ìŠ¤íŠ¸ë¦¬ë°ì„ ì§€ì›í•©ë‹ˆë‹¤. `streamer` ì…ë ¥ì€ `put()` ë° `end()`ë¼ëŠ” ë©”ì„œë“œë¥¼ ê°€ì§„ í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ì™€ í˜¸í™˜ë©ë‹ˆë‹¤. ë‚´ë¶€ì ìœ¼ë¡œ `put()`ì€ ìƒˆë¡œìš´ í† í°ì„ ë°€ì–´ë„£ëŠ” ë° ì‚¬ìš©ë˜ê³ , `end()`ëŠ” í…ìŠ¤íŠ¸ ìƒì„±ì˜ ì¢…ë£Œë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.

<Tip warning={true}>

ìŠ¤íŠ¸ë¦¬ë° í´ë˜ìŠ¤ì— ëŒ€í•œ APIëŠ” ì•„ì§ ê°œë°œ ì¤‘ì´ë©° ì•ìœ¼ë¡œ ë³€ê²½ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

</Tip>

ì‹¤ì œë¡œ ë‹¤ì–‘í•œ ëª©ì ì„ ìœ„í•´ ì§ì ‘ ìŠ¤íŠ¸ë¦¬ë° í´ë˜ìŠ¤ë¥¼ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ë˜í•œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê¸°ë³¸ ìŠ¤íŠ¸ë¦¬ë° í´ë˜ìŠ¤ë„ ì œê³µë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ [`TextStreamer`] í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ `generate()`ì˜ ì¶œë ¥ì„ í•œ ë²ˆì— í•œ ë‹¨ì–´ì”© í™”ë©´ì— ìŠ¤íŠ¸ë¦¬ë°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
>>> from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer

>>> tok = AutoTokenizer.from_pretrained("gpt2")
>>> model = AutoModelForCausalLM.from_pretrained("gpt2")
>>> inputs = tok(["An increasing sequence: one,"], return_tensors="pt")
>>> streamer = TextStreamer(tok)

>>> # Despite returning the usual output, the streamer will also print the generated text to stdout.
>>> _ = model.generate(**inputs, streamer=streamer, max_new_tokens=20)
An increasing sequence: one, two, three, four, five, six, seven, eight, nine, ten, eleven,
```

## ë””ì½”ë”© ì „ëµ

[`generate`] ë©”ì„œë“œì˜ ë§¤ê°œë³€ìˆ˜ì™€ ë§ˆì§€ë§‰ìœ¼ë¡œ `generation_config`ì˜ íŠ¹ì • ì¡°í•©ì„ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ë””ì½”ë”© ì „ëµì„ í™œì„±í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê°œë…ì— ëŒ€í•´ ì²˜ìŒ ì ‘í•˜ì‹ ë‹¤ë©´, [ê³µí†µ ë””ì½”ë”© ì „ëµì˜ ì‘ë™ ë°©ì‹ì„ ì„¤ëª…í•˜ëŠ” ë¸”ë¡œê·¸ ê¸€](https://huggingface.co/blog/how-to-generate)ì„ ì½ì–´ë³´ì‹œê¸°ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.

ì—¬ê¸°ì—ì„œëŠ” ë””ì½”ë”© ì „ëµì„ ì œì–´í•˜ëŠ” ì¼ë¶€ ë§¤ê°œë³€ìˆ˜ë¥¼ ë³´ì—¬ì£¼ê³ , í•´ë‹¹ ë§¤ê°œë³€ìˆ˜ë¥¼ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ëŠ”ì§€ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤.

### íƒìš•ì  íƒìƒ‰

[`generate`]ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ íƒìš•ì  íƒìƒ‰ ë””ì½”ë”©ì„ ì‚¬ìš©í•˜ë¯€ë¡œ, ì´ë¥¼ í™œì„±í™”í•˜ê¸° ìœ„í•´ ë³„ë„ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ì „ë‹¬í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. ì´ëŠ” `num_beams` ë§¤ê°œë³€ìˆ˜ê°€ 1ë¡œ ì„¤ì •ë˜ê³  `do_sample=False`ì„ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

```python
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> prompt = "I look forward to"
>>> checkpoint = "distilgpt2"

>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
>>> inputs = tokenizer(prompt, return_tensors="pt")

>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)
>>> outputs = model.generate(**inputs)
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
['I look forward to seeing you all again!\n\n\n\n\n\n\n\n\n\n\n']
```

### ëŒ€ì¡°ì  íƒìƒ‰

ëŒ€ì¡°ì  íƒìƒ‰ ë””ì½”ë”© ì „ëµì€ 2022ë…„ ë…¼ë¬¸ [A Contrastive Framework for Neural Text Generation](https://arxiv.org/abs/2202.06417)ì—ì„œ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” ë°˜ë³µë˜ì§€ ì•Šìœ¼ë©´ì„œë„ ì¼ê´€ëœ ê¸´ ì¶œë ¥ì„ ìƒì„±í•˜ëŠ” ë° ìš°ìˆ˜í•œ ê²°ê³¼ë¥¼ ë³´ì…ë‹ˆë‹¤. ëŒ€ì¡°ì  íƒìƒ‰ì˜ ì‘ë™ ë°©ì‹ì„ ì•Œì•„ë³´ë ¤ë©´ [ì´ ë¸”ë¡œê·¸ ê¸€](https://huggingface.co/blog/introducing-csearch)ì„ í™•ì¸í•˜ì„¸ìš”. ëŒ€ì¡°ì  íƒìƒ‰ì„ í™œì„±í™”í•˜ê³  ì œì–´í•˜ëŠ” ë‘ ê°€ì§€ ì£¼ìš” ë§¤ê°œë³€ìˆ˜ëŠ” `penalty_alpha`ì™€ `top_k`ì…ë‹ˆë‹¤:

```python
>>> from transformers import AutoTokenizer, AutoModelForCausalLM

>>> checkpoint = "gpt2-large"
>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)

>>> prompt = "Hugging Face Company is"
>>> inputs = tokenizer(prompt, return_tensors="pt")

>>> outputs = model.generate(**inputs, penalty_alpha=0.6, top_k=4, max_new_tokens=100)
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
['Hugging Face Company is a family owned and operated business. \
We pride ourselves on being the best in the business and our customer service is second to none.\
\n\nIf you have any questions about our products or services, feel free to contact us at any time.\
 We look forward to hearing from you!']
```

### ë‹¤í•­ ë¶„í¬ ìƒ˜í”Œë§

ìµœëŒ€ í™•ë¥ ì„ ê°–ëŠ” í† í°ì„ í•­ìƒ ì„ íƒí•˜ëŠ” íƒìš•ì  íƒìƒ‰ê³¼ëŠ” ë‹¬ë¦¬, ë‹¤í•­ ë¶„í¬ ìƒ˜í”Œë§(ì¡°ìƒ ìƒ˜í”Œë§ì´ë¼ê³ ë„ í•¨)ì€ ëª¨ë¸ì´ ì œê³µí•˜ëŠ” ì „ì²´ ì–´íœ˜ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ í† í°ì„ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•©ë‹ˆë‹¤. í™•ë¥ ì´ 0ì´ ì•„ë‹Œ ëª¨ë“  í† í°ì€ ì„ íƒë  ìˆ˜ ìˆëŠ” ê¸°íšŒë¥¼ ê°€ì§€ë¯€ë¡œ ë°˜ë³µ ìœ„í—˜ì„ ì¤„ì…ë‹ˆë‹¤.

ë‹¤í•­ ë¶„í¬ ìƒ˜í”Œë§ì„ í™œì„±í™”í•˜ë ¤ë©´ `do_sample=True` ë° `num_beams=1`ë¡œ ì„¤ì •í•˜ì„¸ìš”.

```python
>>> from transformers import AutoTokenizer, AutoModelForCausalLM

>>> checkpoint = "gpt2-large"
>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)

>>> prompt = "Today was an amazing day because"
>>> inputs = tokenizer(prompt, return_tensors="pt")

>>> outputs = model.generate(**inputs, do_sample=True, num_beams=1, max_new_tokens=100)
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
['Today was an amazing day because we are now in the final stages of our trip to New York City which was very tough. \
It is a difficult schedule and a challenging part of the year but still worth it. I have been taking things easier and \
I feel stronger and more motivated to be out there on their tour. Hopefully, that experience is going to help them with \
their upcoming events which are currently scheduled in Australia.\n\nWe love that they are here. They want to make a \
name for themselves and become famous for what they']
```

### ë¹” íƒìƒ‰ ë””ì½”ë”©

íƒìš•ì  íƒìƒ‰ê³¼ëŠ” ë‹¬ë¦¬, ë¹” íƒìƒ‰ ë””ì½”ë”©ì€ ê° ì‹œê°„ ë‹¨ê³„ì—ì„œ ì—¬ëŸ¬ ê°œì˜ ê°€ì„¤ì„ ìœ ì§€í•˜ê³ , ìµœì¢…ì ìœ¼ë¡œ ì „ì²´ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ì „ë°˜ì ì¸ í™•ë¥ ì´ ê°€ì¥ ë†’ì€ ê°€ì„¤ì„ ì„ íƒí•©ë‹ˆë‹¤. ì´ëŠ” ì´ˆê¸° í† í°ì˜ í™•ë¥ ì´ ë‚®ì€ ìƒí™©ì—ì„œ ì‹œì‘í•˜ëŠ” ë†’ì€ í™•ë¥ ì˜ ì‹œí€€ìŠ¤ë¥¼ ì‹ë³„í•˜ëŠ” ì¥ì ì´ ìˆìœ¼ë©°, íƒìš•ì  íƒìƒ‰ì—ì„œëŠ” ë¬´ì‹œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ ë””ì½”ë”© ì „ëµì„ í™œì„±í™”í•˜ë ¤ë©´ `num_beams` (ì¦‰, ì¶”ì í•  ê°€ì„¤ ìˆ˜)ë¥¼ 1ë³´ë‹¤ í° ê°’ìœ¼ë¡œ ì§€ì •í•˜ì„¸ìš”.

```python
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> prompt = "It is astonishing how one can"
>>> checkpoint = "gpt2-medium"

>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
>>> inputs = tokenizer(prompt, return_tensors="pt")

>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)

>>> outputs = model.generate(**inputs, num_beams=5, max_new_tokens=50)
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
['It is astonishing how one can have such a profound impact on the lives of so many people in such a short period of \
time."\n\nHe added: "I am very proud of the work I have been able to do in the last few years.\n\n"I have']
```

### ë¹” íƒìƒ‰ ë‹¤í•­ ë¶„í¬ ìƒ˜í”Œë§

ì´ ë””ì½”ë”© ì „ëµì€ ë¹” íƒìƒ‰ê³¼ ë‹¤í•­ ë¶„í¬ ìƒ˜í”Œë§ì„ ê²°í•©í•œ ì „ëµì…ë‹ˆë‹¤. `num_beams`ì„ 1ë³´ë‹¤ í° ê°’ìœ¼ë¡œ ì§€ì •í•˜ê³ , `do_sample=True`ë¡œ ì„¤ì •í•˜ì—¬ ì´ ë””ì½”ë”© ì „ëµì„ ì‚¬ìš©í•˜ì„¸ìš”.

```python
>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

>>> prompt = "translate English to German: The house is wonderful."
>>> checkpoint = "t5-small"

>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
>>> inputs = tokenizer(prompt, return_tensors="pt")

>>> model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)

>>> outputs = model.generate(**inputs, num_beams=5, do_sample=True)
>>> tokenizer.decode(outputs[0], skip_special_tokens=True)
'Das Haus ist wunderbar.'
```

### ë‹¤ì–‘í•œ ë¹” íƒìƒ‰ ë””ì½”ë”©

ë‹¤ì–‘í•œ ë¹” íƒìƒ‰ ë””ì½”ë”© ì „ëµì€ ë¹” íƒìƒ‰ ì „ëµì˜ í™•ì¥ìœ¼ë¡œ, ë” ë‹¤ì–‘í•œ ë¹” ì‹œí€€ìŠ¤ ì„¸íŠ¸ë¥¼ ìƒì„±í•˜ì—¬ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‘ë™ ë°©ì‹ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ [Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models](https://arxiv.org/pdf/1610.02424.pdf)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”. ì´ ì ‘ê·¼ ë°©ì‹ì—ëŠ” `num_beams`ì™€ `num_beam_groups`ë¼ëŠ” ë‘ ê°€ì§€ ì£¼ìš” ë§¤ê°œë³€ìˆ˜ê°€ ìˆìŠµë‹ˆë‹¤. ê·¸ë£¹ì€ ë‹¤ë¥¸ ê·¸ë£¹ê³¼ ì¶©ë¶„íˆ êµ¬ë³„ë˜ë„ë¡ ì„ íƒë˜ë©°, ê° ê·¸ë£¹ ë‚´ì—ì„œëŠ” ì¼ë°˜ì ì¸ ë¹” íƒìƒ‰ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.

```python
>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

>>> checkpoint = "google/pegasus-xsum"
>>> prompt = "The Permaculture Design Principles are a set of universal design principles \
>>> that can be applied to any location, climate and culture, and they allow us to design \
>>> the most efficient and sustainable human habitation and food production systems. \
>>> Permaculture is a design system that encompasses a wide variety of disciplines, such \
>>> as ecology, landscape design, environmental science and energy conservation, and the \
>>> Permaculture design principles are drawn from these various disciplines. Each individual \
>>> design principle itself embodies a complete conceptual framework based on sound \
>>> scientific principles. When we bring all these separate  principles together, we can \
>>> create a design system that both looks at whole systems, the parts that these systems \
>>> consist of, and how those parts interact with each other to create a complex, dynamic, \
>>> living system. Each design principle serves as a tool that allows us to integrate all \
>>> the separate parts of a design, referred to as elements, into a functional, synergistic, \
>>> whole system, where the elements harmoniously interact and work together in the most \
>>> efficient way possible."

>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
>>> inputs = tokenizer(prompt, return_tensors="pt")

>>> model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)

>>> outputs = model.generate(**inputs, num_beams=5, num_beam_groups=5, max_new_tokens=30)
>>> tokenizer.decode(outputs[0], skip_special_tokens=True)
'The Design Principles are a set of universal design principles that can be applied to any location, climate and culture, and they allow us to design the most efficient and sustainable human habitation and food production systems.'
```

ì´ ì•ˆë‚´ì„œì—ì„œëŠ” ë‹¤ì–‘í•œ ë””ì½”ë”© ì „ëµì„ í™œì„±í™”í•˜ëŠ” ì£¼ìš” ë§¤ê°œë³€ìˆ˜ë¥¼ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. [`generate`] ë©”ì„œë“œì˜ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ë§¤ê°œë³€ìˆ˜ì˜ ì™„ì „í•œ ëª©ë¡ì€ [API ë¬¸ì„œ](./main_classes/text_generation.mdx)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

### ë³´ì¡° ë””ì½”ë”©

ë³´ì¡° ë””ì½”ë”©ì€ ìœ„ì˜ ë””ì½”ë”© ì „ëµì„ ìˆ˜ì •í•œ ê²ƒìœ¼ë¡œ, ë™ì¼í•œ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ëŠ” (ì´ìƒì ìœ¼ë¡œ í›¨ì”¬ ì‘ì€) ë³´ì¡° ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ëª‡ ê°œì˜ í›„ë³´ í† í°ì„ íƒìš•ì ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì£¼ìš” ëª¨ë¸ì€ í›„ë³´ í† í°ì„ í•˜ë‚˜ì˜ ì „ë°© í†µê³¼ë¡œ ê²€ì¦í•˜ì—¬ ë””ì½”ë”© í”„ë¡œì„¸ìŠ¤ë¥¼ ê°€ì†í™”í•©ë‹ˆë‹¤. í˜„ì¬ ë³´ì¡° ë””ì½”ë”©ì€ íƒìš•ì  íƒìƒ‰ê³¼ ìƒ˜í”Œë§ë§Œ ì§€ì›í•˜ë©°, ë°°ì¹˜ ì…ë ¥ì€ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [ì´ ë¸”ë¡œê·¸ ê¸€](https://huggingface.co/blog/assisted-generation)ì„ ì°¸ì¡°í•˜ì„¸ìš”.

ë³´ì¡° ë””ì½”ë”©ì„ í™œì„±í™”í•˜ë ¤ë©´ `assistant_model` ì¸ìì— ëª¨ë¸ì„ ì§€ì •í•˜ì„¸ìš”.

```python
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> prompt = "Alice and Bob"
>>> checkpoint = "EleutherAI/pythia-1.4b-deduped"
>>> assistant_checkpoint = "EleutherAI/pythia-160m-deduped"

>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
>>> inputs = tokenizer(prompt, return_tensors="pt")

>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)
>>> assistant_model = AutoModelForCausalLM.from_pretrained(assistant_checkpoint)
>>> outputs = model.generate(**inputs, assistant_model=assistant_model)
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
['Alice and Bob are sitting in a bar. Alice is drinking a beer and Bob is drinking a']
```

ìƒ˜í”Œë§ ë°©ë²•ìœ¼ë¡œ ë³´ì¡° ë””ì½”ë”©ì„ ì‚¬ìš©í•  ë•ŒëŠ” ë‹¤í•­ ë¶„í¬ ìƒ˜í”Œë§ì—ì„œì™€ ë§ˆì°¬ê°€ì§€ë¡œ `temperature` ì¸ìë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ì˜ì„±ì„ ì œì–´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë³´ì¡° ë””ì½”ë”©ì—ì„œëŠ” ì˜¨ë„ë¥¼ ë‚®ì¶”ë©´ ëŒ€ê¸° ì‹œê°„ì´ í–¥ìƒë˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.

```python
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> prompt = "Alice and Bob"
>>> checkpoint = "EleutherAI/pythia-1.4b-deduped"
>>> assistant_checkpoint = "EleutherAI/pythia-160m-deduped"

>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
>>> inputs = tokenizer(prompt, return_tensors="pt")

>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)
>>> assistant_model = AutoModelForCausalLM.from_pretrained(assistant_checkpoint)
>>> outputs = model.generate(**inputs, assistant_model=assistant_model, do_sample=True, temperature=0.5)
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
["Alice and Bob are sitting on the sofa. Alice says, 'I'm going to my room"]
```
