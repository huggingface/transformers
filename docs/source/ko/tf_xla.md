<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# TensorFlow ëª¨ë¸ì„ ìœ„í•œ XLA í†µí•© [[xla-integration-for-tensorflow-models]]

[[open-in-colab]]

XLA(Accelerated Linear Algebra)ëŠ” TensorFlow ëª¨ë¸ì˜ ì‹¤í–‰ ì‹œê°„ì„ ê°€ì†í™”í•˜ê¸° ìœ„í•œ ì»´íŒŒì¼ëŸ¬ì…ë‹ˆë‹¤. [ê³µì‹ ë¬¸ì„œ](https://www.tensorflow.org/xla)ì— ë”°ë¥´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

XLA(Accelerated Linear Algebra)ëŠ” ì„ í˜• ëŒ€ìˆ˜ë¥¼ ìœ„í•œ ë„ë©”ì¸ íŠ¹í™” ì»´íŒŒì¼ëŸ¬ë¡œ, TensorFlow ëª¨ë¸ì„ ì†ŒìŠ¤ ì½”ë“œ ë³€ê²½ ì—†ì´ ê°€ì†í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

TensorFlowì—ì„œ XLAë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ê°„ë‹¨í•©ë‹ˆë‹¤. XLAëŠ” `tensorflow` ë¼ì´ë¸ŒëŸ¬ë¦¬ ë‚´ì— íŒ¨í‚¤ì§€ë¡œ ì œê³µë˜ë©°, [`tf.function`](https://www.tensorflow.org/guide/intro_to_graphs)ê³¼ ê°™ì€ ê·¸ë˜í”„ ìƒì„± í•¨ìˆ˜ì—ì„œ `jit_compile` ì¸ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í™œì„±í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `fit()` ë° `predict()`ì™€ ê°™ì€ Keras ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, `jit_compile` ì¸ìˆ˜ë¥¼ `model.compile()`ì— ì „ë‹¬í•˜ì—¬ XLAë¥¼ ê°„ë‹¨í•˜ê²Œ í™œì„±í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ XLAëŠ” ì´ëŸ¬í•œ ë©”ì†Œë“œì— êµ­í•œë˜ì§€ ì•Šê³  ì„ì˜ì˜ `tf.function`ì„ ê°€ì†í™”í•˜ëŠ” ë°ì—ë„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ¤— Transformersì—ì„œëŠ” [GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2), [T5](https://huggingface.co/docs/transformers/model_doc/t5), [OPT](https://huggingface.co/docs/transformers/model_doc/opt)ì™€ ê°™ì€ ëª¨ë¸ì˜ í…ìŠ¤íŠ¸ ìƒì„±, ê·¸ë¦¬ê³  [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper)ì™€ ê°™ì€ ëª¨ë¸ì˜ ìŒì„± ì²˜ë¦¬ë¥¼ í¬í•¨í•˜ì—¬ ì—¬ëŸ¬ TensorFlow ë©”ì†Œë“œê°€ XLAì™€ í˜¸í™˜ë˜ë„ë¡ ë‹¤ì‹œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.

ì •í™•í•œ ì†ë„ í–¥ìƒì€ ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¥´ì§€ë§Œ, ğŸ¤— Transformers ë‚´ì˜ TensorFlow í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ì˜ ê²½ìš° ìµœëŒ€ 100ë°°ì˜ ì†ë„ í–¥ìƒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì´ ë¬¸ì„œì—ì„œëŠ” ì´ëŸ¬í•œ ëª¨ë¸ì— ëŒ€í•´ XLAë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœëŒ€ ì„±ëŠ¥ì„ ì–»ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤. ë˜í•œ XLA í†µí•©ì˜ ë²¤ì¹˜ë§ˆí¬ ë° ë””ìì¸ ì² í•™ì— ëŒ€í•œ ì¶”ê°€ ìë£Œ ë§í¬ë„ ì œê³µí•  ê²ƒì…ë‹ˆë‹¤.

## XLAë¥¼ ì‚¬ìš©í•˜ì—¬ TF í•¨ìˆ˜ ì‹¤í–‰í•˜ê¸° [[running-tf-functions-with-xla]]

TensorFlowì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ëª¨ë¸ì„ ê³ ë ¤í•´ ë´…ì‹œë‹¤:

```py
import tensorflow as tf

model = tf.keras.Sequential(
    [tf.keras.layers.Dense(10, input_shape=(10,), activation="relu"), tf.keras.layers.Dense(5, activation="softmax")]
)
```

ìœ„ ëª¨ë¸ì€ ì°¨ì›ì´ `(10, )`ì¸ ì…ë ¥ì„ ë°›ìŠµë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìˆœì „íŒŒë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
# ëª¨ë¸ì— ëŒ€í•œ ì„ì˜ì˜ ì…ë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.
batch_size = 16
input_vector_dim = 10
random_inputs = tf.random.normal((batch_size, input_vector_dim))

# ìˆœì „íŒŒë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
_ = model(random_inputs)
```

XLAë¡œ ì»´íŒŒì¼ëœ í•¨ìˆ˜ë¡œ ìˆœì „íŒŒë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ í•´ì•¼ í•©ë‹ˆë‹¤:

```py
xla_fn = tf.function(model, jit_compile=True)
_ = xla_fn(random_inputs)
```

`model`ì˜ ê¸°ë³¸ `call()` í•¨ìˆ˜ëŠ” XLA ê·¸ë˜í”„ë¥¼ ì»´íŒŒì¼í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë‹¤ë¥¸ ëª¨ë¸ í•¨ìˆ˜ë¥¼ XLAë¡œ ì»´íŒŒì¼í•˜ë ¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:

```py
my_xla_fn = tf.function(model.my_xla_fn, jit_compile=True)
```

## ğŸ¤— Transformersì—ì„œ XLAë¥¼ ì‚¬ìš©í•˜ì—¬ TF í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ ì‹¤í–‰í•˜ê¸° [[running-a-tf-text-generation-model-with-xla-from-transformers]]

ğŸ¤— Transformersì—ì„œ XLAë¡œ ê°€ì†í™”ëœ ìƒì„±ì„ í™œì„±í™”í•˜ë ¤ë©´ ìµœì‹  ë²„ì „ì˜ `transformers`ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì´ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
pip install transformers --upgrade
```

ê·¸ë¦¬ê³  ë‹¤ìŒ ì½”ë“œë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForCausalLM

# ìµœì†Œ ë²„ì „ì˜ Transformersê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šë‹¤ë©´ ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤.
from transformers.utils import check_min_version

check_min_version("4.21.0")


tokenizer = AutoTokenizer.from_pretrained("openai-community/gpt2", padding_side="left", pad_token="</s>")
model = TFAutoModelForCausalLM.from_pretrained("openai-community/gpt2")
input_string = ["TensorFlow is"]

# XLA ìƒì„± í•¨ìˆ˜ë¥¼ ë§Œë“¤ê¸° ìœ„í•œ í•œ ì¤„
xla_generate = tf.function(model.generate, jit_compile=True)

tokenized_input = tokenizer(input_string, return_tensors="tf")
generated_tokens = xla_generate(**tokenized_input, num_beams=2)

decoded_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)
print(f"Generated -- {decoded_text}")
# Generated -- TensorFlow is an open-source, open-source, distributed-source application # framework for the
```

ì•Œ ìˆ˜ ìˆë“¯ì´, `generate()`ì—ì„œ XLAë¥¼ í™œì„±í™”í•˜ëŠ” ê²ƒì€ ë‹¨ í•œ ì¤„ì˜ ì½”ë“œì…ë‹ˆë‹¤. ì½”ë“œì˜ ë‚˜ë¨¸ì§€ ë¶€ë¶„ì€ ë³€ê²½ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ìœ„ ì½”ë“œ ìŠ¤ë‹ˆí«ì—ì„œëŠ” XLAì— íŠ¹ì •í•œ ëª‡ ê°€ì§€ ì£¼ì˜í•  ì ì´ ìˆìŠµë‹ˆë‹¤. XLAê°€ ê°€ì ¸ë‹¤ì¤„ ì†ë„ í–¥ìƒì„ ì‹¤í˜„í•˜ê¸° ìœ„í•´ì„œëŠ” ì´ë¥¼ ì•Œê³  ìˆì–´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ìŒ ì„¹ì…˜ì—ì„œ ì´ì— ëŒ€í•´ ë…¼ì˜í•©ë‹ˆë‹¤.

## ì£¼ì˜í•  ì  [[gotchas-to-be-aware-of]]

XLA í™œì„±í™” í•¨ìˆ˜(`xla_generate()`ì™€ ê°™ì€)ë¥¼ ì²˜ìŒ ì‹¤í–‰í•  ë•Œ ë‚´ë¶€ì ìœ¼ë¡œ ê³„ì‚° ê·¸ë˜í”„ë¥¼ ì¶”ë¡ í•˜ë ¤ê³  í•˜ë©°, ì´ëŠ” ì‹œê°„ì´ ì†Œìš”ë©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ [â€œì¶”ì (tracing)â€](https://www.tensorflow.org/guide/intro_to_graphs#when_is_a_function_tracing)ì´ë¼ê³  ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤.

ìƒì„± ì‹œê°„ì´ ë¹ ë¥´ì§€ ì•Šë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤. `xla_generate()`(ë˜ëŠ” ë‹¤ë¥¸ XLA í™œì„±í™” í•¨ìˆ˜)ì˜ ì—°ì† í˜¸ì¶œì€ í•¨ìˆ˜ì— ì „ë‹¬ëœ ì…ë ¥ì´ ì´ˆê¸°ì— êµ¬ì¶•ëœ ê³„ì‚° ê·¸ë˜í”„ì™€ ë™ì¼í•œ í˜•íƒœë¥¼ ë”°ë¥¸ë‹¤ë©´, ê³„ì‚° ê·¸ë˜í”„ë¥¼ ì¶”ë¡ í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. ì´ëŠ” ì…ë ¥ í˜•íƒœê°€ ê³ ì •ëœ ëª¨ë‹¬ë¦¬í‹°(ì˜ˆ: ì´ë¯¸ì§€)ì—ëŠ” ë¬¸ì œê°€ ë˜ì§€ ì•Šì§€ë§Œ, ê°€ë³€ ì…ë ¥ í˜•íƒœ ëª¨ë‹¬ë¦¬í‹°(ì˜ˆ: í…ìŠ¤íŠ¸)ë¥¼ ì‚¬ìš©í•  ë•Œ ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤.

`xla_generate()`ê°€ í•­ìƒ ë™ì¼í•œ ì…ë ¥ í˜•íƒœë¡œ ë™ì‘í•˜ë„ë¡ í•˜ë ¤ë©´, í† í¬ë‚˜ì´ì €ë¥¼ í˜¸ì¶œí•  ë•Œ `padding` ì¸ìˆ˜ë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("openai-community/gpt2", padding_side="left", pad_token="</s>")
model = TFAutoModelForCausalLM.from_pretrained("openai-community/gpt2")
input_string = ["TensorFlow is"]

xla_generate = tf.function(model.generate, jit_compile=True)

# ì—¬ê¸°ì„œ, padding ì˜µì…˜ì´ ìˆëŠ” í† í¬ë‚˜ì´ì €ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
tokenized_input = tokenizer(input_string, pad_to_multiple_of=8, padding=True, return_tensors="tf")

generated_tokens = xla_generate(**tokenized_input, num_beams=2)
decoded_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)
print(f"Generated -- {decoded_text}")
```

ì´ë ‡ê²Œ í•˜ë©´ `xla_generate()`ì— ëŒ€í•œ ì…ë ¥ì´ í•­ìƒ ì¶”ì ëœ í˜•íƒœë¡œ ì „ë‹¬ë˜ì–´ ìƒì„± ì‹œê°„ì´ ê°€ì†í™”ë©ë‹ˆë‹¤. ë‹¤ìŒ ì½”ë“œë¡œ ì´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
import time
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("openai-community/gpt2", padding_side="left", pad_token="</s>")
model = TFAutoModelForCausalLM.from_pretrained("openai-community/gpt2")

xla_generate = tf.function(model.generate, jit_compile=True)

for input_string in ["TensorFlow is", "TensorFlow is a", "TFLite is a"]:
    tokenized_input = tokenizer(input_string, pad_to_multiple_of=8, padding=True, return_tensors="tf")
    start = time.time_ns()
    generated_tokens = xla_generate(**tokenized_input, num_beams=2)
    end = time.time_ns()
    print(f"Execution time -- {(end - start) / 1e6:.1f} ms\n")
```

Tesla T4 GPUì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì¶œë ¥ì„ ì˜ˆìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
Execution time -- 30819.6 ms

Execution time -- 79.0 ms

Execution time -- 78.9 ms
```
`xla_generate()`ì˜ ì²« ë²ˆì§¸ í˜¸ì¶œì€ ì¶”ì  ë•Œë¬¸ì— ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ì§€ë§Œ, ì—°ì† í˜¸ì¶œì€ ëª‡ ë°°ë‚˜ ë¹ ë¦…ë‹ˆë‹¤. ìƒì„± ì˜µì…˜ì— ëŒ€í•œ ì–´ë–¤ ë³€ê²½ì´ë“  ë‹¤ì‹œ ì¶”ì ì„ ìœ ë°œí•˜ë¯€ë¡œ ìƒì„± ì‹œê°„ì´ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŒì„ ëª…ì‹¬í•˜ì„¸ìš”.

ì´ ë¬¸ì„œì—ì„œëŠ” ğŸ¤— Transformersì—ì„œ ì œê³µí•˜ëŠ” ëª¨ë“  í…ìŠ¤íŠ¸ ìƒì„± ì˜µì…˜ì„ ë‹¤ë£¨ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê³ ê¸‰ ì‚¬ìš© ì‚¬ë¡€ì— ëŒ€í•´ ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.

## ì¶”ê°€ ìë£Œ [[additional-resources]]

ì—¬ê¸°ì— ğŸ¤— Transformersì™€ XLAì— ëŒ€í•´ ë” ìì„¸íˆ ì•Œê³  ì‹¶ì€ ê²½ìš° ë„ì›€ì´ ë  ìˆ˜ ìˆëŠ” ëª‡ ê°€ì§€ ì¶”ê°€ ìë£Œë¥¼ ì œê³µí•©ë‹ˆë‹¤. 
 
* [ì´ Colab ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/91_tf_xla_generate.ipynb)ì€ XLAì™€ í˜¸í™˜ë˜ëŠ” ì¸ì½”ë”-ë””ì½”ë”([T5](https://huggingface.co/docs/transformers/model_doc/t5)ì™€ ê°™ì€) ë° ë””ì½”ë” ì „ìš©([GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)ì™€ ê°™ì€) í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ì„ ì‹¤í—˜í•´ ë³¼ ìˆ˜ ìˆëŠ” ëŒ€í™”í˜• ë°ëª¨ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
* [ì´ ë¸”ë¡œê·¸ ê¸€](https://huggingface.co/blog/tf-xla-generate)ì€ TensorFlowì—ì„œ XLAì— ëŒ€í•œ ì¹œì ˆí•œ ì†Œê°œì™€ í•¨ê»˜ XLAì™€ í˜¸í™˜ë˜ëŠ” ëª¨ë¸ì˜ ë¹„êµ ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ ê°œìš”ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
* [ì´ ë¸”ë¡œê·¸ ê¸€](https://blog.tensorflow.org/2022/11/how-hugging-face-improved-text-generation-performance-with-xla.html)ì€ ğŸ¤— Transformersì˜ TensorFlow ëª¨ë¸ì— XLA ì§€ì›ì„ ì¶”ê°€í•˜ëŠ” ê²ƒì— ëŒ€í•œ ë””ìì¸ ì² í•™ì„ ë…¼ì˜í•©ë‹ˆë‹¤.
* XLAì™€ TensorFlow ê·¸ë˜í”„ì— ëŒ€í•´ ë” ìì„¸íˆ ì•Œê³  ì‹¶ì€ ê²½ìš° ì¶”ì²œí•˜ëŠ” ê¸€:
    * [XLA: ê¸°ê³„ í•™ìŠµì„ ìœ„í•œ ìµœì í™” ì»´íŒŒì¼ëŸ¬](https://www.tensorflow.org/xla)
    * [ê·¸ë˜í”„ ë° tf.function ì†Œê°œ](https://www.tensorflow.org/guide/intro_to_graphs)
    * [tf.functionìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒí•˜ê¸°](https://www.tensorflow.org/guide/function) 