<!--Copyright 2023 The HuggingFace Team. All rights reserved.
Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.
-->

# ğŸ¤— PEFTë¡œ ì–´ëŒ‘í„° ê°€ì ¸ì˜¤ê¸° [[load-adapters-with-peft]]

[[open-in-colab]]

[Parameter-Efficient Fine Tuning (PEFT)](https://huggingface.co/blog/peft) ë°©ë²•ì€ ì‚¬ì „í›ˆë ¨ëœ ëª¨ë¸ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ë¯¸ì„¸ ì¡°ì • ì¤‘ ê³ ì •ì‹œí‚¤ê³ , ê·¸ ìœ„ì— í›ˆë ¨í•  ìˆ˜ ìˆëŠ” ë§¤ìš° ì ì€ ìˆ˜ì˜ ë§¤ê°œë³€ìˆ˜(ì–´ëŒ‘í„°)ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. ì–´ëŒ‘í„°ëŠ” ì‘ì—…ë³„ ì •ë³´ë¥¼ í•™ìŠµí•˜ë„ë¡ í›ˆë ¨ë©ë‹ˆë‹¤. ì´ ì ‘ê·¼ ë°©ì‹ì€ ì™„ì „íˆ ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì— í•„ì í•˜ëŠ” ê²°ê³¼ë¥¼ ìƒì„±í•˜ë©´ì„œ, ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì´ê³  ë¹„êµì  ì ì€ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

ë˜í•œ PEFTë¡œ í›ˆë ¨ëœ ì–´ëŒ‘í„°ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì „ì²´ ëª¨ë¸ë³´ë‹¤ í›¨ì”¬ ì‘ê¸° ë•Œë¬¸ì— ê³µìœ , ì €ì¥ ë° ê°€ì ¸ì˜¤ê¸°ê°€ í¸ë¦¬í•©ë‹ˆë‹¤.

<div class="flex flex-col justify-center">
  <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/PEFT-hub-screenshot.png"/>
  <figcaption class="text-center">Hubì— ì €ì¥ëœ OPTForCausalLM ëª¨ë¸ì˜ ì–´ëŒ‘í„° ê°€ì¤‘ì¹˜ëŠ” ìµœëŒ€ 700MBì— ë‹¬í•˜ëŠ” ëª¨ë¸ ê°€ì¤‘ì¹˜ì˜ ì „ì²´ í¬ê¸°ì— ë¹„í•´ ì•½ 6MBì— ë¶ˆê³¼í•©ë‹ˆë‹¤.</figcaption>
</div>

ğŸ¤— PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ë ¤ë©´ [ë¬¸ì„œ](https://huggingface.co/docs/peft/index)ë¥¼ í™•ì¸í•˜ì„¸ìš”.

## ì„¤ì • [[setup]]

ğŸ¤— PEFTë¥¼ ì„¤ì¹˜í•˜ì—¬ ì‹œì‘í•˜ì„¸ìš”:

```bash
pip install peft
```

ìƒˆë¡œìš´ ê¸°ëŠ¥ì„ ì‚¬ìš©í•´ë³´ê³  ì‹¶ë‹¤ë©´, ë‹¤ìŒ ì†ŒìŠ¤ì—ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤:

```bash
pip install git+https://github.com/huggingface/peft.git
```

## ì§€ì›ë˜ëŠ” PEFT ëª¨ë¸ [[supported-peft-models]]

ğŸ¤— TransformersëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì¼ë¶€ PEFT ë°©ë²•ì„ ì§€ì›í•˜ë©°, ë¡œì»¬ì´ë‚˜ Hubì— ì €ì¥ëœ ì–´ëŒ‘í„° ê°€ì¤‘ì¹˜ë¥¼ ê°€ì ¸ì˜¤ê³  ëª‡ ì¤„ì˜ ì½”ë“œë§Œìœ¼ë¡œ ì‰½ê²Œ ì‹¤í–‰í•˜ê±°ë‚˜ í›ˆë ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ë°©ë²•ì„ ì§€ì›í•©ë‹ˆë‹¤:

- [Low Rank Adapters](https://huggingface.co/docs/peft/conceptual_guides/lora)
- [IA3](https://huggingface.co/docs/peft/conceptual_guides/ia3)
- [AdaLoRA](https://arxiv.org/abs/2303.10512)

ğŸ¤— PEFTì™€ ê´€ë ¨ëœ ë‹¤ë¥¸ ë°©ë²•(ì˜ˆ: í”„ë¡¬í”„íŠ¸ í›ˆë ¨ ë˜ëŠ” í”„ë¡¬í”„íŠ¸ íŠœë‹) ë˜ëŠ” ì¼ë°˜ì ì¸ ğŸ¤— PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ë ¤ë©´ [ë¬¸ì„œ](https://huggingface.co/docs/peft/index)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.


## PEFT ì–´ëŒ‘í„° ê°€ì ¸ì˜¤ê¸° [[load-a-peft-adapter]]

ğŸ¤— Transformersì—ì„œ PEFT ì–´ëŒ‘í„° ëª¨ë¸ì„ ê°€ì ¸ì˜¤ê³  ì‚¬ìš©í•˜ë ¤ë©´ Hub ì €ì¥ì†Œë‚˜ ë¡œì»¬ ë””ë ‰í„°ë¦¬ì— `adapter_config.json` íŒŒì¼ê³¼ ì–´ëŒ‘í„° ê°€ì¤‘ì¹˜ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì‹­ì‹œì˜¤. ê·¸ëŸ° ë‹¤ìŒ `AutoModelFor` í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ PEFT ì–´ëŒ‘í„° ëª¨ë¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì¸ê³¼ ê´€ê³„ ì–¸ì–´ ëª¨ë¸ìš© PEFT ì–´ëŒ‘í„° ëª¨ë¸ì„ ê°€ì ¸ì˜¤ë ¤ë©´ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¥´ì‹­ì‹œì˜¤:

1. PEFT ëª¨ë¸ IDë¥¼ ì§€ì •í•˜ì‹­ì‹œì˜¤.
2. [`AutoModelForCausalLM`] í´ë˜ìŠ¤ì— ì „ë‹¬í•˜ì‹­ì‹œì˜¤.

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

peft_model_id = "ybelkada/opt-350m-lora"
model = AutoModelForCausalLM.from_pretrained(peft_model_id)
```

<Tip>

`AutoModelFor` í´ë˜ìŠ¤ë‚˜ ê¸°ë³¸ ëª¨ë¸ í´ë˜ìŠ¤(ì˜ˆ: `OPTForCausalLM` ë˜ëŠ” `LlamaForCausalLM`) ì¤‘ í•˜ë‚˜ë¥¼ ì‚¬ìš©í•˜ì—¬ PEFT ì–´ëŒ‘í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

</Tip>

`load_adapter` ë©”ì†Œë“œë¥¼ í˜¸ì¶œí•˜ì—¬ PEFT ì–´ëŒ‘í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "facebook/opt-350m"
peft_model_id = "ybelkada/opt-350m-lora"

model = AutoModelForCausalLM.from_pretrained(model_id)
model.load_adapter(peft_model_id)
```

## 8ë¹„íŠ¸ ë˜ëŠ” 4ë¹„íŠ¸ë¡œ ê°€ì ¸ì˜¤ê¸° [[load-in-8bit-or-4bit]]

`bitsandbytes` í†µí•©ì€ 8ë¹„íŠ¸ì™€ 4ë¹„íŠ¸ ì •ë°€ë„ ë°ì´í„° ìœ í˜•ì„ ì§€ì›í•˜ë¯€ë¡œ í° ëª¨ë¸ì„ ê°€ì ¸ì˜¬ ë•Œ ìœ ìš©í•˜ë©´ì„œ ë©”ëª¨ë¦¬ë„ ì ˆì•½í•©ë‹ˆë‹¤. ëª¨ë¸ì„ í•˜ë“œì›¨ì–´ì— íš¨ê³¼ì ìœ¼ë¡œ ë¶„ë°°í•˜ë ¤ë©´ [`~PreTrainedModel.from_pretrained`]ì— `load_in_8bit` ë˜ëŠ” `load_in_4bit` ë§¤ê°œë³€ìˆ˜ë¥¼ ì¶”ê°€í•˜ê³  `device_map="auto"`ë¥¼ ì„¤ì •í•˜ì„¸ìš”:

```py
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

peft_model_id = "ybelkada/opt-350m-lora"
model = AutoModelForCausalLM.from_pretrained(peft_model_id, quantization_config=BitsAndBytesConfig(load_in_8bit=True))
```

## ìƒˆ ì–´ëŒ‘í„° ì¶”ê°€ [[add-a-new-adapter]]

ìƒˆ ì–´ëŒ‘í„°ê°€ í˜„ì¬ ì–´ëŒ‘í„°ì™€ ë™ì¼í•œ ìœ í˜•ì¸ ê²½ìš°ì— í•œí•´ ê¸°ì¡´ ì–´ëŒ‘í„°ê°€ ìˆëŠ” ëª¨ë¸ì— ìƒˆ ì–´ëŒ‘í„°ë¥¼ ì¶”ê°€í•˜ë ¤ë©´ [`~peft.PeftModel.add_adapter`]ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ëª¨ë¸ì— ê¸°ì¡´ LoRA ì–´ëŒ‘í„°ê°€ ì—°ê²°ë˜ì–´ ìˆëŠ” ê²½ìš°:

```py
from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
from peft import PeftConfig

model_id = "facebook/opt-350m"
model = AutoModelForCausalLM.from_pretrained(model_id)

lora_config = LoraConfig(
    target_modules=["q_proj", "k_proj"],
    init_lora_weights=False
)

model.add_adapter(lora_config, adapter_name="adapter_1")
```

ìƒˆ ì–´ëŒ‘í„°ë¥¼ ì¶”ê°€í•˜ë ¤ë©´:

```py
# attach new adapter with same config
model.add_adapter(lora_config, adapter_name="adapter_2")
```

ì´ì œ [`~peft.PeftModel.set_adapter`]ë¥¼ ì‚¬ìš©í•˜ì—¬ ì–´ëŒ‘í„°ë¥¼ ì‚¬ìš©í•  ì–´ëŒ‘í„°ë¡œ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
# use adapter_1
model.set_adapter("adapter_1")
output = model.generate(**inputs)
print(tokenizer.decode(output_disabled[0], skip_special_tokens=True))

# use adapter_2
model.set_adapter("adapter_2")
output_enabled = model.generate(**inputs)
print(tokenizer.decode(output_enabled[0], skip_special_tokens=True))
```

## ì–´ëŒ‘í„° í™œì„±í™” ë° ë¹„í™œì„±í™” [[enable-and-disable-adapters]]

ëª¨ë¸ì— ì–´ëŒ‘í„°ë¥¼ ì¶”ê°€í•œ í›„ ì–´ëŒ‘í„° ëª¨ë“ˆì„ í™œì„±í™” ë˜ëŠ” ë¹„í™œì„±í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì–´ëŒ‘í„° ëª¨ë“ˆì„ í™œì„±í™”í•˜ë ¤ë©´:

```py
from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
from peft import PeftConfig

model_id = "facebook/opt-350m"
adapter_model_id = "ybelkada/opt-350m-lora"
tokenizer = AutoTokenizer.from_pretrained(model_id)
text = "Hello"
inputs = tokenizer(text, return_tensors="pt")

model = AutoModelForCausalLM.from_pretrained(model_id)
peft_config = PeftConfig.from_pretrained(adapter_model_id)

# to initiate with random weights
peft_config.init_lora_weights = False

model.add_adapter(peft_config)
model.enable_adapters()
output = model.generate(**inputs)
```

ì–´ëŒ‘í„° ëª¨ë“ˆì„ ë¹„í™œì„±í™”í•˜ë ¤ë©´:

```py
model.disable_adapters()
output = model.generate(**inputs)
```

## PEFT ì–´ëŒ‘í„° í›ˆë ¨ [[train-a-peft-adapter]]

PEFT ì–´ëŒ‘í„°ëŠ” [`Trainer`] í´ë˜ìŠ¤ì—ì„œ ì§€ì›ë˜ë¯€ë¡œ íŠ¹ì • ì‚¬ìš© ì‚¬ë¡€ì— ë§ê²Œ ì–´ëŒ‘í„°ë¥¼ í›ˆë ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª‡ ì¤„ì˜ ì½”ë“œë¥¼ ì¶”ê°€í•˜ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ LoRA ì–´ëŒ‘í„°ë¥¼ í›ˆë ¨í•˜ë ¤ë©´:

<Tip>

[`Trainer`]ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ê²ƒì´ ìµìˆ™í•˜ì§€ ì•Šë‹¤ë©´ [ì‚¬ì „í›ˆë ¨ëœ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ê¸°](training) íŠœí† ë¦¬ì–¼ì„ í™•ì¸í•˜ì„¸ìš”.

</Tip>

1. ì‘ì—… ìœ í˜• ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì •í•˜ì—¬ ì–´ëŒ‘í„° êµ¬ì„±ì„ ì •ì˜í•©ë‹ˆë‹¤. í•˜ì´í¼íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ [`~peft.LoraConfig`]ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

```py
from peft import LoraConfig

peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="CAUSAL_LM",
)
```

2. ëª¨ë¸ì— ì–´ëŒ‘í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.

```py
model.add_adapter(peft_config)
```

3. ì´ì œ ëª¨ë¸ì„ [`Trainer`]ì— ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

```py
trainer = Trainer(model=model, ...)
trainer.train()
```

í›ˆë ¨í•œ ì–´ëŒ‘í„°ë¥¼ ì €ì¥í•˜ê³  ë‹¤ì‹œ ê°€ì ¸ì˜¤ë ¤ë©´:

```py
model.save_pretrained(save_dir)
model = AutoModelForCausalLM.from_pretrained(save_dir)
```
