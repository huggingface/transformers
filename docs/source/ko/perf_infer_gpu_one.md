<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# ë‹¨ì¼ GPUì—ì„œ íš¨ìœ¨ì ì¸ ì¶”ë¡  [[efficient-inference-on-a-single-gpu]]

ì´ ê°€ì´ë“œ ì™¸ì—ë„, [ë‹¨ì¼ GPUì—ì„œì˜ í›ˆë ¨ ê°€ì´ë“œ](perf_train_gpu_one)ì™€ [CPUì—ì„œì˜ ì¶”ë¡  ê°€ì´ë“œ](perf_infer_cpu)ì—ì„œë„ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## Better Transformer: PyTorch ë„¤ì´í‹°ë¸Œ Transformer íŒ¨ìŠ¤íŠ¸íŒ¨ìŠ¤ [[better-transformer-pytorchnative-transformer-fastpath]]

PyTorch ë„¤ì´í‹°ë¸Œ [`nn.MultiHeadAttention`](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/) ì–´í…ì…˜ íŒ¨ìŠ¤íŠ¸íŒ¨ìŠ¤ì¸ BetterTransformerëŠ” [ğŸ¤— Optimum ë¼ì´ë¸ŒëŸ¬ë¦¬](https://huggingface.co/docs/optimum/bettertransformer/overview)ì˜ í†µí•©ì„ í†µí•´ Transformersì™€ í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

PyTorchì˜ ì–´í…ì…˜ íŒ¨ìŠ¤íŠ¸íŒ¨ìŠ¤ëŠ” ì»¤ë„ í“¨ì „ê³¼ [ì¤‘ì²©ëœ í…ì„œ](https://pytorch.org/docs/stable/nested.html)ì˜ ì‚¬ìš©ì„ í†µí•´ ì¶”ë¡  ì†ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë²¤ì¹˜ë§ˆí¬ëŠ” [ì´ ë¸”ë¡œê·¸ ê¸€](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

[`optimum`](https://github.com/huggingface/optimum) íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•œ í›„ì—ëŠ” ì¶”ë¡  ì¤‘ Better Transformerë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ [`~PreTrainedModel.to_bettertransformer`]ë¥¼ í˜¸ì¶œí•˜ì—¬ ê´€ë ¨ ë‚´ë¶€ ëª¨ë“ˆì„ ëŒ€ì²´í•©ë‹ˆë‹¤:

```python
model = model.to_bettertransformer()
```

[`~PreTrainedModel.reverse_bettertransformer`] ë©”ì†Œë“œëŠ” ì •ê·œí™”ëœ transformers ëª¨ë¸ë§ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ëª¨ë¸ì„ ì €ì¥í•˜ê¸° ì „ ì›ë˜ì˜ ëª¨ë¸ë§ìœ¼ë¡œ ëŒì•„ê°ˆ ìˆ˜ ìˆë„ë¡ í•´ì¤ë‹ˆë‹¤:

```python
model = model.reverse_bettertransformer()
model.save_pretrained("saved_model")
```

PyTorch 2.0ë¶€í„°ëŠ” ì–´í…ì…˜ íŒ¨ìŠ¤íŠ¸íŒ¨ìŠ¤ê°€ ì¸ì½”ë”ì™€ ë””ì½”ë” ëª¨ë‘ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤. ì§€ì›ë˜ëŠ” ì•„í‚¤í…ì²˜ ëª©ë¡ì€ [ì—¬ê¸°](https://huggingface.co/docs/optimum/bettertransformer/overview#supported-models)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## FP4 í˜¼í•© ì •ë°€ë„ ì¶”ë¡ ì„ ìœ„í•œ `bitsandbytes` í†µí•© [[bitsandbytes-integration-for-fp4-mixedprecision-inference]]

`bitsandbytes`ë¥¼ ì„¤ì¹˜í•˜ë©´ GPUì—ì„œ ì†ì‰½ê²Œ ëª¨ë¸ì„ ì••ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. FP4 ì–‘ìí™”ë¥¼ ì‚¬ìš©í•˜ë©´ ì›ë˜ì˜ ì „ì²´ ì •ë°€ë„ ë²„ì „ê³¼ ë¹„êµí•˜ì—¬ ëª¨ë¸ í¬ê¸°ë¥¼ ìµœëŒ€ 8ë°° ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì—ì„œ ì‹œì‘í•˜ëŠ” ë°©ë²•ì„ í™•ì¸í•˜ì„¸ìš”.

<Tip>

ì´ ê¸°ëŠ¥ì€ ë‹¤ì¤‘ GPU ì„¤ì •ì—ì„œë„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

</Tip>

### ìš”êµ¬ ì‚¬í•­ [[requirements-for-fp4-mixedprecision-inference]]

- ìµœì‹  `bitsandbytes` ë¼ì´ë¸ŒëŸ¬ë¦¬
`pip install bitsandbytes>=0.39.0`

- ìµœì‹  `accelerate`ë¥¼ ì†ŒìŠ¤ì—ì„œ ì„¤ì¹˜
`pip install git+https://github.com/huggingface/accelerate.git`

- ìµœì‹  `transformers`ë¥¼ ì†ŒìŠ¤ì—ì„œ ì„¤ì¹˜
`pip install git+https://github.com/huggingface/transformers.git`

### FP4 ëª¨ë¸ ì‹¤í–‰ - ë‹¨ì¼ GPU ì„¤ì • - ë¹ ë¥¸ ì‹œì‘ [[running-fp4-models-single-gpu-setup-quickstart]]

ë‹¤ìŒ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ ë‹¨ì¼ GPUì—ì„œ ë¹ ë¥´ê²Œ FP4 ëª¨ë¸ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```py
from transformers import AutoModelForCausalLM

model_name = "bigscience/bloom-2b5"
model_4bit = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_4bit=True)
```
`device_map`ì€ ì„ íƒ ì‚¬í•­ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ `device_map = 'auto'`ë¡œ ì„¤ì •í•˜ëŠ” ê²ƒì´ ì‚¬ìš© ê°€ëŠ¥í•œ ë¦¬ì†ŒìŠ¤ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë””ìŠ¤íŒ¨ì¹˜í•˜ê¸° ë•Œë¬¸ì— ì¶”ë¡ ì— ìˆì–´ ê¶Œì¥ë©ë‹ˆë‹¤.

### FP4 ëª¨ë¸ ì‹¤í–‰ - ë‹¤ì¤‘ GPU ì„¤ì • [[running-fp4-models-multi-gpu-setup]]

ë‹¤ì¤‘ GPUì—ì„œ í˜¼í•© 4ë¹„íŠ¸ ëª¨ë¸ì„ ê°€ì ¸ì˜¤ëŠ” ë°©ë²•ì€ ë‹¨ì¼ GPU ì„¤ì •ê³¼ ë™ì¼í•©ë‹ˆë‹¤(ë™ì¼í•œ ëª…ë ¹ì–´ ì‚¬ìš©):
```py
model_name = "bigscience/bloom-2b5"
model_4bit = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_4bit=True)
```
í•˜ì§€ë§Œ `accelerate`ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° GPUì— í• ë‹¹í•  GPU RAMì„ ì œì–´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì´ `max_memory` ì¸ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:

```py
max_memory_mapping = {0: "600MB", 1: "1GB"}
model_name = "bigscience/bloom-3b"
model_4bit = AutoModelForCausalLM.from_pretrained(
    model_name, device_map="auto", load_in_4bit=True, max_memory=max_memory_mapping
)
```
ì´ ì˜ˆì—ì„œëŠ” ì²« ë²ˆì§¸ GPUê°€ 600MBì˜ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•˜ê³  ë‘ ë²ˆì§¸ GPUê°€ 1GBë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

### ê³ ê¸‰ ì‚¬ìš©ë²• [[advanced-usage]]

ì´ ë°©ë²•ì˜ ë” ê³ ê¸‰ ì‚¬ìš©ë²•ì— ëŒ€í•´ì„œëŠ” [ì–‘ìí™”](main_classes/quantization) ë¬¸ì„œ í˜ì´ì§€ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

## Int8 í˜¼í•© ì •ë°€ë„ í–‰ë ¬ ë¶„í•´ë¥¼ ìœ„í•œ `bitsandbytes` í†µí•© [[bitsandbytes-integration-for-int8-mixedprecision-matrix-decomposition]]

<Tip>

ì´ ê¸°ëŠ¥ì€ ë‹¤ì¤‘ GPU ì„¤ì •ì—ì„œë„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

</Tip>

[`LLM.int8() : 8-bit Matrix Multiplication for Transformers at Scale`](https://arxiv.org/abs/2208.07339) ë…¼ë¬¸ì—ì„œ ìš°ë¦¬ëŠ” ëª‡ ì¤„ì˜ ì½”ë“œë¡œ Hubì˜ ëª¨ë“  ëª¨ë¸ì— ëŒ€í•œ Hugging Face í†µí•©ì„ ì§€ì›í•©ë‹ˆë‹¤.
ì´ ë°©ë²•ì€ `float16` ë° `bfloat16` ê°€ì¤‘ì¹˜ì— ëŒ€í•´ `nn.Linear` í¬ê¸°ë¥¼ 2ë°°ë¡œ ì¤„ì´ê³ , `float32` ê°€ì¤‘ì¹˜ì— ëŒ€í•´ 4ë°°ë¡œ ì¤„ì…ë‹ˆë‹¤. ì´ëŠ” ì ˆë°˜ ì •ë°€ë„ì—ì„œ ì´ìƒì¹˜ë¥¼ ì²˜ë¦¬í•¨ìœ¼ë¡œì¨ í’ˆì§ˆì— ê±°ì˜ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

![HFxbitsandbytes.png](https://cdn-uploads.huggingface.co/production/uploads/1659861207959-62441d1d9fdefb55a0b7d12c.png)

Int8 í˜¼í•© ì •ë°€ë„ í–‰ë ¬ ë¶„í•´ëŠ” í–‰ë ¬ ê³±ì…ˆì„ ë‘ ê°œì˜ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤: (1) fp16ë¡œ ê³±í•´ì§€ëŠ” ì²´ê³„ì ì¸ íŠ¹ì´ê°’ ì´ìƒì¹˜ ìŠ¤íŠ¸ë¦¼ í–‰ë ¬(0.01%) ë° (2) int8 í–‰ë ¬ ê³±ì…ˆì˜ ì¼ë°˜ì ì¸ ìŠ¤íŠ¸ë¦¼(99.9%). ì´ ë°©ë²•ì„ ì‚¬ìš©í•˜ë©´ ë§¤ìš° í° ëª¨ë¸ì— ëŒ€í•´ ì˜ˆì¸¡ ì €í•˜ ì—†ì´ int8 ì¶”ë¡ ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
ì´ ë°©ë²•ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ [ë…¼ë¬¸](https://arxiv.org/abs/2208.07339)ì´ë‚˜ [í†µí•©ì— ê´€í•œ ë¸”ë¡œê·¸ ê¸€](https://huggingface.co/blog/hf-bitsandbytes-integration)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![MixedInt8.gif](https://cdn-uploads.huggingface.co/production/uploads/1660567469965-62441d1d9fdefb55a0b7d12c.gif)

ì»¤ë„ì€ GPU ì „ìš©ìœ¼ë¡œ ì»´íŒŒì¼ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— í˜¼í•© 8ë¹„íŠ¸ ëª¨ë¸ì„ ì‹¤í–‰í•˜ë ¤ë©´ GPUê°€ í•„ìš”í•©ë‹ˆë‹¤. ì´ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ê¸° ì „ì— ëª¨ë¸ì˜ 1/4(ë˜ëŠ” ëª¨ë¸ ê°€ì¤‘ì¹˜ê°€ ì ˆë°˜ ì •ë°€ë„ì¸ ê²½ìš° ì ˆë°˜)ì„ ì €ì¥í•  ì¶©ë¶„í•œ GPU ë©”ëª¨ë¦¬ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
ì´ ëª¨ë“ˆì„ ì‚¬ìš©í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ëª‡ ê°€ì§€ ì°¸ê³  ì‚¬í•­ì´ ì•„ë˜ì— ë‚˜ì™€ ìˆìŠµë‹ˆë‹¤. ë˜ëŠ” [Google colab](#colab-demos)ì—ì„œ ë°ëª¨ë¥¼ ë”°ë¼í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

### ìš”êµ¬ ì‚¬í•­ [[requirements-for-int8-mixedprecision-matrix-decomposition]]

- `bitsandbytes<0.37.0`ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, 8ë¹„íŠ¸ í…ì„œ ì½”ì–´(Turing, Ampere ë˜ëŠ” ì´í›„ ì•„í‚¤í…ì²˜ - ì˜ˆ: T4, RTX20s RTX30s, A40-A100)ë¥¼ ì§€ì›í•˜ëŠ” NVIDIA GPUì—ì„œ ì‹¤í–‰í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. `bitsandbytes>=0.37.0`ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, ëª¨ë“  GPUê°€ ì§€ì›ë©ë‹ˆë‹¤.
- ì˜¬ë°”ë¥¸ ë²„ì „ì˜ `bitsandbytes`ë¥¼ ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:
`pip install bitsandbytes>=0.31.5`
- `accelerate`ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”
`pip install accelerate>=0.12.0`

### í˜¼í•© Int8 ëª¨ë¸ ì‹¤í–‰ - ë‹¨ì¼ GPU ì„¤ì • [[running-mixedint8-models-single-gpu-setup]]

í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•œ í›„ í˜¼í•© 8ë¹„íŠ¸ ëª¨ë¸ì„ ê°€ì ¸ì˜¤ëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

```py
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

model_name = "bigscience/bloom-2b5"
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True))
```

í…ìŠ¤íŠ¸ ìƒì„±ì˜ ê²½ìš°:

* `pipeline()` í•¨ìˆ˜ ëŒ€ì‹  ëª¨ë¸ì˜ `generate()` ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤. `pipeline()` í•¨ìˆ˜ë¡œëŠ” ì¶”ë¡ ì´ ê°€ëŠ¥í•˜ì§€ë§Œ, í˜¼í•© 8ë¹„íŠ¸ ëª¨ë¸ì— ìµœì í™”ë˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— `generate()` ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, nucleus ìƒ˜í”Œë§ê³¼ ê°™ì€ ì¼ë¶€ ìƒ˜í”Œë§ ì „ëµì€ í˜¼í•© 8ë¹„íŠ¸ ëª¨ë¸ì— ëŒ€í•´ `pipeline()` í•¨ìˆ˜ì—ì„œ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
* ì…ë ¥ì„ ëª¨ë¸ê³¼ ë™ì¼í•œ GPUì— ë°°ì¹˜í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

ë‹¤ìŒì€ ê°„ë‹¨í•œ ì˜ˆì…ë‹ˆë‹¤:

```py
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_name = "bigscience/bloom-2b5"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True))

prompt = "Hello, my llama is cute"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
generated_ids = model.generate(**inputs)
outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
```


### í˜¼í•© Int8 ëª¨ë¸ ì‹¤í–‰ - ë‹¤ì¤‘ GPU ì„¤ì • [[running-mixedint8-models-multi-gpu-setup]]

ë‹¤ì¤‘ GPUì—ì„œ í˜¼í•© 8ë¹„íŠ¸ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ë°©ë²•ì€ ë‹¨ì¼ GPU ì„¤ì •ê³¼ ë™ì¼í•©ë‹ˆë‹¤(ë™ì¼í•œ ëª…ë ¹ì–´ ì‚¬ìš©):
```py
model_name = "bigscience/bloom-2b5"
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True))
```
í•˜ì§€ë§Œ `accelerate`ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° GPUì— í• ë‹¹í•  GPU RAMì„ ì œì–´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì´ `max_memory` ì¸ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:

```py
max_memory_mapping = {0: "1GB", 1: "2GB"}
model_name = "bigscience/bloom-3b"
model_8bit = AutoModelForCausalLM.from_pretrained(
    model_name, device_map="auto", load_in_8bit=True, max_memory=max_memory_mapping
)
```
ì´ ì˜ˆì‹œì—ì„œëŠ” ì²« ë²ˆì§¸ GPUê°€ 1GBì˜ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•˜ê³  ë‘ ë²ˆì§¸ GPUê°€ 2GBë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

### Colab ë°ëª¨ [[colab-demos]]

ì´ ë°©ë²•ì„ ì‚¬ìš©í•˜ë©´ ì´ì „ì— Google Colabì—ì„œ ì¶”ë¡ í•  ìˆ˜ ì—†ì—ˆë˜ ëª¨ë¸ì— ëŒ€í•´ ì¶”ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
Google Colabì—ì„œ 8ë¹„íŠ¸ ì–‘ìí™”ë¥¼ ì‚¬ìš©í•˜ì—¬ T5-11b(42GB in fp32)ë¥¼ ì‹¤í–‰í•˜ëŠ” ë°ëª¨ë¥¼ í™•ì¸í•˜ì„¸ìš”:

[![Open In Colab: T5-11b demo](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1YORPWx4okIHXnjW7MSAidXN29mPVNT7F?usp=sharing)

ë˜ëŠ” BLOOM-3Bì— ëŒ€í•œ ë°ëª¨ë¥¼ í™•ì¸í•˜ì„¸ìš”:

[![Open In Colab: BLOOM-3b demo](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1qOjXfQIAULfKvZqwCen8-MoWKGdSatZ4?usp=sharing)