<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Text generation strategies[[text-generation-strategies]]

í…ìŠ¤íŠ¸ ìƒì„±ì€ ê°œë°©í˜• í…ìŠ¤íŠ¸ ìž‘ì„±, ìš”ì•½, ë²ˆì—­ ë“± ë‹¤ì–‘í•œ ìžì—°ì–´ ì²˜ë¦¬(NLP) ìž‘ì—…ì— í•„ìˆ˜ì ìž…ë‹ˆë‹¤. ì´ëŠ” ë˜í•œ ìŒì„±-í…ìŠ¤íŠ¸ ë³€í™˜, ì‹œê°-í…ìŠ¤íŠ¸ ë³€í™˜ê³¼ ê°™ì´ í…ìŠ¤íŠ¸ë¥¼ ì¶œë ¥ìœ¼ë¡œ í•˜ëŠ” ì—¬ëŸ¬ í˜¼í•© ëª¨ë‹¬ë¦¬í‹° ì‘ìš© í”„ë¡œê·¸ëž¨ì—ì„œë„ ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤. í…ìŠ¤íŠ¸ ìƒì„±ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ëª‡ëª‡ ëª¨ë¸ë¡œëŠ” GPT2, XLNet, OpenAI GPT, CTRL, TransformerXL, XLM, Bart, T5, GIT, Whisper ë“±ì´ ìžˆìŠµë‹ˆë‹¤.


[`~generation.GenerationMixin.generate`] ë©”ì„œë“œë¥¼ í™œìš©í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ ë‹¤ì–‘í•œ ìž‘ì—…ë“¤ì— ëŒ€í•´ í…ìŠ¤íŠ¸ ê²°ê³¼ë¬¼ì„ ìƒì„±í•˜ëŠ” ëª‡ ê°€ì§€ ì˜ˆì‹œë¥¼ ì‚´íŽ´ë³´ì„¸ìš”:
* [í…ìŠ¤íŠ¸ ìš”ì•½](./tasks/summarization#inference)
* [ì´ë¯¸ì§€ ìº¡ì…”ë‹](./model_doc/git#transformers.GitForCausalLM.forward.example)
* [ì˜¤ë””ì˜¤ ì „ì‚¬](./model_doc/whisper#transformers.WhisperForConditionalGeneration.forward.example)

generate ë©”ì†Œë“œì— ìž…ë ¥ë˜ëŠ” ê°’ë“¤ì€ ëª¨ë¸ì˜ ë°ì´í„° í˜•íƒœì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ì´ ê°’ë“¤ì€ AutoTokenizerë‚˜ AutoProcessorì™€ ê°™ì€ ëª¨ë¸ì˜ ì „ì²˜ë¦¬ í´ëž˜ìŠ¤ì— ì˜í•´ ë°˜í™˜ë©ë‹ˆë‹¤. ëª¨ë¸ì˜ ì „ì²˜ë¦¬ ìž¥ì¹˜ê°€ í•˜ë‚˜ ì´ìƒì˜ ìž…ë ¥ ìœ í˜•ì„ ìƒì„±í•˜ëŠ” ê²½ìš°, ëª¨ë“  ìž…ë ¥ì„ generate()ì— ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤. ê° ëª¨ë¸ì˜ ì „ì²˜ë¦¬ ìž¥ì¹˜ì— ëŒ€í•´ì„œëŠ” í•´ë‹¹ ëª¨ë¸ì˜ ë¬¸ì„œì—ì„œ ìžì„¸ížˆ ì•Œì•„ë³¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ ì¶œë ¥ í† í°ì„ ì„ íƒí•˜ëŠ” ê³¼ì •ì„ ë””ì½”ë”©ì´ë¼ê³  í•˜ë©°, `generate()` ë©”ì†Œë“œê°€ ì‚¬ìš©í•  ë””ì½”ë”© ì „ëžµì„ ì‚¬ìš©ìžê°€ ì»¤ìŠ¤í„°ë§ˆì´ì§•í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ë””ì½”ë”© ì „ëžµì„ ìˆ˜ì •í•˜ëŠ” ê²ƒì€ í›ˆë ¨ ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ì˜ ê°’ë“¤ì„ ë³€ê²½í•˜ì§€ ì•Šì§€ë§Œ, ìƒì„±ëœ ì¶œë ¥ì˜ í’ˆì§ˆì— ëˆˆì— ë„ëŠ” ì˜í–¥ì„ ì¤„ ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ì´ëŠ” í…ìŠ¤íŠ¸ì—ì„œ ë°˜ë³µì„ ì¤„ì´ê³ , ë” ì¼ê´€ì„± ìžˆê²Œ ë§Œë“œëŠ” ë° ë„ì›€ì„ ì¤„ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.


ì´ ê°€ì´ë“œì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë‚´ìš©ì„ ë‹¤ë£¹ë‹ˆë‹¤:
* ê¸°ë³¸ ìƒì„± ì„¤ì •
* ì¼ë°˜ì ì¸ ë””ì½”ë”© ì „ëžµê³¼ ì£¼ìš” íŒŒë¼ë¯¸í„°
* ðŸ¤— Hubì—ì„œ ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ê³¼ í•¨ê»˜ ì‚¬ìš©ìž ì •ì˜ ìƒì„± ì„¤ì •ì„ ì €ìž¥í•˜ê³  ê³µìœ í•˜ëŠ” ë°©ë²•

## ê¸°ë³¸ í…ìŠ¤íŠ¸ ìƒì„± ì„¤ì •[[default-text-generation-configuration]]

ëª¨ë¸ì˜ ë””ì½”ë”© ì „ëžµì€ ìƒì„± ì„¤ì •ì—ì„œ ì •ì˜ë©ë‹ˆë‹¤. ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ [`pipeline`] ë‚´ì—ì„œ ì¶”ë¡ ì— ì‚¬ìš©í•  ë•Œ, ëª¨ë¸ì€ ë‚´ë¶€ì ìœ¼ë¡œ ê¸°ë³¸ ìƒì„± ì„¤ì •ì„ ì ìš©í•˜ëŠ” `PreTrainedModel.generate()` ë©”ì†Œë“œë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤. ì‚¬ìš©ìžê°€ ëª¨ë¸ê³¼ í•¨ê»˜ ì‚¬ìš©ìž ì •ì˜ ì„¤ì •ì„ ì €ìž¥í•˜ì§€ ì•Šì•˜ì„ ê²½ìš°ì—ë„ ê¸°ë³¸ ì„¤ì •ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.

ëª¨ë¸ì„ ëª…ì‹œì ìœ¼ë¡œ ë¡œë“œí•  ë•Œ, `model.generation_config`ì„ í†µí•´ ì œê³µë˜ëŠ” ìƒì„± ì„¤ì •ì„ ê²€ì‚¬í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

```python
>>> from transformers import AutoModelForCausalLM

>>> model = AutoModelForCausalLM.from_pretrained("distilbert/distilgpt2")
>>> model.generation_config
GenerationConfig {
    "bos_token_id": 50256,
    "eos_token_id": 50256,
}
```

 `model.generation_config`ë¥¼ ì¶œë ¥í•˜ë©´ ê¸°ë³¸ ì„¤ì •ê³¼ ë‹¤ë¥¸ ê°’ë“¤ë§Œ í‘œì‹œë˜ê³ , ê¸°ë³¸ê°’ë“¤ì€ ë‚˜ì—´ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

ê¸°ë³¸ ìƒì„± ì„¤ì •ì€ ìž…ë ¥ í”„ë¡¬í”„íŠ¸ì™€ ì¶œë ¥ì„ í•©ì¹œ ìµœëŒ€ í¬ê¸°ë¥¼ 20 í† í°ìœ¼ë¡œ ì œí•œí•˜ì—¬ ë¦¬ì†ŒìŠ¤ ë¶€ì¡±ì„ ë°©ì§€í•©ë‹ˆë‹¤. ê¸°ë³¸ ë””ì½”ë”© ì „ëžµì€ íƒìš• íƒìƒ‰(greedy search)ìœ¼ë¡œ, ë‹¤ìŒ í† í°ìœ¼ë¡œ ê°€ìž¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í† í°ì„ ì„ íƒí•˜ëŠ” ê°€ìž¥ ë‹¨ìˆœí•œ ë””ì½”ë”© ì „ëžµìž…ë‹ˆë‹¤. ë§Žì€ ìž‘ì—…ê³¼ ìž‘ì€ ì¶œë ¥ í¬ê¸°ì— ëŒ€í•´ì„œëŠ” ì´ ë°©ë²•ì´ ìž˜ ìž‘ë™í•˜ì§€ë§Œ, ë” ê¸´ ì¶œë ¥ì„ ìƒì„±í•  ë•Œ ì‚¬ìš©í•˜ë©´ ë§¤ìš° ë°˜ë³µì ì¸ ê²°ê³¼ë¥¼ ìƒì„±í•˜ê²Œ ë  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

## í…ìŠ¤íŠ¸ ìƒì„± ì‚¬ìš©ìž ì •ì˜[[customize-text-generation]]

íŒŒë¼ë¯¸í„°ì™€ í•´ë‹¹ ê°’ì„ [`generate`] ë©”ì†Œë“œì— ì§ì ‘ ì „ë‹¬í•˜ì—¬ `generation_config`ì„ ìž¬ì •ì˜í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤:

```python
>>> my_model.generate(**inputs, num_beams=4, do_sample=True)  # doctest: +SKIP
```

ê¸°ë³¸ ë””ì½”ë”© ì „ëžµì´ ëŒ€ë¶€ë¶„ì˜ ìž‘ì—…ì— ìž˜ ìž‘ë™í•œë‹¤ í•˜ë”ë¼ë„, ì¡°ì •í•  ìˆ˜ ìžˆëŠ” ëª‡ ê°€ì§€ íŒŒë¼ë¯¸í„°ê°€ ìžˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì¡°ì •ë˜ëŠ” íŒŒë¼ë¯¸í„°ì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê²ƒë“¤ì´ í¬í•¨ë©ë‹ˆë‹¤:

- `max_new_tokens`: ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ìž…ë‹ˆë‹¤. ì¦‰, í”„ë¡¬í”„íŠ¸ì— ìžˆëŠ” í† í°ì„ ì œì™¸í•œ ì¶œë ¥ ì‹œí€€ìŠ¤ì˜ í¬ê¸°ìž…ë‹ˆë‹¤. ì¶œë ¥ì˜ ê¸¸ì´ë¥¼ ì¤‘ë‹¨ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹ , ì „ì²´ ìƒì„±ë¬¼ì´ ì¼ì • ì‹œê°„ì„ ì´ˆê³¼í•  ë•Œ ìƒì„±ì„ ì¤‘ë‹¨í•˜ê¸°ë¡œ ì„ íƒí•  ìˆ˜ë„ ìžˆìŠµë‹ˆë‹¤. ë” ì•Œì•„ë³´ë ¤ë©´ [`StoppingCriteria`]ë¥¼ í™•ì¸í•˜ì„¸ìš”.
- `num_beams`: 1ë³´ë‹¤ í° ìˆ˜ì˜ ë¹”ì„ ì§€ì •í•¨ìœ¼ë¡œì¨, íƒìš• íƒìƒ‰(greedy search)ì—ì„œ ë¹” íƒìƒ‰(beam search)ìœ¼ë¡œ ì „í™˜í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ ì „ëžµì€ ê° ì‹œê°„ ë‹¨ê³„ì—ì„œ ì—¬ëŸ¬ ê°€ì„¤ì„ í‰ê°€í•˜ê³  ê²°êµ­ ì „ì²´ ì‹œí€€ìŠ¤ì— ëŒ€í•´ ê°€ìž¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ ê°€ì„¤ì„ ì„ íƒí•©ë‹ˆë‹¤. ì´ëŠ” ì´ˆê¸° í† í°ì˜ í™•ë¥ ì´ ë‚®ì•„ íƒìš• íƒìƒ‰ì— ì˜í•´ ë¬´ì‹œë˜ì—ˆì„ ë†’ì€ í™•ë¥ ì˜ ì‹œí€€ìŠ¤ë¥¼ ì‹ë³„í•  ìˆ˜ ìžˆëŠ” ìž¥ì ì„ ê°€ì§‘ë‹ˆë‹¤.
- `do_sample`: ì´ ë§¤ê°œë³€ìˆ˜ë¥¼ `True`ë¡œ ì„¤ì •í•˜ë©´, ë‹¤í•­ ìƒ˜í”Œë§, ë¹” íƒìƒ‰ ë‹¤í•­ ìƒ˜í”Œë§, Top-K ìƒ˜í”Œë§ ë° Top-p ìƒ˜í”Œë§ê³¼ ê°™ì€ ë””ì½”ë”© ì „ëžµì„ í™œì„±í™”í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì „ëžµë“¤ì€ ì „ì²´ ì–´íœ˜ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ì—ì„œ ë‹¤ìŒ í† í°ì„ ì„ íƒí•˜ë©°, ì „ëžµë³„ë¡œ íŠ¹ì • ì¡°ì •ì´ ì ìš©ë©ë‹ˆë‹¤.
- `num_return_sequences`: ê° ìž…ë ¥ì— ëŒ€í•´ ë°˜í™˜í•  ì‹œí€€ìŠ¤ í›„ë³´ì˜ ìˆ˜ìž…ë‹ˆë‹¤. ì´ ì˜µì…˜ì€ ë¹” íƒìƒ‰(beam search)ì˜ ë³€í˜•ê³¼ ìƒ˜í”Œë§ê³¼ ê°™ì´ ì—¬ëŸ¬ ì‹œí€€ìŠ¤ í›„ë³´ë¥¼ ì§€ì›í•˜ëŠ” ë””ì½”ë”© ì „ëžµì—ë§Œ ì‚¬ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. íƒìš• íƒìƒ‰(greedy search) ê°™ì€ ë””ì½”ë”© ì „ëžµì€ ë‹¨ì¼ ì¶œë ¥ ì‹œí€€ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

## ëª¨ë¸ì— ì‚¬ìš©ìž ì •ì˜ ë””ì½”ë”© ì „ëžµ ì €ìž¥[[save-a-custom-decoding-strategy-with-your-model]]

íŠ¹ì • ìƒì„± ì„¤ì •ì„ ê°€ì§„ ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì„ ê³µìœ í•˜ê³ ìž í•  ë•Œ, ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¥¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤:
* [`GenerationConfig`] í´ëž˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
* ë””ì½”ë”© ì „ëžµ íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
* ìƒì„± ì„¤ì •ì„ [`GenerationConfig.save_pretrained`]ë¥¼ ì‚¬ìš©í•˜ì—¬ ì €ìž¥í•˜ë©°, `config_file_name` ì¸ìžëŠ” ë¹„ì›Œë‘¡ë‹ˆë‹¤.
* ëª¨ë¸ì˜ ì €ìž¥ì†Œì— ì„¤ì •ì„ ì—…ë¡œë“œí•˜ê¸° ìœ„í•´ `push_to_hub`ë¥¼ `True`ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.

```python
>>> from transformers import AutoModelForCausalLM, GenerationConfig

>>> model = AutoModelForCausalLM.from_pretrained("my_account/my_model")  # doctest: +SKIP
>>> generation_config = GenerationConfig(
...     max_new_tokens=50, do_sample=True, top_k=50, eos_token_id=model.config.eos_token_id
... )
>>> generation_config.save_pretrained("my_account/my_model", push_to_hub=True)  # doctest: +SKIP
```

ë‹¨ì¼ ë””ë ‰í† ë¦¬ì— ì—¬ëŸ¬ ìƒì„± ì„¤ì •ì„ ì €ìž¥í•  ìˆ˜ ìžˆìœ¼ë©°, ì´ë•Œ [`GenerationConfig.save_pretrained`]ì˜ `config_file_name` ì¸ìžë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ë‚˜ì¤‘ì— [`GenerationConfig.from_pretrained`]ë¡œ ì´ë“¤ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ì´ëŠ” ë‹¨ì¼ ëª¨ë¸ì— ëŒ€í•´ ì—¬ëŸ¬ ìƒì„± ì„¤ì •ì„ ì €ìž¥í•˜ê³  ì‹¶ì„ ë•Œ ìœ ìš©í•©ë‹ˆë‹¤(ì˜ˆ: ìƒ˜í”Œë§ì„ ì´ìš©í•œ ì°½ì˜ì  í…ìŠ¤íŠ¸ ìƒì„±ì„ ìœ„í•œ í•˜ë‚˜, ë¹” íƒìƒ‰ì„ ì´ìš©í•œ ìš”ì•½ì„ ìœ„í•œ ë‹¤ë¥¸ í•˜ë‚˜ ë“±). ëª¨ë¸ì— ì„¤ì • íŒŒì¼ì„ ì¶”ê°€í•˜ê¸° ìœ„í•´ ì ì ˆí•œ Hub ê¶Œí•œì„ ê°€ì§€ê³  ìžˆì–´ì•¼ í•©ë‹ˆë‹¤.

```python
>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig

>>> tokenizer = AutoTokenizer.from_pretrained("google-t5/t5-small")
>>> model = AutoModelForSeq2SeqLM.from_pretrained("google-t5/t5-small")

>>> translation_generation_config = GenerationConfig(
...     num_beams=4,
...     early_stopping=True,
...     decoder_start_token_id=0,
...     eos_token_id=model.config.eos_token_id,
...     pad_token=model.config.pad_token_id,
... )

>>> # íŒ: Hubì— pushí•˜ë ¤ë©´ `push_to_hub=True`ë¥¼ ì¶”ê°€
>>> translation_generation_config.save_pretrained("/tmp", "translation_generation_config.json")

>>> # ëª…ëª…ëœ ìƒì„± ì„¤ì • íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ ìƒì„±ì„ ë§¤ê°œë³€ìˆ˜í™”í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
>>> generation_config = GenerationConfig.from_pretrained("/tmp", "translation_generation_config.json")
>>> inputs = tokenizer("translate English to French: Configuration files are easy to use!", return_tensors="pt")
>>> outputs = model.generate(**inputs, generation_config=generation_config)
>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
['Les fichiers de configuration sont faciles Ã  utiliser!']
```

## ìŠ¤íŠ¸ë¦¬ë°[[streaming]]

`generate()` ë©”ì†Œë“œëŠ” `streamer` ìž…ë ¥ì„ í†µí•´ ìŠ¤íŠ¸ë¦¬ë°ì„ ì§€ì›í•©ë‹ˆë‹¤. `streamer` ìž…ë ¥ì€ `put()`ê³¼ `end()` ë©”ì†Œë“œë¥¼ ê°€ì§„ í´ëž˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ì™€ í˜¸í™˜ë©ë‹ˆë‹¤. ë‚´ë¶€ì ìœ¼ë¡œ, `put()`ì€ ìƒˆ í† í°ì„ ì¶”ê°€í•˜ëŠ” ë° ì‚¬ìš©ë˜ë©°, `end()`ëŠ” í…ìŠ¤íŠ¸ ìƒì„±ì˜ ëì„ í‘œì‹œí•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.

> [!WARNING]
> ìŠ¤íŠ¸ë¦¬ë¨¸ í´ëž˜ìŠ¤ì˜ APIëŠ” ì•„ì§ ê°œë°œ ì¤‘ì´ë©°, í–¥í›„ ë³€ê²½ë  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

ì‹¤ì œë¡œ ë‹¤ì–‘í•œ ëª©ì ì„ ìœ„í•´ ìžì²´ ìŠ¤íŠ¸ë¦¬ë° í´ëž˜ìŠ¤ë¥¼ ë§Œë“¤ ìˆ˜ ìžˆìŠµë‹ˆë‹¤! ë˜í•œ, ê¸°ë³¸ì ì¸ ìŠ¤íŠ¸ë¦¬ë° í´ëž˜ìŠ¤ë“¤ë„ ì¤€ë¹„ë˜ì–´ ìžˆì–´ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, [`TextStreamer`] í´ëž˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ `generate()`ì˜ ì¶œë ¥ì„ í™”ë©´ì— í•œ ë‹¨ì–´ì”© ìŠ¤íŠ¸ë¦¬ë°í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤:

```python
>>> from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer

>>> tok = AutoTokenizer.from_pretrained("openai-community/gpt2")
>>> model = AutoModelForCausalLM.from_pretrained("openai-community/gpt2")
>>> inputs = tok(["An increasing sequence: one,"], return_tensors="pt")
>>> streamer = TextStreamer(tok)

>>> # ìŠ¤íŠ¸ë¦¬ë¨¸ëŠ” í‰ì†Œì™€ ê°™ì€ ì¶œë ¥ê°’ì„ ë°˜í™˜í•  ë¿ë§Œ ì•„ë‹ˆë¼ ìƒì„±ëœ í…ìŠ¤íŠ¸ë„ í‘œì¤€ ì¶œë ¥(stdout)ìœ¼ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.
>>> _ = model.generate(**inputs, streamer=streamer, max_new_tokens=20)
An increasing sequence: one, two, three, four, five, six, seven, eight, nine, ten, eleven,
```

## ë””ì½”ë”© ì „ëžµ[[decoding-strategies]]

`generate()` ë§¤ê°œë³€ìˆ˜ì™€ ê¶ê·¹ì ìœ¼ë¡œ `generation_config`ì˜ íŠ¹ì • ì¡°í•©ì„ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ë””ì½”ë”© ì „ëžµì„ í™œì„±í™”í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ì´ ê°œë…ì´ ì²˜ìŒì´ë¼ë©´, í”ížˆ ì‚¬ìš©ë˜ëŠ” ë””ì½”ë”© ì „ëžµì´ ì–´ë–»ê²Œ ìž‘ë™í•˜ëŠ”ì§€ ì„¤ëª…í•˜ëŠ” [ì´ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸](https://huggingface.co/blog/how-to-generate)ë¥¼ ì½ì–´ë³´ëŠ” ê²ƒì„ ì¶”ì²œí•©ë‹ˆë‹¤.

ì—¬ê¸°ì„œëŠ” ë””ì½”ë”© ì „ëžµì„ ì œì–´í•˜ëŠ” ëª‡ ê°€ì§€ ë§¤ê°œë³€ìˆ˜ë¥¼ ë³´ì—¬ì£¼ê³ , ì´ë¥¼ ì–´ë–»ê²Œ ì‚¬ìš©í•  ìˆ˜ ìžˆëŠ”ì§€ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤.

### íƒìš• íƒìƒ‰(Greedy Search)[[greedy-search]]

[`generate`]ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ íƒìš• íƒìƒ‰ ë””ì½”ë”©ì„ ì‚¬ìš©í•˜ë¯€ë¡œ ì´ë¥¼ í™œì„±í™”í•˜ê¸° ìœ„í•´ ë³„ë„ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ì§€ì •í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. ì´ëŠ” `num_beams`ê°€ 1ë¡œ ì„¤ì •ë˜ê³  `do_sample=False`ë¡œ ë˜ì–´ ìžˆë‹¤ëŠ” ì˜ë¯¸ìž…ë‹ˆë‹¤."

```python
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> prompt = "I look forward to"
>>> checkpoint = "distilbert/distilgpt2"

>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
>>> inputs = tokenizer(prompt, return_tensors="pt")

>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)
>>> outputs = model.generate(**inputs)
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
['I look forward to seeing you all again!\n\n\n\n\n\n\n\n\n\n\n']
```

### ë‹¤í•­ ìƒ˜í”Œë§(Multinomial sampling)[[multinomial-sampling]]

íƒìš• íƒìƒ‰(greedy search)ì´ í•­ìƒ ê°€ìž¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í† í°ì„ ë‹¤ìŒ í† í°ìœ¼ë¡œ ì„ íƒí•˜ëŠ” ê²ƒê³¼ ë‹¬ë¦¬, ë‹¤í•­ ìƒ˜í”Œë§(multinomial sampling, ì¡°ìƒ ìƒ˜í”Œë§(ancestral sampling)ì´ë¼ê³ ë„ í•¨)ì€ ëª¨ë¸ì´ ì œê³µí•˜ëŠ” ì „ì²´ ì–´íœ˜ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ í† í°ì„ ë¬´ìž‘ìœ„ë¡œ ì„ íƒí•©ë‹ˆë‹¤. 0ì´ ì•„ë‹Œ í™•ë¥ ì„ ê°€ì§„ ëª¨ë“  í† í°ì€ ì„ íƒë  ê¸°íšŒê°€ ìžˆìœ¼ë¯€ë¡œ, ë°˜ë³µì˜ ìœ„í—˜ì„ ì¤„ì¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

ë‹¤í•­ ìƒ˜í”Œë§ì„ í™œì„±í™”í•˜ë ¤ë©´ `do_sample=True` ë° `num_beams=1`ì„ ì„¤ì •í•˜ì„¸ìš”.

```python
>>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed
>>> set_seed(0)  # ìž¬í˜„ì„±ì„ ìœ„í•´

>>> checkpoint = "openai-community/gpt2-large"
>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)

>>> prompt = "Today was an amazing day because"
>>> inputs = tokenizer(prompt, return_tensors="pt")

>>> outputs = model.generate(**inputs, do_sample=True, num_beams=1, max_new_tokens=100)
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
['Today was an amazing day because when you go to the World Cup and you don\'t, or when you don\'t get invited,
that\'s a terrible feeling."']
```

### ë¹” íƒìƒ‰(Beam-search) ë””ì½”ë”©[[beam-search-decoding]]

íƒìš• ê²€ìƒ‰(greedy search)ê³¼ ë‹¬ë¦¬, ë¹” íƒìƒ‰(beam search) ë””ì½”ë”©ì€ ê° ì‹œê°„ ë‹¨ê³„ì—ì„œ ì—¬ëŸ¬ ê°€ì„¤ì„ ìœ ì§€í•˜ê³  ê²°êµ­ ì „ì²´ ì‹œí€€ìŠ¤ì— ëŒ€í•´ ê°€ìž¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ ê°€ì„¤ì„ ì„ íƒí•©ë‹ˆë‹¤. ì´ëŠ” ë‚®ì€ í™•ë¥ ì˜ ì´ˆê¸° í† í°ìœ¼ë¡œ ì‹œìž‘í•˜ê³  ê·¸ë¦¬ë”” ê²€ìƒ‰ì—ì„œ ë¬´ì‹œë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ ë†’ì€ ì‹œí€€ìŠ¤ë¥¼ ì‹ë³„í•˜ëŠ” ì´ì ì´ ìžˆìŠµë‹ˆë‹¤.

ì´ ë””ì½”ë”© ì „ëžµì„ í™œì„±í™”í•˜ë ¤ë©´ `num_beams` (ì¶”ì í•  ê°€ì„¤ ìˆ˜ë¼ê³ ë„ í•¨)ë¥¼ 1ë³´ë‹¤ í¬ê²Œ ì§€ì •í•˜ì„¸ìš”.

```python
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> prompt = "It is astonishing how one can"
>>> checkpoint = "openai-community/gpt2-medium"

>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
>>> inputs = tokenizer(prompt, return_tensors="pt")

>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)

>>> outputs = model.generate(**inputs, num_beams=5, max_new_tokens=50)
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
['It is astonishing how one can have such a profound impact on the lives of so many people in such a short period of
time."\n\nHe added: "I am very proud of the work I have been able to do in the last few years.\n\n"I have']
```

### ë¹” íƒìƒ‰ ë‹¤í•­ ìƒ˜í”Œë§(Beam-search multinomial sampling)[[beam-search-multinomial-sampling]]

ì´ ë””ì½”ë”© ì „ëžµì€ ì´ë¦„ì—ì„œ ì•Œ ìˆ˜ ìžˆë“¯ì´ ë¹” íƒìƒ‰ê³¼ ë‹¤í•­ ìƒ˜í”Œë§ì„ ê²°í•©í•œ ê²ƒìž…ë‹ˆë‹¤. ì´ ë””ì½”ë”© ì „ëžµì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” `num_beams`ë¥¼ 1ë³´ë‹¤ í° ê°’ìœ¼ë¡œ ì„¤ì •í•˜ê³ , `do_sample=True`ë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.

```python
>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, set_seed
>>> set_seed(0)  # ìž¬í˜„ì„±ì„ ìœ„í•´

>>> prompt = "translate English to German: The house is wonderful."
>>> checkpoint = "google-t5/t5-small"

>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
>>> inputs = tokenizer(prompt, return_tensors="pt")

>>> model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)

>>> outputs = model.generate(**inputs, num_beams=5, do_sample=True)
>>> tokenizer.decode(outputs[0], skip_special_tokens=True)
'Das Haus ist wunderbar.'
```

### ì¶”ë¡  ë””ì½”ë”©(Speculative Decoding)[[speculative-decoding]]

ì¶”ë¡  ë””ì½”ë”©(ë³´ì¡° ë””ì½”ë”©(assisted decoding)ìœ¼ë¡œë„ ì•Œë ¤ì§)ì€ ë™ì¼í•œ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ëŠ” í›¨ì”¬ ìž‘ì€ ë³´ì¡° ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ëª‡ ê°€ì§€ í›„ë³´ í† í°ì„ ìƒì„±í•˜ëŠ” ìƒìœ„ ëª¨ë¸ì˜ ë””ì½”ë”© ì „ëžµì„ ìˆ˜ì •í•œ ê²ƒìž…ë‹ˆë‹¤. ì£¼ ëª¨ë¸ì€ ë‹¨ì¼ ì „ë°© í†µê³¼ë¡œ í›„ë³´ í† í°ì„ ê²€ì¦í•¨ìœ¼ë¡œì¨ ë””ì½”ë”© ê³¼ì •ì„ ê°€ì†í™”í•©ë‹ˆë‹¤. `do_sample=True`ì¼ ê²½ìš°, [ì¶”ë¡  ë””ì½”ë”© ë…¼ë¬¸](https://huggingface.co/papers/2211.17192)ì— ì†Œê°œëœ í† í° ê²€ì¦ê³¼ ìž¬ìƒ˜í”Œë§ ë°©ì‹ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.

í˜„ìž¬, íƒìš• ê²€ìƒ‰(greedy search)ê³¼ ìƒ˜í”Œë§ë§Œì´ ì§€ì›ë˜ëŠ” ë³´ì¡° ë””ì½”ë”©(assisted decoding) ê¸°ëŠ¥ì„ í†µí•´, ë³´ì¡° ë””ì½”ë”©ì€ ë°°ì¹˜ ìž…ë ¥ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë³´ì¡° ë””ì½”ë”©ì— ëŒ€í•´ ë” ì•Œê³  ì‹¶ë‹¤ë©´, [ì´ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸](https://huggingface.co/blog/assisted-generation)ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.

ë³´ì¡° ë””ì½”ë”©ì„ í™œì„±í™”í•˜ë ¤ë©´ ëª¨ë¸ê³¼ í•¨ê»˜ `assistant_model` ì¸ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”.

```python
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> prompt = "Alice and Bob"
>>> checkpoint = "EleutherAI/pythia-1.4b-deduped"
>>> assistant_checkpoint = "EleutherAI/pythia-160m-deduped"

>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
>>> inputs = tokenizer(prompt, return_tensors="pt")

>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)
>>> assistant_model = AutoModelForCausalLM.from_pretrained(assistant_checkpoint)
>>> outputs = model.generate(**inputs, assistant_model=assistant_model)
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
['Alice and Bob are sitting in a bar. Alice is drinking a beer and Bob is drinking a']
```

ìƒ˜í”Œë§ ë°©ë²•ê³¼ í•¨ê»˜ ë³´ì¡° ë””ì½”ë”©ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ë‹¤í•­ ìƒ˜í”Œë§ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ `temperature` ì¸ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬´ìž‘ìœ„ì„±ì„ ì œì–´í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë³´ì¡° ë””ì½”ë”©ì—ì„œëŠ” `temperature`ë¥¼ ë‚®ì¶”ë©´ ëŒ€ê¸° ì‹œê°„ì„ ê°œì„ í•˜ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

```python
>>> from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed
>>> set_seed(42)  # ìž¬í˜„ì„±ì„ ìœ„í•´

>>> prompt = "Alice and Bob"
>>> checkpoint = "EleutherAI/pythia-1.4b-deduped"
>>> assistant_checkpoint = "EleutherAI/pythia-160m-deduped"

>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
>>> inputs = tokenizer(prompt, return_tensors="pt")

>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)
>>> assistant_model = AutoModelForCausalLM.from_pretrained(assistant_checkpoint)
>>> outputs = model.generate(**inputs, assistant_model=assistant_model, do_sample=True, temperature=0.5)
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
['Alice and Bob, who were both in their early twenties, were both in the process of']
```
