<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# ìŠ¤í¬ë¦½íŠ¸ë¡œ ì‹¤í–‰í•˜ê¸°[[train-with-a-script]]

ğŸ¤— Transformers ë…¸íŠ¸ë¶ê³¼ í•¨ê»˜ [PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch), [TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow), ë˜ëŠ” [JAX/Flax](https://github.com/huggingface/transformers/tree/main/examples/flax)ë¥¼ ì‚¬ìš©í•´ íŠ¹ì • íƒœìŠ¤í¬ì— ëŒ€í•œ ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì£¼ëŠ” ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸ë„ ìˆìŠµë‹ˆë‹¤.

ë˜í•œ [ì—°êµ¬ í”„ë¡œì íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/research_projects) ë° [ë ˆê±°ì‹œ ì˜ˆì œ](https://github.com/huggingface/transformers/tree/main/examples/legacy)ì—ì„œ ëŒ€ë¶€ë¶„ ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ì œê³µí•œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
ì´ëŸ¬í•œ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì ê·¹ì ìœ¼ë¡œ ìœ ì§€ ê´€ë¦¬ë˜ì§€ ì•Šìœ¼ë©° ìµœì‹  ë²„ì „ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ í˜¸í™˜ë˜ì§€ ì•Šì„ ê°€ëŠ¥ì„±ì´ ë†’ì€ íŠ¹ì • ë²„ì „ì˜ ğŸ¤— Transformersë¥¼ í•„ìš”ë¡œ í•©ë‹ˆë‹¤.

ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸ê°€ ëª¨ë“  ë¬¸ì œì—ì„œ ë°”ë¡œ ì‘ë™í•˜ëŠ” ê²ƒì€ ì•„ë‹ˆë©°, í•´ê²°í•˜ë ¤ëŠ” ë¬¸ì œì— ë§ê²Œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë³€ê²½í•´ì•¼ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
ì´ë¥¼ ìœ„í•´ ëŒ€ë¶€ë¶„ì˜ ìŠ¤í¬ë¦½íŠ¸ì—ëŠ” ë°ì´í„° ì „ì²˜ë¦¬ ë°©ë²•ì´ ë‚˜ì™€ìˆì–´ í•„ìš”ì— ë”°ë¼ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸ì— êµ¬í˜„í•˜ê³  ì‹¶ì€ ê¸°ëŠ¥ì´ ìˆìœ¼ë©´ pull requestë¥¼ ì œì¶œí•˜ê¸° ì „ì— [í¬ëŸ¼](https://discuss.huggingface.co/) ë˜ëŠ” [ì´ìŠˆ](https://github.com/huggingface/transformers/issues)ì—ì„œ ë…¼ì˜í•´ ì£¼ì„¸ìš”.
ë²„ê·¸ ìˆ˜ì •ì€ í™˜ì˜í•˜ì§€ë§Œ ê°€ë…ì„±ì„ í¬ìƒí•˜ë©´ì„œê¹Œì§€ ë” ë§ì€ ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ëŠ” pull requestëŠ” ë³‘í•©(merge)í•˜ì§€ ì•Šì„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.

ì´ ê°€ì´ë“œì—ì„œëŠ” [PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization) ë° [TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/summarization)ì—ì„œ ìš”ì•½ í›ˆë ¨í•˜ëŠ”
 ìŠ¤í¬ë¦½íŠ¸ ì˜ˆì œë¥¼ ì‹¤í–‰í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.
íŠ¹ë³„í•œ ì„¤ëª…ì´ ì—†ëŠ” í•œ ëª¨ë“  ì˜ˆì œëŠ” ë‘ í”„ë ˆì„ì›Œí¬ ëª¨ë‘ì—ì„œ ì‘ë™í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.

## ì„¤ì •í•˜ê¸°[[setup]]

ìµœì‹  ë²„ì „ì˜ ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰í•˜ë ¤ë©´ ìƒˆ ê°€ìƒ í™˜ê²½ì—ì„œ **ì†ŒìŠ¤ë¡œë¶€í„° ğŸ¤— Transformersë¥¼ ì„¤ì¹˜**í•´ì•¼ í•©ë‹ˆë‹¤:

```bash
git clone https://github.com/huggingface/transformers
cd transformers
pip install .
```

ì´ì „ ë²„ì „ì˜ ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë³´ë ¤ë©´ ì•„ë˜ í† ê¸€ì„ í´ë¦­í•˜ì„¸ìš”:

<details>
  <summary>ì´ì „ ë²„ì „ì˜ ğŸ¤— Transformers ì˜ˆì œ</summary>
	<ul>
		<li><a href="https://github.com/huggingface/transformers/tree/v4.5.1/examples">v4.5.1</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v4.4.2/examples">v4.4.2</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v4.3.3/examples">v4.3.3</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v4.2.2/examples">v4.2.2</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v4.1.1/examples">v4.1.1</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v4.0.1/examples">v4.0.1</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v3.5.1/examples">v3.5.1</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v3.4.0/examples">v3.4.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v3.3.1/examples">v3.3.1</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v3.2.0/examples">v3.2.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v3.1.0/examples">v3.1.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v3.0.2/examples">v3.0.2</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.11.0/examples">v2.11.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.10.0/examples">v2.10.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.9.1/examples">v2.9.1</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.8.0/examples">v2.8.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.7.0/examples">v2.7.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.6.0/examples">v2.6.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.5.1/examples">v2.5.1</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.4.0/examples">v2.4.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.3.0/examples">v2.3.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.2.0/examples">v2.2.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.1.0/examples">v2.1.1</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.0.0/examples">v2.0.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v1.2.0/examples">v1.2.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v1.1.0/examples">v1.1.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v1.0.0/examples">v1.0.0</a></li>
	</ul>
</details>

ê·¸ë¦¬ê³  ë‹¤ìŒê³¼ ê°™ì´ ë³µì œ(clone)í•´ì˜¨ ğŸ¤— Transformers ë²„ì „ì„ íŠ¹ì • ë²„ì „(ì˜ˆ: v3.5.1)ìœ¼ë¡œ ì „í™˜í•˜ì„¸ìš”:

```bash
git checkout tags/v3.5.1
```

ì˜¬ë°”ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ì„ ì„¤ì •í•œ í›„ ì›í•˜ëŠ” ì˜ˆì œ í´ë”ë¡œ ì´ë™í•˜ì—¬ ì˜ˆì œë³„ë¡œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ëŒ€í•œ ìš”êµ¬ ì‚¬í•­(requirements)ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤:

```bash
pip install -r requirements.txt
```

## ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰í•˜ê¸°[[run-a-script]]

<frameworkcontent>
<pt>
ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸ëŠ” ğŸ¤— [Datasets](https://huggingface.co/docs/datasets/) ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
ê·¸ëŸ° ë‹¤ìŒ ìŠ¤í¬ë¦½íŠ¸ëŠ” ìš”ì•½ ê¸°ëŠ¥ì„ ì§€ì›í•˜ëŠ” ì•„í‚¤í…ì²˜ì—ì„œ [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë¯¸ì„¸ ì¡°ì •í•©ë‹ˆë‹¤.
ë‹¤ìŒ ì˜ˆëŠ” [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail) ë°ì´í„° ì„¸íŠ¸ì—ì„œ [T5-small](https://huggingface.co/google-t5/t5-small)ì„ ë¯¸ì„¸ ì¡°ì •í•©ë‹ˆë‹¤.
T5 ëª¨ë¸ì€ í›ˆë ¨ ë°©ì‹ì— ë”°ë¼ ì¶”ê°€ `source_prefix` ì¸ìˆ˜ê°€ í•„ìš”í•˜ë©°, ì´ í”„ë¡¬í”„íŠ¸ëŠ” ìš”ì•½ ì‘ì—…ì„ì„ T5ì— ì•Œë ¤ì¤ë‹ˆë‹¤.

```bash
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```
</pt>
<tf>
ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸ëŠ” ğŸ¤— [Datasets](https://huggingface.co/docs/datasets/) ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
ê·¸ëŸ° ë‹¤ìŒ ìŠ¤í¬ë¦½íŠ¸ëŠ” ìš”ì•½ ê¸°ëŠ¥ì„ ì§€ì›í•˜ëŠ” ì•„í‚¤í…ì²˜ì—ì„œ Kerasë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë¯¸ì„¸ ì¡°ì •í•©ë‹ˆë‹¤. 
ë‹¤ìŒ ì˜ˆëŠ” [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail) ë°ì´í„° ì„¸íŠ¸ì—ì„œ [T5-small](https://huggingface.co/google-t5/t5-small)ì„ ë¯¸ì„¸ ì¡°ì •í•©ë‹ˆë‹¤.
T5 ëª¨ë¸ì€ í›ˆë ¨ ë°©ì‹ì— ë”°ë¼ ì¶”ê°€ `source_prefix` ì¸ìˆ˜ê°€ í•„ìš”í•˜ë©°, ì´ í”„ë¡¬í”„íŠ¸ëŠ” ìš”ì•½ ì‘ì—…ì„ì„ T5ì— ì•Œë ¤ì¤ë‹ˆë‹¤.
```bash
python examples/tensorflow/summarization/run_summarization.py  \
    --model_name_or_path google-t5/t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --output_dir /tmp/tst-summarization  \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 16 \
    --num_train_epochs 3 \
    --do_train \
    --do_eval
```
</tf>
</frameworkcontent>

## í˜¼í•© ì •ë°€ë„(mixed precision)ë¡œ ë¶„ì‚° í›ˆë ¨í•˜ê¸°[[distributed-training-and-mixed-precision]]

[Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) í´ë˜ìŠ¤ëŠ” ë¶„ì‚° í›ˆë ¨ê³¼ í˜¼í•© ì •ë°€ë„(mixed precision)ë¥¼ ì§€ì›í•˜ë¯€ë¡œ ìŠ¤í¬ë¦½íŠ¸ì—ì„œë„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì´ ë‘ ê°€ì§€ ê¸°ëŠ¥ì„ ëª¨ë‘ í™œì„±í™”í•˜ë ¤ë©´ ë‹¤ìŒ ë‘ ê°€ì§€ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤:

- `fp16` ì¸ìˆ˜ë¥¼ ì¶”ê°€í•´ í˜¼í•© ì •ë°€ë„(mixed precision)ë¥¼ í™œì„±í™”í•©ë‹ˆë‹¤.
- `nproc_per_node` ì¸ìˆ˜ë¥¼ ì¶”ê°€í•´ ì‚¬ìš©í•  GPU ê°œìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

```bash
torchrun \
    --nproc_per_node 8 pytorch/summarization/run_summarization.py \
    --fp16 \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

TensorFlow ìŠ¤í¬ë¦½íŠ¸ëŠ” ë¶„ì‚° í›ˆë ¨ì„ ìœ„í•´ [`MirroredStrategy`](https://www.tensorflow.org/guide/distributed_training#mirroredstrategy)ë¥¼ í™œìš©í•˜ë©°, í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸ì— ì¸ìˆ˜ë¥¼ ì¶”ê°€í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.
ë‹¤ì¤‘ GPU í™˜ê²½ì´ë¼ë©´, TensorFlow ìŠ¤í¬ë¦½íŠ¸ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì—¬ëŸ¬ ê°œì˜ GPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

## TPU ìœ„ì—ì„œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰í•˜ê¸°[[run-a-script-on-a-tpu]]

<frameworkcontent>
<pt>
Tensor Processing Units (TPUs)ëŠ” ì„±ëŠ¥ì„ ê°€ì†í™”í•˜ê¸° ìœ„í•´ íŠ¹ë³„íˆ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.
PyTorchëŠ” [XLA](https://www.tensorflow.org/xla) ë”¥ëŸ¬ë‹ ì»´íŒŒì¼ëŸ¬ì™€ í•¨ê»˜ TPUë¥¼ ì§€ì›í•©ë‹ˆë‹¤(ìì„¸í•œ ë‚´ìš©ì€ [ì—¬ê¸°](https://github.com/pytorch/xla/blob/master/README.md) ì°¸ì¡°). 
TPUë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ `xla_spawn.py` ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ê³  `num_cores` ì¸ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©í•˜ë ¤ëŠ” TPU ì½”ì–´ ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

```bash
python xla_spawn.py --num_cores 8 \
    summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```
</pt>
<tf>
Tensor Processing Units (TPUs)ëŠ” ì„±ëŠ¥ì„ ê°€ì†í™”í•˜ê¸° ìœ„í•´ íŠ¹ë³„íˆ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.
TensorFlow ìŠ¤í¬ë¦½íŠ¸ëŠ” TPUë¥¼ í›ˆë ¨ì— ì‚¬ìš©í•˜ê¸° ìœ„í•´ [`TPUStrategy`](https://www.tensorflow.org/guide/distributed_training#tpustrategy)ë¥¼ í™œìš©í•©ë‹ˆë‹¤.
TPUë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ TPU ë¦¬ì†ŒìŠ¤ì˜ ì´ë¦„ì„ `tpu` ì¸ìˆ˜ì— ì „ë‹¬í•©ë‹ˆë‹¤.

```bash
python run_summarization.py  \
    --tpu name_of_tpu_resource \
    --model_name_or_path google-t5/t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --output_dir /tmp/tst-summarization  \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 16 \
    --num_train_epochs 3 \
    --do_train \
    --do_eval
```
</tf>
</frameworkcontent>

## ğŸ¤— Accelerateë¡œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰í•˜ê¸°[[run-a-script-with-accelerate]]

ğŸ¤— [Accelerate](https://huggingface.co/docs/accelerate)ëŠ” PyTorch í›ˆë ¨ ê³¼ì •ì— ëŒ€í•œ ì™„ì „í•œ ê°€ì‹œì„±ì„ ìœ ì§€í•˜ë©´ì„œ ì—¬ëŸ¬ ìœ í˜•ì˜ ì„¤ì •(CPU ì „ìš©, ë‹¤ì¤‘ GPU, TPU)ì—ì„œ ëª¨ë¸ì„ í›ˆë ¨í•  ìˆ˜ ìˆëŠ” í†µí•© ë°©ë²•ì„ ì œê³µí•˜ëŠ” PyTorch ì „ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.
ğŸ¤— Accelerateê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:

> ì°¸ê³ : AccelerateëŠ” ë¹ ë¥´ê²Œ ê°œë°œ ì¤‘ì´ë¯€ë¡œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ accelerateë¥¼ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
```bash
pip install git+https://github.com/huggingface/accelerate
```

`run_summarization.py` ìŠ¤í¬ë¦½íŠ¸ ëŒ€ì‹  `run_summarization_no_trainer.py` ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
ğŸ¤— Accelerate í´ë˜ìŠ¤ê°€ ì§€ì›ë˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ëŠ” í´ë”ì— `task_no_trainer.py` íŒŒì¼ì´ ìˆìŠµë‹ˆë‹¤.
ë‹¤ìŒ ëª…ë ¹ì„ ì‹¤í–‰í•˜ì—¬ êµ¬ì„± íŒŒì¼ì„ ìƒì„±í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤:
```bash
accelerate config
```

ì„¤ì •ì„ í…ŒìŠ¤íŠ¸í•˜ì—¬ ì˜¬ë°”ë¥´ê²Œ êµ¬ì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤:

```bash
accelerate test
```

ì´ì œ í›ˆë ¨ì„ ì‹œì‘í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤:

```bash
accelerate launch run_summarization_no_trainer.py \
    --model_name_or_path google-t5/t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir ~/tmp/tst-summarization
```

## ì‚¬ìš©ì ì •ì˜ ë°ì´í„° ì„¸íŠ¸ ì‚¬ìš©í•˜ê¸°[[use-a-custom-dataset]]

ìš”ì•½ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì‚¬ìš©ì ì§€ì • ë°ì´í„° ì„¸íŠ¸ê°€ CSV ë˜ëŠ” JSON íŒŒì¼ì¸ ê²½ìš° ì§€ì›í•©ë‹ˆë‹¤.
ì‚¬ìš©ì ì§€ì • ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ì—ëŠ” ëª‡ ê°€ì§€ ì¶”ê°€ ì¸ìˆ˜ë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤:

- `train_file`ê³¼ `validation_file`ì€ í›ˆë ¨ ë° ê²€ì¦ íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
- `text_column`ì€ ìš”ì•½í•  ì…ë ¥ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
- `summary_column`ì€ ì¶œë ¥í•  ëŒ€ìƒ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ì§€ì • ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ” ìš”ì•½ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

```bash
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --train_file path_to_csv_or_jsonlines_file \
    --validation_file path_to_csv_or_jsonlines_file \
    --text_column text_column_name \
    --summary_column summary_column_name \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --overwrite_output_dir \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --predict_with_generate
```

## ìŠ¤í¬ë¦½íŠ¸ í…ŒìŠ¤íŠ¸í•˜ê¸°[[test-a-script]]

ì „ì²´ ë°ì´í„° ì„¸íŠ¸ë¥¼ ëŒ€ìƒìœ¼ë¡œ í›ˆë ¨ì„ ì™„ë£Œí•˜ëŠ”ë° ê½¤ ì˜¤ëœ ì‹œê°„ì´ ê±¸ë¦¬ê¸° ë•Œë¬¸ì—, ì‘ì€ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ëª¨ë“  ê²ƒì´ ì˜ˆìƒëŒ€ë¡œ ì‹¤í–‰ë˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

ë‹¤ìŒ ì¸ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì„¸íŠ¸ë¥¼ ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ë¡œ ì˜ë¼ëƒ…ë‹ˆë‹¤:
- `max_train_samples`
- `max_eval_samples`
- `max_predict_samples`

```bash
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --max_train_samples 50 \
    --max_eval_samples 50 \
    --max_predict_samples 50 \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

ëª¨ë“  ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸ê°€ `max_predict_samples` ì¸ìˆ˜ë¥¼ ì§€ì›í•˜ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤.
ìŠ¤í¬ë¦½íŠ¸ê°€ ì´ ì¸ìˆ˜ë¥¼ ì§€ì›í•˜ëŠ”ì§€ í™•ì‹¤í•˜ì§€ ì•Šì€ ê²½ìš° `-h` ì¸ìˆ˜ë¥¼ ì¶”ê°€í•˜ì—¬ í™•ì¸í•˜ì„¸ìš”:

```bash
examples/pytorch/summarization/run_summarization.py -h
```

## ì²´í¬í¬ì¸íŠ¸(checkpoint)ì—ì„œ í›ˆë ¨ ì´ì–´ì„œ í•˜ê¸°[[resume-training-from-checkpoint]]

ë˜ ë‹¤ë¥¸ ìœ ìš©í•œ ì˜µì…˜ì€ ì´ì „ ì²´í¬í¬ì¸íŠ¸ì—ì„œ í›ˆë ¨ì„ ì¬ê°œí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. 
ì´ë ‡ê²Œ í•˜ë©´ í›ˆë ¨ì´ ì¤‘ë‹¨ë˜ë”ë¼ë„ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘í•˜ì§€ ì•Šê³  ì¤‘ë‹¨í•œ ë¶€ë¶„ë¶€í„° ë‹¤ì‹œ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì²´í¬í¬ì¸íŠ¸ì—ì„œ í›ˆë ¨ì„ ì¬ê°œí•˜ëŠ” ë°©ë²•ì—ëŠ” ë‘ ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤.

ì²« ë²ˆì§¸ëŠ” `output_dir previous_output_dir` ì¸ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ `output_dir`ì— ì €ì¥ëœ ìµœì‹  ì²´í¬í¬ì¸íŠ¸ë¶€í„° í›ˆë ¨ì„ ì¬ê°œí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.
ì´ ê²½ìš° `overwrite_output_dir`ì„ ì œê±°í•´ì•¼ í•©ë‹ˆë‹¤:
```bash
python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --output_dir previous_output_dir \
    --predict_with_generate
```

ë‘ ë²ˆì§¸ëŠ” `resume_from_checkpoint path_to_specific_checkpoint` ì¸ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ì²´í¬í¬ì¸íŠ¸ í´ë”ì—ì„œ í›ˆë ¨ì„ ì¬ê°œí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

```bash
python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --resume_from_checkpoint path_to_specific_checkpoint \
    --predict_with_generate
```

## ëª¨ë¸ ê³µìœ í•˜ê¸°[[share-your-model]]

ëª¨ë“  ìŠ¤í¬ë¦½íŠ¸ëŠ” ìµœì¢… ëª¨ë¸ì„ [Model Hub](https://huggingface.co/models)ì— ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì‹œì‘í•˜ê¸° ì „ì— Hugging Faceì— ë¡œê·¸ì¸í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:
```bash
huggingface-cli login
```

ê·¸ëŸ° ë‹¤ìŒ ìŠ¤í¬ë¦½íŠ¸ì— `push_to_hub` ì¸ìˆ˜ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
ì´ ì¸ìˆ˜ëŠ” Hugging Face ì‚¬ìš©ì ì´ë¦„ê³¼ `output_dir`ì— ì§€ì •ëœ í´ë” ì´ë¦„ìœ¼ë¡œ ì €ì¥ì†Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

ì €ì¥ì†Œì— íŠ¹ì • ì´ë¦„ì„ ì§€ì •í•˜ë ¤ë©´ `push_to_hub_model_id` ì¸ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶”ê°€í•©ë‹ˆë‹¤.
ì €ì¥ì†ŒëŠ” ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì•„ë˜ì— ìë™ìœ¼ë¡œ ë‚˜ì—´ë©ë‹ˆë‹¤.
ë‹¤ìŒ ì˜ˆëŠ” íŠ¹ì • ì €ì¥ì†Œ ì´ë¦„ìœ¼ë¡œ ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤:

```bash
python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --push_to_hub \
    --push_to_hub_model_id finetuned-t5-cnn_dailymail \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```