<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# ìë™ ìŒì„± ì¸ì‹[[automatic-speech-recognition]]

[[open-in-colab]]

<Youtube id="TksaY_FDgnk"/>

ìë™ ìŒì„± ì¸ì‹(Automatic Speech Recognition, ASR)ì€ ìŒì„± ì‹ í˜¸ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ìŒì„± ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ í…ìŠ¤íŠ¸ ì¶œë ¥ì— ë§¤í•‘í•©ë‹ˆë‹¤. 
Siriì™€ Alexaì™€ ê°™ì€ ê°€ìƒ ì–´ì‹œìŠ¤í„´íŠ¸ëŠ” ASR ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì¼ìƒì ìœ¼ë¡œ ì‚¬ìš©ìë¥¼ ë•ê³  ìˆìœ¼ë©°, íšŒì˜ ì¤‘ ë¼ì´ë¸Œ ìº¡ì…˜ ë° ë©”ëª¨ ì‘ì„±ê³¼ ê°™ì€ ìœ ìš©í•œ ì‚¬ìš©ì ì¹œí™”ì  ì‘ìš© í”„ë¡œê·¸ë¨ë„ ë§ì´ ìˆìŠµë‹ˆë‹¤.

ì´ ê°€ì´ë“œì—ì„œ ì†Œê°œí•  ë‚´ìš©ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤:

1. [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) ë°ì´í„° ì„¸íŠ¸ì—ì„œ [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base)ë¥¼ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ì˜¤ë””ì˜¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
2. ë¯¸ì„¸ ì¡°ì •í•œ ëª¨ë¸ì„ ì¶”ë¡ ì— ì‚¬ìš©í•©ë‹ˆë‹¤.

<Tip>

ì´ ì‘ì—…ê³¼ í˜¸í™˜ë˜ëŠ” ëª¨ë“  ì•„í‚¤í…ì²˜ì™€ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë³´ë ¤ë©´ [ì‘ì—… í˜ì´ì§€](https://huggingface.co/tasks/automatic-speech-recognition)ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

</Tip>

ì‹œì‘í•˜ê¸° ì „ì— í•„ìš”í•œ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:

```bash
pip install transformers datasets evaluate jiwer
```

Hugging Face ê³„ì •ì— ë¡œê·¸ì¸í•˜ë©´ ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ê³  ì»¤ë®¤ë‹ˆí‹°ì— ê³µìœ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í† í°ì„ ì…ë ¥í•˜ì—¬ ë¡œê·¸ì¸í•˜ì„¸ìš”.

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## MInDS-14 ë°ì´í„° ì„¸íŠ¸ ê°€ì ¸ì˜¤ê¸°[[load-minds-14-dataset]]

ë¨¼ì €, ğŸ¤— Datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) ë°ì´í„° ì„¸íŠ¸ì˜ ì¼ë¶€ë¶„ì„ ê°€ì ¸ì˜¤ì„¸ìš”. 
ì´ë ‡ê²Œ í•˜ë©´ ì „ì²´ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ í›ˆë ¨ì— ì‹œê°„ì„ ë“¤ì´ê¸° ì „ì— ëª¨ë“  ê²ƒì´ ì‘ë™í•˜ëŠ”ì§€ ì‹¤í—˜í•˜ê³  ê²€ì¦í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```py
>>> from datasets import load_dataset, Audio

>>> minds = load_dataset("PolyAI/minds14", name="en-US", split="train[:100]")
```

[`~Dataset.train_test_split`] ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì„¸íŠ¸ì˜ `train`ì„ í›ˆë ¨ ì„¸íŠ¸ì™€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë‚˜ëˆ„ì„¸ìš”:

```py
>>> minds = minds.train_test_split(test_size=0.2)
```

ê·¸ë¦¬ê³  ë°ì´í„° ì„¸íŠ¸ë¥¼ í™•ì¸í•˜ì„¸ìš”:

```py
>>> minds
DatasetDict({
    train: Dataset({
        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],
        num_rows: 16
    })
    test: Dataset({
        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],
        num_rows: 4
    })
})
```

ë°ì´í„° ì„¸íŠ¸ì—ëŠ” `lang_id`ì™€ `english_transcription`ê³¼ ê°™ì€ ìœ ìš©í•œ ì •ë³´ê°€ ë§ì´ í¬í•¨ë˜ì–´ ìˆì§€ë§Œ, ì´ ê°€ì´ë“œì—ì„œëŠ” `audio`ì™€ `transcription`ì— ì´ˆì ì„ ë§ì¶œ ê²ƒì…ë‹ˆë‹¤. ë‹¤ë¥¸ ì—´ì€ [`~datasets.Dataset.remove_columns`] ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì œê±°í•˜ì„¸ìš”:

```py
>>> minds = minds.remove_columns(["english_transcription", "intent_class", "lang_id"])
```

ì˜ˆì‹œë¥¼ ë‹¤ì‹œ í•œë²ˆ í™•ì¸í•´ë³´ì„¸ìš”:

```py
>>> minds["train"][0]
{'audio': {'array': array([-0.00024414,  0.        ,  0.        , ...,  0.00024414,
          0.00024414,  0.00024414], dtype=float32),
  'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav',
  'sampling_rate': 8000},
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav',
 'transcription': "hi I'm trying to use the banking app on my phone and currently my checking and savings account balance is not refreshing"}
```

ë‘ ê°œì˜ í•„ë“œê°€ ìˆìŠµë‹ˆë‹¤:

- `audio`: ì˜¤ë””ì˜¤ íŒŒì¼ì„ ê°€ì ¸ì˜¤ê³  ë¦¬ìƒ˜í”Œë§í•˜ê¸° ìœ„í•´ í˜¸ì¶œí•´ì•¼ í•˜ëŠ” ìŒì„± ì‹ í˜¸ì˜ 1ì°¨ì› `array(ë°°ì—´)`
- `transcription`: ëª©í‘œ í…ìŠ¤íŠ¸

## ì „ì²˜ë¦¬[[preprocess]]

ë‹¤ìŒìœ¼ë¡œ ì˜¤ë””ì˜¤ ì‹ í˜¸ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ Wav2Vec2 í”„ë¡œì„¸ì„œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤:

```py
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base")
```

MInDS-14 ë°ì´í„° ì„¸íŠ¸ì˜ ìƒ˜í”Œë§ ë ˆì´íŠ¸ëŠ” 8000kHzì´ë¯€ë¡œ([ë°ì´í„° ì„¸íŠ¸ ì¹´ë“œ](https://huggingface.co/datasets/PolyAI/minds14)ì—ì„œ í™•ì¸), ì‚¬ì „ í›ˆë ¨ëœ Wav2Vec2 ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ ë°ì´í„° ì„¸íŠ¸ë¥¼ 16000kHzë¡œ ë¦¬ìƒ˜í”Œë§í•´ì•¼ í•©ë‹ˆë‹¤:

```py
>>> minds = minds.cast_column("audio", Audio(sampling_rate=16_000))
>>> minds["train"][0]
{'audio': {'array': array([-2.38064706e-04, -1.58618059e-04, -5.43987835e-06, ...,
          2.78103951e-04,  2.38446111e-04,  1.18740834e-04], dtype=float32),
  'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav',
  'sampling_rate': 16000},
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav',
 'transcription': "hi I'm trying to use the banking app on my phone and currently my checking and savings account balance is not refreshing"}
```

ìœ„ì˜ 'transcription'ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ í…ìŠ¤íŠ¸ëŠ” ëŒ€ë¬¸ìì™€ ì†Œë¬¸ìê°€ ì„ì—¬ ìˆìŠµë‹ˆë‹¤. Wav2Vec2 í† í¬ë‚˜ì´ì €ëŠ” ëŒ€ë¬¸ì ë¬¸ìì— ëŒ€í•´ì„œë§Œ í›ˆë ¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ í…ìŠ¤íŠ¸ê°€ í† í¬ë‚˜ì´ì €ì˜ ì–´íœ˜ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤:

```py
>>> def uppercase(example):
...     return {"transcription": example["transcription"].upper()}


>>> minds = minds.map(uppercase)
```

ì´ì œ ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•  ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤:

1. `audio` ì—´ì„ í˜¸ì¶œí•˜ì—¬ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ê°€ì ¸ì˜¤ê³  ë¦¬ìƒ˜í”Œë§í•©ë‹ˆë‹¤.
2. ì˜¤ë””ì˜¤ íŒŒì¼ì—ì„œ `input_values`ë¥¼ ì¶”ì¶œí•˜ê³  í”„ë¡œì„¸ì„œë¡œ `transcription` ì—´ì„ í† í°í™”í•©ë‹ˆë‹¤.

```py
>>> def prepare_dataset(batch):
...     audio = batch["audio"]
...     batch = processor(audio["array"], sampling_rate=audio["sampling_rate"], text=batch["transcription"])
...     batch["input_length"] = len(batch["input_values"][0])
...     return batch
```

ì „ì²´ ë°ì´í„° ì„¸íŠ¸ì— ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ì ìš©í•˜ë ¤ë©´ ğŸ¤— Datasets [`~datasets.Dataset.map`] í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. `num_proc` ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë¡œì„¸ìŠ¤ ìˆ˜ë¥¼ ëŠ˜ë¦¬ë©´ `map`ì˜ ì†ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. [`~datasets.Dataset.remove_columns`] ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í•„ìš”í•˜ì§€ ì•Šì€ ì—´ì„ ì œê±°í•˜ì„¸ìš”:

```py
>>> encoded_minds = minds.map(prepare_dataset, remove_columns=minds.column_names["train"], num_proc=4)
```

ğŸ¤— Transformersì—ëŠ” ìë™ ìŒì„± ì¸ì‹ìš© ë°ì´í„° ì½œë ˆì´í„°ê°€ ì—†ìœ¼ë¯€ë¡œ ì˜ˆì œ ë°°ì¹˜ë¥¼ ìƒì„±í•˜ë ¤ë©´ [`DataCollatorWithPadding`]ì„ ì¡°ì •í•´ì•¼ í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ë°ì´í„° ì½œë ˆì´í„°ëŠ” í…ìŠ¤íŠ¸ì™€ ë ˆì´ë¸”ì„ ë°°ì¹˜ì—ì„œ ê°€ì¥ ê¸´ ìš”ì†Œì˜ ê¸¸ì´ì— ë™ì ìœ¼ë¡œ íŒ¨ë”©í•˜ì—¬ ê¸¸ì´ë¥¼ ê· ì¼í•˜ê²Œ í•©ë‹ˆë‹¤. `tokenizer` í•¨ìˆ˜ì—ì„œ `padding=True`ë¥¼ ì„¤ì •í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ íŒ¨ë”©í•  ìˆ˜ ìˆì§€ë§Œ, ë™ì  íŒ¨ë”©ì´ ë” íš¨ìœ¨ì ì…ë‹ˆë‹¤.

ë‹¤ë¥¸ ë°ì´í„° ì½œë ˆì´í„°ì™€ ë‹¬ë¦¬ ì´ íŠ¹ì • ë°ì´í„° ì½œë ˆì´í„°ëŠ” `input_values`ì™€ `labels`ì— ëŒ€í•´ ë‹¤ë¥¸ íŒ¨ë”© ë°©ë²•ì„ ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

```py
>>> import torch

>>> from dataclasses import dataclass, field
>>> from typing import Any, Dict, List, Optional, Union


>>> @dataclass
... class DataCollatorCTCWithPadding:
...     processor: AutoProcessor
...     padding: Union[bool, str] = "longest"

...     def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
...         # ì…ë ¥ê³¼ ë ˆì´ë¸”ì„ ë¶„í• í•©ë‹ˆë‹¤
...         # ê¸¸ì´ê°€ ë‹¤ë¥´ê³ , ê°ê° ë‹¤ë¥¸ íŒ¨ë”© ë°©ë²•ì„ ì‚¬ìš©í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤
...         input_features = [{"input_values": feature["input_values"][0]} for feature in features]
...         label_features = [{"input_ids": feature["labels"]} for feature in features]

...         batch = self.processor.pad(input_features, padding=self.padding, return_tensors="pt")

...         labels_batch = self.processor.pad(labels=label_features, padding=self.padding, return_tensors="pt")

...         # íŒ¨ë”©ì— ëŒ€í•´ ì†ì‹¤ì„ ì ìš©í•˜ì§€ ì•Šë„ë¡ -100ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤
...         labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

...         batch["labels"] = labels

...         return batch
```

ì´ì œ `DataCollatorForCTCWithPadding`ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•©ë‹ˆë‹¤:

```py
>>> data_collator = DataCollatorCTCWithPadding(processor=processor, padding="longest")
```

## í‰ê°€í•˜ê¸°[[evaluate]]

í›ˆë ¨ ì¤‘ì— í‰ê°€ ì§€í‘œë¥¼ í¬í•¨í•˜ë©´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ë©´ í‰ê°€ ë°©ë²•ì„ ë¹ ë¥´ê²Œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
ì´ ì‘ì—…ì—ì„œëŠ” [ë‹¨ì–´ ì˜¤ë¥˜ìœ¨(Word Error Rate, WER)](https://huggingface.co/spaces/evaluate-metric/wer) í‰ê°€ ì§€í‘œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
(í‰ê°€ ì§€í‘œë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ê³„ì‚°í•˜ëŠ” ë°©ë²•ì€ ğŸ¤— Evaluate [ë‘˜ëŸ¬ë³´ê¸°](https://huggingface.co/docs/evaluate/a_quick_tour)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”):

```py
>>> import evaluate

>>> wer = evaluate.load("wer")
```

ê·¸ëŸ° ë‹¤ìŒ ì˜ˆì¸¡ê°’ê³¼ ë ˆì´ë¸”ì„ [`~evaluate.EvaluationModule.compute`]ì— ì „ë‹¬í•˜ì—¬ WERì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤:

```py
>>> import numpy as np


>>> def compute_metrics(pred):
...     pred_logits = pred.predictions
...     pred_ids = np.argmax(pred_logits, axis=-1)

...     pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id

...     pred_str = processor.batch_decode(pred_ids)
...     label_str = processor.batch_decode(pred.label_ids, group_tokens=False)

...     wer = wer.compute(predictions=pred_str, references=label_str)

...     return {"wer": wer}
```

ì´ì œ `compute_metrics` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ì¤€ë¹„ê°€ ë˜ì—ˆìœ¼ë©°, í›ˆë ¨ì„ ì„¤ì •í•  ë•Œ ì´ í•¨ìˆ˜ë¡œ ë˜ëŒì•„ì˜¬ ê²ƒì…ë‹ˆë‹¤.

## í›ˆë ¨í•˜ê¸°[[train]]

<frameworkcontent>
<pt>
<Tip>

[`Trainer`]ë¡œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ê²ƒì´ ìµìˆ™í•˜ì§€ ì•Šë‹¤ë©´, [ì—¬ê¸°](../training#train-with-pytorch-trainer)ì—ì„œ ê¸°ë³¸ íŠœí† ë¦¬ì–¼ì„ í™•ì¸í•´ë³´ì„¸ìš”!

</Tip>

ì´ì œ ëª¨ë¸ í›ˆë ¨ì„ ì‹œì‘í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤! [`AutoModelForCTC`]ë¡œ Wav2Vec2ë¥¼ ê°€ì ¸ì˜¤ì„¸ìš”. `ctc_loss_reduction` ë§¤ê°œë³€ìˆ˜ë¡œ CTC ì†ì‹¤ì— ì ìš©í•  ì¶•ì†Œ(reduction) ë°©ë²•ì„ ì§€ì •í•˜ì„¸ìš”. ê¸°ë³¸ê°’ì¸ í•©ê³„ ëŒ€ì‹  í‰ê· ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì¢‹ì€ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤:

```py
>>> from transformers import AutoModelForCTC, TrainingArguments, Trainer

>>> model = AutoModelForCTC.from_pretrained(
...     "facebook/wav2vec2-base",
...     ctc_loss_reduction="mean",
...     pad_token_id=processor.tokenizer.pad_token_id,
... )
```

ì´ì œ ì„¸ ë‹¨ê³„ë§Œ ë‚¨ì•˜ìŠµë‹ˆë‹¤:

1. [`TrainingArguments`]ì—ì„œ í›ˆë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì •ì˜í•˜ì„¸ìš”. `output_dir`ì€ ëª¨ë¸ì„ ì €ì¥í•  ê²½ë¡œë¥¼ ì§€ì •í•˜ëŠ” ìœ ì¼í•œ í•„ìˆ˜ ë§¤ê°œë³€ìˆ˜ì…ë‹ˆë‹¤. `push_to_hub=True`ë¥¼ ì„¤ì •í•˜ì—¬ ëª¨ë¸ì„ Hubì— ì—…ë¡œë“œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤(ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ë ¤ë©´ Hugging Faceì— ë¡œê·¸ì¸í•´ì•¼ í•©ë‹ˆë‹¤). [`Trainer`]ëŠ” ê° ì—í­ë§ˆë‹¤ WERì„ í‰ê°€í•˜ê³  í›ˆë ¨ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
2. ëª¨ë¸, ë°ì´í„° ì„¸íŠ¸, í† í¬ë‚˜ì´ì €, ë°ì´í„° ì½œë ˆì´í„°, `compute_metrics` í•¨ìˆ˜ì™€ í•¨ê»˜ [`Trainer`]ì— í›ˆë ¨ ì¸ìˆ˜ë¥¼ ì „ë‹¬í•˜ì„¸ìš”.
3. [`~Trainer.train`]ì„ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ì„¸ìš”.

```py
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_asr_mind_model",
...     per_device_train_batch_size=8,
...     gradient_accumulation_steps=2,
...     learning_rate=1e-5,
...     warmup_steps=500,
...     max_steps=2000,
...     gradient_checkpointing=True,
...     fp16=True,
...     group_by_length=True,
...     eval_strategy="steps",
...     per_device_eval_batch_size=8,
...     save_steps=1000,
...     eval_steps=1000,
...     logging_steps=25,
...     load_best_model_at_end=True,
...     metric_for_best_model="wer",
...     greater_is_better=False,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=encoded_minds["train"],
...     eval_dataset=encoded_minds["test"],
...     tokenizer=processor.feature_extractor,
...     data_collator=data_collator,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

í›ˆë ¨ì´ ì™„ë£Œë˜ë©´ ëª¨ë‘ê°€ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ [`~transformers.Trainer.push_to_hub`] ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ Hubì— ê³µìœ í•˜ì„¸ìš”:

```py
>>> trainer.push_to_hub()
```
</pt>
</frameworkcontent>

<Tip>

ìë™ ìŒì„± ì¸ì‹ì„ ìœ„í•´ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë” ìì„¸í•œ ì˜ˆì œëŠ” ì˜ì–´ ìë™ ìŒì„± ì¸ì‹ì„ ìœ„í•œ [ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸](https://huggingface.co/blog/fine-tune-wav2vec2-english)ì™€ ë‹¤êµ­ì–´ ìë™ ìŒì„± ì¸ì‹ì„ ìœ„í•œ [í¬ìŠ¤íŠ¸](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

</Tip>

## ì¶”ë¡ í•˜ê¸°[[inference]]

ì¢‹ì•„ìš”, ì´ì œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í–ˆìœ¼ë‹ˆ ì¶”ë¡ ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

ì¶”ë¡ ì— ì‚¬ìš©í•  ì˜¤ë””ì˜¤ íŒŒì¼ì„ ê°€ì ¸ì˜¤ì„¸ìš”. í•„ìš”í•œ ê²½ìš° ì˜¤ë””ì˜¤ íŒŒì¼ì˜ ìƒ˜í”Œë§ ë¹„ìœ¨ì„ ëª¨ë¸ì˜ ìƒ˜í”Œë§ ë ˆì´íŠ¸ì— ë§ê²Œ ë¦¬ìƒ˜í”Œë§í•˜ëŠ” ê²ƒì„ ìŠì§€ ë§ˆì„¸ìš”!

```py
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", "en-US", split="train")
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
>>> sampling_rate = dataset.features["audio"].sampling_rate
>>> audio_file = dataset[0]["audio"]["path"]
```

ì¶”ë¡ ì„ ìœ„í•´ ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì„ ì‹œí—˜í•´ë³´ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì€ [`pipeline`]ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìë™ ìŒì„± ì¸ì‹ì„ ìœ„í•œ `pipeline`ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ê³  ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì „ë‹¬í•˜ì„¸ìš”:

```py
>>> from transformers import pipeline

>>> transcriber = pipeline("automatic-speech-recognition", model="stevhliu/my_awesome_asr_minds_model")
>>> transcriber(audio_file)
{'text': 'I WOUD LIKE O SET UP JOINT ACOUNT WTH Y PARTNER'}
```

<Tip>

í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ëœ ê²°ê³¼ê°€ ê½¤ ê´œì°®ì§€ë§Œ ë” ì¢‹ì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤! ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ì–»ìœ¼ë ¤ë©´ ë” ë§ì€ ì˜ˆì œë¡œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ì„¸ìš”!

</Tip>

`pipeline`ì˜ ê²°ê³¼ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì¬í˜„í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:

<frameworkcontent>
<pt>
ì˜¤ë””ì˜¤ íŒŒì¼ê³¼ í…ìŠ¤íŠ¸ë¥¼ ì „ì²˜ë¦¬í•˜ê³  PyTorch í…ì„œë¡œ `input`ì„ ë°˜í™˜í•  í”„ë¡œì„¸ì„œë¥¼ ê°€ì ¸ì˜¤ì„¸ìš”:

```py
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("stevhliu/my_awesome_asr_mind_model")
>>> inputs = processor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")
```

ì…ë ¥ì„ ëª¨ë¸ì— ì „ë‹¬í•˜ê³  ë¡œì§“ì„ ë°˜í™˜í•˜ì„¸ìš”:

```py
>>> from transformers import AutoModelForCTC

>>> model = AutoModelForCTC.from_pretrained("stevhliu/my_awesome_asr_mind_model")
>>> with torch.no_grad():
...     logits = model(**inputs).logits
```

ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ `input_ids`ë¥¼ ì˜ˆì¸¡í•˜ê³ , í”„ë¡œì„¸ì„œë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ëœ `input_ids`ë¥¼ ë‹¤ì‹œ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©í•˜ì„¸ìš”:

```py
>>> import torch

>>> predicted_ids = torch.argmax(logits, dim=-1)
>>> transcription = processor.batch_decode(predicted_ids)
>>> transcription
['I WOUL LIKE O SET UP JOINT ACOUNT WTH Y PARTNER']
```
</pt>
</frameworkcontent>