<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# ÏùòÎØ∏Ï†Å Î∂ÑÌï†(Semantic segmentation)[[semantic-segmentation]]

[[open-in-colab]]

<Youtube id="dKE8SIt9C-w"/>

ÏùòÎØ∏Ï†Å Î∂ÑÌï†(semantic segmentation)ÏùÄ Ïù¥ÎØ∏ÏßÄÏùò Í∞Å ÌîΩÏÖÄÏóê Î†àÏù¥Î∏î ÎòêÎäî ÌÅ¥ÎûòÏä§Î•º Ìï†ÎãπÌï©ÎãàÎã§. Î∂ÑÌï†(segmentation)ÏóêÎäî Ïó¨Îü¨ Ï¢ÖÎ•òÍ∞Ä ÏûàÏúºÎ©∞, ÏùòÎØ∏Ï†Å Î∂ÑÌï†Ïùò Í≤ΩÏö∞ ÎèôÏùºÌïú Î¨ºÏ≤¥Ïùò Í≥†Ïú† Ïù∏Ïä§ÌÑ¥Ïä§Î•º Íµ¨Î∂ÑÌïòÏßÄ ÏïäÏäµÎãàÎã§. Îëê Î¨ºÏ≤¥ Î™®Îëê ÎèôÏùºÌïú Î†àÏù¥Î∏îÏù¥ ÏßÄÏ†ïÎê©ÎãàÎã§(ÏòàÏãúÎ°ú, "car-1" Í≥º "car-2" ÎåÄÏã† "car"Î°ú ÏßÄÏ†ïÌï©ÎãàÎã§).
Ïã§ÏÉùÌôúÏóêÏÑú ÌùîÌûà Î≥º Ïàò ÏûàÎäî ÏùòÎØ∏Ï†Å Î∂ÑÌï†Ïùò Ï†ÅÏö© ÏÇ¨Î°ÄÎ°úÎäî Î≥¥ÌñâÏûêÏôÄ Ï§ëÏöîÌïú ÍµêÌÜµ Ï†ïÎ≥¥Î•º ÏãùÎ≥ÑÌïòÎäî ÏûêÏú® Ï£ºÌñâ ÏûêÎèôÏ∞® ÌïôÏäµ, ÏùòÎ£å Ïù¥ÎØ∏ÏßÄÏùò ÏÑ∏Ìè¨ÏôÄ Ïù¥ÏÉÅ ÏßïÌõÑ ÏãùÎ≥Ñ, Í∑∏Î¶¨Í≥† ÏúÑÏÑ± Ïù¥ÎØ∏ÏßÄÏùò ÌôòÍ≤Ω Î≥ÄÌôî Î™®ÎãàÌÑ∞ÎßÅÎì±Ïù¥ ÏûàÏäµÎãàÎã§.

Ïù¥Î≤à Í∞ÄÏù¥ÎìúÏóêÏÑú Î∞∞Ïö∏ ÎÇ¥Ïö©ÏùÄ Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§:

1. [SceneParse150](https://huggingface.co/datasets/scene_parse_150) Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Î•º Ïù¥Ïö©Ìï¥ [SegFormer](https://huggingface.co/docs/transformers/main/en/model_doc/segformer#segformer) ÎØ∏ÏÑ∏ Ï°∞Ï†ïÌïòÍ∏∞.
2. ÎØ∏ÏÑ∏ Ï°∞Ï†ïÎêú Î™®Îç∏ÏùÑ Ï∂îÎ°†Ïóê ÏÇ¨Ïö©ÌïòÍ∏∞.

> [!TIP]
> Ïù¥ ÏûëÏóÖÍ≥º Ìò∏ÌôòÎêòÎäî Î™®Îì† ÏïÑÌÇ§ÌÖçÏ≤òÏôÄ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Î•º Î≥¥Î†§Î©¥ [ÏûëÏóÖ ÌéòÏù¥ÏßÄ](https://huggingface.co/tasks/image-segmentation)Î•º ÌôïÏù∏ÌïòÎäî Í≤ÉÏù¥ Ï¢ãÏäµÎãàÎã§.

ÏãúÏûëÌïòÍ∏∞ Ï†ÑÏóê ÌïÑÏöîÌïú Î™®Îì† ÎùºÏù¥Î∏åÎü¨Î¶¨Í∞Ä ÏÑ§ÏπòÎêòÏóàÎäîÏßÄ ÌôïÏù∏ÌïòÏÑ∏Ïöî:

```bash
pip install -q datasets transformers evaluate
```
Ïª§ÎÆ§ÎãàÌã∞Ïóê Î™®Îç∏ÏùÑ ÏóÖÎ°úÎìúÌïòÍ≥† Í≥µÏú†Ìï† Ïàò ÏûàÎèÑÎ°ù Hugging Face Í≥ÑÏ†ïÏóê Î°úÍ∑∏Ïù∏ÌïòÎäî Í≤ÉÏùÑ Í∂åÏû•Ìï©ÎãàÎã§. ÌîÑÎ°¨ÌîÑÌä∏Í∞Ä ÎÇòÌÉÄÎÇòÎ©¥ ÌÜ†ÌÅ∞ÏùÑ ÏûÖÎ†•ÌïòÏó¨ Î°úÍ∑∏Ïù∏ÌïòÏÑ∏Ïöî:

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## SceneParse150 Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏ Î∂àÎü¨Ïò§Í∏∞[[load-sceneparse150-dataset]]

ü§ó Datasets ÎùºÏù¥Î∏åÎü¨Î¶¨ÏóêÏÑú SceneParse150 Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Ïùò Îçî ÏûëÏùÄ Î∂ÄÎ∂Ñ ÏßëÌï©ÏùÑ Í∞ÄÏ†∏Ïò§Îäî Í≤ÉÏúºÎ°ú ÏãúÏûëÌï©ÎãàÎã§. Ïù¥Î†áÍ≤å ÌïòÎ©¥ Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏ Ï†ÑÏ≤¥Ïóê ÎåÄÌïú ÌõàÎ†®Ïóê ÎßéÏùÄ ÏãúÍ∞ÑÏùÑ Ìï†Ïï†ÌïòÍ∏∞ Ï†ÑÏóê Ïã§ÌóòÏùÑ ÌÜµÌï¥ Î™®Îì† Í≤ÉÏù¥ Ï†úÎåÄÎ°ú ÏûëÎèôÌïòÎäîÏßÄ ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§.

```py
>>> from datasets import load_dataset

>>> ds = load_dataset("scene_parse_150", split="train[:50]")
```

Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Ïùò `train`ÏùÑ [`~datasets.Dataset.train_test_split`] Î©îÏÜåÎìúÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÌõàÎ†® Î∞è ÌÖåÏä§Ìä∏ ÏÑ∏Ìä∏Î°ú Î∂ÑÌï†ÌïòÏÑ∏Ïöî:

```py
>>> ds = ds.train_test_split(test_size=0.2)
>>> train_ds = ds["train"]
>>> test_ds = ds["test"]
```

Í∑∏Î¶¨Í≥† ÏòàÏãúÎ•º ÏÇ¥Ìé¥Î≥¥ÏÑ∏Ïöî:

```py
>>> train_ds[0]
{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x683 at 0x7F9B0C201F90>,
 'annotation': <PIL.PngImagePlugin.PngImageFile image mode=L size=512x683 at 0x7F9B0C201DD0>,
 'scene_category': 368}
```

- `image`: Ïû•Î©¥Ïùò PIL Ïù¥ÎØ∏ÏßÄÏûÖÎãàÎã§.
- `annotation`: Î∂ÑÌï† ÏßÄÎèÑ(segmentation map)Ïùò PIL Ïù¥ÎØ∏ÏßÄÏûÖÎãàÎã§. Î™®Îç∏Ïùò ÌÉÄÍ≤üÏù¥Í∏∞ÎèÑ Ìï©ÎãàÎã§.
- `scene_category`: "Ï£ºÎ∞©" ÎòêÎäî "ÏÇ¨Î¨¥Ïã§"Í≥º Í∞ôÏù¥ Ïù¥ÎØ∏ÏßÄ Ïû•Î©¥ÏùÑ ÏÑ§Î™ÖÌïòÎäî Ïπ¥ÌÖåÍ≥†Î¶¨ IDÏûÖÎãàÎã§. Ïù¥ Í∞ÄÏù¥ÎìúÏóêÏÑúÎäî Îëò Îã§ PIL Ïù¥ÎØ∏ÏßÄÏù∏ `image`ÏôÄ `annotation`ÎßåÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§.

ÎÇòÏ§ëÏóê Î™®Îç∏ÏùÑ ÏÑ§Ï†ïÌï† Îïå Ïú†Ïö©ÌïòÍ≤å ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎèÑÎ°ù Î†àÏù¥Î∏î IDÎ•º Î†àÏù¥Î∏î ÌÅ¥ÎûòÏä§Ïóê Îß§ÌïëÌïòÎäî ÏÇ¨Ï†ÑÎèÑ ÎßåÎì§Í≥† Ïã∂ÏùÑ Í≤ÉÏûÖÎãàÎã§. HubÏóêÏÑú Îß§ÌïëÏùÑ Îã§Ïö¥Î°úÎìúÌïòÍ≥† `id2label` Î∞è `label2id` ÏÇ¨Ï†ÑÏùÑ ÎßåÎìúÏÑ∏Ïöî:

```py
>>> import json
>>> from pathlib import Path
>>> from huggingface_hub import hf_hub_download

>>> repo_id = "huggingface/label-files"
>>> filename = "ade20k-id2label.json"
>>> id2label = json.loads(Path(hf_hub_download(repo_id, filename, repo_type="dataset")).read_text())
>>> id2label = {int(k): v for k, v in id2label.items()}
>>> label2id = {v: k for k, v in id2label.items()}
>>> num_labels = len(id2label)
```

## Ï†ÑÏ≤òÎ¶¨ÌïòÍ∏∞[[preprocess]

Îã§Ïùå Îã®Í≥ÑÎäî Î™®Îç∏Ïóê ÏÇ¨Ïö©Ìï† Ïù¥ÎØ∏ÏßÄÏôÄ Ï£ºÏÑùÏùÑ Ï§ÄÎπÑÌïòÍ∏∞ ÏúÑÌï¥ SegFormer Ïù¥ÎØ∏ÏßÄ ÌîÑÎ°úÏÑ∏ÏÑúÎ•º Î∂àÎü¨Ïò§Îäî Í≤ÉÏûÖÎãàÎã§. Ïö∞Î¶¨Í∞Ä ÏÇ¨Ïö©ÌïòÎäî Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏ÏôÄ Í∞ôÏùÄ ÏùºÎ∂Ä Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Îäî Î∞∞Í≤Ω ÌÅ¥ÎûòÏä§Î°ú Ï†úÎ°ú Ïù∏Îç±Ïä§Î•º ÏÇ¨Ïö©Ìï©ÎãàÎã§. ÌïòÏßÄÎßå Î∞∞Í≤Ω ÌÅ¥ÎûòÏä§Îäî 150Í∞úÏùò ÌÅ¥ÎûòÏä§Ïóê Ïã§Ï†úÎ°úÎäî Ìè¨Ìï®ÎêòÏßÄ ÏïäÍ∏∞ ÎïåÎ¨∏Ïóê `do_reduce_labels=True` Î•º ÏÑ§Ï†ïÌï¥ Î™®Îì† Î†àÏù¥Î∏îÏóêÏÑú Î∞∞Í≤Ω ÌÅ¥ÎûòÏä§Î•º Ï†úÍ±∞Ìï¥Ïïº Ìï©ÎãàÎã§. Ï†úÎ°ú Ïù∏Îç±Ïä§Îäî `255`Î°ú ÎåÄÏ≤¥ÎêòÎØÄÎ°ú SegFormerÏùò ÏÜêÏã§ Ìï®ÏàòÏóêÏÑú Î¨¥ÏãúÎê©ÎãàÎã§:

```py
>>> from transformers import AutoImageProcessor

>>> checkpoint = "nvidia/mit-b0"
>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint, do_reduce_labels=True)
```


Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Ïóê Îç∞Ïù¥ÌÑ∞ Ï¶ùÍ∞ïÏùÑ Ï†ÅÏö©ÌïòÏó¨ Í≥ºÏ†ÅÌï©Ïóê ÎåÄÌï¥ Î™®Îç∏ÏùÑ Î≥¥Îã§ Í∞ïÍ±¥ÌïòÍ≤å ÎßåÎìúÎäî Í≤ÉÏù¥ ÏùºÎ∞òÏ†ÅÏûÖÎãàÎã§. Ïù¥ Í∞ÄÏù¥ÎìúÏóêÏÑúÎäî [torchvision](https://pytorch.org/vision/stable/index.html)Ïùò [`ColorJitter`](https://pytorch.org/vision/stable/generated/torchvision.transforms.ColorJitter.html)Î•º ÏÇ¨Ïö©ÌïòÏó¨ Ïù¥ÎØ∏ÏßÄÏùò ÏÉâÏÉÅ ÏÜçÏÑ±ÏùÑ ÏûÑÏùòÎ°ú Î≥ÄÍ≤ΩÌï©ÎãàÎã§. ÌïòÏßÄÎßå, ÏûêÏã†Ïù¥ ÏõêÌïòÎäî Ïù¥ÎØ∏ÏßÄ ÎùºÏù¥Î∏åÎü¨Î¶¨Î•º ÏÇ¨Ïö©Ìï† ÏàòÎèÑ ÏûàÏäµÎãàÎã§.

```py
>>> from torchvision.transforms import ColorJitter

>>> jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)
```

Ïù¥Ï†ú Î™®Îç∏Ïóê ÏÇ¨Ïö©Ìï† Ïù¥ÎØ∏ÏßÄÏôÄ Ï£ºÏÑùÏùÑ Ï§ÄÎπÑÌïòÍ∏∞ ÏúÑÌï¥ Îëê Í∞úÏùò Ï†ÑÏ≤òÎ¶¨ Ìï®ÏàòÎ•º ÎßåÎì≠ÎãàÎã§. Ïù¥ Ìï®ÏàòÎì§ÏùÄ Ïù¥ÎØ∏ÏßÄÎ•º `pixel_values`Î°ú, Ï£ºÏÑùÏùÑ `labels`Î°ú Î≥ÄÌôòÌï©ÎãàÎã§. ÌõàÎ†® ÏÑ∏Ìä∏Ïùò Í≤ΩÏö∞ Ïù¥ÎØ∏ÏßÄ ÌîÑÎ°úÏÑ∏ÏÑúÏóê Ïù¥ÎØ∏ÏßÄÎ•º Ï†úÍ≥µÌïòÍ∏∞ Ï†ÑÏóê `jitter`Î•º Ï†ÅÏö©Ìï©ÎãàÎã§. ÌÖåÏä§Ìä∏ ÏÑ∏Ìä∏Ïùò Í≤ΩÏö∞ Ïù¥ÎØ∏ÏßÄ ÌîÑÎ°úÏÑ∏ÏÑúÎäî `images`Î•º ÏûêÎ•¥Í≥† Ï†ïÍ∑úÌôîÌïòÎ©∞, ÌÖåÏä§Ìä∏ Ï§ëÏóêÎäî Îç∞Ïù¥ÌÑ∞ Ï¶ùÍ∞ïÏù¥ Ï†ÅÏö©ÎêòÏßÄ ÏïäÏúºÎØÄÎ°ú `labels`Îßå ÏûêÎ¶ÖÎãàÎã§.

```py
>>> def train_transforms(example_batch):
...     images = [jitter(x) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs


>>> def val_transforms(example_batch):
...     images = [x for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs
```

Î™®Îì† Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Ïóê `jitter`Î•º Ï†ÅÏö©ÌïòÎ†§Î©¥, ü§ó Datasets [`~datasets.Dataset.set_transform`] Ìï®ÏàòÎ•º ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî. Ï¶âÏãú Î≥ÄÌôòÏù¥ Ï†ÅÏö©ÎêòÍ∏∞ ÎïåÎ¨∏Ïóê Îçî Îπ†Î•¥Í≥† ÎîîÏä§ÌÅ¨ Í≥µÍ∞ÑÏùÑ Îçú Ï∞®ÏßÄÌï©ÎãàÎã§:

```py
>>> train_ds.set_transform(train_transforms)
>>> test_ds.set_transform(val_transforms)
```


## ÌèâÍ∞ÄÌïòÍ∏∞[[evaluate]]

ÌõàÎ†® Ï§ëÏóê Î©îÌä∏Î¶≠ÏùÑ Ìè¨Ìï®ÌïòÎ©¥ Î™®Îç∏Ïùò ÏÑ±Îä•ÏùÑ ÌèâÍ∞ÄÌïòÎäî Îç∞ ÎèÑÏõÄÏù¥ ÎêòÎäî Í≤ΩÏö∞Í∞Ä ÎßéÏäµÎãàÎã§. ü§ó [Evaluate](https://huggingface.co/docs/evaluate/index) ÎùºÏù¥Î∏åÎü¨Î¶¨Î•º ÏÇ¨Ïö©ÌïòÏó¨ ÌèâÍ∞Ä Î∞©Î≤ïÏùÑ Îπ†Î•¥Í≤å Î°úÎìúÌï† Ïàò ÏûàÏäµÎãàÎã§. Ïù¥ ÌÉúÏä§ÌÅ¨ÏóêÏÑúÎäî [mean Intersection over Union](https://huggingface.co/spaces/evaluate-metric/accuracy) (IoU) Î©îÌä∏Î¶≠ÏùÑ Î°úÎìúÌïòÏÑ∏Ïöî (Î©îÌä∏Î¶≠ÏùÑ Î°úÎìúÌïòÍ≥† Í≥ÑÏÇ∞ÌïòÎäî Î∞©Î≤ïÏóê ÎåÄÌï¥ ÏûêÏÑ∏Ìûà ÏïåÏïÑÎ≥¥Î†§Î©¥ ü§ó Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour)Î•º ÏÇ¥Ìé¥Î≥¥ÏÑ∏Ïöî).

```py
>>> import evaluate

>>> metric = evaluate.load("mean_iou")
```

Í∑∏Îü∞ Îã§Ïùå Î©îÌä∏Î¶≠ÏùÑ [`~evaluate.EvaluationModule.compute`]ÌïòÎäî Ìï®ÏàòÎ•º ÎßåÎì≠ÎãàÎã§. ÏòàÏ∏°ÏùÑ Î®ºÏ†Ä Î°úÏßìÏúºÎ°ú Î≥ÄÌôòÌïú Îã§Ïùå, Î†àÏù¥Î∏îÏùò ÌÅ¨Í∏∞Ïóê ÎßûÍ≤å Î™®ÏñëÏùÑ Îã§Ïãú ÏßÄÏ†ïÌï¥Ïïº [`~evaluate.EvaluationModule.compute`]Î•º Ìò∏Ï∂úÌï† Ïàò ÏûàÏäµÎãàÎã§:


```py
>>> import numpy as np
>>> import torch
>>> from torch import nn

>>> def compute_metrics(eval_pred):
...     with torch.no_grad():
...         logits, labels = eval_pred
...         logits_tensor = torch.from_numpy(logits)
...         logits_tensor = nn.functional.interpolate(
...             logits_tensor,
...             size=labels.shape[-2:],
...             mode="bilinear",
...             align_corners=False,
...         ).argmax(dim=1)

...         pred_labels = logits_tensor.detach().cpu().numpy()
...         metrics = metric.compute(
...             predictions=pred_labels,
...             references=labels,
...             num_labels=num_labels,
...             ignore_index=255,
...             reduce_labels=False,
...         )
...         for key, value in metrics.items():
...             if isinstance(value, np.ndarray):
...                 metrics[key] = value.tolist()
...         return metrics
```



Ïù¥Ï†ú `compute_metrics` Ìï®ÏàòÎ•º ÏÇ¨Ïö©Ìï† Ï§ÄÎπÑÍ∞Ä ÎêòÏóàÏäµÎãàÎã§. Ìä∏Î†àÏù¥ÎãùÏùÑ ÏÑ§Ï†ïÌï† Îïå Ïù¥ Ìï®ÏàòÎ°ú ÎèåÏïÑÍ∞ÄÍ≤å Îê©ÎãàÎã§.

## ÌïôÏäµÌïòÍ∏∞[[train]]
> [!TIP]
> ÎßåÏïΩ [`Trainer`]Î•º ÏÇ¨Ïö©Ìï¥ Î™®Îç∏ÏùÑ ÎØ∏ÏÑ∏ Ï°∞Ï†ïÌïòÎäî Í≤ÉÏóê ÏùµÏàôÌïòÏßÄ ÏïäÎã§Î©¥, [Ïó¨Í∏∞](../training#finetune-with-trainer)ÏóêÏÑú Í∏∞Î≥∏ ÌäúÌÜ†Î¶¨ÏñºÏùÑ ÏÇ¥Ìé¥Î≥¥ÏÑ∏Ïöî!

Ïù¥Ï†ú Î™®Îç∏ ÌïôÏäµÏùÑ ÏãúÏûëÌï† Ï§ÄÎπÑÍ∞Ä ÎêòÏóàÏäµÎãàÎã§! [`AutoModelForSemanticSegmentation`]Î°ú SegFormerÎ•º Î∂àÎü¨Ïò§Í≥†, Î™®Îç∏Ïóê Î†àÏù¥Î∏î IDÏôÄ Î†àÏù¥Î∏î ÌÅ¥ÎûòÏä§ Í∞ÑÏùò Îß§ÌïëÏùÑ Ï†ÑÎã¨Ìï©ÎãàÎã§:

```py
>>> from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer

>>> model = AutoModelForSemanticSegmentation.from_pretrained(checkpoint, id2label=id2label, label2id=label2id)
```

Ïù¥Ï†ú ÏÑ∏ Îã®Í≥ÑÎßå ÎÇ®ÏïòÏäµÎãàÎã§:

1. ÌïôÏäµ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞Î•º [`TrainingArguments`]Ïóê Ï†ïÏùòÌï©ÎãàÎã§. `image` Ïó¥Ïù¥ ÏÇ≠Ï†úÎêòÍ∏∞ ÎïåÎ¨∏Ïóê ÏÇ¨Ïö©ÌïòÏßÄ ÏïäÎäî Ïó¥ÏùÑ Ï†úÍ±∞ÌïòÏßÄ ÏïäÎäî Í≤ÉÏù¥ Ï§ëÏöîÌï©ÎãàÎã§. `image` Ïó¥Ïù¥ ÏóÜÏúºÎ©¥ `pixel_values`ÏùÑ ÏÉùÏÑ±Ìï† Ïàò ÏóÜÏäµÎãàÎã§. Ïù¥Îü∞ Í≤ΩÏö∞Î•º Î∞©ÏßÄÌïòÎ†§Î©¥ `remove_unused_columns=False`Î°ú ÏÑ§Ï†ïÌïòÏÑ∏Ïöî! Ïú†ÏùºÌïòÍ≤å ÌïÑÏöîÌïú Îã§Î•∏ Îß§Í∞úÎ≥ÄÏàòÎäî Î™®Îç∏ÏùÑ Ï†ÄÏû•Ìï† ÏúÑÏπòÎ•º ÏßÄÏ†ïÌïòÎäî `output_dir`ÏûÖÎãàÎã§. `push_to_hub=True`Î•º ÏÑ§Ï†ïÌïòÏó¨ Ïù¥ Î™®Îç∏ÏùÑ HubÏóê Ìë∏ÏãúÌï©ÎãàÎã§(Î™®Îç∏ÏùÑ ÏóÖÎ°úÎìúÌïòÎ†§Î©¥ Hugging FaceÏóê Î°úÍ∑∏Ïù∏Ìï¥Ïïº Ìï©ÎãàÎã§). Í∞Å ÏóêÌè¨ÌÅ¨Í∞Ä ÎÅùÎÇ† ÎïåÎßàÎã§ [`Trainer`]Í∞Ä IoU Î©îÌä∏Î¶≠ÏùÑ ÌèâÍ∞ÄÌïòÍ≥† ÌïôÏäµ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Î•º Ï†ÄÏû•Ìï©ÎãàÎã§.
2. Î™®Îç∏, Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏, ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä, Îç∞Ïù¥ÌÑ∞ ÏΩúÎ†àÏù¥ÌÑ∞, `compute_metrics` Ìï®ÏàòÏôÄ Ìï®Íªò ÌïôÏäµ Ïù∏ÏûêÎ•º [`Trainer`]Ïóê Ï†ÑÎã¨ÌïòÏÑ∏Ïöî.
3. Î™®Îç∏ÏùÑ ÎØ∏ÏÑ∏ Ï°∞Ï†ïÌïòÍ∏∞ ÏúÑÌï¥ [`~Trainer.train`]Î•º Ìò∏Ï∂úÌïòÏÑ∏Ïöî.

```py
>>> training_args = TrainingArguments(
...     output_dir="segformer-b0-scene-parse-150",
...     learning_rate=6e-5,
...     num_train_epochs=50,
...     per_device_train_batch_size=2,
...     per_device_eval_batch_size=2,
...     save_total_limit=3,
...     eval_strategy="steps",
...     save_strategy="steps",
...     save_steps=20,
...     eval_steps=20,
...     logging_steps=1,
...     eval_accumulation_steps=5,
...     remove_unused_columns=False,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=train_ds,
...     eval_dataset=test_ds,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```
ÌïôÏäµÏù¥ ÏôÑÎ£åÎêòÎ©¥, ÎàÑÍµ¨ÎÇò Î™®Îç∏ÏùÑ ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎèÑÎ°ù [`~transformers.Trainer.push_to_hub`] Î©îÏÑúÎìúÎ•º ÏÇ¨Ïö©Ìï¥ HubÏóê Î™®Îç∏ÏùÑ Í≥µÏú†ÌïòÏÑ∏Ïöî:

```py
>>> trainer.push_to_hub()
```


## Ï∂îÎ°†ÌïòÍ∏∞[[inference]]

Ïù¥Ï†ú Î™®Îç∏ÏùÑ ÎØ∏ÏÑ∏ Ï°∞Ï†ïÌñàÏúºÎãà Ï∂îÎ°†Ïóê ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§!

Ï∂îÎ°†Ìï† Ïù¥ÎØ∏ÏßÄÎ•º Î°úÎìúÌïòÏÑ∏Ïöî:

```py
>>> image = ds[0]["image"]
>>> image
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-image.png" alt="Image of bedroom"/>
</div>


Ï∂îÎ°†ÏùÑ ÏúÑÌï¥ ÎØ∏ÏÑ∏ Ï°∞Ï†ïÌïú Î™®Îç∏ÏùÑ ÏãúÌóòÌï¥ Î≥¥Îäî Í∞ÄÏû• Í∞ÑÎã®Ìïú Î∞©Î≤ïÏùÄ [`pipeline`]ÏóêÏÑú ÏÇ¨Ïö©ÌïòÎäî Í≤ÉÏûÖÎãàÎã§. Î™®Îç∏ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Ïù¥ÎØ∏ÏßÄ Î∂ÑÌï†ÏùÑ ÏúÑÌïú `pipeline`ÏùÑ Ïù∏Ïä§ÌÑ¥Ïä§ÌôîÌïòÍ≥† Ïù¥ÎØ∏ÏßÄÎ•º Ï†ÑÎã¨Ìï©ÎãàÎã§:

```py
>>> from transformers import pipeline

>>> segmenter = pipeline("image-segmentation", model="my_awesome_seg_model")
>>> segmenter(image)
[{'score': None,
  'label': 'wall',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062690>},
 {'score': None,
  'label': 'sky',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062A50>},
 {'score': None,
  'label': 'floor',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062B50>},
 {'score': None,
  'label': 'ceiling',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062A10>},
 {'score': None,
  'label': 'bed ',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062E90>},
 {'score': None,
  'label': 'windowpane',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062390>},
 {'score': None,
  'label': 'cabinet',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062550>},
 {'score': None,
  'label': 'chair',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062D90>},
 {'score': None,
  'label': 'armchair',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062E10>}]
```
ÏõêÌïòÎäî Í≤ΩÏö∞ `pipeline`Ïùò Í≤∞Í≥ºÎ•º ÏàòÎèôÏúºÎ°ú Î≥µÏ†úÌï† ÏàòÎèÑ ÏûàÏäµÎãàÎã§. Ïù¥ÎØ∏ÏßÄ ÌîÑÎ°úÏÑ∏ÏÑúÎ°ú Ïù¥ÎØ∏ÏßÄÎ•º Ï≤òÎ¶¨ÌïòÍ≥† `pixel_values`ÏùÑ GPUÏóê Î∞∞ÏπòÌï©ÎãàÎã§:

```py
>>> device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # Í∞ÄÎä•ÌïòÎã§Î©¥ GPUÎ•º ÏÇ¨Ïö©ÌïòÍ≥†, Í∑∏Î†áÏßÄ ÏïäÎã§Î©¥ CPUÎ•º ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî
>>> encoding = image_processor(image, return_tensors="pt")
>>> pixel_values = encoding.pixel_values.to(device)
```

Î™®Îç∏Ïóê ÏûÖÎ†•ÏùÑ Ï†ÑÎã¨ÌïòÍ≥† `logits`Î•º Î∞òÌôòÌï©ÎãàÎã§:

```py
>>> outputs = model(pixel_values=pixel_values)
>>> logits = outputs.logits.cpu()
```
Í∑∏Îü∞ Îã§Ïùå Î°úÏßìÏùò ÌÅ¨Í∏∞Î•º ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞Î°ú Îã§Ïãú Ï°∞Ï†ïÌï©ÎãàÎã§:

```py
>>> upsampled_logits = nn.functional.interpolate(
...     logits,
...     size=image.size[::-1],
...     mode="bilinear",
...     align_corners=False,
... )

>>> pred_seg = upsampled_logits.argmax(dim=1)[0]
```


Í≤∞Í≥ºÎ•º ÏãúÍ∞ÅÌôîÌïòÎ†§Î©¥ [dataset color palette](https://github.com/tensorflow/models/blob/3f1ca33afe3c1631b733ea7e40c294273b9e406d/research/deeplab/utils/get_dataset_colormap.py#L51)Î•º Í∞Å ÌÅ¥ÎûòÏä§Î•º RGB Í∞íÏóê Îß§ÌïëÌïòÎäî `ade_palette()`Î°ú Î°úÎìúÌï©ÎãàÎã§. Í∑∏Îü∞ Îã§Ïùå Ïù¥ÎØ∏ÏßÄÏôÄ ÏòàÏ∏°Îêú Î∂ÑÌï† ÏßÄÎèÑ(segmentation map)ÏùÑ Í≤∞Ìï©ÌïòÏó¨ Íµ¨ÏÑ±Ìï† Ïàò ÏûàÏäµÎãàÎã§:

```py
>>> import matplotlib.pyplot as plt
>>> import numpy as np

>>> color_seg = np.zeros((pred_seg.shape[0], pred_seg.shape[1], 3), dtype=np.uint8)
>>> palette = np.array(ade_palette())
>>> for label, color in enumerate(palette):
...     color_seg[pred_seg == label, :] = color
>>> color_seg = color_seg[..., ::-1]  # BGRÎ°ú Î≥ÄÌôò

>>> img = np.array(image) * 0.5 + color_seg * 0.5  # Î∂ÑÌï† ÏßÄÎèÑÏúºÎ°ú Ïù¥ÎØ∏ÏßÄ Íµ¨ÏÑ±
>>> img = img.astype(np.uint8)

>>> plt.figure(figsize=(15, 10))
>>> plt.imshow(img)
>>> plt.show()
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-preds.png" alt="Image of bedroom overlaid with segmentation map"/>
</div>
