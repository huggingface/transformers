<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# ì˜ë¯¸ì  ë¶„í• (Semantic segmentation)[[semantic-segmentation]]

[[open-in-colab]]

<Youtube id="dKE8SIt9C-w"/>

ì˜ë¯¸ì  ë¶„í• (semantic segmentation)ì€ ì´ë¯¸ì§€ì˜ ê° í”½ì…€ì— ë ˆì´ë¸” ë˜ëŠ” í´ë˜ìŠ¤ë¥¼ í• ë‹¹í•©ë‹ˆë‹¤. ë¶„í• (segmentation)ì—ëŠ” ì—¬ëŸ¬ ì¢…ë¥˜ê°€ ìˆìœ¼ë©°, ì˜ë¯¸ì  ë¶„í• ì˜ ê²½ìš° ë™ì¼í•œ ë¬¼ì²´ì˜ ê³ ìœ  ì¸ìŠ¤í„´ìŠ¤ë¥¼ êµ¬ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‘ ë¬¼ì²´ ëª¨ë‘ ë™ì¼í•œ ë ˆì´ë¸”ì´ ì§€ì •ë©ë‹ˆë‹¤(ì˜ˆì‹œë¡œ, "car-1" ê³¼ "car-2" ëŒ€ì‹  "car"ë¡œ ì§€ì •í•©ë‹ˆë‹¤).
ì‹¤ìƒí™œì—ì„œ í”íˆ ë³¼ ìˆ˜ ìˆëŠ” ì˜ë¯¸ì  ë¶„í• ì˜ ì ìš© ì‚¬ë¡€ë¡œëŠ” ë³´í–‰ìì™€ ì¤‘ìš”í•œ êµí†µ ì •ë³´ë¥¼ ì‹ë³„í•˜ëŠ” ììœ¨ ì£¼í–‰ ìë™ì°¨ í•™ìŠµ, ì˜ë£Œ ì´ë¯¸ì§€ì˜ ì„¸í¬ì™€ ì´ìƒ ì§•í›„ ì‹ë³„, ê·¸ë¦¬ê³  ìœ„ì„± ì´ë¯¸ì§€ì˜ í™˜ê²½ ë³€í™” ëª¨ë‹ˆí„°ë§ë“±ì´ ìˆìŠµë‹ˆë‹¤.

ì´ë²ˆ ê°€ì´ë“œì—ì„œ ë°°ìš¸ ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1. [SceneParse150](https://huggingface.co/datasets/scene_parse_150) ë°ì´í„° ì„¸íŠ¸ë¥¼ ì´ìš©í•´ [SegFormer](https://huggingface.co/docs/transformers/main/en/model_doc/segformer#segformer) ë¯¸ì„¸ ì¡°ì •í•˜ê¸°.
2. ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì„ ì¶”ë¡ ì— ì‚¬ìš©í•˜ê¸°.

<Tip>
ì´ íŠœí† ë¦¬ì–¼ì—ì„œ ì„¤ëª…í•˜ëŠ” ì‘ì—…ì€ ë‹¤ìŒ ëª¨ë¸ ì•„í‚¤í…ì²˜ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤:

<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->

[BEiT](../model_doc/beit), [Data2VecVision](../model_doc/data2vec-vision), [DPT](../model_doc/dpt), [MobileNetV2](../model_doc/mobilenet_v2), [MobileViT](../model_doc/mobilevit), [MobileViTV2](../model_doc/mobilevitv2), [SegFormer](../model_doc/segformer), [UPerNet](../model_doc/upernet)

<!--End of the generated tip-->

</Tip>

ì‹œì‘í•˜ê¸° ì „ì— í•„ìš”í•œ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:

```bash
pip install -q datasets transformers evaluate
```
ì»¤ë®¤ë‹ˆí‹°ì— ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ê³  ê³µìœ í•  ìˆ˜ ìˆë„ë¡ Hugging Face ê³„ì •ì— ë¡œê·¸ì¸í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ê°€ ë‚˜íƒ€ë‚˜ë©´ í† í°ì„ ì…ë ¥í•˜ì—¬ ë¡œê·¸ì¸í•˜ì„¸ìš”:

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## SceneParse150 ë°ì´í„° ì„¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°[[load-sceneparse150-dataset]]

ğŸ¤— Datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ SceneParse150 ë°ì´í„° ì„¸íŠ¸ì˜ ë” ì‘ì€ ë¶€ë¶„ ì§‘í•©ì„ ê°€ì ¸ì˜¤ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ë°ì´í„° ì„¸íŠ¸ ì „ì²´ì— ëŒ€í•œ í›ˆë ¨ì— ë§ì€ ì‹œê°„ì„ í• ì• í•˜ê¸° ì „ì— ì‹¤í—˜ì„ í†µí•´ ëª¨ë“  ê²ƒì´ ì œëŒ€ë¡œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```py
>>> from datasets import load_dataset

>>> ds = load_dataset("scene_parse_150", split="train[:50]")
```

ë°ì´í„° ì„¸íŠ¸ì˜ `train`ì„ [`~datasets.Dataset.train_test_split`] ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë¶„í• í•˜ì„¸ìš”:

```py
>>> ds = ds.train_test_split(test_size=0.2)
>>> train_ds = ds["train"]
>>> test_ds = ds["test"]
```

ê·¸ë¦¬ê³  ì˜ˆì‹œë¥¼ ì‚´í´ë³´ì„¸ìš”:

```py
>>> train_ds[0]
{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x683 at 0x7F9B0C201F90>,
 'annotation': <PIL.PngImagePlugin.PngImageFile image mode=L size=512x683 at 0x7F9B0C201DD0>,
 'scene_category': 368}
```

- `image`: ì¥ë©´ì˜ PIL ì´ë¯¸ì§€ì…ë‹ˆë‹¤.
- `annotation`: ë¶„í•  ì§€ë„(segmentation map)ì˜ PIL ì´ë¯¸ì§€ì…ë‹ˆë‹¤. ëª¨ë¸ì˜ íƒ€ê²Ÿì´ê¸°ë„ í•©ë‹ˆë‹¤.
- `scene_category`: "ì£¼ë°©" ë˜ëŠ” "ì‚¬ë¬´ì‹¤"ê³¼ ê°™ì´ ì´ë¯¸ì§€ ì¥ë©´ì„ ì„¤ëª…í•˜ëŠ” ì¹´í…Œê³ ë¦¬ IDì…ë‹ˆë‹¤. ì´ ê°€ì´ë“œì—ì„œëŠ” ë‘˜ ë‹¤ PIL ì´ë¯¸ì§€ì¸ `image`ì™€ `annotation`ë§Œì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

ë‚˜ì¤‘ì— ëª¨ë¸ì„ ì„¤ì •í•  ë•Œ ìœ ìš©í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë ˆì´ë¸” IDë¥¼ ë ˆì´ë¸” í´ë˜ìŠ¤ì— ë§¤í•‘í•˜ëŠ” ì‚¬ì „ë„ ë§Œë“¤ê³  ì‹¶ì„ ê²ƒì…ë‹ˆë‹¤. Hubì—ì„œ ë§¤í•‘ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  `id2label` ë° `label2id` ì‚¬ì „ì„ ë§Œë“œì„¸ìš”:

```py
>>> import json
>>> from huggingface_hub import cached_download, hf_hub_url

>>> repo_id = "huggingface/label-files"
>>> filename = "ade20k-id2label.json"
>>> id2label = json.load(open(cached_download(hf_hub_url(repo_id, filename, repo_type="dataset")), "r"))
>>> id2label = {int(k): v for k, v in id2label.items()}
>>> label2id = {v: k for k, v in id2label.items()}
>>> num_labels = len(id2label)
```

## ì „ì²˜ë¦¬í•˜ê¸°[[preprocess]

ë‹¤ìŒ ë‹¨ê³„ëŠ” ëª¨ë¸ì— ì‚¬ìš©í•  ì´ë¯¸ì§€ì™€ ì£¼ì„ì„ ì¤€ë¹„í•˜ê¸° ìœ„í•´ SegFormer ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” ë°ì´í„° ì„¸íŠ¸ì™€ ê°™ì€ ì¼ë¶€ ë°ì´í„° ì„¸íŠ¸ëŠ” ë°°ê²½ í´ë˜ìŠ¤ë¡œ ì œë¡œ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ë°°ê²½ í´ë˜ìŠ¤ëŠ” 150ê°œì˜ í´ë˜ìŠ¤ì— ì‹¤ì œë¡œëŠ” í¬í•¨ë˜ì§€ ì•Šê¸° ë•Œë¬¸ì— `reduce_labels=True` ë¥¼ ì„¤ì •í•´ ëª¨ë“  ë ˆì´ë¸”ì—ì„œ ë°°ê²½ í´ë˜ìŠ¤ë¥¼ ì œê±°í•´ì•¼ í•©ë‹ˆë‹¤. ì œë¡œ ì¸ë±ìŠ¤ëŠ” `255`ë¡œ ëŒ€ì²´ë˜ë¯€ë¡œ SegFormerì˜ ì†ì‹¤ í•¨ìˆ˜ì—ì„œ ë¬´ì‹œë©ë‹ˆë‹¤:

```py
>>> from transformers import AutoImageProcessor

>>> checkpoint = "nvidia/mit-b0"
>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint, reduce_labels=True)
```

<frameworkcontent>
<pt>

ì´ë¯¸ì§€ ë°ì´í„° ì„¸íŠ¸ì— ë°ì´í„° ì¦ê°•ì„ ì ìš©í•˜ì—¬ ê³¼ì í•©ì— ëŒ€í•´ ëª¨ë¸ì„ ë³´ë‹¤ ê°•ê±´í•˜ê²Œ ë§Œë“œëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤. ì´ ê°€ì´ë“œì—ì„œëŠ” [torchvision](https://pytorch.org/vision/stable/index.html)ì˜ [`ColorJitter`](https://pytorch.org/vision/stable/generated/torchvision.transforms.ColorJitter.html)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì˜ ìƒ‰ìƒ ì†ì„±ì„ ì„ì˜ë¡œ ë³€ê²½í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ, ìì‹ ì´ ì›í•˜ëŠ” ì´ë¯¸ì§€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

```py
>>> from torchvision.transforms import ColorJitter

>>> jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)
```

ì´ì œ ëª¨ë¸ì— ì‚¬ìš©í•  ì´ë¯¸ì§€ì™€ ì£¼ì„ì„ ì¤€ë¹„í•˜ê¸° ìœ„í•´ ë‘ ê°œì˜ ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤. ì´ í•¨ìˆ˜ë“¤ì€ ì´ë¯¸ì§€ë¥¼ `pixel_values`ë¡œ, ì£¼ì„ì„ `labels`ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. í›ˆë ¨ ì„¸íŠ¸ì˜ ê²½ìš° ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œì— ì´ë¯¸ì§€ë¥¼ ì œê³µí•˜ê¸° ì „ì— `jitter`ë¥¼ ì ìš©í•©ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ ê²½ìš° ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œëŠ” `images`ë¥¼ ìë¥´ê³  ì •ê·œí™”í•˜ë©°, í…ŒìŠ¤íŠ¸ ì¤‘ì—ëŠ” ë°ì´í„° ì¦ê°•ì´ ì ìš©ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ `labels`ë§Œ ìë¦…ë‹ˆë‹¤.

```py
>>> def train_transforms(example_batch):
...     images = [jitter(x) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs


>>> def val_transforms(example_batch):
...     images = [x for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs
```

ëª¨ë“  ë°ì´í„° ì„¸íŠ¸ì— `jitter`ë¥¼ ì ìš©í•˜ë ¤ë©´, ğŸ¤— Datasets [`~datasets.Dataset.set_transform`] í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ì¦‰ì‹œ ë³€í™˜ì´ ì ìš©ë˜ê¸° ë•Œë¬¸ì— ë” ë¹ ë¥´ê³  ë””ìŠ¤í¬ ê³µê°„ì„ ëœ ì°¨ì§€í•©ë‹ˆë‹¤:

```py
>>> train_ds.set_transform(train_transforms)
>>> test_ds.set_transform(val_transforms)
```

</pt>
</frameworkcontent>

<frameworkcontent>
<tf>

ì´ë¯¸ì§€ ë°ì´í„° ì„¸íŠ¸ì— ë°ì´í„° ì¦ê°•ì„ ì ìš©í•˜ì—¬ ê³¼ì í•©ì— ëŒ€í•´ ëª¨ë¸ì„ ë³´ë‹¤ ê°•ê±´í•˜ê²Œ ë§Œë“œëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤. ì´ ê°€ì´ë“œì—ì„œëŠ” [`tf.image`](https://www.tensorflow.org/api_docs/python/tf/image)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì˜ ìƒ‰ìƒ ì†ì„±ì„ ì„ì˜ë¡œ ë³€ê²½í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ, ìì‹ ì´ ì›í•˜ëŠ” ì´ë¯¸ì§€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

ë³„ê°œì˜ ë‘ ë³€í™˜ í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤:
- ì´ë¯¸ì§€ ì¦ê°•ì„ í¬í•¨í•˜ëŠ” í•™ìŠµ ë°ì´í„° ë³€í™˜
- ğŸ¤— Transformersì˜ ì»´í“¨í„° ë¹„ì „ ëª¨ë¸ì€ ì±„ë„ ìš°ì„  ë ˆì´ì•„ì›ƒì„ ê¸°ëŒ€í•˜ê¸° ë•Œë¬¸ì—, ì´ë¯¸ì§€ë§Œ ë°”ê¾¸ëŠ” ê²€ì¦ ë°ì´í„° ë³€í™˜

```py
>>> import tensorflow as tf


>>> def aug_transforms(image):
...     image = tf.keras.utils.img_to_array(image)
...     image = tf.image.random_brightness(image, 0.25)
...     image = tf.image.random_contrast(image, 0.5, 2.0)
...     image = tf.image.random_saturation(image, 0.75, 1.25)
...     image = tf.image.random_hue(image, 0.1)
...     image = tf.transpose(image, (2, 0, 1))
...     return image


>>> def transforms(image):
...     image = tf.keras.utils.img_to_array(image)
...     image = tf.transpose(image, (2, 0, 1))
...     return image
```

ê·¸ëŸ° ë‹¤ìŒ ëª¨ë¸ì„ ìœ„í•´ ë‘ ê°œì˜ ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ì´ë¯¸ì§€ ë° ì£¼ì„ ë°°ì¹˜ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤. ì´ í•¨ìˆ˜ë“¤ì€ ì´ë¯¸ì§€ ë³€í™˜ì„ ì ìš©í•˜ê³  ì´ì „ì— ë¡œë“œí•œ `image_processor`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ `pixel_values`ë¡œ, ì£¼ì„ì„ `label`ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. `ImageProcessor` ëŠ” ì´ë¯¸ì§€ì˜ í¬ê¸° ì¡°ì •ê³¼ ì •ê·œí™”ë„ ì²˜ë¦¬í•©ë‹ˆë‹¤.

```py
>>> def train_transforms(example_batch):
...     images = [aug_transforms(x.convert("RGB")) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs


>>> def val_transforms(example_batch):
...     images = [transforms(x.convert("RGB")) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs
```

ì „ì²´ ë°ì´í„° ì§‘í•©ì— ì „ì²˜ë¦¬ ë³€í™˜ì„ ì ìš©í•˜ë ¤ë©´ ğŸ¤— Datasets [`~datasets.Dataset.set_transform`] í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
ì¦‰ì‹œ ë³€í™˜ì´ ì ìš©ë˜ê¸° ë•Œë¬¸ì— ë” ë¹ ë¥´ê³  ë””ìŠ¤í¬ ê³µê°„ì„ ëœ ì°¨ì§€í•©ë‹ˆë‹¤:

```py
>>> train_ds.set_transform(train_transforms)
>>> test_ds.set_transform(val_transforms)
```
</tf>
</frameworkcontent>

## í‰ê°€í•˜ê¸°[[evaluate]]

í›ˆë ¨ ì¤‘ì— ë©”íŠ¸ë¦­ì„ í¬í•¨í•˜ë©´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê°€ ë°©ë²•ì„ ë¹ ë¥´ê²Œ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ íƒœìŠ¤í¬ì—ì„œëŠ” [mean Intersection over Union](https://huggingface.co/spaces/evaluate-metric/accuracy) (IoU) ë©”íŠ¸ë¦­ì„ ë¡œë“œí•˜ì„¸ìš” (ë©”íŠ¸ë¦­ì„ ë¡œë“œí•˜ê³  ê³„ì‚°í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ë ¤ë©´ ğŸ¤— Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour)ë¥¼ ì‚´í´ë³´ì„¸ìš”).

```py
>>> import evaluate

>>> metric = evaluate.load("mean_iou")
```

ê·¸ëŸ° ë‹¤ìŒ ë©”íŠ¸ë¦­ì„ [`~evaluate.EvaluationModule.compute`]í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤. ì˜ˆì¸¡ì„ ë¨¼ì € ë¡œì§“ìœ¼ë¡œ ë³€í™˜í•œ ë‹¤ìŒ, ë ˆì´ë¸”ì˜ í¬ê¸°ì— ë§ê²Œ ëª¨ì–‘ì„ ë‹¤ì‹œ ì§€ì •í•´ì•¼ [`~evaluate.EvaluationModule.compute`]ë¥¼ í˜¸ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

<frameworkcontent>
<pt>

```py
>>> import numpy as np
>>> import torch
>>> from torch import nn

>>> def compute_metrics(eval_pred):
...     with torch.no_grad():
...         logits, labels = eval_pred
...         logits_tensor = torch.from_numpy(logits)
...         logits_tensor = nn.functional.interpolate(
...             logits_tensor,
...             size=labels.shape[-2:],
...             mode="bilinear",
...             align_corners=False,
...         ).argmax(dim=1)

...         pred_labels = logits_tensor.detach().cpu().numpy()
...         metrics = metric.compute(
...             predictions=pred_labels,
...             references=labels,
...             num_labels=num_labels,
...             ignore_index=255,
...             reduce_labels=False,
...         )
...         for key, value in metrics.items():
...             if isinstance(value, np.ndarray):
...                 metrics[key] = value.tolist()
...         return metrics
```

</pt>
</frameworkcontent>


<frameworkcontent>
<tf>

```py
>>> def compute_metrics(eval_pred):
...     logits, labels = eval_pred
...     logits = tf.transpose(logits, perm=[0, 2, 3, 1])
...     logits_resized = tf.image.resize(
...         logits,
...         size=tf.shape(labels)[1:],
...         method="bilinear",
...     )

...     pred_labels = tf.argmax(logits_resized, axis=-1)
...     metrics = metric.compute(
...         predictions=pred_labels,
...         references=labels,
...         num_labels=num_labels,
...         ignore_index=-1,
...         reduce_labels=image_processor.do_reduce_labels,
...     )

...     per_category_accuracy = metrics.pop("per_category_accuracy").tolist()
...     per_category_iou = metrics.pop("per_category_iou").tolist()

...     metrics.update({f"accuracy_{id2label[i]}": v for i, v in enumerate(per_category_accuracy)})
...     metrics.update({f"iou_{id2label[i]}": v for i, v in enumerate(per_category_iou)})
...     return {"val_" + k: v for k, v in metrics.items()}
```

</tf>
</frameworkcontent>

ì´ì œ `compute_metrics` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. íŠ¸ë ˆì´ë‹ì„ ì„¤ì •í•  ë•Œ ì´ í•¨ìˆ˜ë¡œ ëŒì•„ê°€ê²Œ ë©ë‹ˆë‹¤.

## í•™ìŠµí•˜ê¸°[[train]]
<frameworkcontent>
<pt>
<Tip>

ë§Œì•½ [`Trainer`]ë¥¼ ì‚¬ìš©í•´ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ê²ƒì— ìµìˆ™í•˜ì§€ ì•Šë‹¤ë©´, [ì—¬ê¸°](../training#finetune-with-trainer)ì—ì„œ ê¸°ë³¸ íŠœí† ë¦¬ì–¼ì„ ì‚´í´ë³´ì„¸ìš”!

</Tip>

ì´ì œ ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤! [`AutoModelForSemanticSegmentation`]ë¡œ SegFormerë¥¼ ë¶ˆëŸ¬ì˜¤ê³ , ëª¨ë¸ì— ë ˆì´ë¸” IDì™€ ë ˆì´ë¸” í´ë˜ìŠ¤ ê°„ì˜ ë§¤í•‘ì„ ì „ë‹¬í•©ë‹ˆë‹¤:

```py
>>> from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer

>>> model = AutoModelForSemanticSegmentation.from_pretrained(checkpoint, id2label=id2label, label2id=label2id)
```

ì´ì œ ì„¸ ë‹¨ê³„ë§Œ ë‚¨ì•˜ìŠµë‹ˆë‹¤:

1. í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ [`TrainingArguments`]ì— ì •ì˜í•©ë‹ˆë‹¤. `image` ì—´ì´ ì‚­ì œë˜ê¸° ë•Œë¬¸ì— ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì—´ì„ ì œê±°í•˜ì§€ ì•ŠëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. `image` ì—´ì´ ì—†ìœ¼ë©´ `pixel_values`ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ëŸ° ê²½ìš°ë¥¼ ë°©ì§€í•˜ë ¤ë©´ `remove_unused_columns=False`ë¡œ ì„¤ì •í•˜ì„¸ìš”! ìœ ì¼í•˜ê²Œ í•„ìš”í•œ ë‹¤ë¥¸ ë§¤ê°œë³€ìˆ˜ëŠ” ëª¨ë¸ì„ ì €ì¥í•  ìœ„ì¹˜ë¥¼ ì§€ì •í•˜ëŠ” `output_dir`ì…ë‹ˆë‹¤. `push_to_hub=True`ë¥¼ ì„¤ì •í•˜ì—¬ ì´ ëª¨ë¸ì„ Hubì— í‘¸ì‹œí•©ë‹ˆë‹¤(ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ë ¤ë©´ Hugging Faceì— ë¡œê·¸ì¸í•´ì•¼ í•©ë‹ˆë‹¤). ê° ì—í¬í¬ê°€ ëë‚  ë•Œë§ˆë‹¤ [`Trainer`]ê°€ IoU ë©”íŠ¸ë¦­ì„ í‰ê°€í•˜ê³  í•™ìŠµ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
2. ëª¨ë¸, ë°ì´í„° ì„¸íŠ¸, í† í¬ë‚˜ì´ì €, ë°ì´í„° ì½œë ˆì´í„°, `compute_metrics` í•¨ìˆ˜ì™€ í•¨ê»˜ í•™ìŠµ ì¸ìë¥¼ [`Trainer`]ì— ì „ë‹¬í•˜ì„¸ìš”.
3. ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ê¸° ìœ„í•´ [`~Trainer.train`]ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”.

```py
>>> training_args = TrainingArguments(
...     output_dir="segformer-b0-scene-parse-150",
...     learning_rate=6e-5,
...     num_train_epochs=50,
...     per_device_train_batch_size=2,
...     per_device_eval_batch_size=2,
...     save_total_limit=3,
...     eval_strategy="steps",
...     save_strategy="steps",
...     save_steps=20,
...     eval_steps=20,
...     logging_steps=1,
...     eval_accumulation_steps=5,
...     remove_unused_columns=False,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=train_ds,
...     eval_dataset=test_ds,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```
í•™ìŠµì´ ì™„ë£Œë˜ë©´, ëˆ„êµ¬ë‚˜ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ [`~transformers.Trainer.push_to_hub`] ë©”ì„œë“œë¥¼ ì‚¬ìš©í•´ Hubì— ëª¨ë¸ì„ ê³µìœ í•˜ì„¸ìš”:

```py
>>> trainer.push_to_hub()
```
</pt>
</frameworkcontent>

<frameworkcontent>
<tf>
<Tip>

Kerasë¡œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë° ìµìˆ™í•˜ì§€ ì•Šì€ ê²½ìš°, ë¨¼ì € [ê¸°ë³¸ íŠœí† ë¦¬ì–¼](../training#train-a-tensorflow-model-with-keras)ì„ í™•ì¸í•´ë³´ì„¸ìš”!

</Tip>

TensorFlowì—ì„œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ë ¤ë©´ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¥´ì„¸ìš”:
1. í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì •ì˜í•˜ê³  ì˜µí‹°ë§ˆì´ì €ì™€ í•™ìŠµë¥  ìŠ¤ì¼€ì¥´ëŸ¬ë¥¼ ì„¤ì •í•˜ì„¸ìš”.
2. ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ì„¸ìš”.
3. ğŸ¤— Datasetì„ `tf.data.Dataset`ë¡œ ë³€í™˜í•˜ì„¸ìš”.
4. ëª¨ë¸ì„ ì»´íŒŒì¼í•˜ì„¸ìš”.
5. ì½œë°±ì„ ì¶”ê°€í•˜ì—¬ ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•˜ê³  ğŸ¤— Hubì— ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.
6. `fit()` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ì„ ì‹¤í–‰í•˜ì„¸ìš”.

í•˜ì´í¼íŒŒë¼ë¯¸í„°, ì˜µí‹°ë§ˆì´ì €, í•™ìŠµë¥  ìŠ¤ì¼€ì¥´ëŸ¬ë¥¼ ì •ì˜í•˜ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘í•˜ì„¸ìš”:

```py
>>> from transformers import create_optimizer

>>> batch_size = 2
>>> num_epochs = 50
>>> num_train_steps = len(train_ds) * num_epochs
>>> learning_rate = 6e-5
>>> weight_decay_rate = 0.01

>>> optimizer, lr_schedule = create_optimizer(
...     init_lr=learning_rate,
...     num_train_steps=num_train_steps,
...     weight_decay_rate=weight_decay_rate,
...     num_warmup_steps=0,
... )
```

ê·¸ëŸ° ë‹¤ìŒ ë ˆì´ë¸” ë§¤í•‘ê³¼ í•¨ê»˜ [`TFAutoModelForSemanticSegmentation`]ì„ ì‚¬ìš©í•˜ì—¬ SegFormerë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ì˜µí‹°ë§ˆì´ì €ë¡œ ì»´íŒŒì¼í•©ë‹ˆë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì€ ëª¨ë‘ ë””í´íŠ¸ë¡œ íƒœìŠ¤í¬ ê´€ë ¨ ì†ì‹¤ í•¨ìˆ˜ê°€ ìˆìœ¼ë¯€ë¡œ ì›ì¹˜ ì•Šìœ¼ë©´ ì§€ì •í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤:

```py
>>> from transformers import TFAutoModelForSemanticSegmentation

>>> model = TFAutoModelForSemanticSegmentation.from_pretrained(
...     checkpoint,
...     id2label=id2label,
...     label2id=label2id,
... )
>>> model.compile(optimizer=optimizer)  # ì†ì‹¤ í•¨ìˆ˜ ì¸ìê°€ ì—†ìŠµë‹ˆë‹¤!
```

[`~datasets.Dataset.to_tf_dataset`] ì™€ [`DefaultDataCollator`]ë¥¼ ì‚¬ìš©í•´ ë°ì´í„° ì„¸íŠ¸ë¥¼ `tf.data.Dataset` í¬ë§·ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”:

```py
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator(return_tensors="tf")

>>> tf_train_dataset = train_ds.to_tf_dataset(
...     columns=["pixel_values", "label"],
...     shuffle=True,
...     batch_size=batch_size,
...     collate_fn=data_collator,
... )

>>> tf_eval_dataset = test_ds.to_tf_dataset(
...     columns=["pixel_values", "label"],
...     shuffle=True,
...     batch_size=batch_size,
...     collate_fn=data_collator,
... )
```

ì˜ˆì¸¡ìœ¼ë¡œ ì •í™•ë„ë¥¼ ê³„ì‚°í•˜ê³  ëª¨ë¸ì„ ğŸ¤— Hubë¡œ í‘¸ì‹œí•˜ë ¤ë©´ [Keras callbacks](../main_classes/keras_callbacks)ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. `compute_metrics` í•¨ìˆ˜ë¥¼ [`KerasMetricCallback`]ì— ì „ë‹¬í•˜ê³ , ëª¨ë¸ ì—…ë¡œë“œë¥¼ ìœ„í•´ [`PushToHubCallback`]ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:

```py
>>> from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback

>>> metric_callback = KerasMetricCallback(
...     metric_fn=compute_metrics, eval_dataset=tf_eval_dataset, batch_size=batch_size, label_cols=["labels"]
... )

>>> push_to_hub_callback = PushToHubCallback(output_dir="scene_segmentation", tokenizer=image_processor)

>>> callbacks = [metric_callback, push_to_hub_callback]
```

ì´ì œ ëª¨ë¸ì„ í›ˆë ¨í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤! í›ˆë ¨ ë° ê²€ì¦ ë°ì´í„° ì„¸íŠ¸, ì—í¬í¬ ìˆ˜ì™€ í•¨ê»˜ `fit()`ì„ í˜¸ì¶œí•˜ê³ , ì½œë°±ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•©ë‹ˆë‹¤:

```py
>>> model.fit(
...     tf_train_dataset,
...     validation_data=tf_eval_dataset,
...     callbacks=callbacks,
...     epochs=num_epochs,
... )
```

ì¶•í•˜í•©ë‹ˆë‹¤! ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ê³  ğŸ¤— Hubì— ê³µìœ í–ˆìŠµë‹ˆë‹¤. ì´ì œ ì¶”ë¡ ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

</tf>
</frameworkcontent>


## ì¶”ë¡ í•˜ê¸°[[inference]]

ì´ì œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í–ˆìœ¼ë‹ˆ ì¶”ë¡ ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

ì¶”ë¡ í•  ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•˜ì„¸ìš”:

```py
>>> image = ds[0]["image"]
>>> image
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-image.png" alt="Image of bedroom"/>
</div>

<frameworkcontent>
<pt>

ì¶”ë¡ ì„ ìœ„í•´ ë¯¸ì„¸ ì¡°ì •í•œ ëª¨ë¸ì„ ì‹œí—˜í•´ ë³´ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì€ [`pipeline`]ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ë¶„í• ì„ ìœ„í•œ `pipeline`ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ê³  ì´ë¯¸ì§€ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤:

```py
>>> from transformers import pipeline

>>> segmenter = pipeline("image-segmentation", model="my_awesome_seg_model")
>>> segmenter(image)
[{'score': None,
  'label': 'wall',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062690>},
 {'score': None,
  'label': 'sky',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062A50>},
 {'score': None,
  'label': 'floor',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062B50>},
 {'score': None,
  'label': 'ceiling',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062A10>},
 {'score': None,
  'label': 'bed ',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062E90>},
 {'score': None,
  'label': 'windowpane',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062390>},
 {'score': None,
  'label': 'cabinet',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062550>},
 {'score': None,
  'label': 'chair',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062D90>},
 {'score': None,
  'label': 'armchair',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062E10>}]
```
ì›í•˜ëŠ” ê²½ìš° `pipeline`ì˜ ê²°ê³¼ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ë³µì œí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¡œ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ê³  `pixel_values`ì„ GPUì— ë°°ì¹˜í•©ë‹ˆë‹¤:

```py
>>> device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # ê°€ëŠ¥í•˜ë‹¤ë©´ GPUë¥¼ ì‚¬ìš©í•˜ê³ , ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ CPUë¥¼ ì‚¬ìš©í•˜ì„¸ìš”
>>> encoding = image_processor(image, return_tensors="pt")
>>> pixel_values = encoding.pixel_values.to(device)
```

ëª¨ë¸ì— ì…ë ¥ì„ ì „ë‹¬í•˜ê³  `logits`ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤:

```py
>>> outputs = model(pixel_values=pixel_values)
>>> logits = outputs.logits.cpu()
```
ê·¸ëŸ° ë‹¤ìŒ ë¡œì§“ì˜ í¬ê¸°ë¥¼ ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ë‹¤ì‹œ ì¡°ì •í•©ë‹ˆë‹¤:

```py
>>> upsampled_logits = nn.functional.interpolate(
...     logits,
...     size=image.size[::-1],
...     mode="bilinear",
...     align_corners=False,
... )

>>> pred_seg = upsampled_logits.argmax(dim=1)[0]
```

</pt>
</frameworkcontent>

<frameworkcontent>
<tf>
ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¥¼ ë¡œë“œí•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ì „ì²˜ë¦¬í•˜ê³  ì…ë ¥ì„ TensorFlow í…ì„œë¡œ ë°˜í™˜í•©ë‹ˆë‹¤:

```py
>>> from transformers import AutoImageProcessor

>>> image_processor = AutoImageProcessor.from_pretrained("MariaK/scene_segmentation")
>>> inputs = image_processor(image, return_tensors="tf")
```

ëª¨ë¸ì— ì…ë ¥ì„ ì „ë‹¬í•˜ê³  `logits`ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤:

```py
>>> from transformers import TFAutoModelForSemanticSegmentation

>>> model = TFAutoModelForSemanticSegmentation.from_pretrained("MariaK/scene_segmentation")
>>> logits = model(**inputs).logits
```

ê·¸ëŸ° ë‹¤ìŒ ë¡œê·¸ë¥¼ ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ì¬ì¡°ì •í•˜ê³  í´ë˜ìŠ¤ ì°¨ì›ì— argmaxë¥¼ ì ìš©í•©ë‹ˆë‹¤:

```py
>>> logits = tf.transpose(logits, [0, 2, 3, 1])

>>> upsampled_logits = tf.image.resize(
...     logits,
...     # `image.size`ê°€ ë„ˆë¹„ì™€ ë†’ì´ë¥¼ ë°˜í™˜í•˜ê¸° ë•Œë¬¸ì— `image`ì˜ ëª¨ì–‘ì„ ë°˜ì „ì‹œí‚µë‹ˆë‹¤
...     image.size[::-1],
... )

>>> pred_seg = tf.math.argmax(upsampled_logits, axis=-1)[0]
```

</tf>
</frameworkcontent>

ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ë ¤ë©´ [dataset color palette](https://github.com/tensorflow/models/blob/3f1ca33afe3c1631b733ea7e40c294273b9e406d/research/deeplab/utils/get_dataset_colormap.py#L51)ë¥¼ ê° í´ë˜ìŠ¤ë¥¼ RGB ê°’ì— ë§¤í•‘í•˜ëŠ” `ade_palette()`ë¡œ ë¡œë“œí•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì´ë¯¸ì§€ì™€ ì˜ˆì¸¡ëœ ë¶„í•  ì§€ë„(segmentation map)ì„ ê²°í•©í•˜ì—¬ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
>>> import matplotlib.pyplot as plt
>>> import numpy as np

>>> color_seg = np.zeros((pred_seg.shape[0], pred_seg.shape[1], 3), dtype=np.uint8)
>>> palette = np.array(ade_palette())
>>> for label, color in enumerate(palette):
...     color_seg[pred_seg == label, :] = color
>>> color_seg = color_seg[..., ::-1]  # BGRë¡œ ë³€í™˜

>>> img = np.array(image) * 0.5 + color_seg * 0.5  # ë¶„í•  ì§€ë„ìœ¼ë¡œ ì´ë¯¸ì§€ êµ¬ì„±
>>> img = img.astype(np.uint8)

>>> plt.figure(figsize=(15, 10))
>>> plt.imshow(img)
>>> plt.show()
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-preds.png" alt="Image of bedroom overlaid with segmentation map"/>
</div>
