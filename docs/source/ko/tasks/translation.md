<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# ë²ˆì—­[[translation]]

[[open-in-colab]]

<Youtube id="1JvfrvZgi6c"/>

ë²ˆì—­ì€ í•œ ì–¸ì–´ë¡œ ëœ ì‹œí€€ìŠ¤ë¥¼ ë‹¤ë¥¸ ì–¸ì–´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ë²ˆì—­ì´ë‚˜ ìš”ì•½ì€ ì…ë ¥ì„ ë°›ì•„ ì¼ë ¨ì˜ ì¶œë ¥ì„ ë°˜í™˜í•˜ëŠ” ê°•ë ¥í•œ í”„ë ˆì„ì›Œí¬ì¸ ì‹œí€€ìŠ¤-íˆ¬-ì‹œí€€ìŠ¤ ë¬¸ì œë¡œ êµ¬ì„±í•  ìˆ˜ ìˆëŠ” ëŒ€í‘œì ì¸ íƒœìŠ¤í¬ì…ë‹ˆë‹¤. ë²ˆì—­ ì‹œìŠ¤í…œì€ ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ë¥¸ ì–¸ì–´ë¡œ ëœ í…ìŠ¤íŠ¸ ê°„ì˜ ë²ˆì—­ì— ì‚¬ìš©ë˜ì§€ë§Œ, ìŒì„± ê°„ì˜ í†µì—­ì´ë‚˜ í…ìŠ¤íŠ¸-ìŒì„± ë˜ëŠ” ìŒì„±-í…ìŠ¤íŠ¸ì™€ ê°™ì€ ì¡°í•©ì—ë„ ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ ê°€ì´ë“œì—ì„œ í•™ìŠµí•  ë‚´ìš©ì€:

1. ì˜ì–´ í…ìŠ¤íŠ¸ë¥¼ í”„ë‘ìŠ¤ì–´ë¡œ ë²ˆì—­í•˜ê¸° ìœ„í•´ [T5](https://huggingface.co/google-t5/t5-small) ëª¨ë¸ì„ OPUS Books ë°ì´í„°ì„¸íŠ¸ì˜ ì˜ì–´-í”„ë‘ìŠ¤ì–´ í•˜ìœ„ ì§‘í•©ìœ¼ë¡œ íŒŒì¸íŠœë‹í•˜ëŠ” ë°©ë²•ê³¼
2. íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ì¶”ë¡ ì— ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

<Tip>
ì´ íƒœìŠ¤í¬ ê°€ì´ë“œëŠ” ì•„ë˜ ëª¨ë¸ ì•„í‚¤í…ì²˜ì—ë„ ì‘ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->

[BART](../model_doc/bart), [BigBird-Pegasus](../model_doc/bigbird_pegasus), [Blenderbot](../model_doc/blenderbot), [BlenderbotSmall](../model_doc/blenderbot-small), [Encoder decoder](../model_doc/encoder-decoder), [FairSeq Machine-Translation](../model_doc/fsmt), [GPTSAN-japanese](../model_doc/gptsan-japanese), [LED](../model_doc/led), [LongT5](../model_doc/longt5), [M2M100](../model_doc/m2m_100), [Marian](../model_doc/marian), [mBART](../model_doc/mbart), [MT5](../model_doc/mt5), [MVP](../model_doc/mvp), [NLLB](../model_doc/nllb), [NLLB-MOE](../model_doc/nllb-moe), [Pegasus](../model_doc/pegasus), [PEGASUS-X](../model_doc/pegasus_x), [PLBart](../model_doc/plbart), [ProphetNet](../model_doc/prophetnet), [SwitchTransformers](../model_doc/switch_transformers), [T5](../model_doc/t5), [XLM-ProphetNet](../model_doc/xlm-prophetnet)

<!--End of the generated tip-->

</Tip>

ì‹œì‘í•˜ê¸° ì „ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ëª¨ë‘ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:

```bash
pip install transformers datasets evaluate sacrebleu
```

ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ê³  ì»¤ë®¤ë‹ˆí‹°ì™€ ê³µìœ í•  ìˆ˜ ìˆë„ë¡ Hugging Face ê³„ì •ì— ë¡œê·¸ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ì°½ì´ í‘œì‹œë˜ë©´ í† í°ì„ ì…ë ¥í•˜ì—¬ ë¡œê·¸ì¸í•˜ì„¸ìš”.

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## OPUS Books ë°ì´í„°ì„¸íŠ¸ ê°€ì ¸ì˜¤ê¸°[[load-opus-books-dataset]]

ë¨¼ì € ğŸ¤— Datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ [OPUS Books](https://huggingface.co/datasets/opus_books) ë°ì´í„°ì„¸íŠ¸ì˜ ì˜ì–´-í”„ë‘ìŠ¤ì–´ í•˜ìœ„ ì§‘í•©ì„ ê°€ì ¸ì˜¤ì„¸ìš”.

```py
>>> from datasets import load_dataset

>>> books = load_dataset("opus_books", "en-fr")
```

ë°ì´í„°ì„¸íŠ¸ë¥¼ [`~datasets.Dataset.train_test_split`] ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„í• í•˜ì„¸ìš”.

```py
>>> books = books["train"].train_test_split(test_size=0.2)
```

í›ˆë ¨ ë°ì´í„°ì—ì„œ ì˜ˆì‹œë¥¼ ì‚´í´ë³¼ê¹Œìš”?

```py
>>> books["train"][0]
{'id': '90560',
 'translation': {'en': 'But this lofty plateau measured only a few fathoms, and soon we reentered Our Element.',
  'fr': 'Mais ce plateau Ã©levÃ© ne mesurait que quelques toises, et bientÃ´t nous fÃ»mes rentrÃ©s dans notre Ã©lÃ©ment.'}}
```

ë°˜í™˜ëœ ë”•ì…”ë„ˆë¦¬ì˜ `translation` í‚¤ê°€ í…ìŠ¤íŠ¸ì˜ ì˜ì–´, í”„ë‘ìŠ¤ì–´ ë²„ì „ì„ í¬í•¨í•˜ê³  ìˆëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ì „ì²˜ë¦¬[[preprocess]]

<Youtube id="XAR8jnZZuUs"/>

ë‹¤ìŒ ë‹¨ê³„ë¡œ ì˜ì–´-í”„ë‘ìŠ¤ì–´ ìŒì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ T5 í† í¬ë‚˜ì´ì €ë¥¼ ê°€ì ¸ì˜¤ì„¸ìš”.

```py
>>> from transformers import AutoTokenizer

>>> checkpoint = "google-t5/t5-small"
>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

ë§Œë“¤ ì „ì²˜ë¦¬ í•¨ìˆ˜ëŠ” ì•„ë˜ ìš”êµ¬ì‚¬í•­ì„ ì¶©ì¡±í•´ì•¼ í•©ë‹ˆë‹¤:

1. T5ê°€ ë²ˆì—­ íƒœìŠ¤í¬ì„ì„ ì¸ì§€í•  ìˆ˜ ìˆë„ë¡ ì…ë ¥ ì•ì— í”„ë¡¬í”„íŠ¸ë¥¼ ì¶”ê°€í•˜ì„¸ìš”. ì—¬ëŸ¬ NLP íƒœìŠ¤í¬ë¥¼ í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ ì¤‘ ì¼ë¶€ëŠ” ì´ë ‡ê²Œ íƒœìŠ¤í¬ í”„ë¡¬í”„íŠ¸ë¥¼ ë¯¸ë¦¬ ì¤˜ì•¼í•©ë‹ˆë‹¤.
2. ì›ì–´(ì˜ì–´)ê³¼ ë²ˆì—­ì–´(í”„ë‘ìŠ¤ì–´)ë¥¼ ë³„ë„ë¡œ í† í°í™”í•˜ì„¸ìš”. ì˜ì–´ ì–´íœ˜ë¡œ ì‚¬ì „ í•™ìŠµëœ í† í¬ë‚˜ì´ì €ë¡œ í”„ë‘ìŠ¤ì–´ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•  ìˆ˜ëŠ” ì—†ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
3. `max_length` ë§¤ê°œë³€ìˆ˜ë¡œ ì„¤ì •í•œ ìµœëŒ€ ê¸¸ì´ë³´ë‹¤ ê¸¸ì§€ ì•Šë„ë¡ ì‹œí€€ìŠ¤ë¥¼ truncateí•˜ì„¸ìš”.

```py
>>> source_lang = "en"
>>> target_lang = "fr"
>>> prefix = "translate English to French: "


>>> def preprocess_function(examples):
...     inputs = [prefix + example[source_lang] for example in examples["translation"]]
...     targets = [example[target_lang] for example in examples["translation"]]
...     model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)
...     return model_inputs
```

ì „ì²´ ë°ì´í„°ì„¸íŠ¸ì— ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ì ìš©í•˜ë ¤ë©´ ğŸ¤— Datasetsì˜ [`~datasets.Dataset.map`] ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. `map` í•¨ìˆ˜ì˜ ì†ë„ë¥¼ ë†’ì´ë ¤ë©´ `batched=True`ë¥¼ ì„¤ì •í•˜ì—¬ ë°ì´í„°ì„¸íŠ¸ì˜ ì—¬ëŸ¬ ìš”ì†Œë¥¼ í•œ ë²ˆì— ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤.

```py
>>> tokenized_books = books.map(preprocess_function, batched=True)
```

ì´ì œ [`DataCollatorForSeq2Seq`]ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì œ ë°°ì¹˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ë°ì´í„°ì„¸íŠ¸ì˜ ìµœëŒ€ ê¸¸ì´ë¡œ ì „ë¶€ë¥¼ paddingí•˜ëŠ” ëŒ€ì‹ , ë°ì´í„° ì •ë ¬ ì¤‘ ê° ë°°ì¹˜ì˜ ìµœëŒ€ ê¸¸ì´ë¡œ ë¬¸ì¥ì„ *ë™ì ìœ¼ë¡œ padding*í•˜ëŠ” ê²ƒì´ ë” íš¨ìœ¨ì ì…ë‹ˆë‹¤.

<frameworkcontent>
<pt>
```py
>>> from transformers import DataCollatorForSeq2Seq

>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)
```
</pt>
<tf>

```py
>>> from transformers import DataCollatorForSeq2Seq

>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors="tf")
```
</tf>
</frameworkcontent>

## í‰ê°€[[evalulate]]

í›ˆë ¨ ì¤‘ì— ë©”íŠ¸ë¦­ì„ í¬í•¨í•˜ë©´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤. ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ í‰ê°€ ë°©ë²•(evaluation method)ì„ ë¹ ë¥´ê²Œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í˜„ì¬ íƒœìŠ¤í¬ì— ì í•©í•œ SacreBLEU ë©”íŠ¸ë¦­ì„ ê°€ì ¸ì˜¤ì„¸ìš”. (ë©”íŠ¸ë¦­ì„ ê°€ì ¸ì˜¤ê³  ê³„ì‚°í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ë ¤ë©´ ğŸ¤— Evaluate [ë‘˜ëŸ¬ë³´ê¸°](https://huggingface.co/docs/evaluate/a_quick_tour)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”):

```py
>>> import evaluate

>>> metric = evaluate.load("sacrebleu")
```

ê·¸ëŸ° ë‹¤ìŒ [`~evaluate.EvaluationModule.compute`]ì— ì˜ˆì¸¡ê°’ê³¼ ë ˆì´ë¸”ì„ ì „ë‹¬í•˜ì—¬ SacreBLEU ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ë¥¼ ìƒì„±í•˜ì„¸ìš”:

```py
>>> import numpy as np


>>> def postprocess_text(preds, labels):
...     preds = [pred.strip() for pred in preds]
...     labels = [[label.strip()] for label in labels]

...     return preds, labels


>>> def compute_metrics(eval_preds):
...     preds, labels = eval_preds
...     if isinstance(preds, tuple):
...         preds = preds[0]
...     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

...     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
...     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

...     decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

...     result = metric.compute(predictions=decoded_preds, references=decoded_labels)
...     result = {"bleu": result["score"]}

...     prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
...     result["gen_len"] = np.mean(prediction_lens)
...     result = {k: round(v, 4) for k, v in result.items()}
...     return result
```

ì´ì œ `compute_metrics` í•¨ìˆ˜ëŠ” ì¤€ë¹„ë˜ì—ˆê³ , í›ˆë ¨ ê³¼ì •ì„ ì„¤ì •í•  ë•Œ ë‹¤ì‹œ ì‚´í´ë³¼ ì˜ˆì •ì…ë‹ˆë‹¤.

## í›ˆë ¨[[train]]

<frameworkcontent>
<pt>
<Tip>

[`Trainer`]ë¡œ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ëŠ” ë°©ë²•ì— ìµìˆ™í•˜ì§€ ì•Šë‹¤ë©´ [ì—¬ê¸°](../training#train-with-pytorch-trainer)ì—ì„œ ê¸°ë³¸ íŠœí† ë¦¬ì–¼ì„ ì‚´í´ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤!

</Tip>

ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¬ ì¤€ë¹„ê°€ ë˜ì—ˆêµ°ìš”! [`AutoModelForSeq2SeqLM`]ìœ¼ë¡œ T5ë¥¼ ë¡œë“œí•˜ì„¸ìš”:

```py
>>> from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer

>>> model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)
```

ì´ì œ ì„¸ ë‹¨ê³„ë§Œ ê±°ì¹˜ë©´ ëì…ë‹ˆë‹¤:

1. [`Seq2SeqTrainingArguments`]ì—ì„œ í›ˆë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì •ì˜í•˜ì„¸ìš”. ìœ ì¼í•œ í•„ìˆ˜ ë§¤ê°œë³€ìˆ˜ëŠ” ëª¨ë¸ì„ ì €ì¥í•  ìœ„ì¹˜ì¸ `output_dir`ì…ë‹ˆë‹¤. ëª¨ë¸ì„ Hubì— í‘¸ì‹œí•˜ê¸° ìœ„í•´ `push_to_hub=True`ë¡œ ì„¤ì •í•˜ì„¸ìš”. (ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ë ¤ë©´ Hugging Faceì— ë¡œê·¸ì¸í•´ì•¼ í•©ë‹ˆë‹¤.) [`Trainer`]ëŠ” ì—í­ì´ ëë‚ ë•Œë§ˆë‹¤ SacreBLEU ë©”íŠ¸ë¦­ì„ í‰ê°€í•˜ê³  í›ˆë ¨ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
2. [`Seq2SeqTrainer`]ì— í›ˆë ¨ ì¸ìˆ˜ë¥¼ ì „ë‹¬í•˜ì„¸ìš”. ëª¨ë¸, ë°ì´í„° ì„¸íŠ¸, í† í¬ë‚˜ì´ì €, data collator ë° `compute_metrics` í•¨ìˆ˜ë„ ë©ë‹¬ì•„ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤.
3. [`~Trainer.train`]ì„ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ì„¸ìš”.

```py
>>> training_args = Seq2SeqTrainingArguments(
...     output_dir="my_awesome_opus_books_model",
...     eval_strategy="epoch",
...     learning_rate=2e-5,
...     per_device_train_batch_size=16,
...     per_device_eval_batch_size=16,
...     weight_decay=0.01,
...     save_total_limit=3,
...     num_train_epochs=2,
...     predict_with_generate=True,
...     fp16=True,
...     push_to_hub=True,
... )

>>> trainer = Seq2SeqTrainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_books["train"],
...     eval_dataset=tokenized_books["test"],
...     tokenizer=tokenizer,
...     data_collator=data_collator,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

í•™ìŠµì´ ì™„ë£Œë˜ë©´ [`~transformers.Trainer.push_to_hub`] ë©”ì„œë“œë¡œ ëª¨ë¸ì„ Hubì— ê³µìœ í•˜ì„¸ìš”. ì´ëŸ¬ë©´ ëˆ„êµ¬ë‚˜ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤:

```py
>>> trainer.push_to_hub()
```
</pt>
<tf>
<Tip>

Kerasë¡œ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ëŠ” ë°©ë²•ì´ ìµìˆ™í•˜ì§€ ì•Šë‹¤ë©´, [ì—¬ê¸°](../training#train-a-tensorflow-model-with-keras)ì—ì„œ ê¸°ë³¸ íŠœí† ë¦¬ì–¼ì„ ì‚´í´ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤!

</Tip>
TensorFlowì—ì„œ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ë ¤ë©´ ìš°ì„  optimizer í•¨ìˆ˜, í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ ë“±ì˜ í›ˆë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•˜ì„¸ìš”:

```py
>>> from transformers import AdamWeightDecay

>>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)
```

ì´ì œ [`TFAutoModelForSeq2SeqLM`]ë¡œ T5ë¥¼ ê°€ì ¸ì˜¤ì„¸ìš”:

```py
>>> from transformers import TFAutoModelForSeq2SeqLM

>>> model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)
```

[`~transformers.TFPreTrainedModel.prepare_tf_dataset`]ë¡œ ë°ì´í„° ì„¸íŠ¸ë¥¼ `tf.data.Dataset` í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”:

```py
>>> tf_train_set = model.prepare_tf_dataset(
...     tokenized_books["train"],
...     shuffle=True,
...     batch_size=16,
...     collate_fn=data_collator,
... )

>>> tf_test_set = model.prepare_tf_dataset(
...     tokenized_books["test"],
...     shuffle=False,
...     batch_size=16,
...     collate_fn=data_collator,
... )
```

í›ˆë ¨í•˜ê¸° ìœ„í•´ [`compile`](https://keras.io/api/models/model_training_apis/#compile-method) ë©”ì„œë“œë¡œ ëª¨ë¸ì„ êµ¬ì„±í•˜ì„¸ìš”:

```py
>>> import tensorflow as tf

>>> model.compile(optimizer=optimizer)
```

í›ˆë ¨ì„ ì‹œì‘í•˜ê¸° ì „ì— ì˜ˆì¸¡ê°’ìœ¼ë¡œë¶€í„° SacreBLEU ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•˜ëŠ” ë°©ë²•ê³¼ ëª¨ë¸ì„ Hubì— ì—…ë¡œë“œí•˜ëŠ” ë°©ë²• ë‘ ê°€ì§€ë¥¼ ë¯¸ë¦¬ ì„¤ì •í•´ë‘¬ì•¼ í•©ë‹ˆë‹¤. ë‘˜ ë‹¤ [Keras callbacks](../main_classes/keras_callbacks)ë¡œ êµ¬í˜„í•˜ì„¸ìš”.

[`~transformers.KerasMetricCallback`]ì— `compute_metrics` í•¨ìˆ˜ë¥¼ ì „ë‹¬í•˜ì„¸ìš”.

```py
>>> from transformers.keras_callbacks import KerasMetricCallback

>>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)
```

ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì—…ë¡œë“œí•  ìœ„ì¹˜ë¥¼ [`~transformers.PushToHubCallback`]ì—ì„œ ì§€ì •í•˜ì„¸ìš”:

```py
>>> from transformers.keras_callbacks import PushToHubCallback

>>> push_to_hub_callback = PushToHubCallback(
...     output_dir="my_awesome_opus_books_model",
...     tokenizer=tokenizer,
... )
```

ì´ì œ ì½œë°±ë“¤ì„ í•œë°ë¡œ ë¬¶ì–´ì£¼ì„¸ìš”:

```py
>>> callbacks = [metric_callback, push_to_hub_callback]
```

ë“œë””ì–´ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¬ ëª¨ë“  ì¤€ë¹„ë¥¼ ë§ˆì³¤êµ°ìš”! ì´ì œ í›ˆë ¨ ë° ê²€ì¦ ë°ì´í„° ì„¸íŠ¸ì— [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) ë©”ì„œë“œë¥¼ ì—í­ ìˆ˜ì™€ ë§Œë“¤ì–´ë‘” ì½œë°±ê³¼ í•¨ê»˜ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ì„¸ìš”:

```py
>>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=callbacks)
```

í•™ìŠµì´ ì™„ë£Œë˜ë©´ ëª¨ë¸ì´ ìë™ìœ¼ë¡œ Hubì— ì—…ë¡œë“œë˜ê³ , ëˆ„êµ¬ë‚˜ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤!
</tf>
</frameworkcontent>

<Tip>

ë²ˆì—­ì„ ìœ„í•´ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ë³´ë‹¤ ìì„¸í•œ ì˜ˆì œëŠ” í•´ë‹¹ [PyTorch ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb) ë˜ëŠ” [TensorFlow ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb)ì„ ì°¸ì¡°í•˜ì„¸ìš”.

</Tip>

## ì¶”ë¡ [[inference]]

ì¢‹ì•„ìš”, ì´ì œ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í–ˆìœ¼ë‹ˆ ì¶”ë¡ ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

ë‹¤ë¥¸ ì–¸ì–´ë¡œ ë²ˆì—­í•˜ê³  ì‹¶ì€ í…ìŠ¤íŠ¸ë¥¼ ì¨ë³´ì„¸ìš”. T5ì˜ ê²½ìš° ì›í•˜ëŠ” íƒœìŠ¤í¬ë¥¼ ì…ë ¥ì˜ ì ‘ë‘ì‚¬ë¡œ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì˜ì–´ì—ì„œ í”„ë‘ìŠ¤ì–´ë¡œ ë²ˆì—­í•˜ëŠ” ê²½ìš°, ì•„ë˜ì™€ ê°™ì€ ì ‘ë‘ì‚¬ê°€ ì¶”ê°€ë©ë‹ˆë‹¤:

```py
>>> text = "translate English to French: Legumes share resources with nitrogen-fixing bacteria."
```

íŒŒì¸íŠœë‹ëœ ëª¨ë¸ë¡œ ì¶”ë¡ í•˜ê¸°ì— ì œì¼ ê°„ë‹¨í•œ ë°©ë²•ì€ [`pipeline`]ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. í•´ë‹¹ ëª¨ë¸ë¡œ ë²ˆì—­ `pipeline`ì„ ë§Œë“  ë’¤, í…ìŠ¤íŠ¸ë¥¼ ì „ë‹¬í•˜ì„¸ìš”:

```py
>>> from transformers import pipeline

# Change `xx` to the language of the input and `yy` to the language of the desired output. 
# Examples: "en" for English, "fr" for French, "de" for German, "es" for Spanish, "zh" for Chinese, etc; translation_en_to_fr translates English to French
# You can view all the lists of languages here - https://huggingface.co/languages
>>> translator = pipeline("translation_xx_to_yy", model="my_awesome_opus_books_model")
>>> translator(text)
[{'translation_text': 'Legumes partagent des ressources avec des bactÃ©ries azotantes.'}]
```

ì›í•œë‹¤ë©´ `pipeline`ì˜ ê²°ê³¼ë¥¼ ì§ì ‘ ë³µì œí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:

<frameworkcontent>
<pt>
í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³  `input_ids`ë¥¼ PyTorch í…ì„œë¡œ ë°˜í™˜í•˜ì„¸ìš”:

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("my_awesome_opus_books_model")
>>> inputs = tokenizer(text, return_tensors="pt").input_ids
```

[`~transformers.generation_utils.GenerationMixin.generate`] ë©”ì„œë“œë¡œ ë²ˆì—­ì„ ìƒì„±í•˜ì„¸ìš”. ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ìƒì„± ì „ëµ ë° ìƒì„±ì„ ì œì–´í•˜ê¸° ìœ„í•œ ë§¤ê°œë³€ìˆ˜ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ [Text Generation](../main_classes/text_generation) APIë¥¼ ì‚´í´ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤.

```py
>>> from transformers import AutoModelForSeq2SeqLM

>>> model = AutoModelForSeq2SeqLM.from_pretrained("my_awesome_opus_books_model")
>>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)
```

ìƒì„±ëœ í† í° IDë“¤ì„ ë‹¤ì‹œ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©í•˜ì„¸ìš”:

```py
>>> tokenizer.decode(outputs[0], skip_special_tokens=True)
'Les lignÃ©es partagent des ressources avec des bactÃ©ries enfixant l'azote.'
```
</pt>
<tf>
í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³  `input_ids`ë¥¼ TensorFlow í…ì„œë¡œ ë°˜í™˜í•˜ì„¸ìš”:

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("my_awesome_opus_books_model")
>>> inputs = tokenizer(text, return_tensors="tf").input_ids
```

[`~transformers.generation_tf_utils.TFGenerationMixin.generate`] ë©”ì„œë“œë¡œ ë²ˆì—­ì„ ìƒì„±í•˜ì„¸ìš”. ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ìƒì„± ì „ëµ ë° ìƒì„±ì„ ì œì–´í•˜ê¸° ìœ„í•œ ë§¤ê°œë³€ìˆ˜ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ [Text Generation](../main_classes/text_generation) APIë¥¼ ì‚´í´ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤.

```py
>>> from transformers import TFAutoModelForSeq2SeqLM

>>> model = TFAutoModelForSeq2SeqLM.from_pretrained("my_awesome_opus_books_model")
>>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)
```

ìƒì„±ëœ í† í° IDë“¤ì„ ë‹¤ì‹œ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©í•˜ì„¸ìš”:

```py
>>> tokenizer.decode(outputs[0], skip_special_tokens=True)
'Les lugumes partagent les ressources avec des bactÃ©ries fixatrices d'azote.'
```
</tf>
</frameworkcontent>
