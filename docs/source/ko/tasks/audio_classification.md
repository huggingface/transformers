<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# ì˜¤ë””ì˜¤ ë¶„ë¥˜[[audio_classification]]

[[open-in-colab]]

<Youtube id="KWwzcmG98Ds"/>

ì˜¤ë””ì˜¤ ë¶„ë¥˜ëŠ” í…ìŠ¤íŠ¸ì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì…ë ¥ ë°ì´í„°ì— í´ë˜ìŠ¤ ë ˆì´ë¸” ì¶œë ¥ì„ í• ë‹¹í•©ë‹ˆë‹¤. ìœ ì¼í•œ ì°¨ì´ì ì€ í…ìŠ¤íŠ¸ ì…ë ¥ ëŒ€ì‹  ì›ì‹œ ì˜¤ë””ì˜¤ íŒŒí˜•ì´ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì˜¤ë””ì˜¤ ë¶„ë¥˜ì˜ ì‹¤ì œ ì ìš© ë¶„ì•¼ì—ëŠ” í™”ìì˜ ì˜ë„ íŒŒì•…, ì–¸ì–´ ë¶„ë¥˜, ì†Œë¦¬ë¡œ ë™ë¬¼ ì¢…ì„ ì‹ë³„í•˜ëŠ” ê²ƒ ë“±ì´ ìˆìŠµë‹ˆë‹¤.

ì´ ë¬¸ì„œì—ì„œ ë°©ë²•ì„ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤:

1. [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) ë°ì´í„° ì„¸íŠ¸ë¥¼ [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base)ë¡œ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ í™”ìì˜ ì˜ë„ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤.
2. ì¶”ë¡ ì— ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì„¸ìš”.

<Tip>
ì´ íŠœí† ë¦¬ì–¼ì—ì„œ ì„¤ëª…í•˜ëŠ” ì‘ì—…ì€ ì•„ë˜ì˜ ëª¨ë¸ ì•„í‚¤í…ì²˜ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤:

<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->

[Audio Spectrogram Transformer](../model_doc/audio-spectrogram-transformer), [Data2VecAudio](../model_doc/data2vec-audio), [Hubert](../model_doc/hubert), [SEW](../model_doc/sew), [SEW-D](../model_doc/sew-d), [UniSpeech](../model_doc/unispeech), [UniSpeechSat](../model_doc/unispeech-sat), [Wav2Vec2](../model_doc/wav2vec2), [Wav2Vec2-Conformer](../model_doc/wav2vec2-conformer), [WavLM](../model_doc/wavlm), [Whisper](../model_doc/whisper)

<!--End of the generated tip-->

</Tip>

ì‹œì‘í•˜ê¸° ì „ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ëª¨ë‘ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:

```bash
pip install transformers datasets evaluate
```

ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ê³  ì»¤ë®¤ë‹ˆí‹°ì™€ ê³µìœ í•  ìˆ˜ ìˆë„ë¡ í—ˆê¹…í˜ì´ìŠ¤ ê³„ì •ì— ë¡œê·¸ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ë©”ì‹œì§€ê°€ í‘œì‹œë˜ë©´ í† í°ì„ ì…ë ¥í•˜ì—¬ ë¡œê·¸ì¸í•©ë‹ˆë‹¤:

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## MInDS-14 ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°[[load_minds_14_dataset]]

ë¨¼ì € ğŸ¤— Datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ MinDS-14 ë°ì´í„° ì„¸íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤:

```py
>>> from datasets import load_dataset, Audio

>>> minds = load_dataset("PolyAI/minds14", name="en-US", split="train")
```

ë°ì´í„° ì„¸íŠ¸ì˜ `train` ë¶„í• ì„ [`~datasets.Dataset.train_test_split`] ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë” ì‘ì€ í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ì§‘í•©ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ì „ì²´ ë°ì´í„° ì„¸íŠ¸ì— ë” ë§ì€ ì‹œê°„ì„ ì†Œë¹„í•˜ê¸° ì „ì— ëª¨ë“  ê²ƒì´ ì‘ë™í•˜ëŠ”ì§€ ì‹¤í—˜í•˜ê³  í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```py
>>> minds = minds.train_test_split(test_size=0.2)
```

ì´ì œ ë°ì´í„° ì§‘í•©ì„ ì‚´í´ë³¼ê²Œìš”:

```py
>>> minds
DatasetDict({
    train: Dataset({
        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],
        num_rows: 450
    })
    test: Dataset({
        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],
        num_rows: 113
    })
})
```

ë°ì´í„° ì„¸íŠ¸ì—ëŠ” `lang_id` ë° `english_transcription`ê³¼ ê°™ì€ ìœ ìš©í•œ ì •ë³´ê°€ ë§ì´ í¬í•¨ë˜ì–´ ìˆì§€ë§Œ ì´ ê°€ì´ë“œì—ì„œëŠ” `audio` ë° `intent_class`ì— ì¤‘ì ì„ ë‘˜ ê²ƒì…ë‹ˆë‹¤. ë‹¤ë¥¸ ì—´ì€ [`~datasets.Dataset.remove_columns`] ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì œê±°í•©ë‹ˆë‹¤:

```py
>>> minds = minds.remove_columns(["path", "transcription", "english_transcription", "lang_id"])
```

ì˜ˆì‹œë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤:

```py
>>> minds["train"][0]
{'audio': {'array': array([ 0.        ,  0.        ,  0.        , ..., -0.00048828,
         -0.00024414, -0.00024414], dtype=float32),
  'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602b9a5fbb1e6d0fbce91f52.wav',
  'sampling_rate': 8000},
 'intent_class': 2}
```

ë‘ ê°œì˜ í•„ë“œê°€ ìˆìŠµë‹ˆë‹¤:

- `audio`: ì˜¤ë””ì˜¤ íŒŒì¼ì„ ê°€ì ¸ì˜¤ê³  ë¦¬ìƒ˜í”Œë§í•˜ê¸° ìœ„í•´ í˜¸ì¶œí•´ì•¼ í•˜ëŠ” ìŒì„± ì‹ í˜¸ì˜ 1ì°¨ì› `ë°°ì—´`ì…ë‹ˆë‹¤.
- `intent_class`: í™”ìì˜ ì˜ë„ì— ëŒ€í•œ í´ë˜ìŠ¤ IDë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

ëª¨ë¸ì´ ë ˆì´ë¸” IDì—ì„œ ë ˆì´ë¸” ì´ë¦„ì„ ì‰½ê²Œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë„ë¡ ë ˆì´ë¸” ì´ë¦„ì„ ì •ìˆ˜ë¡œ ë§¤í•‘í•˜ëŠ” ì‚¬ì „ì„ ë§Œë“¤ê±°ë‚˜ ê·¸ ë°˜ëŒ€ë¡œ ë§¤í•‘í•˜ëŠ” ì‚¬ì „ì„ ë§Œë“­ë‹ˆë‹¤:

```py
>>> labels = minds["train"].features["intent_class"].names
>>> label2id, id2label = dict(), dict()
>>> for i, label in enumerate(labels):
...     label2id[label] = str(i)
...     id2label[str(i)] = label
```

ì´ì œ ë ˆì´ë¸” IDë¥¼ ë ˆì´ë¸” ì´ë¦„ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
>>> id2label[str(2)]
'app_error'
```

## ì „ì²˜ë¦¬[[preprocess]]

ë‹¤ìŒ ë‹¨ê³„ëŠ” ì˜¤ë””ì˜¤ ì‹ í˜¸ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ Wav2Vec2 íŠ¹ì§• ì¶”ì¶œê¸°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ê²ƒì…ë‹ˆë‹¤:

```py
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base")
```

MinDS-14 ë°ì´í„° ì„¸íŠ¸ì˜ ìƒ˜í”Œë§ ì†ë„ëŠ” 8000khzì´ë¯€ë¡œ(ì´ ì •ë³´ëŠ” [ë°ì´í„°ì„¸íŠ¸ ì¹´ë“œ](https://huggingface.co/datasets/PolyAI/minds14)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤), ì‚¬ì „ í›ˆë ¨ëœ Wav2Vec2 ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ ë°ì´í„° ì„¸íŠ¸ë¥¼ 16000kHzë¡œ ë¦¬ìƒ˜í”Œë§í•´ì•¼ í•©ë‹ˆë‹¤:

```py
>>> minds = minds.cast_column("audio", Audio(sampling_rate=16_000))
>>> minds["train"][0]
{'audio': {'array': array([ 2.2098757e-05,  4.6582241e-05, -2.2803260e-05, ...,
         -2.8419291e-04, -2.3305941e-04, -1.1425107e-04], dtype=float32),
  'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602b9a5fbb1e6d0fbce91f52.wav',
  'sampling_rate': 16000},
 'intent_class': 2}
```

ì´ì œ ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤:

1. ê°€ì ¸ì˜¬ `ì˜¤ë””ì˜¤` ì—´ì„ í˜¸ì¶œí•˜ê³  í•„ìš”í•œ ê²½ìš° ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë¦¬ìƒ˜í”Œë§í•©ë‹ˆë‹¤.
2. ì˜¤ë””ì˜¤ íŒŒì¼ì˜ ìƒ˜í”Œë§ ì†ë„ê°€ ëª¨ë¸ì— ì‚¬ì „ í›ˆë ¨ëœ ì˜¤ë””ì˜¤ ë°ì´í„°ì˜ ìƒ˜í”Œë§ ì†ë„ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤. ì´ ì •ë³´ëŠ” Wav2Vec2 [ëª¨ë¸ ì¹´ë“œ](https://huggingface.co/facebook/wav2vec2-base)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
3. ê¸´ ì…ë ¥ì´ ì˜ë¦¬ì§€ ì•Šê³  ì¼ê´„ ì²˜ë¦¬ë˜ë„ë¡ ìµœëŒ€ ì…ë ¥ ê¸¸ì´ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

```py
>>> def preprocess_function(examples):
...     audio_arrays = [x["array"] for x in examples["audio"]]
...     inputs = feature_extractor(
...         audio_arrays, sampling_rate=feature_extractor.sampling_rate, max_length=16000, truncation=True
...     )
...     return inputs
```

ì „ì²´ ë°ì´í„° ì„¸íŠ¸ì— ì „ì²˜ë¦¬ ê¸°ëŠ¥ì„ ì ìš©í•˜ë ¤ë©´ ğŸ¤— Datasets [`~datasets.Dataset.map`] í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. `batched=True`ë¥¼ ì„¤ì •í•˜ì—¬ ë°ì´í„° ì§‘í•©ì˜ ì—¬ëŸ¬ ìš”ì†Œë¥¼ í•œ ë²ˆì— ì²˜ë¦¬í•˜ë©´ `map`ì˜ ì†ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•„ìš”í•˜ì§€ ì•Šì€ ì—´ì„ ì œê±°í•˜ê³  `intent_class`ì˜ ì´ë¦„ì„ ëª¨ë¸ì´ ì˜ˆìƒí•˜ëŠ” ì´ë¦„ì¸ `label`ë¡œ ë³€ê²½í•©ë‹ˆë‹¤:

```py
>>> encoded_minds = minds.map(preprocess_function, remove_columns="audio", batched=True)
>>> encoded_minds = encoded_minds.rename_column("intent_class", "label")
```

## í‰ê°€í•˜ê¸°[[evaluate]]

í›ˆë ¨ ì¤‘ì— ë©”íŠ¸ë¦­ì„ í¬í•¨í•˜ë©´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê°€ ë°©ë²•ì„ ë¹ ë¥´ê²Œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ì‘ì—…ì—ì„œëŠ” [accuracy(ì •í™•ë„)](https://huggingface.co/spaces/evaluate-metric/accuracy) ë©”íŠ¸ë¦­ì„ ê°€ì ¸ì˜µë‹ˆë‹¤(ë©”íŠ¸ë¦­ì„ ê°€ì ¸ì˜¤ê³  ê³„ì‚°í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ ğŸ¤— Evalutate [ë¹ ë¥¸ ë‘˜ëŸ¬ë³´ê¸°](https://huggingface.co/docs/evaluate/a_quick_tour) ì°¸ì¡°í•˜ì„¸ìš”):

```py
>>> import evaluate

>>> accuracy = evaluate.load("accuracy")
```

ê·¸ëŸ° ë‹¤ìŒ ì˜ˆì¸¡ê³¼ ë ˆì´ë¸”ì„ [`~evaluate.EvaluationModule.compute`]ì— ì „ë‹¬í•˜ì—¬ ì •í™•ë„ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤:

```py
>>> import numpy as np


>>> def compute_metrics(eval_pred):
...     predictions = np.argmax(eval_pred.predictions, axis=1)
...     return accuracy.compute(predictions=predictions, references=eval_pred.label_ids)
```

ì´ì œ `compute_metrics` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ì¤€ë¹„ê°€ ë˜ì—ˆìœ¼ë©°, íŠ¸ë ˆì´ë‹ì„ ì„¤ì •í•  ë•Œ ì´ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

## í›ˆë ¨[[train]]

<frameworkcontent>
<pt>
<Tip>

[`Trainer`]ë¡œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë° ìµìˆ™í•˜ì§€ ì•Šë‹¤ë©´ ê¸°ë³¸ íŠœí† ë¦¬ì–¼ [ì—¬ê¸°](../training#train-with-pytorch-trainer)ì„ ì‚´í´ë³´ì„¸ìš”!

</Tip>

ì´ì œ ëª¨ë¸ í›ˆë ¨ì„ ì‹œì‘í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤! [`AutoModelForAudioClassification`]ì„ ì´ìš©í•´ì„œ Wav2Vec2ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. ì˜ˆìƒë˜ëŠ” ë ˆì´ë¸” ìˆ˜ì™€ ë ˆì´ë¸” ë§¤í•‘ì„ ì§€ì •í•©ë‹ˆë‹¤:

```py
>>> from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer

>>> num_labels = len(id2label)
>>> model = AutoModelForAudioClassification.from_pretrained(
...     "facebook/wav2vec2-base", num_labels=num_labels, label2id=label2id, id2label=id2label
... )
```

ì´ì œ ì„¸ ë‹¨ê³„ë§Œ ë‚¨ì•˜ìŠµë‹ˆë‹¤:

1. í›ˆë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ [`TrainingArguments`]ì— ì •ì˜í•©ë‹ˆë‹¤. ìœ ì¼í•œ í•„ìˆ˜ ë§¤ê°œë³€ìˆ˜ëŠ” ëª¨ë¸ì„ ì €ì¥í•  ìœ„ì¹˜ë¥¼ ì§€ì •í•˜ëŠ” `output_dir`ì…ë‹ˆë‹¤. `push_to_hub = True`ë¥¼ ì„¤ì •í•˜ì—¬ ì´ ëª¨ë¸ì„ í—ˆë¸Œë¡œ í‘¸ì‹œí•©ë‹ˆë‹¤(ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ë ¤ë©´ í—ˆê¹… í˜ì´ìŠ¤ì— ë¡œê·¸ì¸í•´ì•¼ í•©ë‹ˆë‹¤). ê° ì—í­ì´ ëë‚  ë•Œë§ˆë‹¤ [`Trainer`]ê°€ ì •í™•ë„ë¥¼ í‰ê°€í•˜ê³  í›ˆë ¨ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
2. ëª¨ë¸, ë°ì´í„° ì„¸íŠ¸, í† í¬ë‚˜ì´ì €, ë°ì´í„° ì½œë ˆì´í„°, `compute_metrics` í•¨ìˆ˜ì™€ í•¨ê»˜ í›ˆë ¨ ì¸ìë¥¼ [`Trainer`]ì— ì „ë‹¬í•©ë‹ˆë‹¤.
3. [`~Trainer.train`]ì„ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•©ë‹ˆë‹¤.


```py
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_mind_model",
...     eval_strategy="epoch",
...     save_strategy="epoch",
...     learning_rate=3e-5,
...     per_device_train_batch_size=32,
...     gradient_accumulation_steps=4,
...     per_device_eval_batch_size=32,
...     num_train_epochs=10,
...     warmup_ratio=0.1,
...     logging_steps=10,
...     load_best_model_at_end=True,
...     metric_for_best_model="accuracy",
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=encoded_minds["train"],
...     eval_dataset=encoded_minds["test"],
...     tokenizer=feature_extractor,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

í›ˆë ¨ì´ ì™„ë£Œë˜ë©´ ëª¨ë“  ì‚¬ëŒì´ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ [`~transformers.Trainer.push_to_hub`] ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í—ˆë¸Œì— ê³µìœ í•˜ì„¸ìš”:

```py
>>> trainer.push_to_hub()
```
</pt>
</frameworkcontent>

<Tip>

For a more in-depth example of how to finetune a model for audio classification, take a look at the corresponding [PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb).

</Tip>

## ì¶”ë¡ [[inference]]

ì´ì œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í–ˆìœ¼ë‹ˆ ì¶”ë¡ ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

ì¶”ë¡ ì„ ì‹¤í–‰í•  ì˜¤ë””ì˜¤ íŒŒì¼ì„ ê°€ì ¸ì˜µë‹ˆë‹¤. í•„ìš”í•œ ê²½ìš° ì˜¤ë””ì˜¤ íŒŒì¼ì˜ ìƒ˜í”Œë§ ì†ë„ë¥¼ ëª¨ë¸ì˜ ìƒ˜í”Œë§ ì†ë„ì™€ ì¼ì¹˜í•˜ë„ë¡ ë¦¬ìƒ˜í”Œë§í•˜ëŠ” ê²ƒì„ ìŠì§€ ë§ˆì„¸ìš”!

```py
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
>>> sampling_rate = dataset.features["audio"].sampling_rate
>>> audio_file = dataset[0]["audio"]["path"]
```

ì¶”ë¡ ì„ ìœ„í•´ ë¯¸ì„¸ ì¡°ì •í•œ ëª¨ë¸ì„ ì‹œí—˜í•´ ë³´ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì€ [`pipeline`]ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ ë¶„ë¥˜ë¥¼ ìœ„í•œ `pipeline`ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ê³  ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì „ë‹¬í•©ë‹ˆë‹¤:

```py
>>> from transformers import pipeline

>>> classifier = pipeline("audio-classification", model="stevhliu/my_awesome_minds_model")
>>> classifier(audio_file)
[
    {'score': 0.09766869246959686, 'label': 'cash_deposit'},
    {'score': 0.07998877018690109, 'label': 'app_error'},
    {'score': 0.0781070664525032, 'label': 'joint_account'},
    {'score': 0.07667109370231628, 'label': 'pay_bill'},
    {'score': 0.0755252093076706, 'label': 'balance'}
]
```

ì›í•˜ëŠ” ê²½ìš° `pipeline`ì˜ ê²°ê³¼ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ë³µì œí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:

<frameworkcontent>
<pt>
íŠ¹ì§• ì¶”ì¶œê¸°ë¥¼ ê°€ì ¸ì™€ì„œ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì „ì²˜ë¦¬í•˜ê³  `ì…ë ¥`ì„ PyTorch í…ì„œë¡œ ë°˜í™˜í•©ë‹ˆë‹¤:

```py
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("stevhliu/my_awesome_minds_model")
>>> inputs = feature_extractor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")
```

ëª¨ë¸ì— ì…ë ¥ì„ ì „ë‹¬í•˜ê³  ë¡œì§“ì„ ë°˜í™˜í•©ë‹ˆë‹¤:

```py
>>> from transformers import AutoModelForAudioClassification

>>> model = AutoModelForAudioClassification.from_pretrained("stevhliu/my_awesome_minds_model")
>>> with torch.no_grad():
...     logits = model(**inputs).logits
```

í™•ë¥ ì´ ê°€ì¥ ë†’ì€ í´ë˜ìŠ¤ë¥¼ ê°€ì ¸ì˜¨ ë‹¤ìŒ ëª¨ë¸ì˜ `id2label` ë§¤í•‘ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¥¼ ë ˆì´ë¸”ë¡œ ë³€í™˜í•©ë‹ˆë‹¤:

```py
>>> import torch

>>> predicted_class_ids = torch.argmax(logits).item()
>>> predicted_label = model.config.id2label[predicted_class_ids]
>>> predicted_label
'cash_deposit'
```
</pt>
</frameworkcontent>