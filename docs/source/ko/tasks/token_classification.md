<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# í† í° ë¶„ë¥˜[[token-classification]]

[[open-in-colab]]

<Youtube id="wVHdVlPScxA"/>

í† í° ë¶„ë¥˜ëŠ” ë¬¸ì¥ì˜ ê°œë³„ í† í°ì— ë ˆì´ë¸”ì„ í• ë‹¹í•©ë‹ˆë‹¤. ê°€ì¥ ì¼ë°˜ì ì¸ í† í° ë¶„ë¥˜ ì‘ì—… ì¤‘ í•˜ë‚˜ëŠ” ê°œì²´ëª… ì¸ì‹(Named Entity Recognition, NER)ì…ë‹ˆë‹¤. ê°œì²´ëª… ì¸ì‹ì€ ë¬¸ì¥ì—ì„œ ì‚¬ëŒ, ìœ„ì¹˜ ë˜ëŠ” ì¡°ì§ê³¼ ê°™ì€ ê° ê°œì²´ì˜ ë ˆì´ë¸”ì„ ì°¾ìœ¼ë ¤ê³  ì‹œë„í•©ë‹ˆë‹¤.

ì´ ê°€ì´ë“œì—ì„œ í•™ìŠµí•  ë‚´ìš©ì€:

1. [WNUT 17](https://huggingface.co/datasets/wnut_17) ë°ì´í„° ì„¸íŠ¸ì—ì„œ [DistilBERT](https://huggingface.co/distilbert/distilbert-base-uncased)ë¥¼ íŒŒì¸ íŠœë‹í•˜ì—¬ ìƒˆë¡œìš´ ê°œì²´ë¥¼ íƒì§€í•©ë‹ˆë‹¤.
2. ì¶”ë¡ ì„ ìœ„í•´ íŒŒì¸ íŠœë‹ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

<Tip>

ì´ ì‘ì—…ê³¼ í˜¸í™˜ë˜ëŠ” ëª¨ë“  ì•„í‚¤í…ì²˜ì™€ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë³´ë ¤ë©´ [ì‘ì—… í˜ì´ì§€](https://huggingface.co/tasks/token-classification)ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

</Tip>

ì‹œì‘í•˜ê¸° ì „ì—, í•„ìš”í•œ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:

```bash
pip install transformers datasets evaluate seqeval
```

Hugging Face ê³„ì •ì— ë¡œê·¸ì¸í•˜ì—¬ ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ê³  ì»¤ë®¤ë‹ˆí‹°ì— ê³µìœ í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤. ë©”ì‹œì§€ê°€ í‘œì‹œë˜ë©´, í† í°ì„ ì…ë ¥í•˜ì—¬ ë¡œê·¸ì¸í•˜ì„¸ìš”:

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## WNUT 17 ë°ì´í„° ì„¸íŠ¸ ê°€ì ¸ì˜¤ê¸°[[load-wnut-17-dataset]]

ë¨¼ì € ğŸ¤— Datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ WNUT 17 ë°ì´í„° ì„¸íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤:

```py
>>> from datasets import load_dataset

>>> wnut = load_dataset("wnut_17")
```

ë‹¤ìŒ ì˜ˆì œë¥¼ ì‚´í´ë³´ì„¸ìš”:

```py
>>> wnut["train"][0]
{'id': '0',
 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0],
 'tokens': ['@paulwalk', 'It', "'s", 'the', 'view', 'from', 'where', 'I', "'m", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.']
}
```

`ner_tags`ì˜ ê° ìˆ«ìëŠ” ê°œì²´ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ìˆ«ìë¥¼ ë ˆì´ë¸” ì´ë¦„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ê°œì²´ê°€ ë¬´ì—‡ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤:

```py
>>> label_list = wnut["train"].features[f"ner_tags"].feature.names
>>> label_list
[
    "O",
    "B-corporation",
    "I-corporation",
    "B-creative-work",
    "I-creative-work",
    "B-group",
    "I-group",
    "B-location",
    "I-location",
    "B-person",
    "I-person",
    "B-product",
    "I-product",
]
```

ê° `ner_tag`ì˜ ì•ì— ë¶™ì€ ë¬¸ìëŠ” ê°œì²´ì˜ í† í° ìœ„ì¹˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤:

- `B-`ëŠ” ê°œì²´ì˜ ì‹œì‘ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
- `I-`ëŠ” í† í°ì´ ë™ì¼í•œ ê°œì²´ ë‚´ë¶€ì— í¬í•¨ë˜ì–´ ìˆìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤(ì˜ˆë¥¼ ë“¤ì–´ `State` í† í°ì€ `Empire State Building`ì™€ ê°™ì€ ê°œì²´ì˜ ì¼ë¶€ì…ë‹ˆë‹¤).
- `0`ëŠ” í† í°ì´ ì–´ë–¤ ê°œì²´ì—ë„ í•´ë‹¹í•˜ì§€ ì•ŠìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

## ì „ì²˜ë¦¬[[preprocess]]

<Youtube id="iY2AZYdZAr0"/>

ë‹¤ìŒìœ¼ë¡œ `tokens` í•„ë“œë¥¼ ì „ì²˜ë¦¬í•˜ê¸° ìœ„í•´ DistilBERT í† í¬ë‚˜ì´ì €ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤:

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
```

ìœ„ì˜ ì˜ˆì œ `tokens` í•„ë“œë¥¼ ë³´ë©´ ì…ë ¥ì´ ì´ë¯¸ í† í°í™”ëœ ê²ƒì²˜ëŸ¼ ë³´ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì‹¤ì œë¡œ ì…ë ¥ì€ ì•„ì§ í† í°í™”ë˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ ë‹¨ì–´ë¥¼ í•˜ìœ„ ë‹¨ì–´ë¡œ í† í°í™”í•˜ê¸° ìœ„í•´ `is_split_into_words=True`ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆì œë¡œ í™•ì¸í•©ë‹ˆë‹¤:

```py
>>> example = wnut["train"][0]
>>> tokenized_input = tokenizer(example["tokens"], is_split_into_words=True)
>>> tokens = tokenizer.convert_ids_to_tokens(tokenized_input["input_ids"])
>>> tokens
['[CLS]', '@', 'paul', '##walk', 'it', "'", 's', 'the', 'view', 'from', 'where', 'i', "'", 'm', 'living', 'for', 'two', 'weeks', '.', 'empire', 'state', 'building', '=', 'es', '##b', '.', 'pretty', 'bad', 'storm', 'here', 'last', 'evening', '.', '[SEP]']
```

ê·¸ëŸ¬ë‚˜ ì´ë¡œ ì¸í•´ `[CLS]`ê³¼ `[SEP]`ë¼ëŠ” íŠ¹ìˆ˜ í† í°ì´ ì¶”ê°€ë˜ê³ , í•˜ìœ„ ë‹¨ì–´ í† í°í™”ë¡œ ì¸í•´ ì…ë ¥ê³¼ ë ˆì´ë¸” ê°„ì— ë¶ˆì¼ì¹˜ê°€ ë°œìƒí•©ë‹ˆë‹¤. í•˜ë‚˜ì˜ ë ˆì´ë¸”ì— í•´ë‹¹í•˜ëŠ” ë‹¨ì¼ ë‹¨ì–´ëŠ” ì´ì œ ë‘ ê°œì˜ í•˜ìœ„ ë‹¨ì–´ë¡œ ë¶„í• ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í† í°ê³¼ ë ˆì´ë¸”ì„ ë‹¤ìŒê³¼ ê°™ì´ ì¬ì •ë ¬í•´ì•¼ í•©ë‹ˆë‹¤:

1. [`word_ids`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.BatchEncoding.word_ids) ë©”ì†Œë“œë¡œ ëª¨ë“  í† í°ì„ í•´ë‹¹ ë‹¨ì–´ì— ë§¤í•‘í•©ë‹ˆë‹¤.
2. íŠ¹ìˆ˜ í† í° `[CLS]`ì™€ `[SEP]`ì— `-100` ë ˆì´ë¸”ì„ í• ë‹¹í•˜ì—¬, PyTorch ì†ì‹¤ í•¨ìˆ˜ê°€ í•´ë‹¹ í† í°ì„ ë¬´ì‹œí•˜ë„ë¡ í•©ë‹ˆë‹¤.
3. ì£¼ì–´ì§„ ë‹¨ì–´ì˜ ì²« ë²ˆì§¸ í† í°ì—ë§Œ ë ˆì´ë¸”ì„ ì§€ì •í•©ë‹ˆë‹¤. ê°™ì€ ë‹¨ì–´ì˜ ë‹¤ë¥¸ í•˜ìœ„ í† í°ì— `-100`ì„ í• ë‹¹í•©ë‹ˆë‹¤.

ë‹¤ìŒì€ í† í°ê³¼ ë ˆì´ë¸”ì„ ì¬ì •ë ¬í•˜ê³  DistilBERTì˜ ìµœëŒ€ ì…ë ¥ ê¸¸ì´ë³´ë‹¤ ê¸¸ì§€ ì•Šë„ë¡ ì‹œí€€ìŠ¤ë¥¼ ì˜ë¼ë‚´ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“œëŠ” ë°©ë²•ì…ë‹ˆë‹¤:

```py
>>> def tokenize_and_align_labels(examples):
...     tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)

...     labels = []
...     for i, label in enumerate(examples[f"ner_tags"]):
...         word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.
...         previous_word_idx = None
...         label_ids = []
...         for word_idx in word_ids:  # Set the special tokens to -100.
...             if word_idx is None:
...                 label_ids.append(-100)
...             elif word_idx != previous_word_idx:  # Only label the first token of a given word.
...                 label_ids.append(label[word_idx])
...             else:
...                 label_ids.append(-100)
...             previous_word_idx = word_idx
...         labels.append(label_ids)

...     tokenized_inputs["labels"] = labels
...     return tokenized_inputs
```

ì „ì²´ ë°ì´í„° ì„¸íŠ¸ì— ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ì ìš©í•˜ë ¤ë©´, ğŸ¤— Datasets [`~datasets.Dataset.map`] í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. `batched=True`ë¡œ ì„¤ì •í•˜ì—¬ ë°ì´í„° ì„¸íŠ¸ì˜ ì—¬ëŸ¬ ìš”ì†Œë¥¼ í•œ ë²ˆì— ì²˜ë¦¬í•˜ë©´ `map` í•¨ìˆ˜ì˜ ì†ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:
```py
>>> tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)
```

ì´ì œ [`DataCollatorWithPadding`]ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì œ ë°°ì¹˜ë¥¼ ë§Œë“¤ì–´ë´…ì‹œë‹¤. ë°ì´í„° ì„¸íŠ¸ ì „ì²´ë¥¼ ìµœëŒ€ ê¸¸ì´ë¡œ íŒ¨ë”©í•˜ëŠ” ëŒ€ì‹ , *ë™ì  íŒ¨ë”©*ì„ ì‚¬ìš©í•˜ì—¬ ë°°ì¹˜ì—ì„œ ê°€ì¥ ê¸´ ê¸¸ì´ì— ë§ê²Œ ë¬¸ì¥ì„ íŒ¨ë”©í•˜ëŠ” ê²ƒì´ íš¨ìœ¨ì ì…ë‹ˆë‹¤.

<frameworkcontent>
<pt>
```py
>>> from transformers import DataCollatorForTokenClassification

>>> data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)
```
</pt>
<tf>
```py
>>> from transformers import DataCollatorForTokenClassification

>>> data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors="tf")
```
</tf>
</frameworkcontent>

## í‰ê°€[[evaluation]]

í›ˆë ¨ ì¤‘ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ í‰ê°€ ì§€í‘œë¥¼ í¬í•¨í•˜ëŠ” ê²ƒì´ ìœ ìš©í•©ë‹ˆë‹¤. ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹ ë¥´ê²Œ í‰ê°€ ë°©ë²•ì„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ì‘ì—…ì—ì„œëŠ” [seqeval](https://huggingface.co/spaces/evaluate-metric/seqeval) í‰ê°€ ì§€í‘œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. (í‰ê°€ ì§€í‘œë¥¼ ê°€ì ¸ì˜¤ê³  ê³„ì‚°í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œëŠ” ğŸ¤— Evaluate [ë¹ ë¥¸ ë‘˜ëŸ¬ë³´ê¸°](https://huggingface.co/docs/evaluate/a_quick_tour)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”). Seqevalì€ ì‹¤ì œë¡œ ì •ë°€ë„, ì¬í˜„ë¥ , F1 ë° ì •í™•ë„ì™€ ê°™ì€ ì—¬ëŸ¬ ì ìˆ˜ë¥¼ ì‚°ì¶œí•©ë‹ˆë‹¤.

```py
>>> import evaluate

>>> seqeval = evaluate.load("seqeval")
```

ë¨¼ì € NER ë ˆì´ë¸”ì„ ê°€ì ¸ì˜¨ ë‹¤ìŒ, [`~evaluate.EvaluationModule.compute`]ì— ì‹¤ì œ ì˜ˆì¸¡ê³¼ ì‹¤ì œ ë ˆì´ë¸”ì„ ì „ë‹¬í•˜ì—¬ ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤:

```py
>>> import numpy as np

>>> labels = [label_list[i] for i in example[f"ner_tags"]]


>>> def compute_metrics(p):
...     predictions, labels = p
...     predictions = np.argmax(predictions, axis=2)

...     true_predictions = [
...         [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
...         for prediction, label in zip(predictions, labels)
...     ]
...     true_labels = [
...         [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
...         for prediction, label in zip(predictions, labels)
...     ]

...     results = seqeval.compute(predictions=true_predictions, references=true_labels)
...     return {
...         "precision": results["overall_precision"],
...         "recall": results["overall_recall"],
...         "f1": results["overall_f1"],
...         "accuracy": results["overall_accuracy"],
...     }
```

ì´ì œ `compute_metrics` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ì¤€ë¹„ê°€ ë˜ì—ˆìœ¼ë©°, í›ˆë ¨ì„ ì„¤ì •í•˜ë©´ ì´ í•¨ìˆ˜ë¡œ ë˜ëŒì•„ì˜¬ ê²ƒì…ë‹ˆë‹¤.

## í›ˆë ¨[[train]]

ëª¨ë¸ì„ í›ˆë ¨í•˜ê¸° ì „ì—, `id2label`ì™€ `label2id`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆìƒë˜ëŠ” idì™€ ë ˆì´ë¸”ì˜ ë§µì„ ìƒì„±í•˜ì„¸ìš”:

```py
>>> id2label = {
...     0: "O",
...     1: "B-corporation",
...     2: "I-corporation",
...     3: "B-creative-work",
...     4: "I-creative-work",
...     5: "B-group",
...     6: "I-group",
...     7: "B-location",
...     8: "I-location",
...     9: "B-person",
...     10: "I-person",
...     11: "B-product",
...     12: "I-product",
... }
>>> label2id = {
...     "O": 0,
...     "B-corporation": 1,
...     "I-corporation": 2,
...     "B-creative-work": 3,
...     "I-creative-work": 4,
...     "B-group": 5,
...     "I-group": 6,
...     "B-location": 7,
...     "I-location": 8,
...     "B-person": 9,
...     "I-person": 10,
...     "B-product": 11,
...     "I-product": 12,
... }
```

<frameworkcontent>
<pt>
<Tip>

[`Trainer`]ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ íŒŒì¸ íŠœë‹í•˜ëŠ” ë°©ë²•ì— ìµìˆ™í•˜ì§€ ì•Šì€ ê²½ìš°, [ì—¬ê¸°](../training#train-with-pytorch-trainer)ì—ì„œ ê¸°ë³¸ íŠœí† ë¦¬ì–¼ì„ í™•ì¸í•˜ì„¸ìš”!

</Tip>

ì´ì œ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¬ ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤! [`AutoModelForSequenceClassification`]ë¡œ DistilBERTë¥¼ ê°€ì ¸ì˜¤ê³  ì˜ˆìƒë˜ëŠ” ë ˆì´ë¸” ìˆ˜ì™€ ë ˆì´ë¸” ë§¤í•‘ì„ ì§€ì •í•˜ì„¸ìš”:

```py
>>> from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer

>>> model = AutoModelForTokenClassification.from_pretrained(
...     "distilbert/distilbert-base-uncased", num_labels=13, id2label=id2label, label2id=label2id
... )
```

ì´ì œ ì„¸ ë‹¨ê³„ë§Œ ê±°ì¹˜ë©´ ëì…ë‹ˆë‹¤:

1. [`TrainingArguments`]ì—ì„œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì •ì˜í•˜ì„¸ìš”. `output_dir`ëŠ” ëª¨ë¸ì„ ì €ì¥í•  ìœ„ì¹˜ë¥¼ ì§€ì •í•˜ëŠ” ìœ ì¼í•œ ë§¤ê°œë³€ìˆ˜ì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì„ í—ˆë¸Œì— ì—…ë¡œë“œí•˜ê¸° ìœ„í•´ `push_to_hub=True`ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤(ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ê¸° ìœ„í•´ Hugging Faceì— ë¡œê·¸ì¸í•´ì•¼í•©ë‹ˆë‹¤.) ê° ì—í­ì´ ëë‚  ë•Œë§ˆë‹¤, [`Trainer`]ëŠ” seqeval ì ìˆ˜ë¥¼ í‰ê°€í•˜ê³  í›ˆë ¨ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
2. [`Trainer`]ì— í›ˆë ¨ ì¸ìˆ˜ì™€ ëª¨ë¸, ë°ì´í„° ì„¸íŠ¸, í† í¬ë‚˜ì´ì €, ë°ì´í„° ì½œë ˆì´í„° ë° `compute_metrics` í•¨ìˆ˜ë¥¼ ì „ë‹¬í•˜ì„¸ìš”.
3. [`~Trainer.train`]ë¥¼ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ì„ íŒŒì¸ íŠœë‹í•˜ì„¸ìš”.

```py
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_wnut_model",
...     learning_rate=2e-5,
...     per_device_train_batch_size=16,
...     per_device_eval_batch_size=16,
...     num_train_epochs=2,
...     weight_decay=0.01,
...     eval_strategy="epoch",
...     save_strategy="epoch",
...     load_best_model_at_end=True,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_wnut["train"],
...     eval_dataset=tokenized_wnut["test"],
...     processing_class=tokenizer,
...     data_collator=data_collator,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

í›ˆë ¨ì´ ì™„ë£Œë˜ë©´, [`~transformers.Trainer.push_to_hub`] ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í—ˆë¸Œì— ê³µìœ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```py
>>> trainer.push_to_hub()
```
</pt>
<tf>
<Tip>

Kerasë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ íŒŒì¸ íŠœë‹í•˜ëŠ” ë°©ë²•ì— ìµìˆ™í•˜ì§€ ì•Šì€ ê²½ìš°, [ì—¬ê¸°](../training#train-a-tensorflow-model-with-keras)ì˜ ê¸°ë³¸ íŠœí† ë¦¬ì–¼ì„ í™•ì¸í•˜ì„¸ìš”!

</Tip>
TensorFlowì—ì„œ ëª¨ë¸ì„ íŒŒì¸ íŠœë‹í•˜ë ¤ë©´, ë¨¼ì € ì˜µí‹°ë§ˆì´ì € í•¨ìˆ˜ì™€ í•™ìŠµë¥  ìŠ¤ì¼€ì¥´, ê·¸ë¦¬ê³  ì¼ë¶€ í›ˆë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤:

```py
>>> from transformers import create_optimizer

>>> batch_size = 16
>>> num_train_epochs = 3
>>> num_train_steps = (len(tokenized_wnut["train"]) // batch_size) * num_train_epochs
>>> optimizer, lr_schedule = create_optimizer(
...     init_lr=2e-5,
...     num_train_steps=num_train_steps,
...     weight_decay_rate=0.01,
...     num_warmup_steps=0,
... )
```

ê·¸ëŸ° ë‹¤ìŒ [`TFAutoModelForSequenceClassification`]ì„ ì‚¬ìš©í•˜ì—¬ DistilBERTë¥¼ ê°€ì ¸ì˜¤ê³ , ì˜ˆìƒë˜ëŠ” ë ˆì´ë¸” ìˆ˜ì™€ ë ˆì´ë¸” ë§¤í•‘ì„ ì§€ì •í•©ë‹ˆë‹¤:

```py
>>> from transformers import TFAutoModelForTokenClassification

>>> model = TFAutoModelForTokenClassification.from_pretrained(
...     "distilbert/distilbert-base-uncased", num_labels=13, id2label=id2label, label2id=label2id
... )
```

[`~transformers.TFPreTrainedModel.prepare_tf_dataset`]ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì„¸íŠ¸ë¥¼ `tf.data.Dataset` í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤:

```py
>>> tf_train_set = model.prepare_tf_dataset(
...     tokenized_wnut["train"],
...     shuffle=True,
...     batch_size=16,
...     collate_fn=data_collator,
... )

>>> tf_validation_set = model.prepare_tf_dataset(
...     tokenized_wnut["validation"],
...     shuffle=False,
...     batch_size=16,
...     collate_fn=data_collator,
... )
```

[`compile`](https://keras.io/api/models/model_training_apis/#compile-method)ë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨í•  ëª¨ë¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤:

```py
>>> import tensorflow as tf

>>> model.compile(optimizer=optimizer)
```

í›ˆë ¨ì„ ì‹œì‘í•˜ê¸° ì „ì— ì„¤ì •í•´ì•¼í•  ë§ˆì§€ë§‰ ë‘ ê°€ì§€ëŠ” ì˜ˆì¸¡ì—ì„œ seqeval ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ê³ , ëª¨ë¸ì„ í—ˆë¸Œì— ì—…ë¡œë“œí•  ë°©ë²•ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ëª¨ë‘ [Keras callbacks](../main_classes/keras_callbacks)ë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜í–‰ë©ë‹ˆë‹¤.

[`~transformers.KerasMetricCallback`]ì— `compute_metrics` í•¨ìˆ˜ë¥¼ ì „ë‹¬í•˜ì„¸ìš”:

```py
>>> from transformers.keras_callbacks import KerasMetricCallback

>>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)
```

[`~transformers.PushToHubCallback`]ì—ì„œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì—…ë¡œë“œí•  ìœ„ì¹˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤:

```py
>>> from transformers.keras_callbacks import PushToHubCallback

>>> push_to_hub_callback = PushToHubCallback(
...     output_dir="my_awesome_wnut_model",
...     tokenizer=tokenizer,
... )
```

ê·¸ëŸ° ë‹¤ìŒ ì½œë°±ì„ í•¨ê»˜ ë¬¶ìŠµë‹ˆë‹¤:

```py
>>> callbacks = [metric_callback, push_to_hub_callback]
```

ë“œë””ì–´, ëª¨ë¸ í›ˆë ¨ì„ ì‹œì‘í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤! [`fit`](https://keras.io/api/models/model_training_apis/#fit-method)ì— í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸, ê²€ì¦ ë°ì´í„° ì„¸íŠ¸, ì—í­ì˜ ìˆ˜ ë° ì½œë°±ì„ ì „ë‹¬í•˜ì—¬ íŒŒì¸ íŠœë‹í•©ë‹ˆë‹¤:

```py
>>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=callbacks)
```

í›ˆë ¨ì´ ì™„ë£Œë˜ë©´, ëª¨ë¸ì´ ìë™ìœ¼ë¡œ í—ˆë¸Œì— ì—…ë¡œë“œë˜ì–´ ëˆ„êµ¬ë‚˜ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!
</tf>
</frameworkcontent>

<Tip>

í† í° ë¶„ë¥˜ë¥¼ ìœ„í•œ ëª¨ë¸ì„ íŒŒì¸ íŠœë‹í•˜ëŠ” ìì„¸í•œ ì˜ˆì œëŠ” ë‹¤ìŒ
[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb)
ë˜ëŠ” [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

</Tip>

## ì¶”ë¡ [[inference]]

ì¢‹ì•„ìš”, ì´ì œ ëª¨ë¸ì„ íŒŒì¸ íŠœë‹í–ˆìœ¼ë‹ˆ ì¶”ë¡ ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ê³ ì í•˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì™€ë´…ì‹œë‹¤:

```py
>>> text = "The Golden State Warriors are an American professional basketball team based in San Francisco."
```

íŒŒì¸ íŠœë‹ëœ ëª¨ë¸ë¡œ ì¶”ë¡ ì„ ì‹œë„í•˜ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì€ [`pipeline`]ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ëª¨ë¸ë¡œ NERì˜ `pipeline`ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ê³ , í…ìŠ¤íŠ¸ë¥¼ ì „ë‹¬í•´ë³´ì„¸ìš”:

```py
>>> from transformers import pipeline

>>> classifier = pipeline("ner", model="stevhliu/my_awesome_wnut_model")
>>> classifier(text)
[{'entity': 'B-location',
  'score': 0.42658573,
  'index': 2,
  'word': 'golden',
  'start': 4,
  'end': 10},
 {'entity': 'I-location',
  'score': 0.35856336,
  'index': 3,
  'word': 'state',
  'start': 11,
  'end': 16},
 {'entity': 'B-group',
  'score': 0.3064001,
  'index': 4,
  'word': 'warriors',
  'start': 17,
  'end': 25},
 {'entity': 'B-location',
  'score': 0.65523505,
  'index': 13,
  'word': 'san',
  'start': 80,
  'end': 83},
 {'entity': 'B-location',
  'score': 0.4668663,
  'index': 14,
  'word': 'francisco',
  'start': 84,
  'end': 93}]
```

ì›í•œë‹¤ë©´, `pipeline`ì˜ ê²°ê³¼ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ë³µì œí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:

<frameworkcontent>
<pt>
í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³  PyTorch í…ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤:

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("stevhliu/my_awesome_wnut_model")
>>> inputs = tokenizer(text, return_tensors="pt")
```

ì…ë ¥ì„ ëª¨ë¸ì— ì „ë‹¬í•˜ê³  `logits`ì„ ë°˜í™˜í•©ë‹ˆë‹¤:

```py
>>> from transformers import AutoModelForTokenClassification

>>> model = AutoModelForTokenClassification.from_pretrained("stevhliu/my_awesome_wnut_model")
>>> with torch.no_grad():
...     logits = model(**inputs).logits
```

ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ë¥¼ ëª¨ë¸ì˜ `id2label` ë§¤í•‘ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë ˆì´ë¸”ë¡œ ë³€í™˜í•©ë‹ˆë‹¤:

```py
>>> predictions = torch.argmax(logits, dim=2)
>>> predicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]
>>> predicted_token_class
['O',
 'O',
 'B-location',
 'I-location',
 'B-group',
 'O',
 'O',
 'O',
 'O',
 'O',
 'O',
 'O',
 'O',
 'B-location',
 'B-location',
 'O',
 'O']
```
</pt>
<tf>
í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³  TensorFlow í…ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤:

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("stevhliu/my_awesome_wnut_model")
>>> inputs = tokenizer(text, return_tensors="tf")
```

ì…ë ¥ê°’ì„ ëª¨ë¸ì— ì „ë‹¬í•˜ê³  `logits`ì„ ë°˜í™˜í•©ë‹ˆë‹¤:

```py
>>> from transformers import TFAutoModelForTokenClassification

>>> model = TFAutoModelForTokenClassification.from_pretrained("stevhliu/my_awesome_wnut_model")
>>> logits = model(**inputs).logits
```

ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ë¥¼ ëª¨ë¸ì˜ `id2label` ë§¤í•‘ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë ˆì´ë¸”ë¡œ ë³€í™˜í•©ë‹ˆë‹¤:

```py
>>> predicted_token_class_ids = tf.math.argmax(logits, axis=-1)
>>> predicted_token_class = [model.config.id2label[t] for t in predicted_token_class_ids[0].numpy().tolist()]
>>> predicted_token_class
['O',
 'O',
 'B-location',
 'I-location',
 'B-group',
 'O',
 'O',
 'O',
 'O',
 'O',
 'O',
 'O',
 'O',
 'B-location',
 'B-location',
 'O',
 'O']
```
</tf>
</frameworkcontent>
