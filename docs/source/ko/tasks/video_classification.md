<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# ì˜ìƒ ë¶„ë¥˜ [[video-classification]]

[[open-in-colab]]


ì˜ìƒ ë¶„ë¥˜ëŠ” ì˜ìƒ ì „ì²´ì— ë ˆì´ë¸” ë˜ëŠ” í´ëž˜ìŠ¤ë¥¼ ì§€ì •í•˜ëŠ” ìž‘ì—…ìž…ë‹ˆë‹¤. ê° ì˜ìƒì—ëŠ” í•˜ë‚˜ì˜ í´ëž˜ìŠ¤ê°€ ìžˆì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤. ì˜ìƒ ë¶„ë¥˜ ëª¨ë¸ì€ ì˜ìƒì„ ìž…ë ¥ìœ¼ë¡œ ë°›ì•„ ì–´ëŠ í´ëž˜ìŠ¤ì— ì†í•˜ëŠ”ì§€ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ëª¨ë¸ì€ ì˜ìƒì´ ì–´ë–¤ ë‚´ìš©ì¸ì§€ ë¶„ë¥˜í•˜ëŠ” ë° ì‚¬ìš©ë  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ì˜ìƒ ë¶„ë¥˜ì˜ ì‹¤ì œ ì‘ìš© ì˜ˆëŠ” í”¼íŠ¸ë‹ˆìŠ¤ ì•±ì—ì„œ ìœ ìš©í•œ ë™ìž‘ / ìš´ë™ ì¸ì‹ ì„œë¹„ìŠ¤ê°€ ìžˆìŠµë‹ˆë‹¤. ì´ëŠ” ë˜í•œ ì‹œê° ìž¥ì• ì¸ì´ ì´ë™í•  ë•Œ ë³´ì¡°í•˜ëŠ”ë° ì‚¬ìš©ë  ìˆ˜ ìžˆìŠµë‹ˆë‹¤

ì´ ê°€ì´ë“œì—ì„œëŠ” ë‹¤ìŒì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤:

1. [UCF101](https://www.crcv.ucf.edu/data/UCF101.php) ë°ì´í„° ì„¸íŠ¸ì˜ í•˜ìœ„ ì§‘í•©ì„ í†µí•´ [VideoMAE](https://huggingface.co/docs/transformers/main/en/model_doc/videomae) ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ê¸°.
2. ë¯¸ì„¸ ì¡°ì •í•œ ëª¨ë¸ì„ ì¶”ë¡ ì— ì‚¬ìš©í•˜ê¸°.

> [!TIP]
> ì´ ìž‘ì—…ê³¼ í˜¸í™˜ë˜ëŠ” ëª¨ë“  ì•„í‚¤í…ì²˜ì™€ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë³´ë ¤ë©´ [ìž‘ì—… íŽ˜ì´ì§€](https://huggingface.co/tasks/video-classification)ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.


ì‹œìž‘í•˜ê¸° ì „ì— í•„ìš”í•œ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:
```bash
pip install -q pytorchvideo transformers evaluate
```

ì˜ìƒì„ ì²˜ë¦¬í•˜ê³  ì¤€ë¹„í•˜ê¸° ìœ„í•´ [PyTorchVideo](https://pytorchvideo.org/)(ì´í•˜ `pytorchvideo`)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

ì»¤ë®¤ë‹ˆí‹°ì— ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ê³  ê³µìœ í•  ìˆ˜ ìžˆë„ë¡ Hugging Face ê³„ì •ì— ë¡œê·¸ì¸í•˜ëŠ” ê²ƒì„ ê¶Œìž¥í•©ë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ê°€ ë‚˜íƒ€ë‚˜ë©´ í† í°ì„ ìž…ë ¥í•˜ì—¬ ë¡œê·¸ì¸í•˜ì„¸ìš”:

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## UCF101 ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸° [[load-ufc101-dataset]]

[UCF-101](https://www.crcv.ucf.edu/data/UCF101.php) ë°ì´í„° ì„¸íŠ¸ì˜ í•˜ìœ„ ì§‘í•©(subset)ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ê²ƒìœ¼ë¡œ ì‹œìž‘í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ì „ì²´ ë°ì´í„° ì„¸íŠ¸ë¥¼ í•™ìŠµí•˜ëŠ”ë° ë” ë§Žì€ ì‹œê°„ì„ í• ì• í•˜ê¸° ì „ì— ë°ì´í„°ì˜ í•˜ìœ„ ì§‘í•©ì„ ë¶ˆëŸ¬ì™€ ëª¨ë“  ê²ƒì´ ìž˜ ìž‘ë™í•˜ëŠ”ì§€ ì‹¤í—˜í•˜ê³  í™•ì¸í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

```py
>>> from huggingface_hub import hf_hub_download

>>> hf_dataset_identifier = "sayakpaul/ucf101-subset"
>>> filename = "UCF101_subset.tar.gz"
>>> file_path = hf_hub_download(repo_id=hf_dataset_identifier, filename=filename, repo_type="dataset")
```

ë°ì´í„° ì„¸íŠ¸ì˜ í•˜ìœ„ ì§‘í•©ì´ ë‹¤ìš´ë¡œë“œ ë˜ë©´, ì••ì¶•ëœ íŒŒì¼ì˜ ì••ì¶•ì„ í•´ì œí•´ì•¼ í•©ë‹ˆë‹¤:
```py
>>> import tarfile

>>> with tarfile.open(file_path) as t:
...      t.extractall(".")
```

ì „ì²´ ë°ì´í„° ì„¸íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ì´ êµ¬ì„±ë˜ì–´ ìžˆìŠµë‹ˆë‹¤.

```bash
UCF101_subset/
    train/
        BandMarching/
            video_1.mp4
            video_2.mp4
            ...
        Archery
            video_1.mp4
            video_2.mp4
            ...
        ...
    val/
        BandMarching/
            video_1.mp4
            video_2.mp4
            ...
        Archery
            video_1.mp4
            video_2.mp4
            ...
        ...
    test/
        BandMarching/
            video_1.mp4
            video_2.mp4
            ...
        Archery
            video_1.mp4
            video_2.mp4
            ...
        ...
```


ì •ë ¬ëœ ì˜ìƒì˜ ê²½ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

```bash
...
'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c04.avi',
'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c06.avi',
'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi',
'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c02.avi',
'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c06.avi'
...
```

ë™ì¼í•œ ê·¸ë£¹/ìž¥ë©´ì— ì†í•˜ëŠ” ì˜ìƒ í´ë¦½ì€ íŒŒì¼ ê²½ë¡œì—ì„œ `g`ë¡œ í‘œì‹œë˜ì–´ ìžˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´, `v_ApplyEyeMakeup_g07_c04.avi`ì™€ `v_ApplyEyeMakeup_g07_c06.avi` ì´ ìžˆìŠµë‹ˆë‹¤. ì´ ë‘˜ì€ ê°™ì€ ê·¸ë£¹ìž…ë‹ˆë‹¤.

ê²€ì¦ ë° í‰ê°€ ë°ì´í„° ë¶„í• ì„ í•  ë•Œ, [ë°ì´í„° ëˆ„ì¶œ(data leakage)](https://www.kaggle.com/code/alexisbcook/data-leakage)ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ë™ì¼í•œ ê·¸ë£¹ / ìž¥ë©´ì˜ ì˜ìƒ í´ë¦½ì„ ì‚¬ìš©í•˜ì§€ ì•Šì•„ì•¼ í•©ë‹ˆë‹¤. ì´ íŠœí† ë¦¬ì–¼ì—ì„œ ì‚¬ìš©í•˜ëŠ” í•˜ìœ„ ì§‘í•©ì€ ì´ëŸ¬í•œ ì •ë³´ë¥¼ ê³ ë ¤í•˜ê³  ìžˆìŠµë‹ˆë‹¤.

ê·¸ ë‹¤ìŒìœ¼ë¡œ, ë°ì´í„° ì„¸íŠ¸ì— ì¡´ìž¬í•˜ëŠ” ë¼ë²¨ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. ë˜í•œ, ëª¨ë¸ì„ ì´ˆê¸°í™”í•  ë•Œ ë„ì›€ì´ ë  ë”•ì…”ë„ˆë¦¬(dictionary data type)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

* `label2id`: í´ëž˜ìŠ¤ ì´ë¦„ì„ ì •ìˆ˜ì— ë§¤í•‘í•©ë‹ˆë‹¤.
* `id2label`: ì •ìˆ˜ë¥¼ í´ëž˜ìŠ¤ ì´ë¦„ì— ë§¤í•‘í•©ë‹ˆë‹¤.

```py
>>> class_labels = sorted({str(path).split("/")[2] for path in all_video_file_paths})
>>> label2id = {label: i for i, label in enumerate(class_labels)}
>>> id2label = {i: label for label, i in label2id.items()}

>>> print(f"Unique classes: {list(label2id.keys())}.")

# Unique classes: ['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress'].
```

ì´ ë°ì´í„° ì„¸íŠ¸ì—ëŠ” ì´ 10ê°œì˜ ê³ ìœ í•œ í´ëž˜ìŠ¤ê°€ ìžˆìŠµë‹ˆë‹¤. ê° í´ëž˜ìŠ¤ë§ˆë‹¤ 30ê°œì˜ ì˜ìƒì´ í›ˆë ¨ ì„¸íŠ¸ì— ìžˆìŠµë‹ˆë‹¤

## ë¯¸ì„¸ ì¡°ì •í•˜ê¸° ìœ„í•´ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° [[load-a-model-to-fine-tune]]

ì‚¬ì „ í›ˆë ¨ëœ ì²´í¬í¬ì¸íŠ¸ì™€ ì²´í¬í¬ì¸íŠ¸ì— ì—°ê´€ëœ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ìƒ ë¶„ë¥˜ ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•©ë‹ˆë‹¤. ëª¨ë¸ì˜ ì¸ì½”ë”ì—ëŠ” ë¯¸ë¦¬ í•™ìŠµëœ ë§¤ê°œë³€ìˆ˜ê°€ ì œê³µë˜ë©°, ë¶„ë¥˜ í—¤ë“œ(ë°ì´í„°ë¥¼ ë¶„ë¥˜í•˜ëŠ” ë§ˆì§€ë§‰ ë ˆì´ì–´)ëŠ” ë¬´ìž‘ìœ„ë¡œ ì´ˆê¸°í™”ë©ë‹ˆë‹¤. ë°ì´í„° ì„¸íŠ¸ì˜ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ìž‘ì„±í•  ë•ŒëŠ” ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œê°€ ìœ ìš©í•©ë‹ˆë‹¤.

```py
>>> from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification

>>> model_ckpt = "MCG-NJU/videomae-base"
>>> image_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)
>>> model = VideoMAEForVideoClassification.from_pretrained(
...     model_ckpt,
...     label2id=label2id,
...     id2label=id2label,
...     ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint
... )
```

ëª¨ë¸ì„ ê°€ì ¸ì˜¤ëŠ” ë™ì•ˆ, ë‹¤ìŒê³¼ ê°™ì€ ê²½ê³ ë¥¼ ë§ˆì£¼ì¹  ìˆ˜ ìžˆìŠµë‹ˆë‹¤:

```bash
Some weights of the model checkpoint at MCG-NJU/videomae-base were not used when initializing VideoMAEForVideoClassification: [..., 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.key.weight']
- This IS expected if you are initializing VideoMAEForVideoClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing VideoMAEForVideoClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
```


ìœ„ ê²½ê³ ëŠ” ìš°ë¦¬ê°€ ì¼ë¶€ ê°€ì¤‘ì¹˜(ì˜ˆ: `classifier` ì¸µì˜ ê°€ì¤‘ì¹˜ì™€ íŽ¸í–¥)ë¥¼ ë²„ë¦¬ê³  ìƒˆë¡œìš´ `classifier` ì¸µì˜ ê°€ì¤‘ì¹˜ì™€ íŽ¸í–¥ì„ ë¬´ìž‘ìœ„ë¡œ ì´ˆê¸°í™”í•˜ê³  ìžˆë‹¤ëŠ” ê²ƒì„ ì•Œë ¤ì¤ë‹ˆë‹¤. ì´ ê²½ìš°ì—ëŠ” ë¯¸ë¦¬ í•™ìŠµëœ ê°€ì¤‘ì¹˜ê°€ ì—†ëŠ” ìƒˆë¡œìš´ í—¤ë“œë¥¼ ì¶”ê°€í•˜ê³  ìžˆìœ¼ë¯€ë¡œ, ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ëª¨ë¸ì„ ì¶”ë¡ ì— ì‚¬ìš©í•˜ê¸° ì „ì— ë¯¸ì„¸ ì¡°ì •í•˜ë¼ê³  ê²½ê³ ë¥¼ ë³´ë‚´ëŠ” ê²ƒì€ ë‹¹ì—°í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì´ì œ ìš°ë¦¬ëŠ” ì´ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•  ì˜ˆì •ìž…ë‹ˆë‹¤.

**ì°¸ê³ ** ì´ [ì²´í¬í¬ì¸íŠ¸](https://huggingface.co/MCG-NJU/videomae-base-finetuned-kinetics)ëŠ” ë„ë©”ì¸ì´ ë§Žì´ ì¤‘ì²©ëœ ìœ ì‚¬í•œ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ìž‘ì—…ì— ëŒ€í•´ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ì–»ì€ ì²´í¬í¬ì¸íŠ¸ì´ë¯€ë¡œ ì´ ìž‘ì—…ì—ì„œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤. `MCG-NJU/videomae-base-finetuned-kinetics` ë°ì´í„° ì„¸íŠ¸ë¥¼ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ì–»ì€ [ì²´í¬í¬ì¸íŠ¸](https://huggingface.co/sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset)ë„ ìžˆìŠµë‹ˆë‹¤.

## í›ˆë ¨ì„ ìœ„í•œ ë°ì´í„° ì„¸íŠ¸ ì¤€ë¹„í•˜ê¸°[[prepare-the-datasets-for-training]]

ì˜ìƒ ì „ì²˜ë¦¬ë¥¼ ìœ„í•´ [PyTorchVideo ë¼ì´ë¸ŒëŸ¬ë¦¬](https://pytorchvideo.org/)ë¥¼ í™œìš©í•  ê²ƒìž…ë‹ˆë‹¤. í•„ìš”í•œ ì¢…ì†ì„±ì„ ê°€ì ¸ì˜¤ëŠ” ê²ƒìœ¼ë¡œ ì‹œìž‘í•˜ì„¸ìš”.

```py
>>> import pytorchvideo.data

>>> from pytorchvideo.transforms import (
...     ApplyTransformToKey,
...     Normalize,
...     RandomShortSideScale,
...     RemoveKey,
...     ShortSideScale,
...     UniformTemporalSubsample,
... )

>>> from torchvision.transforms import (
...     Compose,
...     Lambda,
...     RandomCrop,
...     RandomHorizontalFlip,
...     Resize,
... )
```

í•™ìŠµ ë°ì´í„° ì„¸íŠ¸ ë³€í™˜ì—ëŠ” 'ê· ì¼í•œ ì‹œê°„ ìƒ˜í”Œë§(uniform temporal subsampling)', 'í”½ì…€ ì •ê·œí™”(pixel normalization)', 'ëžœë¤ ìž˜ë¼ë‚´ê¸°(random cropping)' ë° 'ëžœë¤ ìˆ˜í‰ ë’¤ì§‘ê¸°(random horizontal flipping)'ì˜ ì¡°í•©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ê²€ì¦ ë° í‰ê°€ ë°ì´í„° ì„¸íŠ¸ ë³€í™˜ì—ëŠ” 'ëžœë¤ ìž˜ë¼ë‚´ê¸°'ì™€ 'ëžœë¤ ë’¤ì§‘ê¸°'ë¥¼ ì œì™¸í•œ ë™ì¼í•œ ë³€í™˜ ì²´ì¸ì„ ìœ ì§€í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë³€í™˜ì— ëŒ€í•´ ìžì„¸ížˆ ì•Œì•„ë³´ë ¤ë©´ [PyTorchVideo ê³µì‹ ë¬¸ì„œ](https://pytorchvideo.org)ë¥¼ í™•ì¸í•˜ì„¸ìš”.

ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ê³¼ ê´€ë ¨ëœ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìžˆìŠµë‹ˆë‹¤:

* ì˜ìƒ í”„ë ˆìž„ í”½ì…€ì„ ì •ê·œí™”í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ì´ë¯¸ì§€ í‰ê· ê³¼ í‘œì¤€ íŽ¸ì°¨
* ì˜ìƒ í”„ë ˆìž„ì´ ì¡°ì •ë  ê³µê°„ í•´ìƒë„


ë¨¼ì €, ëª‡ ê°€ì§€ ìƒìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

```py
>>> mean = image_processor.image_mean
>>> std = image_processor.image_std
>>> if "shortest_edge" in image_processor.size:
...     height = width = image_processor.size["shortest_edge"]
>>> else:
...     height = image_processor.size["height"]
...     width = image_processor.size["width"]
>>> resize_to = (height, width)

>>> num_frames_to_sample = model.config.num_frames
>>> sample_rate = 4
>>> fps = 30
>>> clip_duration = num_frames_to_sample * sample_rate / fps
```

ì´ì œ ë°ì´í„° ì„¸íŠ¸ì— íŠ¹í™”ëœ ì „ì²˜ë¦¬(transform)ê³¼ ë°ì´í„° ì„¸íŠ¸ ìžì²´ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ë¨¼ì € í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸ë¡œ ì‹œìž‘í•©ë‹ˆë‹¤:

```py
>>> train_transform = Compose(
...     [
...         ApplyTransformToKey(
...             key="video",
...             transform=Compose(
...                 [
...                     UniformTemporalSubsample(num_frames_to_sample),
...                     Lambda(lambda x: x / 255.0),
...                     Normalize(mean, std),
...                     RandomShortSideScale(min_size=256, max_size=320),
...                     RandomCrop(resize_to),
...                     RandomHorizontalFlip(p=0.5),
...                 ]
...             ),
...         ),
...     ]
... )

>>> train_dataset = pytorchvideo.data.Ucf101(
...     data_path=os.path.join(dataset_root_path, "train"),
...     clip_sampler=pytorchvideo.data.make_clip_sampler("random", clip_duration),
...     decode_audio=False,
...     transform=train_transform,
... )
```

ê°™ì€ ë°©ì‹ì˜ ìž‘ì—… íë¦„ì„ ê²€ì¦ê³¼ í‰ê°€ ì„¸íŠ¸ì—ë„ ì ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

```py
>>> val_transform = Compose(
...     [
...         ApplyTransformToKey(
...             key="video",
...             transform=Compose(
...                 [
...                     UniformTemporalSubsample(num_frames_to_sample),
...                     Lambda(lambda x: x / 255.0),
...                     Normalize(mean, std),
...                     Resize(resize_to),
...                 ]
...             ),
...         ),
...     ]
... )

>>> val_dataset = pytorchvideo.data.Ucf101(
...     data_path=os.path.join(dataset_root_path, "val"),
...     clip_sampler=pytorchvideo.data.make_clip_sampler("uniform", clip_duration),
...     decode_audio=False,
...     transform=val_transform,
... )

>>> test_dataset = pytorchvideo.data.Ucf101(
...     data_path=os.path.join(dataset_root_path, "test"),
...     clip_sampler=pytorchvideo.data.make_clip_sampler("uniform", clip_duration),
...     decode_audio=False,
...     transform=val_transform,
... )
```


**ì°¸ê³ **: ìœ„ì˜ ë°ì´í„° ì„¸íŠ¸ì˜ íŒŒì´í”„ë¼ì¸ì€ [ê³µì‹ íŒŒì´í† ì¹˜ ì˜ˆì œ](https://pytorchvideo.org/docs/tutorial_classification#dataset)ì—ì„œ ê°€ì ¸ì˜¨ ê²ƒìž…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” UCF-101 ë°ì´í„°ì…‹ì— ë§žê²Œ [`pytorchvideo.data.Ucf101()`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.Ucf101) í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê³  ìžˆìŠµë‹ˆë‹¤. ë‚´ë¶€ì ìœ¼ë¡œ ì´ í•¨ìˆ˜ëŠ” [`pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.LabeledVideoDataset) ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. `LabeledVideoDataset` í´ëž˜ìŠ¤ëŠ” PyTorchVideo ë°ì´í„°ì…‹ì—ì„œ ëª¨ë“  ì˜ìƒ ê´€ë ¨ ìž‘ì—…ì˜ ê¸°ë³¸ í´ëž˜ìŠ¤ìž…ë‹ˆë‹¤. ë”°ë¼ì„œ PyTorchVideoì—ì„œ ë¯¸ë¦¬ ì œê³µí•˜ì§€ ì•ŠëŠ” ì‚¬ìš©ìž ì§€ì • ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´, ì´ í´ëž˜ìŠ¤ë¥¼ ì ì ˆí•˜ê²Œ í™•ìž¥í•˜ë©´ ë©ë‹ˆë‹¤. ë” ìžì„¸í•œ ì‚¬í•­ì´ ì•Œê³  ì‹¶ë‹¤ë©´ `data` API [ë¬¸ì„œ](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html) ë¥¼ ì°¸ê³ í•˜ì„¸ìš”. ë˜í•œ ìœ„ì˜ ì˜ˆì‹œì™€ ìœ ì‚¬í•œ êµ¬ì¡°ë¥¼ ê°–ëŠ” ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ê³  ìžˆë‹¤ë©´, `pytorchvideo.data.Ucf101()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ë° ë¬¸ì œê°€ ì—†ì„ ê²ƒìž…ë‹ˆë‹¤.

ë°ì´í„° ì„¸íŠ¸ì— ì˜ìƒì˜ ê°œìˆ˜ë¥¼ ì•Œê¸° ìœ„í•´ `num_videos` ì¸ìˆ˜ì— ì ‘ê·¼í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

```py
>>> print(train_dataset.num_videos, val_dataset.num_videos, test_dataset.num_videos)
# (300, 30, 75)
```

## ë” ë‚˜ì€ ë””ë²„ê¹…ì„ ìœ„í•´ ì „ì²˜ë¦¬ ì˜ìƒ ì‹œê°í™”í•˜ê¸°[[visualize-the-preprocessed-video-for-better-debugging]]

```py
>>> import imageio
>>> import numpy as np
>>> from IPython.display import Image

>>> def unnormalize_img(img):
...     """Un-normalizes the image pixels."""
...     img = (img * std) + mean
...     img = (img * 255).astype("uint8")
...     return img.clip(0, 255)

>>> def create_gif(video_tensor, filename="sample.gif"):
...     """Prepares a GIF from a video tensor.
...
...     The video tensor is expected to have the following shape:
...     (num_frames, num_channels, height, width).
...     """
...     frames = []
...     for video_frame in video_tensor:
...         frame_unnormalized = unnormalize_img(video_frame.permute(1, 2, 0).numpy())
...         frames.append(frame_unnormalized)
...     kargs = {"duration": 0.25}
...     imageio.mimsave(filename, frames, "GIF", **kargs)
...     return filename

>>> def display_gif(video_tensor, gif_name="sample.gif"):
...     """Prepares and displays a GIF from a video tensor."""
...     video_tensor = video_tensor.permute(1, 0, 2, 3)
...     gif_filename = create_gif(video_tensor, gif_name)
...     return Image(filename=gif_filename)

>>> sample_video = next(iter(train_dataset))
>>> video_tensor = sample_video["video"]
>>> display_gif(video_tensor)
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_gif.gif" alt="Person playing basketball"/>
</div>

## ëª¨ë¸ í›ˆë ¨í•˜ê¸°[[train-the-model]]

ðŸ¤— Transformersì˜ [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer)ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨ì‹œì¼œë³´ì„¸ìš”. `Trainer`ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ë ¤ë©´ í›ˆë ¨ ì„¤ì •ê³¼ í‰ê°€ ì§€í‘œë¥¼ ì •ì˜í•´ì•¼ í•©ë‹ˆë‹¤.  ê°€ìž¥ ì¤‘ìš”í•œ ê²ƒì€ [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments)ìž…ë‹ˆë‹¤. ì´ í´ëž˜ìŠ¤ëŠ” í›ˆë ¨ì„ êµ¬ì„±í•˜ëŠ” ëª¨ë“  ì†ì„±ì„ í¬í•¨í•˜ë©°, í›ˆë ¨ ì¤‘ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ìž¥í•  ì¶œë ¥ í´ë” ì´ë¦„ì„ í•„ìš”ë¡œ í•©ë‹ˆë‹¤. ë˜í•œ ðŸ¤— Hubì˜ ëª¨ë¸ ì €ìž¥ì†Œì˜ ëª¨ë“  ì •ë³´ë¥¼ ë™ê¸°í™”í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.

ëŒ€ë¶€ë¶„ì˜ í›ˆë ¨ ì¸ìˆ˜ëŠ” ë”°ë¡œ ì„¤ëª…í•  í•„ìš”ëŠ” ì—†ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì—¬ê¸°ì—ì„œ ì¤‘ìš”í•œ ì¸ìˆ˜ëŠ” `remove_unused_columns=False` ìž…ë‹ˆë‹¤. ì´ ì¸ìžëŠ” ëª¨ë¸ì˜ í˜¸ì¶œ í•¨ìˆ˜ì—ì„œ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ëª¨ë“  ì†ì„± ì—´(columns)ì„ ì‚­ì œí•©ë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ ì¼ë°˜ì ìœ¼ë¡œ Trueìž…ë‹ˆë‹¤. ì´ëŠ” ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ê¸°ëŠ¥ ì—´ì„ ì‚­ì œí•˜ëŠ” ê²ƒì´ ì´ìƒì ì´ë©°, ìž…ë ¥ì„ ëª¨ë¸ì˜ í˜¸ì¶œ í•¨ìˆ˜ë¡œ í’€ê¸°(unpack)ê°€ ì‰¬ì›Œì§€ê¸° ë•Œë¬¸ìž…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ ê²½ìš°ì—ëŠ” `pixel_values`(ëª¨ë¸ì˜ ìž…ë ¥ìœ¼ë¡œ í•„ìˆ˜ì ì¸ í‚¤)ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ê¸°ëŠ¥('video'ê°€ íŠ¹ížˆ ê·¸ë ‡ìŠµë‹ˆë‹¤)ì´ í•„ìš”í•©ë‹ˆë‹¤. ë”°ë¼ì„œ remove_unused_columnsì„ Falseë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.

```py
>>> from transformers import TrainingArguments, Trainer

>>> model_name = model_ckpt.split("/")[-1]
>>> new_model_name = f"{model_name}-finetuned-ucf101-subset"
>>> num_epochs = 4

>>> args = TrainingArguments(
...     new_model_name,
...     remove_unused_columns=False,
...     eval_strategy="epoch",
...     save_strategy="epoch",
...     learning_rate=5e-5,
...     per_device_train_batch_size=batch_size,
...     per_device_eval_batch_size=batch_size,
...     warmup_ratio=0.1,
...     logging_steps=10,
...     load_best_model_at_end=True,
...     metric_for_best_model="accuracy",
...     push_to_hub=True,
...     max_steps=(train_dataset.num_videos // batch_size) * num_epochs,
... )
```

`pytorchvideo.data.Ucf101()` í•¨ìˆ˜ë¡œ ë°˜í™˜ë˜ëŠ” ë°ì´í„° ì„¸íŠ¸ëŠ” `__len__` ë©”ì†Œë“œê°€ ì´ì‹ë˜ì–´ ìžˆì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ,  `TrainingArguments`ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•  ë•Œ `max_steps`ë¥¼ ì •ì˜í•´ì•¼ í•©ë‹ˆë‹¤.

ë‹¤ìŒìœ¼ë¡œ, í‰ê°€ì§€í‘œë¥¼ ë¶ˆëŸ¬ì˜¤ê³ , ì˜ˆì¸¡ê°’ì—ì„œ í‰ê°€ì§€í‘œë¥¼ ê³„ì‚°í•  í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. í•„ìš”í•œ ì „ì²˜ë¦¬ ìž‘ì—…ì€ ì˜ˆì¸¡ëœ ë¡œì§“(logits)ì— argmax ê°’ì„ ì·¨í•˜ëŠ” ê²ƒë¿ìž…ë‹ˆë‹¤:

```py
import evaluate

metric = evaluate.load("accuracy")


def compute_metrics(eval_pred):
    predictions = np.argmax(eval_pred.predictions, axis=1)
    return metric.compute(predictions=predictions, references=eval_pred.label_ids)
```

**í‰ê°€ì— ëŒ€í•œ ì°¸ê³ ì‚¬í•­**:

[VideoMAE ë…¼ë¬¸](https://huggingface.co/papers/2203.12602)ì—ì„œ ì €ìžëŠ” ë‹¤ìŒê³¼ ê°™ì€ í‰ê°€ ì „ëžµì„ ì‚¬ìš©í•©ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ì˜ìƒì—ì„œ ì—¬ëŸ¬ í´ë¦½ì„ ì„ íƒí•˜ê³  ê·¸ í´ë¦½ì— ë‹¤ì–‘í•œ í¬ë¡­ì„ ì ìš©í•˜ì—¬ ì§‘ê³„ ì ìˆ˜ë¥¼ ë³´ê³ í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ë²ˆ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” ê°„ë‹¨í•¨ê³¼ ê°„ê²°í•¨ì„ ìœ„í•´ í•´ë‹¹ ì „ëžµì„ ê³ ë ¤í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

ë˜í•œ, ì˜ˆì œë¥¼ ë¬¶ì–´ì„œ ë°°ì¹˜ë¥¼ í˜•ì„±í•˜ëŠ” `collate_fn`ì„ ì •ì˜í•´ì•¼í•©ë‹ˆë‹¤. ê° ë°°ì¹˜ëŠ” `pixel_values`ì™€ `labels`ë¼ëŠ” 2ê°œì˜ í‚¤ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

```py
>>> def collate_fn(examples):
...     # permute to (num_frames, num_channels, height, width)
...     pixel_values = torch.stack(
...         [example["video"].permute(1, 0, 2, 3) for example in examples]
...     )
...     labels = torch.tensor([example["label"] for example in examples])
...     return {"pixel_values": pixel_values, "labels": labels}
```

ê·¸ëŸ° ë‹¤ìŒ ì´ ëª¨ë“  ê²ƒì„ ë°ì´í„° ì„¸íŠ¸ì™€ í•¨ê»˜ `Trainer`ì— ì „ë‹¬í•˜ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤:

```py
>>> trainer = Trainer(
...     model,
...     args,
...     train_dataset=train_dataset,
...     eval_dataset=val_dataset,
...     processing_class=image_processor,
...     compute_metrics=compute_metrics,
...     data_collator=collate_fn,
... )
```

ë°ì´í„°ë¥¼ ì´ë¯¸ ì²˜ë¦¬í–ˆëŠ”ë°ë„ ë¶ˆêµ¬í•˜ê³  `image_processor`ë¥¼ í† í¬ë‚˜ì´ì € ì¸ìˆ˜ë¡œ ë„£ì€ ì´ìœ ëŠ” JSONìœ¼ë¡œ ì €ìž¥ë˜ëŠ” ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ êµ¬ì„± íŒŒì¼ì´ Hubì˜ ì €ìž¥ì†Œì— ì—…ë¡œë“œë˜ë„ë¡ í•˜ê¸° ìœ„í•¨ìž…ë‹ˆë‹¤.

`train` ë©”ì†Œë“œë¥¼ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ì„¸ìš”:

```py
>>> train_results = trainer.train()
```

í•™ìŠµì´ ì™„ë£Œë˜ë©´, ëª¨ë¸ì„ [`~transformers.Trainer.push_to_hub`] ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í—ˆë¸Œì— ê³µìœ í•˜ì—¬ ëˆ„êµ¬ë‚˜ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìžˆë„ë¡ í•©ë‹ˆë‹¤:
```py
>>> trainer.push_to_hub()
```

## ì¶”ë¡ í•˜ê¸°[[inference]]

ì¢‹ìŠµë‹ˆë‹¤. ì´ì œ ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì„ ì¶”ë¡ í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

ì¶”ë¡ ì— ì‚¬ìš©í•  ì˜ìƒì„ ë¶ˆëŸ¬ì˜¤ì„¸ìš”:
```py
>>> sample_test_video = next(iter(test_dataset))
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_gif_two.gif" alt="Teams playing basketball"/>
</div>

ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì„ ì¶”ë¡ ì— ì‚¬ìš©í•˜ëŠ” ê°€ìž¥ ê°„ë‹¨í•œ ë°©ë²•ì€ [`pipeline`](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.VideoClassificationPipeline)ì—ì„œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒìž…ë‹ˆë‹¤. ëª¨ë¸ë¡œ ì˜ìƒ ë¶„ë¥˜ë¥¼ í•˜ê¸° ìœ„í•´ `pipeline`ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ê³  ì˜ìƒì„ ì „ë‹¬í•˜ì„¸ìš”:

```py
>>> from transformers import pipeline

>>> video_cls = pipeline(model="my_awesome_video_cls_model")
>>> video_cls("https://huggingface.co/datasets/sayakpaul/ucf101-subset/resolve/main/v_BasketballDunk_g14_c06.avi")
[{'score': 0.9272987842559814, 'label': 'BasketballDunk'},
 {'score': 0.017777055501937866, 'label': 'BabyCrawling'},
 {'score': 0.01663011871278286, 'label': 'BalanceBeam'},
 {'score': 0.009560945443809032, 'label': 'BandMarching'},
 {'score': 0.0068979403004050255, 'label': 'BaseballPitch'}]
```

ë§Œì•½ ì›í•œë‹¤ë©´ ìˆ˜ë™ìœ¼ë¡œ `pipeline`ì˜ ê²°ê³¼ë¥¼ ìž¬í˜„í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤:


```py
>>> def run_inference(model, video):
...     # (num_frames, num_channels, height, width)
...     perumuted_sample_test_video = video.permute(1, 0, 2, 3)
...     inputs = {
...         "pixel_values": perumuted_sample_test_video.unsqueeze(0),
...         "labels": torch.tensor(
...             [sample_test_video["label"]]
...         ),  # this can be skipped if you don't have labels available.
...     }

...     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
...     inputs = {k: v.to(device) for k, v in inputs.items()}
...     model = model.to(device)

...     # forward pass
...     with torch.no_grad():
...         outputs = model(**inputs)
...         logits = outputs.logits

...     return logits
```

ëª¨ë¸ì— ìž…ë ¥ê°’ì„ ë„£ê³  `logits`ì„ ë°˜í™˜ë°›ìœ¼ì„¸ìš”:

```py
>>> logits = run_inference(trained_model, sample_test_video["video"])
```

`logits`ì„ ë””ì½”ë”©í•˜ë©´, ìš°ë¦¬ëŠ” ë‹¤ìŒ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìžˆìŠµë‹ˆë‹¤:

```py
>>> predicted_class_idx = logits.argmax(-1).item()
>>> print("Predicted class:", model.config.id2label[predicted_class_idx])
# Predicted class: BasketballDunk
```
