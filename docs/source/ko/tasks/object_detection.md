<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# ê°ì²´ íƒì§€ [[object-detection]]

[[open-in-colab]]

ê°ì²´ íƒì§€ëŠ” ì´ë¯¸ì§€ì—ì„œ ì¸ìŠ¤í„´ìŠ¤(ì˜ˆ: ì‚¬ëŒ, ê±´ë¬¼ ë˜ëŠ” ìë™ì°¨)ë¥¼ ê°ì§€í•˜ëŠ” ì»´í“¨í„° ë¹„ì „ ì‘ì—…ì…ë‹ˆë‹¤. ê°ì²´ íƒì§€ ëª¨ë¸ì€ ì´ë¯¸ì§€ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ê³  íƒì§€ëœ ë°”ìš´ë”© ë°•ìŠ¤ì˜ ì¢Œí‘œì™€ ê´€ë ¨ëœ ë ˆì´ë¸”ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
í•˜ë‚˜ì˜ ì´ë¯¸ì§€ì—ëŠ” ì—¬ëŸ¬ ê°ì²´ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë©° ê°ê°ì€ ìì²´ì ì¸ ë°”ìš´ë”© ë°•ìŠ¤ì™€ ë ˆì´ë¸”ì„ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤(ì˜ˆ: ì°¨ì™€ ê±´ë¬¼ì´ ìˆëŠ” ì´ë¯¸ì§€).
ë˜í•œ ê° ê°ì²´ëŠ” ì´ë¯¸ì§€ì˜ ë‹¤ë¥¸ ë¶€ë¶„ì— ì¡´ì¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤(ì˜ˆ: ì´ë¯¸ì§€ì— ì—¬ëŸ¬ ëŒ€ì˜ ì°¨ê°€ ìˆì„ ìˆ˜ ìˆìŒ).
ì´ ì‘ì—…ì€ ë³´í–‰ì, ë„ë¡œ í‘œì§€íŒ, ì‹ í˜¸ë“±ê³¼ ê°™ì€ ê²ƒë“¤ì„ ê°ì§€í•˜ëŠ” ììœ¨ ì£¼í–‰ì— ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
ë‹¤ë¥¸ ì‘ìš© ë¶„ì•¼ë¡œëŠ” ì´ë¯¸ì§€ ë‚´ ê°ì²´ ìˆ˜ ê³„ì‚° ë° ì´ë¯¸ì§€ ê²€ìƒ‰ ë“±ì´ ìˆìŠµë‹ˆë‹¤.

ì´ ê°€ì´ë“œì—ì„œ ë‹¤ìŒì„ ë°°ìš¸ ê²ƒì…ë‹ˆë‹¤:

 1. í•©ì„±ê³± ë°±ë³¸(ì¸í’‹ ë°ì´í„°ì˜ íŠ¹ì„±ì„ ì¶”ì¶œí•˜ëŠ” í•©ì„±ê³± ë„¤íŠ¸ì›Œí¬)ê³¼ ì¸ì½”ë”-ë””ì½”ë” íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ ê²°í•©í•œ [DETR](https://huggingface.co/docs/transformers/model_doc/detr) ëª¨ë¸ì„ [CPPE-5](https://huggingface.co/datasets/cppe-5) ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•´ ë¯¸ì„¸ì¡°ì • í•˜ê¸°
 2. ë¯¸ì„¸ì¡°ì • í•œ ëª¨ë¸ì„ ì¶”ë¡ ì— ì‚¬ìš©í•˜ê¸°.

> [!TIP]
> ì´ ì‘ì—…ê³¼ í˜¸í™˜ë˜ëŠ” ëª¨ë“  ì•„í‚¤í…ì²˜ì™€ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë³´ë ¤ë©´ [ì‘ì—… í˜ì´ì§€](https://huggingface.co/tasks/object-detection)ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

ì‹œì‘í•˜ê¸° ì „ì— í•„ìš”í•œ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:
```bash
pip install -q datasets transformers evaluate timm albumentations
```

í—ˆê¹…í˜ì´ìŠ¤ í—ˆë¸Œì—ì„œ ë°ì´í„° ì„¸íŠ¸ë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•œ ğŸ¤— Datasetsê³¼ ëª¨ë¸ì„ í•™ìŠµí•˜ê¸° ìœ„í•œ ğŸ¤— Transformers, ë°ì´í„°ë¥¼ ì¦ê°•í•˜ê¸° ìœ„í•œ `albumentations`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
DETR ëª¨ë¸ì˜ í•©ì„±ê³± ë°±ë³¸ì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ì„œëŠ” í˜„ì¬ `timm`ì´ í•„ìš”í•©ë‹ˆë‹¤.

ì»¤ë®¤ë‹ˆí‹°ì— ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ê³  ê³µìœ í•  ìˆ˜ ìˆë„ë¡ Hugging Face ê³„ì •ì— ë¡œê·¸ì¸í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ê°€ ë‚˜íƒ€ë‚˜ë©´ í† í°ì„ ì…ë ¥í•˜ì—¬ ë¡œê·¸ì¸í•˜ì„¸ìš”:

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## CPPE-5 ë°ì´í„° ì„¸íŠ¸ ê°€ì ¸ì˜¤ê¸° [[load-the-CPPE-5-dataset]]

[CPPE-5](https://huggingface.co/datasets/cppe-5) ë°ì´í„° ì„¸íŠ¸ëŠ” COVID-19 ëŒ€ìœ í–‰ ìƒí™©ì—ì„œ ì˜ë£Œ ì „ë¬¸ì¸ë ¥ ë³´í˜¸ ì¥ë¹„(PPE)ë¥¼ ì‹ë³„í•˜ëŠ” ì–´ë…¸í…Œì´ì…˜ì´ í¬í•¨ëœ ì´ë¯¸ì§€ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.

ë°ì´í„° ì„¸íŠ¸ë¥¼ ê°€ì ¸ì˜¤ì„¸ìš”:

```py
>>> from datasets import load_dataset

>>> cppe5 = load_dataset("cppe-5")
>>> cppe5
DatasetDict({
    train: Dataset({
        features: ['image_id', 'image', 'width', 'height', 'objects'],
        num_rows: 1000
    })
    test: Dataset({
        features: ['image_id', 'image', 'width', 'height', 'objects'],
        num_rows: 29
    })
})
```

ì´ ë°ì´í„° ì„¸íŠ¸ëŠ” í•™ìŠµ ì„¸íŠ¸ ì´ë¯¸ì§€ 1,000ê°œì™€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì´ë¯¸ì§€ 29ê°œë¥¼ ê°–ê³  ìˆìŠµë‹ˆë‹¤.

ë°ì´í„°ì— ìµìˆ™í•´ì§€ê¸° ìœ„í•´, ì˜ˆì‹œê°€ ì–´ë–»ê²Œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ì§€ ì‚´í´ë³´ì„¸ìš”.

```py
>>> cppe5["train"][0]
{'image_id': 15,
 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=943x663 at 0x7F9EC9E77C10>,
 'width': 943,
 'height': 663,
 'objects': {'id': [114, 115, 116, 117],
  'area': [3796, 1596, 152768, 81002],
  'bbox': [[302.0, 109.0, 73.0, 52.0],
   [810.0, 100.0, 57.0, 28.0],
   [160.0, 31.0, 248.0, 616.0],
   [741.0, 68.0, 202.0, 401.0]],
  'category': [4, 4, 0, 0]}}
```

ë°ì´í„° ì„¸íŠ¸ì— ìˆëŠ” ì˜ˆì‹œëŠ” ë‹¤ìŒì˜ ì˜ì—­ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤:

- `image_id`: ì˜ˆì‹œ ì´ë¯¸ì§€ id
- `image`: ì´ë¯¸ì§€ë¥¼ í¬í•¨í•˜ëŠ” `PIL.Image.Image` ê°ì²´
- `width`: ì´ë¯¸ì§€ì˜ ë„ˆë¹„
- `height`: ì´ë¯¸ì§€ì˜ ë†’ì´
- `objects`: ì´ë¯¸ì§€ ì•ˆì˜ ê°ì²´ë“¤ì˜ ë°”ìš´ë”© ë°•ìŠ¤ ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬:
  - `id`: ì–´ë…¸í…Œì´ì…˜ id
  - `area`: ë°”ìš´ë”© ë°•ìŠ¤ì˜ ë©´ì 
  - `bbox`: ê°ì²´ì˜ ë°”ìš´ë”© ë°•ìŠ¤ ([COCO í¬ë§·](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/#coco)ìœ¼ë¡œ)
  - `category`: ê°ì²´ì˜ ì¹´í…Œê³ ë¦¬, ê°€ëŠ¥í•œ ê°’ìœ¼ë¡œëŠ” `Coverall (0)`, `Face_Shield (1)`, `Gloves (2)`, `Goggles (3)` ë° `Mask (4)` ê°€ í¬í•¨ë©ë‹ˆë‹¤.

`bbox` í•„ë“œê°€ DETR ëª¨ë¸ì´ ìš”êµ¬í•˜ëŠ” COCO í˜•ì‹ì„ ë”°ë¥¸ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ê·¸ëŸ¬ë‚˜ `objects` ë‚´ë¶€ì˜ í•„ë“œ ê·¸ë£¹ì€ DETRì´ ìš”êµ¬í•˜ëŠ” ì–´ë…¸í…Œì´ì…˜ í˜•ì‹ê³¼ ë‹¤ë¦…ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ ë°ì´í„°ë¥¼ í•™ìŠµì— ì‚¬ìš©í•˜ê¸° ì „ì— ì „ì²˜ë¦¬ë¥¼ ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

ë°ì´í„°ë¥¼ ë” ì˜ ì´í•´í•˜ê¸° ìœ„í•´ì„œ ë°ì´í„° ì„¸íŠ¸ì—ì„œ í•œ ê°€ì§€ ì˜ˆì‹œë¥¼ ì‹œê°í™”í•˜ì„¸ìš”.

```py
>>> import numpy as np
>>> import os
>>> from PIL import Image, ImageDraw

>>> image = cppe5["train"][0]["image"]
>>> annotations = cppe5["train"][0]["objects"]
>>> draw = ImageDraw.Draw(image)

>>> categories = cppe5["train"].features["objects"].feature["category"].names

>>> id2label = {index: x for index, x in enumerate(categories, start=0)}
>>> label2id = {v: k for k, v in id2label.items()}

>>> for i in range(len(annotations["id"])):
...     box = annotations["bbox"][i - 1]
...     class_idx = annotations["category"][i - 1]
...     x, y, w, h = tuple(box)
...     draw.rectangle((x, y, x + w, y + h), outline="red", width=1)
...     draw.text((x, y), id2label[class_idx], fill="white")

>>> image
```

<div class="flex justify-center">
    <img src="https://i.imgur.com/TdaqPJO.png" alt="CPPE-5 Image Example"/>
</div>

ë°”ìš´ë”© ë°•ìŠ¤ì™€ ì—°ê²°ëœ ë ˆì´ë¸”ì„ ì‹œê°í™”í•˜ë ¤ë©´ ë°ì´í„° ì„¸íŠ¸ì˜ ë©”íƒ€ ë°ì´í„°, íŠ¹íˆ `category` í•„ë“œì—ì„œ ë ˆì´ë¸”ì„ ê°€ì ¸ì™€ì•¼ í•©ë‹ˆë‹¤.
ë˜í•œ ë ˆì´ë¸” IDë¥¼ ë ˆì´ë¸” í´ë˜ìŠ¤ì— ë§¤í•‘í•˜ëŠ” `id2label`ê³¼ ë°˜ëŒ€ë¡œ ë§¤í•‘í•˜ëŠ” `label2id` ë”•ì…”ë„ˆë¦¬ë¥¼ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤.
ëª¨ë¸ì„ ì„¤ì •í•  ë•Œ ì´ëŸ¬í•œ ë§¤í•‘ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë§¤í•‘ì€ í—ˆê¹…í˜ì´ìŠ¤ í—ˆë¸Œì—ì„œ ëª¨ë¸ì„ ê³µìœ í–ˆì„ ë•Œ ë‹¤ë¥¸ ì‚¬ëŒë“¤ì´ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë°ì´í„°ë¥¼ ë” ì˜ ì´í•´í•˜ê¸° ìœ„í•œ ìµœì¢… ë‹¨ê³„ë¡œ, ì ì¬ì ì¸ ë¬¸ì œë¥¼ ì°¾ì•„ë³´ì„¸ìš”.
ê°ì²´ ê°ì§€ë¥¼ ìœ„í•œ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œ ì¤‘ í•˜ë‚˜ëŠ” ë°”ìš´ë”© ë°•ìŠ¤ê°€ ì´ë¯¸ì§€ì˜ ê°€ì¥ìë¦¬ë¥¼ ë„˜ì–´ê°€ëŠ” ê²ƒì…ë‹ˆë‹¤.
ì´ëŸ¬í•œ ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ "ë„˜ì–´ê°€ëŠ” ê²ƒ(run away)"ì€ í›ˆë ¨ ì¤‘ì— ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œí‚¬ ìˆ˜ ìˆê¸°ì— ì´ ë‹¨ê³„ì—ì„œ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.
ì´ ë°ì´í„° ì„¸íŠ¸ì—ë„ ê°™ì€ ë¬¸ì œê°€ ìˆëŠ” ëª‡ ê°€ì§€ ì˜ˆê°€ ìˆìŠµë‹ˆë‹¤. ì´ ê°€ì´ë“œì—ì„œëŠ” ê°„ë‹¨í•˜ê²Œí•˜ê¸° ìœ„í•´ ë°ì´í„°ì—ì„œ ì´ëŸ¬í•œ ì´ë¯¸ì§€ë¥¼ ì œê±°í•©ë‹ˆë‹¤.

```py
>>> remove_idx = [590, 821, 822, 875, 876, 878, 879]
>>> keep = [i for i in range(len(cppe5["train"])) if i not in remove_idx]
>>> cppe5["train"] = cppe5["train"].select(keep)
```

## ë°ì´í„° ì „ì²˜ë¦¬í•˜ê¸° [[preprocess-the-data]]

ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì • í•˜ë ¤ë©´, ë¯¸ë¦¬ í•™ìŠµëœ ëª¨ë¸ì—ì„œ ì‚¬ìš©í•œ ì „ì²˜ë¦¬ ë°©ì‹ê³¼ ì •í™•í•˜ê²Œ ì¼ì¹˜í•˜ë„ë¡ ì‚¬ìš©í•  ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.
[`AutoImageProcessor`]ëŠ” ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ì—¬ DETR ëª¨ë¸ì´ í•™ìŠµì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” `pixel_values`, `pixel_mask`, ê·¸ë¦¬ê³  `labels`ë¥¼ ìƒì„±í•˜ëŠ” ì‘ì—…ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
ì´ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œì—ëŠ” ê±±ì •í•˜ì§€ ì•Šì•„ë„ ë˜ëŠ” ëª‡ ê°€ì§€ ì†ì„±ì´ ìˆìŠµë‹ˆë‹¤:

- `image_mean = [0.485, 0.456, 0.406 ]`
- `image_std = [0.229, 0.224, 0.225]`


ì´ ê°’ë“¤ì€ ëª¨ë¸ ì‚¬ì „ í›ˆë ¨ ì¤‘ ì´ë¯¸ì§€ë¥¼ ì •ê·œí™”í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” í‰ê· ê³¼ í‘œì¤€ í¸ì°¨ì…ë‹ˆë‹¤.
ì´ ê°’ë“¤ì€ ì¶”ë¡  ë˜ëŠ” ì‚¬ì „ í›ˆë ¨ëœ ì´ë¯¸ì§€ ëª¨ë¸ì„ ì„¸ë°€í•˜ê²Œ ì¡°ì •í•  ë•Œ ë³µì œí•´ì•¼ í•˜ëŠ” ì¤‘ìš”í•œ ê°’ì…ë‹ˆë‹¤.

ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ê³¼ ë™ì¼í•œ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•©ë‹ˆë‹¤.

```py
>>> from transformers import AutoImageProcessor

>>> checkpoint = "facebook/detr-resnet-50"
>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint)
```

`image_processor`ì— ì´ë¯¸ì§€ë¥¼ ì „ë‹¬í•˜ê¸° ì „ì—, ë°ì´í„° ì„¸íŠ¸ì— ë‘ ê°€ì§€ ì „ì²˜ë¦¬ë¥¼ ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤:

- ì´ë¯¸ì§€ ì¦ê°•
- DETR ëª¨ë¸ì˜ ìš”êµ¬ì— ë§ê²Œ ì–´ë…¸í…Œì´ì…˜ì„ ë‹¤ì‹œ í¬ë§·íŒ…

ì²«ì§¸ë¡œ, ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ì— ê³¼ì í•© ë˜ì§€ ì•Šë„ë¡ ë°ì´í„° ì¦ê°• ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¤‘ ì•„ë¬´ê±°ë‚˜ ì‚¬ìš©í•˜ì—¬ ë³€í™˜ì„ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì—ì„œëŠ” [Albumentations](https://albumentations.ai/docs/) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤...
ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ë³€í™˜ì„ ì´ë¯¸ì§€ì— ì ìš©í•˜ê³  ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ì ì ˆí•˜ê²Œ ì—…ë°ì´íŠ¸í•˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.
ğŸ¤— Datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¬¸ì„œì—ëŠ” [ê°ì²´ íƒì§€ë¥¼ ìœ„í•´ ì´ë¯¸ì§€ë¥¼ ë³´ê°•í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ìì„¸í•œ ê°€ì´ë“œ](https://huggingface.co/docs/datasets/object_detection)ê°€ ìˆìœ¼ë©°,
ì´ ì˜ˆì œì™€ ì •í™•íˆ ë™ì¼í•œ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ê° ì´ë¯¸ì§€ë¥¼ (480, 480) í¬ê¸°ë¡œ ì¡°ì •í•˜ê³ , ì¢Œìš°ë¡œ ë’¤ì§‘ê³ , ë°ê¸°ë¥¼ ë†’ì´ëŠ” ë™ì¼í•œ ì ‘ê·¼ë²•ì„ ì ìš©í•©ë‹ˆë‹¤:


```py
>>> import albumentations
>>> import numpy as np
>>> import torch

>>> transform = albumentations.Compose(
...     [
...         albumentations.Resize(480, 480),
...         albumentations.HorizontalFlip(p=1.0),
...         albumentations.RandomBrightnessContrast(p=1.0),
...     ],
...     bbox_params=albumentations.BboxParams(format="coco", label_fields=["category"]),
... )
```

ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œëŠ” ì–´ë…¸í…Œì´ì…˜ì´ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ì¼ ê²ƒìœ¼ë¡œ ì˜ˆìƒí•©ë‹ˆë‹¤: `{'image_id': int, 'annotations': list[Dict]}`, ì—¬ê¸°ì„œ ê° ë”•ì…”ë„ˆë¦¬ëŠ” COCO ê°ì²´ ì–´ë…¸í…Œì´ì…˜ì…ë‹ˆë‹¤. ë‹¨ì¼ ì˜ˆì œì— ëŒ€í•´ ì–´ë…¸í…Œì´ì…˜ì˜ í˜•ì‹ì„ ë‹¤ì‹œ ì§€ì •í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì¶”ê°€í•´ ë³´ê² ìŠµë‹ˆë‹¤:

```py
>>> def formatted_anns(image_id, category, area, bbox):
...     annotations = []
...     for i in range(0, len(category)):
...         new_ann = {
...             "image_id": image_id,
...             "category_id": category[i],
...             "isCrowd": 0,
...             "area": area[i],
...             "bbox": list(bbox[i]),
...         }
...         annotations.append(new_ann)

...     return annotations
```

ì´ì œ ì´ë¯¸ì§€ì™€ ì–´ë…¸í…Œì´ì…˜ ì „ì²˜ë¦¬ ë³€í™˜ì„ ê²°í•©í•˜ì—¬ ì˜ˆì œ ë°°ì¹˜ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
>>> # transforming a batch
>>> def transform_aug_ann(examples):
...     image_ids = examples["image_id"]
...     images, bboxes, area, categories = [], [], [], []
...     for image, objects in zip(examples["image"], examples["objects"]):
...         image = np.array(image.convert("RGB"))[:, :, ::-1]
...         out = transform(image=image, bboxes=objects["bbox"], category=objects["category"])

...         area.append(objects["area"])
...         images.append(out["image"])
...         bboxes.append(out["bboxes"])
...         categories.append(out["category"])

...     targets = [
...         {"image_id": id_, "annotations": formatted_anns(id_, cat_, ar_, box_)}
...         for id_, cat_, ar_, box_ in zip(image_ids, categories, area, bboxes)
...     ]

...     return image_processor(images=images, annotations=targets, return_tensors="pt")
```

ì´ì „ ë‹¨ê³„ì—ì„œ ë§Œë“  ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ğŸ¤— Datasetsì˜ [`~datasets.Dataset.with_transform`] ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì„¸íŠ¸ ì „ì²´ì— ì ìš©í•©ë‹ˆë‹¤.
ì´ ë©”ì†Œë“œëŠ” ë°ì´í„° ì„¸íŠ¸ì˜ ìš”ì†Œë¥¼ ê°€ì ¸ì˜¬ ë•Œë§ˆë‹¤ ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ì ìš©í•©ë‹ˆë‹¤.

ì´ ì‹œì ì—ì„œëŠ” ì „ì²˜ë¦¬ í›„ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ì˜ˆì‹œ í•˜ë‚˜ë¥¼ ê°€ì ¸ì™€ì„œ ë³€í™˜ í›„ ëª¨ì–‘ì´ ì–´ë–»ê²Œ ë˜ëŠ”ì§€ í™•ì¸í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì´ë•Œ, `pixel_values` í…ì„œ, `pixel_mask` í…ì„œ, ê·¸ë¦¬ê³  `labels`ë¡œ êµ¬ì„±ëœ í…ì„œê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

```py
>>> cppe5["train"] = cppe5["train"].with_transform(transform_aug_ann)
>>> cppe5["train"][15]
{'pixel_values': tensor([[[ 0.9132,  0.9132,  0.9132,  ..., -1.9809, -1.9809, -1.9809],
          [ 0.9132,  0.9132,  0.9132,  ..., -1.9809, -1.9809, -1.9809],
          [ 0.9132,  0.9132,  0.9132,  ..., -1.9638, -1.9638, -1.9638],
          ...,
          [-1.5699, -1.5699, -1.5699,  ..., -1.9980, -1.9980, -1.9980],
          [-1.5528, -1.5528, -1.5528,  ..., -1.9980, -1.9809, -1.9809],
          [-1.5528, -1.5528, -1.5528,  ..., -1.9980, -1.9809, -1.9809]],

         [[ 1.3081,  1.3081,  1.3081,  ..., -1.8431, -1.8431, -1.8431],
          [ 1.3081,  1.3081,  1.3081,  ..., -1.8431, -1.8431, -1.8431],
          [ 1.3081,  1.3081,  1.3081,  ..., -1.8256, -1.8256, -1.8256],
          ...,
          [-1.3179, -1.3179, -1.3179,  ..., -1.8606, -1.8606, -1.8606],
          [-1.3004, -1.3004, -1.3004,  ..., -1.8606, -1.8431, -1.8431],
          [-1.3004, -1.3004, -1.3004,  ..., -1.8606, -1.8431, -1.8431]],

         [[ 1.4200,  1.4200,  1.4200,  ..., -1.6476, -1.6476, -1.6476],
          [ 1.4200,  1.4200,  1.4200,  ..., -1.6476, -1.6476, -1.6476],
          [ 1.4200,  1.4200,  1.4200,  ..., -1.6302, -1.6302, -1.6302],
          ...,
          [-1.0201, -1.0201, -1.0201,  ..., -1.5604, -1.5604, -1.5604],
          [-1.0027, -1.0027, -1.0027,  ..., -1.5604, -1.5430, -1.5430],
          [-1.0027, -1.0027, -1.0027,  ..., -1.5604, -1.5430, -1.5430]]]),
 'pixel_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         ...,
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1]]),
 'labels': {'size': tensor([800, 800]), 'image_id': tensor([756]), 'class_labels': tensor([4]), 'boxes': tensor([[0.7340, 0.6986, 0.3414, 0.5944]]), 'area': tensor([519544.4375]), 'iscrowd': tensor([0]), 'orig_size': tensor([480, 480])}}
```

ê°ê°ì˜ ì´ë¯¸ì§€ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì¦ê°•í•˜ê³  ì´ë¯¸ì§€ì˜ ì–´ë…¸í…Œì´ì…˜ì„ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤.
ê·¸ëŸ¬ë‚˜ ì „ì²˜ë¦¬ëŠ” ì•„ì§ ëë‚˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ ë‹¨ê³„ë¡œ, ì´ë¯¸ì§€ë¥¼ ë°°ì¹˜ë¡œ ë§Œë“¤ ì‚¬ìš©ì ì •ì˜ `collate_fn`ì„ ìƒì„±í•©ë‹ˆë‹¤.
í•´ë‹¹ ë°°ì¹˜ì—ì„œ ê°€ì¥ í° ì´ë¯¸ì§€ì— ì´ë¯¸ì§€(í˜„ì¬ `pixel_values` ì¸)ë¥¼ íŒ¨ë“œí•˜ê³ , ì‹¤ì œ í”½ì…€(1)ê³¼ íŒ¨ë”©(0)ì„ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´ ê·¸ì— í•´ë‹¹í•˜ëŠ” ìƒˆë¡œìš´ `pixel_mask`ë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.

```py
>>> def collate_fn(batch):
...     pixel_values = [item["pixel_values"] for item in batch]
...     encoding = image_processor.pad(pixel_values, return_tensors="pt")
...     labels = [item["labels"] for item in batch]
...     batch = {}
...     batch["pixel_values"] = encoding["pixel_values"]
...     batch["pixel_mask"] = encoding["pixel_mask"]
...     batch["labels"] = labels
...     return batch
```

## DETR ëª¨ë¸ í•™ìŠµì‹œí‚¤ê¸° [[training-the-DETR-model]]

ì´ì „ ì„¹ì…˜ì—ì„œ ëŒ€ë¶€ë¶„ì˜ ì‘ì—…ì„ ìˆ˜í–‰í•˜ì—¬ ì´ì œ ëª¨ë¸ì„ í•™ìŠµí•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤!
ì´ ë°ì´í„° ì„¸íŠ¸ì˜ ì´ë¯¸ì§€ëŠ” ë¦¬ì‚¬ì´ì¦ˆ í›„ì—ë„ ì—¬ì „íˆ ìš©ëŸ‰ì´ í¬ê¸° ë•Œë¬¸ì—, ì´ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì • í•˜ë ¤ë©´ ì ì–´ë„ í•˜ë‚˜ì˜ GPUê°€ í•„ìš”í•©ë‹ˆë‹¤.

í•™ìŠµì€ ë‹¤ìŒì˜ ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤:

1. [`AutoModelForObjectDetection`]ì„ ì‚¬ìš©í•˜ì—¬ ì „ì²˜ë¦¬ì™€ ë™ì¼í•œ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
2. [`TrainingArguments`]ì—ì„œ í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
3. ëª¨ë¸, ë°ì´í„° ì„¸íŠ¸, ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ ë° ë°ì´í„° ì½œë ˆì´í„°ì™€ í•¨ê»˜ [`Trainer`]ì— í›ˆë ¨ ì¸ìˆ˜ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.
4. [`~Trainer.train`]ë¥¼ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì • í•©ë‹ˆë‹¤.

ì „ì²˜ë¦¬ì— ì‚¬ìš©í•œ ì²´í¬í¬ì¸íŠ¸ì™€ ë™ì¼í•œ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ì„ ê°€ì ¸ì˜¬ ë•Œ, ë°ì´í„° ì„¸íŠ¸ì˜ ë©”íƒ€ë°ì´í„°ì—ì„œ ë§Œë“  `label2id`ì™€ `id2label` ë§¤í•‘ì„ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤.
ë˜í•œ, `ignore_mismatched_sizes=True`ë¥¼ ì§€ì •í•˜ì—¬ ê¸°ì¡´ ë¶„ë¥˜ í—¤ë“œ(ëª¨ë¸ì—ì„œ ë¶„ë¥˜ì— ì‚¬ìš©ë˜ëŠ” ë§ˆì§€ë§‰ ë ˆì´ì–´)ë¥¼ ìƒˆ ë¶„ë¥˜ í—¤ë“œë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.

```py
>>> from transformers import AutoModelForObjectDetection

>>> model = AutoModelForObjectDetection.from_pretrained(
...     checkpoint,
...     id2label=id2label,
...     label2id=label2id,
...     ignore_mismatched_sizes=True,
... )
```

[`TrainingArguments`]ì—ì„œ `output_dir`ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì €ì¥í•  ìœ„ì¹˜ë¥¼ ì§€ì •í•œ ë‹¤ìŒ, í•„ìš”ì— ë”°ë¼ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ êµ¬ì„±í•˜ì„¸ìš”.
ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì—´ì„ ì œê±°í•˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤. ë§Œì•½ `remove_unused_columns`ê°€ `True`ì¼ ê²½ìš° ì´ë¯¸ì§€ ì—´ì´ ì‚­ì œë©ë‹ˆë‹¤.
ì´ë¯¸ì§€ ì—´ì´ ì—†ëŠ” ê²½ìš° `pixel_values`ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— `remove_unused_columns`ë¥¼ `False`ë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.
ëª¨ë¸ì„ Hubì— ì—…ë¡œë“œí•˜ì—¬ ê³µìœ í•˜ë ¤ë©´ `push_to_hub`ë¥¼ `True`ë¡œ ì„¤ì •í•˜ì‹­ì‹œì˜¤(í—ˆê¹…í˜ì´ìŠ¤ì— ë¡œê·¸ì¸í•˜ì—¬ ëª¨ë¸ì„ ì—…ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤).


```py
>>> from transformers import TrainingArguments

>>> training_args = TrainingArguments(
...     output_dir="detr-resnet-50_finetuned_cppe5",
...     per_device_train_batch_size=8,
...     num_train_epochs=10,
...     fp16=True,
...     save_steps=200,
...     logging_steps=50,
...     learning_rate=1e-5,
...     weight_decay=1e-4,
...     save_total_limit=2,
...     remove_unused_columns=False,
...     push_to_hub=True,
... )
```

ë§ˆì§€ë§‰ìœ¼ë¡œ `model`, `training_args`, `collate_fn`, `image_processor`ì™€ ë°ì´í„° ì„¸íŠ¸(`cppe5`)ë¥¼ ëª¨ë‘ ê°€ì ¸ì˜¨ í›„, [`~transformers.Trainer.train`]ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.

```py
>>> from transformers import Trainer

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     data_collator=collate_fn,
...     train_dataset=cppe5["train"],
...     processing_class=image_processor,
... )

>>> trainer.train()
```

`training_args`ì—ì„œ `push_to_hub`ë¥¼ `True`ë¡œ ì„¤ì •í•œ ê²½ìš°, í•™ìŠµ ì²´í¬í¬ì¸íŠ¸ëŠ” í—ˆê¹…í˜ì´ìŠ¤ í—ˆë¸Œì— ì—…ë¡œë“œë©ë‹ˆë‹¤.
í•™ìŠµ ì™„ë£Œ í›„, [`~transformers.Trainer.push_to_hub`] ë©”ì†Œë“œë¥¼ í˜¸ì¶œí•˜ì—¬ ìµœì¢… ëª¨ë¸ì„ í—ˆê¹…í˜ì´ìŠ¤ í—ˆë¸Œì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.

```py
>>> trainer.push_to_hub()
```

## í‰ê°€í•˜ê¸° [[evaluate]]

ê°ì²´ íƒì§€ ëª¨ë¸ì€ ì¼ë°˜ì ìœ¼ë¡œ ì¼ë ¨ì˜ <a href="https://cocodataset.org/#detection-eval">COCO-ìŠ¤íƒ€ì¼ ì§€í‘œ</a>ë¡œ í‰ê°€ë©ë‹ˆë‹¤.
ê¸°ì¡´ì— êµ¬í˜„ëœ í‰ê°€ ì§€í‘œ ì¤‘ í•˜ë‚˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆì§€ë§Œ, ì—¬ê¸°ì—ì„œëŠ” í—ˆê¹…í˜ì´ìŠ¤ í—ˆë¸Œì— í‘¸ì‹œí•œ ìµœì¢… ëª¨ë¸ì„ í‰ê°€í•˜ëŠ” ë° `torchvision`ì—ì„œ ì œê³µí•˜ëŠ” í‰ê°€ ì§€í‘œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

`torchvision` í‰ê°€ì(evaluator)ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ì‹¤ì¸¡ê°’ì¸ COCO ë°ì´í„° ì„¸íŠ¸ë¥¼ ì¤€ë¹„í•´ì•¼ í•©ë‹ˆë‹¤.
COCO ë°ì´í„° ì„¸íŠ¸ë¥¼ ë¹Œë“œí•˜ëŠ” APIëŠ” ë°ì´í„°ë¥¼ íŠ¹ì • í˜•ì‹ìœ¼ë¡œ ì €ì¥í•´ì•¼ í•˜ë¯€ë¡œ, ë¨¼ì € ì´ë¯¸ì§€ì™€ ì–´ë…¸í…Œì´ì…˜ì„ ë””ìŠ¤í¬ì— ì €ì¥í•´ì•¼ í•©ë‹ˆë‹¤.
í•™ìŠµì„ ìœ„í•´ ë°ì´í„°ë¥¼ ì¤€ë¹„í•  ë•Œì™€ ë§ˆì°¬ê°€ì§€ë¡œ, cppe5["test"]ì—ì„œì˜ ì–´ë…¸í…Œì´ì…˜ì€ í¬ë§·ì„ ë§ì¶°ì•¼ í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ë¯¸ì§€ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•´ì•¼ í•©ë‹ˆë‹¤.

í‰ê°€ ë‹¨ê³„ëŠ” ì•½ê°„ì˜ ì‘ì—…ì´ í•„ìš”í•˜ì§€ë§Œ, í¬ê²Œ ì„¸ ê°€ì§€ ì£¼ìš” ë‹¨ê³„ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ë¨¼ì €, `cppe5["test"]` ì„¸íŠ¸ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤: ì–´ë…¸í…Œì´ì…˜ì„ í¬ë§·ì— ë§ê²Œ ë§Œë“¤ê³  ë°ì´í„°ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥í•©ë‹ˆë‹¤.

```py
>>> import json


>>> # format annotations the same as for training, no need for data augmentation
>>> def val_formatted_anns(image_id, objects):
...     annotations = []
...     for i in range(0, len(objects["id"])):
...         new_ann = {
...             "id": objects["id"][i],
...             "category_id": objects["category"][i],
...             "iscrowd": 0,
...             "image_id": image_id,
...             "area": objects["area"][i],
...             "bbox": objects["bbox"][i],
...         }
...         annotations.append(new_ann)

...     return annotations


>>> # Save images and annotations into the files torchvision.datasets.CocoDetection expects
>>> def save_cppe5_annotation_file_images(cppe5):
...     output_json = {}
...     path_output_cppe5 = f"{os.getcwd()}/cppe5/"

...     if not os.path.exists(path_output_cppe5):
...         os.makedirs(path_output_cppe5)

...     path_anno = os.path.join(path_output_cppe5, "cppe5_ann.json")
...     categories_json = [{"supercategory": "none", "id": id, "name": id2label[id]} for id in id2label]
...     output_json["images"] = []
...     output_json["annotations"] = []
...     for example in cppe5:
...         ann = val_formatted_anns(example["image_id"], example["objects"])
...         output_json["images"].append(
...             {
...                 "id": example["image_id"],
...                 "width": example["image"].width,
...                 "height": example["image"].height,
...                 "file_name": f"{example['image_id']}.png",
...             }
...         )
...         output_json["annotations"].extend(ann)
...     output_json["categories"] = categories_json

...     with open(path_anno, "w") as file:
...         json.dump(output_json, file, ensure_ascii=False, indent=4)

...     for im, img_id in zip(cppe5["image"], cppe5["image_id"]):
...         path_img = os.path.join(path_output_cppe5, f"{img_id}.png")
...         im.save(path_img)

...     return path_output_cppe5, path_anno
```

ë‹¤ìŒìœ¼ë¡œ, `cocoevaluator`ì™€ í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” `CocoDetection` í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.

```py
>>> import torchvision


>>> class CocoDetection(torchvision.datasets.CocoDetection):
...     def __init__(self, img_folder, image_processor, ann_file):
...         super().__init__(img_folder, ann_file)
...         self.image_processor = image_processor

...     def __getitem__(self, idx):
...         # read in PIL image and target in COCO format
...         img, target = super(CocoDetection, self).__getitem__(idx)

...         # preprocess image and target: converting target to DETR format,
...         # resizing + normalization of both image and target)
...         image_id = self.ids[idx]
...         target = {"image_id": image_id, "annotations": target}
...         encoding = self.image_processor(images=img, annotations=target, return_tensors="pt")
...         pixel_values = encoding["pixel_values"].squeeze()  # remove batch dimension
...         target = encoding["labels"][0]  # remove batch dimension

...         return {"pixel_values": pixel_values, "labels": target}


>>> im_processor = AutoImageProcessor.from_pretrained("devonho/detr-resnet-50_finetuned_cppe5")

>>> path_output_cppe5, path_anno = save_cppe5_annotation_file_images(cppe5["test"])
>>> test_ds_coco_format = CocoDetection(path_output_cppe5, im_processor, path_anno)
```

ë§ˆì§€ë§‰ìœ¼ë¡œ, í‰ê°€ ì§€í‘œë¥¼ ê°€ì ¸ì™€ì„œ í‰ê°€ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

```py
>>> import evaluate
>>> from tqdm import tqdm

>>> model = AutoModelForObjectDetection.from_pretrained("devonho/detr-resnet-50_finetuned_cppe5")
>>> module = evaluate.load("ybelkada/cocoevaluate", coco=test_ds_coco_format.coco)
>>> val_dataloader = torch.utils.data.DataLoader(
...     test_ds_coco_format, batch_size=8, shuffle=False, num_workers=4, collate_fn=collate_fn
... )

>>> with torch.no_grad():
...     for idx, batch in enumerate(tqdm(val_dataloader)):
...         pixel_values = batch["pixel_values"]
...         pixel_mask = batch["pixel_mask"]

...         labels = [
...             {k: v for k, v in t.items()} for t in batch["labels"]
...         ]  # these are in DETR format, resized + normalized

...         # forward pass
...         outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)

...         orig_target_sizes = torch.stack([target["orig_size"] for target in labels], dim=0)
...         results = im_processor.post_process(outputs, orig_target_sizes)  # convert outputs of model to Pascal VOC format (xmin, ymin, xmax, ymax)

...         module.add(prediction=results, reference=labels)
...         del batch

>>> results = module.compute()
>>> print(results)
Accumulating evaluation results...
DONE (t=0.08s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.352
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.681
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.292
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.168
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.208
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.429
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.274
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.484
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.501
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.191
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.323
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.590
```

ì´ëŸ¬í•œ ê²°ê³¼ëŠ” [`~transformers.TrainingArguments`]ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ì—¬ ë”ìš± ê°œì„ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•œë²ˆ ì‹œë„í•´ ë³´ì„¸ìš”!

## ì¶”ë¡ í•˜ê¸° [[inference]]

DETR ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì • ë° í‰ê°€í•˜ê³ , í—ˆê¹…í˜ì´ìŠ¤ í—ˆë¸Œì— ì—…ë¡œë“œ í–ˆìœ¼ë¯€ë¡œ ì¶”ë¡ ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì„ ì¶”ë¡ ì— ì‚¬ìš©í•˜ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì€ [`pipeline`]ì—ì„œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
ëª¨ë¸ê³¼ í•¨ê»˜ ê°ì²´ íƒì§€ë¥¼ ìœ„í•œ íŒŒì´í”„ë¼ì¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ê³ , ì´ë¯¸ì§€ë¥¼ ì „ë‹¬í•˜ì„¸ìš”:

```py
>>> from transformers import pipeline
>>> import requests

>>> url = "https://i.imgur.com/2lnWoly.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> obj_detector = pipeline("object-detection", model="devonho/detr-resnet-50_finetuned_cppe5")
>>> obj_detector(image)
```

ë§Œì•½ ì›í•œë‹¤ë©´ ìˆ˜ë™ìœ¼ë¡œ `pipeline`ì˜ ê²°ê³¼ë¥¼ ì¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
>>> image_processor = AutoImageProcessor.from_pretrained("devonho/detr-resnet-50_finetuned_cppe5")
>>> model = AutoModelForObjectDetection.from_pretrained("devonho/detr-resnet-50_finetuned_cppe5")

>>> with torch.no_grad():
...     inputs = image_processor(images=image, return_tensors="pt")
...     outputs = model(**inputs)
...     target_sizes = torch.tensor([image.size[::-1]])
...     results = image_processor.post_process_object_detection(outputs, threshold=0.5, target_sizes=target_sizes)[0]

>>> for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
...     box = [round(i, 2) for i in box.tolist()]
...     print(
...         f"Detected {model.config.id2label[label.item()]} with confidence "
...         f"{round(score.item(), 3)} at location {box}"
...     )
Detected Coverall with confidence 0.566 at location [1215.32, 147.38, 4401.81, 3227.08]
Detected Mask with confidence 0.584 at location [2449.06, 823.19, 3256.43, 1413.9]
```

ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ê² ìŠµë‹ˆë‹¤:
```py
>>> draw = ImageDraw.Draw(image)

>>> for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
...     box = [round(i, 2) for i in box.tolist()]
...     x, y, x2, y2 = tuple(box)
...     draw.rectangle((x, y, x2, y2), outline="red", width=1)
...     draw.text((x, y), model.config.id2label[label.item()], fill="white")

>>> image
```

<div class="flex justify-center">
    <img src="https://i.imgur.com/4QZnf9A.png" alt="Object detection result on a new image"/>
</div>
