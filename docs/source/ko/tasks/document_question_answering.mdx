<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# ë¬¸ì„œ ì§ˆì˜ ì‘ë‹µ(Document Question Answering)[[document_question_answering]]

[[open-in-colab]]

ë¬¸ì„œ ì‹œê°ì  ì§ˆì˜ ì‘ë‹µ(Document Visual Question Answering)ì´ë¼ê³ ë„ í•˜ëŠ” ë¬¸ì„œ ì§ˆì˜ ì‘ë‹µ(Document Question Answering)ì€ ë¬¸ì„œ ì´ë¯¸ì§€ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€ì„ ì£¼ëŠ” íƒœìŠ¤í¬ì…ë‹ˆë‹¤. 
ì´ íƒœìŠ¤í¬ë¥¼ ì§€ì›í•˜ëŠ” ëª¨ë¸ì˜ ì…ë ¥ì€ ì¼ë°˜ì ìœ¼ë¡œ ì´ë¯¸ì§€ì™€ ì§ˆë¬¸ì˜ ì¡°í•©ì´ê³ , ì¶œë ¥ì€ ìì—°ì–´ë¡œ ëœ ë‹µë³€ì…ë‹ˆë‹¤. ì´ íƒœìŠ¤í¬ë¥¼ ì§€ì›í•˜ëŠ” ëª¨ë¸ì— ëŒ€í•œ ì…ë ¥ì€ ì¼ë°˜ì ìœ¼ë¡œ ì´ë¯¸ì§€ì™€ ì§ˆë¬¸ì˜ ì¡°í•©ì´ë©°, ì¶œë ¥ì€ ìì—°ì–´ë¡œ í‘œí˜„ëœ ë‹µë³€ì…ë‹ˆë‹¤. 
ì´ëŸ¬í•œ ëª¨ë¸ì€ í…ìŠ¤íŠ¸, ë‹¨ì–´ì˜ ìœ„ì¹˜(ë°”ìš´ë”© ë°•ìŠ¤), ì´ë¯¸ì§€ ë“± ë‹¤ì–‘í•œ ëª¨ë‹¬ë¦¬í‹°ë¥¼ í™œìš©í•©ë‹ˆë‹¤.

ì´ ê°€ì´ë“œëŠ” ë‹¤ìŒ ë‚´ìš©ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

- [DocVQA dataset](https://huggingface.co/datasets/nielsr/docvqa_1200_examples_donut)ì„ ì‚¬ìš©í•´ [LayoutLMv2](../model_doc/layoutlmv2) ë¯¸ì„¸ ì¡°ì •í•˜ê¸°
- ì¶”ë¡ ì„ ìœ„í•´ ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸°

<Tip>

ì´ íŠœí† ë¦¬ì–¼ì—ì„œ ì„¤ëª…í•˜ëŠ” íƒœìŠ¤í¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ëª¨ë¸ ì•„í‚¤í…ì²˜ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤:

<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->

[LayoutLM](../model_doc/layoutlm), [LayoutLMv2](../model_doc/layoutlmv2), [LayoutLMv3](../model_doc/layoutlmv3)

<!--End of the generated tip-->

</Tip>

LayoutLMv2ëŠ” í† í°ì˜ ë§ˆì§€ë§‰ ì€ë‹‰ì¸µ ìœ„ì— ì§ˆì˜ ì‘ë‹µ í—¤ë“œë¥¼ ì¶”ê°€í•˜ì—¬ ë‹µë³€ì˜ ì‹œì‘ í† í°ê³¼ ë í† í°ì˜ ìœ„ì¹˜ë¥¼ ì˜ˆì¸¡í•¨ìœ¼ë¡œì¨ ë¬¸ì„œ ì§ˆì˜ ì‘ë‹µ íƒœìŠ¤í¬ë¥¼ í•´ê²°í•©ë‹ˆë‹¤.
ì¦‰, ë¬¸ë§¥ì´ ì£¼ì–´ì¡Œì„ ë•Œ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì¶”ì¶œí˜• ì§ˆì˜ ì‘ë‹µ(Extractive question answering)ìœ¼ë¡œ ë¬¸ì œë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
ë¬¸ë§¥ì€ OCR ì—”ì§„ì˜ ì¶œë ¥ì—ì„œ ë¹„ë¡¯ë˜ë©°, ì—¬ê¸°ì„œëŠ” Googleì˜ í…Œì„œë™íŠ¸(Tesseract)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

ì‹œì‘í•˜ê¸° ì „ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ëª¨ë‘ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. LayoutLMv2ëŠ” detectron2, torchvision ë° í…Œì„œë™íŠ¸ë¥¼ í•„ìš”ë¡œ í•©ë‹ˆë‹¤.

```bash
pip install -q transformers datasets
```

```bash
pip install 'git+https://github.com/facebookresearch/detectron2.git'
pip install torchvision
```

```bash
sudo apt install tesseract-ocr
pip install -q pytesseract
```

í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ëª¨ë‘ ì„¤ì¹˜í•œ í›„ ëŸ°íƒ€ì„ì„ ë‹¤ì‹œ ì‹œì‘í•©ë‹ˆë‹¤.

ì»¤ë®¤ë‹ˆí‹°ì— ë‹¹ì‹ ì˜ ëª¨ë¸ì„ ê³µìœ í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤. Hugging Face ê³„ì •ì— ë¡œê·¸ì¸í•´ì„œ ëª¨ë¸ì„ ğŸ¤— Hubì— ì—…ë¡œë“œí•˜ì„¸ìš”.
í”„ë¡¬í”„íŠ¸ê°€ ì‹¤í–‰ë˜ë©´, ë¡œê·¸ì¸ì„ ìœ„í•´ í† í°ì„ ì…ë ¥í•˜ì„¸ìš”:

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

ëª‡ ê°€ì§€ ì „ì—­ ë³€ìˆ˜ë¥¼ ì •ì˜í•´ ë³´ê² ìŠµë‹ˆë‹¤.

```py
>>> model_checkpoint = "microsoft/layoutlmv2-base-uncased"
>>> batch_size = 4
```

## ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° [[load-the-data]]

ì´ ê°€ì´ë“œì—ì„œëŠ” ğŸ¤— Hubì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” ì‚¬ì „ ì²˜ë¦¬ëœ DocVQAì˜ ì‘ì€ ìƒ˜í”Œì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
DocVQAì˜ ì „ì²´ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ê³  ì‹¶ë‹¤ë©´, [DocVQA homepage](https://rrc.cvc.uab.es/?ch=17)ì—ì„œ ê°€ì… í›„ ë‹¤ìš´ë¡œë“œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì „ì²´ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œ í–ˆë‹¤ë©´, ì´ ê°€ì´ë“œë¥¼ ê³„ì† ì§„í–‰í•˜ê¸° ìœ„í•´ [ğŸ¤— datasetì— íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” ë°©ë²•](https://huggingface.co/docs/datasets/loading#local-and-remote-files)ì„ í™•ì¸í•˜ì„¸ìš”.

```py
>>> from datasets import load_dataset

>>> dataset = load_dataset("nielsr/docvqa_1200_examples")
>>> dataset
DatasetDict({
    train: Dataset({
        features: ['id', 'image', 'query', 'answers', 'words', 'bounding_boxes', 'answer'],
        num_rows: 1000
    })
    test: Dataset({
        features: ['id', 'image', 'query', 'answers', 'words', 'bounding_boxes', 'answer'],
        num_rows: 200
    })
})
```

ë³´ì‹œë‹¤ì‹œí”¼, ë°ì´í„° ì„¸íŠ¸ëŠ” ì´ë¯¸ í›ˆë ¨ ì„¸íŠ¸ì™€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë‚˜ëˆ„ì–´ì ¸ ìˆìŠµë‹ˆë‹¤. ë¬´ì‘ìœ„ë¡œ ì˜ˆì œë¥¼ ì‚´í´ë³´ë©´ì„œ íŠ¹ì„±ì„ í™•ì¸í•´ë³´ì„¸ìš”.

```py
>>> dataset["train"].features
```

ê° í•„ë“œê°€ ë‚˜íƒ€ë‚´ëŠ” ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
* `id`: ì˜ˆì œì˜ id
* `image`: ë¬¸ì„œ ì´ë¯¸ì§€ë¥¼ í¬í•¨í•˜ëŠ” PIL.Image.Image ê°ì²´
* `query`: ì§ˆë¬¸ ë¬¸ìì—´ - ì—¬ëŸ¬ ì–¸ì–´ì˜ ìì—°ì–´ë¡œ ëœ ì§ˆë¬¸
* `answers`: ì‚¬ëŒì´ ì£¼ì„ì„ ë‹¨ ì •ë‹µ ë¦¬ìŠ¤íŠ¸
* `words` and `bounding_boxes`: OCRì˜ ê²°ê³¼ê°’ë“¤ì´ë©° ì´ ê°€ì´ë“œì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•Šì„ ì˜ˆì •
* `answer`: ë‹¤ë¥¸ ëª¨ë¸ê³¼ ì¼ì¹˜í•˜ëŠ” ë‹µë³€ì´ë©° ì´ ê°€ì´ë“œì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•Šì„ ì˜ˆì •

ì˜ì–´ë¡œ ëœ ì§ˆë¬¸ë§Œ ë‚¨ê¸°ê³  ë‹¤ë¥¸ ëª¨ë¸ì— ëŒ€í•œ ì˜ˆì¸¡ì„ í¬í•¨í•˜ëŠ” `answer` íŠ¹ì„±ì„ ì‚­ì œí•˜ê² ìŠµë‹ˆë‹¤.
ê·¸ë¦¬ê³  ì£¼ì„ ì‘ì„±ìê°€ ì œê³µí•œ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ì²« ë²ˆì§¸ ë‹µë³€ì„ ê°€ì ¸ì˜µë‹ˆë‹¤. ë˜ëŠ” ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œì„ ì¶”ì¶œí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

```py
>>> updated_dataset = dataset.map(lambda example: {"question": example["query"]["en"]}, remove_columns=["query"])
>>> updated_dataset = updated_dataset.map(
...     lambda example: {"answer": example["answers"][0]}, remove_columns=["answer", "answers"]
... )
```

ì´ ê°€ì´ë“œì—ì„œ ì‚¬ìš©í•˜ëŠ” LayoutLMv2 ì²´í¬í¬ì¸íŠ¸ëŠ” `max_position_embeddings = 512`ë¡œ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤.(ì´ ì •ë³´ëŠ” [checkpoint's `config.json` file](https://huggingface.co/microsoft/layoutlmv2-base-uncased/blob/main/config.json#L18)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.)
ë°”ë¡œ ì˜ˆì œë¥¼ ì˜ë¼ë‚¼ ìˆ˜ë„ ìˆì§€ë§Œ, ê¸´ ë¬¸ì„œì˜ ëì— ë‹µë³€ì´ ìˆì–´ ì˜ë¦¬ëŠ” ìƒí™©ì„ í”¼í•˜ê¸° ìœ„í•´ ì—¬ê¸°ì„œëŠ” ì„ë² ë”©ì´ 512ë³´ë‹¤ ê¸¸ì–´ì§ˆ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ëª‡ ê°€ì§€ ì˜ˆì œë¥¼ ì œê±°í•˜ê² ìŠµë‹ˆë‹¤.
ë°ì´í„° ì„¸íŠ¸ì— ìˆëŠ” ëŒ€ë¶€ë¶„ì˜ ë¬¸ì„œê°€ ê¸´ ê²½ìš° ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ - ìì„¸í•œ ë‚´ìš©ì„ í™•ì¸í•˜ê³  ì‹¶ìœ¼ë©´ ì´ [notebook](https://github.com/huggingface/notebooks/blob/main/examples/question_answering.ipynb)ì„ í™•ì¸í•˜ì„¸ìš”.

```py
>>> updated_dataset = updated_dataset.filter(lambda x: len(x["words"]) + len(x["question"].split()) < 512)
```

ì´ ì‹œì ì—ì„œ ì´ ë°ì´í„° ì„¸íŠ¸ì˜ OCR íŠ¹ì„±ë„ ì œê±°í•´ ë³´ê² ìŠµë‹ˆë‹¤. OCR íŠ¹ì„±ì€ ë‹¤ë¥¸ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ê¸° ìœ„í•œ ê²ƒìœ¼ë¡œ, ì´ ê°€ì´ë“œì—ì„œ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì˜ ì…ë ¥ ìš”êµ¬ ì‚¬í•­ê³¼ ì¼ì¹˜í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì´ íŠ¹ì„±ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ì¼ë¶€ ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. 
ëŒ€ì‹ , ì›ë³¸ ë°ì´í„°ì— [`LayoutLMv2Processor`]ë¥¼ ì‚¬ìš©í•˜ì—¬ OCR ë° í† í°í™”ë¥¼ ëª¨ë‘ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ëª¨ë¸ì´ ìš”êµ¬í•˜ëŠ” ì…ë ¥ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
ì´ë¯¸ì§€ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ë ¤ë©´, [`LayoutLMv2` model documentation](../model_doc/layoutlmv2)ì—ì„œ ëª¨ë¸ì´ ìš”êµ¬í•˜ëŠ” ì…ë ¥ í¬ë§·ì„ í™•ì¸í•´ë³´ì„¸ìš”.

```py
>>> updated_dataset = updated_dataset.remove_columns("words")
>>> updated_dataset = updated_dataset.remove_columns("bounding_boxes")
```

ë§ˆì§€ë§‰ìœ¼ë¡œ, ë°ì´í„° íƒìƒ‰ì„ ì™„ë£Œí•˜ê¸° ìœ„í•´ ì´ë¯¸ì§€ ì˜ˆì‹œë¥¼ ì‚´í´ë´…ì‹œë‹¤.

```py
>>> updated_dataset["train"][11]["image"]
```

<div class="flex justify-center">
     <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/docvqa_example.jpg" alt="DocVQA Image Example"/>
 </div>

## ë°ì´í„° ì „ì²˜ë¦¬ [[preprocess-the-data]]

ë¬¸ì„œ ì§ˆì˜ ì‘ë‹µ íƒœìŠ¤í¬ëŠ” ë©€í‹°ëª¨ë‹¬ íƒœìŠ¤í¬ì´ë©°, ê° ëª¨ë‹¬ë¦¬í‹°ì˜ ì…ë ¥ì´ ëª¨ë¸ì˜ ìš”êµ¬ì— ë§ê²Œ ì „ì²˜ë¦¬ ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.
ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œì™€ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì¸ì½”ë”©í•  ìˆ˜ ìˆëŠ” í† í¬ë‚˜ì´ì €ë¥¼ ê²°í•©í•œ [`LayoutLMv2Processor`]ë¥¼ ë¡œë“œí•˜ëŠ” ê²ƒë¶€í„° ì‹œì‘í•´ ë³´ê² ìŠµë‹ˆë‹¤.

```py
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained(model_checkpoint)
```

### ë¬¸ì„œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ [[preprocessing-document-images]]

ë¨¼ì €, í”„ë¡œì„¸ì„œì˜ `image_processor`ë¥¼ ì‚¬ìš©í•´ ëª¨ë¸ì— ëŒ€í•œ ë¬¸ì„œ ì´ë¯¸ì§€ë¥¼ ì¤€ë¹„í•´ ë³´ê² ìŠµë‹ˆë‹¤.
ê¸°ë³¸ì ìœ¼ë¡œ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œëŠ” ì´ë¯¸ì§€ í¬ê¸°ë¥¼ 224x224ë¡œ ì¡°ì •í•˜ê³ , ìƒ‰ìƒ ì±„ë„ì˜ ìˆœì„œê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•œ í›„, ë‹¨ì–´ì™€ ì •ê·œí™”ëœ ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ì–»ê¸° ìœ„í•´ í…Œì„œë™íŠ¸ë¥¼ ì‚¬ìš©í•´ OCRë¥¼ ì ìš©í•©ë‹ˆë‹¤.
ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” ì´ëŸ¬í•œ ëª¨ë“  ê¸°ë³¸ê°’ì´ ì •í™•íˆ í•„ìš”í•œ ê²ƒì…ë‹ˆë‹¤. ì´ë¯¸ì§€ ë°°ì¹˜ì— ê¸°ë³¸ ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ ì ìš©í•˜ê³  OCRì˜ ê²°ê³¼ë¥¼ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.

```py
>>> image_processor = processor.image_processor


>>> def get_ocr_words_and_boxes(examples):
...     images = [image.convert("RGB") for image in examples["image"]]
...     encoded_inputs = image_processor(images)


...     examples["image"] = encoded_inputs.pixel_values
...     examples["words"] = encoded_inputs.words
...     examples["boxes"] = encoded_inputs.boxes

...     return examples
```

ì´ ì „ì²˜ë¦¬ë¥¼ ë°ì´í„° ì„¸íŠ¸ ì „ì²´ì— ë¹ ë¥´ê²Œ ì ìš©í•˜ë ¤ë©´ [`~datasets.Dataset.map`]ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

```py
>>> dataset_with_ocr = updated_dataset.map(get_ocr_words_and_boxes, batched=True, batch_size=2)
```

### í…ìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬ [[preprocessing-text-data]]

ì´ë¯¸ì§€ì— OCRì„ ì ìš©í–ˆìœ¼ë©´ ë°ì´í„° ì„¸íŠ¸ì˜ í…ìŠ¤íŠ¸ ë¶€ë¶„ì„ ëª¨ë¸ì— ë§ê²Œ ì¸ì½”ë”©í•´ì•¼ í•©ë‹ˆë‹¤.
ì´ ì¸ì½”ë”©ì—ëŠ” ì´ì „ ë‹¨ê³„ì—ì„œ ê°€ì ¸ì˜¨ ë‹¨ì–´ì™€ ë°•ìŠ¤ë¥¼ í† í° ìˆ˜ì¤€ì˜ `input_ids`, `attention_mask`, `token_type_ids` ë° `bbox`ë¡œ ë³€í™˜í•˜ëŠ” ì‘ì—…ì´ í¬í•¨ë©ë‹ˆë‹¤. 
í…ìŠ¤íŠ¸ë¥¼ ì „ì²˜ë¦¬í•˜ë ¤ë©´ í”„ë¡œì„¸ì„œì˜ `tokenizer`ê°€ í•„ìš”í•©ë‹ˆë‹¤.

```py
>>> tokenizer = processor.tokenizer
```

ìœ„ì—ì„œ ì–¸ê¸‰í•œ ì „ì²˜ë¦¬ ì™¸ì—ë„ ëª¨ë¸ì— ëŒ€í•œ ë ˆì´ë¸”ì„ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤. ğŸ¤— Transformersì˜ `xxxForQuestionAnswering` ëª¨ë¸ì˜ ê²½ìš°, ë ˆì´ë¸”ì€ `start_positions`ì™€ `end_positions`ë¡œ êµ¬ì„±ë˜ë©° ì–´ë–¤ í† í°ì´ ë‹µë³€ì˜ ì‹œì‘ê³¼ ëì— ìˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

ë°©ê¸ˆ ë§í•œ ë ˆì´ë¸” ì¶”ê°€ë¥¼ ì‹œì‘í•´ë³´ê² ìŠµë‹ˆë‹¤. 
Let's start with that. Define a helper function that can find a sublist (the answer split into words) in a larger list (the words list).

This function will take two lists as input, `words_list` and `answer_list`. It will then iterate over the `words_list` and check
if the current word in the `words_list` (words_list[i]) is equal to the first word of answer_list (answer_list[0]) and if
the sublist of `words_list` starting from the current word and of the same length as `answer_list` is equal `to answer_list`.
If this condition is true, it means that a match has been found, and the function will record the match, its starting index (idx),
and its ending index (idx + len(answer_list) - 1). If more than one match was found, the function will return only the first one.
If no match is found, the function returns (`None`, 0, and 0).

```py
>>> def subfinder(words_list, answer_list):
...     matches = []
...     start_indices = []
...     end_indices = []
...     for idx, i in enumerate(range(len(words_list))):
...         if words_list[i] == answer_list[0] and words_list[i : i + len(answer_list)] == answer_list:
...             matches.append(answer_list)
...             start_indices.append(idx)
...             end_indices.append(idx + len(answer_list) - 1)
...     if matches:
...         return matches[0], start_indices[0], end_indices[0]
...     else:
...         return None, 0, 0
```

To illustrate how this function finds the position of the answer, let's use it on an example:

```py
>>> example = dataset_with_ocr["train"][1]
>>> words = [word.lower() for word in example["words"]]
>>> match, word_idx_start, word_idx_end = subfinder(words, example["answer"].lower().split())
>>> print("Question: ", example["question"])
>>> print("Words:", words)
>>> print("Answer: ", example["answer"])
>>> print("start_index", word_idx_start)
>>> print("end_index", word_idx_end)
Question:  Who is in  cc in this letter?
Words: ['wie', 'baw', 'brown', '&', 'williamson', 'tobacco', 'corporation', 'research', '&', 'development', 'internal', 'correspondence', 'to:', 'r.', 'h.', 'honeycutt', 'ce:', 't.f.', 'riehl', 'from:', '.', 'c.j.', 'cook', 'date:', 'may', '8,', '1995', 'subject:', 'review', 'of', 'existing', 'brainstorming', 'ideas/483', 'the', 'major', 'function', 'of', 'the', 'product', 'innovation', 'graup', 'is', 'to', 'develop', 'marketable', 'nove!', 'products', 'that', 'would', 'be', 'profitable', 'to', 'manufacture', 'and', 'sell.', 'novel', 'is', 'defined', 'as:', 'of', 'a', 'new', 'kind,', 'or', 'different', 'from', 'anything', 'seen', 'or', 'known', 'before.', 'innovation', 'is', 'defined', 'as:', 'something', 'new', 'or', 'different', 'introduced;', 'act', 'of', 'innovating;', 'introduction', 'of', 'new', 'things', 'or', 'methods.', 'the', 'products', 'may', 'incorporate', 'the', 'latest', 'technologies,', 'materials', 'and', 'know-how', 'available', 'to', 'give', 'then', 'a', 'unique', 'taste', 'or', 'look.', 'the', 'first', 'task', 'of', 'the', 'product', 'innovation', 'group', 'was', 'to', 'assemble,', 'review', 'and', 'categorize', 'a', 'list', 'of', 'existing', 'brainstorming', 'ideas.', 'ideas', 'were', 'grouped', 'into', 'two', 'major', 'categories', 'labeled', 'appearance', 'and', 'taste/aroma.', 'these', 'categories', 'are', 'used', 'for', 'novel', 'products', 'that', 'may', 'differ', 'from', 'a', 'visual', 'and/or', 'taste/aroma', 'point', 'of', 'view', 'compared', 'to', 'canventional', 'cigarettes.', 'other', 'categories', 'include', 'a', 'combination', 'of', 'the', 'above,', 'filters,', 'packaging', 'and', 'brand', 'extensions.', 'appearance', 'this', 'category', 'is', 'used', 'for', 'novel', 'cigarette', 'constructions', 'that', 'yield', 'visually', 'different', 'products', 'with', 'minimal', 'changes', 'in', 'smoke', 'chemistry', 'two', 'cigarettes', 'in', 'cne.', 'emulti-plug', 'te', 'build', 'yaur', 'awn', 'cigarette.', 'eswitchable', 'menthol', 'or', 'non', 'menthol', 'cigarette.', '*cigarettes', 'with', 'interspaced', 'perforations', 'to', 'enable', 'smoker', 'to', 'separate', 'unburned', 'section', 'for', 'future', 'smoking.', 'Â«short', 'cigarette,', 'tobacco', 'section', '30', 'mm.', 'Â«extremely', 'fast', 'buming', 'cigarette.', 'Â«novel', 'cigarette', 'constructions', 'that', 'permit', 'a', 'significant', 'reduction', 'iretobacco', 'weight', 'while', 'maintaining', 'smoking', 'mechanics', 'and', 'visual', 'characteristics.', 'higher', 'basis', 'weight', 'paper:', 'potential', 'reduction', 'in', 'tobacco', 'weight.', 'Â«more', 'rigid', 'tobacco', 'column;', 'stiffing', 'agent', 'for', 'tobacco;', 'e.g.', 'starch', '*colored', 'tow', 'and', 'cigarette', 'papers;', 'seasonal', 'promotions,', 'e.g.', 'pastel', 'colored', 'cigarettes', 'for', 'easter', 'or', 'in', 'an', 'ebony', 'and', 'ivory', 'brand', 'containing', 'a', 'mixture', 'of', 'all', 'black', '(black', 'paper', 'and', 'tow)', 'and', 'ail', 'white', 'cigarettes.', '499150498']
Answer:  T.F. Riehl
start_index 17
end_index 18
```

Once examples are encoded, however, they will look like this:

```py
>>> encoding = tokenizer(example["question"], example["words"], example["boxes"])
>>> tokenizer.decode(encoding["input_ids"])
[CLS] who is in cc in this letter? [SEP] wie baw brown & williamson tobacco corporation research & development ...
```

We'll need to find the position of the answer in the encoded input.
* `token_type_ids` tells us which tokens are part of the question, and which ones are part of the document's words.
* `tokenizer.cls_token_id` will help find the special token at the beginning of the input.
* `word_ids` will help match the answer found in the original `words` to the same answer in the full encoded input and determine
the start/end position of the answer in the encoded input.

With that in mind, let's create a function to encode a batch of examples in the dataset:

```py
>>> def encode_dataset(examples, max_length=512):
...     questions = examples["question"]
...     words = examples["words"]
...     boxes = examples["boxes"]
...     answers = examples["answer"]

...     # encode the batch of examples and initialize the start_positions and end_positions
...     encoding = tokenizer(questions, words, boxes, max_length=max_length, padding="max_length", truncation=True)
...     start_positions = []
...     end_positions = []

...     # loop through the examples in the batch
...     for i in range(len(questions)):
...         cls_index = encoding["input_ids"][i].index(tokenizer.cls_token_id)

...         # find the position of the answer in example's words
...         words_example = [word.lower() for word in words[i]]
...         answer = answers[i]
...         match, word_idx_start, word_idx_end = subfinder(words_example, answer.lower().split())

...         if match:
...             # if match is found, use `token_type_ids` to find where words start in the encoding
...             token_type_ids = encoding["token_type_ids"][i]
...             token_start_index = 0
...             while token_type_ids[token_start_index] != 1:
...                 token_start_index += 1

...             token_end_index = len(encoding["input_ids"][i]) - 1
...             while token_type_ids[token_end_index] != 1:
...                 token_end_index -= 1

...             word_ids = encoding.word_ids(i)[token_start_index : token_end_index + 1]
...             start_position = cls_index
...             end_position = cls_index

...             # loop over word_ids and increase `token_start_index` until it matches the answer position in words
...             # once it matches, save the `token_start_index` as the `start_position` of the answer in the encoding
...             for id in word_ids:
...                 if id == word_idx_start:
...                     start_position = token_start_index
...                 else:
...                     token_start_index += 1

...             # similarly loop over `word_ids` starting from the end to find the `end_position` of the answer
...             for id in word_ids[::-1]:
...                 if id == word_idx_end:
...                     end_position = token_end_index
...                 else:
...                     token_end_index -= 1

...             start_positions.append(start_position)
...             end_positions.append(end_position)

...         else:
...             start_positions.append(cls_index)
...             end_positions.append(cls_index)

...     encoding["image"] = examples["image"]
...     encoding["start_positions"] = start_positions
...     encoding["end_positions"] = end_positions

...     return encoding
```

Now that we have this preprocessing function, we can encode the entire dataset:

```py
>>> encoded_train_dataset = dataset_with_ocr["train"].map(
...     encode_dataset, batched=True, batch_size=2, remove_columns=dataset_with_ocr["train"].column_names
... )
>>> encoded_test_dataset = dataset_with_ocr["test"].map(
...     encode_dataset, batched=True, batch_size=2, remove_columns=dataset_with_ocr["test"].column_names
... )
```

Let's check what the features of the encoded dataset look like:

```py
>>> encoded_train_dataset.features
{'image': Sequence(feature=Sequence(feature=Sequence(feature=Value(dtype='uint8', id=None), length=-1, id=None), length=-1, id=None), length=-1, id=None),
 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),
 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),
 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),
 'bbox': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None),
 'start_positions': Value(dtype='int64', id=None),
 'end_positions': Value(dtype='int64', id=None)}
```

## Evaluation

Evaluation for document question answering requires a significant amount of postprocessing. To avoid taking up too much
of your time, this guide skips the evaluation step. The [`Trainer`] still calculates the evaluation loss during training so
you're not completely in the dark about your model's performance. Extractive question answering is typically evaluated using F1/exact match.
If you'd like to implement it yourself, check out the [Question Answering chapter](https://huggingface.co/course/chapter7/7?fw=pt#postprocessing)
of the Hugging Face course for inspiration.

## Train

Congratulations! You've successfully navigated the toughest part of this guide and now you are ready to train your own model.
Training involves the following steps:
* Load the model with [`AutoModelForDocumentQuestionAnswering`] using the same checkpoint as in the preprocessing.
* Define your training hyperparameters in [`TrainingArguments`].
* Define a function to batch examples together, here the [`DefaultDataCollator`] will do just fine
* Pass the training arguments to [`Trainer`] along with the model, dataset, and data collator.
* Call [`~Trainer.train`] to finetune your model.

```py
>>> from transformers import AutoModelForDocumentQuestionAnswering

>>> model = AutoModelForDocumentQuestionAnswering.from_pretrained(model_checkpoint)
```

In the [`TrainingArguments`] use `output_dir` to specify where to save your model, and configure hyperparameters as you see fit.
If you wish to share your model with the community, set `push_to_hub` to `True` (you must be signed in to Hugging Face to upload your model).
In this case the `output_dir` will also be the name of the repo where your model checkpoint will be pushed.

```py
>>> from transformers import TrainingArguments

>>> # REPLACE THIS WITH YOUR REPO ID
>>> repo_id = "MariaK/layoutlmv2-base-uncased_finetuned_docvqa"

>>> training_args = TrainingArguments(
...     output_dir=repo_id,
...     per_device_train_batch_size=4,
...     num_train_epochs=20,
...     save_steps=200,
...     logging_steps=50,
...     evaluation_strategy="steps",
...     learning_rate=5e-5,
...     save_total_limit=2,
...     remove_unused_columns=False,
...     push_to_hub=True,
... )
```

Define a simple data collator to batch examples together.

```py
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator()
```

Finally, bring everything together, and call [`~Trainer.train`]:

```py
>>> from transformers import Trainer

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     data_collator=data_collator,
...     train_dataset=encoded_train_dataset,
...     eval_dataset=encoded_test_dataset,
...     tokenizer=processor,
... )

>>> trainer.train()
```

To add the final model to ğŸ¤— Hub, create a model card and call `push_to_hub`:

```py
>>> trainer.create_model_card()
>>> trainer.push_to_hub()
```

## Inference

Now that you have finetuned a LayoutLMv2 model, and uploaded it to the ğŸ¤— Hub, you can use it for inference. The simplest
way to try out your finetuned model for inference is to use it in a [`Pipeline`].

Let's take an example:
```py
>>> example = dataset["test"][2]
>>> question = example["query"]["en"]
>>> image = example["image"]
>>> print(question)
>>> print(example["answers"])
'Who is â€˜presidingâ€™ TRRF GENERAL SESSION (PART 1)?'
['TRRF Vice President', 'lee a. waller']
```

Next, instantiate a pipeline for
document question answering with your model, and pass the image + question combination to it.

```py
>>> from transformers import pipeline

>>> qa_pipeline = pipeline("document-question-answering", model="MariaK/layoutlmv2-base-uncased_finetuned_docvqa")
>>> qa_pipeline(image, question)
[{'score': 0.9949808120727539,
  'answer': 'Lee A. Waller',
  'start': 55,
  'end': 57}]
```

You can also manually replicate the results of the pipeline if you'd like:
1. Take an image and a question, prepare them for the model using the processor from your model.
2. Forward the result or preprocessing through the model.
3. The model returns `start_logits` and `end_logits`, which indicate which token is at the start of the answer and
which token is at the end of the answer. Both have shape (batch_size, sequence_length).
4. Take an argmax on the last dimension of both the `start_logits` and `end_logits` to get the predicted `start_idx` and `end_idx`.
5. Decode the answer with the tokenizer.

```py
>>> import torch
>>> from transformers import AutoProcessor
>>> from transformers import AutoModelForDocumentQuestionAnswering

>>> processor = AutoProcessor.from_pretrained("MariaK/layoutlmv2-base-uncased_finetuned_docvqa")
>>> model = AutoModelForDocumentQuestionAnswering.from_pretrained("MariaK/layoutlmv2-base-uncased_finetuned_docvqa")

>>> with torch.no_grad():
...     encoding = processor(image.convert("RGB"), question, return_tensors="pt")
...     outputs = model(**encoding)
...     start_logits = outputs.start_logits
...     end_logits = outputs.end_logits
...     predicted_start_idx = start_logits.argmax(-1).item()
...     predicted_end_idx = end_logits.argmax(-1).item()

>>> processor.tokenizer.decode(encoding.input_ids.squeeze()[predicted_start_idx : predicted_end_idx + 1])
'lee a. waller'
```