<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# BERT[[BERT]]

<div class="flex flex-wrap space-x-1">
<a href="https://huggingface.co/models?filter=bert">
<img alt="Models" src="https://img.shields.io/badge/All_model_pages-bert-blueviolet">
</a>
<a href="https://huggingface.co/spaces/docs-demos/bert-base-uncased">
<img alt="Spaces" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue">
</a>
</div>

## ê°œìš”[[Overview]]

BERT ëª¨ë¸ì€ Jacob Devlin. Ming-Wei Chang, Kenton Lee, Kristina Touranovaê°€ ì œì•ˆí•œ ë…¼ë¬¸ [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)ì—ì„œ ì†Œê°œë˜ì—ˆìŠµë‹ˆë‹¤. BERTëŠ” ì‚¬ì „ í•™ìŠµëœ ì–‘ë°©í–¥ íŠ¸ëœìŠ¤í¬ë¨¸ë¡œ,  Toronto Book Corpusì™€ Wikipediaë¡œ êµ¬ì„±ëœ ëŒ€ê·œëª¨ ì½”í¼ìŠ¤ì—ì„œ ë§ˆìŠ¤í‚¹ëœ ì–¸ì–´ ëª¨ë¸ë§ê³¼ ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡(Next Sentence Prediction) ëª©í‘œë¥¼ ê²°í•©í•´ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤.

í•´ë‹¹ ë…¼ë¬¸ì˜ ì´ˆë¡ì…ë‹ˆë‹¤:

*ìš°ë¦¬ëŠ” BERT(Bidirectional Encoder Representations from Transformers)ë¼ëŠ” ìƒˆë¡œìš´ ì–¸ì–´ í‘œí˜„ ëª¨ë¸ì„ ì†Œê°œí•©ë‹ˆë‹¤. ìµœê·¼ì˜ ë‹¤ë¥¸ ì–¸ì–´ í‘œí˜„ ëª¨ë¸ë“¤ê³¼ ë‹¬ë¦¬, BERTëŠ” ëª¨ë“  ê³„ì¸µì—ì„œ ì–‘ë°©í–¥ìœ¼ë¡œ ì–‘ìª½ ë¬¸ë§¥ì„ ì¡°ê±´ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ë¹„ì§€ë„ í•™ìŠµëœ í…ìŠ¤íŠ¸ì—ì„œ ê¹Šì´ ìˆëŠ” ì–‘ë°©í–¥ í‘œí˜„ì„ ì‚¬ì „ í•™ìŠµí•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ ê²°ê³¼, ì‚¬ì „ í•™ìŠµëœ BERT ëª¨ë¸ì€ ì¶”ê°€ì ì¸ ì¶œë ¥ ê³„ì¸µ í•˜ë‚˜ë§Œìœ¼ë¡œ ì§ˆë¬¸ ì‘ë‹µ, ì–¸ì–´ ì¶”ë¡ ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ ë¯¸ì„¸ ì¡°ì •ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, íŠ¹ì • ì‘ì—…ì„ ìœ„í•´ ì•„í‚¤í…ì²˜ë¥¼ ìˆ˜ì •í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.*

*BERTëŠ” ê°œë…ì ìœ¼ë¡œ ë‹¨ìˆœí•˜ë©´ì„œë„ ì‹¤ì¦ì ìœ¼ë¡œ ê°•ë ¥í•œ ëª¨ë¸ì…ë‹ˆë‹¤. BERTëŠ” 11ê°œì˜ ìì—°ì–´ ì²˜ë¦¬ ê³¼ì œì—ì„œ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìœ¼ë©°, GLUE ì ìˆ˜ë¥¼ 80.5% (7.7% í¬ì¸íŠ¸ ì ˆëŒ€ ê°œì„ )ë¡œ, MultiNLI ì •í™•ë„ë¥¼ 86.7% (4.6% í¬ì¸íŠ¸ ì ˆëŒ€ ê°œì„ ), SQuAD v1.1 ì§ˆë¬¸ ì‘ë‹µ í…ŒìŠ¤íŠ¸ì—ì„œ F1 ì ìˆ˜ë¥¼ 93.2 (1.5% í¬ì¸íŠ¸ ì ˆëŒ€ ê°œì„ )ë¡œ, SQuAD v2.0ì—ì„œ F1 ì ìˆ˜ë¥¼ 83.1 (5.1% í¬ì¸íŠ¸ ì ˆëŒ€ ê°œì„ )ë¡œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.*

ì´ ëª¨ë¸ì€ [thomwolf](https://huggingface.co/thomwolf)ê°€ ê¸°ì—¬í•˜ì˜€ìŠµë‹ˆë‹¤. ì›ë³¸ ì½”ë“œëŠ” [ì—¬ê¸°](https://github.com/google-research/bert)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ì‚¬ìš© íŒ[[Usage tips]]

- BERTëŠ” ì ˆëŒ€ ìœ„ì¹˜ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì´ë¯€ë¡œ ì…ë ¥ì„ ì™¼ìª½ì´ ì•„ë‹ˆë¼ ì˜¤ë¥¸ìª½ì—ì„œ íŒ¨ë”©í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ìœ¼ë¡œ ê¶Œì¥ë©ë‹ˆë‹¤.
- BERTëŠ” ë§ˆìŠ¤í‚¹ëœ ì–¸ì–´ ëª¨ë¸(MLM)ê³¼ Next Sentence Prediction(NSP) ëª©í‘œë¡œ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” ë§ˆìŠ¤í‚¹ëœ í† í° ì˜ˆì¸¡ê³¼ ì „ë°˜ì ì¸ ìì—°ì–´ ì´í•´(NLU)ì— ë›°ì–´ë‚˜ì§€ë§Œ, í…ìŠ¤íŠ¸ ìƒì„±ì—ëŠ” ìµœì í™”ë˜ì–´ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.    
- BERTì˜ ì‚¬ì „ í•™ìŠµ ê³¼ì •ì—ì„œëŠ” ì…ë ¥ ë°ì´í„°ë¥¼ ë¬´ì‘ìœ„ë¡œ ë§ˆìŠ¤í‚¹í•˜ì—¬ ì¼ë¶€ í† í°ì„ ë§ˆìŠ¤í‚¹í•©ë‹ˆë‹¤. ì „ì²´ í† í° ì¤‘ ì•½ 15%ê°€ ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ë§ˆìŠ¤í‚¹ë©ë‹ˆë‹¤:

    * 80% í™•ë¥ ë¡œ ë§ˆìŠ¤í¬ í† í°ìœ¼ë¡œ ëŒ€ì²´
    * 10% í™•ë¥ ë¡œ ì„ì˜ì˜ ë‹¤ë¥¸ í† í°ìœ¼ë¡œ ëŒ€ì²´
    * 10% í™•ë¥ ë¡œ ì›ë˜ í† í° ê·¸ëŒ€ë¡œ ìœ ì§€

- ëª¨ë¸ì˜ ì£¼ìš” ëª©í‘œëŠ” ì›ë³¸ ë¬¸ì¥ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ì§€ë§Œ, ë‘ ë²ˆì§¸ ëª©í‘œê°€ ìˆìŠµë‹ˆë‹¤: ì…ë ¥ìœ¼ë¡œ ë¬¸ì¥ Aì™€ B (ì‚¬ì´ì—ëŠ” êµ¬ë¶„ í† í°ì´ ìˆìŒ)ê°€ ì£¼ì–´ì§‘ë‹ˆë‹¤. ì´ ë¬¸ì¥ ìŒì´ ì—°ì†ë  í™•ë¥ ì€ 50%ì´ë©°, ë‚˜ë¨¸ì§€ 50%ëŠ” ì„œë¡œ ë¬´ê´€í•œ ë¬¸ì¥ë“¤ì…ë‹ˆë‹¤. ëª¨ë¸ì€ ì´ ë‘ ë¬¸ì¥ì´ ì•„ë‹Œì§€ë¥¼ ì˜ˆì¸¡í•´ì•¼ í•©ë‹ˆë‹¤.

### Scaled Dot Product Attention(SDPA) ì‚¬ìš©í•˜ê¸° [[Using Scaled Dot Product Attention (SDPA)]]

PytorchëŠ” `torch.nn.functional`ì˜ ì¼ë¶€ë¡œ Scaled Dot Product Attention(SDPA) ì—°ì‚°ìë¥¼ ê¸°ë³¸ì ìœ¼ë¡œ ì œê³µí•©ë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ì…ë ¥ê³¼ í•˜ë“œì›¨ì–´ì— ë”°ë¼ ì—¬ëŸ¬ êµ¬í˜„ ë°©ì‹ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [ê³µì‹ ë¬¸ì„œ](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)ë‚˜ [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

`torch>=2.1.1`ì—ì„œëŠ” êµ¬í˜„ì´ ê°€ëŠ¥í•œ ê²½ìš° SDPAê°€ ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©ë˜ì§€ë§Œ, `from_pretrained()`í•¨ìˆ˜ì—ì„œ `attn_implementation="sdpa"`ë¥¼ ì„¤ì •í•˜ì—¬ SDPAë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì‚¬ìš©í•˜ë„ë¡ ì§€ì •í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

```
from transformers import BertModel

model = BertModel.from_pretrained("bert-base-uncased", torch_dtype=torch.float16, attn_implementation="sdpa")
...
```

ìµœì  ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ ëª¨ë¸ì„ ë°˜ì •ë°€ë„(ì˜ˆ: `torch.float16` ë˜ëŠ” `torch.bfloat16`)ë¡œ ë¶ˆëŸ¬ì˜¤ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.

ë¡œì»¬ ë²¤ì¹˜ë§ˆí¬ (A100-80GB, CPUx12, RAM 96.6GB, PyTorch 2.2.0, OS Ubuntu 22.04)ì—ì„œ `float16`ì„ ì‚¬ìš©í•´ í•™ìŠµ ë° ì¶”ë¡ ì„ ìˆ˜í–‰í•œ ê²°ê³¼, ë‹¤ìŒê³¼ ê°™ì€ ì†ë„ í–¥ìƒì´ ê´€ì°°ë˜ì—ˆìŠµë‹ˆë‹¤.

#### í•™ìŠµ [[Training]]

|batch_size|seq_len|Time per batch (eager - s)|Time per batch (sdpa - s)|Speedup (%)|Eager peak mem (MB)|sdpa peak mem (MB)|Mem saving (%)|
|----------|-------|--------------------------|-------------------------|-----------|-------------------|------------------|--------------|
|4         |256    |0.023                     |0.017                    |35.472     |939.213            |764.834           |22.800        |
|4         |512    |0.023                     |0.018                    |23.687     |1970.447           |1227.162          |60.569        |
|8         |256    |0.023                     |0.018                    |23.491     |1594.295           |1226.114          |30.028        |
|8         |512    |0.035                     |0.025                    |43.058     |3629.401           |2134.262          |70.054        |
|16        |256    |0.030                     |0.024                    |25.583     |2874.426           |2134.262          |34.680        |
|16        |512    |0.064                     |0.044                    |46.223     |6964.659           |3961.013          |75.830        |

#### ì¶”ë¡  [[Inference]]

|batch_size|seq_len|Per token latency eager (ms)|Per token latency SDPA (ms)|Speedup (%)|Mem eager (MB)|Mem BT (MB)|Mem saved (%)|
|----------|-------|----------------------------|---------------------------|-----------|--------------|-----------|-------------|
|1         |128    |5.736                       |4.987                      |15.022     |282.661       |282.924    |-0.093       |
|1         |256    |5.689                       |4.945                      |15.055     |298.686       |298.948    |-0.088       |
|2         |128    |6.154                       |4.982                      |23.521     |314.523       |314.785    |-0.083       |
|2         |256    |6.201                       |4.949                      |25.303     |347.546       |347.033    |0.148        |
|4         |128    |6.049                       |4.987                      |21.305     |378.895       |379.301    |-0.107       |
|4         |256    |6.285                       |5.364                      |17.166     |443.209       |444.382    |-0.264       |



## ìë£Œ[[Resources]]

BERTë¥¼ ì‹œì‘í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” Hugging Faceì™€ community ìë£Œ ëª©ë¡(ğŸŒë¡œ í‘œì‹œë¨) ì…ë‹ˆë‹¤. ì—¬ê¸°ì— í¬í•¨ë  ìë£Œë¥¼ ì œì¶œí•˜ê³  ì‹¶ë‹¤ë©´ PR(Pull Request)ë¥¼ ì—´ì–´ì£¼ì„¸ìš”. ë¦¬ë·° í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤! ìë£ŒëŠ” ê¸°ì¡´ ìë£Œë¥¼ ë³µì œí•˜ëŠ” ëŒ€ì‹  ìƒˆë¡œìš´ ë‚´ìš©ì„ ë‹´ê³  ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

<PipelineTag pipeline="text-classification"/>

- [BERT í…ìŠ¤íŠ¸ ë¶„ë¥˜ (ë‹¤ë¥¸ ì–¸ì–´ë¡œ)](https://www.philschmid.de/bert-text-classification-in-a-different-language)ì— ëŒ€í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸.
- [ë‹¤ì¤‘ ë ˆì´ë¸” í…ìŠ¤íŠ¸ ë¶„ë¥˜ë¥¼ ìœ„í•œ BERT (ë° ê´€ë ¨ ëª¨ë¸) ë¯¸ì„¸ ì¡°ì •](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb)ì— ëŒ€í•œ ë…¸íŠ¸ë¶.
- [PyTorchë¥¼ ì´ìš©í•´ BERTë¥¼ ë‹¤ì¤‘ ë ˆì´ë¸” ë¶„ë¥˜ë¥¼ ìœ„í•´ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë°©ë²•](httê¸°ps://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb)ì— ëŒ€í•œ ë…¸íŠ¸ë¶. ğŸŒ
- [BERTë¡œ EncoderDecoder ëª¨ë¸ì„ warm-startí•˜ì—¬ ìš”ì•½í•˜ê¸°](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb)ì— ëŒ€í•œ ë…¸íŠ¸ë¶.
- [`BertForSequenceClassification`]ì´  [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- [`TFBertForSequenceClassification`]ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- [`FlaxBertForSequenceClassification`]ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/flax/text-classification)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_flax.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- [í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì‘ì—… ê°€ì´ë“œ](../tasks/sequence_classification)

<PipelineTag pipeline="token-classification"/>

- [Kerasì™€ í•¨ê»˜ Hugging Face Transformersë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ì˜ë¦¬ BERTë¥¼ ê°œì²´ëª… ì¸ì‹(NER)ìš©ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë°©ë²•](https://www.philschmid.de/huggingface-transformers-keras-tf)ì— ëŒ€í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸.
- [BERTë¥¼ ê°œì²´ëª… ì¸ì‹ì„ ìœ„í•´ ë¯¸ì„¸ ì¡°ì •í•˜ê¸°](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Custom_Named_Entity_Recognition_with_BERT_only_first_wordpiece.ipynb)ì— ëŒ€í•œ ë…¸íŠ¸ë¶. ê° ë‹¨ì–´ì˜ ì²« ë²ˆì§¸ wordpieceì—ë§Œ ë ˆì´ë¸”ì„ ì§€ì •í•˜ì—¬ í•™ìŠµí•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤. ëª¨ë“  wordpieceì— ë ˆì´ë¸”ì„ ì „íŒŒí•˜ëŠ” ë°©ë²•ì€ [ì´ ë²„ì „](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/BERT/Custom_Named_Entity_Recognition_with_BERT.ipynb)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- [`BertForTokenClassification`]ì´  [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification)ì™€  [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- [`TFBertForTokenClassification`]ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/token-classification)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- [`FlaxBertForTokenClassification`]ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/flax/token-classification)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- ğŸ¤— Hugging Face ì½”ìŠ¤ì˜ [í† í° ë¶„ë¥˜ ì±•í„°](https://huggingface.co/course/chapter7/2?fw=pt).
- [í† í° ë¶„ë¥˜ ì‘ì—… ê°€ì´ë“œ](../tasks/token_classification)

<PipelineTag pipeline="fill-mask"/>

- [`BertForMaskedLM`]ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#robertabertdistilbert-and-masked-language-modeling)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- [`TFBertForMaskedLM`]ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_mlmpy) ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- [`FlaxBertForMaskedLM`]ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#masked-language-modeling)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked_language_modeling_flax.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- ğŸ¤— Hugging Face ì½”ìŠ¤ì˜ [ë§ˆìŠ¤í‚¹ëœ ì–¸ì–´ ëª¨ë¸ë§ ì±•í„°](https://huggingface.co/course/chapter7/3?fw=pt).
- [ë§ˆìŠ¤í‚¹ëœ ì–¸ì–´ ëª¨ë¸ë§ ì‘ì—… ê°€ì´ë“œ](../tasks/masked_language_modeling)

<PipelineTag pipeline="question-answering"/>

- [`BertForQuestionAnswering`]ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- [`TFBertForQuestionAnswering`]ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering) ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- [`FlaxBertForQuestionAnswering`]ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/flax/question-answering)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- ğŸ¤— Hugging Face ì½”ìŠ¤ì˜ [ì§ˆë¬¸ ë‹µë³€ ì±•í„°](https://huggingface.co/course/chapter7/7?fw=pt).
- [ì§ˆë¬¸ ë‹µë³€ ì‘ì—… ê°€ì´ë“œ](../tasks/question_answering)

**ë‹¤ì¤‘ ì„ íƒ**
- [`BertForMultipleChoice`]ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- [`TFBertForMultipleChoice`]ì´ [ì—ì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- [ë‹¤ì¤‘ ì„ íƒ ì‘ì—… ê°€ì´ë“œ](../tasks/multiple_choice)

âš¡ï¸ **ì¶”ë¡ **
- [Hugging Face Transformersì™€ AWS Inferentiaë¥¼ ì‚¬ìš©í•˜ì—¬ BERT ì¶”ë¡ ì„ ê°€ì†í™”í•˜ëŠ” ë°©ë²•](https://huggingface.co/blog/bert-inferentia-sagemaker)ì— ëŒ€í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸.
- [GPUì—ì„œ DeepSpeed-Inferenceë¡œ BERT ì¶”ë¡ ì„ ê°€ì†í™”í•˜ëŠ” ë°©ë²•](https://www.philschmid.de/bert-deepspeed-inference)ì— ëŒ€í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸.

âš™ï¸ **ì‚¬ì „ í•™ìŠµ**
- [Hugging Face Optimumìœ¼ë¡œ Transformersë¥¼ ONMXë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•](https://www.philschmid.de/pre-training-bert-habana)ì— ëŒ€í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸.

ğŸš€ **ë°°í¬**
- [Hugging Face Optimumìœ¼ë¡œ Transformersë¥¼ ONMXë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•](https://www.philschmid.de/convert-transformers-to-onnx)ì— ëŒ€í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸.
- [AWSì—ì„œ Hugging Face Transformersë¥¼ ìœ„í•œ Habana Gaudi ë”¥ëŸ¬ë‹ í™˜ê²½ ì„¤ì • ë°©ë²•](https://www.philschmid.de/getting-started-habana-gaudi#conclusion)ì— ëŒ€í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸.
- [Hugging Face Transformers, Amazon SageMaker ë° Terraform ëª¨ë“ˆì„ ì´ìš©í•œ BERT ìë™ í™•ì¥](https://www.philschmid.de/terraform-huggingface-amazon-sagemaker-advanced)ì— ëŒ€í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸.
- [Hugging Face, AWS Lambda, Dockerë¥¼ í™œìš©í•˜ì—¬ ì„œë²„ë¦¬ìŠ¤ BERT ì„¤ì •í•˜ëŠ” ë°©ë²•](https://www.philschmid.de/serverless-bert-with-huggingface-aws-lambda-docker)ì— ëŒ€í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸.
- [Amazon SageMakerì™€ Training Compilerë¥¼ ì‚¬ìš©í•˜ì—¬ Hugging Face Transformersì—ì„œ BERT ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë°©ë²•](https://www.philschmid.de/huggingface-amazon-sagemaker-training-compiler)ì— ëŒ€í•œ ë¸”ë¡œê·¸.
- [Amazon SageMakerë¥¼ ì‚¬ìš©í•œ Transformersì™€ BERTì˜ ì‘ì—…ë³„ ì§€ì‹ ì¦ë¥˜](https://www.philschmid.de/knowledge-distillation-bert-transformers)ì— ëŒ€í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸.

## BertConfig

[[autodoc]] BertConfig
    - all

## BertTokenizer

[[autodoc]] BertTokenizer
    - build_inputs_with_special_tokens
    - get_special_tokens_mask
    - create_token_type_ids_from_sequences
    - save_vocabulary

<frameworkcontent>
<pt>

## BertTokenizerFast

[[autodoc]] BertTokenizerFast

</pt>
<tf>

## TFBertTokenizer

[[autodoc]] TFBertTokenizer

</tf>
</frameworkcontent>

## Bert specific outputs

[[autodoc]] models.bert.modeling_bert.BertForPreTrainingOutput

[[autodoc]] models.bert.modeling_tf_bert.TFBertForPreTrainingOutput

[[autodoc]] models.bert.modeling_flax_bert.FlaxBertForPreTrainingOutput


<frameworkcontent>
<pt>

## BertModel

[[autodoc]] BertModel
    - forward

## BertForPreTraining

[[autodoc]] BertForPreTraining
    - forward

## BertLMHeadModel

[[autodoc]] BertLMHeadModel
    - forward

## BertForMaskedLM

[[autodoc]] BertForMaskedLM
    - forward

## BertForNextSentencePrediction

[[autodoc]] BertForNextSentencePrediction
    - forward

## BertForSequenceClassification

[[autodoc]] BertForSequenceClassification
    - forward

## BertForMultipleChoice

[[autodoc]] BertForMultipleChoice
    - forward

## BertForTokenClassification

[[autodoc]] BertForTokenClassification
    - forward

## BertForQuestionAnswering

[[autodoc]] BertForQuestionAnswering
    - forward

</pt>
<tf>

## TFBertModel

[[autodoc]] TFBertModel
    - call

## TFBertForPreTraining

[[autodoc]] TFBertForPreTraining
    - call

## TFBertModelLMHeadModel

[[autodoc]] TFBertLMHeadModel
    - call

## TFBertForMaskedLM

[[autodoc]] TFBertForMaskedLM
    - call

## TFBertForNextSentencePrediction

[[autodoc]] TFBertForNextSentencePrediction
    - call

## TFBertForSequenceClassification

[[autodoc]] TFBertForSequenceClassification
    - call

## TFBertForMultipleChoice

[[autodoc]] TFBertForMultipleChoice
    - call

## TFBertForTokenClassification

[[autodoc]] TFBertForTokenClassification
    - call

## TFBertForQuestionAnswering

[[autodoc]] TFBertForQuestionAnswering
    - call

</tf>
<jax>

## FlaxBertModel

[[autodoc]] FlaxBertModel
    - __call__

## FlaxBertForPreTraining

[[autodoc]] FlaxBertForPreTraining
    - __call__

## FlaxBertForCausalLM

[[autodoc]] FlaxBertForCausalLM
    - __call__

## FlaxBertForMaskedLM

[[autodoc]] FlaxBertForMaskedLM
    - __call__

## FlaxBertForNextSentencePrediction

[[autodoc]] FlaxBertForNextSentencePrediction
    - __call__

## FlaxBertForSequenceClassification

[[autodoc]] FlaxBertForSequenceClassification
    - __call__

## FlaxBertForMultipleChoice

[[autodoc]] FlaxBertForMultipleChoice
    - __call__

## FlaxBertForTokenClassification

[[autodoc]] FlaxBertForTokenClassification
    - __call__

## FlaxBertForQuestionAnswering

[[autodoc]] FlaxBertForQuestionAnswering
    - __call__

</jax>
</frameworkcontent>


