<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# BERT[[bert]]

<div class="flex flex-wrap space-x-1">
<a href="https://huggingface.co/models?filter=bert">
<img alt="Models" src="https://img.shields.io/badge/All_model_pages-bert-blueviolet">
</a>
<a href="https://huggingface.co/spaces/docs-demos/bert-base-uncased">
<img alt="Spaces" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue">
</a>
</div>

## ê°œìš”[[overview]]

BERT ëª¨ë¸ì€ Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanovaê°€ ë°œí‘œí•œ [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) ë…¼ë¬¸ì—ì„œ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸ë§ê³¼ ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡ì„ ê²°í•©í•˜ì—¬ Toronto Book Corpusì™€ Wikipediaë¡œ êµ¬ì„±ëœ ëŒ€ê·œëª¨ ì½”í¼ìŠ¤ë¡œë¶€í„° ì‚¬ì „í›ˆë ¨ëœ ì–‘ë°©í–¥ íŠ¸ëœìŠ¤í¬ë¨¸ì…ë‹ˆë‹¤.

ë…¼ë¬¸ì˜ ì´ˆë¡ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

*ìš°ë¦¬ëŠ” BERT(Bidirectional Encoder Representations from Transformers)ë¼ëŠ” ìƒˆë¡œìš´ ì–¸ì–´ í‘œí˜„ ëª¨ë¸ì„ ì†Œê°œí•©ë‹ˆë‹¤. ìµœê·¼ì˜ ì–¸ì–´ í‘œí˜„ ëª¨ë¸ê³¼ ë‹¬ë¦¬, BERTëŠ” ëª¨ë“  ë ˆì´ì–´ì—ì„œ ì™¼ìª½ê³¼ ì˜¤ë¥¸ìª½ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë™ì‹œì— ê³ ë ¤í•˜ì—¬ ë¹„ì§€ë„ í…ìŠ¤íŠ¸ë¡œë¶€í„° ê¹Šì€ ì–‘ë°©í–¥ í‘œí˜„ì„ ì‚¬ì „í›ˆë ¨í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ ê²°ê³¼, ì‚¬ì „í›ˆë ¨ëœ BERT ëª¨ë¸ì€ í•˜ë‚˜ì˜ ì¶”ê°€ ì¶œë ¥ ë ˆì´ì–´ë§Œìœ¼ë¡œ ì§ˆì˜ ì‘ë‹µ ë° ì–¸ì–´ ì¶”ë¡ ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ì‘ì—…ì— ëŒ€í•´ ìµœì‹  ëª¨ë¸ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìœ¼ë©°, ë³„ë„ì˜ ì‘ì—…ë³„ ì•„í‚¤í…ì²˜ ìˆ˜ì • ì—†ì´ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤.*

*BERTëŠ” ê°œë…ì ìœ¼ë¡œ ë‹¨ìˆœí•˜ë©´ì„œë„ ì‹¤ì§ˆì ìœ¼ë¡œ ê°•ë ¥í•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ 11ê°œì˜ ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì—ì„œ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìœ¼ë©°, GLUE ì ìˆ˜ë¥¼ 80.5%ë¡œ ëŒì–´ì˜¬ë¦¬ë©°(ìˆ˜ì¹˜ìƒìœ¼ë¡œ 7.7%í¬ì¸íŠ¸ í–¥ìƒ), MultiNLI ì •í™•ë„ë¥¼ 86.7%(ìˆ˜ì¹˜ìƒìœ¼ë¡œ 4.6%í¬ì¸íŠ¸ í–¥ìƒ), SQuAD v1.1 ì§ˆì˜ ì‘ë‹µ í…ŒìŠ¤íŠ¸ F1 ì ìˆ˜ë¥¼ 93.2(ìˆ˜ì¹˜ìƒìœ¼ë¡œ 1.5í¬ì¸íŠ¸ í–¥ìƒ), SQuAD v2.0 í…ŒìŠ¤íŠ¸ F1 ì ìˆ˜ë¥¼ 83.1(ìˆ˜ì¹˜ìƒìœ¼ë¡œ 5.1í¬ì¸íŠ¸ í–¥ìƒ)ë¡œ ëŒì–´ì˜¬ë ¸ìŠµë‹ˆë‹¤.*

ì´ ëª¨ë¸ì€ [thomwolf](https://huggingface.co/thomwolf)ê°€ ê¸°ì—¬í•˜ì˜€ìŠµë‹ˆë‹¤. ì›ë³¸ ì½”ë“œëŠ” [ì—¬ê¸°](https://github.com/google-research/bert)ì—ì„œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ì‚¬ìš© íŒ[[usage-tips]]

- BERTëŠ” ì ˆëŒ€ì  ìœ„ì¹˜ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì´ë¯€ë¡œ ì…ë ¥ì„ ì™¼ìª½ì´ ì•„ë‹Œ ì˜¤ë¥¸ìª½ì—ì„œ íŒ¨ë”©í•˜ëŠ” ê²ƒì„ ì¼ë°˜ì ìœ¼ë¡œ ê¶Œì¥í•©ë‹ˆë‹¤.
- BERTëŠ” ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸ë§(MLM)ê³¼ ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡(NSP) ëª©í‘œë¡œ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤. ë§ˆìŠ¤í‚¹ëœ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” ì‘ì—…ê³¼ ì¼ë°˜ì ì¸ ìì—°ì–´ ì´í•´(NLU)ì— íš¨ìœ¨ì ì´ì§€ë§Œ, í…ìŠ¤íŠ¸ ìƒì„±ì—ëŠ” ìµœì í™”ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.
- ëœë¤ ë§ˆìŠ¤í‚¹ì„ ì‚¬ìš©í•´ ì…ë ¥ì„ ë³€í˜•í•©ë‹ˆë‹¤. ë” êµ¬ì²´ì ìœ¼ë¡œ ë§í•˜ìë©´, ì‚¬ì „í›ˆë ¨ ì¤‘ì—ëŠ” ì£¼ì–´ì§„ í† í°ì˜ ì¼ì • ë¹„ìœ¨(ì¼ë°˜ì ìœ¼ë¡œ 15%)ì´ ë‹¤ìŒê³¼ ê°™ì´ ë§ˆìŠ¤í‚¹ë©ë‹ˆë‹¤:

    * 80% í™•ë¥ ë¡œ íŠ¹ìˆ˜ ë§ˆìŠ¤í¬ í† í°ì„ ì‚¬ìš©
    * 10% í™•ë¥ ë¡œ ë§ˆìŠ¤í‚¹ëœ í† í°ê³¼ ë‹¤ë¥¸ ì„ì˜ì˜ í† í°ì„ ì‚¬ìš©
    * 10% í™•ë¥ ë¡œ ë™ì¼í•œ í† í°ì„ ìœ ì§€

- ëª¨ë¸ì€ ì›ë˜ ë¬¸ì¥ì„ ì˜ˆì¸¡í•´ì•¼ í•˜ë©°, ë™ì‹œì— ë‘ ë²ˆì§¸ ëª©í‘œë„ ìˆìŠµë‹ˆë‹¤: ì…ë ¥ì€ Aì™€ Bë¼ëŠ” ë‘ ë¬¸ì¥(ì‚¬ì´ì— êµ¬ë¶„ í† í°ì´ ìˆìŒ)ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. 50% í™•ë¥ ë¡œ ì´ ë¬¸ì¥ë“¤ì€ ì½”í¼ìŠ¤ì—ì„œ ì—°ì†ì ì´ë©°, ë‚˜ë¨¸ì§€ 50%ëŠ” ì„œë¡œ ê´€ë ¨ì´ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ì€ ë¬¸ì¥ë“¤ì´ ì—°ì†ì ì¸ì§€ ì•„ë‹Œì§€ë¥¼ ì˜ˆì¸¡í•´ì•¼ í•©ë‹ˆë‹¤.

### Scaled Dot Product Attention(SDPA) ì‚¬ìš©[[using-scaled-dot-product-attention-sdpa]]

PyTorchëŠ” `torch.nn.functional`ì˜ ì¼ë¶€ë¡œ ë„¤ì´í‹°ë¸Œ Scaled Dot Product Attention(SDPA) ì—°ì‚°ìë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ì…ë ¥ê³¼ ì‚¬ìš© ì¤‘ì¸ í•˜ë“œì›¨ì–´ì— ë”°ë¼ ì—¬ëŸ¬ êµ¬í˜„ì„ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [ê³µì‹ ë¬¸ì„œ](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) ë˜ëŠ” [GPU ì¶”ë¡ ](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention) í˜ì´ì§€ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

`torch>=2.1.1`ì—ì„œëŠ” ê°€ëŠ¥í•œ ê²½ìš° ê¸°ë³¸ì ìœ¼ë¡œ SDPAê°€ ì‚¬ìš©ë˜ì§€ë§Œ, `from_pretrained()`ì—ì„œ `attn_implementation="sdpa"`ë¥¼ ì„¤ì •í•˜ì—¬ ëª…ì‹œì ìœ¼ë¡œ SDPAë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

```
from transformers import BertModel

model = BertModel.from_pretrained("bert-base-uncased", torch_dtype=torch.float16, attn_implementation="sdpa")
...
```

ìµœìƒì˜ ì†ë„ í–¥ìƒì„ ìœ„í•´ ëª¨ë¸ì„ ë°˜ì •ë°€ë„(e.g. `torch.float16` ë˜ëŠ” `torch.bfloat16`)ë¡œ ë¡œë“œí•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.

ë¡œì»¬ ë²¤ì¹˜ë§ˆí¬(A100-80GB, CPUx12, RAM 96.6GB, PyTorch 2.2.0, OS Ubuntu 22.04)ì—ì„œ `float16`ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ë° ì¶”ë¡  ì¤‘ ë‹¤ìŒê³¼ ê°™ì€ ì†ë„ í–¥ìƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

#### í•™ìŠµ[[training]]

|ë°°ì¹˜ í¬ê¸°|ì‹œí€€ìŠ¤ ê¸¸ì´|ë°°ì¹˜ë‹¹ ì‹œê°„ (eager - ì´ˆ)|ë°°ì¹˜ë‹¹ ì‹œê°„ (sdpa - ì´ˆ)|ì†ë„ í–¥ìƒ (%)|Eager í”¼í¬ ë©”ëª¨ë¦¬ (MB)|SDPA í”¼í¬ ë©”ëª¨ë¦¬ (MB)|ë©”ëª¨ë¦¬ ì ˆì•½ (%)|
|----------|-------|--------------------------|-------------------------|-----------|-------------------|------------------|--------------|
|4         |256    |0.023                     |0.017                    |35.472     |939.213            |764.834           |22.800        |
|4         |512    |0.023                     |0.018                    |23.687     |1970.447           |1227.162          |60.569        |
|8         |256    |0.023                     |0.018                    |23.491     |1594.295           |1226.114          |30.028        |
|8         |512    |0.035                     |0.025                    |43.058     |3629.401           |2134.262          |70.054        |
|16        |256    |0.030                     |0.024                    |25.583     |2874.426           |2134.262          |34.680        |
|16        |512    |0.064                     |0.044                    |46.223     |6964.659           |3961.013          |75.830        |

#### ì¶”ë¡ [[inference]]

|ë°°ì¹˜ í¬ê¸°|ì‹œí€€ìŠ¤ ê¸¸ì´|í† í°ë‹¹ ì§€ì—° ì‹œê°„ (eager - ms)|í† í°ë‹¹ ì§€ì—° ì‹œê°„ (SDPA - ms)|ì†ë„ í–¥ìƒ (%)|Eager ë©”ëª¨ë¦¬ (MB)|BT ë©”ëª¨ë¦¬ (MB)|ë©”ëª¨ë¦¬ ì ˆì•½ (%)|
|----------|-------|----------------------------|---------------------------|-----------|--------------|-----------|-------------|
|1         |128    |5.736                       |4.987                      |15.022     |282.661       |282.924    |-0.093       |
|1         |256    |5.689                       |4.945                      |15.055     |298.686       |298.948    |-0.088       |
|2         |128    |6.154                       |4.982                      |23.521     |314.523       |314.785    |-0.083       |
|2         |256    |6.201                       |4.949                      |25.303     |347.546       |347.033    |0.148        |
|4         |128    |6.049                       |4.987                      |21.305     |378.895       |379.301    |-0.107       |
|4         |256    |6.285                       |5.364                      |17.166     |443.209       |444.382    |-0.264       |


## ê´€ë ¨ ìë£Œ[[resources]]

BERTë¥¼ ì‹œì‘í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ê³µì‹ Hugging Face ë° ì»¤ë®¤ë‹ˆí‹°(ğŸŒë¡œ í‘œì‹œëœ) ìë£Œ ëª©ë¡ì…ë‹ˆë‹¤. ì´ê³³ì— í¬í•¨ë  ìë£Œë¥¼ ì œì¶œí•˜ê³  ì‹¶ë‹¤ë©´ ììœ ë¡­ê²Œ Pull Requestë¥¼ ì—´ì–´ì£¼ì„¸ìš”! ìë£ŒëŠ” ê¸°ì¡´ ìë£Œì™€ ì¤‘ë³µëœ ë‚´ìš©ë³´ë‹¤ëŠ” ìƒˆë¡œìš´ ë‚´ìš©ì„ ë‹¤ë£¨ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

<PipelineTag pipeline="text-classification"/>

- [ë‹¤ë¥¸ ì–¸ì–´ë¡œ BERT í…ìŠ¤íŠ¸ ë¶„ë¥˜](https://www.philschmid.de/bert-text-classification-in-a-different-language)ì— ê´€í•œ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼.
- [ë‹¤ì¤‘ ë ˆì´ë¸” í…ìŠ¤íŠ¸ ë¶„ë¥˜ë¥¼ ìœ„í•œ BERT(ë° ë‹¤ë¥¸ ëª¨ë¸) ë¯¸ì„¸ ì¡°ì •](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb) ë…¸íŠ¸ë¶.
- PyTorchë¥¼ ì‚¬ìš©í•´ [BERTë¥¼ ë‹¤ì¤‘ ë ˆì´ë¸” ë¶„ë¥˜ì— ë¯¸ì„¸ ì¡°ì •](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb)í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•œ ë…¸íŠ¸ë¶. ğŸŒ
- [ìš”ì•½ì„ ìœ„í•´ BERTë¡œ EncoderDecoder ëª¨ë¸ì„ ì›œ ìŠ¤íƒ€íŠ¸](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb)í•˜ëŠ” ë°©ë²•ì— ê´€í•œ ë…¸íŠ¸ë¶.
- [`BertForSequenceClassification`]ì€ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification) ë° [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- [`TFBertForSequenceClassification`]ì€ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification) ë° [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- [`FlaxBertForSequenceClassification`]ì€ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/flax/text-classification) ë° [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_flax.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- [í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì‘ì—… ê°€ì´ë“œ](../tasks/sequence_classification)

<PipelineTag pipeline="token-classification"/>

- [Hugging Face Transformersì™€ Kerasë¥¼ ì‚¬ìš©í•´ ë¹„ì˜ì–´ BERTë¥¼ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ê°œì²´ëª… ì¸ì‹ ìˆ˜í–‰](https://www.philschmid.de/huggingface-transformers-keras-tf)ì— ê´€í•œ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼.
- ì²« ë²ˆì§¸ ì›Œë“œí”¼ìŠ¤ë§Œ ì‚¬ìš©í•´ [BERTë¡œ ê°œì²´ëª… ì¸ì‹ ë¯¸ì„¸ ì¡°ì •](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Custom_Named_Entity_Recognition_with_BERT_only_first_wordpiece.ipynb)í•˜ëŠ” ë…¸íŠ¸ë¶. ëª¨ë“  ì›Œë“œí”¼ìŠ¤ì— ë‹¨ì–´ì˜ ë ˆì´ë¸”ì„ ì „ë‹¬í•˜ë ¤ë©´ ì´ [ë²„ì „](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/BERT/Custom_Named_Entity_Recognition_with_BERT.ipynb)ì„ ì°¸ê³ í•˜ì„¸ìš”.
- [`BertForTokenClassification`]ì€ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification) ë° [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- [`TFBertForTokenClassification`]ì€ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/token-classification) ë° [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- [`FlaxBertForTokenClassification`]ì€ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/flax/token-classification)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- ğŸ¤— Hugging Face ê°•ì¢Œì˜ [í† í° ë¶„ë¥˜](https://huggingface.co/course/chapter7/2?fw=pt) ì±•í„°.
- [í† í° ë¶„ë¥˜ ì‘ì—… ê°€ì´ë“œ](../tasks/token_classification)

<PipelineTag pipeline="fill-mask"/>

- [`BertForMaskedLM`]ì€ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#robertabertdistilbert-and-masked-language-modeling) ë° [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- [`TFBertForMaskedLM`]ì€ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_mlmpy) ë° [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- [`FlaxBertForMaskedLM`]ì€ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#masked-language-modeling) ë° [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked_language_modeling_flax.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- ğŸ¤— Hugging Face ê°•ì¢Œì˜ [ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸ë§](https://huggingface.co/course/chapter7/3?fw=pt) ì±•í„°.
- [ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸ë§ ì‘ì—… ê°€ì´ë“œ](../tasks/masked_language_modeling)

<PipelineTag pipeline="question-answering"/>

- [`BertForQuestionAnswering`]ì€ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) ë° [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- [`TFBertForQuestionAnswering`]ì€ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering) ë° [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- [`FlaxBertForQuestionAnswering`]ì€ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/flax/question-answering)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- ğŸ¤— Hugging Face ê°•ì¢Œì˜ [ì§ˆì˜ ì‘ë‹µ](https://huggingface.co/course/chapter7/7?fw=pt) ì±•í„°.
- [ì§ˆì˜ ì‘ë‹µ ì‘ì—… ê°€ì´ë“œ](../tasks/question_answering)

**ë‹¤ì¤‘ ì„ íƒ ë¬¸ì œ**
- [`BertForMultipleChoice`]ëŠ” [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) ë° [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- [`TFBertForMultipleChoice`]ëŠ” [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice) ë° [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- [ë‹¤ì¤‘ ì„ íƒ ë¬¸ì œ ì‘ì—… ê°€ì´ë“œ](../tasks/multiple_choice)

âš¡ï¸ **ì¶”ë¡ (Inference)**
- [Hugging Face Transformersì™€ AWS Inferentiaë¡œ BERT ì¶”ë¡  ê°€ì†í™”](https://huggingface.co/blog/bert-inferentia-sagemaker)ì— ê´€í•œ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼.
- [GPUì—ì„œ DeepSpeed-Inferenceë¥¼ ì‚¬ìš©í•´ BERT ì¶”ë¡  ê°€ì†í™”](https://www.philschmid.de/bert-deepspeed-inference)ì— ê´€í•œ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼.

âš™ï¸ **ì‚¬ì „í›ˆë ¨(Pretraining)**
- [Hugging Face Transformersì™€ Habana Gaudië¡œ BERT ì‚¬ì „í›ˆë ¨](https://www.philschmid.de/pre-training-bert-habana)ì— ê´€í•œ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼.

ğŸš€ **ë°°í¬(Deploy)**
- [Hugging Face Optimumìœ¼ë¡œ Transformersë¥¼ ONNXë¡œ ë³€í™˜](https://www.philschmid.de/convert-transformers-to-onnx)ì— ê´€í•œ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼.
- [AWSì—ì„œ Habana Gaudië¡œ Hugging Face Transformers ë”¥ëŸ¬ë‹ í™˜ê²½ ì„¤ì •](https://www.philschmid.de/getting-started-habana-gaudi#conclusion)ì— ê´€í•œ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼.
- [Hugging Face Transformers, Amazon SageMaker, Terraform ëª¨ë“ˆë¡œ BERT ìë™ í™•ì¥](https://www.philschmid.de/terraform-huggingface-amazon-sagemaker-advanced)ì— ê´€í•œ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼.
- [Hugging Face, AWS Lambda, Dockerë¥¼ ì‚¬ìš©í•œ ì„œë²„ë¦¬ìŠ¤ BERT](https://www.philschmid.de/serverless-bert-with-huggingface-aws-lambda-docker)ì— ê´€í•œ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼.
- [Amazon SageMaker ë° Training Compilerë¡œ Hugging Face Transformers BERT ë¯¸ì„¸ ì¡°ì •](https://www.philschmid.de/huggingface-amazon-sagemaker-training-compiler)ì— ê´€í•œ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼.
- [Transformersì™€ Amazon SageMakerë¥¼ ì‚¬ìš©í•œ BERTì˜ ì‘ì—…ë³„ ì§€ì‹ ì¦ë¥˜](https://www.philschmid.de/knowledge-distillation-bert-transformers)ì— ê´€í•œ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼.

## BertConfig[[transformers.BertConfig]]

[[autodoc]] BertConfig
    - all

## BertTokenizer[[transformers.BertTokenizer]]

[[autodoc]] BertTokenizer
    - build_inputs_with_special_tokens
    - get_special_tokens_mask
    - create_token_type_ids_from_sequences
    - save_vocabulary

<frameworkcontent>
<pt>

## BertTokenizerFast[[transformers.BertTokenizerFast]]

[[autodoc]] BertTokenizerFast

</pt>
<tf>

## TFBertTokenizer[[transformers.TFBertTokenizer]]

[[autodoc]] TFBertTokenizer

</tf>
</frameworkcontent>

## Bertì— íŠ¹í™”ëœ ì¶œë ¥[[transformers.models.bert.modeling_bert.BertForPreTrainingOutput]]

[[autodoc]] models.bert.modeling_bert.BertForPreTrainingOutput

[[autodoc]] models.bert.modeling_tf_bert.TFBertForPreTrainingOutput

[[autodoc]] models.bert.modeling_flax_bert.FlaxBertForPreTrainingOutput


<frameworkcontent>
<pt>

## BertModel[[transformers.BertModel]]

[[autodoc]] BertModel
    - forward

## BertForPreTraining[[transformers.BertForPreTraining]]

[[autodoc]] BertForPreTraining
    - forward

## BertLMHeadModel[[transformers.BertLMHeadModel]]

[[autodoc]] BertLMHeadModel
    - forward

## BertForMaskedLM[[transformers.BertForMaskedLM]]

[[autodoc]] BertForMaskedLM
    - forward

## BertForNextSentencePrediction[[transformers.BertForNextSentencePrediction]]

[[autodoc]] BertForNextSentencePrediction
    - forward

## BertForSequenceClassification[[transformers.BertForSequenceClassification]]

[[autodoc]] BertForSequenceClassification
    - forward

## BertForMultipleChoice[[transformers.BertForMultipleChoice]]

[[autodoc]] BertForMultipleChoice
    - forward

## BertForTokenClassification[[transformers.BertForTokenClassification]]

[[autodoc]] BertForTokenClassification
    - forward

## BertForQuestionAnswering[[transformers.BertForQuestionAnswering]]

[[autodoc]] BertForQuestionAnswering
    - forward

</pt>
<tf>

## TFBertModel[[transformers.TFBertModel]]

[[autodoc]] TFBertModel
    - call

## TFBertForPreTraining[[transformers.TFBertForPreTraining]]

[[autodoc]] TFBertForPreTraining
    - call

## TFBertModelLMHeadModel[[transformers.TFBertLMHeadModel]]

[[autodoc]] TFBertLMHeadModel
    - call

## TFBertForMaskedLM[[transformers.TFBertForMaskedLM]]

[[autodoc]] TFBertForMaskedLM
    - call

## TFBertForNextSentencePrediction[[transformers.TFBertForNextSentencePrediction]]

[[autodoc]] TFBertForNextSentencePrediction
    - call

## TFBertForSequenceClassification[[transformers.TFBertForSequenceClassification]]

[[autodoc]] TFBertForSequenceClassification
    - call

## TFBertForMultipleChoice[[transformers.TFBertForMultipleChoice]]

[[autodoc]] TFBertForMultipleChoice
    - call

## TFBertForTokenClassification[[transformers.TFBertForTokenClassification]]

[[autodoc]] TFBertForTokenClassification
    - call

## TFBertForQuestionAnswering[[transformers.TFBertForQuestionAnswering]]

[[autodoc]] TFBertForQuestionAnswering
    - call

</tf>
<jax>

## FlaxBertModel[[transformers.FlaxBertModel]]

[[autodoc]] FlaxBertModel
    - __call__

## FlaxBertForPreTraining[[transformers.FlaxBertForPreTraining]]

[[autodoc]] FlaxBertForPreTraining
    - __call__

## FlaxBertForCausalLM[[transformers.FlaxBertForCausalLM]]

[[autodoc]] FlaxBertForCausalLM
    - __call__

## FlaxBertForMaskedLM[[transformers.FlaxBertForMaskedLM]]

[[autodoc]] FlaxBertForMaskedLM
    - __call__

## FlaxBertForNextSentencePrediction[[transformers.FlaxBertForNextSentencePrediction]]

[[autodoc]] FlaxBertForNextSentencePrediction
    - __call__

## FlaxBertForSequenceClassification[[transformers.FlaxBertForSequenceClassification]]

[[autodoc]] FlaxBertForSequenceClassification
    - __call__

## FlaxBertForMultipleChoice[[transformers.FlaxBertForMultipleChoice]]

[[autodoc]] FlaxBertForMultipleChoice
    - __call__

## FlaxBertForTokenClassification[[transformers.FlaxBertForTokenClassification]]

[[autodoc]] FlaxBertForTokenClassification
    - __call__

## FlaxBertForQuestionAnswering[[transformers.FlaxBertForQuestionAnswering]]

[[autodoc]] FlaxBertForQuestionAnswering
    - __call__

</jax>
</frameworkcontent>


