<!--Copyright 2021 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# CLIP[[clip]]

## ê°œìš”[[overview]]

CLIP ëª¨ë¸ì€ Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskeverê°€ ì œì•ˆí•œ [ìì—°ì–´ ì§€ë„(supervision)ë¥¼ í†µí•œ ì „ì´ ê°€ëŠ¥í•œ ì‹œê° ëª¨ë¸ í•™ìŠµ](https://huggingface.co/papers/2103.00020)ë¼ëŠ” ë…¼ë¬¸ì—ì„œ ì†Œê°œë˜ì—ˆìŠµë‹ˆë‹¤. CLIP(Contrastive Language-Image Pre-Training)ì€ ë‹¤ì–‘í•œ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ìŒìœ¼ë¡œ í›ˆë ¨ëœ ì‹ ê²½ë§ ì…ë‹ˆë‹¤. GPT-2ì™€ 3ì˜ ì œë¡œìƒ· ëŠ¥ë ¥ê³¼ ìœ ì‚¬í•˜ê²Œ, í•´ë‹¹ ì‘ì—…ì— ì§ì ‘ì ìœ¼ë¡œ ìµœì í™”í•˜ì§€ ì•Šê³ ë„ ì£¼ì–´ì§„ ì´ë¯¸ì§€ì— ëŒ€í•´ ê°€ì¥ ê´€ë ¨ì„± ìˆëŠ” í…ìŠ¤íŠ¸ ìŠ¤ë‹ˆí«ì„ ì˜ˆì¸¡í•˜ë„ë¡ ìì—°ì–´ë¡œ ì§€ì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

í•´ë‹¹ ë…¼ë¬¸ì˜ ì´ˆë¡ì…ë‹ˆë‹¤.

*ìµœì‹  ì»´í“¨í„° ë¹„ì „ ì‹œìŠ¤í…œì€ ë¯¸ë¦¬ ì •í•´ì§„ ê³ ì •ëœ ê°ì²´ ì¹´í…Œê³ ë¦¬ ì§‘í•©ì„ ì˜ˆì¸¡í•˜ë„ë¡ í›ˆë ¨ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì œí•œëœ í˜•íƒœì˜ ì§€ë„ëŠ” ë‹¤ë¥¸ ì‹œê°ì  ê°œë…ì„ ì§€ì •í•˜ê¸° ìœ„í•´ ì¶”ê°€ì ì¸ ë¼ë²¨ë§ëœ ë°ì´í„°ê°€ í•„ìš”í•˜ë¯€ë¡œ ê·¸ ì¼ë°˜ì„±ê³¼ ì‚¬ìš©ì„±ì„ ì œí•œí•©ë‹ˆë‹¤. ì´ë¯¸ì§€ ì›ì‹œ í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ í•™ìŠµí•˜ëŠ” ê²ƒì€ í›¨ì”¬ ë” ê´‘ë²”ìœ„í•œ ì§€ë„ ì†ŒìŠ¤ë¥¼ í™œìš©í•˜ëŠ” ì•„ì£¼ ì¢‹ì€ ëŒ€ì•ˆì…ë‹ˆë‹¤. ì´ë¯¸ì§€ì™€ ìº¡ì…˜ì„ ë§ì¶”ëŠ” ê°„ë‹¨í•œ ì‚¬ì „ í•™ìŠµ ì‘ì—…ì´, ì¸í„°ë„·ì—ì„œ ìˆ˜ì§‘í•œ 4ì–µ ìŒì˜ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ì—ì„œ SOTA ìˆ˜ì¤€ì˜ ì´ë¯¸ì§€ í‘œí˜„ì„ ì²˜ìŒë¶€í„° íš¨ìœ¨ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•˜ê²Œ í•™ìŠµí•˜ëŠ” ë°©ë²•ì„ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‚¬ì „ í›ˆë ¨ í›„, ìì—°ì–´ëŠ” í•™ìŠµëœ ì‹œê°ì  ê°œë…ì„ ì°¸ì¡°í•˜ê±°ë‚˜ ìƒˆë¡œìš´ ê°œë…ì„ ì„¤ëª…í•˜ëŠ” ë° ì‚¬ìš©ë˜ì–´ ëª¨ë¸ì˜ í•˜ìœ„ ì‘ì—…ìœ¼ë¡œì˜ ì œë¡œìƒ· ì „ì´ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” OCR, ë¹„ë””ì˜¤ ë‚´ í–‰ë™ ì¸ì‹, ì§€ë¦¬ì  ìœ„ì¹˜ íŒŒì•…, ê·¸ë¦¬ê³  ë§ì€ ì¢…ë¥˜ì˜ ì„¸ë°€í•œ ê°ì²´ ë¶„ë¥˜ ë“± 30ê°œ ì´ìƒì˜ ë‹¤ì–‘í•œ ê¸°ì¡´ ì»´í“¨í„° ë¹„ì „ ë°ì´í„°ì…‹ì— ëŒ€í•œ ë²¤ì¹˜ë§ˆí‚¹ì„ í†µí•´ ì´ ì ‘ê·¼ ë°©ì‹ì˜ ì„±ëŠ¥ì„ ì—°êµ¬í•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ëŒ€ë¶€ë¶„ì˜ ì‘ì—…ì— ëŒ€í•´ ì˜ë¯¸ ìˆê²Œ ì „ì´ë˜ë©°, ì¢…ì¢… ë°ì´í„°ì…‹ë³„ í›ˆë ¨ ì—†ì´ë„ ì™„ì „ ì§€ë„ í•™ìŠµ ê¸°ì¤€ì„ ê³¼ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ImageNetì—ì„œ ì›ë˜ ResNet-50ì˜ ì •í™•ë„ë¥¼ ì œë¡œìƒ·ìœ¼ë¡œ ì¼ì¹˜ì‹œí‚¤ëŠ”ë°, ì´ëŠ” ResNet-50ì´ í›ˆë ¨ëœ 128ë§Œ ê°œì˜ í›ˆë ¨ ì˜ˆì œë¥¼ ì „í˜€ ì‚¬ìš©í•  í•„ìš”ê°€ ì—†ì—ˆìŠµë‹ˆë‹¤. ì½”ë“œ ë° ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ëŠ” ì´ https URLì—ì„œ ê³µê°œí•©ë‹ˆë‹¤.*

ì´ ëª¨ë¸ì€ [valhalla](https://huggingface.co/valhalla)ì— ì˜í•´ ê¸°ì—¬ë˜ì—ˆìŠµë‹ˆë‹¤. 
ì›ë³¸ ì½”ë“œëŠ” [ì´ê³³](https://github.com/openai/CLIP)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ì‚¬ìš© íŒê³¼ ì˜ˆì‹œ[[usage-tips-and-example]]

CLIPì€ ë©€í‹°ëª¨ë‹¬ ë¹„ì „ ë°’ ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤. ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°ê³¼ ì œë¡œìƒ· ì´ë¯¸ì§€ ë¶„ë¥˜ì— ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. CLIPì€ ViTì™€ ìœ ì‚¬í•œ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê°ì  íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê³ , ì¸ê³¼ì  ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ íŠ¹ì§•ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. ê·¸ í›„ í…ìŠ¤íŠ¸ì™€ ì‹œê°ì  íŠ¹ì§• ëª¨ë‘ ë™ì¼í•œ ì°¨ì›ì˜ ì ì¬(latent) ê³µê°„ìœ¼ë¡œ íˆ¬ì˜ë©ë‹ˆë‹¤. íˆ¬ì˜ëœ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ íŠ¹ì§• ì‚¬ì´ì˜ ë‚´ì ì´ ìœ ì‚¬ë„ ì ìˆ˜ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.

íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”ì— ì´ë¯¸ì§€ë¥¼ ì…ë ¥í•˜ê¸° ìœ„í•´, ê° ì´ë¯¸ì§€ëŠ” ê³ ì • í¬ê¸°ì˜ ê²¹ì¹˜ì§€ ì•ŠëŠ” íŒ¨ì¹˜ë“¤ì˜ ì‹œí€€ìŠ¤ë¡œ ë¶„í• ë˜ê³ , ì´í›„ ì„ í˜• ì„ë² ë”©ë©ë‹ˆë‹¤. [CLS]í† í°ì´ ì „ì²´ ì´ë¯¸ì§€ì˜ í‘œí˜„ìœ¼ë¡œ ì¶”ê°€ë©ë‹ˆë‹¤. ì €ìë“¤ì€ ë˜í•œ ì ˆëŒ€ ìœ„ì¹˜ ì„ë² ë”©ì„ ì¶”ê°€í•˜ê³ , ê²°ê³¼ë¡œ ë‚˜ì˜¨ ë²¡í„° ì‹œí€€ìŠ¤ë¥¼ í‘œì¤€ íŠ¸ëœìŠ¤í¬ë¨¸ ì¸í† ë”ì— ì…ë ¥í•©ë‹ˆë‹¤. [`CLIPImageProcessor`]ëŠ” ëª¨ë¸ì„ ìœ„í•´ ì´ë¯¸ì§€ë¥¼ ë¦¬ì‚¬ì´ì¦ˆ(ë˜ëŠ” ì¬ìŠ¤ìºì¼ë§)í•˜ê³  ì •ê·œí™”í•˜ëŠ”ë° ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

[`CLIPTokenizer`]ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì¸ì½”ë”©í•˜ëŠ”ë° ì‚¬ìš©ë©ë‹ˆë‹¤. [`CLIPProcessor`]ëŠ” [`CLIPImageProcessor`]ì™€ [`CLIPTokenizer`]ë¥¼ í•˜ë‚˜ì˜ ì¸ìŠ¤í„´ìŠ¤ë¡œ ê°ì‹¸ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¸ì½”ë”©í•˜ê³  ì´ë¯¸ì§€ë¥¼ ì¤€ë¹„í•˜ëŠ”ë° ëª¨ë‘ ì‚¬ìš©ë©ë‹ˆë‹¤. 

ë‹¤ìŒ ì˜ˆì‹œëŠ” [`CLIPProcessor`]ì™€ [`CLIPModel`]ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ì–»ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.


```python
>>> from PIL import Image
>>> import requests

>>> from transformers import CLIPProcessor, CLIPModel

>>> model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
>>> processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(text=["a photo of a cat", "a photo of a dog"], images=image, return_tensors="pt", padding=True)

>>> outputs = model(**inputs)
>>> logits_per_image = outputs.logits_per_image  # ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìœ ì‚¬ì„± ì ìˆ˜
>>> probs = logits_per_image.softmax(dim=1)  # í™•ë¥ ì„ ë ˆì´ë¸”ë§ í•˜ê¸°ìœ„í•´ì„œ ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ì·¨í•©ë‹ˆë‹¤.
```


### CLIPê³¼ í”Œë˜ì‹œ ì–´í…ì…˜2 ê²°í•©[[combining-clip-and-flash-attention-2]]

ë¨¼ì € ìµœì‹ ë²„ì „ì˜ í”Œë˜ì‹œ ì–´í…ì…˜2ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.

```bash
pip install -U flash-attn --no-build-isolation
```

í”Œë˜ì‹œ ì–´í…ì…˜2ì™€ í˜¸í™˜ë˜ëŠ” í•˜ë“œì›¨ì–´ë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. ì´ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ flash-attn ë¦¬í¬ì§€í† ë¦¬ì˜ ê³µì‹ë¬¸ì„œì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ ëª¨ë¸ì„ ë°˜ì •ë°€ë„(`torch.float16`)ë¡œ ë¡œë“œí•˜ëŠ” ê²ƒì„ ìŠì§€ ë§ˆì„¸ìš”.

<Tip warning={true}>

ì‘ì€ ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•  ë•Œ, í”Œë˜ì‹œ ì–´í…ì…˜ì„ ì‚¬ìš©í•˜ë©´ ëª¨ë¸ì´ ëŠë ¤ì§€ëŠ” ê²ƒì„ ëŠë‚„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.ì•„ë˜ì˜ [í”Œë˜ì‹œ ì–´í…ì…˜ê³¼ SDPAë¥¼ ì‚¬ìš©í•œ ì˜ˆìƒ ì†ë„ í–¥ìƒ](#Expected-speedups-with-Flash-Attention-and-SDPA) ì„¹ì…˜ì„ ì°¸ì¡°í•˜ì—¬ ì ì ˆí•œ ì–´í…ì…˜ êµ¬í˜„ì„ ì„ íƒí•˜ì„¸ìš”.

</Tip>

í”Œë˜ì‹œ ì–´í…ì…˜2ë¥¼ ì‚¬ìš©í•´ì„œ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  êµ¬ë™í•˜ê¸° ìœ„í•´ì„œ ë‹¤ìŒ ìŠ¤ë‹ˆí«ì„ ì°¸ê³ í•˜ì„¸ìš”:

```python
>>> import torch
>>> import requests
>>> from PIL import Image

>>> from transformers import CLIPProcessor, CLIPModel

>>> device = "cuda"
>>> dtype = torch.float16

>>> model = CLIPModel.from_pretrained(
...     "openai/clip-vit-base-patch32",
...     attn_implementation="flash_attention_2",
...     device_map=device,
...     dtype=dtype,
... )
>>> processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(text=["a photo of a cat", "a photo of a dog"], images=image, return_tensors="pt", padding=True)
>>> inputs.to(device)

>>> with torch.no_grad():
...     with torch.autocast(device):
...         outputs = model(**inputs)

>>> logits_per_image = outputs.logits_per_image  # ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìœ ì‚¬ì„± ì ìˆ˜
>>> probs = logits_per_image.softmax(dim=1)  # í™•ë¥ ì„ ë ˆì´ë¸”ë§ í•˜ê¸°ìœ„í•´ì„œ ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ì·¨í•©ë‹ˆë‹¤.
>>> print(probs)
tensor([[0.9946, 0.0052]], device='cuda:0', dtype=torch.float16)
```


### ìŠ¤ì¼€ì¼ëœ ë‚´ì  ì–´í…ì…˜ (Scaled dot-product Attention(SDPA)) ì‚¬ìš©í•˜ê¸°[[using-scaled-dot-product-attention-sdpa]]

íŒŒì´í† ì¹˜ëŠ” `torch.nn.functional`ì˜ ì¼ë¶€ë¡œ ë„¤ì´í‹°ë¸Œ ìŠ¤ì¼€ì¼ëœ ë‚´ì  ì–´í…ì…˜(SPDA) ì—°ì‚°ìë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ì…ë ¥ê³¼ ì‚¬ìš© ì¤‘ì¸ í•˜ë“œì›¨ì–´ì— ë”°ë¼ ì ìš©ë  ìˆ˜ ìˆëŠ” ì—¬ëŸ¬ êµ¬í˜„ì„ í¬í•¨í•©ë‹ˆë‹¤. ìì„¸í•œ ì •ë³´ëŠ” [ê³µì‹ë¬¸ì„œ](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)ë‚˜ [GPU ì¶”ë¡ ](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention) í˜ì´ì§€ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

`torch>=2.1.1`ì—ì„œëŠ” êµ¬í˜„ì´ ê°€ëŠ¥í•  ë•Œ SDPAê°€ ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©ë˜ì§€ë§Œ, `from_pretrained()` í•¨ìˆ˜ì—ì„œ `attn_implementation="sdpa"`ë¥¼ ì„¤ì •í•˜ì—¬ SDPAë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì‚¬ìš©í•˜ë„ë¡ ìš”ì²­í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

```python
from transformers import CLIPModel

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32", dtype=torch.float16, attn_implementation="sdpa")
```

ìµœê³ ì˜ ì†ë„í–¥ìƒì„ ìœ„í•´ì„œ, ë°˜ì •ë°€ë„ë¡œ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ê²ƒì„ ì¶”ì²œí•©ë‹ˆë‹¤. (ì˜ˆë¥¼ë“¤ë©´ `torch.float16` ë˜ëŠ” `torch.bfloat16`).

### í”Œë˜ì‹œ ì–´í…ì…˜ê³¼ ìŠ¤ì¼€ì¼ëœ ë‚´ì  ì–´í…ì…˜(SDPA)ìœ¼ë¡œ ì¸í•´ ì˜ˆìƒë˜ëŠ” ì†ë„í–¥ìƒ[[expected-speedups-with-flash-attention-and-sdpa]]

ë¡œì»¬ ë²¤ì¹˜ë§ˆí¬(NVIDIA A10G, PyTorch 2.3.1+cu121)ì—ì„œ `float16`ì„ ì‚¬ìš©í•˜ì—¬ `"openai/clip-vit-large-patch14"` ì²´í¬í¬ì¸íŠ¸ë¡œ ì¶”ë¡ ì„ ìˆ˜í–‰í–ˆì„ ë•Œ, ë‹¤ìŒê³¼ ê°™ì€ ì†ë„ í–¥ìƒì„ í™•ì¸ í–ˆìŠµë‹ˆë‹¤.
[ì½”ë“œ](https://gist.github.com/qubvel/ac691a54e54f9fae8144275f866a7ff8):

#### CLIPTextModel[[cliptextmodel]]

|   Num text labels |   Eager (s/iter) |   FA2 (s/iter) |   FA2 speedup |   SDPA (s/iter) |   SDPA speedup |
|------------------:|-----------------:|---------------:|--------------:|----------------:|---------------:|
|                 4 |            0.009 |          0.012 |         0.737 |           0.007 |          1.269 |
|                16 |            0.009 |          0.014 |         0.659 |           0.008 |          1.187 |
|                32 |            0.018 |          0.021 |         0.862 |           0.016 |          1.142 |
|                64 |            0.034 |          0.034 |         1.001 |           0.03  |          1.163 |
|               128 |            0.063 |          0.058 |         1.09  |           0.054 |          1.174 |

![clip_text_model_viz_3](https://github.com/user-attachments/assets/e9826b43-4e66-4f4c-952b-af4d90bd38eb)

#### CLIPVisionModel[[clipvisionmodel]]

|   Image batch size |   Eager (s/iter) |   FA2 (s/iter) |   FA2 speedup |   SDPA (s/iter) |   SDPA speedup |
|-------------------:|-----------------:|---------------:|--------------:|----------------:|---------------:|
|                  1 |            0.016 |          0.013 |         1.247 |           0.012 |          1.318 |
|                  4 |            0.025 |          0.021 |         1.198 |           0.021 |          1.202 |
|                 16 |            0.093 |          0.075 |         1.234 |           0.075 |          1.24  |
|                 32 |            0.181 |          0.147 |         1.237 |           0.146 |          1.241 |

![clip_image_model_viz_3](https://github.com/user-attachments/assets/50a36206-e3b9-4adc-ac8e-926b8b071d63)

#### CLIPModel[[clipmodel]]

|   Image batch size |   Num text labels |   Eager (s/iter) |   FA2 (s/iter) |   FA2 speedup |   SDPA (s/iter) |   SDPA speedup |
|-------------------:|------------------:|-----------------:|---------------:|--------------:|----------------:|---------------:|
|                  1 |                 4 |            0.025 |          0.026 |         0.954 |           0.02  |          1.217 |
|                  1 |                16 |            0.026 |          0.028 |         0.918 |           0.02  |          1.287 |
|                  1 |                64 |            0.042 |          0.046 |         0.906 |           0.036 |          1.167 |
|                  4 |                 4 |            0.028 |          0.033 |         0.849 |           0.024 |          1.189 |
|                  4 |                16 |            0.034 |          0.035 |         0.955 |           0.029 |          1.169 |
|                  4 |                64 |            0.059 |          0.055 |         1.072 |           0.05  |          1.179 |
|                 16 |                 4 |            0.096 |          0.088 |         1.091 |           0.078 |          1.234 |
|                 16 |                16 |            0.102 |          0.09  |         1.129 |           0.083 |          1.224 |
|                 16 |                64 |            0.127 |          0.11  |         1.157 |           0.105 |          1.218 |
|                 32 |                 4 |            0.185 |          0.159 |         1.157 |           0.149 |          1.238 |
|                 32 |                16 |            0.19  |          0.162 |         1.177 |           0.154 |          1.233 |
|                 32 |                64 |            0.216 |          0.181 |         1.19  |           0.176 |          1.228 |

## ìë£Œ[[resources]]

CLIPì„ ì‹œì‘í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” Hugging Faceì™€ community ìë£Œ ëª©ë¡(ğŸŒë¡œ í‘œì‹œë¨) ì…ë‹ˆë‹¤.

- [ì›ê²© ì„¼ì‹± (ì¸ê³µìœ„ì„±) ì´ë¯¸ì§€ì™€ ìº¡ì…˜ì„ ê°€ì§€ê³  CLIP ë¯¸ì„¸ì¡°ì •í•˜ê¸°](https://huggingface.co/blog/fine-tune-clip-rsicd): 
[RSICD dataset](https://github.com/201528014227051/RSICD_optimal)ì„ ê°€ì§€ê³  CLIPì„ ë¯¸ì„¸ì¡°ì • í•˜ëŠ” ë°©ë²•ê³¼ ë°ì´í„° ì¦ê°•ì— ëŒ€í•œ ì„±ëŠ¥ ë¹„êµì— ëŒ€í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸
- ì´ [ì˜ˆì‹œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/pytorch/contrastive-image-text)ëŠ” [COCO dataset](https://cocodataset.org/#home)ë¥¼ ì´ìš©í•œ ì‚¬ì „í•™ìŠµëœ ë¹„ì „ê³¼ í…ìŠ¤íŠ¸ì™€ ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•´ì„œ CLIPê°™ì€ ë¹„ì „-í…ìŠ¤íŠ¸ ë“€ì–¼ ëª¨ë¸ì„ ì–´ë–»ê²Œ í•™ìŠµì‹œí‚¤ëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤. 

<PipelineTag pipeline="image-to-text"/>

- ì‚¬ì „í•™ìŠµëœ CLIPëª¨ë¸ì„ ì´ë¯¸ì§€ ìº¡ì…”ë‹ì„ ìœ„í•œ ë¹”ì„œì¹˜ ì¶”ë¡ ì— ì–´ë–»ê²Œ í™œìš©í•˜ëŠ”ì§€ì— ê´€í•œ [ë…¸íŠ¸ë¶](https://colab.research.google.com/drive/1tuoAC5F4sC7qid56Z0ap-stR3rwdk0ZV?usp=sharing)

**ì´ë¯¸ì§€ ê²€ìƒ‰**

- ì‚¬ì „í•™ìŠµëœ CLIPëª¨ë¸ê³¼ MRR(Mean Reciprocal Rank) ì ìˆ˜ ì—°ì‚°ì„ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ê²€ìƒ‰ì— ëŒ€í•œ [ë…¸íŠ¸ë¶](https://colab.research.google.com/drive/1bLVwVKpAndpEDHqjzxVPr_9nGrSbuOQd?usp=sharing). ğŸŒ
- ì´ë¯¸ì§€ ê²€ìƒ‰ê³¼ ìœ ì‚¬ì„± ì ìˆ˜ì— ëŒ€í•´ ë³´ì—¬ì£¼ëŠ” [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/deep-diver/image_search_with_natural_language/blob/main/notebooks/Image_Search_CLIP.ipynb). ğŸŒ
- Multilingual CLIPë¥¼ ì‚¬ìš©í•´ì„œ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ì–´ë–»ê²Œ ê°™ì€ ë²¡í„° ê³µê°„ì— ë§¤í•‘ ì‹œí‚¤ëŠ”ì§€ì— ëŒ€í•œ [ë…¸íŠ¸ë¶](https://colab.research.google.com/drive/1xO-wC_m_GNzgjIBQ4a4znvQkvDoZJvH4?usp=sharing). ğŸŒ 
- [Unsplash](https://unsplash.com)ì™€ [TMDB](https://www.themoviedb.org/) ë°ì´í„°ì…‹ì„ í™œìš©í•œ ì˜ë¯¸ë¡ ì (semantic) ì´ë¯¸ì§€ ê²€ìƒ‰ì—ì„œ CLIPì„ êµ¬ë™í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/vivien000/clip-demo/blob/master/clip.ipynb#scrollTo=uzdFhRGqiWkR). ğŸŒ

**ì„¤ëª… ê°€ëŠ¥ì„±**

- ì…ë ¥ í† í°ê³¼ ì´ë¯¸ì§€ ì¡°ê°(segment) ì‚¬ì´ì˜ ìœ ì‚¬ì„±ì„ ì‹œê°í™” ì‹œí‚¤ëŠ” ë°©ë²•ì— ëŒ€í•œ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/hila-chefer/Transformer-MM-Explainability/blob/main/CLIP_explainability.ipynb). ğŸŒ

ì—¬ê¸°ì— í¬í•¨ë  ìë£Œë¥¼ ì œì¶œí•˜ê³  ì‹¶ìœ¼ì‹œë‹¤ë©´ PR(Pull Request)ë¥¼ ì—´ì–´ì£¼ì„¸ìš”. ë¦¬ë·° í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤! ìë£ŒëŠ” ê¸°ì¡´ ìë£Œë¥¼ ë³µì œí•˜ëŠ” ëŒ€ì‹  ìƒˆë¡œìš´ ë‚´ìš©ì„ ë‹´ê³  ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

## CLIPConfig[[transformers.CLIPConfig]]

[[autodoc]] CLIPConfig
    - from_text_vision_configs

## CLIPTextConfig[[transformers.CLIPTextConfig]]

[[autodoc]] CLIPTextConfig

## CLIPVisionConfig[[transformers.CLIPVisionConfig]]

[[autodoc]] CLIPVisionConfig

## CLIPTokenizer[[transformers.CLIPTokenizer]]

[[autodoc]] CLIPTokenizer
    - build_inputs_with_special_tokens
    - get_special_tokens_mask
    - create_token_type_ids_from_sequences
    - save_vocabulary

## CLIPTokenizerFast[[transformers.CLIPTokenizerFast]]

[[autodoc]] CLIPTokenizerFast

## CLIPImageProcessor[[transformers.CLIPImageProcessor]]

[[autodoc]] CLIPImageProcessor
    - preprocess

## CLIPFeatureExtractor[[transformers.CLIPFeatureExtractor]]

[[autodoc]] CLIPFeatureExtractor

## CLIPProcessor[[transformers.CLIPProcessor]]

[[autodoc]] CLIPProcessor

<frameworkcontent>
<pt>

## CLIPModel[[transformers.CLIPModel]]

[[autodoc]] CLIPModel
    - forward
    - get_text_features
    - get_image_features

## CLIPTextModel[[transformers.CLIPTextModel]]

[[autodoc]] CLIPTextModel
    - forward

## CLIPTextModelWithProjection[[transformers.CLIPTextModelWithProjection]]

[[autodoc]] CLIPTextModelWithProjection
    - forward

## CLIPVisionModelWithProjection[[transformers.CLIPVisionModelWithProjection]]

[[autodoc]] CLIPVisionModelWithProjection
    - forward

## CLIPVisionModel[[transformers.CLIPVisionModel]]

[[autodoc]] CLIPVisionModel
    - forward

## CLIPForImageClassification[[transformers.CLIPForImageClassification]]

[[autodoc]] CLIPForImageClassification
    - forward

</pt>
<tf>

## TFCLIPModel[[transformers.TFCLIPModel]]

[[autodoc]] TFCLIPModel
    - call
    - get_text_features
    - get_image_features

## TFCLIPTextModel[[transformers.TFCLIPTextModel]]

[[autodoc]] TFCLIPTextModel
    - call

## TFCLIPVisionModel[[transformers.TFCLIPVisionModel]]

[[autodoc]] TFCLIPVisionModel
    - call

</tf>
<jax>

## FlaxCLIPModel[[transformers.FlaxCLIPModel]]

[[autodoc]] FlaxCLIPModel
    - __call__
    - get_text_features
    - get_image_features

## FlaxCLIPTextModel[[transformers.FlaxCLIPTextModel]]

[[autodoc]] FlaxCLIPTextModel
    - __call__

## FlaxCLIPTextModelWithProjection[[transformers.FlaxCLIPTextModelWithProjection]]

[[autodoc]] FlaxCLIPTextModelWithProjection
    - __call__

## FlaxCLIPVisionModel[[transformers.FlaxCLIPVisionModel]]

[[autodoc]] FlaxCLIPVisionModel
    - __call__

</jax>
</frameworkcontent>
