<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

<div style="float: right;">
    <div class="flex flex-wrap space-x-1">
        <img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white" >
        <img alt= "TensorFlow" src= "https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white" >
        <img alt= "Flax" src="https://img.shields.io/badge/Flax-29a79b.svg?styleâ€¦Nu+W0m6K/I9gGPd/dfx/EN/wN62AhsBWuAAAAAElFTkSuQmCC">
        <img alt="SDPA" src= "https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white" > 
    </div>
</div>

# ALBERT[[albert]]

[ALBERT](https://huggingface.co/papers/1909.11942)ëŠ” [BERT](./bert)ì˜ í™•ì¥ì„±ê³¼ í•™ìŠµ ì‹œ ë©”ëª¨ë¦¬ í•œê³„ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ëª¨ë¸ì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ë‘ ê°€ì§€ íŒŒë¼ë¯¸í„° ê°ì†Œ ê¸°ë²•ì„ ë„ì…í•©ë‹ˆë‹¤. ì²« ë²ˆì§¸ëŠ” ì„ë² ë”© í–‰ë ¬ ë¶„í•´(factorized embedding parametrization)ë¡œ, í° ì–´íœ˜ ì„ë² ë”© í–‰ë ¬ì„ ë‘ ê°œì˜ ì‘ì€ í–‰ë ¬ë¡œ ë¶„í•´í•˜ì—¬ íˆë“  ì‚¬ì´ì¦ˆë¥¼ ëŠ˜ë ¤ë„ íŒŒë¼ë¯¸í„° ìˆ˜ê°€ í¬ê²Œ ì¦ê°€í•˜ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤. ë‘ ë²ˆì§¸ëŠ” ê³„ì¸µ ê°„ íŒŒë¼ë¯¸í„° ê³µìœ (cross-layer parameter sharing)ë¡œ, ì—¬ëŸ¬ ê³„ì¸µì´ íŒŒë¼ë¯¸í„°ë¥¼ ê³µìœ í•˜ì—¬ í•™ìŠµí•´ì•¼ í•  íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ì¤„ì…ë‹ˆë‹¤.

ALBERTëŠ” BERTì—ì„œ ë°œìƒí•˜ëŠ” GPU/TPU ë©”ëª¨ë¦¬ í•œê³„, ê¸´ í•™ìŠµ ì‹œê°„, ê°‘ì‘ìŠ¤ëŸ° ì„±ëŠ¥ ì €í•˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë§Œë“¤ì–´ì¡ŒìŠµë‹ˆë‹¤. ALBERTëŠ” íŒŒë¼ë¯¸í„°ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ë‘ ê°€ì§€ ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê³  BERTì˜ í•™ìŠµ ì†ë„ë¥¼ ë†’ì…ë‹ˆë‹¤:

- **ì„ë² ë”© í–‰ë ¬ ë¶„í•´:** í° ì–´íœ˜ ì„ë² ë”© í–‰ë ¬ì„ ë‘ ê°œì˜ ë” ì‘ì€ í–‰ë ¬ë¡œ ë¶„í•´í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì…ë‹ˆë‹¤.
- **ê³„ì¸µ ê°„ íŒŒë¼ë¯¸í„° ê³µìœ :** ê° íŠ¸ëœìŠ¤í¬ë¨¸ ê³„ì¸µë§ˆë‹¤ ë³„ë„ì˜ íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµí•˜ëŠ” ëŒ€ì‹ , ì—¬ëŸ¬ ê³„ì¸µì´ íŒŒë¼ë¯¸í„°ë¥¼ ê³µìœ í•˜ì—¬ í•™ìŠµí•´ì•¼ í•  ê°€ì¤‘ì¹˜ ìˆ˜ë¥¼ ë”ìš± ì¤„ì…ë‹ˆë‹¤.

ALBERTëŠ” BERTì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì ˆëŒ€ ìœ„ì¹˜ ì„ë² ë”©(absolute position embeddings)ì„ ì‚¬ìš©í•˜ë¯€ë¡œ, ì…ë ¥ íŒ¨ë”©ì€ ì˜¤ë¥¸ìª½ì— ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤. ì„ë² ë”© í¬ê¸°ëŠ” 128ì´ë©°, BERTì˜ 768ë³´ë‹¤ ì‘ìŠµë‹ˆë‹¤. ALBERTëŠ” í•œ ë²ˆì— ìµœëŒ€ 512ê°œì˜ í† í°ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ëª¨ë“  ê³µì‹ ALBERT ì²´í¬í¬ì¸íŠ¸ëŠ” [ALBERT ì»¤ë®¤ë‹ˆí‹°](https://huggingface.co/albert) ì¡°ì§ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> [!TIP]
> ì˜¤ë¥¸ìª½ ì‚¬ì´ë“œë°”ì˜ ALBERT ëª¨ë¸ì„ í´ë¦­í•˜ì‹œë©´ ë‹¤ì–‘í•œ ì–¸ì–´ ì‘ì—…ì— ALBERTë¥¼ ì ìš©í•˜ëŠ” ì˜ˆì‹œë¥¼ ë” í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì•„ë˜ ì˜ˆì‹œëŠ” [`Pipeline`], [`AutoModel`] ê·¸ë¦¬ê³  ì»¤ë§¨ë“œë¼ì¸ì—ì„œ `[MASK]` í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

<hfoptions id="usage">
<hfoption id="Pipeline">

```py
import torch
from transformers import pipeline

pipeline = pipeline(
    task="fill-mask",
    model="albert-base-v2",
    torch_dtype=torch.float16,
    device=0
)
pipeline("ì‹ë¬¼ì€ ê´‘í•©ì„±ì´ë¼ê³  ì•Œë ¤ì§„ ê³¼ì •ì„ í†µí•´ [MASK]ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.", top_k=5)
```

</hfoption>
<hfoption id="AutoModel">

```py
import torch
from transformers import AutoModelForMaskedLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("albert/albert-base-v2")
model = AutoModelForMaskedLM.from_pretrained(
    "albert/albert-base-v2",
    torch_dtype=torch.float16,
    attn_implementation="sdpa",
    device_map="auto"
)

prompt = "ì‹ë¬¼ì€ [MASK]ì´ë¼ê³  ì•Œë ¤ì§„ ê³¼ì •ì„ í†µí•´ ì—ë„ˆì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

with torch.no_grad():
    outputs = model(**inputs)
    mask_token_index = torch.where(inputs["input_ids"] == tokenizer.mask_token_id)[1]
    predictions = outputs.logits[0, mask_token_index]

top_k = torch.topk(predictions, k=5).indices.tolist()
for token_id in top_k[0]:
    print(f"ì˜ˆì¸¡: {tokenizer.decode([token_id])}")
```

</hfoption>
<hfoption id="transformers CLI">

```bash
echo -e "Plants create [MASK] through a process known as photosynthesis." | transformers run --task fill-mask --model albert-base-v2 --device 0
```

</hfoption>

</hfoptions>

## ì°¸ê³  ì‚¬í•­[[notes]]

- BERTëŠ” ì ˆëŒ€ ìœ„ì¹˜ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ë¯€ë¡œ, ì˜¤ë¥¸ìª½ì— ì…ë ¥ì´ íŒ¨ë”©ë¼ì•¼ í•©ë‹ˆë‹¤.
- ì„ë² ë”© í¬ê¸° `E`ëŠ” íˆë“  í¬ê¸° `H`ì™€ ë‹¤ë¦…ë‹ˆë‹¤. ì„ë² ë”©ì€ ë¬¸ë§¥ì— ë…ë¦½ì (ê° í† í°ë§ˆë‹¤ í•˜ë‚˜ì˜ ì„ë² ë”© ë²¡í„°)ì´ê³ , ì€ë‹‰ ìƒíƒœëŠ” ë¬¸ë§¥ì— ì˜ì¡´ì (í† í° ì‹œí€€ìŠ¤ë§ˆë‹¤ í•˜ë‚˜ì˜ ì€ë‹‰ ìƒíƒœ)ì…ë‹ˆë‹¤. ì„ë² ë”© í–‰ë ¬ì€ `V x E`(V: ì–´íœ˜ í¬ê¸°)ì´ë¯€ë¡œ, ì¼ë°˜ì ìœ¼ë¡œ `H >> E`ê°€ ë” ë…¼ë¦¬ì ì…ë‹ˆë‹¤. `E < H`ì¼ ë•Œ ëª¨ë¸ íŒŒë¼ë¯¸í„°ê°€ ë” ì ì–´ì§‘ë‹ˆë‹¤.

## ì°¸ê³  ìë£Œ[[resources]]

ì•„ë˜ ì„¹ì…˜ì˜ ìë£Œë“¤ì€ ê³µì‹ Hugging Face ë° ì»¤ë®¤ë‹ˆí‹°(ğŸŒ í‘œì‹œ) ìë£Œë¡œ, AlBERTë¥¼ ì‹œì‘í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤. ì—¬ê¸°ì— ì¶”ê°€í•  ìë£Œê°€ ìˆë‹¤ë©´ Pull Requestë¥¼ ë³´ë‚´ì£¼ì„¸ìš”! ê¸°ì¡´ ìë£Œì™€ ì¤‘ë³µë˜ì§€ ì•Šê³  ìƒˆë¡œìš´ ë‚´ìš©ì„ ë‹´ê³  ìˆìœ¼ë©´ ì¢‹ìŠµë‹ˆë‹¤.

<PipelineTag pipeline="text-classification"/>

- [`AlbertForSequenceClassification`]ì€ ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.

- [`TFAlbertForSequenceClassification`]ì€ ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.

- [`FlaxAlbertForSequenceClassification`]ì€ ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/flax/text-classification)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_flax.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- [í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì‘ì—… ê°€ì´ë“œ](../tasks/sequence_classification)ì—ì„œ ëª¨ë¸ ì‚¬ìš©ë²•ì„ í™•ì¸í•˜ì„¸ìš”.

<PipelineTag pipeline="token-classification"/>

- [`AlbertForTokenClassification`]ì€ ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.

- [`TFAlbertForTokenClassification`]ì€ ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/token-classification)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.

- [`FlaxAlbertForTokenClassification`]ì€ ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/flax/token-classification)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- ğŸ¤— Hugging Faceì˜ [í† í° ë¶„ë¥˜](https://huggingface.co/course/chapter7/2?fw=pt) ê°•ì¢Œ
- [í† í° ë¶„ë¥˜ ì‘ì—… ê°€ì´ë“œ](../tasks/token_classification)ì—ì„œ ëª¨ë¸ ì‚¬ìš©ë²•ì„ í™•ì¸í•˜ì„¸ìš”.

<PipelineTag pipeline="fill-mask"/>

- [`AlbertForMaskedLM`]ì€ ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#robertabertdistilbert-and-masked-language-modeling)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- [`TFAlbertForMaskedLM`]ì€ ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_mlmpy)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- [`FlaxAlbertForMaskedLM`]ì€ ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#masked-language-modeling)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked_language_modeling_flax.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- ğŸ¤— Hugging Faceì˜ [ë§ˆìŠ¤í‚¹ ì–¸ì–´ ëª¨ë¸ë§](https://huggingface.co/course/chapter7/3?fw=pt) ê°•ì¢Œ
- [ë§ˆìŠ¤í‚¹ ì–¸ì–´ ëª¨ë¸ë§ ì‘ì—… ê°€ì´ë“œ](../tasks/masked_language_modeling)ì—ì„œ ëª¨ë¸ ì‚¬ìš©ë²•ì„ í™•ì¸í•˜ì„¸ìš”.

<PipelineTag pipeline="question-answering"/>

- [`AlbertForQuestionAnswering`]ì€ ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- [`TFAlbertForQuestionAnswering`]ì€ ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- [`FlaxAlbertForQuestionAnswering`]ì€ ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/flax/question-answering)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- [ì§ˆì˜ì‘ë‹µ](https://huggingface.co/course/chapter7/7?fw=pt) ğŸ¤— Hugging Face ê°•ì¢Œì˜ ì±•í„°.
- [ì§ˆì˜ì‘ë‹µ ì‘ì—… ê°€ì´ë“œ](../tasks/question_answering)ì—ì„œ ëª¨ë¸ ì‚¬ìš©ë²•ì„ í™•ì¸í•˜ì„¸ìš”.

**ë‹¤ì¤‘ ì„ íƒ(Multiple choice)**

- [`AlbertForMultipleChoice`]ëŠ” ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.
- [`TFAlbertForMultipleChoice`]ëŠ” ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.

- [ë‹¤ì¤‘ ì„ íƒ ì‘ì—… ê°€ì´ë“œ](../tasks/multiple_choice)ì—ì„œ ëª¨ë¸ ì‚¬ìš©ë²•ì„ í™•ì¸í•˜ì„¸ìš”.

## AlbertConfig[[albertconfig]]

[[autodoc]] AlbertConfig

## AlbertTokenizer[[alberttokenizer]]

[[autodoc]] AlbertTokenizer - build_inputs_with_special_tokens - get_special_tokens_mask - create_token_type_ids_from_sequences - save_vocabulary

## AlbertTokenizerFast[[alberttokenizerfast]]

[[autodoc]] AlbertTokenizerFast

## Albert íŠ¹í™” ì¶œë ¥[[albert-specific-outputs]]

[[autodoc]] models.albert.modeling_albert.AlbertForPreTrainingOutput

[[autodoc]] models.albert.modeling_tf_albert.TFAlbertForPreTrainingOutput

<frameworkcontent>
<pt>

## AlbertModel[[albertmodel]]

[[autodoc]] AlbertModel - forward

## AlbertForPreTraining[[albertforpretraining]]

[[autodoc]] AlbertForPreTraining - forward

## AlbertForMaskedLM[[albertformaskedlm]]

[[autodoc]] AlbertForMaskedLM - forward

## AlbertForSequenceClassification[[albertforsequenceclassification]]

[[autodoc]] AlbertForSequenceClassification - forward

## AlbertForMultipleChoice[[albertformultiplechoice]]

[[autodoc]] AlbertForMultipleChoice

## AlbertForTokenClassification[[albertfortokenclassification]]

[[autodoc]] AlbertForTokenClassification - forward

## AlbertForQuestionAnswering[[albertforquestionanswering]]

[[autodoc]] AlbertForQuestionAnswering - forward

</pt>

<tf>

## TFAlbertModel[[tfalbertmodel]]

[[autodoc]] TFAlbertModel - call

## TFAlbertForPreTraining[[tfalbertforpretraining]]

[[autodoc]] TFAlbertForPreTraining - call

## TFAlbertForMaskedLM[[tfalbertformaskedlm]]

[[autodoc]] TFAlbertForMaskedLM - call

## TFAlbertForSequenceClassification[[tfalbertforsequenceclassification]]

[[autodoc]] TFAlbertForSequenceClassification - call

## TFAlbertForMultipleChoice[[tfalbertformultiplechoice]]

[[autodoc]] TFAlbertForMultipleChoice - call

## TFAlbertForTokenClassification[[tfalbertfortokenclassification]]

[[autodoc]] TFAlbertForTokenClassification - call

## TFAlbertForQuestionAnswering[[tfalbertforquestionanswering]]

[[autodoc]] TFAlbertForQuestionAnswering - call

</tf>
<jax>

## FlaxAlbertModel[[flaxalbertmodel]]

[[autodoc]] FlaxAlbertModel - **call**

## FlaxAlbertForPreTraining[[flaxalbertforpretraining]]

[[autodoc]] FlaxAlbertForPreTraining - **call**

## FlaxAlbertForMaskedLM[[flaxalbertformaskedlm]]

[[autodoc]] FlaxAlbertForMaskedLM - **call**

## FlaxAlbertForSequenceClassification[[flaxalbertforsequenceclassification]]

[[autodoc]] FlaxAlbertForSequenceClassification - **call**

## FlaxAlbertForMultipleChoice[[flaxalbertformultiplechoice]]

[[autodoc]] FlaxAlbertForMultipleChoice - **call**

## FlaxAlbertForTokenClassification[[flaxalbertfortokenclassification]]

[[autodoc]] FlaxAlbertForTokenClassification - **call**

## FlaxAlbertForQuestionAnswering[[flaxalbertforquestionanswering]]

[[autodoc]] FlaxAlbertForQuestionAnswering - **call**

</jax>
</frameworkcontent>
