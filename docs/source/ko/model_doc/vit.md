<!--Copyright 2021 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Vision Transformer (ViT) [[vision-transformer-vit]]

## ê°œìš” [[overview]]

Vision Transformer (ViT) ëª¨ë¸ì€ Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsbyê°€ ì œì•ˆí•œ ë…¼ë¬¸ [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://huggingface.co/papers/2010.11929)ì—ì„œ ì†Œê°œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” Transformer ì¸ì½”ë”ë¥¼ ImageNetì—ì„œ ì„±ê³µì ìœ¼ë¡œ í›ˆë ¨ì‹œí‚¨ ì²« ë²ˆì§¸ ë…¼ë¬¸ìœ¼ë¡œ, ê¸°ì¡´ì˜ ì˜ ì•Œë ¤ì§„ í•©ì„±ê³± ì‹ ê²½ë§(CNN) êµ¬ì¡°ì™€ ë¹„êµí•´ ë§¤ìš° ìš°ìˆ˜í•œ ê²°ê³¼ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

ë…¼ë¬¸ì˜ ì´ˆë¡ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

*Transformer ì•„í‚¤í…ì²˜ëŠ” ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì—ì„œ ì‚¬ì‹¤ìƒ í‘œì¤€ìœ¼ë¡œ ìë¦¬ ì¡ì•˜ìœ¼ë‚˜, ì»´í“¨í„° ë¹„ì „ ë¶„ì•¼ì—ì„œì˜ ì ìš©ì€ ì—¬ì „íˆ ì œí•œì ì…ë‹ˆë‹¤. ë¹„ì „ì—ì„œ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì€ ì¢…ì¢… í•©ì„±ê³± ì‹ ê²½ë§(CNN)ê³¼ ê²°í•©í•˜ì—¬ ì‚¬ìš©ë˜ê±°ë‚˜, ì „ì²´ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë©´ì„œ í•©ì„±ê³± ì‹ ê²½ë§ì˜ íŠ¹ì • êµ¬ì„± ìš”ì†Œë¥¼ ëŒ€ì²´í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ CNN ì˜ì¡´ì„±ì´ í•„ìš”í•˜ì§€ ì•Šìœ¼ë©°, ì´ë¯¸ì§€ íŒ¨ì¹˜ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì…ë ¥ë°›ëŠ” ìˆœìˆ˜í•œ Transformerê°€ ì´ë¯¸ì§€ ë¶„ë¥˜ ì‘ì—…ì—ì„œ ë§¤ìš° ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ëŒ€ê·œëª¨ ë°ì´í„°ë¡œ ì‚¬ì „ í•™ìŠµëœ í›„, ImageNet, CIFAR-100, VTAB ë“± ë‹¤ì–‘í•œ ì¤‘ì†Œí˜• ì´ë¯¸ì§€ ì¸ì‹ ë²¤ì¹˜ë§ˆí¬ì— ì ìš©í•˜ë©´ Vision Transformer(ViT)ëŠ” ìµœì‹  í•©ì„±ê³± ì‹ ê²½ë§ê³¼ ë¹„êµí•´ ë§¤ìš° ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ë©´ì„œë„ í›ˆë ¨ì— í•„ìš”í•œ ê³„ì‚° ìì›ì„ ìƒë‹¹íˆ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.*

<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vit_architecture.jpg"
alt="drawing" width="600"/>

<small> ViT ì•„í‚¤í…ì²˜. <a href="https://huggingface.co/papers/2010.11929">ì›ë³¸ ë…¼ë¬¸</a>ì—ì„œ ë°œì·Œ. </small>

ì›ë˜ì˜ Vision Transformerì— ì´ì–´, ì—¬ëŸ¬ í›„ì† ì—°êµ¬ë“¤ì´ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤:


- [DeiT](deit) (Data-efficient Image Transformers) (Facebook AI ê°œë°œ). DeiT ëª¨ë¸ì€ distilled vision transformersì…ë‹ˆë‹¤.
  DeiTì˜ ì €ìë“¤ì€ ë” íš¨ìœ¨ì ìœ¼ë¡œ í›ˆë ¨ëœ ViT ëª¨ë¸ë„ ê³µê°œí–ˆìœ¼ë©°, ì´ëŠ” [`ViTModel`] ë˜ëŠ” [`ViTForImageClassification`]ì— ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì—ëŠ” 3ê°€ì§€ í¬ê¸°ë¡œ 4ê°œì˜ ë³€í˜•ì´ ì œê³µë©ë‹ˆë‹¤: *facebook/deit-tiny-patch16-224*, *facebook/deit-small-patch16-224*, *facebook/deit-base-patch16-224* and *facebook/deit-base-patch16-384*. ê·¸ë¦¬ê³  ëª¨ë¸ì— ì´ë¯¸ì§€ë¥¼ ì¤€ë¹„í•˜ë ¤ë©´ [`DeiTImageProcessor`]ë¥¼ ì‚¬ìš©í•´ì•¼ í•œë‹¤ëŠ” ì ì— ìœ ì˜í•˜ì‹­ì‹œì˜¤.

- [BEiT](beit) (BERT pre-training of Image Transformers) (Microsoft Research ê°œë°œ). BEiT ëª¨ë¸ì€ BERT (masked image modeling)ì— ì˜ê°ì„  ë°›ê³  VQ-VAEì— ê¸°ë°˜í•œ self-supervised ë°©ë²•ì„ ì´ìš©í•˜ì—¬ supervised pre-trained vision transformersë³´ë‹¤ ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. 

- DINO (Vision Transformersì˜ self-supervised í›ˆë ¨ì„ ìœ„í•œ ë°©ë²•) (Facebook AI ê°œë°œ). DINO ë°©ë²•ìœ¼ë¡œ í›ˆë ¨ëœ Vision TransformerëŠ” í•™ìŠµë˜ì§€ ì•Šì€ ìƒíƒœì—ì„œë„ ê°ì²´ë¥¼ ë¶„í• í•  ìˆ˜ ìˆëŠ” í•©ì„±ê³± ì‹ ê²½ë§ì—ì„œëŠ” ë³¼ ìˆ˜ ì—†ëŠ” ë§¤ìš° í¥ë¯¸ë¡œìš´ ëŠ¥ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. DINO ì²´í¬í¬ì¸íŠ¸ëŠ” [hub](https://huggingface.co/models?other=dino)ì—ì„œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- [MAE](vit_mae) (Masked Autoencoders) (Facebook AI ê°œë°œ). Vision Transformerë¥¼ ë¹„ëŒ€ì¹­ ì¸ì½”ë”-ë””ì½”ë” ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë§ˆìŠ¤í¬ëœ íŒ¨ì¹˜ì˜ ë†’ì€ ë¹„ìœ¨(75%)ì—ì„œ í”½ì…€ ê°’ì„ ì¬êµ¬ì„±í•˜ë„ë¡ ì‚¬ì „ í•™ìŠµí•¨ìœ¼ë¡œì¨, ì €ìë“¤ì€ ì´ ê°„ë‹¨í•œ ë°©ë²•ì´ ë¯¸ì„¸ ì¡°ì • í›„ supervised ë°©ì‹ì˜ ì‚¬ì „ í•™ìŠµì„ ëŠ¥ê°€í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.

ì´ ëª¨ë¸ì€ [nielsr](https://huggingface.co/nielsr)ì— ì˜í•´ ê¸°ì—¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì›ë³¸ ì½”ë“œ(JAXë¡œ ì‘ì„±ë¨)ì€ [ì—¬ê¸°](https://github.com/google-research/vision_transformer)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


ì°¸ê³ ë¡œ, ìš°ë¦¬ëŠ” Ross Wightmanì˜ [timm ë¼ì´ë¸ŒëŸ¬ë¦¬](https://github.com/rwightman/pytorch-image-models)ì—ì„œ JAXì—ì„œ PyTorchë¡œ ë³€í™˜ëœ ê°€ì¤‘ì¹˜ë¥¼ ë‹¤ì‹œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤. ëª¨ë“  ê³µë¡œëŠ” ê·¸ì—ê²Œ ëŒë¦½ë‹ˆë‹¤!

## ì‚¬ìš© íŒ [[usage-tips]]

- Transformer ì¸ì½”ë”ì— ì´ë¯¸ì§€ë¥¼ ì…ë ¥í•˜ê¸° ìœ„í•´, ê° ì´ë¯¸ì§€ëŠ” ê³ ì • í¬ê¸°ì˜ ê²¹ì¹˜ì§€ ì•ŠëŠ” íŒ¨ì¹˜ë“¤ë¡œ ë¶„í• ëœ í›„ ì„ í˜• ì„ë² ë”©ë©ë‹ˆë‹¤. ì „ì²´ ì´ë¯¸ì§€ë¥¼ ëŒ€í‘œí•˜ëŠ” [CLS] í† í°ì´ ì¶”ê°€ë˜ì–´, ë¶„ë¥˜ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì €ìë“¤ì€ ë˜í•œ ì ˆëŒ€ ìœ„ì¹˜ ì„ë² ë”©ì„ ì¶”ê°€í•˜ì—¬, ê²°ê³¼ì ìœ¼ë¡œ ìƒì„±ëœ ë²¡í„° ì‹œí€€ìŠ¤ë¥¼ í‘œì¤€ Transformer ì¸ì½”ë”ì— ì…ë ¥í•©ë‹ˆë‹¤.
- Vision TransformerëŠ” ëª¨ë“  ì´ë¯¸ì§€ê°€ ë™ì¼í•œ í¬ê¸°(í•´ìƒë„)ì—¬ì•¼ í•˜ë¯€ë¡œ, [ViTImageProcessor]ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ëª¨ë¸ì— ë§ê²Œ ë¦¬ì‚¬ì´ì¦ˆ(ë˜ëŠ” ë¦¬ìŠ¤ì¼€ì¼)í•˜ê³  ì •ê·œí™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì‚¬ì „ í•™ìŠµì´ë‚˜ ë¯¸ì„¸ ì¡°ì • ì‹œ ì‚¬ìš©ëœ íŒ¨ì¹˜ í•´ìƒë„ì™€ ì´ë¯¸ì§€ í•´ìƒë„ëŠ” ê° ì²´í¬í¬ì¸íŠ¸ì˜ ì´ë¦„ì— ë°˜ì˜ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, `google/vit-base-patch16-224`ëŠ” íŒ¨ì¹˜ í•´ìƒë„ê°€ 16x16ì´ê³  ë¯¸ì„¸ ì¡°ì • í•´ìƒë„ê°€ 224x224ì¸ ê¸°ë³¸ í¬ê¸° ì•„í‚¤í…ì²˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ëª¨ë“  ì²´í¬í¬ì¸íŠ¸ëŠ” [hub](https://huggingface.co/models?search=vit)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì²´í¬í¬ì¸íŠ¸ëŠ” (1) [ImageNet-21k](http://www.image-net.org/) (1,400ë§Œ ê°œì˜ ì´ë¯¸ì§€ì™€ 21,000ê°œì˜ í´ë˜ìŠ¤)ì—ì„œë§Œ ì‚¬ì „ í•™ìŠµë˜ì—ˆê±°ë‚˜, ë˜ëŠ” (2) [ImageNet](http://www.image-net.org/challenges/LSVRC/2012/) (ILSVRC 2012, 130ë§Œ ê°œì˜ ì´ë¯¸ì§€ì™€ 1,000ê°œì˜ í´ë˜ìŠ¤)ì—ì„œ ì¶”ê°€ë¡œ ë¯¸ì„¸ ì¡°ì •ëœ ê²½ìš°ì…ë‹ˆë‹¤.
- Vision TransformerëŠ” 224x224 í•´ìƒë„ë¡œ ì‚¬ì „ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤. ë¯¸ì„¸ ì¡°ì • ì‹œ, ì‚¬ì „ í•™ìŠµë³´ë‹¤ ë” ë†’ì€ í•´ìƒë„ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ìœ ë¦¬í•œ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤ ([(Touvron et al., 2019)](https://huggingface.co/papers/1906.06423), [(Kolesnikovet al., 2020)](https://huggingface.co/papers/1912.11370). ë” ë†’ì€ í•´ìƒë„ë¡œ ë¯¸ì„¸ ì¡°ì •í•˜ê¸° ìœ„í•´, ì €ìë“¤ì€ ì›ë³¸ ì´ë¯¸ì§€ì—ì„œì˜ ìœ„ì¹˜ì— ë”°ë¼ ì‚¬ì „ í•™ìŠµëœ ìœ„ì¹˜ ì„ë² ë”©ì˜ 2D ë³´ê°„(interpolation)ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
- ìµœê³ ì˜ ê²°ê³¼ëŠ” supervised ë°©ì‹ì˜ ì‚¬ì „ í•™ìŠµì—ì„œ ì–»ì–´ì¡Œìœ¼ë©°, ì´ëŠ” NLPì—ì„œëŠ” í•´ë‹¹ë˜ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ì €ìë“¤ì€ ë§ˆìŠ¤í¬ëœ íŒ¨ì¹˜ ì˜ˆì¸¡(ë§ˆìŠ¤í¬ëœ ì–¸ì–´ ëª¨ë¸ë§ì—ì„œ ì˜ê°ì„ ë°›ì€ self-supervised ì‚¬ì „ í•™ìŠµ ëª©í‘œ)ì„ ì‚¬ìš©í•œ ì‹¤í—˜ë„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. ì´ ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ ë” ì‘ì€ ViT-B/16 ëª¨ë¸ì€ ImageNetì—ì„œ 79.9%ì˜ ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ì˜€ìœ¼ë©°, ì´ëŠ” ì²˜ìŒë¶€í„° í•™ìŠµí•œ ê²ƒë³´ë‹¤ 2% ê°œì„ ëœ ê²°ê³¼ì´ì§€ë§Œ, ì—¬ì „íˆ supervised ì‚¬ì „ í•™ìŠµë³´ë‹¤ 4% ë‚®ìŠµë‹ˆë‹¤.

### Scaled Dot Product Attention (SDPA) ì‚¬ìš©í•˜ê¸° [[using-scaled-dot-product-attention-sdpa]]

PyTorchëŠ” `torch.nn.functional`ì˜ ì¼ë¶€ë¡œì„œ native scaled dot-product attention (SDPA) ì—°ì‚°ìë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ì…ë ¥ ë° ì‚¬ìš© ì¤‘ì¸ í•˜ë“œì›¨ì–´ì— ë”°ë¼ ì—¬ëŸ¬ êµ¬í˜„ ë°©ì‹ì„ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ìì„¸í•œ ë‚´ìš©ì€ [ê³µì‹ ë¬¸ì„œ](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)ë‚˜ [GPU ì¶”ë¡ ](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention) í˜ì´ì§€ë¥¼ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤.

SDPAëŠ” `torch>=2.1.1`ì—ì„œ êµ¬í˜„ì´ ê°€ëŠ¥í•œ ê²½ìš° ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©ë˜ì§€ë§Œ, `from_pretrained()`ì—ì„œ `attn_implementation="sdpa"`ë¡œ ì„¤ì •í•˜ì—¬ SDPAë¥¼ ëª…ì‹œì ìœ¼ë¡œ ìš”ì²­í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

```
from transformers import ViTForImageClassification
model = ViTForImageClassification.from_pretrained("google/vit-base-patch16-224", attn_implementation="sdpa", dtype=torch.float16)
...
```

ìµœì ì˜ ì†ë„ í–¥ìƒì„ ìœ„í•´ ëª¨ë¸ì„ ë°˜ì •ë°€ë„(ì˜ˆ: `torch.float16` ë˜ëŠ” `torch.bfloat16`)ë¡œ ë¡œë“œí•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.

ë¡œì»¬ ë²¤ì¹˜ë§ˆí¬(A100-40GB, PyTorch 2.3.0, OS Ubuntu 22.04)ì—ì„œ `float32`ì™€ `google/vit-base-patch16-224` ëª¨ë¸ì„ ì‚¬ìš©í•œ ì¶”ë¡  ì‹œ, ë‹¤ìŒê³¼ ê°™ì€ ì†ë„ í–¥ìƒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

|   Batch size |   Average inference time (ms), eager mode |   Average inference time (ms), sdpa model |   Speed up, Sdpa / Eager (x) |
|--------------|-------------------------------------------|-------------------------------------------|------------------------------|
|            1 |                                         7 |                                         6 |                      1.17 |
|            2 |                                         8 |                                         6 |                      1.33 |
|            4 |                                         8 |                                         6 |                      1.33 |
|            8 |                                         8 |                                         6 |                      1.33 |

## ë¦¬ì†ŒìŠ¤ [[resources]]

ViTì˜ ì¶”ë¡  ë° ì»¤ìŠ¤í…€ ë°ì´í„°ì— ëŒ€í•œ ë¯¸ì„¸ ì¡°ì •ê³¼ ê´€ë ¨ëœ ë°ëª¨ ë…¸íŠ¸ë¶ì€ [ì—¬ê¸°](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/VisionTransformer)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Hugging Faceì—ì„œ ê³µì‹ì ìœ¼ë¡œ ì œê³µí•˜ëŠ” ìë£Œì™€ ì»¤ë®¤ë‹ˆí‹°(ğŸŒë¡œ í‘œì‹œëœ) ìë£Œ ëª©ë¡ì€ ViTë¥¼ ì‹œì‘í•˜ëŠ” ë° ë„ì›€ì´ ë  ê²ƒì…ë‹ˆë‹¤. ì´ ëª©ë¡ì— í¬í•¨ë  ìë£Œë¥¼ ì œì¶œí•˜ê³  ì‹¶ë‹¤ë©´ Pull Requestë¥¼ ì—´ì–´ ì£¼ì‹œë©´ ê²€í† í•˜ê² ìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ë‚´ìš©ì„ ì„¤ëª…í•˜ëŠ” ìë£Œê°€ ê°€ì¥ ì´ìƒì ì´ë©°, ê¸°ì¡´ ìë£Œë¥¼ ì¤‘ë³µí•˜ì§€ ì•Šë„ë¡ í•´ì£¼ì‹­ì‹œì˜¤.

`ViTForImageClassification` ì€ ë‹¤ìŒì—ì„œ ì§€ì›ë©ë‹ˆë‹¤:
<PipelineTag pipeline="image-classification"/>

- [Hugging Face Transformersë¡œ ViTë¥¼ ì´ë¯¸ì§€ ë¶„ë¥˜ì— ë§ê²Œ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë°©ë²•](https://huggingface.co/blog/fine-tune-vit)ì— ëŒ€í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸
- [Hugging Face Transformersì™€ `Keras`ë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ë¶„ë¥˜](https://www.philschmid.de/image-classification-huggingface-transformers-keras)ì— ëŒ€í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸
- [Hugging Face Transformersë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ë¶„ë¥˜ ë¯¸ì„¸ ì¡°ì •](https://github.com/huggingface/notebooks/blob/main/examples/image_classification.ipynb)ì— ëŒ€í•œ ë…¸íŠ¸ë¶
- [Hugging Face Trainerë¡œ CIFAR-10ì—ì„œ Vision Transformer ë¯¸ì„¸ ì¡°ì •](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb)ì— ëŒ€í•œ ë…¸íŠ¸ë¶
- [PyTorch Lightningìœ¼ë¡œ CIFAR-10ì—ì„œ Vision Transformer ë¯¸ì„¸ ì¡°ì •](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb)ì— ëŒ€í•œ ë…¸íŠ¸ë¶

âš—ï¸ ìµœì í™”

- [Optimumì„ ì‚¬ìš©í•œ ì–‘ìí™”ë¥¼ í†µí•´ Vision Transformer(ViT) ê°€ì†](https://www.philschmid.de/optimizing-vision-transformer)ì— ëŒ€í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ 

âš¡ï¸ ì¶”ë¡ 

- [Google Brainì˜ Vision Transformer(ViT) ë¹ ë¥¸ ë°ëª¨](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Quick_demo_of_HuggingFace_version_of_Vision_Transformer_inference.ipynb)ì— ëŒ€í•œ ë…¸íŠ¸ë¶

ğŸš€ ë°°í¬

- [TF Servingìœ¼ë¡œ Hugging Faceì—ì„œ Tensorflow Vision ëª¨ë¸ ë°°í¬](https://huggingface.co/blog/tf-serving-vision)ì— ëŒ€í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸
- [Vertex AIì—ì„œ Hugging Face ViT ë°°í¬](https://huggingface.co/blog/deploy-vertex-ai)ì— ëŒ€í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸
- [TF Servingì„ ì‚¬ìš©í•˜ì—¬ Kubernetesì—ì„œ Hugging Face ViT ë°°í¬](https://huggingface.co/blog/deploy-tfserving-kubernetes)ì— ëŒ€í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸

## ViTConfig [[transformers.ViTConfig]]

[[autodoc]] ViTConfig

## ViTFeatureExtractor [[transformers.ViTFeatureExtractor]]

[[autodoc]] ViTFeatureExtractor
    - __call__

## ViTImageProcessor [[transformers.ViTImageProcessor]]

[[autodoc]] ViTImageProcessor
    - preprocess

## ViTImageProcessorFast [[transformers.ViTImageProcessorFast]]

[[autodoc]] ViTImageProcessorFast
    - preprocess

<frameworkcontent>
<pt>

## ViTModel [[transformers.ViTModel]]

[[autodoc]] ViTModel
    - forward

## ViTForMaskedImageModeling [[transformers.ViTForMaskedImageModeling]]

[[autodoc]] ViTForMaskedImageModeling
    - forward

## ViTForImageClassification [[transformers.ViTForImageClassification]]

[[autodoc]] ViTForImageClassification
    - forward

</pt>
<tf>

## TFViTModel [[transformers.TFViTModel]]

[[autodoc]] TFViTModel
    - call

## TFViTForImageClassification [[transformers.TFViTForImageClassification]]

[[autodoc]] TFViTForImageClassification
    - call

</tf>
<jax>

## FlaxVitModel [[transformers.FlaxViTModel]]

[[autodoc]] FlaxViTModel
    - __call__

## FlaxViTForImageClassification [[transformers.FlaxViTForImageClassification]]

[[autodoc]] FlaxViTForImageClassification
    - __call__

</jax>
</frameworkcontent>