# Cohere[[cohere]]

## ê°œìš”[[overview]]

The Cohere Command-R ëª¨ë¸ì€ CohereíŒ€ì´ [Command-R: í”„ë¡œë•ì…˜ ê·œëª¨ì˜ ê²€ìƒ‰ ì¦ê°• ìƒì„±](https://txt.cohere.com/command-r/)ë¼ëŠ” ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ì—ì„œ ì†Œê°œ ë˜ì—ˆìŠµë‹ˆë‹¤.

ë…¼ë¬¸ ì´ˆë¡:

*Command-Rì€ ê¸°ì—…ì˜ í”„ë¡œë•ì…˜ ê·œëª¨ AIë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ê¸° ìœ„í•´ RAG(ê²€ìƒ‰ ì¦ê°• ìƒì„±)ì™€ ë„êµ¬ ì‚¬ìš©ì„ ëª©í‘œë¡œ í•˜ëŠ” í™•ì¥ ê°€ëŠ¥í•œ ìƒì„± ëª¨ë¸ì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ìš°ë¦¬ëŠ” ëŒ€ê·œëª¨ í”„ë¡œë•ì…˜ ì›Œí¬ë¡œë“œë¥¼ ëª©í‘œë¡œ í•˜ëŠ” ìƒˆë¡œìš´ LLMì¸ Command-Rì„ ì†Œê°œí•©ë‹ˆë‹¤. Command-Rì€ ë†’ì€ íš¨ìœ¨ì„±ê³¼ ê°•ë ¥í•œ ì •í™•ì„±ì˜ ê· í˜•ì„ ë§ì¶”ëŠ” 'í™•ì¥ ê°€ëŠ¥í•œ' ëª¨ë¸ ì¹´í…Œê³ ë¦¬ë¥¼ ëŒ€ìƒìœ¼ë¡œ í•˜ì—¬, ê¸°ì—…ë“¤ì´ ê°œë… ì¦ëª…ì„ ë„˜ì–´ í”„ë¡œë•ì…˜ ë‹¨ê³„ë¡œ ë‚˜ì•„ê°ˆ ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.*

*Command-Rì€ ê²€ìƒ‰ ì¦ê°• ìƒì„±(RAG)ì´ë‚˜ ì™¸ë¶€ API ë° ë„êµ¬ ì‚¬ìš©ê³¼ ê°™ì€ ê¸´ ë¬¸ë§¥ ì‘ì—…ì— ìµœì í™”ëœ ìƒì„± ëª¨ë¸ì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ RAG ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ìœ„í•œ ìµœê³  ìˆ˜ì¤€ì˜ í†µí•©ì„ ì œê³µí•˜ê³  ê¸°ì—… ì‚¬ìš© ì‚¬ë¡€ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ê¸° ìœ„í•´ ìš°ë¦¬ì˜ ì—…ê³„ ì„ ë„ì ì¸ Embed ë° Rerank ëª¨ë¸ê³¼ ì¡°í™”ë¡­ê²Œ ì‘ë™í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ê¸°ì—…ì´ ëŒ€ê·œëª¨ë¡œ êµ¬í˜„í•  ìˆ˜ ìˆë„ë¡ ë§Œë“¤ì–´ì§„ ëª¨ë¸ë¡œì„œ, Command-Rì€ ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì„ ìë‘í•©ë‹ˆë‹¤:
- RAG ë° ë„êµ¬ ì‚¬ìš©ì— ëŒ€í•œ ê°•ë ¥í•œ ì •í™•ì„±
- ë‚®ì€ ì§€ì—° ì‹œê°„ê³¼ ë†’ì€ ì²˜ë¦¬ëŸ‰
- ë” ê¸´ 128k ì»¨í…ìŠ¤íŠ¸ì™€ ë‚®ì€ ê°€ê²©
- 10ê°œì˜ ì£¼ìš” ì–¸ì–´ì— ê±¸ì¹œ ê°•ë ¥í•œ ê¸°ëŠ¥
- ì—°êµ¬ ë° í‰ê°€ë¥¼ ìœ„í•´ HuggingFaceì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ê°€ì¤‘ì¹˜

ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ëŠ” [ì´ê³³](https://huggingface.co/CohereForAI/c4ai-command-r-v01)ì—ì„œ í™•ì¸í•˜ì„¸ìš”.
ì´ ëª¨ë¸ì€ [Saurabh Dash](https://huggingface.co/saurabhdash)ê³¼ [Ahmet ÃœstÃ¼n](https://huggingface.co/ahmetustun)ì— ì˜í•´ ê¸°ì—¬ ë˜ì—ˆìŠµë‹ˆë‹¤. Hugging Faceì—ì„œ ì´ ì½”ë“œì˜ êµ¬í˜„ì€ [GPT-NeoX](https://github.com/EleutherAI/gpt-neox)ì— ê¸°ë°˜í•˜ì˜€ìŠµë‹ˆë‹¤.

## ì‚¬ìš© íŒ[[usage-tips]]

<Tip warning={true}>

Hubì— ì—…ë¡œë“œëœ ì²´í¬í¬ì¸íŠ¸ë“¤ì€ `dtype = 'float16'`ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. 
ì´ëŠ” `AutoModel` APIê°€ ì²´í¬í¬ì¸íŠ¸ë¥¼ `torch.float32`ì—ì„œ `torch.float16`ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. 

ì˜¨ë¼ì¸ ê°€ì¤‘ì¹˜ì˜ `dtype`ì€ `model = AutoModelForCausalLM.from_pretrained("path", dtype = "auto")`ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì´ˆê¸°í™”í•  ë•Œ `dtype="auto"`ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” í•œ ëŒ€ë¶€ë¶„ ë¬´ê´€í•©ë‹ˆë‹¤. ê·¸ ì´ìœ ëŠ” ëª¨ë¸ì´ ë¨¼ì € ë‹¤ìš´ë¡œë“œë˜ê³ (ì˜¨ë¼ì¸ ì²´í¬í¬ì¸íŠ¸ì˜ `dtype` ì‚¬ìš©), ê·¸ ë‹¤ìŒ `torch`ì˜ ê¸°ë³¸ `dtype`ìœ¼ë¡œ ë³€í™˜ë˜ë©°(ì´ë•Œ `torch.float32`ê°€ ë¨), ë§ˆì§€ë§‰ìœ¼ë¡œ configì— `dtype`ì´ ì œê³µëœ ê²½ìš° ì´ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

ëª¨ë¸ì„ `float16`ìœ¼ë¡œ í›ˆë ¨í•˜ëŠ” ê²ƒì€ ê¶Œì¥ë˜ì§€ ì•Šìœ¼ë©° `nan`ì„ ìƒì„±í•˜ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ëª¨ë¸ì€ `bfloat16`ìœ¼ë¡œ í›ˆë ¨í•´ì•¼ í•©ë‹ˆë‹¤.
</Tip>
ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
# pip install transformers
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "CohereForAI/c4ai-command-r-v01"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)

# Format message with the command-r chat template
messages = [{"role": "user", "content": "Hello, how are you?"}]
input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt")
## <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Hello, how are you?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>

gen_tokens = model.generate(
    input_ids, 
    max_new_tokens=100, 
    do_sample=True, 
    temperature=0.3,
    )

gen_text = tokenizer.decode(gen_tokens[0])
print(gen_text)
```

- Flash Attention 2ë¥¼ `attn_implementation="flash_attention_2"`ë¥¼ í†µí•´ ì‚¬ìš©í•  ë•ŒëŠ”, `from_pretrained` í´ë˜ìŠ¤ ë©”ì„œë“œì— `dtype`ì„ ì „ë‹¬í•˜ì§€ ë§ê³  ìë™ í˜¼í•© ì •ë°€ë„ í›ˆë ¨(Automatic Mixed-Precision training)ì„ ì‚¬ìš©í•˜ì„¸ìš”. `Trainer`ë¥¼ ì‚¬ìš©í•  ë•ŒëŠ” ë‹¨ìˆœíˆ `fp16` ë˜ëŠ” `bf16`ì„ `True`ë¡œ ì§€ì •í•˜ë©´ ë©ë‹ˆë‹¤. ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš°ì—ëŠ” `torch.autocast`ë¥¼ ì‚¬ìš©í•˜ê³  ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. ì´ëŠ” Flash Attentionì´ `fp16`ì™€ `bf16` ë°ì´í„° íƒ€ì…ë§Œ ì§€ì›í•˜ê¸° ë•Œë¬¸ì— í•„ìš”í•©ë‹ˆë‹¤.

## ë¦¬ì†ŒìŠ¤[[resources]]

Command-Rì„ ì‹œì‘í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” Hugging Faceì™€ community ìë£Œ ëª©ë¡(ğŸŒë¡œ í‘œì‹œë¨) ì…ë‹ˆë‹¤. ì—¬ê¸°ì— í¬í•¨ë  ìë£Œë¥¼ ì œì¶œí•˜ê³  ì‹¶ìœ¼ì‹œë‹¤ë©´ PR(Pull Request)ë¥¼ ì—´ì–´ì£¼ì„¸ìš”. ë¦¬ë·° í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤! ìë£ŒëŠ” ê¸°ì¡´ ìë£Œë¥¼ ë³µì œí•˜ëŠ” ëŒ€ì‹  ìƒˆë¡œìš´ ë‚´ìš©ì„ ë‹´ê³  ìˆì–´ì•¼ í•©ë‹ˆë‹¤.


<PipelineTag pipeline="text-generation"/>

FP16 ëª¨ë¸ ë¡œë”©
```python
# pip install transformers
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "CohereForAI/c4ai-command-r-v01"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)

# command-r ì±— í…œí”Œë¦¿ìœ¼ë¡œ ë©”ì„¸ì§€ í˜•ì‹ì„ ì •í•˜ì„¸ìš”
messages = [{"role": "user", "content": "Hello, how are you?"}]
input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt")
## <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Hello, how are you?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>

gen_tokens = model.generate(
    input_ids, 
    max_new_tokens=100, 
    do_sample=True, 
    temperature=0.3,
    )

gen_text = tokenizer.decode(gen_tokens[0])
print(gen_text)
```

bitsandbytes ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•´ì„œ 4bit ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë”©
```python
# pip install transformers bitsandbytes accelerate
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(load_in_4bit=True)

model_id = "CohereForAI/c4ai-command-r-v01"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config)

gen_tokens = model.generate(
    input_ids, 
    max_new_tokens=100, 
    do_sample=True, 
    temperature=0.3,
    )

gen_text = tokenizer.decode(gen_tokens[0])
print(gen_text)
```


## CohereConfig[[transformers.CohereConfig]]

[[autodoc]] CohereConfig

## CohereTokenizerFast[[transformers.CohereTokenizerFast]]

[[autodoc]] CohereTokenizerFast
    - build_inputs_with_special_tokens
    - get_special_tokens_mask
    - create_token_type_ids_from_sequences
    - update_post_processor
    - save_vocabulary

## CohereModel[[transformers.CohereModel]]

[[autodoc]] CohereModel
    - forward


## CohereForCausalLM[[transformers.CohereForCausalLM]]

[[autodoc]] CohereForCausalLM
    - forward


