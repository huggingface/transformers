<!--Copyright 2023 Mistral AI and The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See thze License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Mistral[[mistral]]

## ê°œìš”[[overview]]

ë¯¸ìŠ¤íŠ¸ë„ì€ Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayedê°€ ì‘ì„±í•œ [ì´ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸](https://mistral.ai/news/announcing-mistral-7b/)ì—ì„œ ì†Œê°œë˜ì—ˆìŠµë‹ˆë‹¤.

ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ì˜ ì„œë‘ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

*ë¯¸ìŠ¤íŠ¸ë„ AIíŒ€ì€ í˜„ì¡´í•˜ëŠ” ì–¸ì–´ ëª¨ë¸ ì¤‘ í¬ê¸° ëŒ€ë¹„ ê°€ì¥ ê°•ë ¥í•œ ë¯¸ìŠ¤íŠ¸ë„7Bë¥¼ ì¶œì‹œí•˜ê²Œ ë˜ì–´ ìë‘ìŠ¤ëŸ½ìŠµë‹ˆë‹¤.*

ë¯¸ìŠ¤íŠ¸ë„-7BëŠ” [mistral.ai](https://mistral.ai/)ì—ì„œ ì¶œì‹œí•œ ì²« ë²ˆì§¸ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì…ë‹ˆë‹¤.

### ì•„í‚¤í…ì²˜ ì„¸ë¶€ì‚¬í•­[[architectural-details]]

ë¯¸ìŠ¤íŠ¸ë„-7BëŠ” ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ì  íŠ¹ì§•ì„ ê°€ì§„ ë””ì½”ë” ì „ìš© íŠ¸ëœìŠ¤í¬ë¨¸ì…ë‹ˆë‹¤:

- ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì–´í…ì…˜: 8k ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ì™€ ê³ ì • ìºì‹œ í¬ê¸°ë¡œ í›ˆë ¨ë˜ì—ˆìœ¼ë©°, ì´ë¡ ìƒ 128K í† í°ì˜ ì–´í…ì…˜ ë²”ìœ„ë¥¼ ê°€ì§‘ë‹ˆë‹¤.
- GQA(Grouped Query Attention): ë” ë¹ ë¥¸ ì¶”ë¡ ì´ ê°€ëŠ¥í•˜ê³  ë” ì‘ì€ í¬ê¸°ì˜ ìºì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
- ë°”ì´íŠ¸ í´ë°±(Byte-fallback) BPE í† í¬ë‚˜ì´ì €: ë¬¸ìë“¤ì´ ì ˆëŒ€ ì–´íœ˜ ëª©ë¡ ì™¸ì˜ í† í°ìœ¼ë¡œ ë§¤í•‘ë˜ì§€ ì•Šë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.

ë” ìì„¸í•œ ë‚´ìš©ì€ [ì¶œì‹œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸](https://mistral.ai/news/announcing-mistral-7b/)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

### ë¼ì´ì„ ìŠ¤[[license]]

`ë¯¸ìŠ¤íŠ¸ë„-7B`ëŠ” ì•„íŒŒì¹˜ 2.0 ë¼ì´ì„ ìŠ¤ë¡œ ì¶œì‹œë˜ì—ˆìŠµë‹ˆë‹¤.

## ì‚¬ìš© íŒ[[usage-tips]]

ë¯¸ìŠ¤íŠ¸ë„ AIíŒ€ì€ ë‹¤ìŒ 3ê°€ì§€ ì²´í¬í¬ì¸íŠ¸ë¥¼ ê³µê°œí–ˆìŠµë‹ˆë‹¤:

- ê¸°ë³¸ ëª¨ë¸ì¸ [ë¯¸ìŠ¤íŠ¸ë„-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)ì€ ì¸í„°ë„· ê·œëª¨ì˜ ë°ì´í„°ì—ì„œ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ë„ë¡ ì‚¬ì „ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤.
- ì§€ì‹œ ì¡°ì • ëª¨ë¸ì¸ [ë¯¸ìŠ¤íŠ¸ë„-7B-Instruct-v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)ì€ ì§€ë„ ë¯¸ì„¸ ì¡°ì •(SFT)ê³¼ ì§ì ‘ ì„ í˜¸ë„ ìµœì í™”(DPO)ë¥¼ ì‚¬ìš©í•œ ì±„íŒ…ì— ìµœì í™”ëœ ê¸°ë³¸ ëª¨ë¸ì…ë‹ˆë‹¤.
- ê°œì„ ëœ ì§€ì‹œ ì¡°ì • ëª¨ë¸ì¸ [ë¯¸ìŠ¤íŠ¸ë„-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)ëŠ” v1ì„ ê°œì„ í•œ ë²„ì „ì…ë‹ˆë‹¤.

ê¸°ë³¸ ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-v0.1", device_map="auto")
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")

>>> prompt = "My favourite condiment is"

>>> model_inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
>>> model.to(device)

>>> generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)
>>> tokenizer.batch_decode(generated_ids)[0]
"My favourite condiment is to ..."
```

ì§€ì‹œ ì¡°ì • ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2", device_map="auto")
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2")

>>> messages = [
...     {"role": "user", "content": "What is your favourite condiment?"},
...     {"role": "assistant", "content": "Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!"},
...     {"role": "user", "content": "Do you have mayonnaise recipes?"}
... ]

>>> model_inputs = tokenizer.apply_chat_template(messages, return_tensors="pt").to("cuda")

>>> generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)
>>> tokenizer.batch_decode(generated_ids)[0]
"Mayonnaise can be made as follows: (...)"
```

ì§€ì‹œ ì¡°ì • ëª¨ë¸ì€ ì…ë ¥ì´ ì˜¬ë°”ë¥¸ í˜•ì‹ìœ¼ë¡œ ì¤€ë¹„ë˜ë„ë¡ [ì±„íŒ… í…œí”Œë¦¿](../chat_templating)ì„ ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

## í”Œë˜ì‹œ ì–´í…ì…˜ì„ ì´ìš©í•œ ë¯¸ìŠ¤íŠ¸ë„ ì†ë„í–¥ìƒ[[speeding-up-mistral-by-using-flash-attention]]

ìœ„ì˜ ì½”ë“œ ìŠ¤ë‹ˆí«ë“¤ì€ ì–´ë–¤ ìµœì í™” ê¸°ë²•ë„ ì‚¬ìš©í•˜ì§€ ì•Šì€ ì¶”ë¡  ê³¼ì •ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. í•˜ì§€ë§Œ ëª¨ë¸ ë‚´ë¶€ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì˜ ë” ë¹ ë¥¸ êµ¬í˜„ì¸ [í”Œë˜ì‹œ ì–´í…ì…˜2](../perf_train_gpu_one.md#flash-attention-2)ì„ í™œìš©í•˜ë©´ ëª¨ë¸ì˜ ì†ë„ë¥¼ í¬ê²Œ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë¨¼ì €, ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì–´í…ì…˜ ê¸°ëŠ¥ì„ í¬í•¨í•˜ëŠ” í”Œë˜ì‹œ ì–´í…ì…˜2ì˜ ìµœì‹  ë²„ì „ì„ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.

```bash
pip install -U flash-attn --no-build-isolation
```

í•˜ë“œì›¨ì–´ì™€ í”Œë˜ì‹œ ì–´í…ì…˜2ì˜ í˜¸í™˜ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”. ì´ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ [í”Œë˜ì‹œ ì–´í…ì…˜ ì €ì¥ì†Œ](https://github.com/Dao-AILab/flash-attention)ì˜ ê³µì‹ ë¬¸ì„œì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ ëª¨ë¸ì„ ë°˜ì •ë°€ë„(ì˜ˆ: `torch.float16`)ë¡œ ë¶ˆëŸ¬ì™€ì•¼í•©ë‹ˆë‹¤.

í”Œë˜ì‹œ ì–´í…ì…˜2ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ê³  ì‹¤í–‰í•˜ë ¤ë©´ ì•„ë˜ ì½”ë“œ ìŠ¤ë‹ˆí«ì„ ì°¸ì¡°í•˜ì„¸ìš”:

```python
>>> import torch
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-v0.1", torch_dtype=torch.float16, attn_implementation="flash_attention_2", device_map="auto")
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")

>>> prompt = "My favourite condiment is"

>>> model_inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
>>> model.to(device)

>>> generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)
>>> tokenizer.batch_decode(generated_ids)[0]
"My favourite condiment is to (...)"
```

### ê¸°ëŒ€í•˜ëŠ” ì†ë„ í–¥ìƒ[[expected-speedups]]

ë‹¤ìŒì€ `mistralai/Mistral-7B-v0.1` ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•œ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ê¸°ë³¸ êµ¬í˜„ê³¼ í”Œë˜ì‹œ ì–´í…ì…˜2 ë²„ì „ ëª¨ë¸ ì‚¬ì´ì˜ ìˆœìˆ˜ ì¶”ë¡  ì‹œê°„ì„ ë¹„êµí•œ ì˜ˆìƒ ì†ë„ í–¥ìƒ ë‹¤ì´ì–´ê·¸ë¨ì…ë‹ˆë‹¤.

<div style="text-align: center">
<img src="https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/mistral-7b-inference-large-seqlen.png">
</div>

### ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì–´í…ì…˜[[sliding-window-attention]]

í˜„ì¬ êµ¬í˜„ì€ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ê³¼ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ìºì‹œ ê´€ë¦¬ ê¸°ëŠ¥ì„ ì§€ì›í•©ë‹ˆë‹¤. ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì–´í…ì…˜ì„ í™œì„±í™”í•˜ë ¤ë©´, ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì–´í…ì…˜ê³¼ í˜¸í™˜ë˜ëŠ”`flash-attn`(`>=2.3.0`)ë²„ì „ì„ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤. 

ë˜í•œ í”Œë˜ì‹œ ì–´í…ì…˜2 ëª¨ë¸ì€ ë” ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ìºì‹œ ìŠ¬ë¼ì´ì‹± ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë¯¸ìŠ¤íŠ¸ë„ ëª¨ë¸ì˜ ê³µì‹ êµ¬í˜„ì—ì„œ ê¶Œì¥í•˜ëŠ” ë¡¤ë§ ìºì‹œ ë©”ì»¤ë‹ˆì¦˜ì„ ë”°ë¼, ìºì‹œ í¬ê¸°ë¥¼ ê³ ì •(`self.config.sliding_window`)ìœ¼ë¡œ ìœ ì§€í•˜ê³ , `padding_side="left"`ì¸ ê²½ìš°ì—ë§Œ ë°°ì¹˜ ìƒì„±(batch generation)ì„ ì§€ì›í•˜ë©°, í˜„ì¬ í† í°ì˜ ì ˆëŒ€ ìœ„ì¹˜ë¥¼ ì‚¬ìš©í•´ ìœ„ì¹˜ ì„ë² ë”©ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

## ì–‘ìí™”ë¡œ ë¯¸ìŠ¤íŠ¸ë„ í¬ê¸° ì¤„ì´ê¸°[[shrinking-down-mistral-using-quantization]]

ë¯¸ìŠ¤íŠ¸ë„ ëª¨ë¸ì€ 70ì–µ ê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§€ê³  ìˆì–´, ì ˆë°˜ì˜ ì •ë°€ë„(float16)ë¡œ ì•½ 14GBì˜ GPU RAMì´ í•„ìš”í•©ë‹ˆë‹¤. ê° íŒŒë¼ë¯¸í„°ê°€ 2ë°”ì´íŠ¸ë¡œ ì €ì¥ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ [ì–‘ìí™”](../quantization.md)ë¥¼ ì‚¬ìš©í•˜ë©´ ëª¨ë¸ í¬ê¸°ë¥¼ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ì„ 4ë¹„íŠ¸(ì¦‰, íŒŒë¼ë¯¸í„°ë‹¹ ë°˜ ë°”ì´íŠ¸)ë¡œ ì–‘ìí™”í•˜ë©´ ì•½ 3.5GBì˜ RAMë§Œ í•„ìš”í•©ë‹ˆë‹¤.

ëª¨ë¸ì„ ì–‘ìí™”í•˜ëŠ” ê²ƒì€ `quantization_config`ë¥¼ ëª¨ë¸ì— ì „ë‹¬í•˜ëŠ” ê²ƒë§Œí¼ ê°„ë‹¨í•©ë‹ˆë‹¤. ì•„ë˜ì—ì„œëŠ” BitsAndBytes ì–‘ìí™”ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, ë‹¤ë¥¸ ì–‘ìí™” ë°©ë²•ì€ [ì´ í˜ì´ì§€](../quantization.md)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”:

```python
>>> import torch
>>> from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

>>> # specify how to quantize the model
>>> quantization_config = BitsAndBytesConfig(
...         load_in_4bit=True,
...         bnb_4bit_quant_type="nf4",
...         bnb_4bit_compute_dtype="torch.float16",
... )

>>> model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2", quantization_config=True, device_map="auto")
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2")

>>> prompt = "My favourite condiment is"

>>> messages = [
...     {"role": "user", "content": "What is your favourite condiment?"},
...     {"role": "assistant", "content": "Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!"},
...     {"role": "user", "content": "Do you have mayonnaise recipes?"}
... ]

>>> model_inputs = tokenizer.apply_chat_template(messages, return_tensors="pt").to("cuda")

>>> generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)
>>> tokenizer.batch_decode(generated_ids)[0]
"The expected output"
```

ì´ ëª¨ë¸ì€ [Younes Belkada](https://huggingface.co/ybelkada)ì™€ [Arthur Zucker](https://huggingface.co/ArthurZ)ê°€ ê¸°ì—¬í–ˆìŠµë‹ˆë‹¤.
ì›ë³¸ ì½”ë“œëŠ” [ì´ê³³](https://github.com/mistralai/mistral-src)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ë¦¬ì†ŒìŠ¤[[resources]]

ë¯¸ìŠ¤íŠ¸ë„ì„ ì‹œì‘í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” Hugging Faceì™€ community ìë£Œ ëª©ë¡(ğŸŒë¡œ í‘œì‹œë¨) ì…ë‹ˆë‹¤. ì—¬ê¸°ì— í¬í•¨ë  ìë£Œë¥¼ ì œì¶œí•˜ê³  ì‹¶ìœ¼ì‹œë‹¤ë©´ PR(Pull Request)ë¥¼ ì—´ì–´ì£¼ì„¸ìš”. ë¦¬ë·°í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤! ìë£ŒëŠ” ê¸°ì¡´ ìë£Œë¥¼ ë³µì œí•˜ëŠ” ëŒ€ì‹  ìƒˆë¡œìš´ ë‚´ìš©ì„ ë‹´ê³  ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

<PipelineTag pipeline="text-generation"/>

- ë¯¸ìŠ¤íŠ¸ë„-7Bì˜ ì§€ë„í˜• ë¯¸ì„¸ì¡°ì •(SFT)ì„ ìˆ˜í–‰í•˜ëŠ” ë°ëª¨ ë…¸íŠ¸ë¶ì€ [ì´ê³³](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Mistral/Supervised_fine_tuning_(SFT)_of_an_LLM_using_Hugging_Face_tooling.ipynb)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸŒ
- 2024ë…„ì— Hugging Face ë„êµ¬ë¥¼ ì‚¬ìš©í•´ LLMì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ [ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl). ğŸŒ
- Hugging Faceì˜ [ì •ë ¬(Alignment) í•¸ë“œë¶](https://github.com/huggingface/alignment-handbook)ì—ëŠ” ë¯¸ìŠ¤íŠ¸ë„-7Bë¥¼ ì‚¬ìš©í•œ ì§€ë„í˜• ë¯¸ì„¸ ì¡°ì •(SFT) ë° ì§ì ‘ ì„ í˜¸ ìµœì í™”(DPO)ë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ìŠ¤í¬ë¦½íŠ¸ì™€ ë ˆì‹œí”¼ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì—ëŠ” ë‹¨ì¼ GPUì—ì„œ QLoRa ë° ë‹¤ì¤‘ GPUë¥¼ ì‚¬ìš©í•œ ì „ì²´ ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•œ ìŠ¤í¬ë¦½íŠ¸ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
- [ì¸ê³¼ì  ì–¸ì–´ ëª¨ë¸ë§ ì‘ì—… ê°€ì´ë“œ](../tasks/language_modeling)

## MistralConfig[[transformers.MistralConfig]]

[[autodoc]] MistralConfig

## MistralModel[[transformers.MistralModel]]

[[autodoc]] MistralModel
    - forward

## MistralForCausalLM[[transformers.MistralForCausalLM]]

[[autodoc]] MistralForCausalLM
    - forward

## MistralForSequenceClassification[[transformers.MistralForSequenceClassification]]

[[autodoc]] MistralForSequenceClassification
    - forward

## MistralForTokenClassification[[transformers.MistralForTokenClassification]]

[[autodoc]] MistralForTokenClassification
    - forward

## FlaxMistralModel[[transformers.FlaxMistralModel]]

[[autodoc]] FlaxMistralModel
    - __call__

## FlaxMistralForCausalLM[[transformers.FlaxMistralForCausalLM]]

[[autodoc]] FlaxMistralForCausalLM
    - __call__

## TFMistralModel[[transformers.TFMistralModel]]

[[autodoc]] TFMistralModel
    - call

## TFMistralForCausalLM[[transformers.TFMistralForCausalLM]]

[[autodoc]] TFMistralForCausalLM
    - call

## TFMistralForSequenceClassification[[transformers.TFMistralForSequenceClassification]]

[[autodoc]] TFMistralForSequenceClassification
    - call