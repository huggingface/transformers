<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# TorchScriptë¡œ ë‚´ë³´ë‚´ê¸°[[export-to-torchscript]]

<Tip>

TorchScriptë¥¼ í™œìš©í•œ ì‹¤í—˜ì€ ì•„ì§ ì´ˆê¸° ë‹¨ê³„ë¡œ, ê°€ë³€ì ì¸ ì…ë ¥ í¬ê¸° ëª¨ë¸ë“¤ì„ í†µí•´ ê·¸ ê¸°ëŠ¥ì„±ì„ ê³„ì† íƒêµ¬í•˜ê³  ìˆìŠµë‹ˆë‹¤. 
ì´ ê¸°ëŠ¥ì€ ì €í¬ê°€ ê´€ì‹¬ì„ ë‘ê³  ìˆëŠ” ë¶„ì•¼ ì¤‘ í•˜ë‚˜ì´ë©°, 
ì•ìœ¼ë¡œ ì¶œì‹œë  ë²„ì „ì—ì„œ ë” ë§ì€ ì½”ë“œ ì˜ˆì œ, ë” ìœ ì—°í•œ êµ¬í˜„, ê·¸ë¦¬ê³  Python ê¸°ë°˜ ì½”ë“œì™€ ì»´íŒŒì¼ëœ TorchScriptë¥¼ ë¹„êµí•˜ëŠ” ë²¤ì¹˜ë§ˆí¬ë¥¼ ë“±ì„ í†µí•´ ë¶„ì„ì„ ì‹¬í™”í•  ì˜ˆì •ì…ë‹ˆë‹¤.

</Tip>

[TorchScript ë¬¸ì„œ](https://pytorch.org/docs/stable/jit.html)ì—ì„œëŠ” ì´ë ‡ê²Œ ë§í•©ë‹ˆë‹¤.

> TorchScriptëŠ” PyTorch ì½”ë“œì—ì„œ ì§ë ¬í™” ë° ìµœì í™” ê°€ëŠ¥í•œ ëª¨ë¸ì„ ìƒì„±í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

[JITê³¼ TRACE](https://pytorch.org/docs/stable/jit.html)ëŠ” ê°œë°œìê°€ ëª¨ë¸ì„ ë‚´ë³´ë‚´ì„œ íš¨ìœ¨ ì§€í–¥ì ì¸ C++ í”„ë¡œê·¸ë¨ê³¼ ê°™ì€ ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ì—ì„œ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” PyTorch ëª¨ë“ˆì…ë‹ˆë‹¤.

PyTorch ê¸°ë°˜ Python í”„ë¡œê·¸ë¨ê³¼ ë‹¤ë¥¸ í™˜ê²½ì—ì„œ ëª¨ë¸ì„ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡, ğŸ¤— Transformers ëª¨ë¸ì„ TorchScriptë¡œ ë‚´ë³´ë‚¼ ìˆ˜ ìˆëŠ” ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. 
ì´ ë¬¸ì„œì—ì„œëŠ” TorchScriptë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë‚´ë³´ë‚´ê³  ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

ëª¨ë¸ì„ ë‚´ë³´ë‚´ë ¤ë©´ ë‘ ê°€ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤:

- `torchscript` í”Œë˜ê·¸ë¡œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤í™”
- ë”ë¯¸ ì…ë ¥ì„ ì‚¬ìš©í•œ ìˆœì „íŒŒ(forward pass)

ì´ í•„ìˆ˜ ì¡°ê±´ë“¤ì€ ì•„ë˜ì— ìì„¸íˆ ì„¤ëª…ëœ ê²ƒì²˜ëŸ¼ ê°œë°œìë“¤ì´ ì£¼ì˜í•´ì•¼ í•  ì—¬ëŸ¬ ì‚¬í•­ë“¤ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

## TorchScript í”Œë˜ê·¸ì™€ ë¬¶ì¸ ê°€ì¤‘ì¹˜(tied weights)[[torchscript-flag-and-tied-weights]]

`torchscript` í”Œë˜ê·¸ê°€ í•„ìš”í•œ ì´ìœ ëŠ” ëŒ€ë¶€ë¶„ì˜ ğŸ¤— Transformers ì–¸ì–´ ëª¨ë¸ì—ì„œ `Embedding` ë ˆì´ì–´ì™€ `Decoding` ë ˆì´ì–´ ê°„ì˜ ë¬¶ì¸ ê°€ì¤‘ì¹˜(tied weights)ê°€ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
TorchScriptëŠ” ë¬¶ì¸ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§„ ëª¨ë¸ì„ ë‚´ë³´ë‚¼ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, ë¯¸ë¦¬ ê°€ì¤‘ì¹˜ë¥¼ í’€ê³  ë³µì œí•´ì•¼ í•©ë‹ˆë‹¤.

`torchscript` í”Œë˜ê·¸ë¡œ ì¸ìŠ¤í„´ìŠ¤í™”ëœ ëª¨ë¸ì€ `Embedding` ë ˆì´ì–´ì™€ `Decoding` ë ˆì´ì–´ê°€ ë¶„ë¦¬ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì´í›„ì— í›ˆë ¨í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.
í›ˆë ¨ì„ í•˜ê²Œ ë˜ë©´ ë‘ ë ˆì´ì–´ ê°„ ë™ê¸°í™”ê°€ í•´ì œë˜ì–´ ì˜ˆìƒì¹˜ ëª»í•œ ê²°ê³¼ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì–¸ì–´ ëª¨ë¸ í—¤ë“œë¥¼ ê°–ì§€ ì•Šì€ ëª¨ë¸ì€ ê°€ì¤‘ì¹˜ê°€ ë¬¶ì—¬ ìˆì§€ ì•Šì•„ì„œ ì´ ë¬¸ì œê°€ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
ì´ëŸ¬í•œ ëª¨ë¸ë“¤ì€ `torchscript` í”Œë˜ê·¸ ì—†ì´ ì•ˆì „í•˜ê²Œ ë‚´ë³´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ë”ë¯¸ ì…ë ¥ê³¼ í‘œì¤€ ê¸¸ì´[[dummy-inputs-and-standard-lengths]]

ë”ë¯¸ ì…ë ¥(dummy inputs)ì€ ëª¨ë¸ì˜ ìˆœì „íŒŒ(forward pass)ì— ì‚¬ìš©ë©ë‹ˆë‹¤. 
ì…ë ¥ ê°’ì´ ë ˆì´ì–´ë¥¼ í†µí•´ ì „íŒŒë˜ëŠ” ë™ì•ˆ, PyTorchëŠ” ê° í…ì„œì—ì„œ ì‹¤í–‰ëœ ë‹¤ë¥¸ ì—°ì‚°ì„ ì¶”ì í•©ë‹ˆë‹¤. 
ì´ëŸ¬í•œ ê¸°ë¡ëœ ì—°ì‚°ì€ ëª¨ë¸ì˜ *ì¶”ì (trace)*ì„ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.

ì¶”ì ì€ ì…ë ¥ì˜ ì°¨ì›ì„ ê¸°ì¤€ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤. 
ë”°ë¼ì„œ ë”ë¯¸ ì…ë ¥ì˜ ì°¨ì›ì— ì œí•œë˜ì–´, ë‹¤ë¥¸ ì‹œí€€ìŠ¤ ê¸¸ì´ë‚˜ ë°°ì¹˜ í¬ê¸°ì—ì„œëŠ” ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. 
ë‹¤ë¥¸ í¬ê¸°ë¡œ ì‹œë„í•  ê²½ìš° ë‹¤ìŒê³¼ ê°™ì€ ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤:

```
`The expanded size of the tensor (3) must match the existing size (7) at non-singleton dimension 2`
```
ì¶”ë¡  ì¤‘ ëª¨ë¸ì— ê³µê¸‰ë  ê°€ì¥ í° ì…ë ¥ë§Œí¼ í° ë”ë¯¸ ì…ë ¥ í¬ê¸°ë¡œ ëª¨ë¸ì„ ì¶”ì í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. 
íŒ¨ë”©ì€ ëˆ„ë½ëœ ê°’ì„ ì±„ìš°ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
ê·¸ëŸ¬ë‚˜ ëª¨ë¸ì´ ë” í° ì…ë ¥ í¬ê¸°ë¡œ ì¶”ì ë˜ê¸° ë•Œë¬¸ì—, í–‰ë ¬ì˜ ì°¨ì›ì´ ì»¤ì§€ê³  ê³„ì‚°ëŸ‰ì´ ë§ì•„ì§‘ë‹ˆë‹¤.

ë‹¤ì–‘í•œ ì‹œí€€ìŠ¤ ê¸¸ì´ ëª¨ë¸ì„ ë‚´ë³´ë‚¼ ë•ŒëŠ” ê° ì…ë ¥ì— ëŒ€í•´ ìˆ˜í–‰ë˜ëŠ” ì´ ì—°ì‚° íšŸìˆ˜ì— ì£¼ì˜í•˜ê³  ì„±ëŠ¥ì„ ì£¼ì˜ ê¹Šê²Œ í™•ì¸í•˜ì„¸ìš”.

## Pythonì—ì„œ TorchScript ì‚¬ìš©í•˜ê¸°[[using-torchscript-in-python]]

ì´ ì„¹ì…˜ì—ì„œëŠ” ëª¨ë¸ì„ ì €ì¥í•˜ê³  ê°€ì ¸ì˜¤ëŠ” ë°©ë²•, ì¶”ì ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡ í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

### ëª¨ë¸ ì €ì¥í•˜ê¸°[[saving-a-model]]

`BertModel`ì„ TorchScriptë¡œ ë‚´ë³´ë‚´ë ¤ë©´ `BertConfig` í´ë˜ìŠ¤ì—ì„œ `BertModel`ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•œ ë‹¤ìŒ, `traced_bert.pt`ë¼ëŠ” íŒŒì¼ëª…ìœ¼ë¡œ ë””ìŠ¤í¬ì— ì €ì¥í•˜ë©´ ë©ë‹ˆë‹¤.

```python
from transformers import BertModel, BertTokenizer, BertConfig
import torch

enc = BertTokenizer.from_pretrained("google-bert/bert-base-uncased")

# ì…ë ¥ í…ìŠ¤íŠ¸ í† í°í™”í•˜ê¸°
text = "[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]"
tokenized_text = enc.tokenize(text)

# ì…ë ¥ í† í° ì¤‘ í•˜ë‚˜ë¥¼ ë§ˆìŠ¤í‚¹í•˜ê¸°
masked_index = 8
tokenized_text[masked_index] = "[MASK]"
indexed_tokens = enc.convert_tokens_to_ids(tokenized_text)
segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]

# ë”ë¯¸ ì…ë ¥ ë§Œë“¤ê¸°
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])
dummy_input = [tokens_tensor, segments_tensors]

# torchscript í”Œë˜ê·¸ë¡œ ëª¨ë¸ ì´ˆê¸°í™”í•˜ê¸°
# ì´ ëª¨ë¸ì€ LM í—¤ë“œê°€ ì—†ìœ¼ë¯€ë¡œ í•„ìš”í•˜ì§€ ì•Šì§€ë§Œ, í”Œë˜ê·¸ë¥¼ Trueë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
config = BertConfig(
    vocab_size_or_config_json_file=32000,
    hidden_size=768,
    num_hidden_layers=12,
    num_attention_heads=12,
    intermediate_size=3072,
    torchscript=True,
)

# ëª¨ë¸ì„ ì¸ìŠ¤í„´íŠ¸í™”í•˜ê¸°
model = BertModel(config)

# ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ë‘ì–´ì•¼ í•©ë‹ˆë‹¤.
model.eval()

# ë§Œì•½ *from_pretrained*ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ëŠ” ê²½ìš°, TorchScript í”Œë˜ê·¸ë¥¼ ì‰½ê²Œ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
model = BertModel.from_pretrained("google-bert/bert-base-uncased", torchscript=True)

# ì¶”ì  ìƒì„±í•˜ê¸°
traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])
torch.jit.save(traced_model, "traced_bert.pt")
```

### ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°[[loading-a-model]]

ì´ì œ ì´ì „ì— ì €ì¥í•œ `BertModel`, ì¦‰ `traced_bert.pt`ë¥¼ ë””ìŠ¤í¬ì—ì„œ ê°€ì ¸ì˜¤ê³ , ì´ì „ì— ì´ˆê¸°í™”í•œ `dummy_input`ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
loaded_model = torch.jit.load("traced_bert.pt")
loaded_model.eval()

all_encoder_layers, pooled_output = loaded_model(*dummy_input)
```

### ì¶”ì ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡ í•˜ê¸°[[using-a-traced-model-for-inference]]

`__call__` ì´ì¤‘ ì–¸ë”ìŠ¤ì½”ì–´(dunder) ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡ ì— ì¶”ì ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì„¸ìš”:

```python
traced_model(tokens_tensor, segments_tensors)
```

## Neuron SDKë¡œ Hugging Face TorchScript ëª¨ë¸ì„ AWSì— ë°°í¬í•˜ê¸°[[deploy-hugging-face-torchscript-models-to-aws-with-the-neuron-sdk]]

AWSê°€ í´ë¼ìš°ë“œì—ì„œ ì €ë¹„ìš©, ê³ ì„±ëŠ¥ ë¨¸ì‹  ëŸ¬ë‹ ì¶”ë¡ ì„ ìœ„í•œ [Amazon EC2 Inf1](https://aws.amazon.com/ec2/instance-types/inf1/) ì¸ìŠ¤í„´ìŠ¤ ì œí’ˆêµ°ì„ ì¶œì‹œí–ˆìŠµë‹ˆë‹¤. 
Inf1 ì¸ìŠ¤í„´ìŠ¤ëŠ” ë”¥ëŸ¬ë‹ ì¶”ë¡  ì›Œí¬ë¡œë“œì— íŠ¹í™”ëœ ë§ì¶¤ í•˜ë“œì›¨ì–´ ê°€ì†ê¸°ì¸ AWS Inferentia ì¹©ìœ¼ë¡œ êµ¬ë™ë©ë‹ˆë‹¤. 
[AWS Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/#)ì€ Inferentiaë¥¼ ìœ„í•œ SDKë¡œ, Inf1ì— ë°°í¬í•˜ê¸° ìœ„í•œ transformers ëª¨ë¸ ì¶”ì  ë° ìµœì í™”ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. 
Neuron SDKëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:

1. ì½”ë“œ í•œ ì¤„ë§Œ ë³€ê²½í•˜ë©´ í´ë¼ìš°ë“œ ì¶”ë¡ ë¥¼ ìœ„í•´ TorchScript ëª¨ë¸ì„ ì¶”ì í•˜ê³  ìµœì í™”í•  ìˆ˜ ìˆëŠ” ì‰¬ìš´ API
2. ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ ì„±ëŠ¥ ìµœì í™”ë¡œ [ë¹„ìš© íš¨ìœ¨ í–¥ìƒ](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/benchmark/>)
3. [PyTorch](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/bert_tutorial/tutorial_pretrained_bert.html) ë˜ëŠ” [TensorFlow](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/tensorflow/huggingface_bert/huggingface_bert.html)ë¡œ êµ¬ì¶•ëœ Hugging Face transformers ëª¨ë¸ ì§€ì›

### ì‹œì‚¬ì [[implications]]

[BERT (Bidirectional Encoder Representations from Transformers)](https://huggingface.co/docs/transformers/main/model_doc/bert) ì•„í‚¤í…ì²˜ ë˜ëŠ” ê·¸ ë³€í˜•ì¸ [distilBERT](https://huggingface.co/docs/transformers/main/model_doc/distilbert) ë° [roBERTa](https://huggingface.co/docs/transformers/main/model_doc/roberta)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ Transformers ëª¨ë¸ì€ ì¶”ì¶œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ, ì‹œí€€ìŠ¤ ë¶„ë¥˜ ë° í† í° ë¶„ë¥˜ì™€ ê°™ì€ ë¹„ìƒì„± ì‘ì—… ì‹œ Inf1ì—ì„œ ìµœìƒì˜ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. 
ê·¸ëŸ¬ë‚˜ í…ìŠ¤íŠ¸ ìƒì„± ì‘ì—…ë„ [AWS Neuron MarianMT íŠœí† ë¦¬ì–¼](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/transformers-marianmt.html)ì„ ë”°ë¼ Inf1ì—ì„œ ì‹¤í–‰ë˜ë„ë¡ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

Inferentiaì—ì„œ ë°”ë¡œ ë³€í™˜í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì— ëŒ€í•œ ìì„¸í•œ ì •ë³´ëŠ” Neuron ë¬¸ì„œì˜ [Model Architecture Fit](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/models/models-inferentia.html#models-inferentia) ì„¹ì…˜ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ì¢…ì†ì„±[[dependencies]]

AWS Neuronì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë³€í™˜í•˜ë ¤ë©´ [Neuron SDK í™˜ê²½](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-frameworks/pytorch-neuron/index.html#installation-guide)ì´ í•„ìš”í•©ë‹ˆë‹¤.
 ì´ëŠ” [AWS Deep Learning AMI](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-launching.html)ì— ë¯¸ë¦¬ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

### AWS Neuronìœ¼ë¡œ ëª¨ë¸ ë³€í™˜í•˜ê¸°[[converting-a-model-for-aws-neuron]]

`BertModel`ì„ ì¶”ì í•˜ë ¤ë©´, [Pythonì—ì„œ TorchScript ì‚¬ìš©í•˜ê¸°](torchscript#using-torchscript-in-python)ì—ì„œì™€ ë™ì¼í•œ ì½”ë“œë¥¼ ì‚¬ìš©í•´ì„œ AWS NEURONìš© ëª¨ë¸ì„ ë³€í™˜í•©ë‹ˆë‹¤. 
`torch.neuron` í”„ë ˆì„ì›Œí¬ ìµìŠ¤í…ì…˜ì„ ê°€ì ¸ì™€ Python APIë¥¼ í†µí•´ Neuron SDKì˜ êµ¬ì„± ìš”ì†Œì— ì ‘ê·¼í•©ë‹ˆë‹¤:

```python
from transformers import BertModel, BertTokenizer, BertConfig
import torch
import torch.neuron
```

ë‹¤ìŒ ì¤„ë§Œ ìˆ˜ì •í•˜ë©´ ë©ë‹ˆë‹¤:

```diff
- torch.jit.trace(model, [tokens_tensor, segments_tensors])
+ torch.neuron.trace(model, [token_tensor, segments_tensors])
```

ì´ë¡œì¨ Neuron SDKê°€ ëª¨ë¸ì„ ì¶”ì í•˜ê³  Inf1 ì¸ìŠ¤í„´ìŠ¤ì— ìµœì í™”í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.

AWS Neuron SDKì˜ ê¸°ëŠ¥, ë„êµ¬, ì˜ˆì œ íŠœí† ë¦¬ì–¼ ë° ìµœì‹  ì—…ë°ì´íŠ¸ì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ë ¤ë©´ [AWS NeuronSDK ë¬¸ì„œ](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.
