<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# ë‘˜ëŸ¬ë³´ê¸° [[quick-tour]]

[[open-in-colab]]

ğŸ¤— Transformersë¥¼ ì‹œì‘í•´ë³´ì„¸ìš”! ê°œë°œí•´ë³¸ ì ì´ ì—†ë”ë¼ë„ ì‰½ê²Œ ì½ì„ ìˆ˜ ìˆë„ë¡ ì“°ì¸ ì´ ê¸€ì€ [`pipeline`](./main_classes/pipelines)ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡ í•˜ê³ , ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ê³¼ ì „ì²˜ë¦¬ê¸°ë¥¼ [AutoClass](./model_doc/auto)ë¡œ ë¡œë“œí•˜ê³ , PyTorch ë˜ëŠ” TensorFlowë¡œ ëª¨ë¸ì„ ë¹ ë¥´ê²Œ í•™ìŠµì‹œí‚¤ëŠ” ë°©ë²•ì„ ì†Œê°œí•´ ë“œë¦´ ê²ƒì…ë‹ˆë‹¤. ë³¸ ê°€ì´ë“œì—ì„œ ì†Œê°œë˜ëŠ” ê°œë…ì„ (íŠ¹íˆ ì´ˆë³´ìì˜ ê´€ì ìœ¼ë¡œ) ë” ì¹œì ˆí•˜ê²Œ ì ‘í•˜ê³  ì‹¶ë‹¤ë©´, íŠœí† ë¦¬ì–¼ì´ë‚˜ [ì½”ìŠ¤](https://huggingface.co/course/chapter1/1)ë¥¼ ì°¸ì¡°í•˜ê¸°ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.

ì‹œì‘í•˜ê¸° ì „ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ëª¨ë‘ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:

```bash
!pip install transformers datasets evaluate accelerate
```

ë˜í•œ ì„ í˜¸í•˜ëŠ” ë¨¸ì‹  ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ë¥¼ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤:

<frameworkcontent>
<pt>

```bash
pip install torch
```
</pt>
<tf>

```bash
pip install tensorflow
```
</tf>
</frameworkcontent>

## íŒŒì´í”„ë¼ì¸ [[pipeline]]

<Youtube id="tiZFewofSLM"/>

[`pipeline`](./main_classes/pipelines)ì€ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ë¡œ ì¶”ë¡ í•˜ê¸°ì— ê°€ì¥ ì‰½ê³  ë¹ ë¥¸ ë°©ë²•ì…ë‹ˆë‹¤. [`pipeline`]ì€ ì—¬ëŸ¬ ëª¨ë‹¬ë¦¬í‹°ì—ì„œ ë‹¤ì–‘í•œ ê³¼ì—…ì„ ì‰½ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìœ¼ë©°, ì•„ë˜ í‘œì— í‘œì‹œëœ ëª‡ ê°€ì§€ ê³¼ì—…ì„ ê¸°ë³¸ì ìœ¼ë¡œ ì§€ì›í•©ë‹ˆë‹¤:

<Tip>

ì‚¬ìš© ê°€ëŠ¥í•œ ì‘ì—…ì˜ ì „ì²´ ëª©ë¡ì€ [Pipelines API ì°¸ì¡°](./main_classes/pipelines)ë¥¼ í™•ì¸í•˜ì„¸ìš”.

</Tip>

| **íƒœìŠ¤í¬**      | **ì„¤ëª…**                                                             | **ëª¨ë‹¬ë¦¬í‹°**     | **íŒŒì´í”„ë¼ì¸ ID**                             |
|-----------------|----------------------------------------------------------------------|------------------|-----------------------------------------------|
| í…ìŠ¤íŠ¸ ë¶„ë¥˜      | í…ìŠ¤íŠ¸ì— ì•Œë§ì€ ë ˆì´ë¸” ë¶™ì´ê¸°                                         | ìì—°ì–´ ì²˜ë¦¬(NLP) | pipeline(task="sentiment-analysis")           |
| í…ìŠ¤íŠ¸ ìƒì„±      | ì£¼ì–´ì§„ ë¬¸ìì—´ ì…ë ¥ê³¼ ì´ì–´ì§€ëŠ” í…ìŠ¤íŠ¸ ìƒì„±í•˜ê¸°                       | ìì—°ì–´ ì²˜ë¦¬(NLP) | pipeline(task="text-generation")              |
| ê°œì²´ëª… ì¸ì‹      | ë¬¸ìì—´ì˜ ê° í† í°ë§ˆë‹¤ ì•Œë§ì€ ë ˆì´ë¸” ë¶™ì´ê¸° (ì¸ë¬¼, ì¡°ì§, ì¥ì†Œ ë“±ë“±)     | ìì—°ì–´ ì²˜ë¦¬(NLP) | pipeline(task="ner")                          |
| ì§ˆì˜ì‘ë‹µ         | ì£¼ì–´ì§„ ë¬¸ë§¥ê³¼ ì§ˆë¬¸ì— ë”°ë¼ ì˜¬ë°”ë¥¸ ëŒ€ë‹µí•˜ê¸°                           | ìì—°ì–´ ì²˜ë¦¬(NLP) | pipeline(task="question-answering")           |
| ë¹ˆì¹¸ ì±„ìš°ê¸°      | ë¬¸ìì—´ì˜ ë¹ˆì¹¸ì— ì•Œë§ì€ í† í° ë§ì¶”ê¸°                                  | ìì—°ì–´ ì²˜ë¦¬(NLP) | pipeline(task="fill-mask")                    |
| ìš”ì•½             | í…ìŠ¤íŠ¸ë‚˜ ë¬¸ì„œë¥¼ ìš”ì•½í•˜ê¸°                                            | ìì—°ì–´ ì²˜ë¦¬(NLP) | pipeline(task="summarization")                |
| ë²ˆì—­             | í…ìŠ¤íŠ¸ë¥¼ í•œ ì–¸ì–´ì—ì„œ ë‹¤ë¥¸ ì–¸ì–´ë¡œ ë²ˆì—­í•˜ê¸°                           | ìì—°ì–´ ì²˜ë¦¬(NLP) | pipeline(task="translation")                  |
| ì´ë¯¸ì§€ ë¶„ë¥˜      | ì´ë¯¸ì§€ì— ì•Œë§ì€ ë ˆì´ë¸” ë¶™ì´ê¸°                                         | ì»´í“¨í„° ë¹„ì „(CV)  | pipeline(task="image-classification")         |
| ì´ë¯¸ì§€ ë¶„í•       | ì´ë¯¸ì§€ì˜ í”½ì…€ë§ˆë‹¤ ë ˆì´ë¸” ë¶™ì´ê¸°(ì‹œë§¨í‹±, íŒŒë†‰í‹± ë° ì¸ìŠ¤í„´ìŠ¤ ë¶„í•  í¬í•¨) | ì»´í“¨í„° ë¹„ì „(CV)  | pipeline(task="image-segmentation")           |
| ê°ì²´ íƒì§€        | ì´ë¯¸ì§€ ì† ê°ì²´ì˜ ê²½ê³„ ìƒìë¥¼ ê·¸ë¦¬ê³  í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•˜ê¸°               | ì»´í“¨í„° ë¹„ì „(CV)  | pipeline(task="object-detection")             |
| ì˜¤ë””ì˜¤ ë¶„ë¥˜      | ì˜¤ë””ì˜¤ íŒŒì¼ì— ì•Œë§ì€ ë ˆì´ë¸” ë¶™ì´ê¸°                                    | ì˜¤ë””ì˜¤           | pipeline(task="audio-classification")         |
| ìë™ ìŒì„± ì¸ì‹   | ì˜¤ë””ì˜¤ íŒŒì¼ ì† ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë°”ê¾¸ê¸°                               | ì˜¤ë””ì˜¤           | pipeline(task="automatic-speech-recognition") |
| ì‹œê° ì§ˆì˜ì‘ë‹µ    | ì£¼ì–´ì§„ ì´ë¯¸ì§€ì™€ ì§ˆë¬¸ì— ëŒ€í•´ ì˜¬ë°”ë¥´ê²Œ ëŒ€ë‹µí•˜ê¸°                       | ë©€í‹°ëª¨ë‹¬         | pipeline(task="vqa")                          |
| ë¬¸ì„œ ì§ˆì˜ì‘ë‹µ    | ì£¼ì–´ì§„ ë¬¸ì„œì™€ ì§ˆë¬¸ì— ëŒ€í•´ ì˜¬ë°”ë¥´ê²Œ ëŒ€ë‹µí•˜ê¸°                         | ë©€í‹°ëª¨ë‹¬         | pipeline(task="document-question-answering")  |
| ì´ë¯¸ì§€ ìº¡ì…˜ ë‹¬ê¸° | ì£¼ì–´ì§„ ì´ë¯¸ì§€ì˜ ìº¡ì…˜ ìƒì„±í•˜ê¸°                                       | ë©€í‹°ëª¨ë‹¬         | pipeline(task="image-to-text")                |

ë¨¼ì € [`pipeline`]ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ê³  ì‚¬ìš©í•  ì‘ì—…ì„ ì§€ì •í•©ë‹ˆë‹¤. ì´ ê°€ì´ë“œì—ì„œëŠ” ê°ì • ë¶„ì„ì„ ìœ„í•´ [`pipeline`]ì„ ì‚¬ìš©í•˜ëŠ” ì˜ˆì œë¥¼ ë³´ì—¬ë“œë¦¬ê² ìŠµë‹ˆë‹¤:

```py
>>> from transformers import pipeline

>>> classifier = pipeline("sentiment-analysis")
```

[`pipeline`]ì€ ê°ì • ë¶„ì„ì„ ìœ„í•œ [ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸](https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english)ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ê³  ìºì‹œí•©ë‹ˆë‹¤. ì´ì œ `classifier`ë¥¼ ëŒ€ìƒ í…ìŠ¤íŠ¸ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
>>> classifier("We are very happy to show you the ğŸ¤— Transformers library.")
[{'label': 'POSITIVE', 'score': 0.9998}]
```

ë§Œì•½ ì…ë ¥ì´ ì—¬ëŸ¬ ê°œ ìˆëŠ” ê²½ìš°, ì…ë ¥ì„ ë¦¬ìŠ¤íŠ¸ë¡œ [`pipeline`]ì— ì „ë‹¬í•˜ì—¬, ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì˜ ì¶œë ¥ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ì´ë£¨ì–´ì§„ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
>>> results = classifier(["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."])
>>> for result in results:
...     print(f"label: {result['label']}, with score: {round(result['score'], 4)}")
label: POSITIVE, with score: 0.9998
label: NEGATIVE, with score: 0.5309
```

[`pipeline`]ì€ ì£¼ì–´ì§„ ê³¼ì—…ì— ê´€ê³„ì—†ì´ ë°ì´í„°ì…‹ ì „ë¶€ë¥¼ ìˆœíšŒí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì´ ì˜ˆì œì—ì„œëŠ” ìë™ ìŒì„± ì¸ì‹ì„ ê³¼ì—…ìœ¼ë¡œ ì„ íƒí•´ ë³´ê² ìŠµë‹ˆë‹¤:

```py
>>> import torch
>>> from transformers import pipeline

>>> speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h")
```

ë°ì´í„°ì…‹ì„ ë¡œë“œí•  ì°¨ë¡€ì…ë‹ˆë‹¤. (ìì„¸í•œ ë‚´ìš©ì€ ğŸ¤— Datasets [ì‹œì‘í•˜ê¸°](https://huggingface.co/docs/datasets/quickstart#audio)ì„ ì°¸ì¡°í•˜ì„¸ìš”) ì—¬ê¸°ì—ì„œëŠ” [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ê² ìŠµë‹ˆë‹¤:

```py
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")  # doctest: +IGNORE_RESULT
```

ë°ì´í„°ì…‹ì˜ ìƒ˜í”Œë§ ë ˆì´íŠ¸ê°€ ê¸°ì¡´ ëª¨ë¸ì¸ [`facebook/wav2vec2-base-960h`](https://huggingface.co/facebook/wav2vec2-base-960h)ì˜ í›ˆë ¨ ë‹¹ì‹œ ìƒ˜í”Œë§ ë ˆì´íŠ¸ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤:

```py
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))
```

`"audio"` ì—´ì„ í˜¸ì¶œí•˜ë©´ ìë™ìœ¼ë¡œ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ê°€ì ¸ì™€ì„œ ë¦¬ìƒ˜í”Œë§í•©ë‹ˆë‹¤. ì²« 4ê°œ ìƒ˜í”Œì—ì„œ ì›ì‹œ ì›¨ì´ë¸Œí¼ ë°°ì—´ì„ ì¶”ì¶œí•˜ê³  íŒŒì´í”„ë¼ì¸ì— ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬í•˜ì„¸ìš”:

```py
>>> result = speech_recognizer(dataset[:4]["audio"])
>>> print([d["text"] for d in result])
['I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT', "FONDERING HOW I'D SET UP A JOIN TO HELL T WITH MY WIFE AND WHERE THE AP MIGHT BE", "I I'D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I'M NOT SEEING THE OPTION TO DO IT ON THE APSO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AN I'M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS", 'HOW DO I FURN A JOINA COUT']
```

ìŒì„±ì´ë‚˜ ë¹„ì „ê³¼ ê°™ì´ ì…ë ¥ì´ í° ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì˜ ê²½ìš°, ëª¨ë“  ì…ë ¥ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œí•˜ë ¤ë©´ ë¦¬ìŠ¤íŠ¸ ëŒ€ì‹  ì œë„ˆë ˆì´í„° í˜•íƒœë¡œ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [Pipelines API ì°¸ì¡°](./main_classes/pipelines)ë¥¼ í™•ì¸í•˜ì„¸ìš”.

### íŒŒì´í”„ë¼ì¸ì—ì„œ ë‹¤ë¥¸ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì‚¬ìš©í•˜ê¸° [[use-another-model-and-tokenizer-in-the-pipeline]]

[`pipeline`]ì€ [Hub](https://huggingface.co/models)ì˜ ëª¨ë“  ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, [`pipeline`]ì„ ë‹¤ë¥¸ ìš©ë„ì— ë§ê²Œ ì‰½ê²Œ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í”„ë‘ìŠ¤ì–´ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„  Hubì˜ íƒœê·¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì ì ˆí•œ ëª¨ë¸ì„ í•„í„°ë§í•˜ë©´ ë©ë‹ˆë‹¤. í•„í„°ë§ëœ ê²°ê³¼ì˜ ìƒìœ„ í•­ëª©ìœ¼ë¡œëŠ” í”„ë‘ìŠ¤ì–´ í…ìŠ¤íŠ¸ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë‹¤êµ­ì–´ [BERT ëª¨ë¸](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment)ì´ ë°˜í™˜ë©ë‹ˆë‹¤:

```py
>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
```

<frameworkcontent>
<pt>
[`AutoModelForSequenceClassification`]ê³¼ [`AutoTokenizer`]ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ê³¼ ê´€ë ¨ëœ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ì„¸ìš” (ë‹¤ìŒ ì„¹ì…˜ì—ì„œ [`AutoClass`]ì— ëŒ€í•´ ë” ìì„¸íˆ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤):

```py
>>> from transformers import AutoTokenizer, AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained(model_name)
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```
</pt>
<tf>
[`TFAutoModelForSequenceClassification`]ê³¼ [`AutoTokenizer`]ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ê³¼ ê´€ë ¨ëœ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ì„¸ìš” (ë‹¤ìŒ ì„¹ì…˜ì—ì„œ [`TFAutoClass`]ì— ëŒ€í•´ ë” ìì„¸íˆ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤):

```py
>>> from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

>>> model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```
</tf>
</frameworkcontent>

[`pipeline`]ì—ì„œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì§€ì •í•˜ë©´, ì´ì œ `classifier`ë¥¼ í”„ë‘ìŠ¤ì–´ í…ìŠ¤íŠ¸ì— ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
>>> classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
>>> classifier("Nous sommes trÃ¨s heureux de vous prÃ©senter la bibliothÃ¨que ğŸ¤— Transformers.")
[{'label': '5 stars', 'score': 0.7273}]
```

ë§ˆë•…í•œ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ë¯¸ì„¸ì¡°ì •í•´ì•¼ í•©ë‹ˆë‹¤. ë¯¸ì„¸ì¡°ì • ë°©ë²•ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ [ë¯¸ì„¸ì¡°ì • íŠœí† ë¦¬ì–¼](./training)ì„ ì°¸ì¡°í•˜ì„¸ìš”. ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ë¯¸ì„¸ì¡°ì •í•œ í›„ì—ëŠ” ëª¨ë¸ì„ Hubì˜ ì»¤ë®¤ë‹ˆí‹°ì™€ ê³µìœ í•˜ì—¬ ë¨¸ì‹ ëŸ¬ë‹ ë¯¼ì£¼í™”ì— ê¸°ì—¬í•´ì£¼ì„¸ìš”! ğŸ¤—

## AutoClass [[autoclass]]

<Youtube id="AhChOFRegn4"/>

[`AutoModelForSequenceClassification`]ê³¼ [`AutoTokenizer`] í´ë˜ìŠ¤ëŠ” ìœ„ì—ì„œ ë‹¤ë£¬ [`pipeline`]ì˜ ê¸°ëŠ¥ì„ êµ¬í˜„í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. [AutoClass](./model_doc/auto)ëŠ” ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì˜ ì•„í‚¤í…ì²˜ë¥¼ ì´ë¦„ì´ë‚˜ ê²½ë¡œì—ì„œ ìë™ìœ¼ë¡œ ê°€ì ¸ì˜¤ëŠ” 'ë°”ë¡œê°€ê¸°'ì…ë‹ˆë‹¤. ê³¼ì—…ì— ì í•©í•œ `AutoClass`ë¥¼ ì„ íƒí•˜ê³  í•´ë‹¹ ì „ì²˜ë¦¬ í´ë˜ìŠ¤ë¥¼ ì„ íƒí•˜ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤.

ì´ì „ ì„¹ì…˜ì˜ ì˜ˆì œë¡œ ëŒì•„ê°€ì„œ [`pipeline`]ì˜ ê²°ê³¼ë¥¼ `AutoClass`ë¥¼ í™œìš©í•´ ë³µì œí•˜ëŠ” ë°©ë²•ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

### AutoTokenizer [[autotokenizer]]

í† í¬ë‚˜ì´ì €ëŠ” í…ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ìˆ«ì ë°°ì—´ í˜•íƒœë¡œ ì „ì²˜ë¦¬í•˜ëŠ” ì—­í• ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤. í† í°í™” ê³¼ì •ì—ëŠ” ë‹¨ì–´ë¥¼ ì–´ë””ì—ì„œ ëŠì„ì§€, ì–´ëŠ ìˆ˜ì¤€ê¹Œì§€ ë‚˜ëˆŒì§€ì™€ ê°™ì€ ì—¬ëŸ¬ ê·œì¹™ë“¤ì´ ìˆìŠµë‹ˆë‹¤ (í† í°í™”ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ [í† í¬ë‚˜ì´ì € ìš”ì•½](./tokenizer_summary)ì„ ì°¸ì¡°í•˜ì„¸ìš”). ê°€ì¥ ì¤‘ìš”í•œ ì ì€ ëª¨ë¸ì´ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ê³¼ ë™ì¼í•œ í† í°í™” ê·œì¹™ì„ ì‚¬ìš©í•˜ë„ë¡ ë™ì¼í•œ ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ í† í¬ë‚˜ì´ì €ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.

[`AutoTokenizer`]ë¡œ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ì„¸ìš”:

```py
>>> from transformers import AutoTokenizer

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```

í…ìŠ¤íŠ¸ë¥¼ í† í¬ë‚˜ì´ì €ì— ì „ë‹¬í•˜ì„¸ìš”:

```py
>>> encoding = tokenizer("We are very happy to show you the ğŸ¤— Transformers library.")
>>> print(encoding)
{'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

í† í¬ë‚˜ì´ì €ëŠ” ë‹¤ìŒì„ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤:

* [input_ids](./glossary#input-ids): í† í°ì˜ ìˆ«ì í‘œí˜„.
* [attention_mask](.glossary#attention-mask): ì–´ë–¤ í† í°ì— ì£¼ì˜ë¥¼ ê¸°ìš¸ì—¬ì•¼ í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

í† í¬ë‚˜ì´ì €ëŠ” ì…ë ¥ì„ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œë„ ë°›ì„ ìˆ˜ ìˆìœ¼ë©°, í…ìŠ¤íŠ¸ë¥¼ íŒ¨ë”©í•˜ê³  ì˜ë¼ë‚´ì–´ ì¼ì •í•œ ê¸¸ì´ì˜ ë¬¶ìŒì„ ë°˜í™˜í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:

<frameworkcontent>
<pt>

```py
>>> pt_batch = tokenizer(
...     ["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."],
...     padding=True,
...     truncation=True,
...     max_length=512,
...     return_tensors="pt",
... )
```
</pt>
<tf>

```py
>>> tf_batch = tokenizer(
...     ["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."],
...     padding=True,
...     truncation=True,
...     max_length=512,
...     return_tensors="tf",
... )
```
</tf>
</frameworkcontent>

<Tip>

[ì „ì²˜ë¦¬](./preprocessing) íŠœí† ë¦¬ì–¼ì„ ì°¸ì¡°í•˜ì‹œë©´ í† í°í™”ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ê³¼ í•¨ê»˜ ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ì™€ ë©€í‹°ëª¨ë‹¬ ì…ë ¥ì„ ì „ì²˜ë¦¬í•˜ê¸° ìœ„í•œ [`AutoImageProcessor`]ì™€ [`AutoFeatureExtractor`], [`AutoProcessor`]ì˜ ì‚¬ìš©ë°©ë²•ë„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

</Tip>

### AutoModel [[automodel]]

<frameworkcontent>
<pt>
ğŸ¤— TransformersëŠ” ì‚¬ì „ í›ˆë ¨ëœ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê°„ë‹¨í•˜ê³  í†µí•©ëœ ë°©ë²•ìœ¼ë¡œ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¦‰, [`AutoTokenizer`]ì²˜ëŸ¼ [`AutoModel`]ì„ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìœ ì¼í•œ ì°¨ì´ì ì€ ê³¼ì—…ì— ì•Œë§ì€ [`AutoModel`]ì„ ì„ íƒí•´ì•¼ í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤. í…ìŠ¤íŠ¸ (ë˜ëŠ” ì‹œí€€ìŠ¤) ë¶„ë¥˜ì˜ ê²½ìš° [`AutoModelForSequenceClassification`]ì„ ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤:

```py
>>> from transformers import AutoModelForSequenceClassification

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
```

<Tip>

[`AutoModel`] í´ë˜ìŠ¤ì—ì„œ ì§€ì›í•˜ëŠ” ê³¼ì—…ì— ëŒ€í•´ì„œëŠ” [ê³¼ì—… ìš”ì•½](./task_summary)ì„ ì°¸ì¡°í•˜ì„¸ìš”.

</Tip>

ì´ì œ ì „ì²˜ë¦¬ëœ ì…ë ¥ ë¬¶ìŒì„ ì§ì ‘ ëª¨ë¸ì— ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤. ì•„ë˜ì²˜ëŸ¼ `**`ë¥¼ ì•ì— ë¶™ì—¬ ë”•ì…”ë„ˆë¦¬ë¥¼ í’€ì–´ì£¼ë©´ ë©ë‹ˆë‹¤:

```py
>>> pt_outputs = pt_model(**pt_batch)
```

ëª¨ë¸ì˜ ìµœì¢… í™œì„±í™” í•¨ìˆ˜ ì¶œë ¥ì€ `logits` ì†ì„±ì— ë‹´ê²¨ìˆìŠµë‹ˆë‹¤. `logits`ì— softmax í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ í™•ë¥ ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
>>> from torch import nn

>>> pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
>>> print(pt_predictions)
tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],
        [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)
```
</pt>
<tf>
ğŸ¤— TransformersëŠ” ì‚¬ì „ í›ˆë ¨ëœ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê°„ë‹¨í•˜ê³  í†µí•©ëœ ë°©ë²•ìœ¼ë¡œ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¦‰, [`AutoTokenizer`]ì²˜ëŸ¼ [`TFAutoModel`]ì„ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìœ ì¼í•œ ì°¨ì´ì ì€ ê³¼ì—…ì— ì•Œë§ì€ [`TFAutoModel`]ì„ ì„ íƒí•´ì•¼ í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤. í…ìŠ¤íŠ¸ (ë˜ëŠ” ì‹œí€€ìŠ¤) ë¶„ë¥˜ì˜ ê²½ìš° [`TFAutoModelForSequenceClassification`]ì„ ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤:

```py
>>> from transformers import TFAutoModelForSequenceClassification

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
```

<Tip>

[`AutoModel`] í´ë˜ìŠ¤ì—ì„œ ì§€ì›í•˜ëŠ” ê³¼ì—…ì— ëŒ€í•´ì„œëŠ” [ê³¼ì—… ìš”ì•½](./task_summary)ì„ ì°¸ì¡°í•˜ì„¸ìš”.

</Tip>

ì´ì œ ì „ì²˜ë¦¬ëœ ì…ë ¥ ë¬¶ìŒì„ ì§ì ‘ ëª¨ë¸ì— ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤. ì•„ë˜ì²˜ëŸ¼ ê·¸ëŒ€ë¡œ í…ì„œë¥¼ ì „ë‹¬í•˜ë©´ ë©ë‹ˆë‹¤:

```py
>>> tf_outputs = tf_model(tf_batch)
```

ëª¨ë¸ì˜ ìµœì¢… í™œì„±í™” í•¨ìˆ˜ ì¶œë ¥ì€ `logits` ì†ì„±ì— ë‹´ê²¨ìˆìŠµë‹ˆë‹¤. `logits`ì— softmax í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ í™•ë¥ ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
>>> import tensorflow as tf

>>> tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)
>>> tf_predictions  # doctest: +IGNORE_RESULT
```
</tf>
</frameworkcontent>

<Tip>

ëª¨ë“  ğŸ¤— Transformers ëª¨ë¸(PyTorch ë˜ëŠ” TensorFlow)ì€ (softmaxì™€ ê°™ì€) ìµœì¢… í™œì„±í™” í•¨ìˆ˜ *ì´ì „ì—* í…ì„œë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤. ì™œëƒí•˜ë©´ ìµœì¢… í™œì„±í™” í•¨ìˆ˜ì˜ ì¶œë ¥ì€ ì¢…ì¢… ì†ì‹¤ í•¨ìˆ˜ ì¶œë ¥ê³¼ ê²°í•©ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ëª¨ë¸ ì¶œë ¥ì€ íŠ¹ìˆ˜í•œ ë°ì´í„° í´ë˜ìŠ¤ì´ë¯€ë¡œ IDEì—ì„œ ìë™ ì™„ì„±ë©ë‹ˆë‹¤. ëª¨ë¸ ì¶œë ¥ì€ íŠœí”Œì´ë‚˜ ë”•ì…”ë„ˆë¦¬ì²˜ëŸ¼ ë™ì‘í•˜ë©° (ì •ìˆ˜, ìŠ¬ë¼ì´ìŠ¤ ë˜ëŠ” ë¬¸ìì—´ë¡œ ì¸ë±ì‹± ê°€ëŠ¥), Noneì¸ ì†ì„±ì€ ë¬´ì‹œë©ë‹ˆë‹¤.

</Tip>

### ëª¨ë¸ ì €ì¥í•˜ê¸° [[save-a-model]]

<frameworkcontent>
<pt>
ë¯¸ì„¸ì¡°ì •ëœ ëª¨ë¸ì„ í† í¬ë‚˜ì´ì €ì™€ í•¨ê»˜ ì €ì¥í•˜ë ¤ë©´ [`PreTrainedModel.save_pretrained`]ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:

```py
>>> pt_save_directory = "./pt_save_pretrained"
>>> tokenizer.save_pretrained(pt_save_directory)  # doctest: +IGNORE_RESULT
>>> pt_model.save_pretrained(pt_save_directory)
```

ëª¨ë¸ì„ ë‹¤ì‹œ ì‚¬ìš©í•˜ë ¤ë©´ [`PreTrainedModel.from_pretrained`]ë¡œ ëª¨ë¸ì„ ë‹¤ì‹œ ë¡œë“œí•˜ì„¸ìš”:

```py
>>> pt_model = AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained")
```
</pt>
<tf>
ë¯¸ì„¸ì¡°ì •ëœ ëª¨ë¸ì„ í† í¬ë‚˜ì´ì €ì™€ í•¨ê»˜ ì €ì¥í•˜ë ¤ë©´ [`TFPreTrainedModel.save_pretrained`]ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:

```py
>>> tf_save_directory = "./tf_save_pretrained"
>>> tokenizer.save_pretrained(tf_save_directory)  # doctest: +IGNORE_RESULT
>>> tf_model.save_pretrained(tf_save_directory)
```

ëª¨ë¸ì„ ë‹¤ì‹œ ì‚¬ìš©í•˜ë ¤ë©´ [`TFPreTrainedModel.from_pretrained`]ë¡œ ëª¨ë¸ì„ ë‹¤ì‹œ ë¡œë“œí•˜ì„¸ìš”:

```py
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained("./tf_save_pretrained")
```
</tf>
</frameworkcontent>

ğŸ¤— Transformersì˜ ë©‹ì§„ ê¸°ëŠ¥ ì¤‘ í•˜ë‚˜ëŠ” ëª¨ë¸ì„ PyTorch ë˜ëŠ” TensorFlow ëª¨ë¸ë¡œ ì €ì¥í•´ë’€ë‹¤ê°€ ë‹¤ë¥¸ í”„ë ˆì„ì›Œí¬ë¡œ ë‹¤ì‹œ ë¡œë“œí•  ìˆ˜ ìˆëŠ” ì ì…ë‹ˆë‹¤. `from_pt` ë˜ëŠ” `from_tf` ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í•œ í”„ë ˆì„ì›Œí¬ì—ì„œ ë‹¤ë¥¸ í”„ë ˆì„ì›Œí¬ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

<frameworkcontent>
<pt>

```py
>>> from transformers import AutoModel

>>> tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
>>> pt_model = AutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)
```
</pt>
<tf>

```py
>>> from transformers import TFAutoModel

>>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)
```
</tf>
</frameworkcontent>

## ì»¤ìŠ¤í…€ ëª¨ë¸ êµ¬ì¶•í•˜ê¸° [[custom-model-builds]]

ëª¨ë¸ì˜ êµ¬ì„± í´ë˜ìŠ¤ë¥¼ ìˆ˜ì •í•˜ì—¬ ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ ë°”ê¿€ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ì€ë‹‰ì¸µì´ë‚˜ ì–´í…ì…˜ í—¤ë“œì˜ ìˆ˜ì™€ ê°™ì€) ëª¨ë¸ì˜ ì†ì„±ì€ êµ¬ì„±ì—ì„œ ì§€ì •ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì»¤ìŠ¤í…€ êµ¬ì„± í´ë˜ìŠ¤ë¡œ ëª¨ë¸ì„ ë§Œë“¤ë©´ ì²˜ìŒë¶€í„° ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤. ëª¨ë¸ ì†ì„±ì€ ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”ë˜ë¯€ë¡œ ì˜ë¯¸ ìˆëŠ” ê²°ê³¼ë¥¼ ì–»ìœ¼ë ¤ë©´ ë¨¼ì € ëª¨ë¸ì„ í›ˆë ¨ì‹œì¼œì•¼ í•©ë‹ˆë‹¤.

ë¨¼ì € [`AutoConfig`]ë¥¼ ê°€ì ¸ì˜¤ê³  ìˆ˜ì •í•˜ê³  ì‹¶ì€ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì„ ë¡œë“œí•˜ì„¸ìš”. [`AutoConfig.from_pretrained`] ë‚´ë¶€ì—ì„œ (ì–´í…ì…˜ í—¤ë“œ ìˆ˜ì™€ ê°™ì´) ë³€ê²½í•˜ë ¤ëŠ” ì†ì„±ë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
>>> from transformers import AutoConfig

>>> my_config = AutoConfig.from_pretrained("distilbert/distilbert-base-uncased", n_heads=12)
```

<frameworkcontent>
<pt>
[`AutoModel.from_config`]ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°”ê¾¼ êµ¬ì„±ëŒ€ë¡œ ëª¨ë¸ì„ ìƒì„±í•˜ì„¸ìš”:

```py
>>> from transformers import AutoModel

>>> my_model = AutoModel.from_config(my_config)
```
</pt>
<tf>
[`TFAutoModel.from_config`]ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°”ê¾¼ êµ¬ì„±ëŒ€ë¡œ ëª¨ë¸ì„ ìƒì„±í•˜ì„¸ìš”:

```py
>>> from transformers import TFAutoModel

>>> my_model = TFAutoModel.from_config(my_config)
```
</tf>
</frameworkcontent>

ì»¤ìŠ¤í…€ êµ¬ì„±ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ [ì»¤ìŠ¤í…€ ì•„í‚¤í…ì²˜ ë§Œë“¤ê¸°](./create_a_model) ê°€ì´ë“œë¥¼ í™•ì¸í•˜ì„¸ìš”.

## Trainer - PyTorchì— ìµœì í™”ëœ í›ˆë ¨ ë£¨í”„ [[trainer-a-pytorch-optimized-training-loop]]

ëª¨ë“  ëª¨ë¸ì€ [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)ì´ë¯€ë¡œ ì¼ë°˜ì ì¸ í›ˆë ¨ ë£¨í”„ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì§ì ‘ í›ˆë ¨ ë£¨í”„ë¥¼ ì‘ì„±í•  ìˆ˜ë„ ìˆì§€ë§Œ, ğŸ¤— TransformersëŠ” PyTorchë¥¼ ìœ„í•œ [`Trainer`] í´ë˜ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ í´ë˜ìŠ¤ì—ëŠ” ê¸°ë³¸ í›ˆë ¨ ë£¨í”„ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©° ë¶„ì‚° í›ˆë ¨, í˜¼í•© ì •ë°€ë„ ë“±ê³¼ ê°™ì€ ê¸°ëŠ¥ì„ ì¶”ê°€ë¡œ ì œê³µí•©ë‹ˆë‹¤.

ê³¼ì—…ì— ë”°ë¼ ë‹¤ë¥´ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ [`Trainer`]ì— ë‹¤ìŒ ë§¤ê°œë³€ìˆ˜ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤:

1. [`PreTrainedModel`] ë˜ëŠ” [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)ë¡œ ì‹œì‘í•©ë‹ˆë‹¤:

   ```py
   >>> from transformers import AutoModelForSequenceClassification

   >>> model = AutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
   ```

2. [`TrainingArguments`]ëŠ” í•™ìŠµë¥ , ë°°ì¹˜ í¬ê¸°, í›ˆë ¨í•  ì—í¬í¬ ìˆ˜ì™€ ê°™ì€ ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. í›ˆë ¨ ì¸ìë¥¼ ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ê°’ì´ ì‚¬ìš©ë©ë‹ˆë‹¤:

   ```py
   >>> from transformers import TrainingArguments

   >>> training_args = TrainingArguments(
   ...     output_dir="path/to/save/folder/",
   ...     learning_rate=2e-5,
   ...     per_device_train_batch_size=8,
   ...     per_device_eval_batch_size=8,
   ...     num_train_epochs=2,
   ... )
   ```

3. í† í¬ë‚˜ì´ì €, ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ, íŠ¹ì§• ì¶”ì¶œê¸°(feature extractor) ë˜ëŠ” í”„ë¡œì„¸ì„œì™€ ì „ì²˜ë¦¬ í´ë˜ìŠ¤ë¥¼ ë¡œë“œí•˜ì„¸ìš”:

   ```py
   >>> from transformers import AutoTokenizer

   >>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
   ```

4. ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ì„¸ìš”:

   ```py
   >>> from datasets import load_dataset

   >>> dataset = load_dataset("rotten_tomatoes")  # doctest: +IGNORE_RESULT
   ```

5. ë°ì´í„°ì…‹ì„ í† í°í™”í•˜ëŠ” í•¨ìˆ˜ë¥¼ ìƒì„±í•˜ì„¸ìš”:

   ```py
   >>> def tokenize_dataset(dataset):
   ...     return tokenizer(dataset["text"])
   ```

   ê·¸ë¦¬ê³  [`~datasets.Dataset.map`]ë¡œ ë°ì´í„°ì…‹ ì „ì²´ì— ì ìš©í•˜ì„¸ìš”:

   ```py
   >>> dataset = dataset.map(tokenize_dataset, batched=True)
   ```

6. [`DataCollatorWithPadding`]ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì…‹ì˜ í‘œë³¸ ë¬¶ìŒì„ ë§Œë“œì„¸ìš”:

   ```py
   >>> from transformers import DataCollatorWithPadding

   >>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
   ```

ì´ì œ ìœ„ì˜ ëª¨ë“  í´ë˜ìŠ¤ë¥¼ [`Trainer`]ë¡œ ëª¨ìœ¼ì„¸ìš”:

```py
>>> from transformers import Trainer

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=dataset["train"],
...     eval_dataset=dataset["test"],
...     processing_class=tokenizer,
...     data_collator=data_collator,
... )  # doctest: +SKIP
```

ì¤€ë¹„ê°€ ë˜ì—ˆìœ¼ë©´ [`~Trainer.train`]ì„ í˜¸ì¶œí•˜ì—¬ í›ˆë ¨ì„ ì‹œì‘í•˜ì„¸ìš”:

```py
>>> trainer.train()  # doctest: +SKIP
```

<Tip>

ë²ˆì—­ì´ë‚˜ ìš”ì•½ê³¼ ê°™ì´ ì‹œí€€ìŠ¤-ì‹œí€€ìŠ¤ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê³¼ì—…ì—ëŠ” [`Seq2SeqTrainer`] ë° [`Seq2SeqTrainingArguments`] í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

</Tip>

[`Trainer`] ë‚´ì˜ ë©”ì„œë“œë¥¼ ì„œë¸Œí´ë˜ìŠ¤í™”í•˜ì—¬ í›ˆë ¨ ë£¨í”„ë¥¼ ë°”ê¿€ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬ë©´ ì†ì‹¤ í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì €, ìŠ¤ì¼€ì¤„ëŸ¬ì™€ ê°™ì€ ê¸°ëŠ¥ ë˜í•œ ë°”ê¿€ ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. ë³€ê²½ ê°€ëŠ¥í•œ ë©”ì†Œë“œì— ëŒ€í•´ì„œëŠ” [`Trainer`] ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

í›ˆë ¨ ë£¨í”„ë¥¼ ìˆ˜ì •í•˜ëŠ” ë‹¤ë¥¸ ë°©ë²•ì€ [Callbacks](./main_classes/callback)ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. Callbacksë¡œ ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ í†µí•©í•˜ê³ , í›ˆë ¨ ë£¨í”„ë¥¼ ì²´í¬í•˜ì—¬ ì§„í–‰ ìƒí™©ì„ ë³´ê³ ë°›ê±°ë‚˜, í›ˆë ¨ì„ ì¡°ê¸°ì— ì¤‘ë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Callbacksì€ í›ˆë ¨ ë£¨í”„ ìì²´ë¥¼ ë°”ê¾¸ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤. ì†ì‹¤ í•¨ìˆ˜ì™€ ê°™ì€ ê²ƒì„ ë°”ê¾¸ë ¤ë©´ [`Trainer`]ë¥¼ ì„œë¸Œí´ë˜ìŠ¤í™”í•´ì•¼ í•©ë‹ˆë‹¤.

## TensorFlowë¡œ í›ˆë ¨ì‹œí‚¤ê¸° [[train-with-tensorflow]]

ëª¨ë“  ëª¨ë¸ì€ [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)ì´ë¯€ë¡œ [Keras](https://keras.io/) APIë¥¼ í†µí•´ TensorFlowì—ì„œ í›ˆë ¨ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸ¤— TransformersëŠ” ë°ì´í„°ì…‹ì„ ì‰½ê²Œ `tf.data.Dataset` í˜•íƒœë¡œ ì‰½ê²Œ ë¡œë“œí•  ìˆ˜ ìˆëŠ” [`~TFPreTrainedModel.prepare_tf_dataset`] ë©”ì†Œë“œë¥¼ ì œê³µí•˜ê¸° ë•Œë¬¸ì—, Kerasì˜ [`compile`](https://keras.io/api/models/model_training_apis/#compile-method) ë° [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) ë©”ì†Œë“œë¡œ ë°”ë¡œ í›ˆë ¨ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

1. [`TFPreTrainedModel`] ë˜ëŠ” [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)ë¡œ ì‹œì‘í•©ë‹ˆë‹¤:

   ```py
   >>> from transformers import TFAutoModelForSequenceClassification

   >>> model = TFAutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
   ```

2. í† í¬ë‚˜ì´ì €, ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ, íŠ¹ì§• ì¶”ì¶œê¸°(feature extractor) ë˜ëŠ” í”„ë¡œì„¸ì„œì™€ ê°™ì€ ì „ì²˜ë¦¬ í´ë˜ìŠ¤ë¥¼ ë¡œë“œí•˜ì„¸ìš”:

   ```py
   >>> from transformers import AutoTokenizer

   >>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
   ```

3. ë°ì´í„°ì…‹ì„ í† í°í™”í•˜ëŠ” í•¨ìˆ˜ë¥¼ ìƒì„±í•˜ì„¸ìš”:

   ```py
   >>> def tokenize_dataset(dataset):
   ...     return tokenizer(dataset["text"])  # doctest: +SKIP
   ```

4. [`~datasets.Dataset.map`]ì„ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ë°ì´í„°ì…‹ì— í† í°í™” í•¨ìˆ˜ë¥¼ ì ìš©í•˜ê³ , ë°ì´í„°ì…‹ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ [`~TFPreTrainedModel.prepare_tf_dataset`]ì— ì „ë‹¬í•˜ì„¸ìš”. ë°°ì¹˜ í¬ê¸°ë¥¼ ë³€ê²½í•˜ê±°ë‚˜ ë°ì´í„°ì…‹ì„ ì„ì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:

   ```py
   >>> dataset = dataset.map(tokenize_dataset)  # doctest: +SKIP
   >>> tf_dataset = model.prepare_tf_dataset(
   ...     dataset["train"], batch_size=16, shuffle=True, tokenizer=tokenizer
   ... )  # doctest: +SKIP
   ```

5. ì¤€ë¹„ë˜ì—ˆìœ¼ë©´ `compile` ë° `fit`ë¥¼ í˜¸ì¶œí•˜ì—¬ í›ˆë ¨ì„ ì‹œì‘í•˜ì„¸ìš”. ğŸ¤— Transformersì˜ ëª¨ë“  ëª¨ë¸ì€ ê³¼ì—…ê³¼ ê´€ë ¨ëœ ê¸°ë³¸ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë¯€ë¡œ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤:

   ```py
   >>> from tensorflow.keras.optimizers import Adam

   >>> model.compile(optimizer=Adam(3e-5))  # No loss argument!
   >>> model.fit(tf_dataset)  # doctest: +SKIP
   ```

## ë‹¤ìŒ ë‹¨ê³„ëŠ” ë¬´ì—‡ì¸ê°€ìš”? [[whats-next]]

ğŸ¤— Transformers ë‘˜ëŸ¬ë³´ê¸°ë¥¼ ëª¨ë‘ ì½ìœ¼ì…¨ë‹¤ë©´, ê°€ì´ë“œë¥¼ ì‚´í´ë³´ê³  ë” êµ¬ì²´ì ì¸ ê²ƒì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ì„¸ìš”. ì´ë¥¼í…Œë©´ ì»¤ìŠ¤í…€ ëª¨ë¸ êµ¬ì¶•í•˜ëŠ” ë°©ë²•, ê³¼ì—…ì— ì•Œë§ê²Œ ëª¨ë¸ì„ ë¯¸ì„¸ì¡°ì •í•˜ëŠ” ë°©ë²•, ìŠ¤í¬ë¦½íŠ¸ë¡œ ëª¨ë¸ í›ˆë ¨í•˜ëŠ” ë°©ë²• ë“±ì´ ìˆìŠµë‹ˆë‹¤. ğŸ¤— Transformers í•µì‹¬ ê°œë…ì— ëŒ€í•´ ë” ì•Œì•„ë³´ë ¤ë©´ ì»¤í”¼ í•œ ì” ë“¤ê³  ê°œë… ê°€ì´ë“œë¥¼ ì‚´í´ë³´ì„¸ìš”!
