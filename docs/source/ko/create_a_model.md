<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# ë§ì¶¤í˜• ì•„í‚¤í…ì²˜ ë§Œë“¤ê¸°[[create-a-custom-architecture]]

[`AutoClass`](model_doc/auto)ëŠ” ëª¨ë¸ ì•„í‚¤í…ì²˜ë¥¼ ìë™ìœ¼ë¡œ ì¶”ë¡ í•˜ê³  ë¯¸ë¦¬ í•™ìŠµëœ configurationê³¼ ê°€ì¤‘ì¹˜ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì²´í¬í¬ì¸íŠ¸ì— êµ¬ì• ë°›ì§€ ì•ŠëŠ” ì½”ë“œë¥¼ ìƒì„±í•˜ë ¤ë©´ `AutoClass`ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ íŠ¹ì • ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ë³´ë‹¤ ì„¸ë°€í•˜ê²Œ ì œì–´í•˜ê³ ì í•˜ëŠ” ì‚¬ìš©ìëŠ” ëª‡ ê°€ì§€ ê¸°ë³¸ í´ë˜ìŠ¤ë§Œìœ¼ë¡œ ì»¤ìŠ¤í…€ ğŸ¤— Transformers ëª¨ë¸ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ğŸ¤— Transformers ëª¨ë¸ì„ ì—°êµ¬, êµìœ¡ ë˜ëŠ” ì‹¤í—˜í•˜ëŠ” ë° ê´€ì‹¬ì´ ìˆëŠ” ëª¨ë“  ì‚¬ìš©ìì—ê²Œ íŠ¹íˆ ìœ ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê°€ì´ë“œì—ì„œëŠ” 'AutoClass'ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì»¤ìŠ¤í…€ ëª¨ë¸ì„ ë§Œë“œëŠ” ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤:

- ëª¨ë¸ configurationì„ ê°€ì ¸ì˜¤ê³  ì‚¬ìš©ì ì§€ì •í•©ë‹ˆë‹¤.
- ëª¨ë¸ ì•„í‚¤í…ì²˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
- í…ìŠ¤íŠ¸ì— ì‚¬ìš©í•  ëŠë¦¬ê±°ë‚˜ ë¹ ë¥¸ í† í°í™”ê¸°ë¥¼ ë§Œë“­ë‹ˆë‹¤.
- ë¹„ì „ ì‘ì—…ì„ ìœ„í•œ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
- ì˜¤ë””ì˜¤ ì‘ì—…ì„ ìœ„í•œ íŠ¹ì„± ì¶”ì¶œê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
- ë©€í‹°ëª¨ë‹¬ ì‘ì—…ìš© í”„ë¡œì„¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

## Configuration[[configuration]]

[configuration](main_classes/configuration)ì€ ëª¨ë¸ì˜ íŠ¹ì • ì†ì„±ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ê° ëª¨ë¸ êµ¬ì„±ì—ëŠ” ì„œë¡œ ë‹¤ë¥¸ ì†ì„±ì´ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ëª¨ë“  NLP ëª¨ë¸ì—ëŠ” `hidden_size`, `num_attention_heads`, `num_hidden_layers` ë° `vocab_size` ì†ì„±ì´ ê³µí†µìœ¼ë¡œ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì†ì„±ì€ ëª¨ë¸ì„ êµ¬ì„±í•  attention heads ë˜ëŠ” hidden layersì˜ ìˆ˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.

[DistilBERT](model_doc/distilbert) ì†ì„±ì„ ê²€ì‚¬í•˜ê¸° ìœ„í•´ [`DistilBertConfig`]ì— ì ‘ê·¼í•˜ì—¬ ìì„¸íˆ ì‚´í´ë´…ë‹ˆë‹¤:

```py
>>> from transformers import DistilBertConfig

>>> config = DistilBertConfig()
>>> print(config)
DistilBertConfig {
  "activation": "gelu",
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "transformers_version": "4.16.2",
  "vocab_size": 30522
}
```

[`DistilBertConfig`]ëŠ” ê¸°ë³¸ [`DistilBertModel`]ì„ ë¹Œë“œí•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ëª¨ë“  ê¸°ë³¸ ì†ì„±ì„ í‘œì‹œí•©ë‹ˆë‹¤. ëª¨ë“  ì†ì„±ì€ ì»¤ìŠ¤í„°ë§ˆì´ì§•ì´ ê°€ëŠ¥í•˜ë¯€ë¡œ ì‹¤í—˜ì„ ìœ„í•œ ê³µê°„ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ê¸°ë³¸ ëª¨ë¸ì„ ë‹¤ìŒê³¼ ê°™ì´ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- `activation` íŒŒë¼ë¯¸í„°ë¡œ ë‹¤ë¥¸ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ë³´ì„¸ìš”.
- `attention_dropout` íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì–´í…ì…˜ í™•ë¥ ì— ë” ë†’ì€ ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ì„ ì‚¬ìš©í•˜ì„¸ìš”.

```py
>>> my_config = DistilBertConfig(activation="relu", attention_dropout=0.4)
>>> print(my_config)
DistilBertConfig {
  "activation": "relu",
  "attention_dropout": 0.4,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "transformers_version": "4.16.2",
  "vocab_size": 30522
}
```

ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ì†ì„±ì€ [`~PretrainedConfig.from_pretrained`] í•¨ìˆ˜ì—ì„œ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
>>> my_config = DistilBertConfig.from_pretrained("distilbert/distilbert-base-uncased", activation="relu", attention_dropout=0.4)
```

ëª¨ë¸ êµ¬ì„±ì´ ë§Œì¡±ìŠ¤ëŸ¬ìš°ë©´ [`~PretrainedConfig.save_pretrained`]ë¡œ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì„¤ì • íŒŒì¼ì€ ì§€ì •ëœ ì‘ì—… ê²½ë¡œì— JSON íŒŒì¼ë¡œ ì €ì¥ë©ë‹ˆë‹¤:

```py
>>> my_config.save_pretrained(save_directory="./your_model_save_path")
```

configuration íŒŒì¼ì„ ì¬ì‚¬ìš©í•˜ë ¤ë©´ [`~PretrainedConfig.from_pretrained`]ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì ¸ì˜¤ì„¸ìš”:

```py
>>> my_config = DistilBertConfig.from_pretrained("./your_model_save_path/config.json")
```

<Tip>

configuration íŒŒì¼ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ì €ì¥í•˜ê±°ë‚˜ ì‚¬ìš©ì ì •ì˜ configuration ì†ì„±ê³¼ ê¸°ë³¸ configuration ì†ì„±ì˜ ì°¨ì´ì ë§Œ ì €ì¥í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤! ìì„¸í•œ ë‚´ìš©ì€ [configuration](main_classes/configuration) ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

</Tip>

## ëª¨ë¸[[model]]

ë‹¤ìŒ ë‹¨ê³„ëŠ” [ëª¨ë¸(model)](main_classes/models)ì„ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤. ëŠìŠ¨í•˜ê²Œ ì•„í‚¤í…ì²˜ë¼ê³ ë„ ë¶ˆë¦¬ëŠ” ëª¨ë¸ì€ ê° ê³„ì¸µì´ ìˆ˜í–‰í•˜ëŠ” ë™ì‘ê³¼ ë°œìƒí•˜ëŠ” ì‘ì—…ì„ ì •ì˜í•©ë‹ˆë‹¤. configurationì˜ `num_hidden_layers`ì™€ ê°™ì€ ì†ì„±ì€ ì•„í‚¤í…ì²˜ë¥¼ ì •ì˜í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ëª¨ë“  ëª¨ë¸ì€ ê¸°ë³¸ í´ë˜ìŠ¤ [`PreTrainedModel`]ê³¼ ì…ë ¥ ì„ë² ë”© í¬ê¸° ì¡°ì • ë° ì…€í”„ ì–´í…ì…˜ í—¤ë“œ ê°€ì§€ ì¹˜ê¸°ì™€ ê°™ì€ ëª‡ ê°€ì§€ ì¼ë°˜ì ì¸ ë©”ì†Œë“œë¥¼ ê³µìœ í•©ë‹ˆë‹¤. ë˜í•œ ëª¨ë“  ëª¨ë¸ì€ [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html), [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) ë˜ëŠ” [`flax.linen.Module`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)ì˜ ì„œë¸Œí´ë˜ìŠ¤ì´ê¸°ë„ í•©ë‹ˆë‹¤. ì¦‰, ëª¨ë¸ì€ ê° í”„ë ˆì„ì›Œí¬ì˜ ì‚¬ìš©ë²•ê³¼ í˜¸í™˜ë©ë‹ˆë‹¤.

<frameworkcontent>
<pt>
ì‚¬ìš©ì ì§€ì • configuration ì†ì„±ì„ ëª¨ë¸ì— ê°€ì ¸ì˜µë‹ˆë‹¤:

```py
>>> from transformers import DistilBertModel

>>> my_config = DistilBertConfig.from_pretrained("./your_model_save_path/config.json")
>>> model = DistilBertModel(my_config)
```

ì´ì œ ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ ëŒ€ì‹  ì„ì˜ì˜ ê°’ì„ ê°€ì§„ ëª¨ë¸ì´ ìƒì„±ë©ë‹ˆë‹¤. ì´ ëª¨ë¸ì„ í›ˆë ¨í•˜ê¸° ì „ê¹Œì§€ëŠ” ìœ ìš©í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í›ˆë ¨ì€ ë¹„ìš©ê³¼ ì‹œê°„ì´ ë§ì´ ì†Œìš”ë˜ëŠ” í”„ë¡œì„¸ìŠ¤ì…ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ í›ˆë ¨ì— í•„ìš”í•œ ë¦¬ì†ŒìŠ¤ì˜ ì¼ë¶€ë§Œ ì‚¬ìš©í•˜ë©´ì„œ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ë” ë¹¨ë¦¬ ì–»ìœ¼ë ¤ë©´ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ [`~PreTrainedModel.from_pretrained`]ë¡œ ìƒì„±í•©ë‹ˆë‹¤:

```py
>>> model = DistilBertModel.from_pretrained("distilbert/distilbert-base-uncased")
```

ğŸ¤— Transformersì—ì„œ ì œê³µí•œ ëª¨ë¸ì˜ ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ê¸°ë³¸ ëª¨ë¸ configurationì„ ìë™ìœ¼ë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì›í•˜ëŠ” ê²½ìš° ê¸°ë³¸ ëª¨ë¸ configuration ì†ì„±ì˜ ì¼ë¶€ ë˜ëŠ” ì „ë¶€ë¥¼ ì‚¬ìš©ì ì§€ì •ìœ¼ë¡œ ë°”ê¿€ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
>>> model = DistilBertModel.from_pretrained("distilbert/distilbert-base-uncased", config=my_config)
```
</pt>
<tf>
ì‚¬ìš©ì ì§€ì • configuration ì†ì„±ì„ ëª¨ë¸ì— ë¶ˆëŸ¬ì˜µë‹ˆë‹¤:

```py
>>> from transformers import TFDistilBertModel

>>> my_config = DistilBertConfig.from_pretrained("./your_model_save_path/my_config.json")
>>> tf_model = TFDistilBertModel(my_config)
```

ì´ì œ ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ ëŒ€ì‹  ì„ì˜ì˜ ê°’ì„ ê°€ì§„ ëª¨ë¸ì´ ìƒì„±ë©ë‹ˆë‹¤. ì´ ëª¨ë¸ì„ í›ˆë ¨í•˜ê¸° ì „ê¹Œì§€ëŠ” ìœ ìš©í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í›ˆë ¨ì€ ë¹„ìš©ê³¼ ì‹œê°„ì´ ë§ì´ ì†Œìš”ë˜ëŠ” í”„ë¡œì„¸ìŠ¤ì…ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ í›ˆë ¨ì— í•„ìš”í•œ ë¦¬ì†ŒìŠ¤ì˜ ì¼ë¶€ë§Œ ì‚¬ìš©í•˜ë©´ì„œ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ë” ë¹¨ë¦¬ ì–»ìœ¼ë ¤ë©´ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ [`~TFPreTrainedModel.from_pretrained`]ë¡œ ìƒì„±í•©ë‹ˆë‹¤:

```py
>>> tf_model = TFDistilBertModel.from_pretrained("distilbert/distilbert-base-uncased")
```

ğŸ¤— Transformersì—ì„œ ì œê³µí•œ ëª¨ë¸ì˜ ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ê¸°ë³¸ ëª¨ë¸ configurationì„ ìë™ìœ¼ë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì›í•˜ëŠ” ê²½ìš° ê¸°ë³¸ ëª¨ë¸ configuration ì†ì„±ì˜ ì¼ë¶€ ë˜ëŠ” ì „ë¶€ë¥¼ ì‚¬ìš©ì ì§€ì •ìœ¼ë¡œ ë°”ê¿€ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
>>> tf_model = TFDistilBertModel.from_pretrained("distilbert/distilbert-base-uncased", config=my_config)
```
</tf>
</frameworkcontent>

### ëª¨ë¸ í—¤ë“œ[[model-heads]]

ì´ ì‹œì ì—ì„œ *ì€ë‹‰ ìƒíƒœ(hidden state)*ë¥¼ ì¶œë ¥í•˜ëŠ” ê¸°ë³¸ DistilBERT ëª¨ë¸ì„ ê°–ê²Œ ë©ë‹ˆë‹¤. ì€ë‹‰ ìƒíƒœëŠ” ìµœì¢… ì¶œë ¥ì„ ìƒì„±í•˜ê¸° ìœ„í•´ ëª¨ë¸ í—¤ë“œì— ì…ë ¥ìœ¼ë¡œ ì „ë‹¬ë©ë‹ˆë‹¤. ğŸ¤— TransformersëŠ” ëª¨ë¸ì´ í•´ë‹¹ ì‘ì—…ì„ ì§€ì›í•˜ëŠ” í•œ ê° ì‘ì—…ë§ˆë‹¤ ë‹¤ë¥¸ ëª¨ë¸ í—¤ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤(ì¦‰, ë²ˆì—­ê³¼ ê°™ì€ ì‹œí€€ìŠ¤ ê°„ ì‘ì—…ì—ëŠ” DistilBERTë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ).

<frameworkcontent>
<pt>
ì˜ˆë¥¼ ë“¤ì–´, [`DistilBertForSequenceClassification`]ì€ ì‹œí€€ìŠ¤ ë¶„ë¥˜ í—¤ë“œê°€ ìˆëŠ” ê¸°ë³¸ DistilBERT ëª¨ë¸ì…ë‹ˆë‹¤. ì‹œí€€ìŠ¤ ë¶„ë¥˜ í—¤ë“œëŠ” í’€ë§ëœ ì¶œë ¥ ìœ„ì— ìˆëŠ” ì„ í˜• ë ˆì´ì–´ì…ë‹ˆë‹¤.

```py
>>> from transformers import DistilBertForSequenceClassification

>>> model = DistilBertForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

ë‹¤ë¥¸ ëª¨ë¸ í—¤ë“œë¡œ ì „í™˜í•˜ì—¬ ì´ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë‹¤ë¥¸ ì‘ì—…ì— ì‰½ê²Œ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì§ˆì˜ì‘ë‹µ ì‘ì—…ì˜ ê²½ìš°, [`DistilBertForQuestionAnswering`] ëª¨ë¸ í—¤ë“œë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì§ˆì˜ì‘ë‹µ í—¤ë“œëŠ” ìˆ¨ê²¨ì§„ ìƒíƒœ ì¶œë ¥ ìœ„ì— ì„ í˜• ë ˆì´ì–´ê°€ ìˆë‹¤ëŠ” ì ì„ ì œì™¸í•˜ë©´ ì‹œí€€ìŠ¤ ë¶„ë¥˜ í—¤ë“œì™€ ìœ ì‚¬í•©ë‹ˆë‹¤.

```py
>>> from transformers import DistilBertForQuestionAnswering

>>> model = DistilBertForQuestionAnswering.from_pretrained("distilbert/distilbert-base-uncased")
```
</pt>
<tf>
ì˜ˆë¥¼ ë“¤ì–´, [`TFDistilBertForSequenceClassification`]ì€ ì‹œí€€ìŠ¤ ë¶„ë¥˜ í—¤ë“œê°€ ìˆëŠ” ê¸°ë³¸ DistilBERT ëª¨ë¸ì…ë‹ˆë‹¤. ì‹œí€€ìŠ¤ ë¶„ë¥˜ í—¤ë“œëŠ” í’€ë§ëœ ì¶œë ¥ ìœ„ì— ìˆëŠ” ì„ í˜• ë ˆì´ì–´ì…ë‹ˆë‹¤.

```py
>>> from transformers import TFDistilBertForSequenceClassification

>>> tf_model = TFDistilBertForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

ë‹¤ë¥¸ ëª¨ë¸ í—¤ë“œë¡œ ì „í™˜í•˜ì—¬ ì´ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë‹¤ë¥¸ ì‘ì—…ì— ì‰½ê²Œ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì§ˆì˜ì‘ë‹µ ì‘ì—…ì˜ ê²½ìš°, [`TFDistilBertForQuestionAnswering`] ëª¨ë¸ í—¤ë“œë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì§ˆì˜ì‘ë‹µ í—¤ë“œëŠ” ìˆ¨ê²¨ì§„ ìƒíƒœ ì¶œë ¥ ìœ„ì— ì„ í˜• ë ˆì´ì–´ê°€ ìˆë‹¤ëŠ” ì ì„ ì œì™¸í•˜ë©´ ì‹œí€€ìŠ¤ ë¶„ë¥˜ í—¤ë“œì™€ ìœ ì‚¬í•©ë‹ˆë‹¤.

```py
>>> from transformers import TFDistilBertForQuestionAnswering

>>> tf_model = TFDistilBertForQuestionAnswering.from_pretrained("distilbert/distilbert-base-uncased")
```
</tf>
</frameworkcontent>

## í† í¬ë‚˜ì´ì €[[tokenizer]]

í…ìŠ¤íŠ¸ ë°ì´í„°ì— ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸° ì „ì— ë§ˆì§€ë§‰ìœ¼ë¡œ í•„ìš”í•œ ê¸°ë³¸ í´ë˜ìŠ¤ëŠ” ì›ì‹œ í…ìŠ¤íŠ¸ë¥¼ í…ì„œë¡œ ë³€í™˜í•˜ëŠ” [í† í¬ë‚˜ì´ì €](main_classes/tokenizer)ì…ë‹ˆë‹¤. ğŸ¤— Transformersì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í† í¬ë‚˜ì´ì €ëŠ” ë‘ ê°€ì§€ ìœ í˜•ì´ ìˆìŠµë‹ˆë‹¤:

- [`PreTrainedTokenizer`]: íŒŒì´ì¬ìœ¼ë¡œ êµ¬í˜„ëœ í† í¬ë‚˜ì´ì €ì…ë‹ˆë‹¤.
- [`PreTrainedTokenizerFast`]: Rust ê¸°ë°˜ [ğŸ¤— Tokenizer](https://huggingface.co/docs/tokenizers/python/latest/) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ë§Œë“¤ì–´ì§„ í† í¬ë‚˜ì´ì €ì…ë‹ˆë‹¤. ì´ í† í¬ë‚˜ì´ì €ëŠ” Rustë¡œ êµ¬í˜„ë˜ì–´ ë°°ì¹˜ í† í°í™”ì—ì„œ íŠ¹íˆ ë¹ ë¦…ë‹ˆë‹¤. ë¹ ë¥¸ í† í¬ë‚˜ì´ì €ëŠ” í† í°ì„ ì›ë˜ ë‹¨ì–´ë‚˜ ë¬¸ìì— ë§¤í•‘í•˜ëŠ” *ì˜¤í”„ì…‹ ë§¤í•‘*ê³¼ ê°™ì€ ì¶”ê°€ ë©”ì†Œë“œë„ ì œê³µí•©ë‹ˆë‹¤.
ë‘ í† í¬ë‚˜ì´ì € ëª¨ë‘ ì¸ì½”ë”© ë° ë””ì½”ë”©, ìƒˆ í† í° ì¶”ê°€, íŠ¹ìˆ˜ í† í° ê´€ë¦¬ì™€ ê°™ì€ ì¼ë°˜ì ì¸ ë°©ë²•ì„ ì§€ì›í•©ë‹ˆë‹¤.

<Tip warning={true}>

ëª¨ë“  ëª¨ë¸ì´ ë¹ ë¥¸ í† í¬ë‚˜ì´ì €ë¥¼ ì§€ì›í•˜ëŠ” ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤. ì´ [í‘œ](index#supported-frameworks)ì—ì„œ ëª¨ë¸ì˜ ë¹ ë¥¸ í† í¬ë‚˜ì´ì € ì§€ì› ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”.

</Tip>

í† í¬ë‚˜ì´ì €ë¥¼ ì§ì ‘ í•™ìŠµí•œ ê²½ìš°, *ì–´íœ˜(vocabulary)* íŒŒì¼ì—ì„œ í† í¬ë‚˜ì´ì €ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
>>> from transformers import DistilBertTokenizer

>>> my_tokenizer = DistilBertTokenizer(vocab_file="my_vocab_file.txt", do_lower_case=False, padding_side="left")
```

ì‚¬ìš©ì ì§€ì • í† í¬ë‚˜ì´ì €ì˜ ì–´íœ˜ëŠ” ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì €ì—ì„œ ìƒì„±ëœ ì–´íœ˜ì™€ ë‹¤ë¥¼ ìˆ˜ ìˆë‹¤ëŠ” ì ì„ ê¸°ì–µí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì˜ ì–´íœ˜ë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ë©°, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ì…ë ¥ì´ ì˜ë¯¸ë¥¼ ê°–ì§€ ëª»í•©ë‹ˆë‹¤. [`DistilBertTokenizer`] í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì˜ ì–´íœ˜ë¡œ í† í¬ë‚˜ì´ì €ë¥¼ ìƒì„±í•©ë‹ˆë‹¤:

```py
>>> from transformers import DistilBertTokenizer

>>> slow_tokenizer = DistilBertTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
```

[`DistilBertTokenizerFast`] í´ë˜ìŠ¤ë¡œ ë¹ ë¥¸ í† í¬ë‚˜ì´ì €ë¥¼ ìƒì„±í•©ë‹ˆë‹¤:

```py
>>> from transformers import DistilBertTokenizerFast

>>> fast_tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert/distilbert-base-uncased")
```

<Tip>

[`AutoTokenizer`]ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë¹ ë¥¸ í† í¬ë‚˜ì´ì €ë¥¼ ê°€ì ¸ì˜¤ë ¤ê³  í•©ë‹ˆë‹¤. ì´ ë™ì‘ì„ ë¹„í™œì„±í™”í•˜ë ¤ë©´ `from_pretrained`ì—ì„œ `use_fast=False`ë¥¼ ì„¤ì •í•˜ë©´ ë©ë‹ˆë‹¤.

</Tip>

## ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ[[image-processor]]

ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ(image processor)ëŠ” ë¹„ì „ ì…ë ¥ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤. ê¸°ë³¸ [`~image_processing_utils.ImageProcessingMixin`] í´ë˜ìŠ¤ì—ì„œ ìƒì†í•©ë‹ˆë‹¤.

ì‚¬ìš©í•˜ë ¤ë©´ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ê³¼ ì—°ê²°ëœ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì´ë¯¸ì§€ ë¶„ë¥˜ì— [ViT](model_doc/vit)ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ê¸°ë³¸ [`ViTImageProcessor`]ë¥¼ ìƒì„±í•©ë‹ˆë‹¤:

```py
>>> from transformers import ViTImageProcessor

>>> vit_extractor = ViTImageProcessor()
>>> print(vit_extractor)
ViTImageProcessor {
  "do_normalize": true,
  "do_resize": true,
  "feature_extractor_type": "ViTImageProcessor",
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "resample": 2,
  "size": 224
}
```

<Tip>

ì‚¬ìš©ì ì§€ì •ì„ ì›í•˜ì§€ ì•ŠëŠ” ê²½ìš° `from_pretrained` ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ê¸°ë³¸ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ ë§¤ê°œë³€ìˆ˜ë¥¼ ë¶ˆëŸ¬ì˜¤ë©´ ë©ë‹ˆë‹¤.

</Tip>

ì‚¬ìš©ì ì§€ì • ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¥¼ ìƒì„±í•˜ë ¤ë©´ [`ViTImageProcessor`] íŒŒë¼ë¯¸í„°ë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤:

```py
>>> from transformers import ViTImageProcessor

>>> my_vit_extractor = ViTImageProcessor(resample="PIL.Image.BOX", do_normalize=False, image_mean=[0.3, 0.3, 0.3])
>>> print(my_vit_extractor)
ViTImageProcessor {
  "do_normalize": false,
  "do_resize": true,
  "feature_extractor_type": "ViTImageProcessor",
  "image_mean": [
    0.3,
    0.3,
    0.3
  ],
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "resample": "PIL.Image.BOX",
  "size": 224
}
```

## íŠ¹ì„± ì¶”ì¶œê¸°[[feature-extractor]]

íŠ¹ì„± ì¶”ì¶œê¸°(feature extractor)ëŠ” ì˜¤ë””ì˜¤ ì…ë ¥ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤. ê¸°ë³¸ [`~feature_extraction_utils.FeatureExtractionMixin`] í´ë˜ìŠ¤ì—ì„œ ìƒì†ë˜ë©°, ì˜¤ë””ì˜¤ ì…ë ¥ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ [`SequenceFeatureExtractor`] í´ë˜ìŠ¤ì—ì„œ ìƒì†í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

ì‚¬ìš©í•˜ë ¤ë©´ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ê³¼ ì—°ê²°ëœ íŠ¹ì„± ì¶”ì¶œê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì˜¤ë””ì˜¤ ë¶„ë¥˜ì— [Wav2Vec2](model_doc/wav2vec2)ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ê¸°ë³¸ [`Wav2Vec2FeatureExtractor`]ë¥¼ ìƒì„±í•©ë‹ˆë‹¤:

```py
>>> from transformers import Wav2Vec2FeatureExtractor

>>> w2v2_extractor = Wav2Vec2FeatureExtractor()
>>> print(w2v2_extractor)
Wav2Vec2FeatureExtractor {
  "do_normalize": true,
  "feature_extractor_type": "Wav2Vec2FeatureExtractor",
  "feature_size": 1,
  "padding_side": "right",
  "padding_value": 0.0,
  "return_attention_mask": false,
  "sampling_rate": 16000
}
```

<Tip>

ì‚¬ìš©ì ì§€ì •ì´ í•„ìš”í•˜ì§€ ì•Šì€ ê²½ìš° `from_pretrained` ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ê¸°ë³¸ íŠ¹ì„± ì¶”ì¶œê¸° ã…ê°œë³€ìˆ˜ë¥¼ ë¶ˆëŸ¬ ì˜¤ë©´ ë©ë‹ˆë‹¤.

</Tip>

ì‚¬ìš©ì ì§€ì • íŠ¹ì„± ì¶”ì¶œê¸°ë¥¼ ë§Œë“¤ë ¤ë©´ [`Wav2Vec2FeatureExtractor`] ë§¤ê°œë³€ìˆ˜ë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤:

```py
>>> from transformers import Wav2Vec2FeatureExtractor

>>> w2v2_extractor = Wav2Vec2FeatureExtractor(sampling_rate=8000, do_normalize=False)
>>> print(w2v2_extractor)
Wav2Vec2FeatureExtractor {
  "do_normalize": false,
  "feature_extractor_type": "Wav2Vec2FeatureExtractor",
  "feature_size": 1,
  "padding_side": "right",
  "padding_value": 0.0,
  "return_attention_mask": false,
  "sampling_rate": 8000
}
```


## í”„ë¡œì„¸ì„œ[[processor]]

ë©€í‹°ëª¨ë‹¬ ì‘ì—…ì„ ì§€ì›í•˜ëŠ” ëª¨ë¸ì˜ ê²½ìš°, ğŸ¤— TransformersëŠ” íŠ¹ì„± ì¶”ì¶œê¸° ë° í† í¬ë‚˜ì´ì €ì™€ ê°™ì€ ì²˜ë¦¬ í´ë˜ìŠ¤ë¥¼ ë‹¨ì¼ ê°ì²´ë¡œ í¸ë¦¬í•˜ê²Œ ë˜í•‘í•˜ëŠ” í”„ë¡œì„¸ì„œ í´ë˜ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ìë™ ìŒì„± ì¸ì‹ ì‘ì—…(Automatic Speech Recognition task (ASR))ì— [`Wav2Vec2Processor`]ë¥¼ ì‚¬ìš©í•œë‹¤ê³  ê°€ì •í•´ ë³´ê² ìŠµë‹ˆë‹¤. ìë™ ìŒì„± ì¸ì‹ ì‘ì—…ì€ ì˜¤ë””ì˜¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ë¯€ë¡œ íŠ¹ì„± ì¶”ì¶œê¸°ì™€ í† í¬ë‚˜ì´ì €ê°€ í•„ìš”í•©ë‹ˆë‹¤.

ì˜¤ë””ì˜¤ ì…ë ¥ì„ ì²˜ë¦¬í•  íŠ¹ì„± ì¶”ì¶œê¸°ë¥¼ ë§Œë“­ë‹ˆë‹¤:

```py
>>> from transformers import Wav2Vec2FeatureExtractor

>>> feature_extractor = Wav2Vec2FeatureExtractor(padding_value=1.0, do_normalize=True)
```

í…ìŠ¤íŠ¸ ì…ë ¥ì„ ì²˜ë¦¬í•  í† í¬ë‚˜ì´ì €ë¥¼ ë§Œë“­ë‹ˆë‹¤:

```py
>>> from transformers import Wav2Vec2CTCTokenizer

>>> tokenizer = Wav2Vec2CTCTokenizer(vocab_file="my_vocab_file.txt")
```

[`Wav2Vec2Processor`]ì—ì„œ íŠ¹ì„± ì¶”ì¶œê¸°ì™€ í† í¬ë‚˜ì´ì €ë¥¼ ê²°í•©í•©ë‹ˆë‹¤:

```py
>>> from transformers import Wav2Vec2Processor

>>> processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)
```

configurationê³¼ ëª¨ë¸ì´ë¼ëŠ” ë‘ ê°€ì§€ ê¸°ë³¸ í´ë˜ìŠ¤ì™€ ì¶”ê°€ ì „ì²˜ë¦¬ í´ë˜ìŠ¤(í† í¬ë‚˜ì´ì €, ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ, íŠ¹ì„± ì¶”ì¶œê¸° ë˜ëŠ” í”„ë¡œì„¸ì„œ)ë¥¼ ì‚¬ìš©í•˜ë©´ ğŸ¤— Transformersì—ì„œ ì§€ì›í•˜ëŠ” ëª¨ë“  ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê° ê¸°ë³¸ í´ë˜ìŠ¤ëŠ” êµ¬ì„±ì´ ê°€ëŠ¥í•˜ë¯€ë¡œ ì›í•˜ëŠ” íŠ¹ì • ì†ì„±ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•™ìŠµì„ ìœ„í•´ ëª¨ë¸ì„ ì‰½ê²Œ ì„¤ì •í•˜ê±°ë‚˜ ê¸°ì¡´ì˜ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ìˆ˜ì •í•˜ì—¬ ë¯¸ì„¸ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
