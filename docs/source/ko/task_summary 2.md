<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# ğŸ¤— Transformersë¡œ í•  ìˆ˜ ìˆëŠ” ê²ƒ[[what__transformers_can_do]]

ğŸ¤— TransformersëŠ” ìì—°ì–´ì²˜ë¦¬(NLP), ì»´í“¨í„° ë¹„ì „, ì˜¤ë””ì˜¤ ë° ìŒì„± ì²˜ë¦¬ ì‘ì—…ì— ëŒ€í•œ ì‚¬ì „í›ˆë ¨ëœ ìµœì²¨ë‹¨ ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. 
ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ë¿ë§Œ ì•„ë‹ˆë¼ ì»´í“¨í„° ë¹„ì „ ì‘ì—…ì„ ìœ„í•œ í˜„ëŒ€ì ì¸ í•©ì„±ê³± ì‹ ê²½ë§ê³¼ ê°™ì€ íŠ¸ëœìŠ¤í¬ë¨¸ê°€ ì•„ë‹Œ ëª¨ë¸ë„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. 

ìŠ¤ë§ˆíŠ¸í°, ì•±, í…”ë ˆë¹„ì „ê³¼ ê°™ì€ ì˜¤ëŠ˜ë‚  ê°€ì¥ ì¸ê¸° ìˆëŠ” ì†Œë¹„ì ì œí’ˆì„ ì‚´í´ë³´ë©´, ë”¥ëŸ¬ë‹ ê¸°ìˆ ì´ ê·¸ ë’¤ì— ì‚¬ìš©ë˜ê³  ìˆì„ í™•ë¥ ì´ ë†’ìŠµë‹ˆë‹¤. 
ìŠ¤ë§ˆíŠ¸í°ìœ¼ë¡œ ì´¬ì˜í•œ ì‚¬ì§„ì—ì„œ ë°°ê²½ ê°ì²´ë¥¼ ì œê±°í•˜ê³  ì‹¶ë‹¤ë©´ ì–´ë–»ê²Œ í• ê¹Œìš”? ì´ëŠ” íŒŒë†‰í‹± ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‘ì—…ì˜ ì˜ˆì…ë‹ˆë‹¤(ì•„ì§ ì´ê²Œ ë¬´ì—‡ì¸ì§€ ëª¨ë¥¸ë‹¤ë©´, ë‹¤ìŒ ì„¹ì…˜ì—ì„œ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤!).

ì´ í˜ì´ì§€ëŠ” ë‹¤ì–‘í•œ ìŒì„± ë° ì˜¤ë””ì˜¤, ì»´í“¨í„° ë¹„ì „, NLP ì‘ì—…ì„ ğŸ¤— Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ì—¬ ë‹¤ë£¨ëŠ” ê°„ë‹¨í•œ ì˜ˆì œë¥¼ 3ì¤„ì˜ ì½”ë“œë¡œ ì œê³µí•©ë‹ˆë‹¤. 

## ì˜¤ë””ì˜¤[[audio]]


ìŒì„± ë° ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì‘ì—…ì€ ë‹¤ë¥¸ ëª¨ë‹¬ë¦¬í‹°ì™€ ì•½ê°„ ë‹¤ë¦…ë‹ˆë‹¤. ì´ëŠ” ì£¼ë¡œ ì˜¤ë””ì˜¤ê°€ ì—°ì†ì ì¸ ì‹ í˜¸ë¡œ ì…ë ¥ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. 
í…ìŠ¤íŠ¸ì™€ ë‹¬ë¦¬ ì›ë³¸ ì˜¤ë””ì˜¤ íŒŒí˜•(waveform)ì€ ë¬¸ì¥ì´ ë‹¨ì–´ë¡œ ë‚˜ëˆ ì§€ëŠ” ê²ƒì²˜ëŸ¼ ê¹”ë”í•˜ê²Œ ì´ì‚°ì ì¸ ë¬¶ìŒìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 
ì´ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ì›ë³¸ ì˜¤ë””ì˜¤ ì‹ í˜¸ëŠ” ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§ë©ë‹ˆë‹¤. í•´ë‹¹ ê°„ê²© ë‚´ì—ì„œ ë” ë§ì€ ìƒ˜í”Œì„ ì·¨í•  ê²½ìš° ìƒ˜í”Œë§ë¥ ì´ ë†’ì•„ì§€ë©°, ì˜¤ë””ì˜¤ëŠ” ì›ë³¸ ì˜¤ë””ì˜¤ ì†ŒìŠ¤ì— ë” ê°€ê¹Œì›Œì§‘ë‹ˆë‹¤.

ê³¼ê±°ì˜ ì ‘ê·¼ ë°©ì‹ì€ ì˜¤ë””ì˜¤ì—ì„œ ìœ ìš©í•œ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê¸° ìœ„í•´ ì˜¤ë””ì˜¤ë¥¼ ì „ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ì—ˆìŠµë‹ˆë‹¤. 
í•˜ì§€ë§Œ í˜„ì¬ëŠ” ì›ë³¸ ì˜¤ë””ì˜¤ íŒŒí˜•ì„ íŠ¹ì„± ì¸ì½”ë”ì— ì§ì ‘ ë„£ì–´ì„œ ì˜¤ë””ì˜¤ í‘œí˜„(representation)ì„ ì¶”ì¶œí•˜ëŠ” ê²ƒì´ ë” ì¼ë°˜ì ì…ë‹ˆë‹¤. 
ì´ë ‡ê²Œ í•˜ë©´ ì „ì²˜ë¦¬ ë‹¨ê³„ê°€ ë‹¨ìˆœí•´ì§€ê³  ëª¨ë¸ì´ ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì§•ì„ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ì˜¤ë””ì˜¤ ë¶„ë¥˜[[audio_classification]]


ì˜¤ë””ì˜¤ ë¶„ë¥˜ëŠ” ì˜¤ë””ì˜¤ ë°ì´í„°ì— ë¯¸ë¦¬ ì •ì˜ëœ í´ë˜ìŠ¤ ì§‘í•©ì˜ ë ˆì´ë¸”ì„ ì§€ì •í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. ì´ëŠ” ë§ì€ êµ¬ì²´ì ì¸ ì‘ìš© í”„ë¡œê·¸ë¨ì„ í¬í•¨í•œ ë„“ì€ ë²”ì£¼ì…ë‹ˆë‹¤.

ì¼ë¶€ ì˜ˆì‹œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

* ìŒí–¥ ì¥ë©´ ë¶„ë¥˜: ì˜¤ë””ì˜¤ì— ì¥ë©´ ë ˆì´ë¸”("ì‚¬ë¬´ì‹¤", "í•´ë³€", "ê²½ê¸°ì¥")ì„ ì§€ì •í•©ë‹ˆë‹¤.
* ìŒí–¥ ì´ë²¤íŠ¸ ê°ì§€: ì˜¤ë””ì˜¤ì— ì†Œë¦¬ ì´ë²¤íŠ¸ ë ˆì´ë¸”("ì°¨ ê²½ì ", "ê³ ë˜ ìš¸ìŒì†Œë¦¬", "ìœ ë¦¬ íŒŒì†")ì„ ì§€ì •í•©ë‹ˆë‹¤.
* íƒœê¹…: ì—¬ëŸ¬ ê°€ì§€ ì†Œë¦¬(ìƒˆ ì§€ì €ê·, íšŒì˜ì—ì„œì˜ í™”ì ì‹ë³„)ê°€ í¬í•¨ëœ ì˜¤ë””ì˜¤ì— ë ˆì´ë¸”ì„ ì§€ì •í•©ë‹ˆë‹¤.
* ìŒì•… ë¶„ë¥˜: ìŒì•…ì— ì¥ë¥´ ë ˆì´ë¸”("ë©”íƒˆ", "í™í•©", "ì»¨íŠ¸ë¦¬")ì„ ì§€ì •í•©ë‹ˆë‹¤.

```py
>>> from transformers import pipeline

>>> classifier = pipeline(task="audio-classification", model="superb/hubert-base-superb-er")
>>> preds = classifier("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> preds
[{'score': 0.4532, 'label': 'hap'},
 {'score': 0.3622, 'label': 'sad'},
 {'score': 0.0943, 'label': 'neu'},
 {'score': 0.0903, 'label': 'ang'}]
```

### ìë™ ìŒì„± ì¸ì‹[[automatic_speech_recognition]]


ìë™ ìŒì„± ì¸ì‹(ASR)ì€ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. 
ìŒì„±ì€ ì¸ê°„ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ì˜ì‚¬ì†Œí†µ í˜•íƒœì´ê¸° ë•Œë¬¸ì— ASRì€ ê°€ì¥ ì¼ë°˜ì ì¸ ì˜¤ë””ì˜¤ ì‘ì—… ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. 
ì˜¤ëŠ˜ë‚  ASR ì‹œìŠ¤í…œì€ ìŠ¤í”¼ì»¤, ì „í™” ë° ìë™ì°¨ì™€ ê°™ì€ "ìŠ¤ë§ˆíŠ¸" ê¸°ìˆ  ì œí’ˆì— ë‚´ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤. 
ìš°ë¦¬ëŠ” ê°€ìƒ ë¹„ì„œì—ê²Œ ìŒì•… ì¬ìƒ, ì•Œë¦¼ ì„¤ì • ë° ë‚ ì”¨ ì •ë³´ë¥¼ ìš”ì²­í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

í•˜ì§€ë§Œ íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ê°€ í•´ê²°í•˜ëŠ” ë° ë„ì›€ì„ ì¤€ í•µì‹¬ ë„ì „ ê³¼ì œ ì¤‘ í•˜ë‚˜ëŠ” ì–‘ì´ ë°ì´í„° ì–‘ì´ ì ì€ ì–¸ì–´(low-resource language)ì— ëŒ€í•œ ê²ƒì…ë‹ˆë‹¤. ëŒ€ëŸ‰ì˜ ìŒì„± ë°ì´í„°ë¡œ ì‚¬ì „ í›ˆë ¨í•œ í›„ ë°ì´í„° ì–‘ì´ ì ì€ ì–¸ì–´ì—ì„œ ë ˆì´ë¸”ì´ ì§€ì •ëœ ìŒì„± ë°ì´í„° 1ì‹œê°„ë§Œìœ¼ë¡œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ë©´ ì´ì „ì˜ 100ë°° ë§ì€ ë ˆì´ë¸”ì´ ì§€ì •ëœ ë°ì´í„°ë¡œ í›ˆë ¨ëœ ASR ì‹œìŠ¤í…œë³´ë‹¤ í›¨ì”¬ ë” ë†’ì€ í’ˆì§ˆì˜ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
```py
>>> from transformers import pipeline

>>> transcriber = pipeline(task="automatic-speech-recognition", model="openai/whisper-small")
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
```

## ì»´í“¨í„° ë¹„ì „[[computer_vision]]

ì»´í“¨í„° ë¹„ì „ ì‘ì—… ì¤‘ ê°€ì¥ ì´ˆê¸°ì˜ ì„±ê³µì ì¸ ì‘ì—… ì¤‘ í•˜ë‚˜ëŠ” [í•©ì„±ê³± ì‹ ê²½ë§(CNN)](glossary#convolution)ì„ ì‚¬ìš©í•˜ì—¬ ìš°í¸ë²ˆí˜¸ ìˆ«ì ì´ë¯¸ì§€ë¥¼ ì¸ì‹í•˜ëŠ” ê²ƒì´ì—ˆìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ëŠ” í”½ì…€ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©° ê° í”½ì…€ì€ ìˆ«ì ê°’ìœ¼ë¡œ í‘œí˜„ë©ë‹ˆë‹¤. ì´ë¡œì¨ ì´ë¯¸ì§€ë¥¼ í”½ì…€ ê°’ì˜ í–‰ë ¬ë¡œ ë‚˜íƒ€ë‚´ëŠ” ê²ƒì´ ì‰¬ì›Œì§‘ë‹ˆë‹¤. íŠ¹ì •í•œ í”½ì…€ ê°’ì˜ ì¡°í•©ì€ ì´ë¯¸ì§€ì˜ ìƒ‰ìƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

ì»´í“¨í„° ë¹„ì „ ì‘ì—…ì€ ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ìŒ ë‘ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ì ‘ê·¼ ê°€ëŠ¥í•©ë‹ˆë‹¤:

1. í•©ì„±ê³±ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì˜ ë‚®ì€ ìˆ˜ì¤€ íŠ¹ì§•ì—ì„œ ë†’ì€ ìˆ˜ì¤€ì˜ ì¶”ìƒì ì¸ ìš”ì†Œê¹Œì§€ ê³„ì¸µì ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.

2. ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜ë¡œ ë‚˜ëˆ„ê³  íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì ì§„ì ìœ¼ë¡œ ê° ì´ë¯¸ì§€ íŒ¨ì¹˜ê°€ ì„œë¡œ ì–´ë– í•œ ë°©ì‹ìœ¼ë¡œ ì—°ê´€ë˜ì–´ ì´ë¯¸ì§€ë¥¼ í˜•ì„±í•˜ëŠ”ì§€ í•™ìŠµí•©ë‹ˆë‹¤. `CNN`ì—ì„œ ì„ í˜¸í•˜ëŠ” ìƒí–¥ì‹ ì ‘ê·¼ë²•ê³¼ëŠ” ë‹¬ë¦¬, ì´ ë°©ì‹ì€ íë¦¿í•œ ì´ë¯¸ì§€ë¡œ ì´ˆì•ˆì„ ê·¸ë¦¬ê³  ì ì§„ì ìœ¼ë¡œ ì„ ëª…í•œ ì´ë¯¸ì§€ë¡œ ë§Œë“¤ì–´ê°€ëŠ” ê²ƒê³¼ ìœ ì‚¬í•©ë‹ˆë‹¤.

### ì´ë¯¸ì§€ ë¶„ë¥˜[[image_classification]]


ì´ë¯¸ì§€ ë¶„ë¥˜ëŠ” í•œ ê°œì˜ ì „ì²´ ì´ë¯¸ì§€ì— ë¯¸ë¦¬ ì •ì˜ëœ í´ë˜ìŠ¤ ì§‘í•©ì˜ ë ˆì´ë¸”ì„ ì§€ì •í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. 

ëŒ€ë¶€ë¶„ì˜ ë¶„ë¥˜ ì‘ì—…ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ, ì´ë¯¸ì§€ ë¶„ë¥˜ì—ëŠ” ë‹¤ì–‘í•œ ì‹¤ìš©ì ì¸ ìš©ë„ê°€ ìˆìœ¼ë©°, ì¼ë¶€ ì˜ˆì‹œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:


* ì˜ë£Œ: ì§ˆë³‘ì„ ê°ì§€í•˜ê±°ë‚˜ í™˜ì ê±´ê°•ì„ ëª¨ë‹ˆí„°ë§í•˜ê¸° ìœ„í•´ ì˜ë£Œ ì´ë¯¸ì§€ì— ë ˆì´ë¸”ì„ ì§€ì •í•©ë‹ˆë‹¤.
* í™˜ê²½: ìœ„ì„± ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•˜ì—¬ ì‚°ë¦¼ ë²Œì±„ë¥¼ ê°ì‹œí•˜ê³  ì•¼ìƒ ì§€ì—­ ê´€ë¦¬ë¥¼ ìœ„í•œ ì •ë³´ë¥¼ ì œê³µí•˜ê±°ë‚˜ ì‚°ë¶ˆì„ ê°ì§€í•©ë‹ˆë‹¤. 
* ë†ì—…: ì‘ë¬¼ ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•˜ì—¬ ì‹ë¬¼ ê±´ê°•ì„ í™•ì¸í•˜ê±°ë‚˜ ìœ„ì„± ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•˜ì—¬ í† ì§€ ì´ìš© ê´€ì°°ì— ì‚¬ìš©í•©ë‹ˆë‹¤.
* ìƒíƒœí•™: ë™ë¬¼ì´ë‚˜ ì‹ë¬¼ ì¢… ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•˜ì—¬ ì•¼ìƒ ë™ë¬¼ ê°œì²´êµ°ì„ ì¡°ì‚¬í•˜ê±°ë‚˜ ë©¸ì¢… ìœ„ê¸°ì— ì²˜í•œ ì¢…ì„ ì¶”ì í•©ë‹ˆë‹¤.

```py
>>> from transformers import pipeline

>>> classifier = pipeline(task="image-classification")
>>> preds = classifier(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> print(*preds, sep="\n")
{'score': 0.4335, 'label': 'lynx, catamount'}
{'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}
{'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}
{'score': 0.0239, 'label': 'Egyptian cat'}
{'score': 0.0229, 'label': 'tiger cat'}
```

### ê°ì²´ íƒì§€[[object_detection]]


ì´ë¯¸ì§€ ë¶„ë¥˜ì™€ ë‹¬ë¦¬ ê°ì²´ íƒì§€ëŠ” ì´ë¯¸ì§€ ë‚´ì—ì„œ ì—¬ëŸ¬ ê°ì²´ë¥¼ ì‹ë³„í•˜ê³  ë°”ìš´ë”© ë°•ìŠ¤ë¡œ ì •ì˜ëœ ê°ì²´ì˜ ìœ„ì¹˜ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤. 

ê°ì²´ íƒì§€ì˜ ëª‡ ê°€ì§€ ì‘ìš© ì˜ˆì‹œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

* ììœ¨ ì£¼í–‰ ì°¨ëŸ‰: ë‹¤ë¥¸ ì°¨ëŸ‰, ë³´í–‰ì ë° ì‹ í˜¸ë“±ê³¼ ê°™ì€ ì¼ìƒì ì¸ êµí†µ ê°ì²´ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.
* ì›ê²© ê°ì§€: ì¬ë‚œ ëª¨ë‹ˆí„°ë§, ë„ì‹œ ê³„íš ë° ê¸°ìƒ ì˜ˆì¸¡ ë“±ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
* ê²°í•¨ íƒì§€: ê±´ë¬¼ì˜ ê· ì—´ì´ë‚˜ êµ¬ì¡°ì  ì†ìƒ, ì œì¡° ê²°í•¨ ë“±ì„ íƒì§€í•©ë‹ˆë‹¤.


```py
>>> from transformers import pipeline

>>> detector = pipeline(task="object-detection")
>>> preds = detector(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"], "box": pred["box"]} for pred in preds]
>>> preds
[{'score': 0.9865,
  'label': 'cat',
  'box': {'xmin': 178, 'ymin': 154, 'xmax': 882, 'ymax': 598}}]
```

### ì´ë¯¸ì§€ ë¶„í• [[image_segmentation]]


ì´ë¯¸ì§€ ë¶„í• ì€ í”½ì…€ ì°¨ì›ì˜ ì‘ì—…ìœ¼ë¡œ, ì´ë¯¸ì§€ ë‚´ì˜ ëª¨ë“  í”½ì…€ì„ í´ë˜ìŠ¤ì— í• ë‹¹í•©ë‹ˆë‹¤. ì´ëŠ” ê°ì²´ íƒì§€ì™€ ë‹¤ë¦…ë‹ˆë‹¤. ê°ì²´ íƒì§€ëŠ” ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ë‚´ì˜ ê°ì²´ë¥¼ ë ˆì´ë¸”ë§í•˜ê³  ì˜ˆì¸¡í•˜ëŠ” ë°˜ë©´, ë¶„í• ì€ ë” ì„¸ë¶„í™”ëœ ì‘ì—…ì…ë‹ˆë‹¤. ë¶„í• ì€ í”½ì…€ ìˆ˜ì¤€ì—ì„œ ê°ì²´ë¥¼ ê°ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

ì´ë¯¸ì§€ ë¶„í• ì—ëŠ” ì—¬ëŸ¬ ìœ í˜•ì´ ìˆìŠµë‹ˆë‹¤:

* ì¸ìŠ¤í„´ìŠ¤ ë¶„í• : ê°œì²´ì˜ í´ë˜ìŠ¤ë¥¼ ë ˆì´ë¸”ë§í•˜ëŠ” ê²ƒ ì™¸ì—ë„, ê°œì²´ì˜ ê° êµ¬ë¶„ëœ ì¸ìŠ¤í„´ìŠ¤ì—ë„ ë ˆì´ë¸”ì„ ì§€ì •í•©ë‹ˆë‹¤ ("ê°œ-1", "ê°œ-2" ë“±).
* íŒŒë†‰í‹± ë¶„í• : ì˜ë¯¸ì  ë¶„í• ê³¼ ì¸ìŠ¤í„´ìŠ¤ ë¶„í• ì˜ ì¡°í•©ì…ë‹ˆë‹¤. ê° í”½ì…€ì„ ì˜ë¯¸ì  í´ë˜ìŠ¤ë¡œ ë ˆì´ë¸”ë§í•˜ëŠ” **ë™ì‹œì—** ê°œì²´ì˜ ê°ê° êµ¬ë¶„ëœ ì¸ìŠ¤í„´ìŠ¤ë¡œë„ ë ˆì´ë¸”ì„ ì§€ì •í•©ë‹ˆë‹¤.

ë¶„í•  ì‘ì—…ì€ ììœ¨ ì£¼í–‰ ì°¨ëŸ‰ì—ì„œ ìœ ìš©í•˜ë©°, ì£¼ë³€ í™˜ê²½ì˜ í”½ì…€ ìˆ˜ì¤€ ì§€ë„ë¥¼ ìƒì„±í•˜ì—¬ ë³´í–‰ìì™€ ë‹¤ë¥¸ ì°¨ëŸ‰ ì£¼ë³€ì—ì„œ ì•ˆì „í•˜ê²Œ íƒìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ ì˜ë£Œ ì˜ìƒì—ì„œë„ ìœ ìš©í•©ë‹ˆë‹¤. ë¶„í•  ì‘ì—…ì´ í”½ì…€ ìˆ˜ì¤€ì—ì„œ ê°ì²´ë¥¼ ê°ì§€í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ë¹„ì •ìƒì ì¸ ì„¸í¬ë‚˜ ì¥ê¸°ì˜ íŠ¹ì§•ì„ ì‹ë³„í•˜ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ ë¶„í• ì€ ì˜ë¥˜ ê°€ìƒ ì‹œì°©ì´ë‚˜ ì¹´ë©”ë¼ë¥¼ í†µí•´ ì‹¤ì œ ì„¸ê³„ì— ê°€ìƒ ê°œì²´ë¥¼ ë§ì”Œì›Œ ì¦ê°• í˜„ì‹¤ ê²½í—˜ì„ ë§Œë“œëŠ” ë“± ì „ì ìƒê±°ë˜ ë¶„ì•¼ì—ì„œë„ ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```py
>>> from transformers import pipeline

>>> segmenter = pipeline(task="image-segmentation")
>>> preds = segmenter(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> print(*preds, sep="\n")
{'score': 0.9879, 'label': 'LABEL_184'}
{'score': 0.9973, 'label': 'snow'}
{'score': 0.9972, 'label': 'cat'}
```

### ê¹Šì´ ì¶”ì •[[depth_estimation]]

ê¹Šì´ ì¶”ì •ì€ ì¹´ë©”ë¼ë¡œë¶€í„° ì´ë¯¸ì§€ ë‚´ë¶€ì˜ ê° í”½ì…€ì˜ ê±°ë¦¬ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ì´ ì»´í“¨í„° ë¹„ì „ ì‘ì—…ì€ íŠ¹íˆ ì¥ë©´ ì´í•´ì™€ ì¬êµ¬ì„±ì— ì¤‘ìš”í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ììœ¨ ì£¼í–‰ ì°¨ëŸ‰ì€ ë³´í–‰ì, êµí†µ í‘œì§€íŒ ë° ë‹¤ë¥¸ ì°¨ëŸ‰ê³¼ ê°™ì€ ê°ì²´ì™€ì˜ ê±°ë¦¬ë¥¼ ì´í•´í•˜ì—¬ ì¥ì• ë¬¼ê³¼ ì¶©ëŒì„ í”¼í•´ì•¼ í•©ë‹ˆë‹¤. ê¹Šì´ ì •ë³´ëŠ” ë˜í•œ 2D ì´ë¯¸ì§€ì—ì„œ 3D í‘œí˜„ì„ êµ¬ì„±í•˜ëŠ” ë° ë„ì›€ì´ ë˜ë©° ìƒë¬¼í•™ì  êµ¬ì¡°ë‚˜ ê±´ë¬¼ì˜ ê³ í’ˆì§ˆ 3D í‘œí˜„ì„ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ê¹Šì´ ì¶”ì •ì—ëŠ” ë‘ ê°€ì§€ ì ‘ê·¼ ë°©ì‹ì´ ìˆìŠµë‹ˆë‹¤:

* ìŠ¤í…Œë ˆì˜¤: ì•½ê°„ ë‹¤ë¥¸ ê°ë„ì—ì„œ ì´¬ì˜ëœ ë™ì¼í•œ ì´ë¯¸ì§€ ë‘ ì¥ì„ ë¹„êµí•˜ì—¬ ê¹Šì´ë¥¼ ì¶”ì •í•©ë‹ˆë‹¤.
* ë‹¨ì•ˆ: ë‹¨ì¼ ì´ë¯¸ì§€ì—ì„œ ê¹Šì´ë¥¼ ì¶”ì •í•©ë‹ˆë‹¤.


```py
>>> from transformers import pipeline

>>> depth_estimator = pipeline(task="depth-estimation")
>>> preds = depth_estimator(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
```

## ìì—°ì–´ì²˜ë¦¬[[natural_language_processing]]

í…ìŠ¤íŠ¸ëŠ” ì¸ê°„ì´ ì˜ì‚¬ ì†Œí†µí•˜ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ë°©ì‹ ì¤‘ í•˜ë‚˜ì´ê¸° ë•Œë¬¸ì— ìì—°ì–´ì²˜ë¦¬ ì—­ì‹œ ê°€ì¥ ì¼ë°˜ì ì¸ ì‘ì—… ìœ í˜• ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ëª¨ë¸ì´ ì¸ì‹í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë³€í™˜í•˜ë ¤ë©´ í† í°í™”í•´ì•¼ í•©ë‹ˆë‹¤. ì´ëŠ” í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¥¼ ê°œë³„ ë‹¨ì–´ ë˜ëŠ” í•˜ìœ„ ë‹¨ì–´(í† í°)ë¡œ ë¶„í• í•œ ë‹¤ìŒ ì´ëŸ¬í•œ í† í°ì„ ìˆ«ìë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¥¼ ìˆ«ì ì‹œí€€ìŠ¤ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìœ¼ë©°, ìˆ«ì ì‹œí€€ìŠ¤ë¥¼ ë‹¤ì–‘í•œ ìì—°ì–´ì²˜ë¦¬ ì‘ì—…ì„ í•´ê²°í•˜ê¸° ìœ„í•œ ëª¨ë¸ì— ì…ë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

### í…ìŠ¤íŠ¸ ë¶„ë¥˜[[text_classification]]

ë‹¤ë¥¸ ëª¨ë‹¬ë¦¬í‹°ì—ì„œì˜ ë¶„ë¥˜ ì‘ì—…ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ í…ìŠ¤íŠ¸ ë¶„ë¥˜ëŠ” ë¯¸ë¦¬ ì •ì˜ëœ í´ë˜ìŠ¤ ì§‘í•©ì—ì„œ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤(ë¬¸ì¥ ìˆ˜ì¤€, ë‹¨ë½ ë˜ëŠ” ë¬¸ì„œ ë“±)ì— ë ˆì´ë¸”ì„ ì§€ì •í•©ë‹ˆë‹¤. í…ìŠ¤íŠ¸ ë¶„ë¥˜ì—ëŠ” ë‹¤ì–‘í•œ ì‹¤ìš©ì ì¸ ì‘ìš© ì‚¬ë¡€ê°€ ìˆìœ¼ë©°, ì¼ë¶€ ì˜ˆì‹œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

* ê°ì„± ë¶„ì„: í…ìŠ¤íŠ¸ë¥¼ `ê¸ì •` ë˜ëŠ” `ë¶€ì •`ê³¼ ê°™ì€ ì–´ë–¤ ê·¹ì„±ì— ë”°ë¼ ë ˆì´ë¸”ë§í•˜ì—¬ ì •ì¹˜, ê¸ˆìœµ, ë§ˆì¼€íŒ…ê³¼ ê°™ì€ ë¶„ì•¼ì—ì„œ ì˜ì‚¬ ê²°ì •ì— ì •ë³´ë¥¼ ì œê³µí•˜ê³  ì§€ì›í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* ì½˜í…ì¸  ë¶„ë¥˜: í…ìŠ¤íŠ¸ë¥¼ ì£¼ì œì— ë”°ë¼ ë ˆì´ë¸”ë§(ë‚ ì”¨, ìŠ¤í¬ì¸ , ê¸ˆìœµ ë“±)í•˜ì—¬ ë‰´ìŠ¤ ë° ì†Œì…œ ë¯¸ë””ì–´ í”¼ë“œì—ì„œ ì •ë³´ë¥¼ êµ¬ì„±í•˜ê³  í•„í„°ë§í•˜ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```py
>>> from transformers import pipeline

>>> classifier = pipeline(task="sentiment-analysis")
>>> preds = classifier("Hugging Face is the best thing since sliced bread!")
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> preds
[{'score': 0.9991, 'label': 'POSITIVE'}]
```

### í† í° ë¶„ë¥˜[[token_classification]]

ëª¨ë“  ìì—°ì–´ì²˜ë¦¬ ì‘ì—…ì—ì„œëŠ” í…ìŠ¤íŠ¸ê°€ ê°œë³„ ë‹¨ì–´ë‚˜ í•˜ìœ„ ë‹¨ì–´ë¡œ ë¶„ë¦¬ë˜ì–´ ì „ì²˜ë¦¬ë©ë‹ˆë‹¤. ë¶„ë¦¬ëœ ë‹¨ì–´ë¥¼ [í† í°](/glossary#token)ì´ë¼ê³  í•©ë‹ˆë‹¤. í† í° ë¶„ë¥˜ëŠ” ê° í† í°ì— ë¯¸ë¦¬ ì •ì˜ëœ í´ë˜ìŠ¤ ì§‘í•©ì˜ ë ˆì´ë¸”ì„ í• ë‹¹í•©ë‹ˆë‹¤.

í† í° ë¶„ë¥˜ì˜ ë‘ ê°€ì§€ ì¼ë°˜ì ì¸ ìœ í˜•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

* ê°œì²´ëª… ì¸ì‹ (NER): í† í°ì„ ì¡°ì§, ì¸ë¬¼, ìœ„ì¹˜ ë˜ëŠ” ë‚ ì§œì™€ ê°™ì€ ê°œì²´ ë²”ì£¼ì— ë”°ë¼ ë ˆì´ë¸”ë§í•©ë‹ˆë‹¤. NERì€ íŠ¹íˆ ìœ ì „ì²´í•™ì ì¸ í™˜ê²½ì—ì„œ ìœ ì „ì, ë‹¨ë°±ì§ˆ ë° ì•½ë¬¼ ì´ë¦„ì— ë ˆì´ë¸”ì„ ì§€ì •í•˜ëŠ” ë° ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.
* í’ˆì‚¬ íƒœê¹… (POS): ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ì™€ ê°™ì€ í’ˆì‚¬ì— ë”°ë¼ í† í°ì— ë ˆì´ë¸”ì„ í• ë‹¹í•©ë‹ˆë‹¤. POSëŠ” ë²ˆì—­ ì‹œìŠ¤í…œì´ ë™ì¼í•œ ë‹¨ì–´ê°€ ë¬¸ë²•ì ìœ¼ë¡œ ì–´ë–»ê²Œ ë‹¤ë¥¸ì§€ ì´í•´í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤ (ëª…ì‚¬ë¡œ ì‚¬ìš©ë˜ëŠ” "bank(ì€í–‰)"ê³¼ ë™ì‚¬ë¡œ ì‚¬ìš©ë˜ëŠ” "bank(ì˜ˆê¸ˆì„ ì˜ˆì¹˜í•˜ë‹¤)"ê³¼ ê°™ì€ ê²½ìš°).


```py
>>> from transformers import pipeline

>>> classifier = pipeline(task="ner")
>>> preds = classifier("Hugging Face is a French company based in New York City.")
>>> preds = [
...     {
...         "entity": pred["entity"],
...         "score": round(pred["score"], 4),
...         "index": pred["index"],
...         "word": pred["word"],
...         "start": pred["start"],
...         "end": pred["end"],
...     }
...     for pred in preds
... ]
>>> print(*preds, sep="\n")
{'entity': 'I-ORG', 'score': 0.9968, 'index': 1, 'word': 'Hu', 'start': 0, 'end': 2}
{'entity': 'I-ORG', 'score': 0.9293, 'index': 2, 'word': '##gging', 'start': 2, 'end': 7}
{'entity': 'I-ORG', 'score': 0.9763, 'index': 3, 'word': 'Face', 'start': 8, 'end': 12}
{'entity': 'I-MISC', 'score': 0.9983, 'index': 6, 'word': 'French', 'start': 18, 'end': 24}
{'entity': 'I-LOC', 'score': 0.999, 'index': 10, 'word': 'New', 'start': 42, 'end': 45}
{'entity': 'I-LOC', 'score': 0.9987, 'index': 11, 'word': 'York', 'start': 46, 'end': 50}
{'entity': 'I-LOC', 'score': 0.9992, 'index': 12, 'word': 'City', 'start': 51, 'end': 55}
```

### ì§ˆì˜ì‘ë‹µ[[question_answering]]

ì§ˆì˜ì‘ë‹µì€ ë˜ í•˜ë‚˜ì˜ í† í° ì°¨ì›ì˜ ì‘ì—…ìœ¼ë¡œ, ë¬¸ë§¥ì´ ìˆì„ ë•Œ(ê°œë°©í˜• ë„ë©”ì¸)ì™€ ë¬¸ë§¥ì´ ì—†ì„ ë•Œ(íì‡„í˜• ë„ë©”ì¸) ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ë°˜í™˜í•©ë‹ˆë‹¤. ì´ ì‘ì—…ì€ ê°€ìƒ ë¹„ì„œì—ê²Œ ì‹ë‹¹ì´ ì˜ì—… ì¤‘ì¸ì§€ì™€ ê°™ì€ ì§ˆë¬¸ì„ í•  ë•Œë§ˆë‹¤ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê³ ê° ì§€ì› ë˜ëŠ” ê¸°ìˆ  ì§€ì›ì„ ì œê³µí•˜ê±°ë‚˜ ê²€ìƒ‰ ì—”ì§„ì´ ìš”ì²­í•œ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” ë° ë„ì›€ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì§ˆë¬¸ ë‹µë³€ì—ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë‘ ê°€ì§€ ìœ í˜•ì´ ìˆìŠµë‹ˆë‹¤:

* ì¶”ì¶œí˜•: ì§ˆë¬¸ê³¼ ë¬¸ë§¥ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ëª¨ë¸ì´ ì£¼ì–´ì§„ ë¬¸ë§¥ì˜ ì¼ë¶€ì—ì„œ ê°€ì ¸ì˜¨ í…ìŠ¤íŠ¸ì˜ ë²”ìœ„ë¥¼ ë‹µë³€ìœ¼ë¡œ í•©ë‹ˆë‹¤.
* ìƒì„±í˜•: ì§ˆë¬¸ê³¼ ë¬¸ë§¥ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ì£¼ì–´ì§„ ë¬¸ë§¥ì„ í†µí•´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ ì ‘ê·¼ ë°©ì‹ì€ [`QuestionAnsweringPipeline`] ëŒ€ì‹  [`Text2TextGenerationPipeline`]ì„ í†µí•´ ì²˜ë¦¬ë©ë‹ˆë‹¤.

```py
>>> from transformers import pipeline

>>> question_answerer = pipeline(task="question-answering")
>>> preds = question_answerer(
...     question="What is the name of the repository?",
...     context="The name of the repository is huggingface/transformers",
... )
>>> print(
...     f"score: {round(preds['score'], 4)}, start: {preds['start']}, end: {preds['end']}, answer: {preds['answer']}"
... )
score: 0.9327, start: 30, end: 54, answer: huggingface/transformers
```

### ìš”ì•½[[summarization]]

ìš”ì•½ì€ ì›ë³¸ ë¬¸ì„œì˜ ì˜ë¯¸ë¥¼ ìµœëŒ€í•œ ë³´ì¡´í•˜ë©´ì„œ ê¸´ ë¬¸ì„œë¥¼ ì§§ì€ ë¬¸ì„œë¡œ ë§Œë“œëŠ” ì‘ì—…ì…ë‹ˆë‹¤. ìš”ì•½ì€ `sequence-to-sequence` ì‘ì—…ì…ë‹ˆë‹¤. ì…ë ¥ë³´ë‹¤ ì§§ì€ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤. ìš”ì•½ ì‘ì—…ì€ ë…ìê°€ ì¥ë¬¸ ë¬¸ì„œë“¤ì˜ ì£¼ìš” í¬ì¸íŠ¸ë¥¼ ë¹ ë¥´ê²Œ ì´í•´í•˜ëŠ” ë° ë„ì›€ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì…ë²•ì•ˆ, ë²•ë¥  ë° ê¸ˆìœµ ë¬¸ì„œ, íŠ¹í—ˆ ë° ê³¼í•™ ë…¼ë¬¸ì€ ìš”ì•½ ì‘ì—…ì´ ë…ìì˜ ì‹œê°„ì„ ì ˆì•½í•˜ê³  ë…ì„œ ë³´ì¡° ë„êµ¬ë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆëŠ” ëª‡ ê°€ì§€ ì˜ˆì‹œì…ë‹ˆë‹¤.

ì§ˆë¬¸ ë‹µë³€ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ìš”ì•½ì—ëŠ” ë‘ ê°€ì§€ ìœ í˜•ì´ ìˆìŠµë‹ˆë‹¤:

* ì¶”ì¶œí˜•: ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë¬¸ì¥ì„ ì‹ë³„í•˜ê³  ì¶”ì¶œí•©ë‹ˆë‹¤.
* ìƒì„±í˜•: ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ëª©í‘œ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤. ì…ë ¥ ë¬¸ì„œì— ì—†ëŠ” ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ í¬í•¨í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. [`SummarizationPipeline`]ì€ ìƒì„±í˜• ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

```py
>>> from transformers import pipeline

>>> summarizer = pipeline(task="summarization")
>>> summarizer(
...     "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles."
... )
[{'summary_text': ' The Transformer is the first sequence transduction model based entirely on attention . It replaces the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention . For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .'}]
```

### ë²ˆì—­[[translation]]

ë²ˆì—­ì€ í•œ ì–¸ì–´ë¡œ ëœ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¥¼ ë‹¤ë¥¸ ì–¸ì–´ë¡œ ë³€í™˜í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. ì´ëŠ” ì„œë¡œ ë‹¤ë¥¸ ë°°ê²½ì„ ê°€ì§„ ì‚¬ëŒë“¤ì´ ì„œë¡œ ì†Œí†µí•˜ëŠ” ë° ë„ì›€ì„ ì£¼ëŠ” ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤. ë” ë„“ì€ ëŒ€ì¤‘ì—ê²Œ ì½˜í…ì¸ ë¥¼ ë²ˆì—­í•˜ì—¬ ì „ë‹¬í•˜ê±°ë‚˜, ìƒˆë¡œìš´ ì–¸ì–´ë¥¼ ë°°ìš°ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” í•™ìŠµ ë„êµ¬ê°€ ë  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ìš”ì•½ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ, ë²ˆì—­ì€ `sequence-to-sequence` ì‘ì—…ì…ë‹ˆë‹¤. ì¦‰, ëª¨ë¸ì€ ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ë°›ì•„ì„œ ì¶œë ¥ì´ ë˜ëŠ” ëª©í‘œ ì‹œí€€ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

ì´ˆê¸°ì˜ ë²ˆì—­ ëª¨ë¸ì€ ëŒ€ë¶€ë¶„ ë‹¨ì¼ ì–¸ì–´ë¡œ ì´ë£¨ì–´ì ¸ ìˆì—ˆì§€ë§Œ, ìµœê·¼ì—ëŠ” ë§ì€ ì–¸ì–´ ìŒ ê°„ì— ë²ˆì—­ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ë‹¤ì¤‘ ì–¸ì–´ ëª¨ë¸ì— ëŒ€í•œ ê´€ì‹¬ì´ ë†’ì•„ì§€ê³  ìˆìŠµë‹ˆë‹¤.

```py
>>> from transformers import pipeline

>>> text = "translate English to French: Hugging Face is a community-based open-source platform for machine learning."
>>> translator = pipeline(task="translation", model="t5-small")
>>> translator(text)
[{'translation_text': "Hugging Face est une tribune communautaire de l'apprentissage des machines."}]
```

### ì–¸ì–´ ëª¨ë¸ë§[[language_modeling]]

ì–¸ì–´ ëª¨ë¸ë§ì€ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ì—ì„œ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. ì‚¬ì „ í›ˆë ¨ëœ ì–¸ì–´ ëª¨ë¸ì€ ë§ì€ ë‹¤ë¥¸ í•˜ìœ„ ì‘ì—…ì— ë”°ë¼ ë¯¸ì„¸ ì¡°ì •ë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ë§¤ìš° ì¸ê¸° ìˆëŠ” ìì—°ì–´ì²˜ë¦¬ ì‘ì—…ì´ ë˜ì—ˆìŠµë‹ˆë‹¤. ìµœê·¼ì—ëŠ” ì œë¡œ ìƒ·(zero-shot) ë˜ëŠ” í“¨ ìƒ·(few-shot) í•™ìŠµì´ ê°€ëŠ¥í•œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(Large Language Models, LLM)ì— ëŒ€í•œ ë§ì€ ê´€ì‹¬ì´ ë°œìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ ëª…ì‹œì ìœ¼ë¡œ í›ˆë ¨ë˜ì§€ ì•Šì€ ì‘ì—…ë„ í•´ê²°í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤! ì–¸ì–´ ëª¨ë¸ì€ ìœ ì°½í•˜ê³  ì„¤ë“ë ¥ ìˆëŠ” í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©ë  ìˆ˜ ìˆì§€ë§Œ, í…ìŠ¤íŠ¸ê°€ í•­ìƒ ì •í™•í•˜ì§€ëŠ” ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.

ì–¸ì–´ ëª¨ë¸ë§ì—ëŠ” ë‘ ê°€ì§€ ìœ í˜•ì´ ìˆìŠµë‹ˆë‹¤:

* ì¸ê³¼ì  ì–¸ì–´ ëª¨ë¸ë§: ì´ ëª¨ë¸ì˜ ëª©ì ì€ ì‹œí€€ìŠ¤ì—ì„œ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ë©°, ë¯¸ë˜ í† í°ì´ ë§ˆìŠ¤í‚¹ ë©ë‹ˆë‹¤.
    ```py
    >>> from transformers import pipeline

    >>> prompt = "Hugging Face is a community-based open-source platform for machine learning."
    >>> generator = pipeline(task="text-generation")
    >>> generator(prompt)  # doctest: +SKIP
    ```

* ë§ˆìŠ¤í‚¹ëœ ì–¸ì–´ ëª¨ë¸ë§: ì´ ëª¨ë¸ì˜ ëª©ì ì€ ì‹œí€€ìŠ¤ ë‚´ì˜ ë§ˆìŠ¤í‚¹ëœ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ë©°, ì‹œí€€ìŠ¤ ë‚´ì˜ ëª¨ë“  í† í°ì— ëŒ€í•œ ì ‘ê·¼ì´ ì œê³µë©ë‹ˆë‹¤.
    
    ```py
    >>> text = "Hugging Face is a community-based open-source <mask> for machine learning."
    >>> fill_mask = pipeline(task="fill-mask")
    >>> preds = fill_mask(text, top_k=1)
    >>> preds = [
    ...     {
    ...         "score": round(pred["score"], 4),
    ...         "token": pred["token"],
    ...         "token_str": pred["token_str"],
    ...         "sequence": pred["sequence"],
    ...     }
    ...     for pred in preds
    ... ]
    >>> preds
    [{'score': 0.2236,
      'token': 1761,
      'token_str': ' platform',
      'sequence': 'Hugging Face is a community-based open-source platform for machine learning.'}]
    ```

ì´ í˜ì´ì§€ë¥¼ í†µí•´ ê° ëª¨ë‹¬ë¦¬í‹°ì˜ ë‹¤ì–‘í•œ ì‘ì—… ìœ í˜•ê³¼ ê° ì‘ì—…ì˜ ì‹¤ìš©ì  ì¤‘ìš”ì„±ì— ëŒ€í•´ ì¶”ê°€ì ì¸ ë°°ê²½ ì •ë³´ë¥¼ ì–»ìœ¼ì…¨ê¸°ë¥¼ ë°”ëë‹ˆë‹¤. ë‹¤ìŒ [ì„¹ì…˜](tasks_explained)ì—ì„œëŠ” ğŸ¤— Transformerê°€ ì´ëŸ¬í•œ ì‘ì—…ì„ í•´ê²°í•˜ëŠ” **ë°©ë²•**ì— ëŒ€í•´ ì•Œì•„ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.