<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# ì „ì²˜ë¦¬[[preprocess]]

[[open-in-colab]]

ëª¨ë¸ì„ í•™ìŠµí•˜ë ¤ë©´ ë°ì´í„°ì…‹ì„ ëª¨ë¸ì— ë§ëŠ” ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ì „ì²˜ë¦¬ í•´ì•¼ í•©ë‹ˆë‹¤. ë°ì´í„°ê°€ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ ë˜ëŠ” ì˜¤ë””ì˜¤ì¸ì§€ ì—¬ë¶€ì— ê´€ê³„ì—†ì´ ë°ì´í„°ë¥¼ í…ì„œ ë°°ì¹˜ë¡œ ë³€í™˜í•˜ê³  ì¡°ë¦½í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤. ğŸ¤— TransformersëŠ” ëª¨ë¸ì— ëŒ€í•œ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ì¼ë ¨ì˜ ì „ì²˜ë¦¬ í´ë˜ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” ë‹¤ìŒ ë‚´ìš©ì„ ë°°ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

* í…ìŠ¤íŠ¸ëŠ” [Tokenizer](./main_classes/tokenizer)ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ í† í° ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•˜ê³  í† í°ì˜ ìˆ«ì í‘œí˜„ì„ ë§Œë“  í›„ í…ì„œë¡œ ì¡°ë¦½í•©ë‹ˆë‹¤.
* ìŒì„± ë° ì˜¤ë””ì˜¤ëŠ” [Feature extractor](./main_classes/feature_extractor)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ íŒŒí˜•ì—ì„œ ì‹œí€€ìŠ¤ íŠ¹ì§•ì„ íŒŒì•…í•˜ì—¬ í…ì„œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
* ì´ë¯¸ì§€ ì…ë ¥ì€ [ImageProcessor](./main_classes/image)ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
* ë©€í‹°ëª¨ë‹¬ ì…ë ¥ì€ [Processor](./main_classes/processors)ì„ ì‚¬ìš©í•˜ì—¬ í† í¬ë‚˜ì´ì €ì™€ íŠ¹ì§• ì¶”ì¶œê¸° ë˜ëŠ” ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¥¼ ê²°í•©í•©ë‹ˆë‹¤.

<Tip>

`AutoProcessor`ëŠ” **í•­ìƒ** ì‘ë™í•˜ë©° í† í¬ë‚˜ì´ì €, ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ, íŠ¹ì§• ì¶”ì¶œê¸° ë˜ëŠ” í”„ë¡œì„¸ì„œ ë“± ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ì— ë§ëŠ” í´ë˜ìŠ¤ë¥¼ ìë™ìœ¼ë¡œ ì„ íƒí•©ë‹ˆë‹¤.

</Tip>

ì‹œì‘í•˜ê¸° ì „ì— ğŸ¤— Datasetsë¥¼ ì„¤ì¹˜í•˜ì—¬ ì‹¤í—˜ì— ì‚¬ìš©í•  ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
pip install datasets
```

## ìì—°ì–´ì²˜ë¦¬[[natural-language-processing]]

<Youtube id="Yffk5aydLzg"/>

í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ê¸°ë³¸ ë„êµ¬ëŠ” [tokenizer](main_classes/tokenizer)ì…ë‹ˆë‹¤. í† í¬ë‚˜ì´ì €ëŠ” ì¼ë ¨ì˜ ê·œì¹™ì— ë”°ë¼ í…ìŠ¤íŠ¸ë¥¼ *í† í°*ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤. í† í°ì€ ìˆ«ìë¡œ ë³€í™˜ë˜ê³  í…ì„œëŠ” ëª¨ë¸ ì…ë ¥ì´ ë©ë‹ˆë‹¤. ëª¨ë¸ì— í•„ìš”í•œ ì¶”ê°€ ì…ë ¥ì€ í† í¬ë‚˜ì´ì €ì— ì˜í•´ ì¶”ê°€ë©ë‹ˆë‹¤.

<Tip>

ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ì‚¬ìš©í•  ê³„íšì´ë¼ë©´ ëª¨ë¸ê³¼ í•¨ê»˜ ì‚¬ì „ í›ˆë ¨ëœ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ í…ìŠ¤íŠ¸ê°€ ì‚¬ì „ í›ˆë ¨ ë§ë­‰ì¹˜ì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ë¶„í• ë˜ê³  ì‚¬ì „ í›ˆë ¨ ì¤‘ì— ë™ì¼í•œ í•´ë‹¹ í† í°-ì¸ë±ìŠ¤ ìŒ(ì¼ë°˜ì ìœ¼ë¡œ *vocab*ì´ë¼ê³  í•¨)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

</Tip>

ì‹œì‘í•˜ë ¤ë©´ [`AutoTokenizer.from_pretrained`] ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ì „ í›ˆë ¨ëœ í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜¤ì„¸ìš”. ëª¨ë¸ê³¼ í•¨ê»˜ ì‚¬ì „ í›ˆë ¨ëœ *vocab*ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤:

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

ê·¸ ë‹¤ìŒìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ í† í¬ë‚˜ì´ì €ì— ë„£ì–´ì£¼ì„¸ìš”:

```py
>>> encoded_input = tokenizer("Do not meddle in the affairs of wizards, for they are subtle and quick to anger.")
>>> print(encoded_input)
{'input_ids': [101, 2079, 2025, 19960, 10362, 1999, 1996, 3821, 1997, 16657, 1010, 2005, 2027, 2024, 11259, 1998, 4248, 2000, 4963, 1012, 102], 
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

í† í¬ë‚˜ì´ì €ëŠ” ì„¸ ê°€ì§€ ì¤‘ìš”í•œ í•­ëª©ì„ í¬í•¨í•œ ì‚¬ì „ì„ ë°˜í™˜í•©ë‹ˆë‹¤:

* [input_ids](glossary#input-ids)ëŠ” ë¬¸ì¥ì˜ ê° í† í°ì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ì…ë‹ˆë‹¤.
* [attention_mask](glossary#attention-mask)ëŠ” í† í°ì„ ì²˜ë¦¬í•´ì•¼ í•˜ëŠ”ì§€ ì—¬ë¶€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
* [token_type_ids](glossary#token-type-ids)ëŠ” ë‘ ê°œ ì´ìƒì˜ ì‹œí€€ìŠ¤ê°€ ìˆì„ ë•Œ í† í°ì´ ì†í•œ ì‹œí€€ìŠ¤ë¥¼ ì‹ë³„í•©ë‹ˆë‹¤.

`input_ids`ë¥¼ ë””ì½”ë”©í•˜ì—¬ ì…ë ¥ì„ ë°˜í™˜í•©ë‹ˆë‹¤:

```py
>>> tokenizer.decode(encoded_input["input_ids"])
'[CLS] Do not meddle in the affairs of wizards, for they are subtle and quick to anger. [SEP]'
```

í† í¬ë‚˜ì´ì €ê°€ ë‘ ê°œì˜ íŠ¹ìˆ˜í•œ í† í°(ë¶„ë¥˜ í† í° CLSì™€ êµ¬ë¶„ í† í° SEP)ì„ ë¬¸ì¥ì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.
ëª¨ë“  ëª¨ë¸ì— íŠ¹ìˆ˜í•œ í† í°ì´ í•„ìš”í•œ ê²ƒì€ ì•„ë‹ˆì§€ë§Œ, í•„ìš”í•œ ê²½ìš° í† í¬ë‚˜ì´ì €ê°€ ìë™ìœ¼ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.

ì „ì²˜ë¦¬í•  ë¬¸ì¥ì´ ì—¬ëŸ¬ ê°œ ìˆëŠ” ê²½ìš° ì´ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ í† í¬ë‚˜ì´ì €ì— ì „ë‹¬í•©ë‹ˆë‹¤:

```py
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_inputs = tokenizer(batch_sentences)
>>> print(encoded_inputs)
{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102], 
               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102], 
               [101, 1327, 1164, 5450, 23434, 136, 102]], 
 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], 
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
                    [0, 0, 0, 0, 0, 0, 0]], 
 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], 
                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 
                    [1, 1, 1, 1, 1, 1, 1]]}
```

### íŒ¨ë”©[[pad]]

ëª¨ë¸ ì…ë ¥ì¸ í…ì„œëŠ” ê· ì¼í•œ ëª¨ì–‘ì„ ê°€ì ¸ì•¼ í•˜ëŠ”ë°, ë¬¸ì¥ì˜ ê¸¸ì´ê°€ í•­ìƒ ê°™ì§€ ì•Šì•„ì„œ ë¬¸ì œê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŒ¨ë”©ì€ ì§§ì€ ë¬¸ì¥ì— íŠ¹ìˆ˜í•œ *íŒ¨ë”© í† í°*ì„ ì¶”ê°€í•˜ì—¬ í…ì„œê°€ ì§ì‚¬ê°í˜• ëª¨ì–‘ì´ ë˜ë„ë¡ í•˜ëŠ” ì „ëµì…ë‹ˆë‹¤.

`padding` ë§¤ê°œë³€ìˆ˜ë¥¼ `True`ë¡œ ì„¤ì •í•˜ì—¬ ë°°ì¹˜ì˜ ì§§ì€ ì‹œí€€ìŠ¤ë¥¼ ê°€ì¥ ê¸´ ì‹œí€€ìŠ¤ì™€ ì¼ì¹˜í•˜ë„ë¡ íŒ¨ë”©í•©ë‹ˆë‹¤.

```py
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_input = tokenizer(batch_sentences, padding=True)
>>> print(encoded_input)
{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0], 
               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102], 
               [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]], 
 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 
 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], 
                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 
                    [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}
```

ê¸¸ì´ê°€ ì§§ì€ ì²« ë¬¸ì¥ê³¼ ì„¸ ë²ˆì§¸ ë¬¸ì¥ì€ ì´ì œ `0`ìœ¼ë¡œ ì±„ì›Œì§‘ë‹ˆë‹¤.

### ìƒëµ[[truncation]]

í•œí¸, ë•Œë¡œëŠ” ì‹œí€€ìŠ¤ê°€ ëª¨ë¸ì—ì„œ ì²˜ë¦¬í•˜ê¸°ì— ë„ˆë¬´ ê¸¸ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì´ ê²½ìš°, ì‹œí€€ìŠ¤ë¥¼ ë” ì§§ì€ ê¸¸ì´ë¡œ ì¤„ì¼ í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤.

ëª¨ë¸ì—ì„œ í—ˆìš©í•˜ëŠ” ìµœëŒ€ ê¸¸ì´ë¡œ ì‹œí€€ìŠ¤ë¥¼ ìë¥´ë ¤ë©´ `truncation` ë§¤ê°œë³€ìˆ˜ë¥¼ `True`ë¡œ ì„¤ì •í•˜ì„¸ìš”:

```py
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True)
>>> print(encoded_input)
{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0], 
               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102], 
               [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]], 
 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 
 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], 
                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 
                    [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}
```

<Tip>

ë‹¤ì–‘í•œ íŒ¨ë”© ë° ìƒëµ ì¸ìˆ˜ì— ëŒ€í•´ ë” ì•Œì•„ë³´ë ¤ë©´ [Padding and truncation](./pad_truncation) ê°œë… ê°€ì´ë“œë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.

</Tip>

### í…ì„œ ë§Œë“¤ê¸°[[build-tensors]]

ë§ˆì§€ë§‰ìœ¼ë¡œ, í† í¬ë‚˜ì´ì €ê°€ ëª¨ë¸ì— ê³µê¸‰ë˜ëŠ” ì‹¤ì œ í…ì„œë¥¼ ë°˜í™˜í•˜ë„ë¡ í•©ë‹ˆë‹¤.

`return_tensors` ë§¤ê°œë³€ìˆ˜ë¥¼ PyTorchì˜ ê²½ìš° `pt`, TensorFlowì˜ ê²½ìš° `tf`ë¡œ ì„¤ì •í•˜ì„¸ìš”:

<frameworkcontent>
<pt>

```py
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors="pt")
>>> print(encoded_input)
{'input_ids': tensor([[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],
                      [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],
                      [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]]), 
 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 
 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
                           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                           [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}
```
</pt>
<tf>
```py
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors="tf")
>>> print(encoded_input)
{'input_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=
array([[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],
       [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],
       [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],
      dtype=int32)>, 
 'token_type_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=
array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 
 'attention_mask': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=
array([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}
```
</tf>
</frameworkcontent>

## ì˜¤ë””ì˜¤[[audio]]

ì˜¤ë””ì˜¤ ì‘ì—…ì—ëŠ” ë°ì´í„°ì…‹ì„ ëª¨ë¸ì— ì¤€ë¹„í•˜ê¸° ìœ„í•´ [íŠ¹ì§• ì¶”ì¶œê¸°](main_classes/feature_extractor)ê°€ í•„ìš”í•©ë‹ˆë‹¤. íŠ¹ì§• ì¶”ì¶œê¸°ëŠ” ì›ì‹œ ì˜¤ë””ì˜¤ ë°ì´í„°ì—ì„œ í”¼ì²˜ë¥¼ ì¶”ì¶œí•˜ê³  ì´ë¥¼ í…ì„œë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì´ ëª©ì ì…ë‹ˆë‹¤.

ì˜¤ë””ì˜¤ ë°ì´í„°ì…‹ì— íŠ¹ì§• ì¶”ì¶œê¸°ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ë³´ë ¤ë©´ [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) ë°ì´í„°ì…‹ì„ ê°€ì ¸ì˜¤ì„¸ìš”. (ë°ì´í„°ì…‹ì„ ê°€ì ¸ì˜¤ëŠ” ë°©ë²•ì€ ğŸ¤— [Datasets tutorial](https://huggingface.co/docs/datasets/load_hub.html)ì—ì„œ ìì„¸íˆ ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤).

```py
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")
```

`audio` ì—´ì˜ ì²« ë²ˆì§¸ ìš”ì†Œì— ì•¡ì„¸ìŠ¤í•˜ì—¬ ì…ë ¥ì„ ì‚´í´ë³´ì„¸ìš”. `audio` ì—´ì„ í˜¸ì¶œí•˜ë©´ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ìë™ìœ¼ë¡œ ê°€ì ¸ì˜¤ê³  ë¦¬ìƒ˜í”Œë§í•©ë‹ˆë‹¤:

```py
>>> dataset[0]["audio"]
{'array': array([ 0.        ,  0.00024414, -0.00024414, ..., -0.00024414,
         0.        ,  0.        ], dtype=float32),
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',
 'sampling_rate': 8000}
```

ì´ë ‡ê²Œ í•˜ë©´ ì„¸ ê°€ì§€ í•­ëª©ì´ ë°˜í™˜ë©ë‹ˆë‹¤:

* `array`ëŠ” 1D ë°°ì—´ë¡œ ê°€ì ¸ì™€ì„œ (í•„ìš”í•œ ê²½ìš°) ë¦¬ìƒ˜í”Œë§ëœ ìŒì„± ì‹ í˜¸ì…ë‹ˆë‹¤.
* `path`ëŠ” ì˜¤ë””ì˜¤ íŒŒì¼ì˜ ìœ„ì¹˜ë¥¼ ê°€ë¦¬í‚µë‹ˆë‹¤.
* `sampling_rate`ëŠ” ìŒì„± ì‹ í˜¸ì—ì„œ ì´ˆë‹¹ ì¸¡ì •ë˜ëŠ” ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base) ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ëª¨ë¸ ì¹´ë“œë¥¼ ë³´ë©´ Wav2Vec2ê°€ 16kHz ìƒ˜í”Œë§ëœ ìŒì„± ì˜¤ë””ì˜¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ì „ í•™ìŠµëœ ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
ëª¨ë¸ì„ ì‚¬ì „ í•™ìŠµí•˜ëŠ” ë° ì‚¬ìš©ëœ ë°ì´í„°ì…‹ì˜ ìƒ˜í”Œë§ ì†ë„ì™€ ì˜¤ë””ì˜¤ ë°ì´í„°ì˜ ìƒ˜í”Œë§ ì†ë„ê°€ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤. ë°ì´í„°ì˜ ìƒ˜í”Œë§ ì†ë„ê°€ ë‹¤ë¥´ë©´ ë°ì´í„°ë¥¼ ë¦¬ìƒ˜í”Œë§í•´ì•¼ í•©ë‹ˆë‹¤.

1. ğŸ¤— Datasetsì˜ [`~datasets.Dataset.cast_column`] ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒ˜í”Œë§ ì†ë„ë¥¼ 16kHzë¡œ ì—…ìƒ˜í”Œë§í•˜ì„¸ìš”:

```py
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=16_000))
```

2. ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë¦¬ìƒ˜í”Œë§í•˜ê¸° ìœ„í•´ `audio` ì—´ì„ ë‹¤ì‹œ í˜¸ì¶œí•©ë‹ˆë‹¤:

```py
>>> dataset[0]["audio"]
{'array': array([ 2.3443763e-05,  2.1729663e-04,  2.2145823e-04, ...,
         3.8356509e-05, -7.3497440e-06, -2.1754686e-05], dtype=float32),
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',
 'sampling_rate': 16000}
```

ë‹¤ìŒìœ¼ë¡œ, ì…ë ¥ì„ ì •ê·œí™”í•˜ê³  íŒ¨ë”©í•˜ëŠ” íŠ¹ì§• ì¶”ì¶œê¸°ë¥¼ ê°€ì ¸ì˜¤ì„¸ìš”. í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ê²½ìš°, ë” ì§§ì€ ì‹œí€€ìŠ¤ì— ëŒ€í•´ `0`ì´ ì¶”ê°€ë©ë‹ˆë‹¤. ì˜¤ë””ì˜¤ ë°ì´í„°ì—ë„ ê°™ì€ ê°œë…ì´ ì ìš©ë©ë‹ˆë‹¤. 
íŠ¹ì§• ì¶”ì¶œê¸°ëŠ” ë°°ì—´ì— ëŒ€í•´ `0`(ë¬µìŒìœ¼ë¡œ í•´ì„)ì„ ì¶”ê°€í•©ë‹ˆë‹¤.

[`AutoFeatureExtractor.from_pretrained`]ë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì§• ì¶”ì¶œê¸°ë¥¼ ê°€ì ¸ì˜¤ì„¸ìš”:

```py
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base")
```

ì˜¤ë””ì˜¤ `array`ë¥¼ íŠ¹ì§• ì¶”ì¶œê¸°ì— ì „ë‹¬í•˜ì„¸ìš”. ë˜í•œ, íŠ¹ì§• ì¶”ì¶œê¸°ì— `sampling_rate` ì¸ìë¥¼ ì¶”ê°€í•˜ì—¬ ë°œìƒí•  ìˆ˜ ìˆëŠ” silent errors(ì¦‰ê°ì ì¸ ì˜¤ë¥˜ ë©”ì‹œì§€ê°€ ë°œìƒí•˜ì§€ ì•ŠëŠ” ì˜¤ë¥˜)ë¥¼ ë” ì˜ ë””ë²„ê¹…í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.

```py
>>> audio_input = [dataset[0]["audio"]["array"]]
>>> feature_extractor(audio_input, sampling_rate=16000)
{'input_values': [array([ 3.8106556e-04,  2.7506407e-03,  2.8015103e-03, ...,
        5.6335266e-04,  4.6588284e-06, -1.7142107e-04], dtype=float32)]}
```

í† í¬ë‚˜ì´ì €ì™€ ë§ˆì°¬ê°€ì§€ë¡œ ë°°ì¹˜ ë‚´ì—ì„œ ê°€ë³€ì ì¸ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ íŒ¨ë”© ë˜ëŠ” ìƒëµì„ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë‘ ê°œì˜ ì˜¤ë””ì˜¤ ìƒ˜í”Œì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”:

```py
>>> dataset[0]["audio"]["array"].shape
(173398,)

>>> dataset[1]["audio"]["array"].shape
(106496,)
```

ì˜¤ë””ì˜¤ ìƒ˜í”Œì˜ ê¸¸ì´ê°€ ë™ì¼í•˜ë„ë¡ ë°ì´í„°ì…‹ì„ ì „ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ë³´ì„¸ìš”. ìµœëŒ€ ìƒ˜í”Œ ê¸¸ì´ë¥¼ ì§€ì •í•˜ë©´, íŠ¹ì§• ì¶”ì¶œê¸°ê°€ í•´ë‹¹ ê¸¸ì´ì— ë§ì¶° ì‹œí€€ìŠ¤ë¥¼ íŒ¨ë”©í•˜ê±°ë‚˜ ìƒëµí•©ë‹ˆë‹¤:

```py
>>> def preprocess_function(examples):
...     audio_arrays = [x["array"] for x in examples["audio"]]
...     inputs = feature_extractor(
...         audio_arrays,
...         sampling_rate=16000,
...         padding=True,
...         max_length=100000,
...         truncation=True,
...     )
...     return inputs
```

`preprocess_function`ì„ ë°ì´í„°ì…‹ì˜ ì²˜ìŒ ëª‡ ê°€ì§€ ì˜ˆì œì— ì ìš©í•´ë³´ì„¸ìš”:

```py
>>> processed_dataset = preprocess_function(dataset[:5])
```

ì´ì œ ìƒ˜í”Œ ê¸¸ì´ê°€ ëª¨ë‘ ê°™ê³  ì§€ì •ëœ ìµœëŒ€ ê¸¸ì´ì— ë§ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ë“œë””ì–´ ì „ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ì„ ëª¨ë¸ì— ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

```py
>>> processed_dataset["input_values"][0].shape
(100000,)

>>> processed_dataset["input_values"][1].shape
(100000,)
```

## ì»´í“¨í„°ë¹„ì „[[computer-vision]]

For computer vision tasks, you'll need an [image processor](main_classes/image_processor) to prepare your dataset for the model.
Image preprocessing consists of several steps that convert images into the input expected by the model. These steps
include but are not limited to resizing, normalizing, color channel correction, and converting images to tensors.

<Tip>

Image preprocessing often follows some form of image augmentation. Both image preprocessing and image augmentation
transform image data, but they serve different purposes:

* Image augmentation alters images in a way that can help prevent overfitting and increase the robustness of the model. You can get creative in how you augment your data - adjust brightness and colors, crop, rotate, resize, zoom, etc. However, be mindful not to change the meaning of the images with your augmentations.
* Image preprocessing guarantees that the images match the modelâ€™s expected input format. When fine-tuning a computer vision model, images must be preprocessed exactly as when the model was initially trained.

You can use any library you like for image augmentation. For image preprocessing, use the `ImageProcessor` associated with the model.

</Tip>

Load the [food101](https://huggingface.co/datasets/food101) dataset (see the ğŸ¤— [Datasets tutorial](https://huggingface.co/docs/datasets/load_hub.html) for more details on how to load a dataset) to see how you can use an image processor with computer vision datasets:

<Tip>

Use ğŸ¤— Datasets `split` parameter to only load a small sample from the training split since the dataset is quite large!

</Tip>

```py
>>> from datasets import load_dataset

>>> dataset = load_dataset("food101", split="train[:100]")
```

Next, take a look at the image with ğŸ¤— Datasets [`Image`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=image#datasets.Image) feature:

```py
>>> dataset[0]["image"]
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/vision-preprocess-tutorial.png"/>
</div>

Load the image processor with [`AutoImageProcessor.from_pretrained`]:

```py
>>> from transformers import AutoImageProcessor

>>> image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
```

First, let's add some image augmentation. You can use any library you prefer, but in this tutorial, we'll use torchvision's [`transforms`](https://pytorch.org/vision/stable/transforms.html) module. If you're interested in using another data augmentation library, learn how in the [Albumentations](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb) or [Kornia notebooks](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_kornia.ipynb).

1. Here we use [`Compose`](https://pytorch.org/vision/master/generated/torchvision.transforms.Compose.html) to chain together a couple of
transforms - [`RandomResizedCrop`](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html) and [`ColorJitter`](https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html).
Note that for resizing, we can get the image size requirements from the `image_processor`. For some models, an exact height and
width are expected, for others only the `shortest_edge` is defined.

```py
>>> from torchvision.transforms import RandomResizedCrop, ColorJitter, Compose

>>> size = (
...     image_processor.size["shortest_edge"]
...     if "shortest_edge" in image_processor.size
...     else (image_processor.size["height"], image_processor.size["width"])
... )

>>> _transforms = Compose([RandomResizedCrop(size), ColorJitter(brightness=0.5, hue=0.5)])
```

2. The model accepts [`pixel_values`](model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderModel.forward.pixel_values)
as its input. `ImageProcessor` can take care of normalizing the images, and generating appropriate tensors.
Create a function that combines image augmentation and image preprocessing for a batch of images and generates `pixel_values`:

```py
>>> def transforms(examples):
...     images = [_transforms(img.convert("RGB")) for img in examples["image"]]
...     examples["pixel_values"] = image_processor(images, do_resize=False, return_tensors="pt")["pixel_values"]
...     return examples
```

<Tip>

In the example above we set `do_resize=False` because we have already resized the images in the image augmentation transformation,
and leveraged the `size` attribute from the appropriate `image_processor`. If you do not resize images during image augmentation,
leave this parameter out. By default, `ImageProcessor` will handle the resizing.

If you wish to normalize images as a part of the augmentation transformation, use the `image_processor.image_mean`,
and `image_processor.image_std` values.
</Tip>

3. Then use ğŸ¤— Datasets [`set_transform`](https://huggingface.co/docs/datasets/process.html#format-transform) to apply the transforms on the fly:

```py
>>> dataset.set_transform(transforms)
```

4. Now when you access the image, you'll notice the image processor has added `pixel_values`. You can pass your processed dataset to the model now!

```py
>>> dataset[0].keys()
```

Here is what the image looks like after the transforms are applied. The image has been randomly cropped and it's color properties are different.

```py
>>> import numpy as np
>>> import matplotlib.pyplot as plt

>>> img = dataset[0]["pixel_values"]
>>> plt.imshow(img.permute(1, 2, 0))
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/preprocessed_image.png"/>
</div>

<Tip>

For tasks like object detection, semantic segmentation, instance segmentation, and panoptic segmentation, `ImageProcessor`
offers post processing methods. These methods convert model's raw outputs into meaningful predictions such as bounding boxes,
or segmentation maps.

</Tip>

### íŒ¨ë“œ[[pad]]

In some cases, for instance, when fine-tuning [DETR](./model_doc/detr), the model applies scale augmentation at training
time. This may cause images to be different sizes in a batch. You can use [`DetrImageProcessor.pad_and_create_pixel_mask`]
from [`DetrImageProcessor`] and define a custom `collate_fn` to batch images together.

```py
>>> def collate_fn(batch):
...     pixel_values = [item["pixel_values"] for item in batch]
...     encoding = image_processor.pad_and_create_pixel_mask(pixel_values, return_tensors="pt")
...     labels = [item["labels"] for item in batch]
...     batch = {}
...     batch["pixel_values"] = encoding["pixel_values"]
...     batch["pixel_mask"] = encoding["pixel_mask"]
...     batch["labels"] = labels
...     return batch
```

## ë©€í‹°ëª¨ë‹¬[[multimodal]]

For tasks involving multimodal inputs, you'll need a [processor](main_classes/processors) to prepare your dataset for the model. A processor couples together two processing objects such as as tokenizer and feature extractor.

Load the [LJ Speech](https://huggingface.co/datasets/lj_speech) dataset (see the ğŸ¤— [Datasets tutorial](https://huggingface.co/docs/datasets/load_hub.html) for more details on how to load a dataset) to see how you can use a processor for automatic speech recognition (ASR):

```py
>>> from datasets import load_dataset

>>> lj_speech = load_dataset("lj_speech", split="train")
```

For ASR, you're mainly focused on `audio` and `text` so you can remove the other columns:

```py
>>> lj_speech = lj_speech.map(remove_columns=["file", "id", "normalized_text"])
```

Now take a look at the `audio` and `text` columns:

```py
>>> lj_speech[0]["audio"]
{'array': array([-7.3242188e-04, -7.6293945e-04, -6.4086914e-04, ...,
         7.3242188e-04,  2.1362305e-04,  6.1035156e-05], dtype=float32),
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/917ece08c95cf0c4115e45294e3cd0dee724a1165b7fc11798369308a465bd26/LJSpeech-1.1/wavs/LJ001-0001.wav',
 'sampling_rate': 22050}

>>> lj_speech[0]["text"]
'Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition'
```

Remember you should always [resample](preprocessing#audio) your audio dataset's sampling rate to match the sampling rate of the dataset used to pretrain a model!

```py
>>> lj_speech = lj_speech.cast_column("audio", Audio(sampling_rate=16_000))
```

Load a processor with [`AutoProcessor.from_pretrained`]:

```py
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")
```

1. Create a function to process the audio data contained in `array` to `input_values`, and tokenize `text` to `labels`. These are the inputs to the model:

```py
>>> def prepare_dataset(example):
...     audio = example["audio"]

...     example.update(processor(audio=audio["array"], text=example["text"], sampling_rate=16000))

...     return example
```

2. Apply the `prepare_dataset` function to a sample:

```py
>>> prepare_dataset(lj_speech[0])
```

The processor has now added `input_values` and `labels`, and the sampling rate has also been correctly downsampled to 16kHz. You can pass your processed dataset to the model now!
