<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# ë‘˜ëŸ¬ë³´ê¸°[[quick-tour]]

[[open-in-colab]]
ğŸ¤— Transformerë¥¼ ì‹œì‘í•´ë´ìš”! ë‘˜ëŸ¬ë³´ê¸°ëŠ” ê°œë°œìì™€ ì¼ë°˜ ì‚¬ìš©ì ëª¨ë‘ë¥¼ ìœ„í•´ ì“°ì—¬ì¡ŒìŠµë‹ˆë‹¤. [`pipeline`]ìœ¼ë¡œ ì¶”ë¡ í•˜ëŠ” ë°©ë²•, [AutoClass](./model_doc/auto)ë¡œ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ê³¼ ì „ì²˜ë¦¬ê¸°ë¥¼ ì ì¬í•˜ëŠ” ë°©ë²•ê³¼ PyTorch ë˜ëŠ” TensorFlowë¡œ ì‹ ì†í•˜ê²Œ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ê¸°ë³¸ì„ ë°°ìš°ê³  ì‹¶ë‹¤ë©´ íŠœí† ë¦¬ì–¼ì´ë‚˜ [course](https://huggingface.co/course/chapter1/1)ì—ì„œ ì—¬ê¸° ì†Œê°œëœ ê°œë…ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì„ í™•ì¸í•˜ì‹œê¸¸ ê¶Œì¥í•©ë‹ˆë‹¤.

ì‹œì‘í•˜ê¸° ì „ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ëª¨ë‘ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³ ,

```bash
!pip install transformers datasets
```

ì¢‹ì•„í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ë„ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.

<frameworkcontent>
<pt>
```bash
pip install torch
```
</pt>
<tf>
```bash
pip install tensorflow
```
</tf>
</frameworkcontent>

## Pipeline (íŒŒì´í”„ë¼ì¸)

<Youtube id="tiZFewofSLM"/>

[`pipeline`]ì€ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•´ ì¶”ë¡ í•  ë•Œ ì œì¼ ì‰¬ìš´ ë°©ë²•ì…ë‹ˆë‹¤. ì—¬ëŸ¬ ëª¨ë‹¬ë¦¬í‹°ì˜ ìˆ˜ë§ì€ íƒœìŠ¤í¬ì— [`pipeline`]ì„ ì¦‰ì‹œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì§€ì›í•˜ëŠ” íƒœìŠ¤í¬ì˜ ì˜ˆì‹œëŠ” ì•„ë˜ í‘œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

| **íƒœìŠ¤í¬**     | **ì„¤ëª…**                                                            | **ëª¨ë‹¬ë¦¬í‹°**     | **íŒŒì´í”„ë¼ì¸ ID**                             |
|----------------|---------------------------------------------------------------------|------------------|-----------------------------------------------|
| í…ìŠ¤íŠ¸ ë¶„ë¥˜    | í…ìŠ¤íŠ¸ì— ì•Œë§ì€ ë¼ë²¨ ë¶™ì´ê¸°                                         | ìì—°ì–´ ì²˜ë¦¬(NLP) | pipeline(task="sentiment-analysis")           |
| í…ìŠ¤íŠ¸ ìƒì„±    | ì£¼ì–´ì§„ ë¬¸ìì—´ ì…ë ¥ê³¼ ì´ì–´ì§€ëŠ” í…ìŠ¤íŠ¸ ìƒì„±í•˜ê¸°                       | ìì—°ì–´ ì²˜ë¦¬(NLP) | pipeline(task="text-generation")              |
| ê°œì²´ëª… ì¸ì‹    | ë¬¸ìì—´ì˜ ê° í† í°ë§ˆë‹¤ ì•Œë§ì€ ë¼ë²¨ ë¶™ì´ê¸° (ì¸ë¬¼, ì¡°ì§, ì¥ì†Œ ë“±ë“±)     | ìì—°ì–´ ì²˜ë¦¬(NLP) | pipeline(task="ner")                          |
| ì§ˆì˜ì‘ë‹µ       | ì£¼ì–´ì§„ ë¬¸ë§¥ê³¼ ì§ˆë¬¸ì— ë”°ë¼ ì˜¬ë°”ë¥¸ ëŒ€ë‹µí•˜ê¸°                           | ìì—°ì–´ ì²˜ë¦¬(NLP) | pipeline(task="question-answering")           |
| ë¹ˆì¹¸ ì±„ìš°ê¸°    | ë¬¸ìì—´ì˜ ë¹ˆì¹¸ì— ì•Œë§ì€ í† í° ë§ì¶”ê¸°                                  | ìì—°ì–´ ì²˜ë¦¬(NLP) | pipeline(task="fill-mask")                    |
| ìš”ì•½           | í…ìŠ¤íŠ¸ë‚˜ ë¬¸ì„œë¥¼ ìš”ì•½í•˜ê¸°                                            | ìì—°ì–´ ì²˜ë¦¬(NLP) | pipeline(task="summarization")                |
| ë²ˆì—­           | í…ìŠ¤íŠ¸ë¥¼ í•œ ì–¸ì–´ì—ì„œ ë‹¤ë¥¸ ì–¸ì–´ë¡œ ë²ˆì—­í•˜ê¸°                           | ìì—°ì–´ ì²˜ë¦¬(NLP) | pipeline(task="translation")                  |
| ì´ë¯¸ì§€ ë¶„ë¥˜    | ì´ë¯¸ì§€ì— ì•Œë§ì€ ë¼ë²¨ ë¶™ì´ê¸°                                         | ì»´í“¨í„° ë¹„ì „(CV)  | pipeline(task="image-classification")         |
| ì´ë¯¸ì§€ ë¶„í•     | ì´ë¯¸ì§€ì˜ í”½ì…€ë§ˆë‹¤ ë¼ë²¨ ë¶™ì´ê¸°(ì‹œë§¨í‹±, íŒŒë†‰í‹± ë° ì¸ìŠ¤í„´ìŠ¤ ë¶„í•  í¬í•¨) | ì»´í“¨í„° ë¹„ì „(CV)  | pipeline(task="image-segmentation")           |
| ê°ì²´ íƒì§€      | ì´ë¯¸ì§€ ì† ê°ì²´ì˜ ê²½ê³„ ìƒìë¥¼ ê·¸ë¦¬ê³  í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•˜ê¸°               | ì»´í“¨í„° ë¹„ì „(CV)  | pipeline(task="object-detection")             |
| ì˜¤ë””ì˜¤ ë¶„ë¥˜    | ì˜¤ë””ì˜¤ íŒŒì¼ì— ì•Œë§ì€ ë¼ë²¨ ë¶™ì´ê¸°                                    | ì˜¤ë””ì˜¤           | pipeline(task="audio-classification")         |
| ìë™ ìŒì„± ì¸ì‹ | ì˜¤ë””ì˜¤ íŒŒì¼ ì† ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë°”ê¾¸ê¸°                               | ì˜¤ë””ì˜¤           | pipeline(task="automatic-speech-recognition") |
| ì‹œê° ì§ˆì˜ì‘ë‹µ  | ì£¼ì–´ì§„ ì´ë¯¸ì§€ì™€ ì´ë¯¸ì§€ì— ëŒ€í•œ ì§ˆë¬¸ì— ë”°ë¼ ì˜¬ë°”ë¥´ê²Œ ëŒ€ë‹µí•˜ê¸°         | ë©€í‹°ëª¨ë‹¬         | pipeline(task="vqa")                          |

ë¨¼ì € [`pipeline`]ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“¤ì–´ ì ìš©í•  íƒœìŠ¤í¬ë¥¼ ê³ ë¥´ì„¸ìš”. ìœ„ íƒœìŠ¤í¬ë“¤ì€ ëª¨ë‘ [`pipeline`]ì„ ì‚¬ìš©í•  ìˆ˜ ìˆê³ , ì§€ì›í•˜ëŠ” íƒœìŠ¤í¬ì˜ ì „ì²´ ëª©ë¡ì„ ë³´ë ¤ë©´ [pipeline API ë ˆí¼ëŸ°ìŠ¤](./main_classes/pipelines)ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”. ê°„ë‹¨í•œ ì˜ˆì‹œë¡œ ê°ì • ë¶„ì„ íƒœìŠ¤í¬ì— [`pipeline`]ë¥¼ ì ìš©í•´ ë³´ê² ìŠµë‹ˆë‹¤.

```py
>>> from transformers import pipeline

>>> classifier = pipeline("sentiment-analysis")
```

[`pipeline`]ì€ ê¸°ë³¸ [ì‚¬ì „í•™ìŠµëœ ëª¨ë¸(ì˜ì–´)](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)ì™€ ê°ì • ë¶„ì„ì„ í•˜ê¸° ìœ„í•œ tokenizerë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ìºì‹œí•´ë†“ìŠµë‹ˆë‹¤. ì´ì œ ì›í•˜ëŠ” í…ìŠ¤íŠ¸ì— `classifier`ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```py
>>> classifier("We are very happy to show you the ğŸ¤— Transformers library.")
[{'label': 'POSITIVE', 'score': 0.9998}]
```

ì…ë ¥ì´ ì—¬ëŸ¬ ê°œë¼ë©´, ì…ë ¥ì„ [`pipeline`]ì— ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬í•´ì„œ ë”•ì…”ë„ˆë¦¬ë¡œ ëœ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```py
>>> results = classifier(["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."])
>>> for result in results:
...     print(f"label: {result['label']}, with score: {round(result['score'], 4)}")
label: POSITIVE, with score: 0.9998
label: NEGATIVE, with score: 0.5309
```

[`pipeline`]ì€ íŠ¹ì • íƒœìŠ¤í¬ìš© ë°ì´í„°ì…‹ë¥¼ ì „ë¶€ ìˆœíšŒí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ìë™ ìŒì„± ì¸ì‹ íƒœìŠ¤í¬ì— ì ìš©í•´ ë³´ê² ìŠµë‹ˆë‹¤.

```py
>>> import torch
>>> from transformers import pipeline

>>> speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h")
```

ì´ì œ ìˆœíšŒí•  ì˜¤ë””ì˜¤ ë°ì´í„°ì…‹ë¥¼ ì ì¬í•˜ê² ìŠµë‹ˆë‹¤. (ìì„¸í•œ ë‚´ìš©ì€ ğŸ¤— Datasets [ì‹œì‘í•˜ê¸°](https://huggingface.co/docs/datasets/quickstart#audio)ë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”) [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) ë°ì´í„°ì…‹ë¡œ í•´ë³¼ê¹Œìš”?

```py
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")  # doctest: +IGNORE_RESULT
```

ë°ì´í„°ì…‹ì˜ ìƒ˜í”Œë§ ë ˆì´íŠ¸ê°€ [`facebook/wav2vec2-base-960h`](https://huggingface.co/facebook/wav2vec2-base-960h)ì˜ í›ˆë ¨ ë‹¹ì‹œ ìƒ˜í”Œë§ ë ˆì´íŠ¸ì™€ ì¼ì¹˜í•´ì•¼ë§Œ í•©ë‹ˆë‹¤.

```py
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))
```

ì˜¤ë””ì˜¤ íŒŒì¼ì€ `"audio"` ì—´ì„ í˜¸ì¶œí•  ë•Œ ìë™ìœ¼ë¡œ ì ì¬ë˜ê³  ë‹¤ì‹œ ìƒ˜í”Œë§ë©ë‹ˆë‹¤.
ì²˜ìŒ 4ê°œ ìƒ˜í”Œì—ì„œ ìŒì„±ì„ ì¶”ì¶œí•˜ì—¬ íŒŒì´í”„ë¼ì¸ì— ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì „ë‹¬í•´ë³´ê² ìŠµë‹ˆë‹¤.

```py
>>> result = speech_recognizer(dataset[:4]["audio"])
>>> print([d["text"] for d in result])
['I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT', "FODING HOW I'D SET UP A JOIN TO HET WITH MY WIFE AND WHERE THE AP MIGHT BE", "I I'D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I'M NOT SEEING THE OPTION TO DO IT ON THE AP SO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AND I'M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS", 'HOW DO I THURN A JOIN A COUNT']
```

(ìŒì„±ì´ë‚˜ ë¹„ì „ì²˜ëŸ¼) ì…ë ¥ì´ í° ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì˜ ê²½ìš°, ë©”ëª¨ë¦¬ì— ì ì¬ì‹œí‚¤ê¸° ìœ„í•´ ë¦¬ìŠ¤íŠ¸ ëŒ€ì‹  ì œë„ˆë ˆì´í„°ë¡œ ì…ë ¥ì„ ëª¨ë‘ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [pipeline API ë ˆí¼ëŸ°ìŠ¤](./main_classes/pipelines)ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.

### íŒŒì´í”„ë¼ì¸ì—ì„œ ë‹¤ë¥¸ ëª¨ë¸ì´ë‚˜ tokenizer ì‚¬ìš©í•˜ëŠ” ë°©ë²•[[use-another-model-and-tokenizer-in-the-pipeline]]

[`pipeline`]ì€ [Hub](https://huggingface.co/models) ì† ëª¨ë“  ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆì–´, ì–¼ë§ˆë“ ì§€ [`pipeline`]ì„ ì‚¬ìš©í•˜ê³  ì‹¶ì€ëŒ€ë¡œ ë°”ê¿€ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ í”„ë‘ìŠ¤ì–´ í…ìŠ¤íŠ¸ë¥¼ ë‹¤ë£° ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ ë§Œë“œë ¤ë©´, Hubì˜ íƒœê·¸ë¡œ ì ì ˆí•œ ëª¨ë¸ì„ ì°¾ì•„ë³´ì„¸ìš”. ìƒìœ„ ê²€ìƒ‰ ê²°ê³¼ë¡œ ëœ¬ ê°ì • ë¶„ì„ì„ ìœ„í•´ íŒŒì¸íŠœë‹ëœ ë‹¤êµ­ì–´ [BERT ëª¨ë¸](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment)ì´ í”„ë‘ìŠ¤ì–´ë¥¼ ì§€ì›í•˜ëŠ”êµ°ìš”.

```py
>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
```

<frameworkcontent>
<pt>
[`AutoModelForSequenceClassification`]ê³¼ [`AutoTokenizer`]ë¡œ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ê³¼ í•¨ê»˜ ì—°ê´€ëœ í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. (`AutoClass`ì— ëŒ€í•œ ë‚´ìš©ì€ ë‹¤ìŒ ì„¹ì…˜ì—ì„œ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤)

```py
>>> from transformers import AutoTokenizer, AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained(model_name)
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```
</pt>
<tf>
[`TFAutoModelForSequenceClassification`]ê³¼ [`AutoTokenizer`]ë¡œ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ê³¼ í•¨ê»˜ ì—°ê´€ëœ í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. (`TFAutoClass`ì— ëŒ€í•œ ë‚´ìš©ì€ ë‹¤ìŒ ì„¹ì…˜ì—ì„œ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤)

```py
>>> from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

>>> model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```
</tf>
</frameworkcontent>

[`pipeline`]ì—ì„œ ì‚¬ìš©í•  ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì…ë ¥í•˜ë©´ ì´ì œ (ê°ì • ë¶„ì„ê¸°ì¸) `classifier`ë¥¼ í”„ë‘ìŠ¤ì–´ í…ìŠ¤íŠ¸ì— ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```py
>>> classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
>>> classifier("Nous sommes trÃ¨s heureux de vous prÃ©senter la bibliothÃ¨que ğŸ¤— Transformers.")
[{'label': '5 stars', 'score': 0.7273}]
```

í•˜ê³ ì‹¶ì€ ê²ƒì— ì ìš©í•  ë§ˆë•…í•œ ëª¨ë¸ì´ ì—†ë‹¤ë©´, ê°€ì§„ ë°ì´í„°ë¡œ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•´ì•¼ í•©ë‹ˆë‹¤. ìì„¸í•œ ë°©ë²•ì€ [íŒŒì¸íŠœë‹ íŠœí† ë¦¬ì–¼](./training)ì„ ì°¸ê³ í•´ì£¼ì„¸ìš”. ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì˜ íŒŒì¸íŠœë‹ì„ ë§ˆì¹˜ì…¨ìœ¼ë©´, ëˆ„êµ¬ë‚˜ ë¨¸ì‹ ëŸ¬ë‹ì„ í•  ìˆ˜ ìˆë„ë¡ [ê³µìœ ](./model_sharing)í•˜ëŠ” ê²ƒì„ ê³ ë ¤í•´ì£¼ì„¸ìš”. ğŸ¤—

## AutoClass

<Youtube id="AhChOFRegn4"/>

ë‚´ë¶€ì ìœ¼ë¡œ ë“¤ì–´ê°€ë©´ ìœ„ì—ì„œ ì‚¬ìš©í–ˆë˜ [`pipeline`]ì€ [`AutoModelForSequenceClassification`]ê³¼ [`AutoTokenizer`] í´ë˜ìŠ¤ë¡œ ì‘ë™í•©ë‹ˆë‹¤. [AutoClass](./model_doc/auto)ë€ ì´ë¦„ì´ë‚˜ ê²½ë¡œë¥¼ ë°›ìœ¼ë©´ ê·¸ì— ì•Œë§ëŠ” ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì„ ê°€ì ¸ì˜¤ëŠ” 'ë°”ë¡œê°€ê¸°'ë¼ê³  ë³¼ ìˆ˜ ìˆëŠ”ë°ìš”. ì›í•˜ëŠ” íƒœìŠ¤í¬ì™€ ì „ì²˜ë¦¬ì— ì í•©í•œ `AutoClass`ë¥¼ ê³ ë¥´ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤.

ì „ì— ì‚¬ìš©í–ˆë˜ ì˜ˆì‹œë¡œ ëŒì•„ê°€ì„œ `AutoClass`ë¡œ [`pipeline`]ê³¼ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

### AutoTokenizer

í† í¬ë‚˜ì´ì €ëŠ” ì „ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•˜ë©°, í…ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ì´ ë°›ì„ ìˆ«ì ë°°ì—´ë¡œ ë°”ê¿‰ë‹ˆë‹¤. í† í°í™” ê³¼ì •ì—ëŠ” ë‹¨ì–´ë¥¼ ì–´ë””ì—ì„œ ëŠì„ì§€, ì–¼ë§Œí¼ ë‚˜ëˆŒì§€ ë“±ì„ í¬í•¨í•œ ì—¬ëŸ¬ ê·œì¹™ì´ ìˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [í† í¬ë‚˜ì´ì € ìš”ì•½](./tokenizer_summary)ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”. ì œì¼ ì¤‘ìš”í•œ ì ì€ ëª¨ë¸ì´ í›ˆë ¨ëì„ ë•Œì™€ ë™ì¼í•œ í† í°í™” ê·œì¹™ì„ ì“°ë„ë¡ ë™ì¼í•œ ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ í† í¬ë‚˜ì´ì € ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤.

[`AutoTokenizer`]ë¡œ í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜¤ê³ ,

```py
>>> from transformers import AutoTokenizer

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```

í† í¬ë‚˜ì´ì €ì— í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ì„¸ìš”.

```py
>>> encoding = tokenizer("We are very happy to show you the ğŸ¤— Transformers library.")
>>> print(encoding)
{'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

ê·¸ëŸ¬ë©´ ë‹¤ìŒì„ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬ê°€ ë°˜í™˜ë©ë‹ˆë‹¤.

* [input_ids](./glossary#input-ids): ìˆ«ìë¡œ í‘œí˜„ëœ í† í°ë“¤
* [attention_mask](.glossary#attention-mask): ì£¼ì‹œí•  í† í°ë“¤

í† í¬ë‚˜ì´ì €ëŠ” ì…ë ¥ì„ ë¦¬ìŠ¤íŠ¸ë¡œë„ ë°›ì„ ìˆ˜ ìˆìœ¼ë©°, í…ìŠ¤íŠ¸ë¥¼ íŒ¨ë“œí•˜ê±°ë‚˜ ì˜ë¼ë‚´ì–´ ê· ì¼í•œ ê¸¸ì´ì˜ ë°°ì¹˜ë¥¼ ë°˜í™˜í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

<frameworkcontent>
<pt>
```py
>>> pt_batch = tokenizer(
...     ["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."],
...     padding=True,
...     truncation=True,
...     max_length=512,
...     return_tensors="pt",
... )
```
</pt>
<tf>
```py
>>> tf_batch = tokenizer(
...     ["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."],
...     padding=True,
...     truncation=True,
...     max_length=512,
...     return_tensors="tf",
... )
```
</tf>
</frameworkcontent>

<Tip>

[ì „ì²˜ë¦¬](./preprocessing) íŠœí† ë¦¬ì–¼ì„ ë³´ì‹œë©´ í† í°í™”ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ê³¼ í•¨ê»˜ ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ì™€ ë©€í‹°ëª¨ë‹¬ ì…ë ¥ì„ ì „ì²˜ë¦¬í•˜ê¸° ìœ„í•œ [`AutoFeatureExtractor`]ê³¼ [`AutoProcessor`]ì˜ ì‚¬ìš©ë°©ë²•ë„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

</Tip>

### AutoModel

<frameworkcontent>
<pt>
ğŸ¤— Transformersë¡œ ì‚¬ì „í•™ìŠµëœ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê°„ë‹¨í•˜ê³  í†µì¼ëœ ë°©ì‹ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬ë©´ [`AutoTokenizer`]ì²˜ëŸ¼ [`AutoModel`]ë„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. ìœ ì¼í•œ ì°¨ì´ì ì€ íƒœìŠ¤í¬ì— ì í•©í•œ [`AutoModel`]ì„ ì„ íƒí•´ì•¼ í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤. í…ìŠ¤íŠ¸(ë˜ëŠ” ì‹œí€€ìŠ¤) ë¶„ë¥˜ì˜ ê²½ìš° [`AutoModelForSequenceClassification`]ì„ ë¶ˆëŸ¬ì™€ì•¼ í•©ë‹ˆë‹¤.

```py
>>> from transformers import AutoModelForSequenceClassification

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
```

<Tip>

[`AutoModel`] í´ë˜ìŠ¤ì—ì„œ ì§€ì›í•˜ëŠ” íƒœìŠ¤í¬ë“¤ì€ [íƒœìŠ¤í¬ ì •ë¦¬](./task_summary) ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”.

</Tip>

ì´ì œ ì „ì²˜ë¦¬ëœ ì…ë ¥ ë°°ì¹˜ë¥¼ ëª¨ë¸ë¡œ ì§ì ‘ ë³´ë‚´ì•¼ í•©ë‹ˆë‹¤. ì•„ë˜ì²˜ëŸ¼ `**`ë¥¼ ì•ì— ë¶™ì—¬ ë”•ì…”ë„ˆë¦¬ë¥¼ í’€ì–´ì£¼ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤.

```py
>>> pt_outputs = pt_model(**pt_batch)
```

ëª¨ë¸ì˜ activation ê²°ê³¼ëŠ” `logits` ì†ì„±ì— ë‹´ê²¨ìˆìŠµë‹ˆë‹¤. `logits`ì— Softmax í•¨ìˆ˜ë¥¼ ì ìš©í•´ì„œ í™•ë¥  í˜•íƒœë¡œ ë°›ìœ¼ì„¸ìš”.

```py
>>> from torch import nn

>>> pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
>>> print(pt_predictions)
tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],
        [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)
```
</pt>
<tf>
ğŸ¤— TransformersëŠ” ì‚¬ì „í•™ìŠµëœ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê°„ë‹¨í•˜ê³  í†µì¼ëœ ë°©ì‹ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬ë©´ [`AutoTokenizer`]ì²˜ëŸ¼ [`TFAutoModel`]ë„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. ìœ ì¼í•œ ì°¨ì´ì ì€ íƒœìŠ¤í¬ì— ì í•©í•œ [`TFAutoModel`]ë¥¼ ì„ íƒí•´ì•¼ í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤. í…ìŠ¤íŠ¸(ë˜ëŠ” ì‹œí€€ìŠ¤) ë¶„ë¥˜ì˜ ê²½ìš° [`TFAutoModelForSequenceClassification`]ì„ ë¶ˆëŸ¬ì™€ì•¼ í•©ë‹ˆë‹¤.

```py
>>> from transformers import TFAutoModelForSequenceClassification

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
```

<Tip>

[`AutoModel`] í´ë˜ìŠ¤ì—ì„œ ì§€ì›í•˜ëŠ” íƒœìŠ¤í¬ë“¤ì€ [íƒœìŠ¤í¬ ì •ë¦¬](./task_summary) ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”.

</Tip>

ì´ì œ ì „ì²˜ë¦¬ëœ ì…ë ¥ ë°°ì¹˜ë¥¼ ëª¨ë¸ë¡œ ì§ì ‘ ë³´ë‚´ì•¼ í•©ë‹ˆë‹¤. ë”•ì…”ë„ˆë¦¬ì˜ í‚¤ë¥¼ í…ì„œì— ì§ì ‘ ë„£ì–´ì£¼ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤.

```py
>>> tf_outputs = tf_model(tf_batch)
```

ëª¨ë¸ì˜ activation ê²°ê³¼ëŠ” `logits` ì†ì„±ì— ë‹´ê²¨ìˆìŠµë‹ˆë‹¤. `logits`ì— Softmax í•¨ìˆ˜ë¥¼ ì ìš©í•´ì„œ í™•ë¥  í˜•íƒœë¡œ ë°›ìœ¼ì„¸ìš”.

```py
>>> import tensorflow as tf

>>> tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)
>>> tf_predictions  # doctest: +IGNORE_RESULT
```
</tf>
</frameworkcontent>

<Tip>

ëª¨ë“  (PyTorch ë˜ëŠ” TensorFlow) ğŸ¤— Transformers ëª¨ë¸ì€ (softmax ë“±ì˜) ìµœì¢… activation í•¨ìˆ˜ *ì´ì „ì—* í…ì„œë¥¼ ë‚´ë†“ìŠµë‹ˆë‹¤. ì™œëƒí•˜ë©´ ìµœì¢… activation í•¨ìˆ˜ë¥¼ ì¢…ì¢… loss í•¨ìˆ˜ì™€ ë™ì¼ì‹œí•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ëª¨ë¸ ì¶œë ¥ì€ íŠ¹ìˆ˜ ë°ì´í„° í´ë˜ìŠ¤ì´ë¯€ë¡œ í•´ë‹¹ ì†ì„±ì€ IDEì—ì„œ ìë™ìœ¼ë¡œ ì™„ì„±ë©ë‹ˆë‹¤. ëª¨ë¸ ì¶œë ¥ì€ íŠœí”Œ ë˜ëŠ” (ì •ìˆ˜, ìŠ¬ë¼ì´ìŠ¤ ë˜ëŠ” ë¬¸ìì—´ë¡œ ì¸ë±ì‹±í•˜ëŠ”) ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì£¼ì–´ì§€ê³  ì´ëŸ° ê²½ìš° Noneì¸ ì†ì„±ì€ ë¬´ì‹œë©ë‹ˆë‹¤.

</Tip>

### ëª¨ë¸ ì €ì¥í•˜ê¸°[[save-a-model]]

<frameworkcontent>
<pt>
ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•œ ë’¤ì—ëŠ” [`PreTrainedModel.save_pretrained`]ë¡œ ëª¨ë¸ì„ í† í¬ë‚˜ì´ì €ì™€ í•¨ê»˜ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```py
>>> pt_save_directory = "./pt_save_pretrained"
>>> tokenizer.save_pretrained(pt_save_directory)  # doctest: +IGNORE_RESULT
>>> pt_model.save_pretrained(pt_save_directory)
```

ëª¨ë¸ì„ ë‹¤ì‹œ ì‚¬ìš©í•  ë•ŒëŠ” [`PreTrainedModel.from_pretrained`]ë¡œ ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¤ë©´ ë©ë‹ˆë‹¤.

```py
>>> pt_model = AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained")
```
</pt>
<tf>
ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•œ ë’¤ì—ëŠ” [`TFPreTrainedModel.save_pretrained`]ë¡œ ëª¨ë¸ì„ í† í¬ë‚˜ì´ì €ì™€ í•¨ê»˜ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```py
>>> tf_save_directory = "./tf_save_pretrained"
>>> tokenizer.save_pretrained(tf_save_directory)  # doctest: +IGNORE_RESULT
>>> tf_model.save_pretrained(tf_save_directory)
```

ëª¨ë¸ì„ ë‹¤ì‹œ ì‚¬ìš©í•  ë•ŒëŠ” [`TFPreTrainedModel.from_pretrained`]ë¡œ ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¤ë©´ ë©ë‹ˆë‹¤.

```py
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained("./tf_save_pretrained")
```
</tf>
</frameworkcontent>

ğŸ¤— Transformers ê¸°ëŠ¥ ì¤‘ íŠ¹íˆ ì¬ë¯¸ìˆëŠ” í•œ ê°€ì§€ëŠ” ëª¨ë¸ì„ ì €ì¥í•˜ê³  PyTorchë‚˜ TensorFlow ëª¨ë¸ë¡œ ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤. 'from_pt' ë˜ëŠ” 'from_tf' ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•´ ëª¨ë¸ì„ ê¸°ì¡´ê³¼ ë‹¤ë¥¸ í”„ë ˆì„ì›Œí¬ë¡œ ë³€í™˜ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<frameworkcontent>
<pt>
```py
>>> from transformers import AutoModel

>>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
>>> pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)
```
</pt>
<tf>
```py
>>> from transformers import TFAutoModel

>>> tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)
```
</tf>
</frameworkcontent>

## ì»¤ìŠ¤í…€ ëª¨ë¸ êµ¬ì¶•í•˜ê¸°[[custom-model-builds]]

ëª¨ë¸ì˜ êµ¬ì„± í´ë˜ìŠ¤ë¥¼ ìˆ˜ì •í•˜ì—¬ ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ ë°”ê¿€ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì€ë‹‰ì¸µ, ì–´í…ì…˜ í—¤ë“œ ìˆ˜ì™€ ê°™ì€ ëª¨ë¸ì˜ ì†ì„±ì„ êµ¬ì„±ì—ì„œ ì§€ì •í•©ë‹ˆë‹¤. ì»¤ìŠ¤í…€ êµ¬ì„± í´ë˜ìŠ¤ì—ì„œ ëª¨ë¸ì„ ë§Œë“¤ë©´ ì²˜ìŒë¶€í„° ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤. ëª¨ë¸ ì†ì„±ì€ ëœë¤í•˜ê²Œ ì´ˆê¸°í™”ë˜ë¯€ë¡œ ì˜ë¯¸ ìˆëŠ” ê²°ê³¼ë¥¼ ì–»ìœ¼ë ¤ë©´ ë¨¼ì € ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¬ í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤.

ë¨¼ì € [`AutoConfig`]ë¥¼ ì„í¬íŠ¸í•˜ê³ , ìˆ˜ì •í•˜ê³  ì‹¶ì€ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ì„¸ìš”. [`AutoConfig.from_pretrained`]ì—ì„œ ì–´í…ì…˜ í—¤ë“œ ìˆ˜ ê°™ì€ ì†ì„±ì„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```py
>>> from transformers import AutoConfig

>>> my_config = AutoConfig.from_pretrained("distilbert-base-uncased", n_heads=12)
```

<frameworkcontent>
<pt>
[`AutoModel.from_config`]ë¥¼ ì‚¬ìš©í•˜ì—¬ ì»¤ìŠ¤í…€ êµ¬ì„±ëŒ€ë¡œ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.

```py
>>> from transformers import AutoModel

>>> my_model = AutoModel.from_config(my_config)
```
</pt>
<tf>
[`TFAutoModel.from_config`]ë¥¼ ì‚¬ìš©í•˜ì—¬ ì»¤ìŠ¤í…€ êµ¬ì„±ëŒ€ë¡œ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.

```py
>>> from transformers import TFAutoModel

>>> my_model = TFAutoModel.from_config(my_config)
```
</tf>
</frameworkcontent>

ì»¤ìŠ¤í…€ êµ¬ì„±ì„ ì‘ì„±í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ [ì»¤ìŠ¤í…€ ì•„í‚¤í…ì²˜ ë§Œë“¤ê¸°](./create_a_model) ê°€ì´ë“œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

## Trainer - PyTorchì— ìµœì í™”ëœ í›ˆë ¨ ë°˜ë³µ ë£¨í”„[[trainer-a-pytorch-optimized-training-loop]]

ëª¨ë“  ëª¨ë¸ì€ [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)ì´ì–´ì„œ ëŒ€ë‹¤ìˆ˜ì˜ í›ˆë ¨ ë°˜ë³µ ë£¨í”„ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‚¬ìš©ìê°€ ì§ì ‘ í›ˆë ¨ ë°˜ë³µ ë£¨í”„ë¥¼ ì‘ì„±í•´ë„ ë˜ì§€ë§Œ, ğŸ¤— TransformersëŠ” PyTorchìš© [`Trainer`] í´ë˜ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ê¸°ë³¸ì ì¸ í›ˆë ¨ ë°˜í­ ë£¨í”„ê°€ í¬í•¨ë˜ì–´ ìˆê³ , ë¶„ì‚° í›ˆë ¨ì´ë‚˜ í˜¼í•© ì •ë°€ë„ ë“±ì˜ ì¶”ê°€ ê¸°ëŠ¥ë„ ìˆìŠµë‹ˆë‹¤.

íƒœìŠ¤í¬ì— ë”°ë¼ ë‹¤ë¥´ì§€ë§Œ, ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ìŒ ë§¤ê°œë³€ìˆ˜ë¥¼ [`Trainer`]ì— ì „ë‹¬í•  ê²ƒì…ë‹ˆë‹¤.

1. [`PreTrainedModel`] ë˜ëŠ” [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.

   ```py
   >>> from transformers import AutoModelForSequenceClassification

   >>> model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")
   ```

2. [`TrainingArguments`]ë¡œ í•™ìŠµë¥ , ë°°ì¹˜ í¬ê¸°ë‚˜ í›ˆë ¨í•  epoch ìˆ˜ì™€ ê°™ì´ ëª¨ë¸ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ í›ˆë ¨ ì¸ìˆ˜ë¥¼ ì „í˜€ ì§€ì •í•˜ì§€ ì•Šì€ ê²½ìš° ì‚¬ìš©ë©ë‹ˆë‹¤.

   ```py
   >>> from transformers import TrainingArguments

   >>> training_args = TrainingArguments(
   ...     output_dir="path/to/save/folder/",
   ...     learning_rate=2e-5,
   ...     per_device_train_batch_size=8,
   ...     per_device_eval_batch_size=8,
   ...     num_train_epochs=2,
   ... )
   ```

3. í† í¬ë‚˜ì´ì €, íŠ¹ì§•ì¶”ì¶œê¸°(feature extractor), ì „ì²˜ë¦¬ê¸°(processor) í´ë˜ìŠ¤ ë“±ìœ¼ë¡œ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

   ```py
   >>> from transformers import AutoTokenizer

   >>> tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
   ```

4. ë°ì´í„°ì…‹ë¥¼ ì ì¬í•©ë‹ˆë‹¤.

   ```py
   >>> from datasets import load_dataset

   >>> dataset = load_dataset("rotten_tomatoes")  # doctest: +IGNORE_RESULT
   ```

5. ë°ì´í„°ì…‹ì„ í† í°í™”í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ê³  [`~datasets.Dataset.map`]ìœ¼ë¡œ ì „ì²´ ë°ì´í„°ì…‹ì— ì ìš©ì‹œí‚µë‹ˆë‹¤.

   ```py
   >>> def tokenize_dataset(dataset):
   ...     return tokenizer(dataset["text"])


   >>> dataset = dataset.map(tokenize_dataset, batched=True)
   ```

6. [`DataCollatorWithPadding`]ë¡œ ë°ì´í„°ì…‹ìœ¼ë¡œë¶€í„° í‘œë³¸ìœ¼ë¡œ ì‚¼ì„ ë°°ì¹˜ë¥¼ ë§Œë“­ë‹ˆë‹¤.

   ```py
   >>> from transformers import DataCollatorWithPadding

   >>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
   ```

ì´ì œ ìœ„ì˜ ëª¨ë“  í´ë˜ìŠ¤ë¥¼ [`Trainer`]ë¡œ ëª¨ìœ¼ì„¸ìš”.

```py
>>> from transformers import Trainer

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=dataset["train"],
...     eval_dataset=dataset["test"],
...     tokenizer=tokenizer,
...     data_collator=data_collator,
... )  # doctest: +SKIP
```

ì¤€ë¹„ë˜ì—ˆìœ¼ë©´ [`~Trainer.train`]ìœ¼ë¡œ í›ˆë ¨ì„ ì‹œì‘í•˜ì„¸ìš”.

```py
>>> trainer.train()  # doctest: +SKIP
```

<Tip>

sequence-to-sequence ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” (ë²ˆì—­ì´ë‚˜ ìš”ì•½ ê°™ì€) íƒœìŠ¤í¬ì˜ ê²½ìš° [`Seq2SeqTrainer`]ì™€ [`Seq2SeqTrainingArguments`] í´ë˜ìŠ¤ë¥¼ ëŒ€ì‹  ì‚¬ìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.

</Tip>

[`Trainer`] ë‚´ë¶€ì˜ ë©”ì„œë“œë¥¼ êµ¬í˜„ ìƒì†(subclassing)í•´ì„œ í›ˆë ¨ ë°˜ë³µ ë£¨í”„ë¥¼ ê°œì¡°í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬ë©´ loss í•¨ìˆ˜, optimizer, scheduler ë“±ì˜ ê¸°ëŠ¥ë„ ê°œì¡°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì–´ë–¤ ë©”ì„œë“œë¥¼ êµ¬í˜„ ìƒì†í•  ìˆ˜ ìˆëŠ”ì§€ ì•Œì•„ë³´ë ¤ë©´ [`Trainer`]ë¥¼ ì°¸ê³ í•˜ì„¸ìš”. 

í›ˆë ¨ ë°˜ë³µ ë£¨í”„ë¥¼ ê°œì¡°í•˜ëŠ” ë‹¤ë¥¸ ë°©ë²•ì€ [Callbacks](./main_classes/callbacks)ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. Callbacksë¡œ ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ í†µí•©í•˜ê³ , í›ˆë ¨ ë°˜ë³µ ë£¨í”„ë¥¼ ìˆ˜ì‹œë¡œ ì²´í¬í•˜ì—¬ ì§„í–‰ ìƒí™©ì„ ë³´ê³ ë°›ê±°ë‚˜, í›ˆë ¨ì„ ì¡°ê¸°ì— ì¤‘ë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Callbacksì€ í›ˆë ¨ ë°˜ë³µ ë£¨í”„ ìì²´ë¥¼ ì „í˜€ ìˆ˜ì •í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë§Œì•½ loss í•¨ìˆ˜ ë“±ì„ ê°œì¡°í•˜ê³  ì‹¶ë‹¤ë©´ [`Trainer`]ë¥¼ êµ¬í˜„ ìƒì†í•´ì•¼ë§Œ í•©ë‹ˆë‹¤.

## TensorFlowë¡œ í›ˆë ¨ì‹œí‚¤ê¸°[[train-with-tensorflow]]

ëª¨ë“  ëª¨ë¸ì€ [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)ì´ì–´ì„œ [Keras](https://keras.io/) APIë¥¼ í†µí•´ TensorFlowì—ì„œ í›ˆë ¨ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸ¤— Transformersì—ì„œ ë°ì´í„°ì…‹ë¥¼ `tf.data.Dataset` í˜•íƒœë¡œ ì‰½ê²Œ ì ì¬í•  ìˆ˜ ìˆëŠ” [`~TFPreTrainedModel.prepare_tf_dataset`] ë©”ì„œë“œë¥¼ ì œê³µí•˜ê¸° ë•Œë¬¸ì—, Kerasì˜ [`compile`](https://keras.io/api/models/model_training_apis/#compile-method) ë° [`fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) ë©”ì„œë“œë¡œ ì¦‰ì‹œ í›ˆë ¨ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

1. [`TFPreTrainedModel`] ë˜ëŠ” [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.

   ```py
   >>> from transformers import TFAutoModelForSequenceClassification

   >>> model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")
   ```

2. í† í¬ë‚˜ì´ì €, íŠ¹ì§•ì¶”ì¶œê¸°(feature extractor), ì „ì²˜ë¦¬ê¸°(processor) í´ë˜ìŠ¤ ë“±ìœ¼ë¡œ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

   ```py
   >>> from transformers import AutoTokenizer

   >>> tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
   ```

3. ë°ì´í„°ì…‹ì„ í† í°í™”í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤.

   ```py
   >>> def tokenize_dataset(dataset):
   ...     return tokenizer(dataset["text"])  # doctest: +SKIP
   ```

4. [`~datasets.Dataset.map`]ìœ¼ë¡œ ì „ì²´ ë°ì´í„°ì…‹ì— ìœ„ í•¨ìˆ˜ë¥¼ ì ìš©ì‹œí‚¨ ë‹¤ìŒ, ë°ì´í„°ì…‹ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ [`~TFPreTrainedModel.prepare_tf_dataset`]ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤. ë°°ì¹˜ í¬ê¸°ë¥¼ ë³€ê²½í•´ë³´ê±°ë‚˜ ë°ì´í„°ì…‹ë¥¼ ì„ì–´ë´ë„ ì¢‹ìŠµë‹ˆë‹¤.

   ```py
   >>> dataset = dataset.map(tokenize_dataset)  # doctest: +SKIP
   >>> tf_dataset = model.prepare_tf_dataset(
   ...     dataset, batch_size=16, shuffle=True, tokenizer=tokenizer
   ... )  # doctest: +SKIP
   ```

5. ì¤€ë¹„ë˜ì—ˆìœ¼ë©´ `compile`ê³¼ `fit`ìœ¼ë¡œ í›ˆë ¨ì„ ì‹œì‘í•˜ì„¸ìš”.

   ```py
   >>> from tensorflow.keras.optimizers import Adam

   >>> model.compile(optimizer=Adam(3e-5))
   >>> model.fit(dataset)  # doctest: +SKIP
   ```

## ì´ì œ ë¬´ì–¼ í•˜ë©´ ë ê¹Œìš”?[[whats-next]]

ğŸ¤— Transformers ë‘˜ëŸ¬ë³´ê¸°ë¥¼ ëª¨ë‘ ì½ìœ¼ì…¨ë‹¤ë©´, ê°€ì´ë“œë¥¼ í†µí•´ íŠ¹ì • ê¸°ìˆ ì„ ë°°ìš¸ ìˆ˜ ìˆì–´ìš”. ì˜ˆë¥¼ ë“¤ì–´ ì»¤ìŠ¤í…€ ëª¨ë¸ì„ ì‘ì„±í•˜ëŠ” ë°©ë²•, íƒœìŠ¤í¬ìš© ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ëŠ” ë°©ë²•, ìŠ¤í¬ë¦½íŠ¸ë¡œ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ëŠ” ë°©ë²• ë“±ì´ ìˆìŠµë‹ˆë‹¤. ğŸ¤— Transformersì˜ í•µì‹¬ ê°œë…ì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ë ¤ë©´ ì»¤í”¼ í•œ ì”ì„ ë§ˆì‹  ë’¤ ê°œë… ê°€ì´ë“œë¥¼ ì‚´í´ë³´ì…”ë„ ì¢‹ìŠµë‹ˆë‹¤!
