<!--Copyright 2021 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# CLIP

## Overview

CLIP ãƒ¢ãƒ‡ãƒ«ã¯ã€Alec Radfordã€Jong Wook Kimã€Chris Hallacyã€Aditya Rameshã€Gabriel Goh Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) ã§ææ¡ˆã•ã‚Œã¾ã—ãŸã€‚
ã‚µãƒ³ãƒ‡ã‚£ãƒ‹ãƒ»ã‚¢ã‚¬ãƒ«ãƒ¯ãƒ«ã€ã‚®ãƒªãƒƒã‚·ãƒ¥ãƒ»ã‚µã‚¹ãƒˆãƒªãƒ¼ã€ã‚¢ãƒãƒ³ãƒ€ãƒ»ã‚¢ã‚¹ã‚±ãƒ«ã€ãƒ‘ãƒ¡ãƒ©ãƒ»ãƒŸã‚·ãƒ¥ã‚­ãƒ³ã€ã‚¸ãƒ£ãƒƒã‚¯ãƒ»ã‚¯ãƒ©ãƒ¼ã‚¯ã€ã‚°ãƒ¬ãƒƒãƒã‚§ãƒ³ãƒ»ã‚¯ãƒ«ãƒ¼ã‚¬ãƒ¼ã€ã‚¤ãƒªãƒ¤ãƒ»ã‚µãƒ„ã‚±ãƒ´ã‚¡ãƒ¼ã€‚ã‚¯ãƒªãƒƒãƒ—
(Contrastive Language-Image Pre-Training) ã¯ã€ã•ã¾ã–ã¾ãª (ç”»åƒã€ãƒ†ã‚­ã‚¹ãƒˆ) ãƒšã‚¢ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ« ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚ã‹ã‚‚ã­
ç›´æ¥æœ€é©åŒ–ã™ã‚‹ã“ã¨ãªãã€ä¸ãˆã‚‰ã‚ŒãŸç”»åƒã‹ã‚‰æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„ãƒ†ã‚­ã‚¹ãƒˆ ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’äºˆæ¸¬ã™ã‚‹ã‚ˆã†ã«è‡ªç„¶è¨€èªã§æŒ‡ç¤ºã•ã‚Œã¾ã™ã€‚
GPT-2 ãŠã‚ˆã³ 3 ã®ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆæ©Ÿèƒ½ã¨åŒæ§˜ã«ã€ã‚¿ã‚¹ã‚¯ã«å¯¾ã—ã¦ã€‚

è«–æ–‡ã®è¦ç´„ã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ã€‚

*æœ€å…ˆç«¯ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ ãƒ“ã‚¸ãƒ§ãƒ³ ã‚·ã‚¹ãƒ†ãƒ ã¯ã€ã‚ã‚‰ã‹ã˜ã‚å®šã‚ã‚‰ã‚ŒãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ ã‚«ãƒ†ã‚´ãƒªã®å›ºå®šã‚»ãƒƒãƒˆã‚’äºˆæ¸¬ã™ã‚‹ã‚ˆã†ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚Œ
åˆ¶é™ã•ã‚ŒãŸå½¢å¼ã®ç›£è¦–ã§ã¯ã€æŒ‡å®šã™ã‚‹ãŸã‚ã«è¿½åŠ ã®ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã¨ãªã‚‹ãŸã‚ã€ä¸€èˆ¬æ€§ã¨ä½¿ã„ã‚„ã™ã•ãŒåˆ¶é™ã•ã‚Œã¾ã™ã€‚
ãã®ä»–ã®è¦–è¦šçš„ãªã‚³ãƒ³ã‚»ãƒ—ãƒˆã€‚ç”»åƒã«é–¢ã™ã‚‹ç”Ÿã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç›´æ¥å­¦ç¿’ã™ã‚‹ã“ã¨ã¯ã€
ã‚ˆã‚Šåºƒç¯„ãªç›£ç£æºã€‚ã©ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã‹ã‚’äºˆæ¸¬ã™ã‚‹ã¨ã„ã†å˜ç´”ãªäº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ã‚¿ã‚¹ã‚¯ãŒæœ‰åŠ¹ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚
400 ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ SOTA ç”»åƒè¡¨ç¾ã‚’æœ€åˆã‹ã‚‰å­¦ç¿’ã™ã‚‹ãŸã‚ã®åŠ¹ç‡çš„ã‹ã¤ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªæ–¹æ³•ã¯ã©ã®ç”»åƒã§ã™ã‹
ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã‹ã‚‰åé›†ã•ã‚ŒãŸæ•°ç™¾ä¸‡ã®ï¼ˆç”»åƒã€ãƒ†ã‚­ã‚¹ãƒˆï¼‰ãƒšã‚¢ã€‚äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã€è‡ªç„¶è¨€èªã‚’ä½¿ç”¨ã—ã¦å‚ç…§ã—ã¾ã™ã€‚
è¦–è¦šçš„ãªæ¦‚å¿µã‚’å­¦ç¿’ã—ï¼ˆã¾ãŸã¯æ–°ã—ã„æ¦‚å¿µã‚’èª¬æ˜ã—ï¼‰ã€ä¸‹æµã®ã‚¿ã‚¹ã‚¯ã¸ã®ãƒ¢ãƒ‡ãƒ«ã®ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆè»¢é€ã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚ç§ãŸã¡ã¯å‹‰å¼·ã—ã¾ã™
30 ã‚’è¶…ãˆã‚‹ã•ã¾ã–ã¾ãªæ—¢å­˜ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ ãƒ“ã‚¸ãƒ§ãƒ³ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚¿ã‚¹ã‚¯ã‚’ã¾ãŸãŒã£ã¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’è¡Œã†ã“ã¨ã«ã‚ˆã‚Šã€ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡ã—ã¾ã™ã€‚
OCRã€ãƒ“ãƒ‡ã‚ªå†…ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³èªè­˜ã€åœ°ç†çš„ä½ç½®ç‰¹å®šã€ãŠã‚ˆã³ã•ã¾ã–ã¾ãªç¨®é¡ã®ãã‚ç´°ã‹ã„ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆåˆ†é¡ãªã©ã€‚ã®
ãƒ¢ãƒ‡ãƒ«ã¯ã»ã¨ã‚“ã©ã®ã‚¿ã‚¹ã‚¯ã«ç°¡å˜ã«ç§»è¡Œã§ãã€å¤šãã®å ´åˆã€å¿…è¦ãŒãªãã¦ã‚‚å®Œå…¨ã«ç›£è¦–ã•ã‚ŒãŸãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ç«¶åˆã—ã¾ã™ã€‚
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå›ºæœ‰ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«é©ã—ã¦ã„ã¾ã™ã€‚ãŸã¨ãˆã°ã€ImageNet ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã§ã¯ã‚ªãƒªã‚¸ãƒŠãƒ«ã® ResNet-50 ã®ç²¾åº¦ã¨ä¸€è‡´ã—ã¾ã™ã€‚
ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä½¿ç”¨ã•ã‚ŒãŸ 128 ä¸‡ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã‚³ãƒ¼ãƒ‰ã‚’ãƒªãƒªãƒ¼ã‚¹ã—ã€äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿
ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã¯ã“ã® https URL ã§ç¢ºèªã§ãã¾ã™ã€‚*

ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ [valhalla](https://huggingface.co/valhalla) ã«ã‚ˆã£ã¦æä¾›ã•ã‚Œã¾ã—ãŸã€‚å…ƒã®ã‚³ãƒ¼ãƒ‰ã¯ [ã“ã“](https://github.com/openai/CLIP) ã«ã‚ã‚Šã¾ã™ã€‚

## Usage tips and example

CLIP ã¯ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªãƒ“ã‚¸ãƒ§ãƒ³ãŠã‚ˆã³è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®é¡ä¼¼æ€§ã‚„ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆç”»åƒã«ä½¿ç”¨ã§ãã¾ã™ã€‚
åˆ†é¡ã€‚ CLIP ã¯ã€ViT ã®ã‚ˆã†ãªãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’ä½¿ç”¨ã—ã¦è¦–è¦šçš„ç‰¹å¾´ã‚’å–å¾—ã—ã€å› æœè¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã—ã¾ã™
ç‰¹å¾´ã€‚æ¬¡ã«ã€ãƒ†ã‚­ã‚¹ãƒˆã¨è¦–è¦šã®ä¸¡æ–¹ã®ç‰¹å¾´ãŒã€åŒã˜æ¬¡å…ƒã®æ½œåœ¨ç©ºé–“ã«æŠ•å½±ã•ã‚Œã¾ã™ã€‚ãƒ‰ãƒƒãƒˆ
æŠ•å½±ã•ã‚ŒãŸç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®ç‰¹å¾´é–“ã®ç©ãŒåŒæ§˜ã®ã‚¹ã‚³ã‚¢ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

ç”»åƒã‚’ Transformer ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã«ä¾›çµ¦ã™ã‚‹ãŸã‚ã«ã€å„ç”»åƒã¯å›ºå®šã‚µã‚¤ã‚ºã®é‡è¤‡ã—ãªã„ãƒ‘ãƒƒãƒã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«åˆ†å‰²ã•ã‚Œã¾ã™ã€‚
ã“ã‚Œã‚‰ã¯ç·šå½¢ã«åŸ‹ã‚è¾¼ã¾ã‚Œã¾ã™ã€‚ [CLS] ãƒˆãƒ¼ã‚¯ãƒ³ã¯ã€ã‚¤ãƒ¡ãƒ¼ã‚¸å…¨ä½“ã®è¡¨ç¾ã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹ãŸã‚ã«è¿½åŠ ã•ã‚Œã¾ã™ã€‚ä½œå®¶ãŸã¡
ã¾ãŸã€çµ¶å¯¾ä½ç½®åŸ‹ã‚è¾¼ã¿ã‚’è¿½åŠ ã—ã€çµæœã¨ã—ã¦å¾—ã‚‰ã‚Œã‚‹ãƒ™ã‚¯ãƒˆãƒ«ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’æ¨™æº–ã® Transformer ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã«ä¾›çµ¦ã—ã¾ã™ã€‚
[`CLIPImageProcessor`] ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®ç”»åƒã®ã‚µã‚¤ã‚ºå¤‰æ›´ (ã¾ãŸã¯å†ã‚¹ã‚±ãƒ¼ãƒ«) ãŠã‚ˆã³æ­£è¦åŒ–ã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚

[`CLIPTokenizer`] ã¯ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚ [`CLIPProcessor`] ã¯ãƒ©ãƒƒãƒ—ã—ã¾ã™
[`CLIPImageProcessor`] ã¨ [`CLIPTokenizer`] ã‚’ä¸¡æ–¹ã®å˜ä¸€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«çµ±åˆ
ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦ç”»åƒã‚’æº–å‚™ã—ã¾ã™ã€‚æ¬¡ã®ä¾‹ã¯ã€æ¬¡ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®é¡ä¼¼æ€§ã‚¹ã‚³ã‚¢ã‚’å–å¾—ã™ã‚‹æ–¹æ³•ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
[`CLIPProcessor`] ã¨ [`CLIPModel`]ã€‚

```python
>>> from PIL import Image
>>> import requests

>>> from transformers import CLIPProcessor, CLIPModel

>>> model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
>>> processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(text=["a photo of a cat", "a photo of a dog"], images=image, return_tensors="pt", padding=True)

>>> outputs = model(**inputs)
>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
>>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities
```

## Resources

CLIP ã‚’ä½¿ã„å§‹ã‚ã‚‹ã®ã«å½¹ç«‹ã¤å…¬å¼ Hugging Face ãŠã‚ˆã³ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ (ğŸŒ ã§ç¤ºã•ã‚Œã¦ã„ã‚‹) ãƒªã‚½ãƒ¼ã‚¹ã®ãƒªã‚¹ãƒˆã€‚

- [ãƒªãƒ¢ãƒ¼ãƒˆ ã‚»ãƒ³ã‚·ãƒ³ã‚° (è¡›æ˜Ÿ) ç”»åƒã¨ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ãŸ CLIP ã®å¾®èª¿æ•´](https://huggingface.co/blog/fine-tune-clip-rsicd)ã€[RSICD ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ] ã‚’ä½¿ç”¨ã—ã¦ CLIP ã‚’å¾®èª¿æ•´ã™ã‚‹æ–¹æ³•ã«é–¢ã™ã‚‹ãƒ–ãƒ­ã‚°æŠ•ç¨¿(https://github.com/201528014227051/RSICD_optimal) ã¨ã€ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã«ã‚ˆã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®å¤‰åŒ–ã®æ¯”è¼ƒã€‚
- ã“ã® [ã‚µãƒ³ãƒ—ãƒ« ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](https://github.com/huggingface/transformers/tree/main/examples/pytorch/contrastive-image-text) ã¯ã€ãƒ—ãƒ¬- [COCO ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ](https://cocodataset.org/#home) ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ“ã‚¸ãƒ§ãƒ³ãŠã‚ˆã³ãƒ†ã‚­ã‚¹ãƒˆ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã€‚

<PipelineTag pipeline="image-to-text"/>

- ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®ãƒ“ãƒ¼ãƒ æ¤œç´¢ã«ã‚ˆã‚‹æ¨è«–ã«äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ CLIP ã‚’ä½¿ç”¨ã™ã‚‹æ–¹æ³•ã«é–¢ã™ã‚‹ [ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/drive/1tuoAC5F4sC7qid56Z0ap-stR3rwdk0ZV?usp=sharing)ã€‚ ğŸŒ

**ç”»åƒæ¤œç´¢**

- äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸ CLIP ã‚’ä½¿ç”¨ã—ãŸç”»åƒæ¤œç´¢ã¨ MRR (å¹³å‡ç›¸äº’ãƒ©ãƒ³ã‚¯) ã‚¹ã‚³ã‚¢ã®è¨ˆç®—ã«é–¢ã™ã‚‹ [ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/drive/1bLVwVKpAndpEDHqjzxVPr_9nGrSbuOQd?usp=sharing)ã€‚ ğŸŒ
- ç”»åƒã®å–å¾—ã¨é¡ä¼¼æ€§ã‚¹ã‚³ã‚¢ã®è¡¨ç¤ºã«é–¢ã™ã‚‹ [ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/github/deep-diver/image_search_with_natural_language/blob/main/notebooks/Image_Search_CLIP.ipynb)ã€‚ ğŸŒ
- å¤šè¨€èª CLIP ã‚’ä½¿ç”¨ã—ã¦ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’åŒã˜ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹æ–¹æ³•ã«é–¢ã™ã‚‹ [ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/drive/1xO-wC_m_GNzgjIBQ4a4znvQkvDoZJvH4?usp=sharing)ã€‚ ğŸŒ
- ã‚’ä½¿ç”¨ã—ã¦ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ ã‚¤ãƒ¡ãƒ¼ã‚¸æ¤œç´¢ã§ CLIP ã‚’å®Ÿè¡Œã™ã‚‹æ–¹æ³•ã«é–¢ã™ã‚‹ [ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/github/vivien000/clip-demo/blob/master/clip.ipynb#scrollTo=uzdFhRGqiWkR) [Unsplash](https://unsplash.com) ãŠã‚ˆã³ [TMDB](https://www.themoviedb.org/) ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚ ğŸŒ

**èª¬æ˜å¯èƒ½æ€§**

- å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã¨ç”»åƒã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®é¡ä¼¼æ€§ã‚’è¦–è¦šåŒ–ã™ã‚‹æ–¹æ³•ã«é–¢ã™ã‚‹ [ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/github/hila-chefer/Transformer-MM-Explainability/blob/main/CLIP_explainability.ipynb)ã€‚ ğŸŒ

ã“ã“ã«å«ã‚ã‚‹ãƒªã‚½ãƒ¼ã‚¹ã®é€ä¿¡ã«èˆˆå‘³ãŒã‚ã‚‹å ´åˆã¯ã€ãŠæ°—è»½ã«ãƒ—ãƒ« ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é–‹ã„ã¦ãã ã•ã„ã€‚å¯©æŸ»ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚
ãƒªã‚½ãƒ¼ã‚¹ã¯ã€æ—¢å­˜ã®ãƒªã‚½ãƒ¼ã‚¹ã‚’è¤‡è£½ã™ã‚‹ã®ã§ã¯ãªãã€ä½•ã‹æ–°ã—ã„ã‚‚ã®ã‚’ç¤ºã™ã“ã¨ãŒç†æƒ³çš„ã§ã™ã€‚

## CLIPConfig

[[autodoc]] CLIPConfig
    - from_text_vision_configs

## CLIPTextConfig

[[autodoc]] CLIPTextConfig

## CLIPVisionConfig

[[autodoc]] CLIPVisionConfig

## CLIPTokenizer

[[autodoc]] CLIPTokenizer
    - build_inputs_with_special_tokens
    - get_special_tokens_mask
    - create_token_type_ids_from_sequences
    - save_vocabulary

## CLIPTokenizerFast

[[autodoc]] CLIPTokenizerFast

## CLIPImageProcessor

[[autodoc]] CLIPImageProcessor
    - preprocess

## CLIPFeatureExtractor

[[autodoc]] CLIPFeatureExtractor

## CLIPProcessor

[[autodoc]] CLIPProcessor

<frameworkcontent>
<pt>

## CLIPModel

[[autodoc]] CLIPModel
    - forward
    - get_text_features
    - get_image_features

## CLIPTextModel

[[autodoc]] CLIPTextModel
    - forward

## CLIPTextModelWithProjection

[[autodoc]] CLIPTextModelWithProjection
    - forward

## CLIPVisionModelWithProjection

[[autodoc]] CLIPVisionModelWithProjection
    - forward

## CLIPVisionModel

[[autodoc]] CLIPVisionModel
    - forward

</pt>
<tf>

## TFCLIPModel

[[autodoc]] TFCLIPModel
    - call
    - get_text_features
    - get_image_features

## TFCLIPTextModel

[[autodoc]] TFCLIPTextModel
    - call

## TFCLIPVisionModel

[[autodoc]] TFCLIPVisionModel
    - call

</tf>
<jax>

## FlaxCLIPModel

[[autodoc]] FlaxCLIPModel
    - __call__
    - get_text_features
    - get_image_features

## FlaxCLIPTextModel

[[autodoc]] FlaxCLIPTextModel
    - __call__

## FlaxCLIPTextModelWithProjection

[[autodoc]] FlaxCLIPTextModelWithProjection
    - __call__

## FlaxCLIPVisionModel

[[autodoc]] FlaxCLIPVisionModel
    - __call__

</jax>
</frameworkcontent>
