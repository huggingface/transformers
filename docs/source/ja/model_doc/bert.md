<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# BERT

<div class="flex flex-wrap space-x-1">
<a href="https://huggingface.co/models?filter=bert">
<img alt="Models" src="https://img.shields.io/badge/All_model_pages-bert-blueviolet">
</a>
<a href="https://huggingface.co/spaces/docs-demos/bert-base-uncased">
<img alt="Spaces" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue">
</a>
</div>

## Overview

BERT ãƒ¢ãƒ‡ãƒ«ã¯ã€Jacob Devlinã€Ming-Wei Changã€Kenton Leeã€Kristina Toutanova ã«ã‚ˆã£ã¦ [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) ã§ææ¡ˆã•ã‚Œã¾ã—ãŸã€‚ãã‚Œã¯
ãƒã‚¹ã‚¯ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ç›®æ¨™ã¨æ¬¡ã®æ–‡ã®çµ„ã¿åˆã‚ã›ã‚’ä½¿ç”¨ã—ã¦äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸåŒæ–¹å‘ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼
Toronto Book Corpus ã¨ Wikipedia ã‹ã‚‰ãªã‚‹å¤§è¦æ¨¡ãªã‚³ãƒ¼ãƒ‘ã‚¹ã§ã®äºˆæ¸¬ã€‚

è«–æ–‡ã®è¦ç´„ã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ã€‚

*BERT ã¨å‘¼ã°ã‚Œã‚‹æ–°ã—ã„è¨€èªè¡¨ç¾ãƒ¢ãƒ‡ãƒ«ã‚’å°å…¥ã—ã¾ã™ã€‚ã“ã‚Œã¯ Bidirectional Encoder Representations ã®ç•¥ã§ã™
ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚ˆã‚Šã€‚æœ€è¿‘ã®è¨€èªè¡¨ç¾ãƒ¢ãƒ‡ãƒ«ã¨ã¯ç•°ãªã‚Šã€BERT ã¯æ·±ã„åŒæ–¹å‘æ€§ã‚’äº‹å‰ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã‚ˆã†ã«è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚
ã™ã¹ã¦ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å·¦ã¨å³ã®ä¸¡æ–¹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å…±åŒã§æ¡ä»¶ä»˜ã‘ã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€ãƒ©ãƒ™ãƒ«ã®ãªã„ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è¡¨ç¾ã—ã¾ã™ã€‚çµæœã¨ã—ã¦ã€
äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸ BERT ãƒ¢ãƒ‡ãƒ«ã¯ã€å‡ºåŠ›å±¤ã‚’ 1 ã¤è¿½åŠ ã™ã‚‹ã ã‘ã§å¾®èª¿æ•´ã—ã¦ã€æœ€å…ˆç«¯ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã§ãã¾ã™ã€‚
å®Ÿè³ªçš„ãªã‚¿ã‚¹ã‚¯å›ºæœ‰ã®ã‚‚ã®ã‚’å¿…è¦ã¨ã›ãšã€è³ªå•å¿œç­”ã‚„è¨€èªæ¨è«–ãªã©ã®å¹…åºƒã„ã‚¿ã‚¹ã‚¯ã«å¯¾å¿œ
ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å¤‰æ›´ã€‚*

*BERT ã¯æ¦‚å¿µçš„ã«ã¯ã‚·ãƒ³ãƒ—ãƒ«ã§ã™ãŒã€çµŒé¨“çš„ã«å¼·åŠ›ã§ã™ã€‚ 11 ã®è‡ªç„¶ãªè¦ç´ ã«é–¢ã™ã‚‹æ–°ã—ã„æœ€å…ˆç«¯ã®çµæœãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚
è¨€èªå‡¦ç†ã‚¿ã‚¹ã‚¯ï¼ˆGLUE ã‚¹ã‚³ã‚¢ã‚’ 80.5% ã«æŠ¼ã—ä¸Šã’ã‚‹ï¼ˆ7.7% ãƒã‚¤ãƒ³ãƒˆã®çµ¶å¯¾æ”¹å–„ï¼‰ã€MultiNLI ã‚’å«ã‚€ï¼‰
ç²¾åº¦ã¯ 86.7% (çµ¶å¯¾å€¤ 4.6% å‘ä¸Š)ã€SQuAD v1.1 è³ªå•å¿œç­”ãƒ†ã‚¹ãƒˆ F1 ã¯ 93.2 (çµ¶å¯¾å€¤ 1.5 ãƒã‚¤ãƒ³ãƒˆ)
æ”¹å–„) ãŠã‚ˆã³ SQuAD v2.0 ãƒ†ã‚¹ãƒˆ F1 ã‹ã‚‰ 83.1 (5.1 ãƒã‚¤ãƒ³ãƒˆã®çµ¶å¯¾æ”¹å–„)ã€‚*

## Usage tips

- BERT ã¯çµ¶å¯¾ä½ç½®åŸ‹ã‚è¾¼ã¿ã‚’å‚™ãˆãŸãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ãŸã‚ã€é€šå¸¸ã¯å…¥åŠ›ã‚’å³å´ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚
  å·¦ã€‚
- BERT ã¯ã€ãƒã‚¹ã‚¯è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚° (MLM) ãŠã‚ˆã³æ¬¡ã®æ–‡äºˆæ¸¬ (NSP) ã®ç›®æ¨™ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¾ã—ãŸã€‚ãã‚Œã¯
  ãƒã‚¹ã‚¯ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã®äºˆæ¸¬ã‚„ NLU ã§ã¯ä¸€èˆ¬ã«åŠ¹ç‡çš„ã§ã™ãŒã€ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã«ã¯æœ€é©ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
- ãƒ©ãƒ³ãƒ€ãƒ  ãƒã‚¹ã‚­ãƒ³ã‚°ã‚’ä½¿ç”¨ã—ã¦å…¥åŠ›ã‚’ç ´å£Šã—ã¾ã™ã€‚ã‚ˆã‚Šæ­£ç¢ºã«ã¯ã€äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ã€ãƒˆãƒ¼ã‚¯ãƒ³ã®æŒ‡å®šã•ã‚ŒãŸå‰²åˆ (é€šå¸¸ã¯ 15%) ãŒæ¬¡ã«ã‚ˆã£ã¦ãƒã‚¹ã‚¯ã•ã‚Œã¾ã™ã€‚

    * ç¢ºç‡0.8ã®ç‰¹åˆ¥ãªãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³
    * ç¢ºç‡ 0.1 ã§ãƒã‚¹ã‚¯ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã¨ã¯ç•°ãªã‚‹ãƒ©ãƒ³ãƒ€ãƒ ãªãƒˆãƒ¼ã‚¯ãƒ³
    * ç¢ºç‡ 0.1 ã®åŒã˜ãƒˆãƒ¼ã‚¯ãƒ³
    
- ãƒ¢ãƒ‡ãƒ«ã¯å…ƒã®æ–‡ã‚’äºˆæ¸¬ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ãŒã€2 ç•ªç›®ã®ç›®çš„ãŒã‚ã‚Šã¾ã™ã€‚å…¥åŠ›ã¯ 2 ã¤ã®æ–‡ A ã¨ B (é–“ã«åˆ†é›¢ãƒˆãƒ¼ã‚¯ãƒ³ã‚ã‚Š) ã§ã™ã€‚ç¢ºç‡ 50% ã§ã¯ã€æ–‡ã¯ã‚³ãƒ¼ãƒ‘ã‚¹å†…ã§é€£ç¶šã—ã¦ã„ã¾ã™ãŒã€æ®‹ã‚Šã® 50% ã§ã¯é–¢é€£æ€§ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ¢ãƒ‡ãƒ«ã¯ã€æ–‡ãŒé€£ç¶šã—ã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’äºˆæ¸¬ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚



ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ [thomwolf](https://huggingface.co/thomwolf) ã«ã‚ˆã£ã¦æä¾›ã•ã‚Œã¾ã—ãŸã€‚å…ƒã®ã‚³ãƒ¼ãƒ‰ã¯ [ã“ã¡ã‚‰](https://github.com/google-research/bert) ã«ã‚ã‚Šã¾ã™ã€‚

## Resources

BERT ã‚’å§‹ã‚ã‚‹ã®ã«å½¹ç«‹ã¤å…¬å¼ Hugging Face ãŠã‚ˆã³ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ (ğŸŒ ã§ç¤ºã•ã‚Œã‚‹) ãƒªã‚½ãƒ¼ã‚¹ã®ãƒªã‚¹ãƒˆã€‚ã“ã“ã«å«ã‚ã‚‹ãƒªã‚½ãƒ¼ã‚¹ã®é€ä¿¡ã«èˆˆå‘³ãŒã‚ã‚‹å ´åˆã¯ã€ãŠæ°—è»½ã«ãƒ—ãƒ« ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é–‹ã„ã¦ãã ã•ã„ã€‚å¯©æŸ»ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚ãƒªã‚½ãƒ¼ã‚¹ã¯ã€æ—¢å­˜ã®ãƒªã‚½ãƒ¼ã‚¹ã‚’è¤‡è£½ã™ã‚‹ã®ã§ã¯ãªãã€ä½•ã‹æ–°ã—ã„ã‚‚ã®ã‚’ç¤ºã™ã“ã¨ãŒç†æƒ³çš„ã§ã™ã€‚

<PipelineTag pipeline="text-classification"/>

- ã«é–¢ã™ã‚‹ãƒ–ãƒ­ã‚°æŠ•ç¨¿ [åˆ¥ã®è¨€èªã§ã® BERT ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡](https://www.philschmid.de/bert-text-classification-in-a-different-language)ã€‚
- [ãƒãƒ«ãƒãƒ©ãƒ™ãƒ« ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã®ãŸã‚ã® BERT (ãŠã‚ˆã³ãã®å‹äºº) ã®å¾®èª¿æ•´](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb) ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯.
-  æ–¹æ³•ã«é–¢ã™ã‚‹ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ [PyTorch ã‚’ä½¿ç”¨ã—ãŸãƒãƒ«ãƒãƒ©ãƒ™ãƒ«åˆ†é¡ã®ãŸã‚ã® BERT ã®å¾®èª¿æ•´](https://colab.research.google.com/github/abhmishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb)ã€‚ 
- æ–¹æ³•ã«é–¢ã™ã‚‹ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ [è¦ç´„ã®ãŸã‚ã« BERT ã‚’ä½¿ç”¨ã—ã¦ EncoderDecoder ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¦ã‚©ãƒ¼ãƒ ã‚¹ã‚¿ãƒ¼ãƒˆã™ã‚‹](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb)ã€‚
- [`BertForSequenceClassification`] ã¯ã€ã“ã® [ã‚µãƒ³ãƒ—ãƒ« ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification) ãŠã‚ˆã³ [ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)ã€‚
- [`TFBertForSequenceClassification`] ã¯ã€ã“ã® [ã‚µãƒ³ãƒ—ãƒ« ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification) ãŠã‚ˆã³ [ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb)ã€‚
- [`FlaxBertForSequenceClassification`] ã¯ã€ã“ã® [ã‚µãƒ³ãƒ—ãƒ« ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](https://github.com/huggingface/transformers/tree/main/examples/flax/text-classification) ãŠã‚ˆã³ [ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_flax.ipynb)ã€‚
- [ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰](../tasks/sequence_classification)

<PipelineTag pipeline="token-classification"/>

- [Hugging Face Transformers with Keras: Fine-tune a non-English BERT for Named Entity Recognition](https://www.philschmid.de/huggingface-transformers-keras-tf) ã®ä½¿ç”¨æ–¹æ³•ã«é–¢ã™ã‚‹ãƒ–ãƒ­ã‚°æŠ•ç¨¿ã€‚
- å„å˜èªã®æœ€åˆã®å˜èªéƒ¨åˆ†ã®ã¿ã‚’ä½¿ç”¨ã—ãŸ [å›ºæœ‰è¡¨ç¾èªè­˜ã®ãŸã‚ã® BERT ã®å¾®èª¿æ•´](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/Custom_Named_Entity_Recognition_with_BERT_only_first_wordpiece.ipynb) ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ä¸­ã®å˜èªãƒ©ãƒ™ãƒ«å†…ã€‚å˜èªã®ãƒ©ãƒ™ãƒ«ã‚’ã™ã¹ã¦ã®å˜èªéƒ¨åˆ†ã«ä¼æ’­ã™ã‚‹ã«ã¯ã€ä»£ã‚ã‚Šã«ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®ã“ã® [ãƒãƒ¼ã‚¸ãƒ§ãƒ³](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/BERT/Custom_Named_Entity_Recognition_with_BERT.ipynb) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
- [`BertForTokenClassification`] ã¯ã€ã“ã® [ã‚µãƒ³ãƒ—ãƒ« ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification) ãŠã‚ˆã³ [ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb)ã€‚
- [`TFBertForTokenClassification`] ã¯ã€ã“ã® [ã‚µãƒ³ãƒ—ãƒ« ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/token-classification) ãŠã‚ˆã³ [ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb)ã€‚
- [`FlaxBertForTokenClassification`] ã¯ã€ã“ã® [ã‚µãƒ³ãƒ—ãƒ« ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](https://github.com/huggingface/transformers/tree/main/examples/flax/token-classification) ã«ã‚ˆã£ã¦ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ã€‚
- [ãƒˆãƒ¼ã‚¯ãƒ³åˆ†é¡](https://huggingface.co/course/chapter7/2?fw=pt) ğŸ¤— ãƒã‚°ãƒ•ã‚§ã‚¤ã‚¹ã‚³ãƒ¼ã‚¹ã®ç« ã€‚
- [ãƒˆãƒ¼ã‚¯ãƒ³åˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰](../tasks/token_classification)

<PipelineTag pipeline="fill-mask"/>

- [`BertForMaskedLM`] ã¯ã€ã“ã® [ã‚µãƒ³ãƒ—ãƒ« ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#robertabertdistilbert-and-masked-language-modeling) ã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ãŠã‚Šã€ [ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)ã€‚
- [`TFBertForMaskedLM`] ã¯ã€ã“ã® [ã‚µãƒ³ãƒ—ãƒ« ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/lang-modeling#run_mlmpy) ãŠã‚ˆã³ [ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)ã€‚
- [`FlaxBertForMaskedLM`] ã¯ã€ã“ã® [ã‚µãƒ³ãƒ—ãƒ« ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#masked-language-modeling) ãŠã‚ˆã³ [ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯]( https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked_language_modeling_flax.ipynb)ã€‚
- [ãƒã‚¹ã‚¯ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°](https://huggingface.co/course/chapter7/3?fw=pt) ğŸ¤— é¡”ãƒã‚° ã‚³ãƒ¼ã‚¹ã®ç« ã€‚
- [ãƒã‚¹ã‚¯ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚° ã‚¿ã‚¹ã‚¯ ã‚¬ã‚¤ãƒ‰](../tasks/masked_lang_modeling)


<PipelineTag pipeline="question-answering"/>

- [`BertForQuestionAnswering`] ã¯ã€ã“ã® [ã‚µãƒ³ãƒ—ãƒ« ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) ãŠã‚ˆã³ [ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)ã€‚
- [`TFBertForQuestionAnswering`] ã¯ã€ã“ã® [ã‚µãƒ³ãƒ—ãƒ« ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering) ãŠã‚ˆã³ [ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb)ã€‚
- [`FlaxBertForQuestionAnswering`] ã¯ã€ã“ã® [ã‚µãƒ³ãƒ—ãƒ« ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](https://github.com/huggingface/transformers/tree/main/examples/flax/question-answering) ã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ã€‚
- [è³ªå•å›ç­”](https://huggingface.co/course/chapter7/7?fw=pt) ğŸ¤— ãƒã‚°ãƒ•ã‚§ã‚¤ã‚¹ã‚³ãƒ¼ã‚¹ã®ç« ã€‚
- [è³ªå•å›ç­”ã‚¿ã‚¹ã‚¯ ã‚¬ã‚¤ãƒ‰](../tasks/question_answering)

**è¤‡æ•°ã®é¸æŠè‚¢**
- [`BertForMultipleChoice`] ã¯ã€ã“ã® [ã‚µãƒ³ãƒ—ãƒ« ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) ãŠã‚ˆã³ [ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)ã€‚
- [`TFBertForMultipleChoice`] ã¯ã€ã“ã® [ã‚µãƒ³ãƒ—ãƒ« ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice) ãŠã‚ˆã³ [ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb)ã€‚
- [å¤šè‚¢é¸æŠã‚¿ã‚¹ã‚¯ ã‚¬ã‚¤ãƒ‰](../tasks/multiple_choice)

âš¡ï¸ **æ¨è«–**
- æ–¹æ³•ã«é–¢ã™ã‚‹ãƒ–ãƒ­ã‚°æŠ•ç¨¿  [Hugging Face Transformers ã¨ AWS Inferentia ã‚’ä½¿ç”¨ã—ã¦ BERT æ¨è«–ã‚’é«˜é€ŸåŒ–ã™ã‚‹](https://huggingface.co/blog/bert-inferentia-sagemaker)ã€‚
- æ–¹æ³•ã«é–¢ã™ã‚‹ãƒ–ãƒ­ã‚°æŠ•ç¨¿ [GPU ä¸Šã® DeepSpeed-Inference ã‚’ä½¿ç”¨ã—ã¦ BERT æ¨è«–ã‚’é«˜é€ŸåŒ–ã™ã‚‹](https://www.philschmid.de/bert-deepspeed-inference)ã€‚

âš™ï¸ **äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°**
- [Hugging Face Transformers ã¨ Habana Gaudi ã‚’ä½¿ç”¨ã—ãŸ BERT ã®äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ã«é–¢ã™ã‚‹ãƒ–ãƒ­ã‚°æŠ•ç¨¿](https://www.philschmid.de/pre-training-bert-habana)ã€‚

ğŸš€ **ãƒ‡ãƒ—ãƒ­ã‚¤**
- æ–¹æ³•ã«é–¢ã™ã‚‹ãƒ–ãƒ­ã‚°æŠ•ç¨¿  [ãƒã‚°ãƒ•ã‚§ã‚¤ã‚¹æœ€é©åŒ–ã§ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’ ONNX ã«å¤‰æ›ã™ã‚‹](https://www.philschmid.de/convert-transformers-to-onnx)ã€‚
- æ–¹æ³•ã«é–¢ã™ã‚‹ãƒ–ãƒ­ã‚°æŠ•ç¨¿ [AWS ä¸Šã® Habana Gaudi ã‚’ä½¿ç”¨ã—ãŸãƒã‚°é¡”ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®ãŸã‚ã®æ·±å±¤å­¦ç¿’ç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—](https://www.philschmid.de/getting-started-habana-gaudi#conclusion)ã€‚
- ã«é–¢ã™ã‚‹ãƒ–ãƒ­ã‚°æŠ•ç¨¿  [Hugging Face Transformersã€Amazon SageMakerã€ãŠã‚ˆã³ Terraform ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ãŸè‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° BERT](https://www.philschmid.de/terraform-huggingface-amazon-sagemaker-advanced)ã€‚
- ã«é–¢ã™ã‚‹ãƒ–ãƒ­ã‚°æŠ•ç¨¿  [HuggingFaceã€AWS Lambdaã€Docker ã‚’ä½¿ç”¨ã—ãŸã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹ BERT](https://www.philschmid.de/serverless-bert-with-huggingface-aws-lambda-docker)ã€‚
- ã«é–¢ã™ã‚‹ãƒ–ãƒ­ã‚°æŠ•ç¨¿ [Amazon SageMaker ã¨ Training Compiler ã‚’ä½¿ç”¨ã—ãŸ Hugging Face Transformers BERT å¾®èª¿æ•´](https://www.philschmid.de/huggingface-amazon-sagemaker-training-compiler)ã€‚
- ã«é–¢ã™ã‚‹ãƒ–ãƒ­ã‚°æŠ•ç¨¿  [Transformers ã¨ Amazon SageMaker ã‚’ä½¿ç”¨ã—ãŸ BERT ã®ã‚¿ã‚¹ã‚¯å›ºæœ‰ã®çŸ¥è­˜ã®è’¸ç•™](https://www.philschmid.de/knowledge-distillation-bert-transformers)

## BertConfig

[[autodoc]] BertConfig
    - all

## BertTokenizer

[[autodoc]] BertTokenizer
    - build_inputs_with_special_tokens
    - get_special_tokens_mask
    - create_token_type_ids_from_sequences
    - save_vocabulary

<frameworkcontent>
<pt>

## BertTokenizerFast

[[autodoc]] BertTokenizerFast

</pt>
<tf>

## TFBertTokenizer

[[autodoc]] TFBertTokenizer

</tf>
</frameworkcontent>

## Bert specific outputs

[[autodoc]] models.bert.modeling_bert.BertForPreTrainingOutput

[[autodoc]] models.bert.modeling_tf_bert.TFBertForPreTrainingOutput

[[autodoc]] models.bert.modeling_flax_bert.FlaxBertForPreTrainingOutput

<frameworkcontent>
<pt>

## BertModel

[[autodoc]] BertModel
    - forward

## BertForPreTraining

[[autodoc]] BertForPreTraining
    - forward

## BertLMHeadModel

[[autodoc]] BertLMHeadModel
    - forward

## BertForMaskedLM

[[autodoc]] BertForMaskedLM
    - forward

## BertForNextSentencePrediction

[[autodoc]] BertForNextSentencePrediction
    - forward

## BertForSequenceClassification

[[autodoc]] BertForSequenceClassification
    - forward

## BertForMultipleChoice

[[autodoc]] BertForMultipleChoice
    - forward

## BertForTokenClassification

[[autodoc]] BertForTokenClassification
    - forward

## BertForQuestionAnswering

[[autodoc]] BertForQuestionAnswering
    - forward

</pt>
<tf>

## TFBertModel

[[autodoc]] TFBertModel
    - call

## TFBertForPreTraining

[[autodoc]] TFBertForPreTraining
    - call

## TFBertModelLMHeadModel

[[autodoc]] TFBertLMHeadModel
    - call

## TFBertForMaskedLM

[[autodoc]] TFBertForMaskedLM
    - call

## TFBertForNextSentencePrediction

[[autodoc]] TFBertForNextSentencePrediction
    - call

## TFBertForSequenceClassification

[[autodoc]] TFBertForSequenceClassification
    - call

## TFBertForMultipleChoice

[[autodoc]] TFBertForMultipleChoice
    - call

## TFBertForTokenClassification

[[autodoc]] TFBertForTokenClassification
    - call

## TFBertForQuestionAnswering

[[autodoc]] TFBertForQuestionAnswering
    - call

</tf>
<jax>


## FlaxBertModel

[[autodoc]] FlaxBertModel
    - __call__

## FlaxBertForPreTraining

[[autodoc]] FlaxBertForPreTraining
    - __call__

## FlaxBertForCausalLM

[[autodoc]] FlaxBertForCausalLM
    - __call__

## FlaxBertForMaskedLM

[[autodoc]] FlaxBertForMaskedLM
    - __call__

## FlaxBertForNextSentencePrediction

[[autodoc]] FlaxBertForNextSentencePrediction
    - __call__

## FlaxBertForSequenceClassification

[[autodoc]] FlaxBertForSequenceClassification
    - __call__

## FlaxBertForMultipleChoice

[[autodoc]] FlaxBertForMultipleChoice
    - __call__

## FlaxBertForTokenClassification

[[autodoc]] FlaxBertForTokenClassification
    - __call__

## FlaxBertForQuestionAnswering

[[autodoc]] FlaxBertForQuestionAnswering
    - __call__

</jax>
</frameworkcontent>