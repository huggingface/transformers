<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Autoformer

## æ¦‚è¦

Autoformerãƒ¢ãƒ‡ãƒ«ã¯ã€ã€Œ[Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting](https://arxiv.org/abs/2106.13008)ã€ã¨ã„ã†è«–æ–‡ã§Haixu Wuã€Jiehui Xuã€Jianmin Wangã€Mingsheng Longã«ã‚ˆã£ã¦ææ¡ˆã•ã‚Œã¾ã—ãŸã€‚

ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€äºˆæ¸¬ãƒ—ãƒ­ã‚»ã‚¹ä¸­ã«ãƒˆãƒ¬ãƒ³ãƒ‰ã¨å­£ç¯€æ€§æˆåˆ†ã‚’é€æ¬¡çš„ã«åˆ†è§£ã§ãã‚‹æ·±å±¤åˆ†è§£ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ã—ã¦Transformerã‚’å¢—å¼·ã—ã¾ã™ã€‚

è«–æ–‡ã®è¦æ—¨ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š

*Extending the forecasting time is a critical demand for real applications, such as extreme weather early warning and long-term energy consumption planning. This paper studies the long-term forecasting problem of time series. Prior Transformer-based models adopt various self-attention mechanisms to discover the long-range dependencies. However, intricate temporal patterns of the long-term future prohibit the model from finding reliable dependencies. Also, Transformers have to adopt the sparse versions of point-wise self-attentions for long series efficiency, resulting in the information utilization bottleneck. Going beyond Transformers, we design Autoformer as a novel decomposition architecture with an Auto-Correlation mechanism. We break with the pre-processing convention of series decomposition and renovate it as a basic inner block of deep models. This design empowers Autoformer with progressive decomposition capacities for complex time series. Further, inspired by the stochastic process theory, we design the Auto-Correlation mechanism based on the series periodicity, which conducts the dependencies discovery and representation aggregation at the sub-series level. Auto-Correlation outperforms self-attention in both efficiency and accuracy. In long-term forecasting, Autoformer yields state-of-the-art accuracy, with a 38% relative improvement on six benchmarks, covering five practical applications: energy, traffic, economics, weather and disease.*

ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯[elisim](https://huggingface.co/elisim)ã¨[kashif](https://huggingface.co/kashif)ã‚ˆã‚Šæä¾›ã•ã‚Œã¾ã—ãŸã€‚
ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ã‚³ãƒ¼ãƒ‰ã¯[ã“ã¡ã‚‰](https://github.com/thuml/Autoformer)ã§è¦‹ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

## å‚è€ƒè³‡æ–™

Autoformerã®ä½¿ç”¨ã‚’é–‹å§‹ã™ã‚‹ã®ã«å½¹ç«‹ã¤å…¬å¼ã®Hugging FaceãŠã‚ˆã³ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ï¼ˆğŸŒã§ç¤ºã•ã‚Œã¦ã„ã‚‹ï¼‰ã®å‚è€ƒè³‡æ–™ã®ä¸€è¦§ã§ã™ã€‚ã“ã“ã«å‚è€ƒè³‡æ–™ã‚’æå‡ºã—ãŸã„å ´åˆã¯ã€æ°—å…¼ã­ãªãPull Requestã‚’é–‹ã„ã¦ãã ã•ã„ã€‚ç§ãŸã¡ã¯ãã‚Œã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã„ãŸã—ã¾ã™ï¼å‚è€ƒè³‡æ–™ã¯ã€æ—¢å­˜ã®ã‚‚ã®ã‚’è¤‡è£½ã™ã‚‹ã®ã§ã¯ãªãã€ä½•ã‹æ–°ã—ã„ã“ã¨ã‚’ç¤ºã™ã“ã¨ãŒç†æƒ³çš„ã§ã™ã€‚

HuggingFaceã®ãƒ–ãƒ­ã‚°ã§Autoformerã«é–¢ã™ã‚‹ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ãã ã•ã„ï¼šã¯ã„ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¯æ™‚ç³»åˆ—äºˆæ¸¬ã«åŠ¹æœçš„ã§ã™ï¼ˆ+ Autoformerï¼‰
- HuggingFaceãƒ–ãƒ­ã‚°ã§Autoformerã«é–¢ã™ã‚‹ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ãã ã•ã„ï¼š[ã¯ã„ã€Transformersã¯æ™‚ç³»åˆ—äºˆæ¸¬ã«åŠ¹æœçš„ã§ã™ï¼ˆ+ Autoformerï¼‰](https://huggingface.co/blog/autoformer)

## AutoformerConfig

[[autodoc]] AutoformerConfig

## AutoformerModel

[[autodoc]] AutoformerModel
    - forward

## AutoformerForPrediction

[[autodoc]] AutoformerForPrediction
    - forward
