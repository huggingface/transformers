<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Image captioning

[[open-in-colab]]

ç”»åƒã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ä»˜ã‘ã¯ã€ç‰¹å®šã®ç”»åƒã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’äºˆæ¸¬ã™ã‚‹ã‚¿ã‚¹ã‚¯ã§ã™ã€‚ä¸€èˆ¬çš„ãªç¾å®Ÿä¸–ç•Œã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ã¯æ¬¡ã®ã‚‚ã®ãŒã‚ã‚Šã¾ã™ã€‚
è¦–è¦šéšœå®³è€…ãŒã•ã¾ã–ã¾ãªçŠ¶æ³ã‚’ä¹—ã‚Šè¶Šãˆã‚‰ã‚Œã‚‹ã‚ˆã†æ”¯æ´ã—ã¾ã™ã€‚ã—ãŸãŒã£ã¦ã€ç”»åƒã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³
ç”»åƒã‚’èª¬æ˜ã™ã‚‹ã“ã¨ã§äººã€…ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¸ã®ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ã‚’å‘ä¸Šã•ã›ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€æ¬¡ã®æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚

* ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚
* å¾®èª¿æ•´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’æ¨è«–ã«ä½¿ç”¨ã—ã¾ã™ã€‚

å§‹ã‚ã‚‹å‰ã«ã€å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã™ã¹ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

```bash
pip install transformers datasets evaluate -q
pip install jiwer -q
```

ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¨å…±æœ‰ã§ãã‚‹ã‚ˆã†ã«ã€Hugging Face ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«ãƒ­ã‚°ã‚¤ãƒ³ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¡¨ç¤ºã•ã‚ŒãŸã‚‰ã€ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã™ã€‚


```python
from huggingface_hub import notebook_login

notebook_login()
```

## Load the PokÃ©mon BLIP captions dataset

ğŸ¤— ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦ã€{image-caption} ãƒšã‚¢ã§æ§‹æˆã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚ç‹¬è‡ªã®ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã™ã‚‹ã«ã¯
PyTorch ã§ã¯ã€[ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/GIT/Fine_tune_GIT_on_an_image_captioning_dataset.ipynb) ã‚’å‚ç…§ã§ãã¾ã™ã€‚

```py
ds = load_dataset("lambdalabs/pokemon-blip-captions")
ds
```

```bash
DatasetDict({
    train: Dataset({
        features: ['image', 'text'],
        num_rows: 833
    })
})
```

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯ `image`ã¨`text`ã® 2 ã¤ã®æ©Ÿèƒ½ãŒã‚ã‚Šã¾ã™ã€‚

<Tip>

å¤šãã®ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯ã€ç”»åƒã”ã¨ã«è¤‡æ•°ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚ã“ã®ã‚ˆã†ãªå ´åˆã€ä¸€èˆ¬çš„ãªæˆ¦ç•¥ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«åˆ©ç”¨å¯èƒ½ãªã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®ä¸­ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã™ã€‚

</Tip>

[`~datasets.Dataset.train_test_split`] ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒˆãƒ¬ã‚¤ãƒ³ ã‚¹ãƒ—ãƒªãƒƒãƒˆã‚’ãƒˆãƒ¬ã‚¤ãƒ³ ã‚»ãƒƒãƒˆã¨ãƒ†ã‚¹ãƒˆ ã‚»ãƒƒãƒˆã«åˆ†å‰²ã—ã¾ã™ã€‚

```python
ds = ds["train"].train_test_split(test_size=0.1)
train_ds = ds["train"]
test_ds = ds["test"]
```

ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ã‚»ãƒƒãƒˆã‹ã‚‰ã®ã„ãã¤ã‹ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’è¦–è¦šåŒ–ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```python
from textwrap import wrap
import matplotlib.pyplot as plt
import numpy as np


def plot_images(images, captions):
    plt.figure(figsize=(20, 20))
    for i in range(len(images)):
        ax = plt.subplot(1, len(images), i + 1)
        caption = captions[i]
        caption = "\n".join(wrap(caption, 12))
        plt.title(caption)
        plt.imshow(images[i])
        plt.axis("off")


sample_images_to_visualize = [np.array(train_ds[i]["image"]) for i in range(5)]
sample_captions = [train_ds[i]["text"] for i in range(5)]
plot_images(sample_images_to_visualize, sample_captions)
```
    
<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_training_images_image_cap.png" alt="Sample training images"/>
</div>

## Preprocess the dataset

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯ 2 ã¤ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ (ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆ) ãŒã‚ã‚‹ãŸã‚ã€å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯ç”»åƒã¨ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’å‰å‡¦ç†ã—ã¾ã™ã€‚

ã“ã‚Œã‚’è¡Œã†ã«ã¯ã€å¾®èª¿æ•´ã—ã‚ˆã†ã¨ã—ã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã«é–¢é€£ä»˜ã‘ã‚‰ã‚ŒãŸãƒ—ãƒ­ã‚»ãƒƒã‚µ ã‚¯ãƒ©ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

```python
from transformers import AutoProcessor

checkpoint = "microsoft/git-base"
processor = AutoProcessor.from_pretrained(checkpoint)
```


ãƒ—ãƒ­ã‚»ãƒƒã‚µã¯å†…éƒ¨ã§ç”»åƒã‚’å‰å‡¦ç†ã— (ã‚µã‚¤ã‚ºå¤‰æ›´ã‚„ãƒ”ã‚¯ã‚»ãƒ« ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’å«ã‚€)ã€ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¾ã™ã€‚

```python
def transforms(example_batch):
    images = [x for x in example_batch["image"]]
    captions = [x for x in example_batch["text"]]
    inputs = processor(images=images, text=captions, padding="max_length")
    inputs.update({"labels": inputs["input_ids"]})
    return inputs


train_ds.set_transform(transforms)
test_ds.set_transform(transforms)
```

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ãŒã§ããŸã‚‰ã€å¾®èª¿æ•´ç”¨ã«ãƒ¢ãƒ‡ãƒ«ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã§ãã¾ã™ã€‚

## Load a base model

["microsoft/git-base"](https://huggingface.co/microsoft/git-base) ã‚’ [`AutoModelForCausalLM`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM) ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚

```python
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(checkpoint)
```

```python
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(checkpoint)
```
## Evaluate

ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ ãƒ¢ãƒ‡ãƒ«ã¯é€šå¸¸ã€[Rouge Score](https://huggingface.co/spaces/evaluate-metric/rouge) ã¾ãŸã¯ [Word Error Rate](https://huggingface.co/spaces/evaluate-metric/) ã§è©•ä¾¡ã•ã‚Œã¾ã™ã€‚ãã†ã ã£ãŸï¼‰ã€‚ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€Word Error Rate (WER) ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

ã“ã‚Œã‚’è¡Œã†ã«ã¯ ğŸ¤— Evaluate ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ WER ã®æ½œåœ¨çš„ãªåˆ¶é™ã‚„ãã®ä»–ã®å•é¡Œç‚¹ã«ã¤ã„ã¦ã¯ã€[ã“ã®ã‚¬ã‚¤ãƒ‰](https://huggingface.co/spaces/evaluate-metric/wer) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

```python
from evaluate import load
import torch

wer = load("wer")


def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predicted = logits.argmax(-1)
    decoded_labels = processor.batch_decode(labels, skip_special_tokens=True)
    decoded_predictions = processor.batch_decode(predicted, skip_special_tokens=True)
    wer_score = wer.compute(predictions=decoded_predictions, references=decoded_labels)
    return {"wer_score": wer_score}
```

## Train!

ã“ã‚Œã§ã€ãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã‚’é–‹å§‹ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚ã“ã‚Œã«ã¯ ğŸ¤— [`Trainer`] ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

ã¾ãšã€[`TrainingArguments`] ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¼•æ•°ã‚’å®šç¾©ã—ã¾ã™ã€‚

```python
from transformers import TrainingArguments, Trainer

model_name = checkpoint.split("/")[1]

training_args = TrainingArguments(
    output_dir=f"{model_name}-pokemon",
    learning_rate=5e-5,
    num_train_epochs=50,
    fp16=True,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    gradient_accumulation_steps=2,
    save_total_limit=3,
    evaluation_strategy="steps",
    eval_steps=50,
    save_strategy="steps",
    save_steps=50,
    logging_steps=50,
    remove_unused_columns=False,
    push_to_hub=True,
    label_names=["labels"],
    load_best_model_at_end=True,
)
```

Trainer æ¬¡ã«ã€æ¬¡ã«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã¨ä¸€ç·’ã« ğŸ¤— ã«æ¸¡ã—ã¾ã™ã€‚

```python
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=test_ds,
    compute_metrics=compute_metrics,
)
```

ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹ã«ã¯ã€[`Trainer`] ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã® [`~Trainer.train`] ã‚’å‘¼ã³å‡ºã™ã ã‘ã§ã™ã€‚

```python 
trainer.train()
```

ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒé€²ã‚€ã«ã¤ã‚Œã¦ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®æå¤±ãŒã‚¹ãƒ ãƒ¼ã‚ºã«æ¸›å°‘ã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚

ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã—ãŸã‚‰ã€ [`~Trainer.push_to_hub`] ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ–ã«å…±æœ‰ã—ã€èª°ã‚‚ãŒãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

```python
trainer.push_to_hub()
```

## Inference

`test_ds` ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã‚’å–å¾—ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚

```python
from PIL import Image
import requests

url = "https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/pokemon.png"
image = Image.open(requests.get(url, stream=True).raw)
image
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/test_image_image_cap.png" alt="Test image"/>
</div>

ãƒ¢ãƒ‡ãƒ«ç”¨ã®ç”»åƒã‚’æº–å‚™ã—ã¾ã™ã€‚

```python
device = "cuda" if torch.cuda.is_available() else "cpu"

inputs = processor(images=image, return_tensors="pt").to(device)
pixel_values = inputs.pixel_values
```

[`generate`] ã‚’å‘¼ã³å‡ºã—ã¦äºˆæ¸¬ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

```python
generated_ids = model.generate(pixel_values=pixel_values, max_length=50)
generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(generated_caption)
```
```bash
a drawing of a pink and blue pokemon
```

å¾®èª¿æ•´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚Šã€éå¸¸ã«å„ªã‚ŒãŸã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãŒç”Ÿæˆã•ã‚ŒãŸã‚ˆã†ã§ã™ã€‚




