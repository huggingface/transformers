<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Document Question Answering

[[open-in-colab]]

æ–‡æ›¸ã«ã‚ˆã‚‹è³ªå•å¿œç­”ã¯ã€æ–‡æ›¸ã«ã‚ˆã‚‹è¦–è¦šçš„ãªè³ªå•å¿œç­”ã¨ã‚‚å‘¼ã°ã‚Œã€ä»¥ä¸‹ã‚’æä¾›ã™ã‚‹ã‚¿ã‚¹ã‚¯ã§ã™ã€‚
ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”»åƒã«é–¢ã™ã‚‹è³ªå•ã¸ã®å›žç­”ã€‚ã“ã®ã‚¿ã‚¹ã‚¯ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã¸ã®å…¥åŠ›ã¯é€šå¸¸ã€ç”»åƒã¨ç”»åƒã®çµ„ã¿åˆã‚ã›ã§ã™ã€‚
è³ªå•ãŒã‚ã‚Šã€å‡ºåŠ›ã¯è‡ªç„¶è¨€èªžã§è¡¨ç¾ã•ã‚ŒãŸå›žç­”ã§ã™ã€‚ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ä»¥ä¸‹ã‚’å«ã‚€è¤‡æ•°ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’åˆ©ç”¨ã—ã¾ã™ã€‚
ãƒ†ã‚­ã‚¹ãƒˆã€å˜èªžã®ä½ç½® (å¢ƒç•Œãƒœãƒƒã‚¯ã‚¹)ã€ãŠã‚ˆã³ç”»åƒè‡ªä½“ã€‚

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€æ¬¡ã®æ–¹æ³•ã‚’èª¬æ˜Žã—ã¾ã™ã€‚

- [DocVQA ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ](https://huggingface.co/datasets/nielsr/docvqa_1200_examples_donut) ã® [LayoutLMv2](../model_doc/layoutlmv2) ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚
- å¾®èª¿æ•´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’æŽ¨è«–ã«ä½¿ç”¨ã—ã¾ã™ã€‚

> [!TIP]
> ã“ã®ã‚¿ã‚¹ã‚¯ã¨äº’æ›æ€§ã®ã‚ã‚‹ã™ã¹ã¦ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ç¢ºèªã™ã‚‹ã«ã¯ã€[ã‚¿ã‚¹ã‚¯ãƒšãƒ¼ã‚¸](https://huggingface.co/tasks/image-to-text) ã‚’ç¢ºèªã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚

LayoutLMv2 ã¯ã€æœ€å¾Œã®éžè¡¨ç¤ºã®ãƒ˜ãƒƒãƒ€ãƒ¼ã®ä¸Šã«è³ªå•å¿œç­”ãƒ˜ãƒƒãƒ‰ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã§ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®è³ªå•å¿œç­”ã‚¿ã‚¹ã‚¯ã‚’è§£æ±ºã—ã¾ã™ã€‚
ãƒˆãƒ¼ã‚¯ãƒ³ã®çŠ¶æ…‹ã‚’èª¿ã¹ã¦ã€ãƒˆãƒ¼ã‚¯ãƒ³ã®é–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³ã¨çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³ã®ä½ç½®ã‚’äºˆæ¸¬ã—ã¾ã™ã€‚
ç­”ãˆã€‚è¨€ã„æ›ãˆã‚Œã°ã€å•é¡Œã¯æŠ½å‡ºçš„è³ªå•å¿œç­”ã¨ã—ã¦æ‰±ã‚ã‚Œã¾ã™ã€‚ã¤ã¾ã‚Šã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è€ƒæ…®ã—ã¦ã€ã©ã®éƒ¨åˆ†ã‚’æŠ½å‡ºã™ã‚‹ã‹ã¨ã„ã†ã“ã¨ã§ã™ã€‚
ã®æƒ…å ±ãŒè³ªå•ã«ç­”ãˆã¾ã™ã€‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¯ OCR ã‚¨ãƒ³ã‚¸ãƒ³ã®å‡ºåŠ›ã‹ã‚‰å–å¾—ã•ã‚Œã¾ã™ã€‚ã“ã“ã§ã¯ Google ã® Tesseract ã§ã™ã€‚

å§‹ã‚ã‚‹å‰ã«ã€å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã™ã¹ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ LayoutLMv2 ã¯ detectron2ã€torchvisionã€tesseract ã«ä¾å­˜ã—ã¾ã™ã€‚

```bash
pip install -q transformers datasets
```

```bash
pip install 'git+https://github.com/facebookresearch/detectron2.git'
pip install torchvision
```

```bash
sudo apt install tesseract-ocr
pip install -q pytesseract
```

ã™ã¹ã¦ã®ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ãŸã‚‰ã€ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’å†èµ·å‹•ã—ã¾ã™ã€‚

ãƒ¢ãƒ‡ãƒ«ã‚’ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¨å…±æœ‰ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ Hugging Face ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ã€ðŸ¤— ãƒãƒ–ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¡¨ç¤ºã•ã‚ŒãŸã‚‰ã€ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã™ã€‚

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

ã„ãã¤ã‹ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‚’å®šç¾©ã—ã¾ã—ã‚‡ã†ã€‚

```py
>>> model_checkpoint = "microsoft/layoutlmv2-base-uncased"
>>> batch_size = 4
```

## Load the data

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€ðŸ¤— Hub ã«ã‚ã‚‹å‰å‡¦ç†ã•ã‚ŒãŸ DocVQA ã®å°ã•ãªã‚µãƒ³ãƒ—ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ãƒ•ãƒ«ã«ä½¿ã„ãŸã„å ´åˆã¯ã€
DocVQA ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€[DocVQA ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸](https://rrc.cvc.uab.es/?ch=17) ã§ç™»éŒ²ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚ãã†ã™ã‚Œã°ã€
ã“ã®ã‚¬ã‚¤ãƒ‰ã‚’é€²ã‚ã¦ã€[ðŸ¤— ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹æ–¹æ³•](https://huggingface.co/docs/datasets/loading#local-and-remote-files) ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚


```py
>>> from datasets import load_dataset

>>> dataset = load_dataset("nielsr/docvqa_1200_examples")
>>> dataset
DatasetDict({
    train: Dataset({
        features: ['id', 'image', 'query', 'answers', 'words', 'bounding_boxes', 'answer'],
        num_rows: 1000
    })
    test: Dataset({
        features: ['id', 'image', 'query', 'answers', 'words', 'bounding_boxes', 'answer'],
        num_rows: 200
    })
})
```

ã”è¦§ã®ã¨ãŠã‚Šã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã™ã§ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ã‚»ãƒƒãƒˆã¨ãƒ†ã‚¹ãƒˆ ã‚»ãƒƒãƒˆã«åˆ†å‰²ã•ã‚Œã¦ã„ã¾ã™ã€‚ç†è§£ã™ã‚‹ãŸã‚ã«ãƒ©ãƒ³ãƒ€ãƒ ãªä¾‹ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†
æ©Ÿèƒ½ã‚’å‚™ãˆãŸè‡ªåˆ†è‡ªèº«ã€‚

```py
>>> dataset["train"].features
```

å€‹ã€…ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒè¡¨ã™å†…å®¹ã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ã€‚
* `id`: ã‚µãƒ³ãƒ—ãƒ«ã®ID
* `image`: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”»åƒã‚’å«ã‚€ PIL.Image.Image ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
* `query`: è³ªå•æ–‡å­—åˆ— - ã„ãã¤ã‹ã®è¨€èªžã§ã®è‡ªç„¶è¨€èªžã«ã‚ˆã‚‹è³ªå•
* `answers`: ãƒ’ãƒ¥ãƒ¼ãƒžãƒ³ ã‚¢ãƒŽãƒ†ãƒ¼ã‚¿ãƒ¼ã«ã‚ˆã£ã¦æä¾›ã•ã‚ŒãŸæ­£è§£ã®ãƒªã‚¹ãƒˆ
* `words` ã¨ `bounding_boxes`: OCR ã®çµæžœã€‚ã“ã“ã§ã¯ä½¿ç”¨ã—ã¾ã›ã‚“ã€‚
* `answer`: åˆ¥ã®ãƒ¢ãƒ‡ãƒ«ã¨ä¸€è‡´ã™ã‚‹ç­”ãˆã€‚ã“ã“ã§ã¯ä½¿ç”¨ã—ã¾ã›ã‚“ã€‚

è‹±èªžã®è³ªå•ã ã‘ã‚’æ®‹ã—ã€åˆ¥ã®ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹äºˆæ¸¬ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã¨æ€ã‚ã‚Œã‚‹`answer`æ©Ÿèƒ½ã‚’å‰Šé™¤ã—ã¾ã—ã‚‡ã†ã€‚
ã¾ãŸã€ã‚¢ãƒŽãƒ†ãƒ¼ã‚¿ãƒ¼ã«ã‚ˆã£ã¦æä¾›ã•ã‚ŒãŸã‚»ãƒƒãƒˆã‹ã‚‰æœ€åˆã®å›žç­”ã‚’å–å¾—ã—ã¾ã™ã€‚ã‚ã‚‹ã„ã¯ã€ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚

```py
>>> updated_dataset = dataset.map(lambda example: {"question": example["query"]["en"]}, remove_columns=["query"])
>>> updated_dataset = updated_dataset.map(
...     lambda example: {"answer": example["answers"][0]}, remove_columns=["answer", "answers"]
... )
```

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ä½¿ç”¨ã™ã‚‹ LayoutLMv2 ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¯ã€`max_position_embeddings = 512` ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ (
ã“ã®æƒ…å ±ã¯ã€[ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã® `config.json` ãƒ•ã‚¡ã‚¤ãƒ«](https://huggingface.co/microsoft/layoutlmv2-base-uncased/blob/main/config.json#L18)) ã§è¦‹ã¤ã‘ã¦ãã ã•ã„ã€‚
ä¾‹ã‚’çœç•¥ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ãŒã€ç­”ãˆãŒå¤§ããªæ–‡æ›¸ã®æœ€å¾Œã«ã‚ã‚Šã€çµå±€çœç•¥ã•ã‚Œã¦ã—ã¾ã†ã¨ã„ã†çŠ¶æ³ã‚’é¿ã‘ã‚‹ãŸã‚ã«ã€
ã“ã“ã§ã¯ã€åŸ‹ã‚è¾¼ã¿ãŒ 512 ã‚’è¶…ãˆã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã„ãã¤ã‹ã®ä¾‹ã‚’å‰Šé™¤ã—ã¾ã™ã€‚
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®ã»ã¨ã‚“ã©ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒé•·ã„å ´åˆã¯ã€ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚° ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æˆ¦ç•¥ã‚’å®Ÿè£…ã§ãã¾ã™ã€‚è©³ç´°ã«ã¤ã„ã¦ã¯ã€[ã“ã®ãƒŽãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://github.com/huggingface/notebooks/blob/main/examples/question_answering.ipynb) ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ ã€‚

```py
>>> updated_dataset = updated_dataset.filter(lambda x: len(x["words"]) + len(x["question"].split()) < 512)
```

ã“ã®æ™‚ç‚¹ã§ã€ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ OCR æ©Ÿèƒ½ã‚‚å‰Šé™¤ã—ã¾ã—ã‚‡ã†ã€‚ã“ã‚Œã‚‰ã¯ã€ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’å¾®èª¿æ•´ã™ã‚‹ãŸã‚ã® OCR ã®çµæžœã§ã™ã€‚
ãƒ¢ãƒ‡ãƒ«ã€‚ã“ã‚Œã‚‰ã¯å…¥åŠ›è¦ä»¶ã¨ä¸€è‡´ã—ãªã„ãŸã‚ã€ä½¿ç”¨ã—ãŸã„å ´åˆã¯ã•ã‚‰ã«å‡¦ç†ãŒå¿…è¦ã«ãªã‚Šã¾ã™ã€‚
ã“ã®ã‚¬ã‚¤ãƒ‰ã§ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ã€‚ä»£ã‚ã‚Šã«ã€OCR ã¨ OCR ã®ä¸¡æ–¹ã®å…ƒã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ [`LayoutLMv2Processor`] ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚
ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã€‚ã“ã®ã‚ˆã†ã«ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®äºˆæƒ³ã•ã‚Œã‚‹å…¥åŠ›ã¨ä¸€è‡´ã™ã‚‹å…¥åŠ›ã‚’å–å¾—ã—ã¾ã™ã€‚ç”»åƒã‚’æ‰‹å‹•ã§åŠ å·¥ã—ãŸã„å ´åˆã¯ã€
ãƒ¢ãƒ‡ãƒ«ãŒã©ã®ã‚ˆã†ãªå…¥åŠ›å½¢å¼ã‚’æƒ³å®šã—ã¦ã„ã‚‹ã‹ã‚’çŸ¥ã‚‹ã«ã¯ã€[`LayoutLMv2` ãƒ¢ãƒ‡ãƒ«ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](../model_doc/layoutlmv2) ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

```py
>>> updated_dataset = updated_dataset.remove_columns("words")
>>> updated_dataset = updated_dataset.remove_columns("bounding_boxes")
```

æœ€å¾Œã«ã€ç”»åƒã‚µãƒ³ãƒ—ãƒ«ã‚’ç¢ºèªã—ãªã„ã¨ãƒ‡ãƒ¼ã‚¿æŽ¢ç´¢ã¯å®Œäº†ã—ã¾ã›ã‚“ã€‚


```py
>>> updated_dataset["train"][11]["image"]
```

<div class="flex justify-center">
     <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/docvqa_example.jpg" alt="DocVQA Image Example"/>
 </div>

## Preprocess the data

æ–‡æ›¸ã®è³ªå•ã«ç­”ãˆã‚‹ã‚¿ã‚¹ã‚¯ã¯ãƒžãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ« ã‚¿ã‚¹ã‚¯ã§ã‚ã‚‹ãŸã‚ã€å„ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‹ã‚‰ã®å…¥åŠ›ãŒç¢ºå®Ÿã«è¡Œã‚ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
ãƒ¢ãƒ‡ãƒ«ã®æœŸå¾…ã«å¾“ã£ã¦å‰å‡¦ç†ã•ã‚Œã¾ã™ã€‚ã¾ãšã€[`LayoutLMv2Processor`] ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚ã“ã‚Œã¯ã€ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã§ãã‚‹ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã¨ãƒ†ã‚­ã‚¹ãƒˆ ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã§ãã‚‹ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’å†…éƒ¨ã§çµ„ã¿åˆã‚ã›ã¦ã„ã¾ã™ã€‚

```py
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained(model_checkpoint)
```

### Preprocessing document images

ã¾ãšã€ãƒ—ãƒ­ã‚»ãƒƒã‚µã‹ã‚‰ã® `image_processor` ã‚’åˆ©ç”¨ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”»åƒã‚’æº–å‚™ã—ã¾ã—ã‚‡ã†ã€‚
ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã¯ç”»åƒã®ã‚µã‚¤ã‚ºã‚’ 224x224 ã«å¤‰æ›´ã—ã€ã‚«ãƒ©ãƒ¼ ãƒãƒ£ãƒãƒ«ã®é †åºãŒæ­£ã—ã„ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚
tesseract ã‚’ä½¿ç”¨ã—ã¦ OCR ã‚’é©ç”¨ã—ã€å˜èªžã¨æ­£è¦åŒ–ã•ã‚ŒãŸå¢ƒç•Œãƒœãƒƒã‚¯ã‚¹ã‚’å–å¾—ã—ã¾ã™ã€‚ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€ã“ã‚Œã‚‰ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã™ã¹ã¦ã€ã¾ã•ã«å¿…è¦ãªã‚‚ã®ã§ã™ã€‚
ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ç”»åƒå‡¦ç†ã‚’ç”»åƒã®ãƒãƒƒãƒã«é©ç”¨ã—ã€OCR ã®çµæžœã‚’è¿”ã™é–¢æ•°ã‚’ä½œæˆã—ã¾ã™ã€‚

```py
>>> image_processor = processor.image_processor


>>> def get_ocr_words_and_boxes(examples):
...     images = [image.convert("RGB") for image in examples["image"]]
...     encoded_inputs = image_processor(images)

...     examples["image"] = encoded_inputs.pixel_values
...     examples["words"] = encoded_inputs.words
...     examples["boxes"] = encoded_inputs.boxes

...     return examples
```

ã“ã®å‰å‡¦ç†ã‚’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã«é«˜é€Ÿã«é©ç”¨ã™ã‚‹ã«ã¯ã€[`~datasets.Dataset.map`] ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

```py
>>> dataset_with_ocr = updated_dataset.map(get_ocr_words_and_boxes, batched=True, batch_size=2)
```

### Preprocessing text data

ç”»åƒã« OCR ã‚’é©ç”¨ã—ãŸã‚‰ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦ãƒ¢ãƒ‡ãƒ«ç”¨ã«æº–å‚™ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
ã“ã‚Œã«ã¯ã€å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã§å–å¾—ã—ãŸå˜èªžã¨ãƒœãƒƒã‚¯ã‚¹ã‚’ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ã® `input_ids`ã€`attention_mask`ã€
`token_type_ids`ã¨`bbox`ã€‚ãƒ†ã‚­ã‚¹ãƒˆã‚’å‰å‡¦ç†ã™ã‚‹ã«ã¯ã€ãƒ—ãƒ­ã‚»ãƒƒã‚µã‹ã‚‰ã®`Tokenizer`ãŒå¿…è¦ã«ãªã‚Šã¾ã™ã€‚

```py
>>> tokenizer = processor.tokenizer
```

å‰è¿°ã®å‰å‡¦ç†ã«åŠ ãˆã¦ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ©ãƒ™ãƒ«ã‚’è¿½åŠ ã™ã‚‹å¿…è¦ã‚‚ã‚ã‚Šã¾ã™ã€‚ `xxxForQuestionAnswering` ãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
ðŸ¤— Transformers ã§ã¯ã€ãƒ©ãƒ™ãƒ«ã¯ `start_positions` ã¨ `end_positions` ã§æ§‹æˆã•ã‚Œã€ã©ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒãã®ä½ç½®ã«ã‚ã‚‹ã‹ã‚’ç¤ºã—ã¾ã™ã€‚
é–‹å§‹ç‚¹ã¨ã€ã©ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒå›žç­”ã®æœ€å¾Œã«ã‚ã‚‹ã‹ã€‚

ãã‚Œã‹ã‚‰å§‹ã‚ã¾ã—ã‚‡ã†ã€‚ã‚ˆã‚Šå¤§ããªãƒªã‚¹ãƒˆ (å˜èªžãƒªã‚¹ãƒˆ) å†…ã®ã‚µãƒ–ãƒªã‚¹ãƒˆ (å˜èªžã«åˆ†å‰²ã•ã‚ŒãŸå›žç­”) ã‚’æ¤œç´¢ã§ãã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã‚’å®šç¾©ã—ã¾ã™ã€‚

ã“ã®é–¢æ•°ã¯ã€`words_list` ã¨ `answer_list` ã¨ã„ã† 2 ã¤ã®ãƒªã‚¹ãƒˆã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å–ã‚Šã¾ã™ã€‚æ¬¡ã«ã€`words_list`ã‚’åå¾©å‡¦ç†ã—ã¦ãƒã‚§ãƒƒã‚¯ã—ã¾ã™ã€‚
`words_list` (words_list[i]) å†…ã®ç¾åœ¨ã®å˜èªžãŒã€answer_list (answer_list[0]) ã®æœ€åˆã®å˜èªžã¨ç­‰ã—ã„ã‹ã©ã†ã‹ã€ãŠã‚ˆã³
ç¾åœ¨ã®å˜èªžã‹ã‚‰å§‹ã¾ã‚Šã€`answer_list` ã¨åŒã˜é•·ã•ã® `words_list` ã®ã‚µãƒ–ãƒªã‚¹ãƒˆã¯ã€`to answer_list` ã¨ç­‰ã—ããªã‚Šã¾ã™ã€‚
ã“ã®æ¡ä»¶ãŒ true ã®å ´åˆã€ä¸€è‡´ãŒè¦‹ã¤ã‹ã£ãŸã“ã¨ã‚’æ„å‘³ã—ã€é–¢æ•°ã¯ä¸€è‡´ã¨ãã®é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ (idx) ã‚’è¨˜éŒ²ã—ã¾ã™ã€‚
ã¨ãã®çµ‚äº†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ (idx + len(answer_list) - 1)ã€‚è¤‡æ•°ã®ä¸€è‡´ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã€é–¢æ•°ã¯æœ€åˆã®ã‚‚ã®ã®ã¿ã‚’è¿”ã—ã¾ã™ã€‚
ä¸€è‡´ã™ã‚‹ã‚‚ã®ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€é–¢æ•°ã¯ (`None`ã€0ã€ãŠã‚ˆã³ 0) ã‚’è¿”ã—ã¾ã™ã€‚

```py
>>> def subfinder(words_list, answer_list):
...     matches = []
...     start_indices = []
...     end_indices = []
...     for idx, i in enumerate(range(len(words_list))):
...         if words_list[i] == answer_list[0] and words_list[i : i + len(answer_list)] == answer_list:
...             matches.append(answer_list)
...             start_indices.append(idx)
...             end_indices.append(idx + len(answer_list) - 1)
...     if matches:
...         return matches[0], start_indices[0], end_indices[0]
...     else:
...         return None, 0, 0
```

ã“ã®é–¢æ•°ãŒç­”ãˆã®ä½ç½®ã‚’è¦‹ã¤ã‘ã‚‹æ–¹æ³•ã‚’èª¬æ˜Žã™ã‚‹ãŸã‚ã«ã€ä¾‹ã§ä½¿ç”¨ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```py
>>> example = dataset_with_ocr["train"][1]
>>> words = [word.lower() for word in example["words"]]
>>> match, word_idx_start, word_idx_end = subfinder(words, example["answer"].lower().split())
>>> print("Question: ", example["question"])
>>> print("Words:", words)
>>> print("Answer: ", example["answer"])
>>> print("start_index", word_idx_start)
>>> print("end_index", word_idx_end)
Question:  Who is in  cc in this letter?
Words: ['wie', 'baw', 'brown', '&', 'williamson', 'tobacco', 'corporation', 'research', '&', 'development', 'internal', 'correspondence', 'to:', 'r.', 'h.', 'honeycutt', 'ce:', 't.f.', 'riehl', 'from:', '.', 'c.j.', 'cook', 'date:', 'may', '8,', '1995', 'subject:', 'review', 'of', 'existing', 'brainstorming', 'ideas/483', 'the', 'major', 'function', 'of', 'the', 'product', 'innovation', 'graup', 'is', 'to', 'develop', 'marketable', 'nove!', 'products', 'that', 'would', 'be', 'profitable', 'to', 'manufacture', 'and', 'sell.', 'novel', 'is', 'defined', 'as:', 'of', 'a', 'new', 'kind,', 'or', 'different', 'from', 'anything', 'seen', 'or', 'known', 'before.', 'innovation', 'is', 'defined', 'as:', 'something', 'new', 'or', 'different', 'introduced;', 'act', 'of', 'innovating;', 'introduction', 'of', 'new', 'things', 'or', 'methods.', 'the', 'products', 'may', 'incorporate', 'the', 'latest', 'technologies,', 'materials', 'and', 'know-how', 'available', 'to', 'give', 'then', 'a', 'unique', 'taste', 'or', 'look.', 'the', 'first', 'task', 'of', 'the', 'product', 'innovation', 'group', 'was', 'to', 'assemble,', 'review', 'and', 'categorize', 'a', 'list', 'of', 'existing', 'brainstorming', 'ideas.', 'ideas', 'were', 'grouped', 'into', 'two', 'major', 'categories', 'labeled', 'appearance', 'and', 'taste/aroma.', 'these', 'categories', 'are', 'used', 'for', 'novel', 'products', 'that', 'may', 'differ', 'from', 'a', 'visual', 'and/or', 'taste/aroma', 'point', 'of', 'view', 'compared', 'to', 'canventional', 'cigarettes.', 'other', 'categories', 'include', 'a', 'combination', 'of', 'the', 'above,', 'filters,', 'packaging', 'and', 'brand', 'extensions.', 'appearance', 'this', 'category', 'is', 'used', 'for', 'novel', 'cigarette', 'constructions', 'that', 'yield', 'visually', 'different', 'products', 'with', 'minimal', 'changes', 'in', 'smoke', 'chemistry', 'two', 'cigarettes', 'in', 'cne.', 'emulti-plug', 'te', 'build', 'yaur', 'awn', 'cigarette.', 'eswitchable', 'menthol', 'or', 'non', 'menthol', 'cigarette.', '*cigarettes', 'with', 'interspaced', 'perforations', 'to', 'enable', 'smoker', 'to', 'separate', 'unburned', 'section', 'for', 'future', 'smoking.', 'Â«short', 'cigarette,', 'tobacco', 'section', '30', 'mm.', 'Â«extremely', 'fast', 'buming', 'cigarette.', 'Â«novel', 'cigarette', 'constructions', 'that', 'permit', 'a', 'significant', 'reduction', 'iretobacco', 'weight', 'while', 'maintaining', 'smoking', 'mechanics', 'and', 'visual', 'characteristics.', 'higher', 'basis', 'weight', 'paper:', 'potential', 'reduction', 'in', 'tobacco', 'weight.', 'Â«more', 'rigid', 'tobacco', 'column;', 'stiffing', 'agent', 'for', 'tobacco;', 'e.g.', 'starch', '*colored', 'tow', 'and', 'cigarette', 'papers;', 'seasonal', 'promotions,', 'e.g.', 'pastel', 'colored', 'cigarettes', 'for', 'easter', 'or', 'in', 'an', 'ebony', 'and', 'ivory', 'brand', 'containing', 'a', 'mixture', 'of', 'all', 'black', '(black', 'paper', 'and', 'tow)', 'and', 'ail', 'white', 'cigarettes.', '499150498']
Answer:  T.F. Riehl
start_index 17
end_index 18
```

ãŸã ã—ã€ã‚µãƒ³ãƒ—ãƒ«ãŒã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚Œã‚‹ã¨ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

```py
>>> encoding = tokenizer(example["question"], example["words"], example["boxes"])
>>> tokenizer.decode(encoding["input_ids"])
[CLS] who is in cc in this letter? [SEP] wie baw brown & williamson tobacco corporation research & development ...
```

ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸå…¥åŠ›å†…ã§ç­”ãˆã®ä½ç½®ã‚’è¦‹ã¤ã‘ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
* `token_type_ids` ã¯ã€ã©ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒè³ªå•ã®ä¸€éƒ¨ã§ã‚ã‚Šã€ã©ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒæ–‡æ›¸ã®å˜èªžã®ä¸€éƒ¨ã§ã‚ã‚‹ã‹ã‚’ç¤ºã—ã¾ã™ã€‚
* `tokenizer.cls_token_id` ã¯ã€å…¥åŠ›ã®å…ˆé ­ã§ç‰¹åˆ¥ãªãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¦‹ã¤ã‘ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚
* `word_ids` ã¯ã€å…ƒã® `words` ã§è¦‹ã¤ã‹ã£ãŸå›žç­”ã‚’ã€å®Œå…¨ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸå…¥åŠ›å†…ã®åŒã˜å›žç­”ã¨ç…§åˆã—ã¦åˆ¤æ–­ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚
ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸå…¥åŠ›å†…ã®å¿œç­”ã®é–‹å§‹/çµ‚äº†ä½ç½®ã€‚

ã“ã‚Œã‚’å¿µé ­ã«ç½®ã„ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®ã‚µãƒ³ãƒ—ãƒ«ã®ãƒãƒƒãƒã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹é–¢æ•°ã‚’ä½œæˆã—ã¾ã—ã‚‡ã†ã€‚


```py
>>> def encode_dataset(examples, max_length=512):
...     questions = examples["question"]
...     words = examples["words"]
...     boxes = examples["boxes"]
...     answers = examples["answer"]

...     # encode the batch of examples and initialize the start_positions and end_positions
...     encoding = tokenizer(questions, words, boxes, max_length=max_length, padding="max_length", truncation=True)
...     start_positions = []
...     end_positions = []

...     # loop through the examples in the batch
...     for i in range(len(questions)):
...         cls_index = encoding["input_ids"][i].index(tokenizer.cls_token_id)

...         # find the position of the answer in example's words
...         words_example = [word.lower() for word in words[i]]
...         answer = answers[i]
...         match, word_idx_start, word_idx_end = subfinder(words_example, answer.lower().split())

...         if match:
...             # if match is found, use `token_type_ids` to find where words start in the encoding
...             token_type_ids = encoding["token_type_ids"][i]
...             token_start_index = 0
...             while token_type_ids[token_start_index] != 1:
...                 token_start_index += 1

...             token_end_index = len(encoding["input_ids"][i]) - 1
...             while token_type_ids[token_end_index] != 1:
...                 token_end_index -= 1

...             word_ids = encoding.word_ids(i)[token_start_index : token_end_index + 1]
...             start_position = cls_index
...             end_position = cls_index

...             # loop over word_ids and increase `token_start_index` until it matches the answer position in words
...             # once it matches, save the `token_start_index` as the `start_position` of the answer in the encoding
...             for id in word_ids:
...                 if id == word_idx_start:
...                     start_position = token_start_index
...                 else:
...                     token_start_index += 1

...             # similarly loop over `word_ids` starting from the end to find the `end_position` of the answer
...             for id in word_ids[::-1]:
...                 if id == word_idx_end:
...                     end_position = token_end_index
...                 else:
...                     token_end_index -= 1

...             start_positions.append(start_position)
...             end_positions.append(end_position)

...         else:
...             start_positions.append(cls_index)
...             end_positions.append(cls_index)

...     encoding["image"] = examples["image"]
...     encoding["start_positions"] = start_positions
...     encoding["end_positions"] = end_positions

...     return encoding
```

ã“ã®å‰å‡¦ç†é–¢æ•°ãŒå®Œæˆã—ãŸã®ã§ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚

```py
>>> encoded_train_dataset = dataset_with_ocr["train"].map(
...     encode_dataset, batched=True, batch_size=2, remove_columns=dataset_with_ocr["train"].column_names
... )
>>> encoded_test_dataset = dataset_with_ocr["test"].map(
...     encode_dataset, batched=True, batch_size=2, remove_columns=dataset_with_ocr["test"].column_names
... )
```

ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç‰¹å¾´ãŒã©ã®ã‚ˆã†ãªã‚‚ã®ã‹ã‚’ç¢ºèªã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```py
>>> encoded_train_dataset.features
{'image': Sequence(feature=Sequence(feature=Sequence(feature=Value(dtype='uint8', id=None), length=-1, id=None), length=-1, id=None), length=-1, id=None),
 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),
 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),
 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),
 'bbox': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None),
 'start_positions': Value(dtype='int64', id=None),
 'end_positions': Value(dtype='int64', id=None)}
```

## Evaluation

æ–‡æ›¸ã®è³ªå•å›žç­”ã®è©•ä¾¡ã«ã¯ã€å¤§é‡ã®å¾Œå‡¦ç†ãŒå¿…è¦ã§ã™ã€‚éŽå‰°æ‘‚å–ã‚’é¿ã‘ã‚‹ãŸã‚ã«
ç¾æ™‚ç‚¹ã§ã¯ã€ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯è©•ä¾¡ã‚¹ãƒ†ãƒƒãƒ—ã‚’çœç•¥ã—ã¦ã„ã¾ã™ã€‚ [`Trainer`] ã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«è©•ä¾¡æå¤±ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã€
ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ã«ã¤ã„ã¦ã¾ã£ãŸãã‚ã‹ã‚‰ãªã„ã‚ã‘ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚æŠ½å‡ºçš„è³ªå•å¿œç­”ã¯é€šå¸¸ã€F1/å®Œå…¨ä¸€è‡´ã‚’ä½¿ç”¨ã—ã¦è©•ä¾¡ã•ã‚Œã¾ã™ã€‚
è‡ªåˆ†ã§å®Ÿè£…ã—ãŸã„å ´åˆã¯ã€[è³ªå•å¿œç­”ã®ç« ](https://huggingface.co/course/chapter7/7?fw=pt#postprocessing) ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚
ã‚¤ãƒ³ã‚¹ãƒ”ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å¾—ã‚‹ãŸã‚ã«ãƒã‚°ãƒ•ã‚§ã‚¤ã‚¹ã‚³ãƒ¼ã‚¹ã®ã€‚

## Train

ãŠã‚ã§ã¨ã†ï¼ã“ã®ã‚¬ã‚¤ãƒ‰ã®æœ€ã‚‚é›£ã—ã„éƒ¨åˆ†ã‚’ç„¡äº‹ã«ãƒŠãƒ“ã‚²ãƒ¼ãƒˆã§ããŸã®ã§ã€ç‹¬è‡ªã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚
ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¯æ¬¡ã®æ‰‹é †ãŒå«ã¾ã‚Œã¾ã™ã€‚
* å‰å‡¦ç†ã¨åŒã˜ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦ã€[`AutoModelForDocumentQuestionAnswering`] ã§ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚
* [`TrainingArguments`] ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®šç¾©ã—ã¾ã™ã€‚
* ã‚µãƒ³ãƒ—ãƒ«ã‚’ãƒãƒƒãƒå‡¦ç†ã™ã‚‹é–¢æ•°ã‚’å®šç¾©ã—ã¾ã™ã€‚ã“ã“ã§ã¯ [`DefaultDataCollatâ€‹â€‹or`] ãŒé©åˆ‡ã«æ©Ÿèƒ½ã—ã¾ã™ã€‚
* ãƒ¢ãƒ‡ãƒ«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ãƒ‡ãƒ¼ã‚¿ç…§åˆå™¨ã¨ã¨ã‚‚ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¼•æ•°ã‚’ [`Trainer`] ã«æ¸¡ã—ã¾ã™ã€‚
* [`~Trainer.train`] ã‚’å‘¼ã³å‡ºã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚

```py
>>> from transformers import AutoModelForDocumentQuestionAnswering

>>> model = AutoModelForDocumentQuestionAnswering.from_pretrained(model_checkpoint)
```

[`TrainingArguments`] ã§ã€`output_dir` ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜å ´æ‰€ã‚’æŒ‡å®šã—ã€å¿…è¦ã«å¿œã˜ã¦ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’æ§‹æˆã—ã¾ã™ã€‚
ãƒ¢ãƒ‡ãƒ«ã‚’ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¨å…±æœ‰ã—ãŸã„å ´åˆã¯ã€`push_to_hub`ã‚’`True`ã«è¨­å®šã—ã¾ã™ (ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã«ã¯ã€Hugging Face ã«ã‚µã‚¤ãƒ³ã‚¤ãƒ³ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™)ã€‚
ã“ã®å ´åˆã€`output_dir`ã¯ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒãƒ—ãƒƒã‚·ãƒ¥ã•ã‚Œã‚‹ãƒªãƒã‚¸ãƒˆãƒªã®åå‰ã«ã‚‚ãªã‚Šã¾ã™ã€‚

```py
>>> from transformers import TrainingArguments

>>> # REPLACE THIS WITH YOUR REPO ID
>>> repo_id = "MariaK/layoutlmv2-base-uncased_finetuned_docvqa"

>>> training_args = TrainingArguments(
...     output_dir=repo_id,
...     per_device_train_batch_size=4,
...     num_train_epochs=20,
...     save_steps=200,
...     logging_steps=50,
...     eval_strategy="steps",
...     learning_rate=5e-5,
...     save_total_limit=2,
...     remove_unused_columns=False,
...     push_to_hub=True,
... )
```

ã‚µãƒ³ãƒ—ãƒ«ã‚’ã¾ã¨ã‚ã¦ãƒãƒƒãƒå‡¦ç†ã™ã‚‹ãŸã‚ã®å˜ç´”ãªãƒ‡ãƒ¼ã‚¿ç…§åˆå™¨ã‚’å®šç¾©ã—ã¾ã™ã€‚

```py
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator()
```

æœ€å¾Œã«ã€ã™ã¹ã¦ã‚’ã¾ã¨ã‚ã¦ã€[`~Trainer.train`] ã‚’å‘¼ã³å‡ºã—ã¾ã™ã€‚

```py
>>> from transformers import Trainer

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     data_collator=data_collator,
...     train_dataset=encoded_train_dataset,
...     eval_dataset=encoded_test_dataset,
...     processing_class=processor,
... )

>>> trainer.train()
```

æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’ ðŸ¤— Hub ã«è¿½åŠ ã™ã‚‹ã«ã¯ã€ãƒ¢ãƒ‡ãƒ« ã‚«ãƒ¼ãƒ‰ã‚’ä½œæˆã—ã€`push_to_hub` ã‚’å‘¼ã³å‡ºã—ã¾ã™ã€‚

```py
>>> trainer.create_model_card()
>>> trainer.push_to_hub()
```

## Inference

LayoutLMv2 ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã€ðŸ¤— ãƒãƒ–ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸã®ã§ã€ãã‚Œã‚’æŽ¨è«–ã«ä½¿ç”¨ã§ãã¾ã™ã€‚ã‚‚ã£ã¨ã‚‚å˜ç´”ãª
æŽ¨è«–ç”¨ã«å¾®èª¿æ•´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™æ–¹æ³•ã¯ã€ãã‚Œã‚’ [`Pipeline`] ã§ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã™ã€‚

ä¾‹ã‚’æŒ™ã’ã¦ã¿ã¾ã—ã‚‡ã†:
```py
>>> example = dataset["test"][2]
>>> question = example["query"]["en"]
>>> image = example["image"]
>>> print(question)
>>> print(example["answers"])
'Who is â€˜presidingâ€™ TRRF GENERAL SESSION (PART 1)?'
['TRRF Vice President', 'lee a. waller']
```

æ¬¡ã«ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã¾ã™ã€‚
ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦è³ªå•ã¸ã®å›žç­”ã‚’æ–‡æ›¸åŒ–ã—ã€ç”»åƒã¨è³ªå•ã®çµ„ã¿åˆã‚ã›ã‚’ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã—ã¾ã™ã€‚

```py
>>> from transformers import pipeline

>>> qa_pipeline = pipeline("document-question-answering", model="MariaK/layoutlmv2-base-uncased_finetuned_docvqa")
>>> qa_pipeline(image, question)
[{'score': 0.9949808120727539,
  'answer': 'Lee A. Waller',
  'start': 55,
  'end': 57}]
```

å¿…è¦ã«å¿œã˜ã¦ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®çµæžœã‚’æ‰‹å‹•ã§è¤‡è£½ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚
1. ç”»åƒã¨è³ªå•ã‚’å–å¾—ã—ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ç”¨ã«æº–å‚™ã—ã¾ã™ã€‚
2. ãƒ¢ãƒ‡ãƒ«ã‚’é€šã˜ã¦çµæžœã¾ãŸã¯å‰å‡¦ç†ã‚’è»¢é€ã—ã¾ã™ã€‚
3. ãƒ¢ãƒ‡ãƒ«ã¯`start_logits`ã¨`end_logits`ã‚’è¿”ã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã¯ã€ã©ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒå¿œç­”ã®å…ˆé ­ã«ã‚ã‚‹ã®ã‹ã‚’ç¤ºã—ã€
ã©ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒå›žç­”ã®æœ€å¾Œã«ã‚ã‚Šã¾ã™ã‹ã€‚ã©ã¡ã‚‰ã‚‚å½¢çŠ¶ (batch_sizeã€sequence_length) ã‚’æŒã¡ã¾ã™ã€‚
4. `start_logits` ã¨ `end_logits` ã®ä¸¡æ–¹ã®æœ€å¾Œã®æ¬¡å…ƒã§ argmax ã‚’å–å¾—ã—ã€äºˆæ¸¬ã•ã‚Œã‚‹ `start_idx` ã¨ `end_idx` ã‚’å–å¾—ã—ã¾ã™ã€‚
5. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨ã—ã¦å›žç­”ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

```py
>>> import torch
>>> from transformers import AutoProcessor
>>> from transformers import AutoModelForDocumentQuestionAnswering

>>> processor = AutoProcessor.from_pretrained("MariaK/layoutlmv2-base-uncased_finetuned_docvqa")
>>> model = AutoModelForDocumentQuestionAnswering.from_pretrained("MariaK/layoutlmv2-base-uncased_finetuned_docvqa")

>>> with torch.no_grad():
...     encoding = processor(image.convert("RGB"), question, return_tensors="pt")
...     outputs = model(**encoding)
...     start_logits = outputs.start_logits
...     end_logits = outputs.end_logits
...     predicted_start_idx = start_logits.argmax(-1).item()
...     predicted_end_idx = end_logits.argmax(-1).item()

>>> processor.tokenizer.decode(encoding.input_ids.squeeze()[predicted_start_idx : predicted_end_idx + 1])
'lee a. waller'
```
