<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Automatic speech recognition

[[open-in-colab]]

<Youtube id="TksaY_FDgnk"/>

è‡ªå‹•éŸ³å£°èªè­˜ (ASR) ã¯éŸ³å£°ä¿¡å·ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã—ã€ä¸€é€£ã®éŸ³å£°å…¥åŠ›ã‚’ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ã«ãƒžãƒƒãƒ”ãƒ³ã‚°ã—ã¾ã™ã€‚ Siri ã‚„ Alexa ãªã©ã®ä»®æƒ³ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¯ ASR ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’æ—¥å¸¸çš„ã«æ”¯æ´ã—ã¦ãŠã‚Šã€ãƒ©ã‚¤ãƒ–ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚„ä¼šè­°ä¸­ã®ãƒ¡ãƒ¢å–ã‚Šãªã©ã€ä»–ã«ã‚‚ä¾¿åˆ©ãªãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒæ•°å¤šãã‚ã‚Šã¾ã™ã€‚

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€æ¬¡ã®æ–¹æ³•ã‚’èª¬æ˜Žã—ã¾ã™ã€‚

1. [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã® [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base) ã‚’å¾®èª¿æ•´ã—ã¦ã€éŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«æ›¸ãèµ·ã“ã—ã¾ã™ã€‚
2. å¾®èª¿æ•´ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’æŽ¨è«–ã«ä½¿ç”¨ã—ã¾ã™ã€‚

> [!TIP]
> ã“ã®ã‚¿ã‚¹ã‚¯ã¨äº’æ›æ€§ã®ã‚ã‚‹ã™ã¹ã¦ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ç¢ºèªã™ã‚‹ã«ã¯ã€[ã‚¿ã‚¹ã‚¯ãƒšãƒ¼ã‚¸](https://huggingface.co/tasks/automatic-speech-recognition) ã‚’ç¢ºèªã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚

å§‹ã‚ã‚‹å‰ã«ã€å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã™ã¹ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

```bash
pip install transformers datasets evaluate jiwer
```

ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¨å…±æœ‰ã§ãã‚‹ã‚ˆã†ã«ã€Hugging Face ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«ãƒ­ã‚°ã‚¤ãƒ³ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¡¨ç¤ºã•ã‚ŒãŸã‚‰ã€ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã™ã€‚

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## Load MInDS-14 dataset

ã¾ãšã€ðŸ¤— ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‹ã‚‰ [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å°ã•ã„ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã•ã‚‰ã«æ™‚é–“ã‚’è²»ã‚„ã™å‰ã«ã€å®Ÿé¨“ã—ã¦ã™ã¹ã¦ãŒæ©Ÿèƒ½ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹æ©Ÿä¼šãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚

```py
>>> from datasets import load_dataset, Audio

>>> minds = load_dataset("PolyAI/minds14", name="en-US", split="train[:100]")
```

[`~Dataset.train_test_split`] ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã® `train` åˆ†å‰²ã‚’ãƒˆãƒ¬ã‚¤ãƒ³ ã‚»ãƒƒãƒˆã¨ãƒ†ã‚¹ãƒˆ ã‚»ãƒƒãƒˆã«åˆ†å‰²ã—ã¾ã™ã€‚

```py
>>> minds = minds.train_test_split(test_size=0.2)
```

æ¬¡ã«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```py
>>> minds
DatasetDict({
    train: Dataset({
        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],
        num_rows: 16
    })
    test: Dataset({
        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],
        num_rows: 4
    })
})
```

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯`lang_id`ã‚„`english_transcription`ãªã©ã®å¤šãã®æœ‰ç”¨ãªæƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ãŒã€ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€Œ`audio`ã€ã¨ã€Œ`transciption`ã€ã«ç„¦ç‚¹ã‚’å½“ã¦ã¾ã™ã€‚ [`~datasets.Dataset.remove_columns`] ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ä»–ã®åˆ—ã‚’å‰Šé™¤ã—ã¾ã™ã€‚

```py
>>> minds = minds.remove_columns(["english_transcription", "intent_class", "lang_id"])
```

ã‚‚ã†ä¸€åº¦ä¾‹ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```py
>>> minds["train"][0]
{'audio': {'array': array([-0.00024414,  0.        ,  0.        , ...,  0.00024414,
          0.00024414,  0.00024414], dtype=float32),
  'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav',
  'sampling_rate': 8000},
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav',
 'transcription': "hi I'm trying to use the banking app on my phone and currently my checking and savings account balance is not refreshing"}
```

æ¬¡ã® 2 ã¤ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒã‚ã‚Šã¾ã™ã€‚

- `audio`: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ãŸã‚ã«å‘¼ã³å‡ºã™å¿…è¦ãŒã‚ã‚‹éŸ³å£°ä¿¡å·ã® 1 æ¬¡å…ƒã® `array`ã€‚
- `transcription`: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ†ã‚­ã‚¹ãƒˆã€‚

## Preprocess

æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€Wav2Vec2 ãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªä¿¡å·ã‚’å‡¦ç†ã—ã¾ã™ã€‚

```py
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base")
```

MInDS-14 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° ãƒ¬ãƒ¼ãƒˆã¯ 8000kHz ã§ã™ (ã“ã®æƒ…å ±ã¯ [ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ã‚«ãƒ¼ãƒ‰](https://huggingface.co/datasets/PolyAI/minds14) ã§ç¢ºèªã§ãã¾ã™)ã€‚ã¤ã¾ã‚Šã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å†ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸ Wav2Vec2 ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€16000kHz ã«è¨­å®šã—ã¾ã™ã€‚

```py
>>> minds = minds.cast_column("audio", Audio(sampling_rate=16_000))
>>> minds["train"][0]
{'audio': {'array': array([-2.38064706e-04, -1.58618059e-04, -5.43987835e-06, ...,
          2.78103951e-04,  2.38446111e-04,  1.18740834e-04], dtype=float32),
  'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav',
  'sampling_rate': 16000},
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav',
 'transcription': "hi I'm trying to use the banking app on my phone and currently my checking and savings account balance is not refreshing"}
```

ä¸Šã® `transcription` ã§ã‚ã‹ã‚‹ã‚ˆã†ã«ã€ãƒ†ã‚­ã‚¹ãƒˆã«ã¯å¤§æ–‡å­—ã¨å°æ–‡å­—ãŒæ··åœ¨ã—ã¦ã„ã¾ã™ã€‚ Wav2Vec2 ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¯å¤§æ–‡å­—ã®ã¿ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã‚‹ãŸã‚ã€ãƒ†ã‚­ã‚¹ãƒˆãŒãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èªžå½™ã¨ä¸€è‡´ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

```py
>>> def uppercase(example):
...     return {"transcription": example["transcription"].upper()}


>>> minds = minds.map(uppercase)
```

æ¬¡ã«ã€æ¬¡ã®å‰å‡¦ç†é–¢æ•°ã‚’ä½œæˆã—ã¾ã™ã€‚

1. `audio`åˆ—ã‚’å‘¼ã³å‡ºã—ã¦ã€ã‚ªãƒ¼ãƒ‡ã‚£ã‚ª ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¾ã™ã€‚
2. ã‚ªãƒ¼ãƒ‡ã‚£ã‚ª ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ `input_values` ã‚’æŠ½å‡ºã—ã€ãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’ä½¿ç”¨ã—ã¦ `transcription` åˆ—ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¾ã™ã€‚

```py
>>> def prepare_dataset(batch):
...     audio = batch["audio"]
...     batch = processor(audio["array"], sampling_rate=audio["sampling_rate"], text=batch["transcription"])
...     batch["input_length"] = len(batch["input_values"][0])
...     return batch
```

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã«å‰å‡¦ç†é–¢æ•°ã‚’é©ç”¨ã™ã‚‹ã«ã¯ã€ðŸ¤— Datasets [`~datasets.Dataset.map`] é–¢æ•°ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ `num_proc` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ãƒ—ãƒ­ã‚»ã‚¹ã®æ•°ã‚’å¢—ã‚„ã™ã“ã¨ã§ã€`map` ã‚’é«˜é€ŸåŒ–ã§ãã¾ã™ã€‚ [`~datasets.Dataset.remove_columns`] ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€ä¸è¦ãªåˆ—ã‚’å‰Šé™¤ã—ã¾ã™ã€‚

```py
>>> encoded_minds = minds.map(prepare_dataset, remove_columns=minds.column_names["train"], num_proc=4)
```

ðŸ¤— Transformers ã«ã¯ ASR ç”¨ã®ãƒ‡ãƒ¼ã‚¿ç…§åˆå™¨ãŒãªã„ãŸã‚ã€[`DataCollatâ€‹â€‹orWithPadding`] ã‚’èª¿æ•´ã—ã¦ã‚µãƒ³ãƒ—ãƒ«ã®ãƒãƒƒãƒã‚’ä½œæˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã¾ãŸã€ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ©ãƒ™ãƒ«ãŒ (ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã§ã¯ãªã) ãƒãƒƒãƒå†…ã®æœ€ã‚‚é•·ã„è¦ç´ ã®é•·ã•ã«åˆã‚ã›ã¦å‹•çš„ã«åŸ‹ã‚è¾¼ã¾ã‚Œã€å‡ä¸€ãªé•·ã•ã«ãªã‚Šã¾ã™ã€‚ `padding=True` ã‚’è¨­å®šã™ã‚‹ã¨ã€`tokenizer` é–¢æ•°ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’åŸ‹ã‚è¾¼ã‚€ã“ã¨ãŒã§ãã¾ã™ãŒã€å‹•çš„ãªåŸ‹ã‚è¾¼ã¿ã®æ–¹ãŒåŠ¹çŽ‡çš„ã§ã™ã€‚

ä»–ã®ãƒ‡ãƒ¼ã‚¿ç…§åˆå™¨ã¨ã¯ç•°ãªã‚Šã€ã“ã®ç‰¹å®šã®ãƒ‡ãƒ¼ã‚¿ç…§åˆå™¨ã¯ã€`input_values`ã¨ `labels`ã€ã«ç•°ãªã‚‹ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°æ–¹æ³•ã‚’é©ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

```py
>>> import torch

>>> from dataclasses import dataclass, field
>>> from typing import Any, Dict, List, Optional, Union


>>> @dataclass
... class DataCollatorCTCWithPadding:
...     processor: AutoProcessor
...     padding: Union[bool, str] = "longest"

...     def __call__(self, features: list[dict[str, Union[list[int], torch.Tensor]]]) -> dict[str, torch.Tensor]:
...         # split inputs and labels since they have to be of different lengths and need
...         # different padding methods
...         input_features = [{"input_values": feature["input_values"][0]} for feature in features]
...         label_features = [{"input_ids": feature["labels"]} for feature in features]

...         batch = self.processor.pad(input_features, padding=self.padding, return_tensors="pt")

...         labels_batch = self.processor.pad(labels=label_features, padding=self.padding, return_tensors="pt")

...         # replace padding with -100 to ignore loss correctly
...         labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

...         batch["labels"] = labels

...         return batch
```

æ¬¡ã«ã€`DataCollatâ€‹â€‹orForCTCWithPadding` ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã¾ã™ã€‚

```py
>>> data_collator = DataCollatorCTCWithPadding(processor=processor, padding="longest")
```

## Evaluate

ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å«ã‚ã‚‹ã¨ã€å¤šãã®å ´åˆã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ã‚’è©•ä¾¡ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚ ðŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦ã€è©•ä¾¡ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ã™ã°ã‚„ããƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚ã“ã®ã‚¿ã‚¹ã‚¯ã§ã¯ã€[å˜èªžã‚¨ãƒ©ãƒ¼çŽ‡](https://huggingface.co/spaces/evaluate-metric/wer) (WER) ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã¿ã¾ã™ (ðŸ¤— Evaluate [ã‚¯ã‚¤ãƒƒã‚¯ ãƒ„ã‚¢ãƒ¼](https://huggingface.co/docs/evaluate/a_quick_tour) ã‚’å‚ç…§ã—ã¦ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦è¨ˆç®—ã™ã‚‹æ–¹æ³•ã®è©³ç´°ã‚’ç¢ºèªã—ã¦ãã ã•ã„)ã€‚

```py
>>> import evaluate

>>> wer = evaluate.load("wer")
```

æ¬¡ã«ã€äºˆæ¸¬ã¨ãƒ©ãƒ™ãƒ«ã‚’ [`~evaluate.EvaluationModule.compute`] ã«æ¸¡ã—ã¦ WER ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°ã‚’ä½œæˆã—ã¾ã™ã€‚

```py
>>> import numpy as np


>>> def compute_metrics(pred):
...     pred_logits = pred.predictions
...     pred_ids = np.argmax(pred_logits, axis=-1)

...     pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id

...     pred_str = processor.batch_decode(pred_ids)
...     label_str = processor.batch_decode(pred.label_ids, group_tokens=False)

...     wer = wer.compute(predictions=pred_str, references=label_str)

...     return {"wer": wer}
```

ã“ã‚Œã§`compute_metrics`é–¢æ•°ã®æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹ã¨ãã«ã“ã®é–¢æ•°ã«æˆ»ã‚Šã¾ã™ã€‚

## Train

> [!TIP]
> [`Trainer`] ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã«æ…£ã‚Œã¦ã„ãªã„å ´åˆã¯ã€[ã“ã“](../training#train-with-pytorch-trainer) ã®åŸºæœ¬çš„ãªãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’ã”è¦§ãã ã•ã„ã€‚

ã“ã‚Œã§ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚ [`AutoModelForCTC`] ã§ Wav2Vec2 ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚ `ctc_loss_reduction` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§é©ç”¨ã™ã‚‹å‰Šæ¸›ã‚’æŒ‡å®šã—ã¾ã™ã€‚å¤šãã®å ´åˆã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®åˆè¨ˆã§ã¯ãªãå¹³å‡ã‚’ä½¿ç”¨ã™ã‚‹æ–¹ãŒé©åˆ‡ã§ã™ã€‚

```py
>>> from transformers import AutoModelForCTC, TrainingArguments, Trainer

>>> model = AutoModelForCTC.from_pretrained(
...     "facebook/wav2vec2-base",
...     ctc_loss_reduction="mean",
...     pad_token_id=processor.tokenizer.pad_token_id,
... )
```

ã“ã®æ™‚ç‚¹ã§æ®‹ã£ã¦ã„ã‚‹æ‰‹é †ã¯æ¬¡ã® 3 ã¤ã ã‘ã§ã™ã€‚

1. [`TrainingArguments`] ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®šç¾©ã—ã¾ã™ã€‚å”¯ä¸€ã®å¿…é ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜å ´æ‰€ã‚’æŒ‡å®šã™ã‚‹ `output_dir` ã§ã™ã€‚ `push_to_hub=True`ã‚’è¨­å®šã—ã¦ã€ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ–ã«ãƒ—ãƒƒã‚·ãƒ¥ã—ã¾ã™ (ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã«ã¯ã€Hugging Face ã«ã‚µã‚¤ãƒ³ã‚¤ãƒ³ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™)ã€‚å„ã‚¨ãƒãƒƒã‚¯ã®çµ‚äº†æ™‚ã«ã€[`ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼`] ã¯ WER ã‚’è©•ä¾¡ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã—ã¾ã™ã€‚
2. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¼•æ•°ã‚’ã€ãƒ¢ãƒ‡ãƒ«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã€ãƒ‡ãƒ¼ã‚¿ç…§åˆå™¨ã€ãŠã‚ˆã³ `compute_metrics` é–¢æ•°ã¨ã¨ã‚‚ã« [`Trainer`] ã«æ¸¡ã—ã¾ã™ã€‚
3. [`~Trainer.train`] ã‚’å‘¼ã³å‡ºã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚

```py
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_asr_mind_model",
...     per_device_train_batch_size=8,
...     gradient_accumulation_steps=2,
...     learning_rate=1e-5,
...     warmup_steps=500,
...     max_steps=2000,
...     gradient_checkpointing=True,
...     fp16=True,
...     group_by_length=True,
...     eval_strategy="steps",
...     per_device_eval_batch_size=8,
...     save_steps=1000,
...     eval_steps=1000,
...     logging_steps=25,
...     load_best_model_at_end=True,
...     metric_for_best_model="wer",
...     greater_is_better=False,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=encoded_minds["train"],
...     eval_dataset=encoded_minds["test"],
...     processing_class=processor,
...     data_collator=data_collator,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã—ãŸã‚‰ã€ [`~transformers.Trainer.push_to_hub`] ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ–ã«å…±æœ‰ã—ã€èª°ã‚‚ãŒãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

```py
>>> trainer.push_to_hub()
```


> [!TIP]
> è‡ªå‹•éŸ³å£°èªè­˜ç”¨ã«ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹æ–¹æ³•ã®ã‚ˆã‚Šè©³ç´°ãªä¾‹ã«ã¤ã„ã¦ã¯ã€è‹±èªž ASR ãŠã‚ˆã³è‹±èªžã®ã“ã®ãƒ–ãƒ­ã‚° [æŠ•ç¨¿](https://huggingface.co/blog/fine-tune-wav2vec2-english) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚å¤šè¨€èªž ASR ã«ã¤ã„ã¦ã¯ã€ã“ã® [æŠ•ç¨¿](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

## Inference

ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ãŸã®ã§ã€ãã‚Œã‚’æŽ¨è«–ã«ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚

æŽ¨è«–ã‚’å®Ÿè¡Œã—ãŸã„éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚å¿…è¦ã«å¿œã˜ã¦ã€ã‚ªãƒ¼ãƒ‡ã‚£ã‚ª ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° ãƒ¬ãƒ¼ãƒˆã‚’ãƒ¢ãƒ‡ãƒ«ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° ãƒ¬ãƒ¼ãƒˆã¨ä¸€è‡´ã™ã‚‹ã‚ˆã†ã«ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã‚’å¿˜ã‚Œãªã„ã§ãã ã•ã„ã€‚

```py
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", "en-US", split="train")
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
>>> sampling_rate = dataset.features["audio"].sampling_rate
>>> audio_file = dataset[0]["audio"]["path"]
```

æŽ¨è«–ç”¨ã«å¾®èª¿æ•´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™æœ€ã‚‚ç°¡å˜ãªæ–¹æ³•ã¯ã€ãã‚Œã‚’ [`pipeline`] ã§ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦è‡ªå‹•éŸ³å£°èªè­˜ç”¨ã®`pipeline`ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã€ã‚ªãƒ¼ãƒ‡ã‚£ã‚ª ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãã‚Œã«æ¸¡ã—ã¾ã™ã€‚

```py
>>> from transformers import pipeline

>>> transcriber = pipeline("automatic-speech-recognition", model="stevhliu/my_awesome_asr_minds_model")
>>> transcriber(audio_file)
{'text': 'I WOUD LIKE O SET UP JOINT ACOUNT WTH Y PARTNER'}
```

> [!TIP]
> è»¢å†™ã¯ã¾ã‚ã¾ã‚ã§ã™ãŒã€ã‚‚ã£ã¨è‰¯ããªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ã•ã‚‰ã«è‰¯ã„çµæžœã‚’å¾—ã‚‹ã«ã¯ã€ã‚ˆã‚Šå¤šãã®ä¾‹ã§ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¦ã¿ã¦ãã ã•ã„ã€‚

å¿…è¦ã«å¿œã˜ã¦ã€ã€Œãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€ã®çµæžœã‚’æ‰‹å‹•ã§è¤‡è£½ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚


ãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚ªãƒ¼ãƒ‡ã‚£ã‚ª ãƒ•ã‚¡ã‚¤ãƒ«ã¨æ–‡å­—èµ·ã“ã—ã‚’å‰å‡¦ç†ã—ã€`input`ã‚’ PyTorch ãƒ†ãƒ³ã‚½ãƒ«ã¨ã—ã¦è¿”ã—ã¾ã™ã€‚

```py
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("stevhliu/my_awesome_asr_mind_model")
>>> inputs = processor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")
```

Pass your inputs to the model and return the logits:

```py
>>> from transformers import AutoModelForCTC

>>> model = AutoModelForCTC.from_pretrained("stevhliu/my_awesome_asr_mind_model")
>>> with torch.no_grad():
...     logits = model(**inputs).logits
```

æœ€ã‚‚é«˜ã„ç¢ºçŽ‡ã§äºˆæ¸¬ã•ã‚ŒãŸ `input_ids` ã‚’å–å¾—ã—ã€ãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’ä½¿ç”¨ã—ã¦äºˆæ¸¬ã•ã‚ŒãŸ `input_ids` ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã«æˆ»ã—ã¾ã™ã€‚


```py
>>> import torch

>>> predicted_ids = torch.argmax(logits, dim=-1)
>>> transcription = processor.batch_decode(predicted_ids)
>>> transcription
['I WOUL LIKE O SET UP JOINT ACOUNT WTH Y PARTNER']
```

