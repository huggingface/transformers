<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Object detection

[[open-in-colab]]

ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ¤œå‡ºã¯ã€ç”»åƒå†…ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ (äººé–“ã€å»ºç‰©ã€è»Šãªã©) ã‚’æ¤œå‡ºã™ã‚‹ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ ãƒ“ã‚¸ãƒ§ãƒ³ ã‚¿ã‚¹ã‚¯ã§ã™ã€‚ç‰©ä½“æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã¯ç”»åƒã‚’å…¥åŠ›ãŠã‚ˆã³å‡ºåŠ›ã¨ã—ã¦å—ã‘å–ã‚Šã¾ã™
æ¤œå‡ºã•ã‚ŒãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å¢ƒç•Œãƒœãƒƒã‚¯ã‚¹ã¨é–¢é€£ã™ã‚‹ãƒ©ãƒ™ãƒ«ã®åº§æ¨™ã€‚ç”»åƒã«ã¯è¤‡æ•°ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å«ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
ãã‚Œãã‚Œã«ç‹¬è‡ªã®å¢ƒç•Œãƒœãƒƒã‚¯ã‚¹ã¨ãƒ©ãƒ™ãƒ«ãŒã‚ã‚Š (ä¾‹: è»Šã¨å»ºç‰©ã‚’æŒã¤ã“ã¨ãŒã§ãã¾ã™)ã€å„ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¯
ç”»åƒã®ã•ã¾ã–ã¾ãªéƒ¨åˆ†ã«å­˜åœ¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ (ãŸã¨ãˆã°ã€ç”»åƒã«ã¯è¤‡æ•°ã®è»ŠãŒå«ã¾ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™)ã€‚
ã“ã®ã‚¿ã‚¹ã‚¯ã¯ã€æ­©è¡Œè€…ã€é“è·¯æ¨™è­˜ã€ä¿¡å·æ©Ÿãªã©ã‚’æ¤œå‡ºã™ã‚‹ãŸã‚ã«è‡ªå‹•é‹è»¢ã§ä¸€èˆ¬çš„ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚
ä»–ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ã¯ã€ç”»åƒå†…ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã‚«ã‚¦ãƒ³ãƒˆã€ç”»åƒæ¤œç´¢ãªã©ãŒå«ã¾ã‚Œã¾ã™ã€‚

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€æ¬¡ã®æ–¹æ³•ã‚’å­¦ç¿’ã—ã¾ã™ã€‚

 1. Finetune [DETR](https://huggingface.co/docs/transformers/model_doc/detr)ã€ç•³ã¿è¾¼ã¿ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’çµ„ã¿åˆã‚ã›ãŸãƒ¢ãƒ‡ãƒ«
 [CPPE-5](https://huggingface.co/datasets/cppe-5) ä¸Šã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼/ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’å‚™ãˆãŸãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³
 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
 2. å¾®èª¿æ•´ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’æ¨è«–ã«ä½¿ç”¨ã—ã¾ã™ã€‚

> [!TIP]
> ã“ã®ã‚¿ã‚¹ã‚¯ã¨äº’æ›æ€§ã®ã‚ã‚‹ã™ã¹ã¦ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ç¢ºèªã™ã‚‹ã«ã¯ã€[ã‚¿ã‚¹ã‚¯ãƒšãƒ¼ã‚¸](https://huggingface.co/tasks/object-detection) ã‚’ç¢ºèªã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚

å§‹ã‚ã‚‹å‰ã«ã€å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã™ã¹ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚


```bash
pip install -q datasets transformers evaluate timm albumentations
```

ğŸ¤— ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¦ Hugging Face Hub ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€ğŸ¤— ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã™ã€‚
ãƒ‡ãƒ¼ã‚¿ã‚’å¢—å¼·ã™ã‚‹ãŸã‚ã®`albumentations`ã€‚ `timm` ã¯ç¾åœ¨ã€DETR ãƒ¢ãƒ‡ãƒ«ã®ç•³ã¿è¾¼ã¿ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã«å¿…è¦ã§ã™ã€‚

ãƒ¢ãƒ‡ãƒ«ã‚’ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¨å…±æœ‰ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ Hugging Face ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ã€ãƒãƒ–ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¡¨ç¤ºã•ã‚ŒãŸã‚‰ã€ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã™ã€‚

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## Load the CPPE-5 dataset

[CPPE-5 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ](https://huggingface.co/datasets/cppe-5) ã«ã¯ã€æ¬¡ã®ç”»åƒãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚
æ–°å‹ã‚³ãƒ­ãƒŠã‚¦ã‚¤ãƒ«ã‚¹æ„ŸæŸ“ç—‡ã®ãƒ‘ãƒ³ãƒ‡ãƒŸãƒƒã‚¯ã«ãŠã‘ã‚‹åŒ»ç™‚ç”¨å€‹äººä¿è­·å…· (PPE) ã‚’è­˜åˆ¥ã™ã‚‹æ³¨é‡ˆã€‚

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ã‹ã‚‰å§‹ã‚ã¾ã™ã€‚

```py
>>> from datasets import load_dataset

>>> cppe5 = load_dataset("cppe-5")
>>> cppe5
DatasetDict({
    train: Dataset({
        features: ['image_id', 'image', 'width', 'height', 'objects'],
        num_rows: 1000
    })
    test: Dataset({
        features: ['image_id', 'image', 'width', 'height', 'objects'],
        num_rows: 29
    })
})
```

ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯ã€1000 æšã®ç”»åƒã‚’å«ã‚€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ã‚»ãƒƒãƒˆã¨ 29 æšã®ç”»åƒã‚’å«ã‚€ãƒ†ã‚¹ãƒˆ ã‚»ãƒƒãƒˆãŒã™ã§ã«ä»˜å±ã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚

ãƒ‡ãƒ¼ã‚¿ã«æ…£ã‚Œã‚‹ãŸã‚ã«ã€ä¾‹ãŒã©ã®ã‚ˆã†ãªã‚‚ã®ã‹ã‚’èª¿ã¹ã¦ãã ã•ã„ã€‚

```py
>>> cppe5["train"][0]
{'image_id': 15,
 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=943x663 at 0x7F9EC9E77C10>,
 'width': 943,
 'height': 663,
 'objects': {'id': [114, 115, 116, 117],
  'area': [3796, 1596, 152768, 81002],
  'bbox': [[302.0, 109.0, 73.0, 52.0],
   [810.0, 100.0, 57.0, 28.0],
   [160.0, 31.0, 248.0, 616.0],
   [741.0, 68.0, 202.0, 401.0]],
  'category': [4, 4, 0, 0]}}
```

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®ä¾‹ã«ã¯æ¬¡ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒã‚ã‚Šã¾ã™ã€‚
- `image_id`: ã‚µãƒ³ãƒ—ãƒ«ã®ç”»åƒID
- `image`: ç”»åƒã‚’å«ã‚€ `PIL.Image.Image` ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
- `width`: ç”»åƒã®å¹…
- `height`: ç”»åƒã®é«˜ã•
- `objects`: ç”»åƒå†…ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å¢ƒç•Œãƒœãƒƒã‚¯ã‚¹ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€è¾æ›¸:
  - `id`: ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ID
  - `area`: å¢ƒç•Œãƒœãƒƒã‚¯ã‚¹ã®é ˜åŸŸ
  - `bbox`: ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å¢ƒç•Œãƒœãƒƒã‚¯ã‚¹ ([COCO å½¢å¼](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/#coco) )
  - `category`: ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã€‚å¯èƒ½ãªå€¤ã«ã¯ã€`Coverall (0)`ã€`Face_Shield (1)`ã€`Gloves (2)`ã€`Goggles (3)`ã€ãŠã‚ˆã³ `Mask (4)` ãŒå«ã¾ã‚Œã¾ã™ã€‚

`bbox`ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒ COCO å½¢å¼ã«å¾“ã£ã¦ã„ã‚‹ã“ã¨ã«æ°—ã¥ãã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ã“ã‚Œã¯ DETR ãƒ¢ãƒ‡ãƒ«ãŒäºˆæœŸã™ã‚‹å½¢å¼ã§ã™ã€‚
ãŸã ã—ã€ã€Œã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€å†…ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã¯ã€DETR ãŒå¿…è¦ã¨ã™ã‚‹æ³¨é‡ˆå½¢å¼ã¨ã¯ç•°ãªã‚Šã¾ã™ã€‚ã‚ãªãŸã¯ã™ã‚‹ã§ã‚ã‚ã†
ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä½¿ç”¨ã™ã‚‹å‰ã«ã€ã„ãã¤ã‹ã®å‰å‡¦ç†å¤‰æ›ã‚’é©ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

ãƒ‡ãƒ¼ã‚¿ã‚’ã•ã‚‰ã«æ·±ãç†è§£ã™ã‚‹ã«ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®ä¾‹ã‚’è¦–è¦šåŒ–ã—ã¾ã™ã€‚

```py
>>> import numpy as np
>>> import os
>>> from PIL import Image, ImageDraw

>>> image = cppe5["train"][0]["image"]
>>> annotations = cppe5["train"][0]["objects"]
>>> draw = ImageDraw.Draw(image)

>>> categories = cppe5["train"].features["objects"].feature["category"].names

>>> id2label = {index: x for index, x in enumerate(categories, start=0)}
>>> label2id = {v: k for k, v in id2label.items()}

>>> for i in range(len(annotations["id"])):
...     box = annotations["bbox"][i]
...     class_idx = annotations["category"][i]
...     x, y, w, h = tuple(box)
...     draw.rectangle((x, y, x + w, y + h), outline="red", width=1)
...     draw.text((x, y), id2label[class_idx], fill="white")

>>> image
```

<div class="flex justify-center">
    <img src="https://i.imgur.com/TdaqPJO.png" alt="CPPE-5 Image Example"/>
</div>

é–¢é€£ä»˜ã‘ã‚‰ã‚ŒãŸãƒ©ãƒ™ãƒ«ã‚’ä½¿ç”¨ã—ã¦å¢ƒç•Œãƒœãƒƒã‚¯ã‚¹ã‚’è¦–è¦šåŒ–ã™ã‚‹ã«ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ©ãƒ™ãƒ«ã‚’å–å¾—ã—ã¾ã™ã€‚
`category`ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã€‚
ã¾ãŸã€ãƒ©ãƒ™ãƒ« ID ã‚’ãƒ©ãƒ™ãƒ« ã‚¯ãƒ©ã‚¹ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹è¾æ›¸ (`id2label`) ã‚„ãã®é€† (`label2id`) ã‚’ä½œæˆã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚
ã“ã‚Œã‚‰ã¯ã€å¾Œã§ãƒ¢ãƒ‡ãƒ«ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹ã¨ãã«ä½¿ç”¨ã§ãã¾ã™ã€‚ã“ã‚Œã‚‰ã®ãƒãƒƒãƒ—ã‚’å«ã‚ã‚‹ã¨ã€å…±æœ‰ã—ãŸå ´åˆã«ä»–ã®äººãŒãƒ¢ãƒ‡ãƒ«ã‚’å†åˆ©ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
ãƒã‚°ãƒ•ã‚§ã‚¤ã‚¹ãƒãƒ–ã«å–ã‚Šä»˜ã‘ã¾ã™ã€‚

ãƒ‡ãƒ¼ã‚¿ã«æ…£ã‚Œã‚‹ãŸã‚ã®æœ€å¾Œã®ã‚¹ãƒ†ãƒƒãƒ—ã¨ã—ã¦ã€æ½œåœ¨çš„ãªå•é¡ŒãŒãªã„ã‹ãƒ‡ãƒ¼ã‚¿ã‚’èª¿æŸ»ã—ã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«é–¢ã™ã‚‹ä¸€èˆ¬çš„ãªå•é¡Œã® 1 ã¤ã¯ã€
ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ¤œå‡ºã¯ã€ç”»åƒã®ç«¯ã‚’è¶Šãˆã¦ã€Œä¼¸ã³ã‚‹ã€å¢ƒç•Œãƒœãƒƒã‚¯ã‚¹ã§ã™ã€‚ã“ã®ã‚ˆã†ãªã€Œæš´èµ°ã€å¢ƒç•Œãƒœãƒƒã‚¯ã‚¹ã¯ã€
ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ãŸã‚ã€ã“ã®æ®µéšã§å¯¾å‡¦ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯ã€ã“ã®å•é¡Œã«é–¢ã™ã‚‹ä¾‹ãŒã„ãã¤ã‹ã‚ã‚Šã¾ã™ã€‚
ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯å†…å®¹ã‚’ã‚ã‹ã‚Šã‚„ã™ãã™ã‚‹ãŸã‚ã«ã€ã“ã‚Œã‚‰ã®ç”»åƒã‚’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å‰Šé™¤ã—ã¾ã™ã€‚

```py
>>> remove_idx = [590, 821, 822, 875, 876, 878, 879]
>>> keep = [i for i in range(len(cppe5["train"])) if i not in remove_idx]
>>> cppe5["train"] = cppe5["train"].select(keep)
```

## Preprocess the data

ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã«ã¯ã€äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã«ä½¿ç”¨ã•ã‚Œã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨æ­£ç¢ºã«ä¸€è‡´ã™ã‚‹ã‚ˆã†ã«ã€ä½¿ç”¨ã™ã‚‹äºˆå®šã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
[`AutoImageProcessor`] ã¯ã€ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã¦ `pixel_values`ã€`pixel_mask`ã€ãŠã‚ˆã³
DETR ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ãã‚‹ã€Œãƒ©ãƒ™ãƒ«ã€ã€‚ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã«ã¯ã€å¿ƒé…ã™ã‚‹å¿…è¦ã®ãªã„ã„ãã¤ã‹ã®å±æ€§ãŒã‚ã‚Šã¾ã™ã€‚

- `image_mean = [0.485, 0.456, 0.406 ]`
- `image_std = [0.229, 0.224, 0.225]`

ã“ã‚Œã‚‰ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ç”»åƒã‚’æ­£è¦åŒ–ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹å¹³å‡ã¨æ¨™æº–åå·®ã§ã™ã€‚ã“ã‚Œã‚‰ã®ä¾¡å€¤è¦³ã¯éå¸¸ã«é‡è¦ã§ã™
äº‹å‰ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸç”»åƒãƒ¢ãƒ‡ãƒ«ã‚’æ¨è«–ã¾ãŸã¯å¾®èª¿æ•´ã™ã‚‹ã¨ãã«è¤‡è£½ã—ã¾ã™ã€‚

å¾®èª¿æ•´ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã¨åŒã˜ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ã‚¤ãƒ¡ãƒ¼ã‚¸ ãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã¾ã™ã€‚

```py
>>> from transformers import AutoImageProcessor

>>> checkpoint = "facebook/detr-resnet-50"
>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint)
```

ç”»åƒã‚’`image_processor`ã«æ¸¡ã™å‰ã«ã€2 ã¤ã®å‰å‡¦ç†å¤‰æ›ã‚’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«é©ç”¨ã—ã¾ã™ã€‚
- ç”»åƒã®æ‹¡å¼µ
- DETR ã®æœŸå¾…ã«å¿œãˆã‚‹ãŸã‚ã®æ³¨é‡ˆã®å†ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

ã¾ãšã€ãƒ¢ãƒ‡ãƒ«ãŒãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒ‡ãƒ¼ã‚¿ã«ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒˆã—ãªã„ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã«ã€ä»»æ„ã®ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦ç”»åƒæ‹¡å¼µã‚’é©ç”¨ã§ãã¾ã™ã€‚ã“ã“ã§ã¯[Albumentations](https://albumentations.ai/docs/)ã‚’ä½¿ç”¨ã—ã¾ã™...
ã“ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯ã€å¤‰æ›ãŒç”»åƒã«å½±éŸ¿ã‚’ä¸ãˆã€ãã‚Œã«å¿œã˜ã¦å¢ƒç•Œãƒœãƒƒã‚¯ã‚¹ã‚’æ›´æ–°ã™ã‚‹ã“ã¨ã‚’ä¿è¨¼ã—ã¾ã™ã€‚
ğŸ¤— ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã¯ã€è©³ç´°ãª [ç‰©ä½“æ¤œå‡ºç”¨ã«ç”»åƒã‚’æ‹¡å¼µã™ã‚‹æ–¹æ³•ã«é–¢ã™ã‚‹ã‚¬ã‚¤ãƒ‰](https://huggingface.co/docs/datasets/object_detection) ãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã™ã€‚
ä¾‹ã¨ã—ã¦ã¾ã£ãŸãåŒã˜ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚ã“ã“ã§ã‚‚åŒã˜ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’é©ç”¨ã—ã€å„ç”»åƒã®ã‚µã‚¤ã‚ºã‚’ (480, 480) ã«å¤‰æ›´ã—ã¾ã™ã€‚
æ°´å¹³ã«åè»¢ã—ã¦æ˜ã‚‹ãã—ã¾ã™ã€‚

```py
>>> import albumentations
>>> import numpy as np
>>> import torch

>>> transform = albumentations.Compose(
...     [
...         albumentations.Resize(480, 480),
...         albumentations.HorizontalFlip(p=1.0),
...         albumentations.RandomBrightnessContrast(p=1.0),
...     ],
...     bbox_params=albumentations.BboxParams(format="coco", label_fields=["category"]),
... )
```

`image_processor` ã¯ã€æ³¨é‡ˆãŒæ¬¡ã®å½¢å¼ã§ã‚ã‚‹ã“ã¨ã‚’æœŸå¾…ã—ã¾ã™: `{'image_id': int, 'annotations': list[Dict]}`,
 ã“ã“ã§ã€å„è¾æ›¸ã¯ COCO ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®æ³¨é‡ˆã§ã™ã€‚ 1 ã¤ã®ä¾‹ã¨ã—ã¦ã€æ³¨é‡ˆã‚’å†ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã™ã‚‹é–¢æ•°ã‚’è¿½åŠ ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

 ```py
>>> def formatted_anns(image_id, category, area, bbox):
...     annotations = []
...     for i in range(0, len(category)):
...         new_ann = {
...             "image_id": image_id,
...             "category_id": category[i],
...             "isCrowd": 0,
...             "area": area[i],
...             "bbox": list(bbox[i]),
...         }
...         annotations.append(new_ann)

...     return annotations
```

ã“ã‚Œã§ã€ç”»åƒã¨æ³¨é‡ˆã®å¤‰æ›ã‚’çµ„ã¿åˆã‚ã›ã¦ã‚µãƒ³ãƒ—ãƒ«ã®ãƒãƒƒãƒã§ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚

```py
>>> # transforming a batch
>>> def transform_aug_ann(examples):
...     image_ids = examples["image_id"]
...     images, bboxes, area, categories = [], [], [], []
...     for image, objects in zip(examples["image"], examples["objects"]):
...         image = np.array(image.convert("RGB"))[:, :, ::-1]
...         out = transform(image=image, bboxes=objects["bbox"], category=objects["category"])

...         area.append(objects["area"])
...         images.append(out["image"])
...         bboxes.append(out["bboxes"])
...         categories.append(out["category"])

...     targets = [
...         {"image_id": id_, "annotations": formatted_anns(id_, cat_, ar_, box_)}
...         for id_, cat_, ar_, box_ in zip(image_ids, categories, area, bboxes)
...     ]

...     return image_processor(images=images, annotations=targets, return_tensors="pt")
```

ğŸ¤— Datasets [`~datasets.Dataset.with_transform`] ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€ã“ã®å‰å‡¦ç†é–¢æ•°ã‚’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã«é©ç”¨ã—ã¾ã™ã€‚ã“ã®æ–¹æ³•ãŒé©ç”¨ã•ã‚Œã‚‹ã®ã¯ã€
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¦ç´ ã‚’èª­ã¿è¾¼ã‚€ã¨ãã«ã€ãã®å ´ã§å¤‰æ›ã—ã¾ã™ã€‚

ã“ã®æ™‚ç‚¹ã§ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¾‹ãŒå¤‰æ›å¾Œã«ã©ã®ã‚ˆã†ã«ãªã‚‹ã‹ã‚’ç¢ºèªã§ãã¾ã™ã€‚ãƒ†ãƒ³ã‚½ãƒ«ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¯ãšã§ã™
`pixel_values`ã€ãƒ†ãƒ³ã‚½ãƒ«ã¨ `pixel_mask`ã€ãŠã‚ˆã³ `labels` ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

```py
>>> cppe5["train"] = cppe5["train"].with_transform(transform_aug_ann)
>>> cppe5["train"][15]
{'pixel_values': tensor([[[ 0.9132,  0.9132,  0.9132,  ..., -1.9809, -1.9809, -1.9809],
          [ 0.9132,  0.9132,  0.9132,  ..., -1.9809, -1.9809, -1.9809],
          [ 0.9132,  0.9132,  0.9132,  ..., -1.9638, -1.9638, -1.9638],
          ...,
          [-1.5699, -1.5699, -1.5699,  ..., -1.9980, -1.9980, -1.9980],
          [-1.5528, -1.5528, -1.5528,  ..., -1.9980, -1.9809, -1.9809],
          [-1.5528, -1.5528, -1.5528,  ..., -1.9980, -1.9809, -1.9809]],

         [[ 1.3081,  1.3081,  1.3081,  ..., -1.8431, -1.8431, -1.8431],
          [ 1.3081,  1.3081,  1.3081,  ..., -1.8431, -1.8431, -1.8431],
          [ 1.3081,  1.3081,  1.3081,  ..., -1.8256, -1.8256, -1.8256],
          ...,
          [-1.3179, -1.3179, -1.3179,  ..., -1.8606, -1.8606, -1.8606],
          [-1.3004, -1.3004, -1.3004,  ..., -1.8606, -1.8431, -1.8431],
          [-1.3004, -1.3004, -1.3004,  ..., -1.8606, -1.8431, -1.8431]],

         [[ 1.4200,  1.4200,  1.4200,  ..., -1.6476, -1.6476, -1.6476],
          [ 1.4200,  1.4200,  1.4200,  ..., -1.6476, -1.6476, -1.6476],
          [ 1.4200,  1.4200,  1.4200,  ..., -1.6302, -1.6302, -1.6302],
          ...,
          [-1.0201, -1.0201, -1.0201,  ..., -1.5604, -1.5604, -1.5604],
          [-1.0027, -1.0027, -1.0027,  ..., -1.5604, -1.5430, -1.5430],
          [-1.0027, -1.0027, -1.0027,  ..., -1.5604, -1.5430, -1.5430]]]),
 'pixel_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         ...,
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1]]),
 'labels': {'size': tensor([800, 800]), 'image_id': tensor([756]), 'class_labels': tensor([4]), 'boxes': tensor([[0.7340, 0.6986, 0.3414, 0.5944]]), 'area': tensor([519544.4375]), 'iscrowd': tensor([0]), 'orig_size': tensor([480, 480])}}
```

å€‹ã€…ã®ç”»åƒã‚’æ­£å¸¸ã«æ‹¡å¼µã—ã€ãã‚Œã‚‰ã®æ³¨é‡ˆã‚’æº–å‚™ã—ã¾ã—ãŸã€‚ãŸã ã—ã€å‰å‡¦ç†ã¯ãã†ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
ã¾ã å®Œæˆã—ã¦ã„ã¾ã™ã€‚æœ€å¾Œã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€ç”»åƒã‚’ãƒãƒƒãƒå‡¦ç†ã™ã‚‹ãŸã‚ã®ã‚«ã‚¹ã‚¿ãƒ  `collatâ€‹â€‹e_fn` ã‚’ä½œæˆã—ã¾ã™ã€‚
ç”»åƒ (ç¾åœ¨ã¯ `pixel_values`) ã‚’ãƒãƒƒãƒå†…ã®æœ€å¤§ã®ç”»åƒã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã—ã€å¯¾å¿œã™ã‚‹ `pixel_mask` ã‚’ä½œæˆã—ã¾ã™
ã©ã®ãƒ”ã‚¯ã‚»ãƒ«ãŒå®Ÿæ•° (1) ã§ã€ã©ã®ãƒ”ã‚¯ã‚»ãƒ«ãŒãƒ‘ãƒ‡ã‚£ãƒ³ã‚° (0) ã§ã‚ã‚‹ã‹ã‚’ç¤ºã—ã¾ã™ã€‚


```py
>>> def collate_fn(batch):
...     pixel_values = [item["pixel_values"] for item in batch]
...     encoding = image_processor.pad(pixel_values, return_tensors="pt")
...     labels = [item["labels"] for item in batch]
...     batch = {}
...     batch["pixel_values"] = encoding["pixel_values"]
...     batch["pixel_mask"] = encoding["pixel_mask"]
...     batch["labels"] = labels
...     return batch
```

## Training the DETR model

å‰ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§é‡åŠ´åƒã®ã»ã¨ã‚“ã©ã‚’å®Œäº†ã—ãŸã®ã§ã€ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚
ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®ç”»åƒã¯ã€ã‚µã‚¤ã‚ºã‚’å¤‰æ›´ã—ãŸå¾Œã§ã‚‚ä¾ç„¶ã¨ã—ã¦éå¸¸ã«å¤§ãã„ã§ã™ã€‚ã“ã‚Œã¯ã€ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã¨ã€
å°‘ãªãã¨ã‚‚ 1 ã¤ã® GPU ãŒå¿…è¦ã§ã™ã€‚

ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¯æ¬¡ã®æ‰‹é †ãŒå«ã¾ã‚Œã¾ã™ã€‚
1. å‰å‡¦ç†ã¨åŒã˜ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦ã€[`AutoModelForObjectDetection`] ã§ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚
2. [`TrainingArguments`] ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®šç¾©ã—ã¾ã™ã€‚
3. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¼•æ•°ã‚’ãƒ¢ãƒ‡ãƒ«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã€ãƒ‡ãƒ¼ã‚¿ç…§åˆå™¨ã¨ã¨ã‚‚ã« [`Trainer`] ã«æ¸¡ã—ã¾ã™ã€‚
4. [`~Trainer.train`] ã‚’å‘¼ã³å‡ºã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚

å‰å‡¦ç†ã«ä½¿ç”¨ã—ãŸã®ã¨åŒã˜ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ãã¯ã€å¿…ãš`label2id`ã‚’æ¸¡ã—ã¦ãã ã•ã„ã€‚
ãŠã‚ˆã³ `id2label` ãƒãƒƒãƒ—ã¯ã€ä»¥å‰ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä½œæˆã—ãŸã‚‚ã®ã§ã™ã€‚ã•ã‚‰ã«ã€`ignore_mismatched_sizes=True`ã‚’æŒ‡å®šã—ã¦ã€æ—¢å­˜ã®åˆ†é¡é ­éƒ¨ã‚’æ–°ã—ã„åˆ†é¡é ­éƒ¨ã«ç½®ãæ›ãˆã¾ã™ã€‚

```py
>>> from transformers import AutoModelForObjectDetection

>>> model = AutoModelForObjectDetection.from_pretrained(
...     checkpoint,
...     id2label=id2label,
...     label2id=label2id,
...     ignore_mismatched_sizes=True,
... )
```

[`TrainingArguments`] ã§ã€`output_dir` ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜å ´æ‰€ã‚’æŒ‡å®šã—ã€å¿…è¦ã«å¿œã˜ã¦ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’æ§‹æˆã—ã¾ã™ã€‚
ç”»åƒåˆ—ãŒå‰Šé™¤ã•ã‚Œã‚‹ãŸã‚ã€æœªä½¿ç”¨ã®åˆ—ã‚’å‰Šé™¤ã—ãªã„ã“ã¨ãŒé‡è¦ã§ã™ã€‚ç”»åƒåˆ—ãŒãªã„ã¨ã€
`pixel_values` ã‚’ä½œæˆã§ãã¾ã›ã‚“ã€‚ã“ã®ãŸã‚ã€`remove_unused_columns`ã‚’`False`ã«è¨­å®šã—ã¾ã™ã€‚
ãƒãƒ–ã«ãƒ—ãƒƒã‚·ãƒ¥ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å…±æœ‰ã—ãŸã„å ´åˆã¯ã€`push_to_hub` ã‚’ `True` ã«è¨­å®šã—ã¾ã™ (Hugging ã«ã‚µã‚¤ãƒ³ã‚¤ãƒ³ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™)
é¡”ã«å‘ã‹ã£ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ï¼‰ã€‚

```py
>>> from transformers import TrainingArguments

>>> training_args = TrainingArguments(
...     output_dir="detr-resnet-50_finetuned_cppe5",
...     per_device_train_batch_size=8,
...     num_train_epochs=10,
...     fp16=True,
...     save_steps=200,
...     logging_steps=50,
...     learning_rate=1e-5,
...     weight_decay=1e-4,
...     save_total_limit=2,
...     remove_unused_columns=False,
...     push_to_hub=True,
... )
```

æœ€å¾Œã«ã€ã™ã¹ã¦ã‚’ã¾ã¨ã‚ã¦ã€[`~transformers.Trainer.train`] ã‚’å‘¼ã³å‡ºã—ã¾ã™ã€‚

```py
>>> from transformers import Trainer

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     data_collator=collate_fn,
...     train_dataset=cppe5["train"],
...     processing_class=image_processor,
... )

>>> trainer.train()
```

`training_args`ã§`push_to_hub`ã‚’`True`ã«è¨­å®šã—ãŸå ´åˆã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¯
ãƒã‚°ãƒ•ã‚§ã‚¤ã‚¹ãƒãƒ–ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã—ãŸã‚‰ã€[`~transformers.Trainer.push_to_hub`] ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã—ã¦ã€æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚‚ãƒãƒ–ã«ãƒ—ãƒƒã‚·ãƒ¥ã—ã¾ã™ã€‚

```py
>>> trainer.push_to_hub()
```

## Evaluate

ç‰©ä½“æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã¯é€šå¸¸ã€ä¸€é€£ã® <a href="https://cocodataset.org/#detection-eval">COCO ã‚¹ã‚¿ã‚¤ãƒ«ã®æŒ‡æ¨™</a>ã‚’ä½¿ç”¨ã—ã¦è©•ä¾¡ã•ã‚Œã¾ã™ã€‚
æ—¢å­˜ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®Ÿè£…ã®ã„ãšã‚Œã‹ã‚’ä½¿ç”¨ã§ãã¾ã™ãŒã€ã“ã“ã§ã¯`torchvision`ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®Ÿè£…ã‚’ä½¿ç”¨ã—ã¦æœ€çµ‚çš„ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è©•ä¾¡ã—ã¾ã™ã€‚
ãƒãƒ–ã«ãƒ—ãƒƒã‚·ãƒ¥ã—ãŸãƒ¢ãƒ‡ãƒ«ã€‚

`torchvision`ã‚¨ãƒãƒªãƒ¥ã‚¨ãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ ãƒˆã‚¥ãƒ«ãƒ¼ã‚¹ COCO ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ COCO ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã® API
ãƒ‡ãƒ¼ã‚¿ã‚’ç‰¹å®šã®å½¢å¼ã§ä¿å­˜ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ãŸã‚ã€æœ€åˆã«ç”»åƒã¨æ³¨é‡ˆã‚’ãƒ‡ã‚£ã‚¹ã‚¯ã«ä¿å­˜ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã¨åŒã˜ã‚ˆã†ã«
ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã™ã‚‹ã¨ãã€`cppe5["test"]` ã‹ã‚‰ã®æ³¨é‡ˆã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ãŸã ã—ã€ç”»åƒ
ãã®ã¾ã¾ã§ã„ã‚‹ã¹ãã§ã™ã€‚

è©•ä¾¡ã‚¹ãƒ†ãƒƒãƒ—ã«ã¯å°‘ã—ä½œæ¥­ãŒå¿…è¦ã§ã™ãŒã€å¤§ãã 3 ã¤ã®ã‚¹ãƒ†ãƒƒãƒ—ã«åˆ†ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
ã¾ãšã€`cppe5["test"]` ã‚»ãƒƒãƒˆã‚’æº–å‚™ã—ã¾ã™ã€‚æ³¨é‡ˆã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã—ã€ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ã‚£ã‚¹ã‚¯ã«ä¿å­˜ã—ã¾ã™ã€‚


```py
>>> import json


>>> # format annotations the same as for training, no need for data augmentation
>>> def val_formatted_anns(image_id, objects):
...     annotations = []
...     for i in range(0, len(objects["id"])):
...         new_ann = {
...             "id": objects["id"][i],
...             "category_id": objects["category"][i],
...             "iscrowd": 0,
...             "image_id": image_id,
...             "area": objects["area"][i],
...             "bbox": objects["bbox"][i],
...         }
...         annotations.append(new_ann)

...     return annotations


>>> # Save images and annotations into the files torchvision.datasets.CocoDetection expects
>>> def save_cppe5_annotation_file_images(cppe5):
...     output_json = {}
...     path_output_cppe5 = f"{os.getcwd()}/cppe5/"

...     if not os.path.exists(path_output_cppe5):
...         os.makedirs(path_output_cppe5)

...     path_anno = os.path.join(path_output_cppe5, "cppe5_ann.json")
...     categories_json = [{"supercategory": "none", "id": id, "name": id2label[id]} for id in id2label]
...     output_json["images"] = []
...     output_json["annotations"] = []
...     for example in cppe5:
...         ann = val_formatted_anns(example["image_id"], example["objects"])
...         output_json["images"].append(
...             {
...                 "id": example["image_id"],
...                 "width": example["image"].width,
...                 "height": example["image"].height,
...                 "file_name": f"{example['image_id']}.png",
...             }
...         )
...         output_json["annotations"].extend(ann)
...     output_json["categories"] = categories_json

...     with open(path_anno, "w") as file:
...         json.dump(output_json, file, ensure_ascii=False, indent=4)

...     for im, img_id in zip(cppe5["image"], cppe5["image_id"]):
...         path_img = os.path.join(path_output_cppe5, f"{img_id}.png")
...         im.save(path_img)

...     return path_output_cppe5, path_anno
```

æ¬¡ã«ã€`cocoevaluator`ã§åˆ©ç”¨ã§ãã‚‹`CocoDetection`ã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”¨æ„ã—ã¾ã™ã€‚


```py
>>> import torchvision


>>> class CocoDetection(torchvision.datasets.CocoDetection):
...     def __init__(self, img_folder, image_processor, ann_file):
...         super().__init__(img_folder, ann_file)
...         self.image_processor = image_processor

...     def __getitem__(self, idx):
...         # read in PIL image and target in COCO format
...         img, target = super(CocoDetection, self).__getitem__(idx)

...         # preprocess image and target: converting target to DETR format,
...         # resizing + normalization of both image and target)
...         image_id = self.ids[idx]
...         target = {"image_id": image_id, "annotations": target}
...         encoding = self.image_processor(images=img, annotations=target, return_tensors="pt")
...         pixel_values = encoding["pixel_values"].squeeze()  # remove batch dimension
...         target = encoding["labels"][0]  # remove batch dimension

...         return {"pixel_values": pixel_values, "labels": target}


>>> im_processor = AutoImageProcessor.from_pretrained("devonho/detr-resnet-50_finetuned_cppe5")

>>> path_output_cppe5, path_anno = save_cppe5_annotation_file_images(cppe5["test"])
>>> test_ds_coco_format = CocoDetection(path_output_cppe5, im_processor, path_anno)
```

æœ€å¾Œã«ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦è©•ä¾¡ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

```py
>>> import evaluate
>>> from tqdm import tqdm

>>> model = AutoModelForObjectDetection.from_pretrained("devonho/detr-resnet-50_finetuned_cppe5")
>>> module = evaluate.load("ybelkada/cocoevaluate", coco=test_ds_coco_format.coco)
>>> val_dataloader = torch.utils.data.DataLoader(
...     test_ds_coco_format, batch_size=8, shuffle=False, num_workers=4, collate_fn=collate_fn
... )

>>> with torch.no_grad():
...     for idx, batch in enumerate(tqdm(val_dataloader)):
...         pixel_values = batch["pixel_values"]
...         pixel_mask = batch["pixel_mask"]

...         labels = [
...             {k: v for k, v in t.items()} for t in batch["labels"]
...         ]  # these are in DETR format, resized + normalized

...         # forward pass
...         outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)

...         orig_target_sizes = torch.stack([target["orig_size"] for target in labels], dim=0)
...         results = im_processor.post_process(outputs, orig_target_sizes)  # convert outputs of model to Pascal VOC format (xmin, ymin, xmax, ymax)

...         module.add(prediction=results, reference=labels)
...         del batch

>>> results = module.compute()
>>> print(results)
Accumulating evaluation results...
DONE (t=0.08s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.352
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.681
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.292
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.168
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.208
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.429
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.274
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.484
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.501
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.191
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.323
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.590
```

ã“ã‚Œã‚‰ã®çµæœã¯ã€[`~transformers.TrainingArguments`] ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´ã™ã‚‹ã“ã¨ã§ã•ã‚‰ã«æ”¹å–„ã§ãã¾ã™ã€‚è©¦ã—ã¦ã”ã‚‰ã‚“ï¼

## Inference

DETR ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¦è©•ä¾¡ã—ã€Hugging Face Hub ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸã®ã§ã€ãã‚Œã‚’æ¨è«–ã«ä½¿ç”¨ã§ãã¾ã™ã€‚
æ¨è«–ç”¨ã«å¾®èª¿æ•´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™æœ€ã‚‚ç°¡å˜ãªæ–¹æ³•ã¯ã€ãã‚Œã‚’ [`pipeline`] ã§ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã™ã€‚ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã™ã‚‹
ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ¤œå‡ºã—ã€ãã‚Œã«ç”»åƒã‚’æ¸¡ã—ã¾ã™ã€‚


```py
>>> from transformers import pipeline
>>> import requests

>>> url = "https://i.imgur.com/2lnWoly.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> obj_detector = pipeline("object-detection", model="devonho/detr-resnet-50_finetuned_cppe5")
>>> obj_detector(image)
```

å¿…è¦ã«å¿œã˜ã¦ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®çµæœã‚’æ‰‹å‹•ã§è¤‡è£½ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚

```py
>>> image_processor = AutoImageProcessor.from_pretrained("devonho/detr-resnet-50_finetuned_cppe5")
>>> model = AutoModelForObjectDetection.from_pretrained("devonho/detr-resnet-50_finetuned_cppe5")

>>> with torch.no_grad():
...     inputs = image_processor(images=image, return_tensors="pt")
...     outputs = model(**inputs)
...     target_sizes = torch.tensor([image.size[::-1]])
...     results = image_processor.post_process_object_detection(outputs, threshold=0.5, target_sizes=target_sizes)[0]

>>> for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
...     box = [round(i, 2) for i in box.tolist()]
...     print(
...         f"Detected {model.config.id2label[label.item()]} with confidence "
...         f"{round(score.item(), 3)} at location {box}"
...     )
Detected Coverall with confidence 0.566 at location [1215.32, 147.38, 4401.81, 3227.08]
Detected Mask with confidence 0.584 at location [2449.06, 823.19, 3256.43, 1413.9]
```

çµæœã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã¦ã¿ã¾ã—ã‚‡ã†:

```py
>>> draw = ImageDraw.Draw(image)

>>> for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
...     box = [round(i, 2) for i in box.tolist()]
...     x, y, x2, y2 = tuple(box)
...     draw.rectangle((x, y, x2, y2), outline="red", width=1)
...     draw.text((x, y), model.config.id2label[label.item()], fill="white")

>>> image
```

<div class="flex justify-center">
    <img src="https://i.imgur.com/4QZnf9A.png" alt="Object detection result on a new image"/>
</div>
