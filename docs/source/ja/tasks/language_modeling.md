<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Causal language modeling


[[open-in-colab]]


è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«ã¯ã€å› æœçš„ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã¨ãƒã‚¹ã‚¯ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã® 2 ã¤ã®ã‚¿ã‚¤ãƒ—ãŒã‚ã‚Šã¾ã™ã€‚ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€å› æœé–¢ä¿‚ã®ã‚ã‚‹è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚
å› æœè¨€èªãƒ¢ãƒ‡ãƒ«ã¯ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã«ã‚ˆãä½¿ç”¨ã•ã‚Œã¾ã™ã€‚ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€æ¬¡ã®ã‚ˆã†ãªã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ä½¿ç”¨ã§ãã¾ã™ã€‚
ç‹¬è‡ªã®ãƒ†ã‚­ã‚¹ãƒˆ ã‚¢ãƒ‰ãƒ™ãƒ³ãƒãƒ£ãƒ¼ã‚’é¸æŠã™ã‚‹ã‹ã€Copilot ã‚„ CodeParrot ãªã©ã®ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã‚’é¸æŠã—ã¾ã™ã€‚

<Youtube id="Vpjb1lu0MDk"/>


å› æœè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã¯ã€ä¸€é€£ã®ãƒˆãƒ¼ã‚¯ãƒ³å†…ã®æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’äºˆæ¸¬ã—ã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã¯ã€æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«ã®ã¿å¯¾å¿œã§ãã¾ã™ã€‚
å·¦ã€‚ã“ã‚Œã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒå°†æ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’èªè­˜ã§ããªã„ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚ GPT-2 ã¯å› æœçš„è¨€èªãƒ¢ãƒ‡ãƒ«ã®ä¸€ä¾‹ã§ã™ã€‚

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€æ¬¡ã®æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚

1. [ELI5](https:/) ã® [r/askscience](https://www.reddit.com/r/askscience/) ã‚µãƒ–ã‚»ãƒƒãƒˆã§ [DistilGPT2](https://huggingface.co/distilbert/distilgpt2) ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚ /huggingface.co/datasets/eli5) ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
2. å¾®èª¿æ•´ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’æ¨è«–ã«ä½¿ç”¨ã—ã¾ã™ã€‚

<Tip>

ã“ã®ã‚¿ã‚¹ã‚¯ã¨äº’æ›æ€§ã®ã‚ã‚‹ã™ã¹ã¦ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ç¢ºèªã™ã‚‹ã«ã¯ã€[ã‚¿ã‚¹ã‚¯ãƒšãƒ¼ã‚¸](https://huggingface.co/tasks/text-generation) ã‚’ç¢ºèªã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚u

</Tip>

å§‹ã‚ã‚‹å‰ã«ã€å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã™ã¹ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

```bash
pip install transformers datasets evaluate
```

ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¨å…±æœ‰ã§ãã‚‹ã‚ˆã†ã«ã€Hugging Face ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«ãƒ­ã‚°ã‚¤ãƒ³ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¡¨ç¤ºã•ã‚ŒãŸã‚‰ã€ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã™ã€‚

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## Load ELI5 dataset


ã¾ãšã€ELI5 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã® r/askscience ã‚µãƒ–ã‚»ãƒƒãƒˆã®å°ã•ã„ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’ ğŸ¤— ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
 ã“ã‚Œã«ã‚ˆã‚Šã€å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã•ã‚‰ã«æ™‚é–“ã‚’è²»ã‚„ã™å‰ã«ã€å®Ÿé¨“ã—ã¦ã™ã¹ã¦ãŒæ©Ÿèƒ½ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹æ©Ÿä¼šãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚

```py
>>> from datasets import load_dataset

>>> eli5 = load_dataset("eli5", split="train_asks[:5000]")
```

[`~datasets.Dataset.train_test_split`] ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã® `train_asks` ã‚’ãƒˆãƒ¬ã‚¤ãƒ³ ã‚»ãƒƒãƒˆã¨ãƒ†ã‚¹ãƒˆ ã‚»ãƒƒãƒˆã«åˆ†å‰²ã—ã¾ã™ã€‚

```py
>>> eli5 = eli5.train_test_split(test_size=0.2)
```

æ¬¡ã«ã€ä¾‹ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```py
>>> eli5["train"][0]
{'answers': {'a_id': ['c3d1aib', 'c3d4lya'],
  'score': [6, 3],
  'text': ["The velocity needed to remain in orbit is equal to the square root of Newton's constant times the mass of earth divided by the distance from the center of the earth. I don't know the altitude of that specific mission, but they're usually around 300 km. That means he's going 7-8 km/s.\n\nIn space there are no other forces acting on either the shuttle or the guy, so they stay in the same position relative to each other. If he were to become unable to return to the ship, he would presumably run out of oxygen, or slowly fall into the atmosphere and burn up.",
   "Hope you don't mind me asking another question, but why aren't there any stars visible in this photo?"]},
 'answers_urls': {'url': []},
 'document': '',
 'q_id': 'nyxfp',
 'selftext': '_URL_0_\n\nThis was on the front page earlier and I have a few questions about it. Is it possible to calculate how fast the astronaut would be orbiting the earth? Also how does he stay close to the shuttle so that he can return safely, i.e is he orbiting at the same speed and can therefore stay next to it? And finally if his propulsion system failed, would he eventually re-enter the atmosphere and presumably die?',
 'selftext_urls': {'url': ['http://apod.nasa.gov/apod/image/1201/freeflyer_nasa_3000.jpg']},
 'subreddit': 'askscience',
 'title': 'Few questions about this space walk photograph.',
 'title_urls': {'url': []}}
```

ã“ã‚Œã¯å¤šãã®ã“ã¨ã®ã‚ˆã†ã«è¦‹ãˆã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã€å®Ÿéš›ã«é–¢å¿ƒãŒã‚ã‚‹ã®ã¯`text`ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã ã‘ã§ã™ã€‚è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®å„ªã‚Œã¦ã„ã‚‹ç‚¹
ã‚¿ã‚¹ã‚¯ã§ã¯ã€æ¬¡ã®å˜èªãŒãƒ©ãƒ™ãƒ« * ã§ã‚ã‚‹ãŸã‚ã€ãƒ©ãƒ™ãƒ« (æ•™å¸«ãªã—ã‚¿ã‚¹ã‚¯ã¨ã‚‚å‘¼ã°ã‚Œã¾ã™) ã¯å¿…è¦ã‚ã‚Šã¾ã›ã‚“ã€‚


## Preprocess

<Youtube id="ma1TrR7gE7I"/>


æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¯ã€`text`ã‚µãƒ–ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å‡¦ç†ã™ã‚‹ãŸã‚ã« DistilGPT2 ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ã§ã™ã€‚

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilgpt2")
```

ä¸Šã®ä¾‹ã‹ã‚‰ã‚ã‹ã‚‹ã‚ˆã†ã«ã€`text`ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯å®Ÿéš›ã«ã¯`answers`å†…ã«ãƒã‚¹ãƒˆã•ã‚Œã¦ã„ã¾ã™ã€‚ã¤ã¾ã‚Šã€æ¬¡ã®ã“ã¨ãŒå¿…è¦ã«ãªã‚Šã¾ã™ã€‚
[` flatten`](https://huggingface.co/docs/datasets/process.html#flatten) ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€ãƒã‚¹ãƒˆã•ã‚ŒãŸæ§‹é€ ã‹ã‚‰ `text` ã‚µãƒ–ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŠ½å‡ºã—ã¾ã™ã€‚

```py
>>> eli5 = eli5.flatten()
>>> eli5["train"][0]
{'answers.a_id': ['c3d1aib', 'c3d4lya'],
 'answers.score': [6, 3],
 'answers.text': ["The velocity needed to remain in orbit is equal to the square root of Newton's constant times the mass of earth divided by the distance from the center of the earth. I don't know the altitude of that specific mission, but they're usually around 300 km. That means he's going 7-8 km/s.\n\nIn space there are no other forces acting on either the shuttle or the guy, so they stay in the same position relative to each other. If he were to become unable to return to the ship, he would presumably run out of oxygen, or slowly fall into the atmosphere and burn up.",
  "Hope you don't mind me asking another question, but why aren't there any stars visible in this photo?"],
 'answers_urls.url': [],
 'document': '',
 'q_id': 'nyxfp',
 'selftext': '_URL_0_\n\nThis was on the front page earlier and I have a few questions about it. Is it possible to calculate how fast the astronaut would be orbiting the earth? Also how does he stay close to the shuttle so that he can return safely, i.e is he orbiting at the same speed and can therefore stay next to it? And finally if his propulsion system failed, would he eventually re-enter the atmosphere and presumably die?',
 'selftext_urls.url': ['http://apod.nasa.gov/apod/image/1201/freeflyer_nasa_3000.jpg'],
 'subreddit': 'askscience',
 'title': 'Few questions about this space walk photograph.',
 'title_urls.url': []}
```

`answers`æ¥é ­è¾ã§ç¤ºã•ã‚Œã‚‹ã‚ˆã†ã«ã€å„ã‚µãƒ–ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯å€‹åˆ¥ã®åˆ—ã«ãªã‚Šã€`text`ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯ãƒªã‚¹ãƒˆã«ãªã‚Šã¾ã—ãŸã€‚ãã®ä»£ã‚ã‚Š
å„æ–‡ã‚’å€‹åˆ¥ã«ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã™ã‚‹å ´åˆã¯ã€ãƒªã‚¹ãƒˆã‚’æ–‡å­—åˆ—ã«å¤‰æ›ã—ã¦ã€ãã‚Œã‚‰ã‚’ã¾ã¨ã‚ã¦ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

ä»¥ä¸‹ã¯ã€å„ä¾‹ã®æ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆã‚’çµåˆã—ã€çµæœã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã™ã‚‹æœ€åˆã®å‰å‡¦ç†é–¢æ•°ã§ã™ã€‚

```py
>>> def preprocess_function(examples):
...     return tokenizer([" ".join(x) for x in examples["answers.text"]])
```

ã“ã®å‰å‡¦ç†é–¢æ•°ã‚’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã«é©ç”¨ã™ã‚‹ã«ã¯ã€ğŸ¤— Datasets [`~datasets.Dataset.map`] ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ `map` é–¢æ•°ã‚’é«˜é€ŸåŒ–ã™ã‚‹ã«ã¯ã€`batched=True` ã‚’è¨­å®šã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¤‡æ•°ã®è¦ç´ ã‚’ä¸€åº¦ã«å‡¦ç†ã—ã€`num_proc` ã§ãƒ—ãƒ­ã‚»ã‚¹ã®æ•°ã‚’å¢—ã‚„ã—ã¾ã™ã€‚ä¸è¦ãªåˆ—ã‚’å‰Šé™¤ã—ã¾ã™ã€‚

```py
>>> tokenized_eli5 = eli5.map(
...     preprocess_function,
...     batched=True,
...     num_proc=4,
...     remove_columns=eli5["train"].column_names,
... )
```

ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯ãƒˆãƒ¼ã‚¯ãƒ³ ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ãŒã€ãã®ä¸€éƒ¨ã¯ãƒ¢ãƒ‡ãƒ«ã®æœ€å¤§å…¥åŠ›é•·ã‚ˆã‚Šã‚‚é•·ããªã‚Šã¾ã™ã€‚

2 ç•ªç›®ã®å‰å‡¦ç†é–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ã€
- ã™ã¹ã¦ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’é€£çµã—ã¾ã™
- é€£çµã•ã‚ŒãŸã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’`block_size`ã§å®šç¾©ã•ã‚ŒãŸçŸ­ã„ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¾ã™ã€‚ã“ã‚Œã¯ã€æœ€å¤§å…¥åŠ›é•·ã‚ˆã‚ŠçŸ­ãã€GPU RAM ã«ååˆ†ãªé•·ã•ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

```py
>>> block_size = 128


>>> def group_texts(examples):
...     # Concatenate all texts.
...     concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
...     total_length = len(concatenated_examples[list(examples.keys())[0]])
...     # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can
...     # customize this part to your needs.
...     if total_length >= block_size:
...         total_length = (total_length // block_size) * block_size
...     # Split by chunks of block_size.
...     result = {
...         k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
...         for k, t in concatenated_examples.items()
...     }
...     result["labels"] = result["input_ids"].copy()
...     return result
```

Apply the `group_texts` function over the entire dataset:

```py
>>> lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)
```

æ¬¡ã«ã€[`DataCollatâ€‹â€‹orForLanguageModeling`] ã‚’ä½¿ç”¨ã—ã¦ã‚µãƒ³ãƒ—ãƒ«ã®ãƒãƒƒãƒã‚’ä½œæˆã—ã¾ã™ã€‚ *å‹•çš„ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°*ã™ã‚‹æ–¹ãŒåŠ¹ç‡çš„ã§ã™ã€‚
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã‚’æœ€å¤§é•·ã¾ã§ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹ã®ã§ã¯ãªãã€ç…§åˆä¸­ã«ãƒãƒƒãƒå†…ã®æ–‡ã‚’æœ€é•·ã®é•·ã•ã«ã—ã¾ã™ã€‚

<frameworkcontent>
<pt>

ã‚·ãƒ¼ã‚±ãƒ³ã‚¹çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚° ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦ä½¿ç”¨ã—ã€`mlm=False` ã‚’è¨­å®šã—ã¾ã™ã€‚ã“ã‚Œã¯ã€å…¥åŠ›ã‚’ 1 è¦ç´ åˆ†å³ã«ã‚·ãƒ•ãƒˆã—ãŸãƒ©ãƒ™ãƒ«ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚

```py
>>> from transformers import DataCollatorForLanguageModeling

>>> tokenizer.pad_token = tokenizer.eos_token
>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
```

</pt>
<tf>
ã‚·ãƒ¼ã‚±ãƒ³ã‚¹çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚° ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦ä½¿ç”¨ã—ã€`mlm=False` ã‚’è¨­å®šã—ã¾ã™ã€‚ã“ã‚Œã¯ã€å…¥åŠ›ã‚’ 1 è¦ç´ åˆ†å³ã«ã‚·ãƒ•ãƒˆã—ãŸãƒ©ãƒ™ãƒ«ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚

```py
>>> from transformers import DataCollatorForLanguageModeling

>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors="tf")
```

</tf>
</frameworkcontent>


## Train

<frameworkcontent>
<pt>
<Tip>

[`Trainer`] ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã«æ…£ã‚Œã¦ã„ãªã„å ´åˆã¯ã€[åŸºæœ¬ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](../training#train-with-pytorch-trainer) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

</Tip>

ã“ã‚Œã§ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚ [`AutoModelForCausalLM`] ã‚’ä½¿ç”¨ã—ã¦ DistilGPT2 ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚


```py
>>> from transformers import AutoModelForCausalLM, TrainingArguments, Trainer

>>> model = AutoModelForCausalLM.from_pretrained("distilbert/distilgpt2")
```

ã“ã®æ™‚ç‚¹ã§æ®‹ã£ã¦ã„ã‚‹æ‰‹é †ã¯æ¬¡ã® 3 ã¤ã ã‘ã§ã™ã€‚

1. [`TrainingArguments`] ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®šç¾©ã—ã¾ã™ã€‚å”¯ä¸€ã®å¿…é ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜å ´æ‰€ã‚’æŒ‡å®šã™ã‚‹ `output_dir` ã§ã™ã€‚ `push_to_hub=True`ã‚’è¨­å®šã—ã¦ã€ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ–ã«ãƒ—ãƒƒã‚·ãƒ¥ã—ã¾ã™ (ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã«ã¯ã€Hugging Face ã«ã‚µã‚¤ãƒ³ã‚¤ãƒ³ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™)ã€‚
2. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¼•æ•°ã‚’ãƒ¢ãƒ‡ãƒ«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ãƒ‡ãƒ¼ã‚¿ç…§åˆå™¨ã¨ã¨ã‚‚ã« [`Trainer`] ã«æ¸¡ã—ã¾ã™ã€‚
3. [`~Trainer.train`] ã‚’å‘¼ã³å‡ºã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚

```py
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_eli5_clm-model",
...     eval_strategy="epoch",
...     learning_rate=2e-5,
...     weight_decay=0.01,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=lm_dataset["train"],
...     eval_dataset=lm_dataset["test"],
...     data_collator=data_collator,
... )

>>> trainer.train()
```

ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã—ãŸã‚‰ã€ [`~transformers.Trainer.evaluate`] ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ã€ãã®è¤‡é›‘ã•ã‚’å–å¾—ã—ã¾ã™ã€‚

```py
>>> import math

>>> eval_results = trainer.evaluate()
>>> print(f"Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
Perplexity: 49.61
```

æ¬¡ã«ã€ [`~transformers.Trainer.push_to_hub`] ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ–ã«å…±æœ‰ã—ã€èª°ã‚‚ãŒãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

```py
>>> trainer.push_to_hub()
```
</pt>
<tf>
<Tip>

Keras ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã«æ…£ã‚Œã¦ã„ãªã„å ´åˆã¯ã€[åŸºæœ¬ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](../training#train-a-tensorflow-model-with-keras) ã‚’ã”è¦§ãã ã•ã„ã€‚

</Tip>
TensorFlow ã§ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã«ã¯ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼é–¢æ•°ã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã€ãŠã‚ˆã³ã„ãã¤ã‹ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹ã“ã¨ã‹ã‚‰å§‹ã‚ã¾ã™ã€‚

```py
>>> from transformers import create_optimizer, AdamWeightDecay

>>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)
```

æ¬¡ã«ã€[`TFAutoModelForCausalLM`] ã‚’ä½¿ç”¨ã—ã¦ DistilGPT2 ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚

```py
>>> from transformers import TFAutoModelForCausalLM

>>> model = TFAutoModelForCausalLM.from_pretrained("distilbert/distilgpt2")
```

[`~transformers.TFPreTrainedModel.prepare_tf_dataset`] ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ `tf.data.Dataset` å½¢å¼ã«å¤‰æ›ã—ã¾ã™ã€‚

```py
>>> tf_train_set = model.prepare_tf_dataset(
...     lm_dataset["train"],
...     shuffle=True,
...     batch_size=16,
...     collate_fn=data_collator,
... )

>>> tf_test_set = model.prepare_tf_dataset(
...     lm_dataset["test"],
...     shuffle=False,
...     batch_size=16,
...     collate_fn=data_collator,
... )
```

[`compile`](https://keras.io/api/models/model_training_apis/#compile-method) ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®šã—ã¾ã™ã€‚ Transformers ãƒ¢ãƒ‡ãƒ«ã«ã¯ã™ã¹ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¿ã‚¹ã‚¯é–¢é€£ã®æå¤±é–¢æ•°ãŒã‚ã‚‹ãŸã‚ã€æ¬¡ã®å ´åˆã‚’é™¤ãã€æå¤±é–¢æ•°ã‚’æŒ‡å®šã™ã‚‹å¿…è¦ã¯ãªã„ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚

```py
>>> import tensorflow as tf

>>> model.compile(optimizer=optimizer)  # No loss argument!
```

ã“ã‚Œã¯ã€ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ [`~transformers.PushToHubCallback`] ã§ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹å ´æ‰€ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§å®Ÿè¡Œã§ãã¾ã™ã€‚



```py
>>> from transformers.keras_callbacks import PushToHubCallback

>>> callback = PushToHubCallback(
...     output_dir="my_awesome_eli5_clm-model",
...     tokenizer=tokenizer,
... )
```

ã¤ã„ã«ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ã‚¨ãƒãƒƒã‚¯æ•°ã€ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’æŒ‡å®šã—ã¦ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) ã‚’å‘¼ã³å‡ºã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚



```py
>>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=[callback])
```

ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã™ã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«ã¯è‡ªå‹•çš„ã«ãƒãƒ–ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã€èª°ã§ã‚‚ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

</tf>
</frameworkcontent>

<Tip>

å› æœè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ç”¨ã«ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹æ–¹æ³•ã®ã‚ˆã‚Šè©³ç´°ãªä¾‹ã«ã¤ã„ã¦ã¯ã€å¯¾å¿œã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
[PyTorch ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)
ã¾ãŸã¯ [TensorFlow ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)ã€‚

</Tip>

## Inference

ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ãŸã®ã§ã€ãã‚Œã‚’æ¨è«–ã«ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚

ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è€ƒãˆå‡ºã—ã¾ã™ã€‚

```py
>>> prompt = "Somatic hypermutation allows the immune system to"
```

æ¨è«–ç”¨ã«å¾®èª¿æ•´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™æœ€ã‚‚ç°¡å˜ãªæ–¹æ³•ã¯ã€ãã‚Œã‚’ [`pipeline`] ã§ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆç”¨ã®`pipeline`ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã€ãã‚Œã«ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¸¡ã—ã¾ã™ã€‚


```py
>>> from transformers import pipeline

>>> generator = pipeline("text-generation", model="my_awesome_eli5_clm-model")
>>> generator(prompt)
[{'generated_text': "Somatic hypermutation allows the immune system to be able to effectively reverse the damage caused by an infection.\n\n\nThe damage caused by an infection is caused by the immune system's ability to perform its own self-correcting tasks."}]
```

<frameworkcontent>
<pt>


ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã€ã€Œinput_idsã€ã‚’ PyTorch ãƒ†ãƒ³ã‚½ãƒ«ã¨ã—ã¦è¿”ã—ã¾ã™ã€‚

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("my_awesome_eli5_clm-model")
>>> inputs = tokenizer(prompt, return_tensors="pt").input_ids
```

[`~generation.GenerationMixin.generate`] ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚
ã•ã¾ã–ã¾ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆæˆ¦ç•¥ã¨ç”Ÿæˆã‚’åˆ¶å¾¡ã™ã‚‹ãŸã‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€[ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆæˆ¦ç•¥](../generation_strategies) ãƒšãƒ¼ã‚¸ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

```py
>>> from transformers import AutoModelForCausalLM

>>> model = AutoModelForCausalLM.from_pretrained("my_awesome_eli5_clm-model")
>>> outputs = model.generate(inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)
```

ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ ID ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã«æˆ»ã—ã¾ã™ã€‚

```py
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
["Somatic hypermutation allows the immune system to react to drugs with the ability to adapt to a different environmental situation. In other words, a system of 'hypermutation' can help the immune system to adapt to a different environmental situation or in some cases even a single life. In contrast, researchers at the University of Massachusetts-Boston have found that 'hypermutation' is much stronger in mice than in humans but can be found in humans, and that it's not completely unknown to the immune system. A study on how the immune system"]
```

</pt>
<tf>

ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã€`input_ids`ã‚’ TensorFlow ãƒ†ãƒ³ã‚½ãƒ«ã¨ã—ã¦è¿”ã—ã¾ã™ã€‚

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("my_awesome_eli5_clm-model")
>>> inputs = tokenizer(prompt, return_tensors="tf").input_ids
```

[`~transformers.generation_tf_utils.TFGenerationMixin.generate`] ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦è¦ç´„ã‚’ä½œæˆã—ã¾ã™ã€‚ã•ã¾ã–ã¾ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆæˆ¦ç•¥ã¨ç”Ÿæˆã‚’åˆ¶å¾¡ã™ã‚‹ãŸã‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€[ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆæˆ¦ç•¥](../generation_strategies) ãƒšãƒ¼ã‚¸ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

```py
>>> from transformers import TFAutoModelForCausalLM

>>> model = TFAutoModelForCausalLM.from_pretrained("my_awesome_eli5_clm-model")
>>> outputs = model.generate(input_ids=inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)
```

ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ ID ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã«æˆ»ã—ã¾ã™ã€‚

```py
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
['Somatic hypermutation allows the immune system to detect the presence of other viruses as they become more prevalent. Therefore, researchers have identified a high proportion of human viruses. The proportion of virus-associated viruses in our study increases with age. Therefore, we propose a simple algorithm to detect the presence of these new viruses in our samples as a sign of improved immunity. A first study based on this algorithm, which will be published in Science on Friday, aims to show that this finding could translate into the development of a better vaccine that is more effective for']
```

</tf>
</frameworkcontent>
