<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Question answering

[[open-in-colab]]

<Youtube id="ajPx5LwJD-I"/>

è³ªå•å¿œç­”ã‚¿ã‚¹ã‚¯ã¯ã€è³ªå•ã«å¯¾ã—ã¦å›ç­”ã‚’è¿”ã—ã¾ã™ã€‚ Alexaã€Siriã€Google ãªã©ã®ä»®æƒ³ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã«å¤©æ°—ã‚’å°‹ã­ãŸã“ã¨ãŒã‚ã‚‹ãªã‚‰ã€è³ªå•å¿œç­”ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸã“ã¨ãŒã‚ã‚‹ã¯ãšã§ã™ã€‚è³ªå•å¿œç­”ã‚¿ã‚¹ã‚¯ã«ã¯ä¸€èˆ¬çš„ã« 2 ã¤ã®ã‚¿ã‚¤ãƒ—ãŒã‚ã‚Šã¾ã™ã€‚

- æŠ½å‡º: ä¸ãˆã‚‰ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å›ç­”ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
- æŠ½è±¡çš„: è³ªå•ã«æ­£ã—ãç­”ãˆã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å›ç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€æ¬¡ã®æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚

1. æŠ½å‡ºçš„è³ªå•å¿œç­”ç”¨ã« [SQuAD](https://huggingface.co/datasets/squad) ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸Šã® [DistilBERT](https://huggingface.co/distilbert/distilbert-base-uncased) ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚
2. å¾®èª¿æ•´ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’æ¨è«–ã«ä½¿ç”¨ã—ã¾ã™ã€‚

<Tip>

ã“ã®ã‚¿ã‚¹ã‚¯ã¨äº’æ›æ€§ã®ã‚ã‚‹ã™ã¹ã¦ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ç¢ºèªã™ã‚‹ã«ã¯ã€[ã‚¿ã‚¹ã‚¯ãƒšãƒ¼ã‚¸](https://huggingface.co/tasks/question-answering) ã‚’ç¢ºèªã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚

</Tip>

å§‹ã‚ã‚‹å‰ã«ã€å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã™ã¹ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

```bash
pip install transformers datasets evaluate
```

ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¨å…±æœ‰ã§ãã‚‹ã‚ˆã†ã«ã€Hugging Face ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«ãƒ­ã‚°ã‚¤ãƒ³ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¡¨ç¤ºã•ã‚ŒãŸã‚‰ã€ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã™ã€‚

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## Load SQuAD dataset

ã¾ãšã€ğŸ¤— ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‹ã‚‰ SQuAD ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å°ã•ã„ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã•ã‚‰ã«æ™‚é–“ã‚’è²»ã‚„ã™å‰ã«ã€å®Ÿé¨“ã—ã¦ã™ã¹ã¦ãŒæ©Ÿèƒ½ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹æ©Ÿä¼šãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚


```py
>>> from datasets import load_dataset

>>> squad = load_dataset("squad", split="train[:5000]")
```

[`~datasets.Dataset.train_test_split`] ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã® `train` åˆ†å‰²ã‚’ãƒˆãƒ¬ã‚¤ãƒ³ ã‚»ãƒƒãƒˆã¨ãƒ†ã‚¹ãƒˆ ã‚»ãƒƒãƒˆã«åˆ†å‰²ã—ã¾ã™ã€‚

```py
>>> squad = squad.train_test_split(test_size=0.2)
```

æ¬¡ã«ã€ä¾‹ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```py
>>> squad["train"][0]
{'answers': {'answer_start': [515], 'text': ['Saint Bernadette Soubirous']},
 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend "Venite Ad Me Omnes". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',
 'id': '5733be284776f41900661182',
 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',
 'title': 'University_of_Notre_Dame'
}
```

ã“ã“ã«ã¯ã„ãã¤ã‹ã®é‡è¦ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒã‚ã‚Šã¾ã™ã€‚

- `answers`: å›ç­”ãƒˆãƒ¼ã‚¯ãƒ³ã¨å›ç­”ãƒ†ã‚­ã‚¹ãƒˆã®é–‹å§‹ä½ç½®ã€‚
- `context`: ãƒ¢ãƒ‡ãƒ«ãŒç­”ãˆã‚’æŠ½å‡ºã™ã‚‹ãŸã‚ã«å¿…è¦ãªèƒŒæ™¯æƒ…å ±ã€‚
- `question`: ãƒ¢ãƒ‡ãƒ«ãŒç­”ãˆã‚‹å¿…è¦ãŒã‚ã‚‹è³ªå•ã€‚

## Preprocess

<Youtube id="qgaM0weJHpA"/>

æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€DistilBERT ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦`question`ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¨`context`ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å‡¦ç†ã—ã¾ã™ã€‚


```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
```

è³ªå•å¿œç­”ã‚¿ã‚¹ã‚¯ã«ç‰¹æœ‰ã®ã€æ³¨æ„ã™ã¹ãå‰å‡¦ç†æ‰‹é †ãŒã„ãã¤ã‹ã‚ã‚Šã¾ã™ã€‚

1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®ä¸€éƒ¨ã®ä¾‹ã«ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®æœ€å¤§å…¥åŠ›é•·ã‚’è¶…ãˆã‚‹éå¸¸ã«é•·ã„ã€Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€ãŒå«ã¾ã‚Œã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚ã‚ˆã‚Šé•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å‡¦ç†ã™ã‚‹ã«ã¯ã€`truncation="only_second"` ã‚’è¨­å®šã—ã¦ `context` ã®ã¿ã‚’åˆ‡ã‚Šæ¨ã¦ã¾ã™ã€‚
2. æ¬¡ã«ã€è¨­å®šã«ã‚ˆã£ã¦ã€å›ç­”ã®é–‹å§‹ä½ç½®ã¨çµ‚äº†ä½ç½®ã‚’å…ƒã® `context`ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã—ã¾ã™ã€‚
   ã€Œ`return_offset_mapping=True`ã€ã€‚
3. ãƒãƒƒãƒ”ãƒ³ã‚°ãŒæ‰‹å…ƒã«ã‚ã‚‹ã®ã§ã€ç­”ãˆã®é–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³ã¨çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ [`~tokenizers.Encoding.sequence_ids`] ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€
   ã‚ªãƒ•ã‚»ãƒƒãƒˆã®ã©ã®éƒ¨åˆ†ãŒ`question`ã«å¯¾å¿œã—ã€ã©ã®éƒ¨åˆ†ãŒ`context`ã«å¯¾å¿œã™ã‚‹ã‹ã‚’è¦‹ã¤ã‘ã¾ã™ã€‚

ä»¥ä¸‹ã«ã€`answer`ã®é–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³ã¨çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³ã‚’åˆ‡ã‚Šè©°ã‚ã¦`context`ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹é–¢æ•°ã‚’ä½œæˆã™ã‚‹æ–¹æ³•ã‚’ç¤ºã—ã¾ã™ã€‚

```py
>>> def preprocess_function(examples):
...     questions = [q.strip() for q in examples["question"]]
...     inputs = tokenizer(
...         questions,
...         examples["context"],
...         max_length=384,
...         truncation="only_second",
...         return_offsets_mapping=True,
...         padding="max_length",
...     )

...     offset_mapping = inputs.pop("offset_mapping")
...     answers = examples["answers"]
...     start_positions = []
...     end_positions = []

...     for i, offset in enumerate(offset_mapping):
...         answer = answers[i]
...         start_char = answer["answer_start"][0]
...         end_char = answer["answer_start"][0] + len(answer["text"][0])
...         sequence_ids = inputs.sequence_ids(i)

...         # Find the start and end of the context
...         idx = 0
...         while sequence_ids[idx] != 1:
...             idx += 1
...         context_start = idx
...         while sequence_ids[idx] == 1:
...             idx += 1
...         context_end = idx - 1

...         # If the answer is not fully inside the context, label it (0, 0)
...         if offset[context_start][0] > end_char or offset[context_end][1] < start_char:
...             start_positions.append(0)
...             end_positions.append(0)
...         else:
...             # Otherwise it's the start and end token positions
...             idx = context_start
...             while idx <= context_end and offset[idx][0] <= start_char:
...                 idx += 1
...             start_positions.append(idx - 1)

...             idx = context_end
...             while idx >= context_start and offset[idx][1] >= end_char:
...                 idx -= 1
...             end_positions.append(idx + 1)

...     inputs["start_positions"] = start_positions
...     inputs["end_positions"] = end_positions
...     return inputs
```

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã«å‰å‡¦ç†é–¢æ•°ã‚’é©ç”¨ã™ã‚‹ã«ã¯ã€ğŸ¤— Datasets [`~datasets.Dataset.map`] é–¢æ•°ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ `batched=True` ã‚’è¨­å®šã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¤‡æ•°ã®è¦ç´ ã‚’ä¸€åº¦ã«å‡¦ç†ã™ã‚‹ã“ã¨ã§ã€`map` é–¢æ•°ã‚’é«˜é€ŸåŒ–ã§ãã¾ã™ã€‚ä¸è¦ãªåˆ—ã‚’å‰Šé™¤ã—ã¾ã™ã€‚

```py
>>> tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad["train"].column_names)
```

æ¬¡ã«ã€[`DefaultDataCollatâ€‹â€‹or`] ã‚’ä½¿ç”¨ã—ã¦ã‚µãƒ³ãƒ—ãƒ«ã®ãƒãƒƒãƒã‚’ä½œæˆã—ã¾ã™ã€‚ ğŸ¤— Transformers ã®ä»–ã®ãƒ‡ãƒ¼ã‚¿ç…§åˆå™¨ã¨ã¯ç•°ãªã‚Šã€[`DefaultDataCollatâ€‹â€‹or`] ã¯ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãªã©ã®è¿½åŠ ã®å‰å‡¦ç†ã‚’é©ç”¨ã—ã¾ã›ã‚“ã€‚

<frameworkcontent>
<pt>
```py
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator()
```
</pt>
<tf>
```py
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator(return_tensors="tf")
```
</tf>
</frameworkcontent>

## Train

<frameworkcontent>
<pt>
<Tip>

[`Trainer`] ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã«æ…£ã‚Œã¦ã„ãªã„å ´åˆã¯ã€[ã“ã“](../training#train-with-pytorch-trainer) ã®åŸºæœ¬çš„ãªãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’ã”è¦§ãã ã•ã„ã€‚

</Tip>

ã“ã‚Œã§ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚ [`AutoModelForQuestionAnswering`] ã‚’ä½¿ç”¨ã—ã¦ DitilBERT ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

```py
>>> from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer

>>> model = AutoModelForQuestionAnswering.from_pretrained("distilbert/distilbert-base-uncased")
```

ã“ã®æ™‚ç‚¹ã§æ®‹ã£ã¦ã„ã‚‹æ‰‹é †ã¯æ¬¡ã® 3 ã¤ã ã‘ã§ã™ã€‚

1. [`TrainingArguments`] ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®šç¾©ã—ã¾ã™ã€‚å”¯ä¸€ã®å¿…é ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜å ´æ‰€ã‚’æŒ‡å®šã™ã‚‹ `output_dir` ã§ã™ã€‚ `push_to_hub=True`ã‚’è¨­å®šã—ã¦ã€ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ–ã«ãƒ—ãƒƒã‚·ãƒ¥ã—ã¾ã™ (ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã«ã¯ã€Hugging Face ã«ã‚µã‚¤ãƒ³ã‚¤ãƒ³ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™)ã€‚
2. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¼•æ•°ã‚’ãƒ¢ãƒ‡ãƒ«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã€ãƒ‡ãƒ¼ã‚¿ç…§åˆå™¨ã¨ã¨ã‚‚ã« [`Trainer`] ã«æ¸¡ã—ã¾ã™ã€‚
3. [`~Trainer.train`] ã‚’å‘¼ã³å‡ºã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚

```py
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_qa_model",
...     eval_strategy="epoch",
...     learning_rate=2e-5,
...     per_device_train_batch_size=16,
...     per_device_eval_batch_size=16,
...     num_train_epochs=3,
...     weight_decay=0.01,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_squad["train"],
...     eval_dataset=tokenized_squad["test"],
...     tokenizer=tokenizer,
...     data_collator=data_collator,
... )

>>> trainer.train()
```

ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã—ãŸã‚‰ã€ [`~transformers.Trainer.push_to_hub`] ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ–ã«å…±æœ‰ã—ã€èª°ã‚‚ãŒãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚


```py
>>> trainer.push_to_hub()
```
</pt>
<tf>
<Tip>

Keras ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã«æ…£ã‚Œã¦ã„ãªã„å ´åˆã¯ã€[ã“ã¡ã‚‰](../training#train-a-tensorflow-model-with-keras) ã®åŸºæœ¬çš„ãªãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’ã”è¦§ãã ã•ã„ã€‚

</Tip>

</ãƒ’ãƒ³ãƒˆ>
TensorFlow ã§ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã«ã¯ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼é–¢æ•°ã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã€ãŠã‚ˆã³ã„ãã¤ã‹ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹ã“ã¨ã‹ã‚‰å§‹ã‚ã¾ã™ã€‚

```py
>>> from transformers import create_optimizer

>>> batch_size = 16
>>> num_epochs = 2
>>> total_train_steps = (len(tokenized_squad["train"]) // batch_size) * num_epochs
>>> optimizer, schedule = create_optimizer(
...     init_lr=2e-5,
...     num_warmup_steps=0,
...     num_train_steps=total_train_steps,
... )
```

æ¬¡ã«ã€[`TFAutoModelForQuestionAnswering`] ã‚’ä½¿ç”¨ã—ã¦ DistilBERT ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚

```py
>>> from transformers import TFAutoModelForQuestionAnswering

>>> model = TFAutoModelForQuestionAnswering("distilbert/distilbert-base-uncased")
```

[`~transformers.TFPreTrainedModel.prepare_tf_dataset`] ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ `tf.data.Dataset` å½¢å¼ã«å¤‰æ›ã—ã¾ã™ã€‚

```py
>>> tf_train_set = model.prepare_tf_dataset(
...     tokenized_squad["train"],
...     shuffle=True,
...     batch_size=16,
...     collate_fn=data_collator,
... )

>>> tf_validation_set = model.prepare_tf_dataset(
...     tokenized_squad["test"],
...     shuffle=False,
...     batch_size=16,
...     collate_fn=data_collator,
... )
```

[`compile`](https://keras.io/api/models/model_training_apis/#compile-method) ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®šã—ã¾ã™ã€‚

```py
>>> import tensorflow as tf

>>> model.compile(optimizer=optimizer)
```

ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹å‰ã«æœ€å¾Œã«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹ã“ã¨ã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ–ã«ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹æ–¹æ³•ã‚’æä¾›ã™ã‚‹ã“ã¨ã§ã™ã€‚ã“ã‚Œã¯ã€ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ [`~transformers.PushToHubCallback`] ã§ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹å ´æ‰€ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§å®Ÿè¡Œã§ãã¾ã™ã€‚

```py
>>> from transformers.keras_callbacks import PushToHubCallback

>>> callback = PushToHubCallback(
...     output_dir="my_awesome_qa_model",
...     tokenizer=tokenizer,
... )
```

ã¤ã„ã«ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ã‚¨ãƒãƒƒã‚¯æ•°ã€ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’æŒ‡å®šã—ã¦ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) ã‚’å‘¼ã³å‡ºã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚

```py
>>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=[callback])
```

ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã™ã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«ã¯è‡ªå‹•çš„ã«ãƒãƒ–ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã€èª°ã§ã‚‚ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
</tf>
</frameworkcontent>

<Tip>

è³ªå•å¿œç­”ç”¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹æ–¹æ³•ã®è©³ç´°ãªä¾‹ã«ã¤ã„ã¦ã¯ã€å¯¾å¿œã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
[PyTorch ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)
ã¾ãŸã¯ [TensorFlow ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb)ã€‚

</Tip>

## Evaluate

è³ªå•å¿œç­”ã®è©•ä¾¡ã«ã¯ã€å¤§é‡ã®å¾Œå‡¦ç†ãŒå¿…è¦ã§ã™ã€‚æ™‚é–“ãŒã‹ã‹ã‚Šã™ããªã„ã‚ˆã†ã«ã€ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯è©•ä¾¡ã‚¹ãƒ†ãƒƒãƒ—ã‚’çœç•¥ã—ã¦ã„ã¾ã™ã€‚ [`Trainer`] ã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«è©•ä¾¡æå¤±ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«ã¤ã„ã¦å®Œå…¨ã«åˆ†ã‹ã‚‰ãªã„ã‚ã‘ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

ã‚‚ã£ã¨æ™‚é–“ãŒã‚ã‚Šã€è³ªå•å¿œç­”ç”¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã™ã‚‹æ–¹æ³•ã«èˆˆå‘³ãŒã‚ã‚‹å ´åˆã¯ã€[è³ªå•å¿œç­”](https://huggingface.co/course/chapter7/7?fw=pt#postprocessing) ã®ç« ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚ ğŸ¤—ãƒã‚°ãƒ•ã‚§ã‚¤ã‚¹ã‚³ãƒ¼ã‚¹ã‹ã‚‰ï¼

## Inference

ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ãŸã®ã§ã€ãã‚Œã‚’æ¨è«–ã«ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚

è³ªå•ã¨ã€ãƒ¢ãƒ‡ãƒ«ã«äºˆæ¸¬ã•ã›ãŸã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è€ƒãˆå‡ºã—ã¾ã™ã€‚

```py
>>> question = "How many programming languages does BLOOM support?"
>>> context = "BLOOM has 176 billion parameters and can generate text in 46 languages natural languages and 13 programming languages."
```

æ¨è«–ç”¨ã«å¾®èª¿æ•´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™æœ€ã‚‚ç°¡å˜ãªæ–¹æ³•ã¯ã€ãã‚Œã‚’ [`pipeline`] ã§ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦è³ªå•å¿œç­”ç”¨ã®`pipeline`ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã€ãã‚Œã«ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¸¡ã—ã¾ã™ã€‚

```py
>>> from transformers import pipeline

>>> question_answerer = pipeline("question-answering", model="my_awesome_qa_model")
>>> question_answerer(question=question, context=context)
{'score': 0.2058267742395401,
 'start': 10,
 'end': 95,
 'answer': '176 billion parameters and can generate text in 46 languages natural languages and 13'}
```

å¿…è¦ã«å¿œã˜ã¦ã€`pipeline`ã®çµæœã‚’æ‰‹å‹•ã§è¤‡è£½ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚

<frameworkcontent>
<pt>

ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¦ PyTorch ãƒ†ãƒ³ã‚½ãƒ«ã‚’è¿”ã—ã¾ã™ã€‚

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("my_awesome_qa_model")
>>> inputs = tokenizer(question, context, return_tensors="pt")
```

å…¥åŠ›ã‚’ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã—ã€`logits`ã‚’è¿”ã—ã¾ã™ã€‚


```py
>>> import torch
>>> from transformers import AutoModelForQuestionAnswering

>>> model = AutoModelForQuestionAnswering.from_pretrained("my_awesome_qa_model")
>>> with torch.no_grad():
...     outputs = model(**inputs)
```

ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã‹ã‚‰é–‹å§‹ä½ç½®ã¨çµ‚äº†ä½ç½®ã®æœ€ã‚‚é«˜ã„ç¢ºç‡ã‚’å–å¾—ã—ã¾ã™ã€‚

```py
>>> answer_start_index = outputs.start_logits.argmax()
>>> answer_end_index = outputs.end_logits.argmax()
```

äºˆæ¸¬ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦ç­”ãˆã‚’å–å¾—ã—ã¾ã™ã€‚

```py
>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
>>> tokenizer.decode(predict_answer_tokens)
'176 billion parameters and can generate text in 46 languages natural languages and 13'
```
</pt>
<tf>

ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã€TensorFlow ãƒ†ãƒ³ã‚½ãƒ«ã‚’è¿”ã—ã¾ã™ã€‚

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("my_awesome_qa_model")
>>> inputs = tokenizer(question, text, return_tensors="tf")
```

å…¥åŠ›ã‚’ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã—ã€`logits`ã‚’è¿”ã—ã¾ã™ã€‚


```py
>>> from transformers import TFAutoModelForQuestionAnswering

>>> model = TFAutoModelForQuestionAnswering.from_pretrained("my_awesome_qa_model")
>>> outputs = model(**inputs)
```

ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã‹ã‚‰é–‹å§‹ä½ç½®ã¨çµ‚äº†ä½ç½®ã®æœ€ã‚‚é«˜ã„ç¢ºç‡ã‚’å–å¾—ã—ã¾ã™ã€‚

```py
>>> answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])
>>> answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])
```

äºˆæ¸¬ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦ç­”ãˆã‚’å–å¾—ã—ã¾ã™ã€‚

```py
>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
>>> tokenizer.decode(predict_answer_tokens)
'176 billion parameters and can generate text in 46 languages natural languages and 13'
```

</tf>
</frameworkcontent>
