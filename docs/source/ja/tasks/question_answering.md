
ã“ã‚Œã§ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚ [`AutoModelForQuestionAnswering`] ã‚’ä½¿ç”¨ã—ã¦ DitilBERT ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

```py
>>> from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer

>>> model = AutoModelForQuestionAnswering.from_pretrained("distilbert/distilbert-base-uncased")
```

ã“ã®æ™‚ç‚¹ã§æ®‹ã£ã¦ã„ã‚‹æ‰‹é †ã¯æ¬¡ã® 3 ã¤ã ã‘ã§ã™ã€‚

1. [`TrainingArguments`] ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®šç¾©ã—ã¾ã™ã€‚å”¯ä¸€ã®å¿…é ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜å ´æ‰€ã‚’æŒ‡å®šã™ã‚‹ `output_dir` ã§ã™ã€‚ `push_to_hub=True`ã‚’è¨­å®šã—ã¦ã€ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ–ã«ãƒ—ãƒƒã‚·ãƒ¥ã—ã¾ã™ (ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã«ã¯ã€Hugging Face ã«ã‚µã‚¤ãƒ³ã‚¤ãƒ³ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™)ã€‚
2. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¼•æ•°ã‚’ãƒ¢ãƒ‡ãƒ«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã€ãƒ‡ãƒ¼ã‚¿ç…§åˆå™¨ã¨ã¨ã‚‚ã« [`Trainer`] ã«æ¸¡ã—ã¾ã™ã€‚
3. [`~Trainer.train`] ã‚’å‘¼ã³å‡ºã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚

```py
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_qa_model",
...     eval_strategy="epoch",
...     learning_rate=2e-5,
...     per_device_train_batch_size=16,
...     per_device_eval_batch_size=16,
...     num_train_epochs=3,
...     weight_decay=0.01,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_squad["train"],
...     eval_dataset=tokenized_squad["test"],
...     processing_class=tokenizer,
...     data_collator=data_collator,
... )

>>> trainer.train()
```

ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã—ãŸã‚‰ã€ [`~transformers.Trainer.push_to_hub`] ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ–ã«å…±æœ‰ã—ã€èª°ã‚‚ãŒãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚


```py
>>> trainer.push_to_hub()
```

<Tip>

è³ªå•å¿œç­”ç”¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹æ–¹æ³•ã®è©³ç´°ãªä¾‹ã«ã¤ã„ã¦ã¯ã€å¯¾å¿œã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
[PyTorch ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)
ã¾ãŸã¯ [TensorFlow ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb)ã€‚

</Tip>

## Evaluate

è³ªå•å¿œç­”ã®è©•ä¾¡ã«ã¯ã€å¤§é‡ã®å¾Œå‡¦ç†ãŒå¿…è¦ã§ã™ã€‚æ™‚é–“ãŒã‹ã‹ã‚Šã™ããªã„ã‚ˆã†ã«ã€ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯è©•ä¾¡ã‚¹ãƒ†ãƒƒãƒ—ã‚’çœç•¥ã—ã¦ã„ã¾ã™ã€‚ [`Trainer`] ã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«è©•ä¾¡æå¤±ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«ã¤ã„ã¦å®Œå…¨ã«åˆ†ã‹ã‚‰ãªã„ã‚ã‘ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

ã‚‚ã£ã¨æ™‚é–“ãŒã‚ã‚Šã€è³ªå•å¿œç­”ç”¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã™ã‚‹æ–¹æ³•ã«èˆˆå‘³ãŒã‚ã‚‹å ´åˆã¯ã€[è³ªå•å¿œç­”](https://huggingface.co/course/chapter7/7?fw=pt#postprocessing) ã®ç« ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚ ğŸ¤—ãƒã‚°ãƒ•ã‚§ã‚¤ã‚¹ã‚³ãƒ¼ã‚¹ã‹ã‚‰ï¼

## Inference

ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ãŸã®ã§ã€ãã‚Œã‚’æ¨è«–ã«ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚

è³ªå•ã¨ã€ãƒ¢ãƒ‡ãƒ«ã«äºˆæ¸¬ã•ã›ãŸã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è€ƒãˆå‡ºã—ã¾ã™ã€‚

```py
>>> question = "How many programming languages does BLOOM support?"
>>> context = "BLOOM has 176 billion parameters and can generate text in 46 languages natural languages and 13 programming languages."
```

æ¨è«–ç”¨ã«å¾®èª¿æ•´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™æœ€ã‚‚ç°¡å˜ãªæ–¹æ³•ã¯ã€ãã‚Œã‚’ [`pipeline`] ã§ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦è³ªå•å¿œç­”ç”¨ã®`pipeline`ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã€ãã‚Œã«ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¸¡ã—ã¾ã™ã€‚

```py
>>> from transformers import pipeline

>>> question_answerer = pipeline("question-answering", model="my_awesome_qa_model")
>>> question_answerer(question=question, context=context)
{'score': 0.2058267742395401,
 'start': 10,
 'end': 95,
 'answer': '176 billion parameters and can generate text in 46 languages natural languages and 13'}
```

å¿…è¦ã«å¿œã˜ã¦ã€`pipeline`ã®çµæœã‚’æ‰‹å‹•ã§è¤‡è£½ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚


ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¦ PyTorch ãƒ†ãƒ³ã‚½ãƒ«ã‚’è¿”ã—ã¾ã™ã€‚

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("my_awesome_qa_model")
>>> inputs = tokenizer(question, context, return_tensors="pt")
```

å…¥åŠ›ã‚’ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã—ã€`logits`ã‚’è¿”ã—ã¾ã™ã€‚


```py
>>> import torch
>>> from transformers import AutoModelForQuestionAnswering

>>> model = AutoModelForQuestionAnswering.from_pretrained("my_awesome_qa_model")
>>> with torch.no_grad():
...     outputs = model(**inputs)
```

ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã‹ã‚‰é–‹å§‹ä½ç½®ã¨çµ‚äº†ä½ç½®ã®æœ€ã‚‚é«˜ã„ç¢ºç‡ã‚’å–å¾—ã—ã¾ã™ã€‚

```py
>>> answer_start_index = outputs.start_logits.argmax()
>>> answer_end_index = outputs.end_logits.argmax()
```

äºˆæ¸¬ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦ç­”ãˆã‚’å–å¾—ã—ã¾ã™ã€‚

```py
>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
>>> tokenizer.decode(predict_answer_tokens)
'176 billion parameters and can generate text in 46 languages natural languages and 13'
```
