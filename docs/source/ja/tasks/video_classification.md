<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Video classification

[[open-in-colab]]


ãƒ“ãƒ‡ã‚ªåˆ†é¡žã¯ã€ãƒ“ãƒ‡ã‚ªå…¨ä½“ã«ãƒ©ãƒ™ãƒ«ã¾ãŸã¯ã‚¯ãƒ©ã‚¹ã‚’å‰²ã‚Šå½“ã¦ã‚‹ã‚¿ã‚¹ã‚¯ã§ã™ã€‚ãƒ“ãƒ‡ã‚ªã«ã¯ã€å„ãƒ“ãƒ‡ã‚ªã« 1 ã¤ã®ã‚¯ãƒ©ã‚¹ã®ã¿ãŒå«ã¾ã‚Œã‚‹ã“ã¨ãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚ãƒ“ãƒ‡ã‚ªåˆ†é¡žãƒ¢ãƒ‡ãƒ«ã¯ãƒ“ãƒ‡ã‚ªã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å–ã‚Šã€ãƒ“ãƒ‡ã‚ªãŒã©ã®ã‚¯ãƒ©ã‚¹ã«å±žã™ã‚‹ã‹ã«ã¤ã„ã¦ã®äºˆæ¸¬ã‚’è¿”ã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ“ãƒ‡ã‚ªã®å†…å®¹ã‚’åˆ†é¡žã§ãã¾ã™ã€‚ãƒ“ãƒ‡ã‚ªåˆ†é¡žã®å®Ÿéš›ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã‚¢ã‚¯ã‚·ãƒ§ãƒ³/ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£èªè­˜ã§ã‚ã‚Šã€ãƒ•ã‚£ãƒƒãƒˆãƒã‚¹ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«å½¹ç«‹ã¡ã¾ã™ã€‚ã¾ãŸã€è¦–è¦šéšœå®³ã®ã‚ã‚‹äººã«ã¨ã£ã¦ã€ç‰¹ã«é€šå‹¤æ™‚ã«å½¹ç«‹ã¡ã¾ã™ã€‚

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€æ¬¡ã®æ–¹æ³•ã‚’èª¬æ˜Žã—ã¾ã™ã€‚

1. [UCF101](https://www.crcv.ucf.edu/) ã®ã‚µãƒ–ã‚»ãƒƒãƒˆã§ [VideoMAE](https://huggingface.co/docs/transformers/main/en/model_doc/videomae) ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚ data/UCF101.php) ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
2. å¾®èª¿æ•´ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’æŽ¨è«–ã«ä½¿ç”¨ã—ã¾ã™ã€‚

> [!TIP]
> ã“ã®ã‚¿ã‚¹ã‚¯ã¨äº’æ›æ€§ã®ã‚ã‚‹ã™ã¹ã¦ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ç¢ºèªã™ã‚‹ã«ã¯ã€[ã‚¿ã‚¹ã‚¯ãƒšãƒ¼ã‚¸](https://huggingface.co/tasks/video-classification) ã‚’ç¢ºèªã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚

å§‹ã‚ã‚‹å‰ã«ã€å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã™ã¹ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚
```bash
pip install -q pytorchvideo transformers evaluate
```

[PyTorchVideo](https://pytorchvideo.org/) (`pytorchvideo` ã¨å‘¼ã°ã‚Œã¾ã™) ã‚’ä½¿ç”¨ã—ã¦ãƒ“ãƒ‡ã‚ªã‚’å‡¦ç†ã—ã€æº–å‚™ã—ã¾ã™ã€‚

ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¨å…±æœ‰ã§ãã‚‹ã‚ˆã†ã«ã€Hugging Face ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«ãƒ­ã‚°ã‚¤ãƒ³ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¡¨ç¤ºã•ã‚ŒãŸã‚‰ã€ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã™ã€‚

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## Load UCF101 dataset

ã¾ãšã€[UCF-101 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ](https://www.crcv.ucf.edu/data/UCF101.php) ã®ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã•ã‚‰ã«æ™‚é–“ã‚’è²»ã‚„ã™å‰ã«ã€å®Ÿé¨“ã—ã¦ã™ã¹ã¦ãŒæ©Ÿèƒ½ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹æ©Ÿä¼šãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚

```py
>>> from huggingface_hub import hf_hub_download

>>> hf_dataset_identifier = "sayakpaul/ucf101-subset"
>>> filename = "UCF101_subset.tar.gz"
>>> file_path = hf_hub_download(repo_id=hf_dataset_identifier, filename=filename, repo_type="dataset")
```

ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸå¾Œã€åœ§ç¸®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚’æŠ½å‡ºã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

```py
>>> import tarfile

>>> with tarfile.open(file_path) as t:
...      t.extractall(".")
```

å¤§ã¾ã‹ã«è¨€ã†ã¨ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯æ¬¡ã®ã‚ˆã†ã«æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚

```bash
UCF101_subset/
    train/
        BandMarching/
            video_1.mp4
            video_2.mp4
            ...
        Archery
            video_1.mp4
            video_2.mp4
            ...
        ...
    val/
        BandMarching/
            video_1.mp4
            video_2.mp4
            ...
        Archery
            video_1.mp4
            video_2.mp4
            ...
        ...
    test/
        BandMarching/
            video_1.mp4
            video_2.mp4
            ...
        Archery
            video_1.mp4
            video_2.mp4
            ...
        ...
```

(`sorted`)ã•ã‚ŒãŸ ãƒ“ãƒ‡ã‚ª ãƒ‘ã‚¹ã¯æ¬¡ã®ã‚ˆã†ã«è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚


```bash
...
'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c04.avi',
'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c06.avi',
'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi',
'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c02.avi',
'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c06.avi'
...
```

åŒã˜ã‚°ãƒ«ãƒ¼ãƒ—/ã‚·ãƒ¼ãƒ³ã«å±žã™ã‚‹ãƒ“ãƒ‡ã‚ª ã‚¯ãƒªãƒƒãƒ—ãŒã‚ã‚Šã€ãƒ“ãƒ‡ã‚ª ãƒ•ã‚¡ã‚¤ãƒ« ãƒ‘ã‚¹ã§ã¯ã‚°ãƒ«ãƒ¼ãƒ—ãŒ`g`ã§ç¤ºã•ã‚Œã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ãŸã¨ãˆã°ã€`v_ApplyEyeMakeup_g07_c04.avi`ã‚„`v_ApplyEyeMakeup_g07_c06.avi`ãªã©ã§ã™ã€‚

æ¤œè¨¼ã¨è©•ä¾¡ã®åˆ†å‰²ã§ã¯ã€[ãƒ‡ãƒ¼ã‚¿æ¼æ´©](https://www.kaggle.com/code/alexisbcook/data-leakage) ã‚’é˜²ããŸã‚ã«ã€åŒã˜ã‚°ãƒ«ãƒ¼ãƒ—/ã‚·ãƒ¼ãƒ³ã‹ã‚‰ã®ãƒ“ãƒ‡ã‚ª ã‚¯ãƒªãƒƒãƒ—ã‚’ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ä½¿ç”¨ã—ã¦ã„ã‚‹ã‚µãƒ–ã‚»ãƒƒãƒˆã§ã¯ã€ã“ã®æƒ…å ±ãŒè€ƒæ…®ã•ã‚Œã¦ã„ã¾ã™ã€‚

æ¬¡ã«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã«å­˜åœ¨ã™ã‚‹ãƒ©ãƒ™ãƒ«ã®ã‚»ãƒƒãƒˆã‚’å–å¾—ã—ã¾ã™ã€‚ã¾ãŸã€ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã™ã‚‹ã¨ãã«å½¹ç«‹ã¤ 2 ã¤ã®è¾žæ›¸ã‚’ä½œæˆã—ã¾ã™ã€‚

* `label2id`: ã‚¯ãƒ©ã‚¹åã‚’æ•´æ•°ã«ãƒžãƒƒãƒ—ã—ã¾ã™ã€‚
* `id2label`: æ•´æ•°ã‚’ã‚¯ãƒ©ã‚¹åã«ãƒžãƒƒãƒ”ãƒ³ã‚°ã—ã¾ã™ã€‚


```py
>>> class_labels = sorted({str(path).split("/")[2] for path in all_video_file_paths})
>>> label2id = {label: i for i, label in enumerate(class_labels)}
>>> id2label = {i: label for label, i in label2id.items()}

>>> print(f"Unique classes: {list(label2id.keys())}.")

# Unique classes: ['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress'].
```

å€‹æ€§çš„ãªã‚¯ãƒ©ã‚¹ãŒ10ç¨®é¡žã‚ã‚Šã¾ã™ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ã‚»ãƒƒãƒˆã«ã¯ã€ã‚¯ãƒ©ã‚¹ã”ã¨ã« 30 å€‹ã®ãƒ“ãƒ‡ã‚ªãŒã‚ã‚Šã¾ã™ã€‚

## Load a model to fine-tune

äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¨ãã‚Œã«é–¢é€£ã™ã‚‹ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã‹ã‚‰ãƒ“ãƒ‡ã‚ªåˆ†é¡žãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã«ã¯äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ãŒä»˜å±žã—ã¦ãŠã‚Šã€åˆ†é¡žãƒ˜ãƒƒãƒ‰ã¯ãƒ©ãƒ³ãƒ€ãƒ ã«åˆæœŸåŒ–ã•ã‚Œã¾ã™ã€‚ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆã™ã‚‹ã¨ãã«å½¹ç«‹ã¡ã¾ã™ã€‚

```py
>>> from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification

>>> model_ckpt = "MCG-NJU/videomae-base"
>>> image_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)
>>> model = VideoMAEForVideoClassification.from_pretrained(
...     model_ckpt,
...     label2id=label2id,
...     id2label=id2label,
...     ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint
... )
```

ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã€æ¬¡ã®è­¦å‘ŠãŒè¡¨ç¤ºã•ã‚Œã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚

```bash
Some weights of the model checkpoint at MCG-NJU/videomae-base were not used when initializing VideoMAEForVideoClassification: [..., 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.key.weight']
- This IS expected if you are initializing VideoMAEForVideoClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing VideoMAEForVideoClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
```

ã“ã®è­¦å‘Šã¯ã€ä¸€éƒ¨ã®é‡ã¿ (ãŸã¨ãˆã°ã€`classifier`å±¤ã®é‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹) ã‚’ç ´æ£„ã—ã€ä»–ã®ã„ãã¤ã‹ã®é‡ã¿ (æ–°ã—ã„`classifier`å±¤ã®é‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹) ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«åˆæœŸåŒ–ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚ã“ã®å ´åˆã€ã“ã‚Œã¯äºˆæƒ³ã•ã‚Œã‚‹ã“ã¨ã§ã™ã€‚äº‹å‰ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸé‡ã¿ã‚’æŒãŸãªã„æ–°ã—ã„é ­éƒ¨ã‚’è¿½åŠ ã—ã¦ã„ã‚‹ãŸã‚ã€æŽ¨è«–ã«ä½¿ç”¨ã™ã‚‹å‰ã«ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè­¦å‘Šã—ã¾ã™ã€‚ã“ã‚Œã¯ã¾ã•ã«ç§ãŸã¡ãŒè¡ŒãŠã†ã¨ã—ã¦ã„ã‚‹ã‚‚ã®ã§ã™ã€‚ã™ã‚‹ã€‚

**æ³¨æ„** [ã“ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ](https://huggingface.co/MCG-NJU/videomae-base-finetuned-kinetics) ã¯ã€åŒæ§˜ã®ãƒ€ã‚¦ãƒ³ã‚¹ãƒˆãƒªãƒ¼ãƒ ã§å¾®èª¿æ•´ã•ã‚Œã¦ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒå–å¾—ã•ã‚ŒãŸãŸã‚ã€ã“ã®ã‚¿ã‚¹ã‚¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãŒå‘ä¸Šã™ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ã‹ãªã‚Šã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã®é‡è¤‡ãŒã‚ã‚‹ã‚¿ã‚¹ã‚¯ã€‚ `MCG-NJU/videomae-base-finetuned-kinetics` ã‚’å¾®èª¿æ•´ã—ã¦å–å¾—ã—ãŸ [ã“ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ](https://huggingface.co/sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset) ã‚’ç¢ºèªã§ãã¾ã™ã€‚ -ã‚­ãƒãƒ†ã‚£ã‚¯ã‚¹`ã€‚

## Prepare the datasets for training

ãƒ“ãƒ‡ã‚ªã®å‰å‡¦ç†ã«ã¯ã€[PyTorchVideo ãƒ©ã‚¤ãƒ–ãƒ©ãƒª](https://pytorchvideo.org/) ã‚’åˆ©ç”¨ã—ã¾ã™ã€‚ã¾ãšã€å¿…è¦ãªä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚


```py
>>> import pytorchvideo.data

>>> from pytorchvideo.transforms import (
...     ApplyTransformToKey,
...     Normalize,
...     RandomShortSideScale,
...     RemoveKey,
...     ShortSideScale,
...     UniformTemporalSubsample,
... )

>>> from torchvision.transforms import (
...     Compose,
...     Lambda,
...     RandomCrop,
...     RandomHorizontalFlip,
...     Resize,
... )
```

ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å¤‰æ›ã«ã¯ã€å‡ä¸€ãªæ™‚é–“ã‚µãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€ãƒ”ã‚¯ã‚»ãƒ«æ­£è¦åŒ–ã€ãƒ©ãƒ³ãƒ€ãƒ  ã‚¯ãƒ­ãƒƒãƒ”ãƒ³ã‚°ã€ãŠã‚ˆã³ãƒ©ãƒ³ãƒ€ãƒ ãªæ°´å¹³åè»¢ã‚’çµ„ã¿åˆã‚ã›ã¦ä½¿ç”¨â€‹â€‹ã—ã¾ã™ã€‚æ¤œè¨¼ãŠã‚ˆã³è©•ä¾¡ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¤‰æ›ã§ã¯ã€ãƒ©ãƒ³ãƒ€ãƒ ãªãƒˆãƒªãƒŸãƒ³ã‚°ã¨æ°´å¹³åè»¢ã‚’é™¤ãã€åŒã˜å¤‰æ›ãƒã‚§ãƒ¼ãƒ³ã‚’ç¶­æŒã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®å¤‰æ›ã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€[PyTorchVideo ã®å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://pytorchvideo.org) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã«é–¢é€£ä»˜ã‘ã‚‰ã‚ŒãŸ`image_processor`ã‚’ä½¿ç”¨ã—ã¦ã€æ¬¡ã®æƒ…å ±ã‚’å–å¾—ã—ã¾ã™ã€‚

* ãƒ“ãƒ‡ã‚ª ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒ”ã‚¯ã‚»ãƒ«ãŒæ­£è¦åŒ–ã•ã‚Œã‚‹ç”»åƒã®å¹³å‡å€¤ã¨æ¨™æº–åå·®ã€‚
* ãƒ“ãƒ‡ã‚ª ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚µã‚¤ã‚ºãŒå¤‰æ›´ã•ã‚Œã‚‹ç©ºé–“è§£åƒåº¦ã€‚

ã¾ãšã€ã„ãã¤ã‹ã®å®šæ•°ã‚’å®šç¾©ã—ã¾ã™ã€‚

```py
>>> mean = image_processor.image_mean
>>> std = image_processor.image_std
>>> if "shortest_edge" in image_processor.size:
...     height = width = image_processor.size["shortest_edge"]
>>> else:
...     height = image_processor.size["height"]
...     width = image_processor.size["width"]
>>> resize_to = (height, width)

>>> num_frames_to_sample = model.config.num_frames
>>> sample_rate = 4
>>> fps = 30
>>> clip_duration = num_frames_to_sample * sample_rate / fps
```

æ¬¡ã«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå›ºæœ‰ã®å¤‰æ›ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãã‚Œãžã‚Œå®šç¾©ã—ã¾ã™ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã‹ã‚‰å§‹ã‚ã¾ã™:


```py
>>> train_transform = Compose(
...     [
...         ApplyTransformToKey(
...             key="video",
...             transform=Compose(
...                 [
...                     UniformTemporalSubsample(num_frames_to_sample),
...                     Lambda(lambda x: x / 255.0),
...                     Normalize(mean, std),
...                     RandomShortSideScale(min_size=256, max_size=320),
...                     RandomCrop(resize_to),
...                     RandomHorizontalFlip(p=0.5),
...                 ]
...             ),
...         ),
...     ]
... )

>>> train_dataset = pytorchvideo.data.Ucf101(
...     data_path=os.path.join(dataset_root_path, "train"),
...     clip_sampler=pytorchvideo.data.make_clip_sampler("random", clip_duration),
...     decode_audio=False,
...     transform=train_transform,
... )
```

åŒã˜ä¸€é€£ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æ¤œè¨¼ã‚»ãƒƒãƒˆã¨è©•ä¾¡ã‚»ãƒƒãƒˆã«é©ç”¨ã§ãã¾ã™ã€‚


```py
>>> val_transform = Compose(
...     [
...         ApplyTransformToKey(
...             key="video",
...             transform=Compose(
...                 [
...                     UniformTemporalSubsample(num_frames_to_sample),
...                     Lambda(lambda x: x / 255.0),
...                     Normalize(mean, std),
...                     Resize(resize_to),
...                 ]
...             ),
...         ),
...     ]
... )

>>> val_dataset = pytorchvideo.data.Ucf101(
...     data_path=os.path.join(dataset_root_path, "val"),
...     clip_sampler=pytorchvideo.data.make_clip_sampler("uniform", clip_duration),
...     decode_audio=False,
...     transform=val_transform,
... )

>>> test_dataset = pytorchvideo.data.Ucf101(
...     data_path=os.path.join(dataset_root_path, "test"),
...     clip_sampler=pytorchvideo.data.make_clip_sampler("uniform", clip_duration),
...     decode_audio=False,
...     transform=val_transform,
... )
```

**æ³¨æ„**: ä¸Šè¨˜ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯ã€[å…¬å¼ PyTorchVideo ã‚µãƒ³ãƒ—ãƒ«](https://pytorchvideo.org/docs/tutorial_classification#dataset) ã‹ã‚‰å–å¾—ã—ãŸã‚‚ã®ã§ã™ã€‚ [`pytorchvideo.data.Ucf101()`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.Ucf101) é–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚ UCF-101 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚å†…éƒ¨ã§ã¯ã€[`pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.LabeledVideoDataset) ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã—ã¾ã™ã€‚ `LabeledVideoDataset` ã‚¯ãƒ©ã‚¹ã¯ã€PyTorchVideo ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®ã™ã¹ã¦ã®ãƒ“ãƒ‡ã‚ªã®åŸºæœ¬ã‚¯ãƒ©ã‚¹ã§ã™ã€‚ã—ãŸãŒã£ã¦ã€PyTorchVideo ã§æ—¢è£½ã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ã‚«ã‚¹ã‚¿ãƒ  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ãŸã„å ´åˆã¯ã€ãã‚Œã«å¿œã˜ã¦ `LabeledVideoDataset` ã‚¯ãƒ©ã‚¹ã‚’æ‹¡å¼µã§ãã¾ã™ã€‚è©³ç´°ã«ã¤ã„ã¦ã¯ã€`data`API [ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚ã¾ãŸã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒåŒæ§˜ã®æ§‹é€  (ä¸Šã«ç¤ºã—ãŸã‚‚ã®) ã«å¾“ã£ã¦ã„ã‚‹å ´åˆã¯ã€`pytorchvideo.data.Ucf101()` ã‚’ä½¿ç”¨ã™ã‚‹ã¨å•é¡Œãªãå‹•ä½œã™ã‚‹ã¯ãšã§ã™ã€‚

`num_videos` å¼•æ•°ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã¨ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®ãƒ“ãƒ‡ã‚ªã®æ•°ã‚’çŸ¥ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚



```py
>>> print(train_dataset.num_videos, val_dataset.num_videos, test_dataset.num_videos)
# (300, 30, 75)
```

## Visualize the preprocessed video for better debugging

```py
>>> import imageio
>>> import numpy as np
>>> from IPython.display import Image

>>> def unnormalize_img(img):
...     """Un-normalizes the image pixels."""
...     img = (img * std) + mean
...     img = (img * 255).astype("uint8")
...     return img.clip(0, 255)

>>> def create_gif(video_tensor, filename="sample.gif"):
...     """Prepares a GIF from a video tensor.
...
...     The video tensor is expected to have the following shape:
...     (num_frames, num_channels, height, width).
...     """
...     frames = []
...     for video_frame in video_tensor:
...         frame_unnormalized = unnormalize_img(video_frame.permute(1, 2, 0).numpy())
...         frames.append(frame_unnormalized)
...     kargs = {"duration": 0.25}
...     imageio.mimsave(filename, frames, "GIF", **kargs)
...     return filename

>>> def display_gif(video_tensor, gif_name="sample.gif"):
...     """Prepares and displays a GIF from a video tensor."""
...     video_tensor = video_tensor.permute(1, 0, 2, 3)
...     gif_filename = create_gif(video_tensor, gif_name)
...     return Image(filename=gif_filename)

>>> sample_video = next(iter(train_dataset))
>>> video_tensor = sample_video["video"]
>>> display_gif(video_tensor)
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_gif.gif" alt="Person playing basketball"/>
</div>

## Train the model

ðŸ¤— Transformers ã® [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer) ã‚’ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«åˆ©ç”¨ã—ã¾ã™ã€‚ `Trainer`ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã™ã‚‹ã«ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ§‹æˆã¨è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å®šç¾©ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚æœ€ã‚‚é‡è¦ãªã®ã¯ [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments) ã§ã€ã“ã‚Œã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’æ§‹æˆã™ã‚‹ãŸã‚ã®ã™ã¹ã¦ã®å±žæ€§ã‚’å«ã‚€ã‚¯ãƒ©ã‚¹ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼åãŒå¿…è¦ã§ã™ã€‚ã¾ãŸã€ðŸ¤— Hub ä¸Šã®ãƒ¢ãƒ‡ãƒ« ãƒªãƒã‚¸ãƒˆãƒªå†…ã®ã™ã¹ã¦ã®æƒ…å ±ã‚’åŒæœŸã™ã‚‹ã®ã«ã‚‚å½¹ç«‹ã¡ã¾ã™ã€‚

ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¼•æ•°ã®ã»ã¨ã‚“ã©ã¯ä¸€ç›®çž­ç„¶ã§ã™ãŒã€ã“ã“ã§éžå¸¸ã«é‡è¦ãªã®ã¯`remove_unused_columns=False`ã§ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã®å‘¼ã³å‡ºã—é–¢æ•°ã§ä½¿ç”¨ã•ã‚Œãªã„æ©Ÿèƒ½ãŒå‰Šé™¤ã•ã‚Œã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯`True`ã§ã™ã€‚ã“ã‚Œã¯ã€é€šå¸¸ã€æœªä½¿ç”¨ã®ç‰¹å¾´åˆ—ã‚’å‰Šé™¤ã—ã€ãƒ¢ãƒ‡ãƒ«ã®å‘¼ã³å‡ºã—é–¢æ•°ã¸ã®å…¥åŠ›ã‚’è§£å‡ã—ã‚„ã™ãã™ã‚‹ã“ã¨ãŒç†æƒ³çš„ã§ã‚ã‚‹ãŸã‚ã§ã™ã€‚ãŸã ã—ã€ã“ã®å ´åˆã€`pixel_values` (ãƒ¢ãƒ‡ãƒ«ãŒå…¥åŠ›ã§æœŸå¾…ã™ã‚‹å¿…é ˆã‚­ãƒ¼ã§ã™) ã‚’ä½œæˆã™ã‚‹ã«ã¯ã€æœªä½¿ç”¨ã®æ©Ÿèƒ½ (ç‰¹ã«`video`) ãŒå¿…è¦ã§ã™ã€‚

```py
>>> from transformers import TrainingArguments, Trainer

>>> model_name = model_ckpt.split("/")[-1]
>>> new_model_name = f"{model_name}-finetuned-ucf101-subset"
>>> num_epochs = 4

>>> args = TrainingArguments(
...     new_model_name,
...     remove_unused_columns=False,
...     eval_strategy="epoch",
...     save_strategy="epoch",
...     learning_rate=5e-5,
...     per_device_train_batch_size=batch_size,
...     per_device_eval_batch_size=batch_size,
...     warmup_ratio=0.1,
...     logging_steps=10,
...     load_best_model_at_end=True,
...     metric_for_best_model="accuracy",
...     push_to_hub=True,
...     max_steps=(train_dataset.num_videos // batch_size) * num_epochs,
... )
```

`pytorchvideo.data.Ucf101()` ã«ã‚ˆã£ã¦è¿”ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ `__len__` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè£…ã—ã¦ã„ã¾ã›ã‚“ã€‚ãã®ãŸã‚ã€`TrainingArguments`ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã™ã‚‹ã¨ãã«`max_steps`ã‚’å®šç¾©ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

æ¬¡ã«ã€äºˆæ¸¬ã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°ã‚’å®šç¾©ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã¯ã€ã“ã‚Œã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ã™ã‚‹`metric`ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚å¿…è¦ãªå‰å‡¦ç†ã¯ã€äºˆæ¸¬ã•ã‚ŒãŸãƒ­ã‚¸ãƒƒãƒˆã® argmax ã‚’å–å¾—ã™ã‚‹ã“ã¨ã ã‘ã§ã™ã€‚

```py
import evaluate

metric = evaluate.load("accuracy")


def compute_metrics(eval_pred):
    predictions = np.argmax(eval_pred.predictions, axis=1)
    return metric.compute(predictions=predictions, references=eval_pred.label_ids)
```

**è©•ä¾¡ã«é–¢ã™ã‚‹æ³¨æ„äº‹é …**:

[VideoMAE è«–æ–‡](https://huggingface.co/papers/2203.12602) ã§ã¯ã€è‘—è€…ã¯æ¬¡ã®è©•ä¾¡æˆ¦ç•¥ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚å½¼ã‚‰ã¯ãƒ†ã‚¹ãƒˆ ãƒ“ãƒ‡ã‚ªã‹ã‚‰ã®ã„ãã¤ã‹ã®ã‚¯ãƒªãƒƒãƒ—ã§ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ã€ãã‚Œã‚‰ã®ã‚¯ãƒªãƒƒãƒ—ã«ã•ã¾ã–ã¾ãªã‚¯ãƒ­ãƒƒãƒ—ã‚’é©ç”¨ã—ã¦ã€åˆè¨ˆã‚¹ã‚³ã‚¢ã‚’å ±å‘Šã—ã¾ã™ã€‚ãŸã ã—ã€å˜ç´”ã•ã¨ç°¡æ½”ã•ã‚’ä¿ã¤ãŸã‚ã«ã€ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ãã‚Œã‚’è€ƒæ…®ã—ã¾ã›ã‚“ã€‚

ã¾ãŸã€ã‚µãƒ³ãƒ—ãƒ«ã‚’ã¾ã¨ã‚ã¦ãƒãƒƒãƒå‡¦ç†ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹ `collatâ€‹â€‹e_fn` ã‚’å®šç¾©ã—ã¾ã™ã€‚å„ãƒãƒƒãƒã¯ã€`pixel_values` ã¨ `labels` ã¨ã„ã† 2 ã¤ã®ã‚­ãƒ¼ã§æ§‹æˆã•ã‚Œã¾ã™ã€‚


```py
>>> def collate_fn(examples):
...     # permute to (num_frames, num_channels, height, width)
...     pixel_values = torch.stack(
...         [example["video"].permute(1, 0, 2, 3) for example in examples]
...     )
...     labels = torch.tensor([example["label"] for example in examples])
...     return {"pixel_values": pixel_values, "labels": labels}
```

æ¬¡ã«ã€ã“ã‚Œã‚‰ã™ã¹ã¦ã‚’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã¨ã‚‚ã«`Trainer`ã«æ¸¡ã™ã ã‘ã§ã™ã€‚

```py
>>> trainer = Trainer(
...     model,
...     args,
...     train_dataset=train_dataset,
...     eval_dataset=val_dataset,
...     processing_class=image_processor,
...     compute_metrics=compute_metrics,
...     data_collator=collate_fn,
... )
```

ã™ã§ã«ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†ã—ã¦ã„ã‚‹ã®ã«ã€ãªãœãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ã—ã¦`image_processor`ã‚’æ¸¡ã—ãŸã®ã‹ä¸æ€è­°ã«æ€ã†ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ã“ã‚Œã¯ã€ã‚¤ãƒ¡ãƒ¼ã‚¸ ãƒ—ãƒ­ã‚»ãƒƒã‚µæ§‹æˆãƒ•ã‚¡ã‚¤ãƒ« (JSON ã¨ã—ã¦ä¿å­˜) ã‚‚ãƒãƒ–ä¸Šã®ãƒªãƒã‚¸ãƒˆãƒªã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã ã‘ã§ã™ã€‚

æ¬¡ã«ã€`train` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚

```py
>>> train_results = trainer.train()
```

ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã—ãŸã‚‰ã€ [`~transformers.Trainer.push_to_hub`] ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ–ã«å…±æœ‰ã—ã€èª°ã‚‚ãŒãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

```py
>>> trainer.push_to_hub()
```

## Inference

ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ãŸã®ã§ã€ãã‚Œã‚’æŽ¨è«–ã«ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚

æŽ¨è«–ã®ãŸã‚ã«ãƒ“ãƒ‡ã‚ªã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

```py
>>> sample_test_video = next(iter(test_dataset))
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_gif_two.gif" alt="Teams playing basketball"/>
</div>

æŽ¨è«–ç”¨ã«å¾®èª¿æ•´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™æœ€ã‚‚ç°¡å˜ãªæ–¹æ³•ã¯ã€ãã‚Œã‚’ [`pipeline`](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.VideoClassificationPipeline). ã§ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒ“ãƒ‡ã‚ªåˆ†é¡žç”¨ã®` pipeline`ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã€ãã‚Œã«ãƒ“ãƒ‡ã‚ªã‚’æ¸¡ã—ã¾ã™ã€‚


```py
>>> from transformers import pipeline

>>> video_cls = pipeline(model="my_awesome_video_cls_model")
>>> video_cls("https://huggingface.co/datasets/sayakpaul/ucf101-subset/resolve/main/v_BasketballDunk_g14_c06.avi")
[{'score': 0.9272987842559814, 'label': 'BasketballDunk'},
 {'score': 0.017777055501937866, 'label': 'BabyCrawling'},
 {'score': 0.01663011871278286, 'label': 'BalanceBeam'},
 {'score': 0.009560945443809032, 'label': 'BandMarching'},
 {'score': 0.0068979403004050255, 'label': 'BaseballPitch'}]
```

å¿…è¦ã«å¿œã˜ã¦ã€`pipeline`ã®çµæžœã‚’æ‰‹å‹•ã§è¤‡è£½ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚

```py
>>> def run_inference(model, video):
...     # (num_frames, num_channels, height, width)
...     perumuted_sample_test_video = video.permute(1, 0, 2, 3)
...     inputs = {
...         "pixel_values": perumuted_sample_test_video.unsqueeze(0),
...         "labels": torch.tensor(
...             [sample_test_video["label"]]
...         ),  # this can be skipped if you don't have labels available.
...     }

...     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
...     inputs = {k: v.to(device) for k, v in inputs.items()}
...     model = model.to(device)

...     # forward pass
...     with torch.no_grad():
...         outputs = model(**inputs)
...         logits = outputs.logits

...     return logits
```

æ¬¡ã«ã€å…¥åŠ›ã‚’ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã—ã€`logits `ã‚’è¿”ã—ã¾ã™ã€‚

```py
>>> logits = run_inference(trained_model, sample_test_video["video"])
```

`logits` ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹ã¨ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

```py
>>> predicted_class_idx = logits.argmax(-1).item()
>>> print("Predicted class:", model.config.id2label[predicted_class_idx])
# Predicted class: BasketballDunk
```
