<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Sequence classification

[[open-in-colab]]

<Youtube id="dKE8SIt9C-w"/>

ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ã€ç”»åƒã®å€‹ã€…ã®ãƒ”ã‚¯ã‚»ãƒ«ã«ãƒ©ãƒ™ãƒ«ã¾ãŸã¯ã‚¯ãƒ©ã‚¹ã‚’å‰²ã‚Šå½“ã¦ã¾ã™ã€‚ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã«ã¯ã„ãã¤ã‹ã®ã‚¿ã‚¤ãƒ—ãŒã‚ã‚Šã¾ã™ãŒã€ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®å ´åˆã€åŒã˜ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ä¸€æ„ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹é–“ã®åŒºåˆ¥ã¯è¡Œã‚ã‚Œã¾ã›ã‚“ã€‚ä¸¡æ–¹ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«åŒã˜ãƒ©ãƒ™ãƒ«ãŒä»˜ã‘ã‚‰ã‚Œã¾ã™ (ãŸã¨ãˆã°ã€ã€Œcar-1ã€ã¨ã€Œcar-2ã€ã®ä»£ã‚ã‚Šã«ã€Œcarã€)ã€‚ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®ä¸€èˆ¬çš„ãªç¾å®Ÿä¸–ç•Œã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ã¯ã€æ­©è¡Œè€…ã‚„é‡è¦ãªäº¤é€šæƒ…å ±ã‚’è­˜åˆ¥ã™ã‚‹ãŸã‚ã®è‡ªå‹•é‹è»¢è»Šã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã€åŒ»ç™‚ç”»åƒå†…ã®ç´°èƒã¨ç•°å¸¸ã®è­˜åˆ¥ã€è¡›æ˜Ÿç”»åƒã‹ã‚‰ã®ç’°å¢ƒå¤‰åŒ–ã®ç›£è¦–ãªã©ãŒå«ã¾ã‚Œã¾ã™ã€‚

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€æ¬¡ã®æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚

1. [SceneParse150](https://huggingface.co/datasets/scene_parse_150) ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã® [SegFormer](https://huggingface.co/docs/transformers/main/en/model_doc/segformer#segformer) ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚
2. å¾®èª¿æ•´ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’æ¨è«–ã«ä½¿ç”¨ã—ã¾ã™ã€‚

<Tip>
ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§èª¬æ˜ã™ã‚‹ã‚¿ã‚¹ã‚¯ã¯ã€æ¬¡ã®ãƒ¢ãƒ‡ãƒ« ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ã€‚

<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->

[BEiT](../model_doc/beit), [Data2VecVision](../model_doc/data2vec-vision), [DPT](../model_doc/dpt), [MobileNetV2](../model_doc/mobilenet_v2), [MobileViT](../model_doc/mobilevit), [MobileViTV2](../model_doc/mobilevitv2), [SegFormer](../model_doc/segformer), [UPerNet](../model_doc/upernet)

<!--End of the generated tip-->

</Tip>

å§‹ã‚ã‚‹å‰ã«ã€å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã™ã¹ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

```bash
pip install -q datasets transformers evaluate
```

ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¨å…±æœ‰ã§ãã‚‹ã‚ˆã†ã«ã€Hugging Face ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«ãƒ­ã‚°ã‚¤ãƒ³ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¡¨ç¤ºã•ã‚ŒãŸã‚‰ã€ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã™ã€‚

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## Load SceneParse150 dataset


ã¾ãšã€SceneParse150 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å°ã•ã„ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’ ğŸ¤— ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã•ã‚‰ã«æ™‚é–“ã‚’è²»ã‚„ã™å‰ã«ã€å®Ÿé¨“ã—ã¦ã™ã¹ã¦ãŒæ©Ÿèƒ½ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹æ©Ÿä¼šãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚

```py
>>> from datasets import load_dataset

>>> ds = load_dataset("scene_parse_150", split="train[:50]")
```

[`~datasets.Dataset.train_test_split`] ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã® `train` åˆ†å‰²ã‚’ãƒˆãƒ¬ã‚¤ãƒ³ ã‚»ãƒƒãƒˆã¨ãƒ†ã‚¹ãƒˆ ã‚»ãƒƒãƒˆã«åˆ†å‰²ã—ã¾ã™ã€‚

```py
>>> ds = ds.train_test_split(test_size=0.2)
>>> train_ds = ds["train"]
>>> test_ds = ds["test"]
```

æ¬¡ã«ã€ä¾‹ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```py
>>> train_ds[0]
{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x683 at 0x7F9B0C201F90>,
 'annotation': <PIL.PngImagePlugin.PngImageFile image mode=L size=512x683 at 0x7F9B0C201DD0>,
 'scene_category': 368}
```

- `image`: ã‚·ãƒ¼ãƒ³ã® PIL ã‚¤ãƒ¡ãƒ¼ã‚¸ã€‚
- `annotation`: ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ ãƒãƒƒãƒ—ã® PIL ã‚¤ãƒ¡ãƒ¼ã‚¸ã€‚ãƒ¢ãƒ‡ãƒ«ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã§ã‚‚ã‚ã‚Šã¾ã™ã€‚
- `scene_category`: ã€Œã‚­ãƒƒãƒãƒ³ã€ã‚„ã€Œã‚ªãƒ•ã‚£ã‚¹ã€ãªã©ã®ç”»åƒã‚·ãƒ¼ãƒ³ã‚’èª¬æ˜ã™ã‚‹ã‚«ãƒ†ã‚´ãƒª IDã€‚ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€ã€Œimageã€ã¨ã€Œannotationã€ã®ã¿ãŒå¿…è¦ã«ãªã‚Šã¾ã™ã€‚ã©ã¡ã‚‰ã‚‚ PIL ã‚¤ãƒ¡ãƒ¼ã‚¸ã§ã™ã€‚

ã¾ãŸã€ãƒ©ãƒ™ãƒ« ID ã‚’ãƒ©ãƒ™ãƒ« ã‚¯ãƒ©ã‚¹ã«ãƒãƒƒãƒ—ã™ã‚‹è¾æ›¸ã‚’ä½œæˆã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ã“ã‚Œã¯ã€å¾Œã§ãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®šã™ã‚‹ã¨ãã«å½¹ç«‹ã¡ã¾ã™ã€‚ãƒãƒ–ã‹ã‚‰ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€`id2label` ãŠã‚ˆã³ `label2id` ãƒ‡ã‚£ã‚¯ã‚·ãƒ§ãƒŠãƒªã‚’ä½œæˆã—ã¾ã™ã€‚

```py
>>> import json
>>> from huggingface_hub import cached_download, hf_hub_url

>>> repo_id = "huggingface/label-files"
>>> filename = "ade20k-id2label.json"
>>> id2label = json.load(open(cached_download(hf_hub_url(repo_id, filename, repo_type="dataset")), "r"))
>>> id2label = {int(k): v for k, v in id2label.items()}
>>> label2id = {v: k for k, v in id2label.items()}
>>> num_labels = len(id2label)
```

## Preprocess

æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€SegFormer ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®ç”»åƒã¨æ³¨é‡ˆã‚’æº–å‚™ã—ã¾ã™ã€‚ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚ˆã†ãªä¸€éƒ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ ã‚¯ãƒ©ã‚¹ã¨ã—ã¦ã‚¼ãƒ­ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ãŸã ã—ã€å®Ÿéš›ã«ã¯èƒŒæ™¯ã‚¯ãƒ©ã‚¹ã¯ 150 å€‹ã®ã‚¯ãƒ©ã‚¹ã«å«ã¾ã‚Œã¦ã„ãªã„ãŸã‚ã€`reduce_labels=True`ã‚’è¨­å®šã—ã¦ã™ã¹ã¦ã®ãƒ©ãƒ™ãƒ«ã‹ã‚‰ 1 ã¤ã‚’å¼•ãå¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã‚¼ãƒ­ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯ `255` ã«ç½®ãæ›ãˆã‚‰ã‚Œã‚‹ãŸã‚ã€SegFormer ã®æå¤±é–¢æ•°ã«ã‚ˆã£ã¦ç„¡è¦–ã•ã‚Œã¾ã™ã€‚

```py
>>> from transformers import AutoImageProcessor

>>> checkpoint = "nvidia/mit-b0"
>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint, reduce_labels=True)
```

<frameworkcontent>
<pt>

ãƒ¢ãƒ‡ãƒ«ã‚’éå­¦ç¿’ã«å¯¾ã—ã¦ã‚ˆã‚Šå …ç‰¢ã«ã™ã‚‹ãŸã‚ã«ã€ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã„ãã¤ã‹ã®ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚’é©ç”¨ã™ã‚‹ã®ãŒä¸€èˆ¬çš„ã§ã™ã€‚ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€[torchvision](https://pytorch.org) ã® [`ColorJitter`](https://pytorch.org/vision/stable/generated/torchvision.transforms.ColorJitter.html) é–¢æ•°ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ /vision/stable/index.html) ã‚’ä½¿ç”¨ã—ã¦ç”»åƒã®è‰²ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«å¤‰æ›´ã—ã¾ã™ãŒã€ä»»æ„ã®ç”»åƒãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚

```py
>>> from torchvision.transforms import ColorJitter

>>> jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)
```

æ¬¡ã«ã€ãƒ¢ãƒ‡ãƒ«ã®ç”»åƒã¨æ³¨é‡ˆã‚’æº–å‚™ã™ã‚‹ãŸã‚ã® 2 ã¤ã®å‰å‡¦ç†é–¢æ•°ã‚’ä½œæˆã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®é–¢æ•°ã¯ã€ç”»åƒã‚’`pixel_values`ã«å¤‰æ›ã—ã€æ³¨é‡ˆã‚’`labels`ã«å¤‰æ›ã—ã¾ã™ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ã‚»ãƒƒãƒˆã®å ´åˆã€ç”»åƒã‚’ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã«æä¾›ã™ã‚‹å‰ã«`jitter`ãŒé©ç”¨ã•ã‚Œã¾ã™ã€‚ãƒ†ã‚¹ãƒˆ ã‚»ãƒƒãƒˆã®å ´åˆã€ãƒ†ã‚¹ãƒˆä¸­ã«ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µãŒé©ç”¨ã•ã‚Œãªã„ãŸã‚ã€ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã¯`images`ã‚’åˆ‡ã‚Šå–ã£ã¦æ­£è¦åŒ–ã—ã€`labels` ã®ã¿ã‚’åˆ‡ã‚Šå–ã‚Šã¾ã™ã€‚

```py
>>> def train_transforms(example_batch):
...     images = [jitter(x) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs


>>> def val_transforms(example_batch):
...     images = [x for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs
```

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã«`jitter`ã‚’é©ç”¨ã™ã‚‹ã«ã¯ã€ğŸ¤— Datasets [`~datasets.Dataset.set_transform`] é–¢æ•°ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚å¤‰æ›ã¯ã‚ªãƒ³ã‚¶ãƒ•ãƒ©ã‚¤ã§é©ç”¨ã•ã‚Œã‚‹ãŸã‚ã€é«˜é€Ÿã§æ¶ˆè²»ã™ã‚‹ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ãŒå°‘ãªããªã‚Šã¾ã™ã€‚

```py
>>> train_ds.set_transform(train_transforms)
>>> test_ds.set_transform(val_transforms)
```

</pt>
</frameworkcontent>

<frameworkcontent>
<tf>

ãƒ¢ãƒ‡ãƒ«ã‚’éå­¦ç¿’ã«å¯¾ã—ã¦ã‚ˆã‚Šå …ç‰¢ã«ã™ã‚‹ãŸã‚ã«ã€ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã„ãã¤ã‹ã®ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚’é©ç”¨ã™ã‚‹ã®ãŒä¸€èˆ¬çš„ã§ã™ã€‚
ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€[`tf.image`](https://www.tensorflow.org/api_docs/python/tf/image) ã‚’ä½¿ç”¨ã—ã¦ç”»åƒã®è‰²ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«å¤‰æ›´ã—ã¾ã™ãŒã€ä»»æ„ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ç”»åƒ
å¥½ããªå›³æ›¸é¤¨ã€‚
2 ã¤ã®åˆ¥ã€…ã®å¤‰æ›é–¢æ•°ã‚’å®šç¾©ã—ã¾ã™ã€‚
- ç”»åƒæ‹¡å¼µã‚’å«ã‚€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒ‡ãƒ¼ã‚¿å¤‰æ›
- ğŸ¤— Transformers ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ ãƒ“ã‚¸ãƒ§ãƒ³ ãƒ¢ãƒ‡ãƒ«ã¯ãƒãƒ£ãƒãƒ«å„ªå…ˆã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’æƒ³å®šã—ã¦ã„ã‚‹ãŸã‚ã€ç”»åƒã‚’è»¢ç½®ã™ã‚‹ã ã‘ã®æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿å¤‰æ›

```py
>>> import tensorflow as tf


>>> def aug_transforms(image):
...     image = tf.keras.utils.img_to_array(image)
...     image = tf.image.random_brightness(image, 0.25)
...     image = tf.image.random_contrast(image, 0.5, 2.0)
...     image = tf.image.random_saturation(image, 0.75, 1.25)
...     image = tf.image.random_hue(image, 0.1)
...     image = tf.transpose(image, (2, 0, 1))
...     return image


>>> def transforms(image):
...     image = tf.keras.utils.img_to_array(image)
...     image = tf.transpose(image, (2, 0, 1))
...     return image
```

æ¬¡ã«ã€ãƒ¢ãƒ‡ãƒ«ã®ç”»åƒã¨æ³¨é‡ˆã®ãƒãƒƒãƒã‚’æº–å‚™ã™ã‚‹ 2 ã¤ã®å‰å‡¦ç†é–¢æ•°ã‚’ä½œæˆã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®æ©Ÿèƒ½ãŒé©ç”¨ã•ã‚Œã¾ã™
ç”»åƒå¤‰æ›ã‚’è¡Œã„ã€ä»¥å‰ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸ `image_processor` ã‚’ä½¿ç”¨ã—ã¦ç”»åƒã‚’ `pixel_values` ã«å¤‰æ›ã—ã€
`labels`ã¸ã®æ³¨é‡ˆã€‚ `ImageProcessor` ã¯ã€ç”»åƒã®ã‚µã‚¤ã‚ºå¤‰æ›´ã¨æ­£è¦åŒ–ã‚‚å‡¦ç†ã—ã¾ã™ã€‚

```py
>>> def train_transforms(example_batch):
...     images = [aug_transforms(x.convert("RGB")) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs


>>> def val_transforms(example_batch):
...     images = [transforms(x.convert("RGB")) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs
```

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã«å‰å‡¦ç†å¤‰æ›ã‚’é©ç”¨ã™ã‚‹ã«ã¯ã€ğŸ¤— Datasets [`~datasets.Dataset.set_transform`] é–¢æ•°ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
å¤‰æ›ã¯ã‚ªãƒ³ã‚¶ãƒ•ãƒ©ã‚¤ã§é©ç”¨ã•ã‚Œã‚‹ãŸã‚ã€é«˜é€Ÿã§æ¶ˆè²»ã™ã‚‹ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ãŒå°‘ãªããªã‚Šã¾ã™ã€‚

```py
>>> train_ds.set_transform(train_transforms)
>>> test_ds.set_transform(val_transforms)
```
</tf>
</frameworkcontent>

## Evaluate

ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å«ã‚ã‚‹ã¨ã€å¤šãã®å ´åˆã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚ ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦ã€è©•ä¾¡ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ã™ã°ã‚„ããƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚ã“ã®ã‚¿ã‚¹ã‚¯ã§ã¯ã€[Mean Intersection over Union](https://huggingface.co/spaces/evaluate-metric/accuracy) (IoU) ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ (ğŸ¤— Evaluate [ã‚¯ã‚¤ãƒƒã‚¯ ãƒ„ã‚¢ãƒ¼](https://huggingface.co) ã‚’å‚ç…§ã—ã¦ãã ã•ã„) /docs/evaluate/a_quick_tour) ã‚’å‚ç…§ã—ã¦ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦è¨ˆç®—ã™ã‚‹æ–¹æ³•ã®è©³ç´°ã‚’ç¢ºèªã—ã¦ãã ã•ã„)ã€‚

```py
>>> import evaluate

>>> metric = evaluate.load("mean_iou")
```

æ¬¡ã«ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ [`~evaluate.EvaluationModule.compute`] ã™ã‚‹é–¢æ•°ã‚’ä½œæˆã—ã¾ã™ã€‚äºˆæ¸¬ã‚’æ¬¡ã®ã‚ˆã†ã«å¤‰æ›ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™
æœ€åˆã«ãƒ­ã‚¸ãƒƒãƒˆã‚’ä½œæˆã—ã€æ¬¡ã« [`~evaluate.EvaluationModule.compute`] ã‚’å‘¼ã³å‡ºã™å‰ã«ãƒ©ãƒ™ãƒ«ã®ã‚µã‚¤ã‚ºã«ä¸€è‡´ã™ã‚‹ã‚ˆã†ã«å†å½¢æˆã—ã¾ã™ã€‚

<frameworkcontent>
<pt>

```py
>>> import numpy as np
>>> import torch
>>> from torch import nn

>>> def compute_metrics(eval_pred):
...     with torch.no_grad():
...         logits, labels = eval_pred
...         logits_tensor = torch.from_numpy(logits)
...         logits_tensor = nn.functional.interpolate(
...             logits_tensor,
...             size=labels.shape[-2:],
...             mode="bilinear",
...             align_corners=False,
...         ).argmax(dim=1)

...         pred_labels = logits_tensor.detach().cpu().numpy()
...         metrics = metric.compute(
...             predictions=pred_labels,
...             references=labels,
...             num_labels=num_labels,
...             ignore_index=255,
...             reduce_labels=False,
...         )
...         for key, value in metrics.items():
...             if type(value) is np.ndarray:
...                 metrics[key] = value.tolist()
...         return metrics
```

</pt>
</frameworkcontent>


<frameworkcontent>
<tf>

```py
>>> def compute_metrics(eval_pred):
...     logits, labels = eval_pred
...     logits = tf.transpose(logits, perm=[0, 2, 3, 1])
...     logits_resized = tf.image.resize(
...         logits,
...         size=tf.shape(labels)[1:],
...         method="bilinear",
...     )

...     pred_labels = tf.argmax(logits_resized, axis=-1)
...     metrics = metric.compute(
...         predictions=pred_labels,
...         references=labels,
...         num_labels=num_labels,
...         ignore_index=-1,
...         reduce_labels=image_processor.do_reduce_labels,
...     )

...     per_category_accuracy = metrics.pop("per_category_accuracy").tolist()
...     per_category_iou = metrics.pop("per_category_iou").tolist()

...     metrics.update({f"accuracy_{id2label[i]}": v for i, v in enumerate(per_category_accuracy)})
...     metrics.update({f"iou_{id2label[i]}": v for i, v in enumerate(per_category_iou)})
...     return {"val_" + k: v for k, v in metrics.items()}
```

</tf>
</frameworkcontent>

ã“ã‚Œã§`compute_metrics`é–¢æ•°ã®æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹ã¨ãã«ã“ã®é–¢æ•°ã«æˆ»ã‚Šã¾ã™ã€‚

## Train
<frameworkcontent>
<pt>
<Tip>

[`Trainer`] ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã«æ…£ã‚Œã¦ã„ãªã„å ´åˆã¯ã€[ã“ã¡ã‚‰](../training#finetune-with-trainer) ã®åŸºæœ¬çš„ãªãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’ã”è¦§ãã ã•ã„ã€‚


</Tip>

ã“ã‚Œã§ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚ [`AutoModelForSemanticSegmentation`] ã‚’ä½¿ç”¨ã—ã¦ SegFormer ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€ãƒ©ãƒ™ãƒ« ID ã¨ãƒ©ãƒ™ãƒ« ã‚¯ãƒ©ã‚¹é–“ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã—ã¾ã™ã€‚

```py
>>> from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer

>>> model = AutoModelForSemanticSegmentation.from_pretrained(checkpoint, id2label=id2label, label2id=label2id)
```

ã“ã®æ™‚ç‚¹ã§æ®‹ã£ã¦ã„ã‚‹æ‰‹é †ã¯æ¬¡ã® 3 ã¤ã ã‘ã§ã™ã€‚

1. [`TrainingArguments`] ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®šç¾©ã—ã¾ã™ã€‚ `image` åˆ—ãŒå‰Šé™¤ã•ã‚Œã‚‹ãŸã‚ã€æœªä½¿ç”¨ã®åˆ—ã‚’å‰Šé™¤ã—ãªã„ã“ã¨ãŒé‡è¦ã§ã™ã€‚ `image` åˆ—ãŒãªã„ã¨ã€`pixel_values` ã‚’ä½œæˆã§ãã¾ã›ã‚“ã€‚ã“ã®å‹•ä½œã‚’é˜²ãã«ã¯ã€`remove_unused_columns=False`ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚ä»–ã«å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜å ´æ‰€ã‚’æŒ‡å®šã™ã‚‹ `output_dir` ã ã‘ã§ã™ã€‚ `push_to_hub=True`ã‚’è¨­å®šã—ã¦ã€ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ–ã«ãƒ—ãƒƒã‚·ãƒ¥ã—ã¾ã™ (ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã«ã¯ã€Hugging Face ã«ã‚µã‚¤ãƒ³ã‚¤ãƒ³ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™)ã€‚å„ã‚¨ãƒãƒƒã‚¯ã®çµ‚äº†æ™‚ã«ã€[`Trainer`] ã¯ IoU ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚’è©•ä¾¡ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã—ã¾ã™ã€‚
2. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¼•æ•°ã‚’ã€ãƒ¢ãƒ‡ãƒ«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã€ãƒ‡ãƒ¼ã‚¿ç…§åˆå™¨ã€ãŠã‚ˆã³ `compute_metrics` é–¢æ•°ã¨ã¨ã‚‚ã« [`Trainer`] ã«æ¸¡ã—ã¾ã™ã€‚
3. [`~Trainer.train`] ã‚’å‘¼ã³å‡ºã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚


```py
>>> training_args = TrainingArguments(
...     output_dir="segformer-b0-scene-parse-150",
...     learning_rate=6e-5,
...     num_train_epochs=50,
...     per_device_train_batch_size=2,
...     per_device_eval_batch_size=2,
...     save_total_limit=3,
...     evaluation_strategy="steps",
...     save_strategy="steps",
...     save_steps=20,
...     eval_steps=20,
...     logging_steps=1,
...     eval_accumulation_steps=5,
...     remove_unused_columns=False,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=train_ds,
...     eval_dataset=test_ds,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã—ãŸã‚‰ã€ [`~transformers.Trainer.push_to_hub`] ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ–ã«å…±æœ‰ã—ã€èª°ã‚‚ãŒãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

```py
>>> trainer.push_to_hub()
```
</pt>
</frameworkcontent>

<frameworkcontent>
<tf>
<Tip>

Keras ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã«æ…£ã‚Œã¦ã„ãªã„å ´åˆã¯ã€ã¾ãš [åŸºæœ¬ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](./training#train-a-tensorflow-model-with-keras) ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

</Tip>

TensorFlow ã§ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã«ã¯ã€æ¬¡ã®æ‰‹é †ã«å¾“ã„ã¾ã™ã€‚
1. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®šç¾©ã—ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã¨å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’è¨­å®šã—ã¾ã™ã€‚
2. äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã¾ã™ã€‚
3. ğŸ¤— ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ `tf.data.Dataset` ã«å¤‰æ›ã—ã¾ã™ã€‚
4. ãƒ¢ãƒ‡ãƒ«ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¾ã™ã€‚
5. ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è¿½åŠ ã—ã¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’ ğŸ¤— Hub ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™
6. `fit()` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

ã¾ãšã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å®šç¾©ã—ã¾ã™ã€‚


```py
>>> from transformers import create_optimizer

>>> batch_size = 2
>>> num_epochs = 50
>>> num_train_steps = len(train_ds) * num_epochs
>>> learning_rate = 6e-5
>>> weight_decay_rate = 0.01

>>> optimizer, lr_schedule = create_optimizer(
...     init_lr=learning_rate,
...     num_train_steps=num_train_steps,
...     weight_decay_rate=weight_decay_rate,
...     num_warmup_steps=0,
... )
```

æ¬¡ã«ã€ãƒ©ãƒ™ãƒ« ãƒãƒƒãƒ”ãƒ³ã‚°ã¨ã¨ã‚‚ã« [`TFAutoModelForSemanticSegmentation`] ã‚’ä½¿ç”¨ã—ã¦ SegFormer ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€ãã‚Œã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¾ã™ã€‚
ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã€‚ Transformers ãƒ¢ãƒ‡ãƒ«ã«ã¯ã™ã¹ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¿ã‚¹ã‚¯é–¢é€£ã®æå¤±é–¢æ•°ãŒã‚ã‚‹ãŸã‚ã€æ¬¡ã®å ´åˆã‚’é™¤ãã€æå¤±é–¢æ•°ã‚’æŒ‡å®šã™ã‚‹å¿…è¦ã¯ãªã„ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚

```py
>>> from transformers import TFAutoModelForSemanticSegmentation

>>> model = TFAutoModelForSemanticSegmentation.from_pretrained(
...     checkpoint,
...     id2label=id2label,
...     label2id=label2id,
... )
>>> model.compile(optimizer=optimizer)  # No loss argument!
```

[`~datasets.Dataset.to_tf_dataset`] ã¨ [`DefaultDataCollatâ€‹â€‹or`] ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ `tf.data.Dataset` å½¢å¼ã«å¤‰æ›ã—ã¾ã™ã€‚

```py
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator(return_tensors="tf")

>>> tf_train_dataset = train_ds.to_tf_dataset(
...     columns=["pixel_values", "label"],
...     shuffle=True,
...     batch_size=batch_size,
...     collate_fn=data_collator,
... )

>>> tf_eval_dataset = test_ds.to_tf_dataset(
...     columns=["pixel_values", "label"],
...     shuffle=True,
...     batch_size=batch_size,
...     collate_fn=data_collator,
... )
```

äºˆæ¸¬ã‹ã‚‰ç²¾åº¦ã‚’è¨ˆç®—ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’ ğŸ¤— ãƒãƒ–ã«ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹ã«ã¯ã€[Keras callbacks](../main_classes/keras_callbacks) ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
`compute_metrics` é–¢æ•°ã‚’ [`KerasMetricCallback`] ã«æ¸¡ã—ã¾ã™ã€‚
ãã—ã¦ [`PushToHubCallback`] ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

```py
>>> from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback

>>> metric_callback = KerasMetricCallback(
...     metric_fn=compute_metrics, eval_dataset=tf_eval_dataset, batch_size=batch_size, label_cols=["labels"]
... )

>>> push_to_hub_callback = PushToHubCallback(output_dir="scene_segmentation", image_processor=image_processor)

>>> callbacks = [metric_callback, push_to_hub_callback]
```

ã¤ã„ã«ã€ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚`fit()`ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ã‚¨ãƒãƒƒã‚¯æ•°ã€
ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ãŸã‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯:

```py
>>> model.fit(
...     tf_train_dataset,
...     validation_data=tf_eval_dataset,
...     callbacks=callbacks,
...     epochs=num_epochs,
... )
```

ãŠã‚ã§ã¨ã†ï¼ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã€ğŸ¤— Hub ã§å…±æœ‰ã—ã¾ã—ãŸã€‚ã“ã‚Œã§æ¨è«–ã«ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚

</tf>
</frameworkcontent>


## Inference

ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ãŸã®ã§ã€ãã‚Œã‚’æ¨è«–ã«ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚

æ¨è«–ã®ãŸã‚ã«ç”»åƒã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

```py
>>> image = ds[0]["image"]
>>> image
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-image.png" alt="Image of bedroom"/>
</div>

<frameworkcontent>
<pt>

æ¨è«–ç”¨ã«å¾®èª¿æ•´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™æœ€ã‚‚ç°¡å˜ãªæ–¹æ³•ã¯ã€ãã‚Œã‚’ [`pipeline`] ã§ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ç”»åƒã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã® `pipeline` ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã€ãã‚Œã«ç”»åƒã‚’æ¸¡ã—ã¾ã™ã€‚

```py
>>> from transformers import pipeline

>>> segmenter = pipeline("image-segmentation", model="my_awesome_seg_model")
>>> segmenter(image)
[{'score': None,
  'label': 'wall',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062690>},
 {'score': None,
  'label': 'sky',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062A50>},
 {'score': None,
  'label': 'floor',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062B50>},
 {'score': None,
  'label': 'ceiling',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062A10>},
 {'score': None,
  'label': 'bed ',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062E90>},
 {'score': None,
  'label': 'windowpane',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062390>},
 {'score': None,
  'label': 'cabinet',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062550>},
 {'score': None,
  'label': 'chair',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062D90>},
 {'score': None,
  'label': 'armchair',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062E10>}]
```

å¿…è¦ã«å¿œã˜ã¦ã€`pipeline` ã®çµæœã‚’æ‰‹å‹•ã§è¤‡è£½ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã§ç”»åƒã‚’å‡¦ç†ã—ã€`pixel_values`ã‚’ GPU ã«é…ç½®ã—ã¾ã™ã€‚

```py
>>> device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # use GPU if available, otherwise use a CPU
>>> encoding = image_processor(image, return_tensors="pt")
>>> pixel_values = encoding.pixel_values.to(device)
```

å…¥åŠ›ã‚’ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã—ã€ã€Œlogitsã€ã‚’è¿”ã—ã¾ã™ã€‚

```py
>>> outputs = model(pixel_values=pixel_values)
>>> logits = outputs.logits.cpu()
```

æ¬¡ã«ã€ãƒ­ã‚¸ãƒƒãƒˆã‚’å…ƒã®ç”»åƒã‚µã‚¤ã‚ºã«å†ã‚¹ã‚±ãƒ¼ãƒ«ã—ã¾ã™ã€‚


```py
>>> upsampled_logits = nn.functional.interpolate(
...     logits,
...     size=image.size[::-1],
...     mode="bilinear",
...     align_corners=False,
... )

>>> pred_seg = upsampled_logits.argmax(dim=1)[0]
```

</pt>
</frameworkcontent>

<frameworkcontent>
<tf>

ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ç”»åƒã‚’å‰å‡¦ç†ã—ã€å…¥åŠ›ã‚’ TensorFlow ãƒ†ãƒ³ã‚½ãƒ«ã¨ã—ã¦è¿”ã—ã¾ã™ã€‚

```py
>>> from transformers import AutoImageProcessor

>>> image_processor = AutoImageProcessor.from_pretrained("MariaK/scene_segmentation")
>>> inputs = image_processor(image, return_tensors="tf")
```

å…¥åŠ›ã‚’ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã—ã€`logits`ã‚’è¿”ã—ã¾ã™ã€‚

```py
>>> from transformers import TFAutoModelForSemanticSegmentation

>>> model = TFAutoModelForSemanticSegmentation.from_pretrained("MariaK/scene_segmentation")
>>> logits = model(**inputs).logits
```

æ¬¡ã«ã€ãƒ­ã‚¸ãƒƒãƒˆã‚’å…ƒã®ç”»åƒã‚µã‚¤ã‚ºã«å†ã‚¹ã‚±ãƒ¼ãƒ«ã—ã€ã‚¯ãƒ©ã‚¹æ¬¡å…ƒã« argmax ã‚’é©ç”¨ã—ã¾ã™ã€‚

```py
>>> logits = tf.transpose(logits, [0, 2, 3, 1])

>>> upsampled_logits = tf.image.resize(
...     logits,
...     # We reverse the shape of `image` because `image.size` returns width and height.
...     image.size[::-1],
... )

>>> pred_seg = tf.math.argmax(upsampled_logits, axis=-1)[0]
```

</tf>
</frameworkcontent>

çµæœã‚’è¦–è¦šåŒ–ã™ã‚‹ã«ã¯ã€[ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ã‚«ãƒ©ãƒ¼ ãƒ‘ãƒ¬ãƒƒãƒˆ](https://github.com/tensorflow/models/blob/3f1ca33afe3c1631b733ea7e40c294273b9e406d/research/deeplab/utils/get_dataset_colormap.py#L51) ã‚’ã€ãã‚Œãã‚Œã‚’ãƒãƒƒãƒ—ã™ã‚‹ `ade_palette()` ã¨ã—ã¦ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚ã‚¯ãƒ©ã‚¹ã‚’ RGB å€¤ã«å¤‰æ›ã—ã¾ã™ã€‚æ¬¡ã«ã€ç”»åƒã¨äºˆæ¸¬ã•ã‚ŒãŸã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ ãƒãƒƒãƒ—ã‚’çµ„ã¿åˆã‚ã›ã¦ãƒ—ãƒ­ãƒƒãƒˆã§ãã¾ã™ã€‚

```py
>>> import matplotlib.pyplot as plt
>>> import numpy as np

>>> color_seg = np.zeros((pred_seg.shape[0], pred_seg.shape[1], 3), dtype=np.uint8)
>>> palette = np.array(ade_palette())
>>> for label, color in enumerate(palette):
...     color_seg[pred_seg == label, :] = color
>>> color_seg = color_seg[..., ::-1]  # convert to BGR

>>> img = np.array(image) * 0.5 + color_seg * 0.5  # plot the image with the segmentation map
>>> img = img.astype(np.uint8)

>>> plt.figure(figsize=(15, 10))
>>> plt.imshow(img)
>>> plt.show()
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-preds.png" alt="Image of bedroom overlaid with segmentation map"/>
</div>
