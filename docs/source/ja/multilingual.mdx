<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# æŽ¨è«–ã®ãŸã‚ã®å¤šè¨€èªžãƒ¢ãƒ‡ãƒ«

[[open-in-colab]]

ðŸ¤— Transformers ã«ã¯ã„ãã¤ã‹ã®å¤šè¨€èªžãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã€ãã‚Œã‚‰ã®æŽ¨è«–ã®ä½¿ç”¨æ–¹æ³•ã¯å˜ä¸€è¨€èªžãƒ¢ãƒ‡ãƒ«ã¨ã¯ç•°ãªã‚Šã¾ã™ã€‚ãŸã ã—ã€å¤šè¨€èªžãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨æ–¹æ³•ãŒã™ã¹ã¦ç•°ãªã‚‹ã‚ã‘ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ [bert-base-multilingual-uncased](https://huggingface.co/bert-base-multilingual-uncased) ãªã©ã®ä¸€éƒ¨ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€å˜ä¸€è¨€èªžãƒ¢ãƒ‡ãƒ«ã¨åŒæ§˜ã«ä½¿ç”¨ã§ãã¾ã™ã€‚ ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€æŽ¨è«–ã®ãŸã‚ã«ä½¿ç”¨æ–¹æ³•ãŒç•°ãªã‚‹å¤šè¨€èªžãƒ¢ãƒ‡ãƒ«ã‚’ã©ã®ã‚ˆã†ã«ä½¿ã†ã‹ã‚’ç¤ºã—ã¾ã™ã€‚

## XLM

XLM ã«ã¯10ã®ç•°ãªã‚‹ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒã‚ã‚Šã€ãã®ã†ã¡ã®1ã¤ã ã‘ãŒå˜ä¸€è¨€èªžã§ã™ã€‚ æ®‹ã‚Šã®9ã¤ã®ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¯ã€è¨€èªžåŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨ã™ã‚‹ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¨ä½¿ç”¨ã—ãªã„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®2ã¤ã®ã‚«ãƒ†ã‚´ãƒªã«åˆ†ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

### è¨€èªžã®åŸ‹ã‚è¾¼ã¿ãŒã‚ã‚‹ XLM

æ¬¡ã® XLM ãƒ¢ãƒ‡ãƒ«ã¯ã€è¨€èªžã®åŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨ã—ã¦ã€æŽ¨è«–ã§ä½¿ç”¨ã•ã‚Œã‚‹è¨€èªžã‚’æŒ‡å®šã—ã¾ã™ã€‚

- `xlm-mlm-ende-1024` (ãƒžã‚¹ã‚¯åŒ–ã•ã‚ŒãŸè¨€èªžãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€è‹±èªž-ãƒ‰ã‚¤ãƒ„èªž)
- `xlm-mlm-enfr-1024` (ãƒžã‚¹ã‚¯åŒ–ã•ã‚ŒãŸè¨€èªžãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€è‹±èªž-ãƒ•ãƒ©ãƒ³ã‚¹èªž)
- `xlm-mlm-enro-1024` (ãƒžã‚¹ã‚¯åŒ–ã•ã‚ŒãŸè¨€èªžãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€è‹±èªž-ãƒ«ãƒ¼ãƒžãƒ‹ã‚¢èªž)
- `xlm-mlm-xnli15-1024` (ãƒžã‚¹ã‚¯åŒ–ã•ã‚ŒãŸè¨€èªžãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€XNLI è¨€èªž)
- `xlm-mlm-tlm-xnli15-1024` (ãƒžã‚¹ã‚¯åŒ–ã•ã‚ŒãŸè¨€èªžãƒ¢ãƒ‡ãƒªãƒ³ã‚° + ç¿»è¨³ + XNLI è¨€èªž)
- `xlm-clm-enfr-1024` (å› æžœè¨€èªžãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€è‹±èªž-ãƒ•ãƒ©ãƒ³ã‚¹èªž)
- `xlm-clm-ende-1024` (å› æžœè¨€èªžãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€è‹±èªž-ãƒ‰ã‚¤ãƒ„èªž)

è¨€èªžã®åŸ‹ã‚è¾¼ã¿ã¯ã€ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã•ã‚Œã‚‹ `input_ids` ã¨åŒã˜å½¢çŠ¶ã®ãƒ†ãƒ³ã‚½ãƒ«ã¨ã—ã¦è¡¨ã•ã‚Œã¾ã™ã€‚ ã“ã‚Œã‚‰ã®ãƒ†ãƒ³ã‚½ãƒ«ã®å€¤ã¯ã€ä½¿ç”¨ã•ã‚Œã‚‹è¨€èªžã«ä¾å­˜ã—ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã® `lang2id` ãŠã‚ˆã³ `id2lang` å±žæ€§ã«ã‚ˆã£ã¦è­˜åˆ¥ã•ã‚Œã¾ã™ã€‚

ã“ã®ä¾‹ã§ã¯ã€`xlm-clm-enfr-1024` ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ (å› æžœè¨€èªžãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€è‹±èªž-ãƒ•ãƒ©ãƒ³ã‚¹èªž)ã€‚

```py
>>> import torch
>>> from transformers import XLMTokenizer, XLMWithLMHeadModel

>>> tokenizer = XLMTokenizer.from_pretrained("xlm-clm-enfr-1024")
>>> model = XLMWithLMHeadModel.from_pretrained("xlm-clm-enfr-1024")
```

ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã® `lang2id` å±žæ€§ã¯ã€ã“ã®ãƒ¢ãƒ‡ãƒ«ã®è¨€èªžã¨ãã® ID ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚

```py
>>> print(tokenizer.lang2id)
{'en': 0, 'fr': 1}
```

æ¬¡ã«ã€å…¥åŠ›ä¾‹ã‚’ä½œæˆã—ã¾ã™ã€‚

```py
>>> input_ids = torch.tensor([tokenizer.encode("Wikipedia was used to")])  # batch size of 1
```

è¨€èªž ID ã‚’ `en` ã«è¨­å®šã—ã€ãã‚Œã‚’ä½¿ç”¨ã—ã¦è¨€èªžã®åŸ‹ã‚è¾¼ã¿ã‚’å®šç¾©ã—ã¾ã™ã€‚ è¨€èªžã®åŸ‹ã‚è¾¼ã¿ã¯ã€è‹±èªžã®è¨€èªž ID ã§ã‚ã‚‹ãŸã‚ã€`0` ã§åŸ‹ã‚ã‚‰ã‚ŒãŸãƒ†ãƒ³ã‚½ãƒ«ã§ã™ã€‚ ã“ã®ãƒ†ãƒ³ã‚½ãƒ«ã¯ `input_ids` ã¨åŒã˜ã‚µã‚¤ã‚ºã«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

```py
>>> language_id = tokenizer.lang2id["en"]  # 0
>>> langs = torch.tensor([language_id] * input_ids.shape[1])  # torch.tensor([0, 0, 0, ..., 0])

>>> # We reshape it to be of size (batch_size, sequence_length)
>>> langs = langs.view(1, -1)  # is now of shape [1, sequence_length] (we have a batch size of 1)
```

ã“ã‚Œã§ã€`input_ids` ã¨è¨€èªžã®åŸ‹ã‚è¾¼ã¿ã‚’ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã™ã“ã¨ãŒã§ãã¾ã™ã€‚

```py
>>> outputs = model(input_ids, langs=langs)
```

[run_generation.py](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-generation/run_generation.py) ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€`xlm-clm` ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦ã€è¨€èªžãŒåŸ‹ã‚è¾¼ã¾ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã§ãã¾ã™ã€‚

### è¨€èªžã®åŸ‹ã‚è¾¼ã¿ãŒãªã„XLM

æ¬¡ã® XLM ãƒ¢ãƒ‡ãƒ«ã¯ã€æŽ¨è«–ä¸­ã«è¨€èªžã®åŸ‹ã‚è¾¼ã¿ã‚’å¿…è¦ã¨ã—ã¾ã›ã‚“ã€‚

- `xlm-mlm-17-1280` (ãƒžã‚¹ã‚¯åŒ–ã•ã‚ŒãŸè¨€èªžãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€17ã®è¨€èªž)
- `xlm-mlm-100-1280` (ãƒžã‚¹ã‚¯åŒ–ã•ã‚ŒãŸè¨€èªžãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€100ã®è¨€èªž)

ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ä»¥å‰ã® XLM ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¨ã¯ç•°ãªã‚Šã€ä¸€èˆ¬çš„ãªæ–‡ã®è¡¨ç¾ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

## BERT

ä»¥ä¸‹ã® BERT ãƒ¢ãƒ‡ãƒ«ã¯ã€å¤šè¨€èªžã‚¿ã‚¹ã‚¯ã«ä½¿ç”¨ã§ãã¾ã™ã€‚

- `bert-base-multilingual-uncased` (ãƒžã‚¹ã‚¯åŒ–ã•ã‚ŒãŸè¨€èªžãƒ¢ãƒ‡ãƒªãƒ³ã‚° + æ¬¡ã®æ–‡ã®äºˆæ¸¬ã€102ã®è¨€èªž)
- `bert-base-multilingual-cased` (ãƒžã‚¹ã‚¯åŒ–ã•ã‚ŒãŸè¨€èªžãƒ¢ãƒ‡ãƒªãƒ³ã‚° + æ¬¡ã®æ–‡ã®äºˆæ¸¬ã€104ã®è¨€èªž)

ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€æŽ¨è«–ä¸­ã«è¨€èªžã®åŸ‹ã‚è¾¼ã¿ã‚’å¿…è¦ã¨ã—ã¾ã›ã‚“ã€‚ æ–‡è„ˆã‹ã‚‰è¨€èªžã‚’è­˜åˆ¥ã—ã€ãã‚Œã«å¿œã˜ã¦æŽ¨æ¸¬ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

## XLM-RoBERTa

æ¬¡ã® XLM-RoBERTa ãƒ¢ãƒ‡ãƒ«ã¯ã€å¤šè¨€èªžã‚¿ã‚¹ã‚¯ã«ä½¿ç”¨ã§ãã¾ã™ã€‚

- `xlm-roberta-base` (ãƒžã‚¹ã‚¯åŒ–ã•ã‚ŒãŸè¨€èªžãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€100ã®è¨€èªž)
- `xlm-roberta-large` (ãƒžã‚¹ã‚¯åŒ–ã•ã‚ŒãŸè¨€èªžãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€100ã®è¨€èªž)

XLM-RoBERTa ã¯ã€100ã®è¨€èªžã§æ–°ã—ãä½œæˆãŠã‚ˆã³ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸ2.5 TB ã® CommonCrawl ãƒ‡ãƒ¼ã‚¿ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¾ã—ãŸã€‚ ã“ã‚Œã¯ã€åˆ†é¡žã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ãƒ©ãƒ™ãƒ«ä»˜ã‘ã€è³ªå•å¿œç­”ãªã©ã®ãƒ€ã‚¦ãƒ³ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚¿ã‚¹ã‚¯ã§ã€mBERT ã‚„ XLM ãªã©ã®ä»¥å‰ã«ãƒªãƒªãƒ¼ã‚¹ã•ã‚ŒãŸå¤šè¨€èªžãƒ¢ãƒ‡ãƒ«ã‚’å¤§å¹…ã«æ”¹å–„ã—ã¾ã™ã€‚

## M2M100

The following M2M100 models can be used for multilingual translation:

- `facebook/m2m100_418M` (Translation)
- `facebook/m2m100_1.2B` (Translation)

In this example, load the `facebook/m2m100_418M` checkpoint to translate from Chinese to English. You can set the source language in the tokenizer:

```py
>>> from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer

>>> en_text = "Do not meddle in the affairs of wizards, for they are subtle and quick to anger."
>>> chinese_text = "ä¸è¦æ’æ‰‹å·«å¸«çš„äº‹å‹™, å› ç‚ºä»–å€‘æ˜¯å¾®å¦™çš„, å¾ˆå¿«å°±æœƒç™¼æ€’."

>>> tokenizer = M2M100Tokenizer.from_pretrained("facebook/m2m100_418M", src_lang="zh")
>>> model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_418M")
```

Tokenize the text:

```py
>>> encoded_zh = tokenizer(chinese_text, return_tensors="pt")
```

M2M100 forces the target language id as the first generated token to translate to the target language. Set the `forced_bos_token_id` to `en` in the `generate` method to translate to English:

```py
>>> generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id("en"))
>>> tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
'Do not interfere with the matters of the witches, because they are delicate and will soon be angry.'
```

## MBart

The following MBart models can be used for multilingual translation:

- `facebook/mbart-large-50-one-to-many-mmt` (One-to-many multilingual machine translation, 50 languages)
- `facebook/mbart-large-50-many-to-many-mmt` (Many-to-many multilingual machine translation, 50 languages)
- `facebook/mbart-large-50-many-to-one-mmt` (Many-to-one multilingual machine translation, 50 languages)
- `facebook/mbart-large-50` (Multilingual translation, 50 languages)
- `facebook/mbart-large-cc25`

In this example, load the `facebook/mbart-large-50-many-to-many-mmt` checkpoint to translate Finnish to English. You can set the source language in the tokenizer:

```py
>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

>>> en_text = "Do not meddle in the affairs of wizards, for they are subtle and quick to anger."
>>> fi_text = "Ã„lÃ¤ sekaannu velhojen asioihin, sillÃ¤ ne ovat hienovaraisia ja nopeasti vihaisia."

>>> tokenizer = AutoTokenizer.from_pretrained("facebook/mbart-large-50-many-to-many-mmt", src_lang="fi_FI")
>>> model = AutoModelForSeq2SeqLM.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")
```

Tokenize the text:

```py
>>> encoded_en = tokenizer(en_text, return_tensors="pt")
```

MBart forces the target language id as the first generated token to translate to the target language. Set the `forced_bos_token_id` to `en` in the `generate` method to translate to English:

```py
>>> generated_tokens = model.generate(**encoded_en, forced_bos_token_id=tokenizer.lang_code_to_id("en_XX"))
>>> tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
"Don't interfere with the wizard's affairs, because they are subtle, will soon get angry."
```

If you are using the `facebook/mbart-large-50-many-to-one-mmt` checkpoint, you don't need to force the target language id as the first generated token otherwise the usage is the same.