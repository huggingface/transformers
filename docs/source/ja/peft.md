<!--Copyright 2023 The HuggingFace Team. All rights reserved.
Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.
-->


# Load adapters with ğŸ¤— PEFT

[[open-in-colab]]

[Parameter-Efficient Fine Tuning (PEFT)](https://huggingface.co/blog/peft) ãƒ¡ã‚½ãƒƒãƒ‰ã¯ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«å‡çµã—ã€ãã®ä¸Šã«ã‚ãšã‹ãªè¨“ç·´å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ï¼‰ã‚’è¿½åŠ ã™ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã™ã€‚ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¯ã€ã‚¿ã‚¹ã‚¯å›ºæœ‰ã®æƒ…å ±ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã«è¨“ç·´ã•ã‚Œã¾ã™ã€‚ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå°‘ãªãã€å®Œå…¨ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã‚’ä½ãæŠ‘ãˆã¤ã¤ã€åŒç­‰ã®çµæœã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚

PEFTã§è¨“ç·´ã•ã‚ŒãŸã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¯é€šå¸¸ã€å®Œå…¨ãªãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºã‚ˆã‚Šã‚‚1æ¡å°ã•ãã€å…±æœ‰ã€ä¿å­˜ã€èª­ã¿è¾¼ã‚€ã®ãŒä¾¿åˆ©ã§ã™ã€‚

<div class="flex flex-col justify-center">
  <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/PEFT-hub-screenshot.png"/>
  <figcaption class="text-center">Hubã«æ ¼ç´ã•ã‚Œã¦ã„ã‚‹OPTForCausalLMãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼é‡ã¿ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®å…¨ä½“ã‚µã‚¤ã‚ºã®ç´„6MBã§ã€ãƒ¢ãƒ‡ãƒ«é‡ã¿ã®å…¨ã‚µã‚¤ã‚ºã¯ç´„700MBã§ã™ã€‚</figcaption>
</div>

ğŸ¤— PEFTãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«ã¤ã„ã¦è©³ã—ãçŸ¥ã‚ŠãŸã„å ´åˆã¯ã€[ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³](https://huggingface.co/docs/peft/index)ã‚’ã”è¦§ãã ã•ã„ã€‚

## Setup

ğŸ¤— PEFTã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦å§‹ã‚ã¾ã—ã‚‡ã†ï¼š


```bash
pip install peft
```

æ–°æ©Ÿèƒ½ã‚’è©¦ã—ã¦ã¿ãŸã„å ´åˆã€ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã“ã¨ã«èˆˆå‘³ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ï¼š

```bash
pip install git+https://github.com/huggingface/peft.git
```

## Supported PEFT models

ğŸ¤— Transformersã¯ã€ã„ãã¤ã‹ã®PEFTï¼ˆParameter Efficient Fine-Tuningï¼‰ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ãƒã‚¤ãƒ†ã‚£ãƒ–ã«ã‚µãƒãƒ¼ãƒˆã—ã¦ãŠã‚Šã€ãƒ­ãƒ¼ã‚«ãƒ«ã¾ãŸã¯Hubã«æ ¼ç´ã•ã‚ŒãŸã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚¦ã‚§ã‚¤ãƒˆã‚’ç°¡å˜ã«èª­ã¿è¾¼ã‚“ã§å®Ÿè¡Œã¾ãŸã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ãã¾ã™ã€‚ä»¥ä¸‹ã®ãƒ¡ã‚½ãƒƒãƒ‰ãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ï¼š

- [Low Rank Adapters](https://huggingface.co/docs/peft/conceptual_guides/lora)
- [IA3](https://huggingface.co/docs/peft/conceptual_guides/ia3)
- [AdaLoRA](https://arxiv.org/abs/2303.10512)

ä»–ã®PEFTãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ãŸã„å ´åˆã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå­¦ç¿’ã‚„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª¿æ•´ãªã©ã«ã¤ã„ã¦è©³ã—ãçŸ¥ã‚ŠãŸã„å ´åˆã€ã¾ãŸã¯ğŸ¤— PEFTãƒ©ã‚¤ãƒ–ãƒ©ãƒªå…¨èˆ¬ã«ã¤ã„ã¦ã¯ã€[ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³](https://huggingface.co/docs/peft/index)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚


## Load a PEFT adapter

ğŸ¤— Transformersã‹ã‚‰PEFTã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ä½¿ç”¨ã™ã‚‹ã«ã¯ã€Hubãƒªãƒã‚¸ãƒˆãƒªã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã« `adapter_config.json` ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚¦ã‚§ã‚¤ãƒˆãŒå«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚æ¬¡ã«ã€`AutoModelFor` ã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨ã—ã¦PEFTã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€ã“ã¨ãŒã§ãã¾ã™ã€‚ãŸã¨ãˆã°ã€å› æœè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ç”¨ã®PEFTã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€ã«ã¯ï¼š

1. PEFTãƒ¢ãƒ‡ãƒ«ã®IDã‚’æŒ‡å®šã—ã¾ã™ã€‚
2. ãã‚Œã‚’[`AutoModelForCausalLM`] ã‚¯ãƒ©ã‚¹ã«æ¸¡ã—ã¾ã™ã€‚


```py
from transformers import AutoModelForCausalLM, AutoTokenizer

peft_model_id = "ybelkada/opt-350m-lora"
model = AutoModelForCausalLM.from_pretrained(peft_model_id)
```

<Tip>

PEFTã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’`AutoModelFor`ã‚¯ãƒ©ã‚¹ã¾ãŸã¯åŸºæœ¬ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹ï¼ˆ`OPTForCausalLM`ã¾ãŸã¯`LlamaForCausalLM`ãªã©ï¼‰ã§èª­ã¿è¾¼ã‚€ã“ã¨ãŒã§ãã¾ã™ã€‚

</Tip>

ã¾ãŸã€`load_adapter`ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã™ã“ã¨ã§ã€PEFTã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’èª­ã¿è¾¼ã‚€ã“ã¨ã‚‚ã§ãã¾ã™ï¼š


```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "facebook/opt-350m"
peft_model_id = "ybelkada/opt-350m-lora"

model = AutoModelForCausalLM.from_pretrained(model_id)
model.load_adapter(peft_model_id)
```

## Load in 8bit or 4bit


`bitsandbytes` çµ±åˆã¯ã€8ãƒ“ãƒƒãƒˆãŠã‚ˆã³4ãƒ“ãƒƒãƒˆã®ç²¾åº¦ãƒ‡ãƒ¼ã‚¿å‹ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ãŠã‚Šã€å¤§è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€éš›ã«ãƒ¡ãƒ¢ãƒªã‚’ç¯€ç´„ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ï¼ˆè©³ç´°ã«ã¤ã„ã¦ã¯ `bitsandbytes` çµ±åˆã®[ã‚¬ã‚¤ãƒ‰](./quantization#bitsandbytes-integration)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ï¼‰ã€‚[`~PreTrainedModel.from_pretrained`] ã« `load_in_8bit` ã¾ãŸã¯ `load_in_4bit` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ã€`device_map="auto"` ã‚’è¨­å®šã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’åŠ¹æœçš„ã«ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã«åˆ†æ•£é…ç½®ã§ãã¾ã™ï¼š

```py
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

peft_model_id = "ybelkada/opt-350m-lora"
model = AutoModelForCausalLM.from_pretrained(peft_model_id, quantization_config=BitsAndBytesConfig(load_in_8bit=True))
```

## Add a new adapter

æ—¢å­˜ã®ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ã«æ–°ã—ã„ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’è¿½åŠ ã™ã‚‹ãŸã‚ã« [`~peft.PeftModel.add_adapter`] ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚ãŸã ã—ã€æ–°ã—ã„ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¯ç¾åœ¨ã®ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¨åŒã˜ã‚¿ã‚¤ãƒ—ã§ã‚ã‚‹é™ã‚Šã€ã“ã‚Œã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚ãŸã¨ãˆã°ã€ãƒ¢ãƒ‡ãƒ«ã«æ—¢å­˜ã® LoRA ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãŒã‚¢ã‚¿ãƒƒãƒã•ã‚Œã¦ã„ã‚‹å ´åˆï¼š


```py
from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
from peft import PeftConfig

model_id = "facebook/opt-350m"
model = AutoModelForCausalLM.from_pretrained(model_id)

lora_config = LoraConfig(
    target_modules=["q_proj", "k_proj"],
    init_lora_weights=False
)

model.add_adapter(lora_config, adapter_name="adapter_1")
```

æ–°ã—ã„ã‚¢ãƒ€ãƒ—ã‚¿ã‚’è¿½åŠ ã™ã‚‹ã«ã¯:


```py
# attach new adapter with same config
model.add_adapter(lora_config, adapter_name="adapter_2")
```

[`~peft.PeftModel.set_adapter`] ã‚’ä½¿ç”¨ã—ã¦ã€ã©ã®ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã‚’è¨­å®šã§ãã¾ã™ï¼š


```py
# use adapter_1
model.set_adapter("adapter_1")
output = model.generate(**inputs)
print(tokenizer.decode(output_disabled[0], skip_special_tokens=True))

# use adapter_2
model.set_adapter("adapter_2")
output_enabled = model.generate(**inputs)
print(tokenizer.decode(output_enabled[0], skip_special_tokens=True))
```

## Enable and disable adapters

ãƒ¢ãƒ‡ãƒ«ã«ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’è¿½åŠ ã—ãŸã‚‰ã€ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’æœ‰åŠ¹ã¾ãŸã¯ç„¡åŠ¹ã«ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã«ã¯ã€æ¬¡ã®æ‰‹é †ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š

```py
from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
from peft import PeftConfig

model_id = "facebook/opt-350m"
adapter_model_id = "ybelkada/opt-350m-lora"
tokenizer = AutoTokenizer.from_pretrained(model_id)
text = "Hello"
inputs = tokenizer(text, return_tensors="pt")

model = AutoModelForCausalLM.from_pretrained(model_id)
peft_config = PeftConfig.from_pretrained(adapter_model_id)

# to initiate with random weights
peft_config.init_lora_weights = False

model.add_adapter(peft_config)
model.enable_adapters()
output = model.generate(**inputs)
```

ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ç„¡åŠ¹ã«ã™ã‚‹ã«ã¯ï¼š

```py
model.disable_adapters()
output = model.generate(**inputs)
```

## Train a PEFT adapter

PEFTã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¯[`Trainer`]ã‚¯ãƒ©ã‚¹ã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ãŠã‚Šã€ç‰¹å®šã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«å¯¾ã—ã¦ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚æ•°è¡Œã®ã‚³ãƒ¼ãƒ‰ã‚’è¿½åŠ ã™ã‚‹ã ã‘ã§æ¸ˆã¿ã¾ã™ã€‚ãŸã¨ãˆã°ã€LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹å ´åˆ:

<Tip>

[`Trainer`]ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã«æ…£ã‚Œã¦ã„ãªã„å ´åˆã¯ã€[äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´](training)ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’ã”è¦§ãã ã•ã„ã€‚

</Tip>

1. ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®æ§‹æˆã‚’å®šç¾©ã—ã¾ã™ï¼ˆãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è©³ç´°ã«ã¤ã„ã¦ã¯[`~peft.LoraConfig`]ã‚’å‚ç…§ã—ã¦ãã ã•ã„ï¼‰ã€‚


```py
from peft import LoraConfig

peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="CAUSAL_LM",
)
```

2. ãƒ¢ãƒ‡ãƒ«ã«ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’è¿½åŠ ã™ã‚‹ã€‚


```py
model.add_adapter(peft_config)
```

3. ã“ã‚Œã§ã€ãƒ¢ãƒ‡ãƒ«ã‚’ [`Trainer`] ã«æ¸¡ã™ã“ã¨ãŒã§ãã¾ã™ï¼

```py
trainer = Trainer(model=model, ...)
trainer.train()
```

ä¿å­˜ã™ã‚‹ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ã‚¢ãƒ€ãƒ—ã‚¿ã¨ãã‚Œã‚’èª­ã¿è¾¼ã‚€ãŸã‚ã®æ‰‹é †ï¼š
