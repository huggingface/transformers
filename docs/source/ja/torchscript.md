<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Export to TorchScript

> [!TIP]
> ã“ã‚Œã¯TorchScriptã‚’ä½¿ç”¨ã—ãŸå®Ÿé¨“ã®æœ€åˆã§ã‚ã‚Šã€å¯å¤‰å…¥åŠ›ã‚µã‚¤ã‚ºã®ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã™ã‚‹ãã®èƒ½åŠ›ã‚’ã¾ã æ¢æ±‚ä¸­ã§ã™ã€‚ã“ã‚Œã¯ç§ãŸã¡ã®é–¢å¿ƒã®ç„¦ç‚¹ã§ã‚ã‚Šã€ä»Šå¾Œã®ãƒªãƒªãƒ¼ã‚¹ã§ã¯ã€ã‚ˆã‚ŠæŸ”è»Ÿãªå®Ÿè£…ã‚„ã€Pythonãƒ™ãƒ¼ã‚¹ã®ã‚³ãƒ¼ãƒ‰ã¨ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã•ã‚ŒãŸTorchScriptã‚’æ¯”è¼ƒã™ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å«ã‚€ã€ã‚ˆã‚Šå¤šãã®ã‚³ãƒ¼ãƒ‰ä¾‹ã§è©³ç´°ãªåˆ†æã‚’è¡Œã„ã¾ã™ã€‚

[TorchScriptã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://pytorch.org/docs/stable/jit.html)ã«ã‚ˆã‚Œã°ï¼š

> TorchScriptã¯ã€PyTorchã‚³ãƒ¼ãƒ‰ã‹ã‚‰ç›´åˆ—åŒ–ãŠã‚ˆã³æœ€é©åŒ–å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã™ã‚‹æ–¹æ³•ã§ã™ã€‚

TorchScriptã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€åŠ¹ç‡å¿—å‘ã®C++ãƒ—ãƒ­ã‚°ãƒ©ãƒ ãªã©ã€ä»–ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§ãƒ¢ãƒ‡ãƒ«ã‚’å†åˆ©ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚PyTorchãƒ™ãƒ¼ã‚¹ã®Pythonãƒ—ãƒ­ã‚°ãƒ©ãƒ ä»¥å¤–ã®ç’°å¢ƒã§ğŸ¤— Transformersãƒ¢ãƒ‡ãƒ«ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¦ä½¿ç”¨ã™ã‚‹ãŸã‚ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚ã“ã“ã§ã¯ã€TorchScriptã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã€ä½¿ç”¨ã™ã‚‹æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚

ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ã«ã¯ã€æ¬¡ã®2ã¤ã®è¦ä»¶ãŒã‚ã‚Šã¾ã™ï¼š

- `torchscript`ãƒ•ãƒ©ã‚°ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
- ãƒ€ãƒŸãƒ¼ã®å…¥åŠ›ã‚’ä½¿ç”¨ã—ãŸãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹

ã“ã‚Œã‚‰ã®å¿…è¦æ¡ä»¶ã¯ã€ä»¥ä¸‹ã§è©³ç´°ã«èª¬æ˜ã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ã«ã€é–‹ç™ºè€…ãŒæ³¨æ„ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã„ãã¤ã‹ã®ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚

## TorchScript flag and tied weights

`torchscript`ãƒ•ãƒ©ã‚°ã¯ã€ã»ã¨ã‚“ã©ã®ğŸ¤— Transformersè¨€èªãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦ã€`Embedding`ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¨`Decoding`ãƒ¬ã‚¤ãƒ¤ãƒ¼é–“ã§é‡ã¿ãŒé€£çµã•ã‚Œã¦ã„ã‚‹ãŸã‚å¿…è¦ã§ã™ã€‚
TorchScriptã§ã¯ã€é‡ã¿ãŒé€£çµã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã®ã§ã€äº‹å‰ã«é‡ã¿ã‚’åˆ‡ã‚Šé›¢ã—ã¦è¤‡è£½ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

`torchscript`ãƒ•ãƒ©ã‚°ã‚’ä½¿ç”¨ã—ã¦ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯ã€`Embedding`ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¨`Decoding`ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒåˆ†é›¢ã•ã‚Œã¦ãŠã‚Šã€ãã®ãŸã‚å¾Œã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚
ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã€ã“ã‚Œã‚‰ã®2ã¤ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’éåŒæœŸã«ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã€äºˆæœŸã—ãªã„çµæœã‚’ã‚‚ãŸã‚‰ã™å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

è¨€èªãƒ¢ãƒ‡ãƒ«ãƒ˜ãƒƒãƒ‰ã‚’æŒãŸãªã„ãƒ¢ãƒ‡ãƒ«ã«ã¯è¨€åŠã—ã¾ã›ã‚“ãŒã€ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã«ã¯é€£çµã•ã‚ŒãŸé‡ã¿ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€`torchscript`ãƒ•ãƒ©ã‚°ãªã—ã§å®‰å…¨ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã§ãã¾ã™ã€‚

## Dummy inputs and standard lengths

ãƒ€ãƒŸãƒ¼å…¥åŠ›ã¯ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚å…¥åŠ›ã®å€¤ã¯ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’é€šã˜ã¦ä¼æ’­ã•ã‚Œã‚‹é–“ã€PyTorchã¯å„ãƒ†ãƒ³ã‚½ãƒ«ã«å®Ÿè¡Œã•ã‚ŒãŸç•°ãªã‚‹æ“ä½œã‚’è¿½è·¡ã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®è¨˜éŒ²ã•ã‚ŒãŸæ“ä½œã¯ã€ãƒ¢ãƒ‡ãƒ«ã®*ãƒˆãƒ¬ãƒ¼ã‚¹*ã‚’ä½œæˆã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

ãƒˆãƒ¬ãƒ¼ã‚¹ã¯å…¥åŠ›ã®å¯¸æ³•ã«å¯¾ã—ã¦ä½œæˆã•ã‚Œã¾ã™ã€‚ãã®ãŸã‚ã€ãƒ€ãƒŸãƒ¼å…¥åŠ›ã®å¯¸æ³•ã«åˆ¶ç´„ã•ã‚Œã€ä»–ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚„ãƒãƒƒãƒã‚µã‚¤ã‚ºã§ã¯å‹•ä½œã—ã¾ã›ã‚“ã€‚ç•°ãªã‚‹ã‚µã‚¤ã‚ºã§è©¦ã™ã¨ã€ä»¥ä¸‹ã®ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã™ï¼š

```
`The expanded size of the tensor (3) must match the existing size (7) at non-singleton dimension 2`
```

ãŠå‹§ã‚ã—ã¾ã™ã®ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ä¸­ã«ä¾›çµ¦ã•ã‚Œã‚‹æœ€å¤§ã®å…¥åŠ›ã¨åŒã˜å¤§ãã•ã®ãƒ€ãƒŸãƒ¼å…¥åŠ›ã‚µã‚¤ã‚ºã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹ã™ã‚‹ã“ã¨ã§ã™ã€‚ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä½¿ç”¨ã—ã¦ä¸è¶³å€¤ã‚’è£œå®Œã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ãŸã ã—ã€ãƒ¢ãƒ‡ãƒ«ãŒã‚ˆã‚Šå¤§ããªå…¥åŠ›ã‚µã‚¤ã‚ºã§ãƒˆãƒ¬ãƒ¼ã‚¹ã•ã‚Œã‚‹ãŸã‚ã€è¡Œåˆ—ã®å¯¸æ³•ã‚‚å¤§ãããªã‚Šã€ã‚ˆã‚Šå¤šãã®è¨ˆç®—ãŒç™ºç”Ÿã—ã¾ã™ã€‚

ç•°ãªã‚‹ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã®ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹éš›ã«ã€å„å…¥åŠ›ã«å¯¾ã—ã¦å®Ÿè¡Œã•ã‚Œã‚‹æ¼”ç®—ã®ç·æ•°ã«æ³¨æ„ã—ã¦ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å¯†æ¥ã«ãƒ•ã‚©ãƒ­ãƒ¼ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚

## Using TorchScript in Python

ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã¨èª­ã¿è¾¼ã¿ã€ãŠã‚ˆã³æ¨è«–ã«ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹æ–¹æ³•ã‚’ç¤ºã—ã¾ã™ã€‚

### Saving a model

TorchScriptã§`BertModel`ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ã«ã¯ã€`BertConfig`ã‚¯ãƒ©ã‚¹ã‹ã‚‰`BertModel`ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã€ãã‚Œã‚’ãƒ•ã‚¡ã‚¤ãƒ«å`traced_bert.pt`ã§ãƒ‡ã‚£ã‚¹ã‚¯ã«ä¿å­˜ã—ã¾ã™ï¼š

```python
from transformers import BertModel, BertTokenizer, BertConfig
import torch

enc = BertTokenizer.from_pretrained("google-bert/bert-base-uncased")

# Tokenizing input text
text = "[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]"
tokenized_text = enc.tokenize(text)

# Masking one of the input tokens
masked_index = 8
tokenized_text[masked_index] = "[MASK]"
indexed_tokens = enc.convert_tokens_to_ids(tokenized_text)
segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]

# Creating a dummy input
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])
dummy_input = [tokens_tensor, segments_tensors]

# Initializing the model with the torchscript flag
# Flag set to True even though it is not necessary as this model does not have an LM Head.
config = BertConfig(
    vocab_size_or_config_json_file=32000,
    hidden_size=768,
    num_hidden_layers=12,
    num_attention_heads=12,
    intermediate_size=3072,
    torchscript=True,
)

# Instantiating the model
model = BertModel(config)

# The model needs to be in evaluation mode
model.eval()

# If you are instantiating the model with *from_pretrained* you can also easily set the TorchScript flag
model = BertModel.from_pretrained("google-bert/bert-base-uncased", torchscript=True)

# Creating the trace
traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])
torch.jit.save(traced_model, "traced_bert.pt")
```

### Loading a model

ä»¥å‰ã«ä¿å­˜ã—ãŸ `BertModel`ã€`traced_bert.pt` ã‚’ãƒ‡ã‚£ã‚¹ã‚¯ã‹ã‚‰èª­ã¿è¾¼ã‚“ã§ã€ä»¥å‰ã«åˆæœŸåŒ–ã—ãŸ `dummy_input` ã§ä½¿ç”¨ã§ãã¾ã™ã€‚

```python
loaded_model = torch.jit.load("traced_bert.pt")
loaded_model.eval()

all_encoder_layers, pooled_output = loaded_model(*dummy_input)
```


### Using a traced model for inference

ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦æ¨è«–ã‚’è¡Œã†ã«ã¯ã€ãã® `__call__` ãƒ€ãƒ³ãƒ€ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

```python
traced_model(tokens_tensor, segments_tensors)
```


## Deploy Hugging Face TorchScript models to AWS with the Neuron SDK

AWSã¯ã‚¯ãƒ©ã‚¦ãƒ‰ã§ã®ä½ã‚³ã‚¹ãƒˆã§é«˜æ€§èƒ½ãªæ©Ÿæ¢°å­¦ç¿’æ¨è«–å‘ã‘ã« [Amazon EC2 Inf1](https://aws.amazon.com/ec2/instance-types/inf1/) ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã‚’å°å…¥ã—ã¾ã—ãŸã€‚Inf1ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã¯AWS Inferentiaãƒãƒƒãƒ—ã«ã‚ˆã£ã¦é§†å‹•ã•ã‚Œã€ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°æ¨è«–ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã«ç‰¹åŒ–ã—ãŸã‚«ã‚¹ã‚¿ãƒ ãƒ“ãƒ«ãƒ‰ã®ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚¿ã§ã™ã€‚[AWS Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/#) ã¯Inferentiaç”¨ã®SDKã§ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹ã—ã¦æœ€é©åŒ–ã—ã€Inf1ã«å±•é–‹ã™ã‚‹ãŸã‚ã®ã‚µãƒãƒ¼ãƒˆã‚’æä¾›ã—ã¾ã™ã€‚

Neuron SDK ãŒæä¾›ã™ã‚‹ã‚‚ã®:

1. ã‚¯ãƒ©ã‚¦ãƒ‰ã§ã®æ¨è«–ã®ãŸã‚ã«TorchScriptãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹ã—ã¦æœ€é©åŒ–ã™ã‚‹ãŸã‚ã®ã€1è¡Œã®ã‚³ãƒ¼ãƒ‰å¤‰æ›´ã§ä½¿ç”¨ã§ãã‚‹ç°¡å˜ãªAPIã€‚
2. [æ”¹å–„ã•ã‚ŒãŸã‚³ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/benchmark/) ã®ãŸã‚ã®ãƒœãƒƒã‚¯ã‚¹å¤–ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã€‚
3. [PyTorch](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/bert_tutorial/tutorial_pretrained_bert.html) ã¾ãŸã¯ [TensorFlow](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/tensorflow/huggingface_bert/huggingface_bert.html) ã§æ§‹ç¯‰ã•ã‚ŒãŸHugging Faceãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã¸ã®ã‚µãƒãƒ¼ãƒˆã€‚

### Implications

BERTï¼ˆBidirectional Encoder Representations from Transformersï¼‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚„ãã®å¤‰ç¨®ï¼ˆ[distilBERT](https://huggingface.co/docs/transformers/main/model_doc/distilbert) ã‚„ [roBERTa](https://huggingface.co/docs/transformers/main/model_doc/roberta) ãªã©ï¼‰ã«åŸºã¥ããƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã¯ã€éç”Ÿæˆã‚¿ã‚¹ã‚¯ï¼ˆæŠ½å‡ºå‹è³ªå•å¿œç­”ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åˆ†é¡ã€ãƒˆãƒ¼ã‚¯ãƒ³åˆ†é¡ãªã©ï¼‰ã«ãŠã„ã¦ã€Inf1ä¸Šã§æœ€é©ã«å‹•ä½œã—ã¾ã™ã€‚ãŸã ã—ã€ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¿ã‚¹ã‚¯ã‚‚ [AWS Neuron MarianMT ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/transformers-marianmt.html) ã«å¾“ã£ã¦Inf1ä¸Šã§å®Ÿè¡Œã§ãã¾ã™ã€‚Inferentiaã§ãƒœãƒƒã‚¯ã‚¹å¤–ã§å¤‰æ›ã§ãã‚‹ãƒ¢ãƒ‡ãƒ«ã«é–¢ã™ã‚‹è©³ç´°æƒ…å ±ã¯ã€Neuronãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã® [Model Architecture Fit](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/models/models-inferentia.html#models-inferentia) ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ã‚ã‚Šã¾ã™ã€‚

### Dependencies

ãƒ¢ãƒ‡ãƒ«ã‚’AWS Neuronã«å¤‰æ›ã™ã‚‹ã«ã¯ã€[Neuron SDK ç’°å¢ƒ](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-frameworks/pytorch-neuron/index.html#installation-guide) ãŒå¿…è¦ã§ã€[AWS Deep Learning AMI](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-launching.html) ã«äº‹å‰ã«æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚

### Converting a model for AWS Neuron

ãƒ¢ãƒ‡ãƒ«ã‚’AWS NEURONç”¨ã«å¤‰æ›ã™ã‚‹ã«ã¯ã€[Pythonã§TorchScriptã‚’ä½¿ç”¨ã™ã‚‹](torchscript#using-torchscript-in-python) ã¨åŒã˜ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ `BertModel` ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹ã—ã¾ã™ã€‚Python APIã‚’ä»‹ã—ã¦Neuron SDKã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãŸã‚ã«ã€`torch.neuron` ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯æ‹¡å¼µã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚

```python
from transformers import BertModel, BertTokenizer, BertConfig
import torch
import torch.neuron
```

æ¬¡ã®è¡Œã‚’å¤‰æ›´ã™ã‚‹ã ã‘ã§æ¸ˆã¿ã¾ã™ã€‚

```diff
- torch.jit.trace(model, [tokens_tensor, segments_tensors])
+ torch.neuron.trace(model, [token_tensor, segments_tensors])
```

ã“ã‚Œã«ã‚ˆã‚Šã€Neuron SDKã¯ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹ã—ã€Inf1ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å‘ã‘ã«æœ€é©åŒ–ã—ã¾ã™ã€‚

AWS Neuron SDKã®æ©Ÿèƒ½ã€ãƒ„ãƒ¼ãƒ«ã€ã‚µãƒ³ãƒ—ãƒ«ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã€æœ€æ–°ã®ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã«ã¤ã„ã¦è©³ã—ãçŸ¥ã‚ŠãŸã„å ´åˆã¯ã€[AWS NeuronSDK ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html) ã‚’ã”è¦§ãã ã•ã„ã€‚



