<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Efficient Inference on a Single GPU

ã“ã®ã‚¬ã‚¤ãƒ‰ã«åŠ ãˆã¦ã€[1ã¤ã®GPUã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰](perf_train_gpu_one)ã¨[CPUã§ã®æ¨è«–ã‚¬ã‚¤ãƒ‰](perf_infer_cpu)ã«é–¢é€£ã™ã‚‹æƒ…å ±ãŒã‚ã‚Šã¾ã™ã€‚

## Flash Attention 2

<Tip>

ã“ã®æ©Ÿèƒ½ã¯å®Ÿé¨“çš„ã§ã‚ã‚Šã€å°†æ¥ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§å¤§å¹…ã«å¤‰æ›´ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ãŸã¨ãˆã°ã€Flash Attention 2 APIã¯è¿‘ã„å°†æ¥`BetterTransformer` APIã«ç§»è¡Œã™ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚

</Tip>

Flash Attention 2ã¯ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¨è«–é€Ÿåº¦ã‚’å¤§å¹…ã«é«˜é€ŸåŒ–ã§ãã¾ã™ã€‚Flash Attention 2ã¯ã€Tri Daoæ°ã«ã‚ˆã£ã¦[å…¬å¼ã®Flash Attentionãƒªãƒã‚¸ãƒˆãƒª](https://github.com/Dao-AILab/flash-attention)ã§å°å…¥ã•ã‚Œã¾ã—ãŸã€‚Flash Attentionã«é–¢ã™ã‚‹ç§‘å­¦è«–æ–‡ã¯[ã“ã¡ã‚‰](https://arxiv.org/abs/2205.14135)ã§è¦‹ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

Flash Attention 2ã‚’æ­£ã—ãã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã«ã¯ã€ä¸Šè¨˜ã®ãƒªãƒã‚¸ãƒˆãƒªã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚¬ã‚¤ãƒ‰ã«å¾“ã£ã¦ãã ã•ã„ã€‚

ä»¥ä¸‹ã®ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦Flash Attention 2ã‚’ãƒã‚¤ãƒ†ã‚£ãƒ–ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ï¼š

- Llama
- Falcon

ã•ã‚‰ã«å¤šãã®ãƒ¢ãƒ‡ãƒ«ã«Flash Attention 2ã®ã‚µãƒãƒ¼ãƒˆã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã‚’GitHubã§ææ¡ˆã™ã‚‹ã“ã¨ã‚‚ã§ãã€å¤‰æ›´ã‚’çµ±åˆã™ã‚‹ãŸã‚ã«ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é–‹ãã“ã¨ã‚‚ã§ãã¾ã™ã€‚ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å«ã‚€ã€æ¨è«–ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä½¿ç”¨ã§ãã¾ã™ï¼ˆç¾åœ¨ã®`BetterTransformer` APIã§ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ï¼‰ã€‚

<Tip>

Flash Attention 2ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®dtypeãŒ`fp16`ã¾ãŸã¯`bf16`ã®å ´åˆã«ã®ã¿ä½¿ç”¨ã§ãã€NVIDIA-GPUãƒ‡ãƒã‚¤ã‚¹ã§ã®ã¿å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚ã“ã®æ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹å‰ã«ã€ãƒ¢ãƒ‡ãƒ«ã‚’é©åˆ‡ãªdtypeã«ã‚­ãƒ£ã‚¹ãƒˆã—ã€ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒã‚¤ã‚¹ã«ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚

</Tip>

### Quick usage

ãƒ¢ãƒ‡ãƒ«ã§Flash Attention 2ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã«ã¯ã€`from_pretrained`ã®å¼•æ•°ã«`attn_implementation="flash_attention_2"`ã‚’è¿½åŠ ã—ã¾ã™ã€‚


```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM

model_id = "tiiuae/falcon-7b"
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(
    model_id, 
    torch_dtype=torch.bfloat16, 
    attn_implementation="flash_attention_2",
)
```

ã“ã¡ã‚‰ã¯ã€ç”Ÿæˆã¾ãŸã¯å¾®èª¿æ•´ã®ãŸã‚ã«ä½¿ç”¨ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚

### Expected speedups

ç‰¹ã«é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¯¾ã—ã¦ã€å¾®èª¿æ•´ã¨æ¨è«–ã®éš›ã«ã¯ã€ã‹ãªã‚Šã®é«˜é€ŸåŒ–ãŒæœŸå¾…ã§ãã¾ã™ã€‚ãŸã ã—ã€Flash Attentionã¯ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨ã—ã¦ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã—ãªã„ãŸã‚ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ãŒå«ã¾ã‚Œã‚‹å ´åˆã€ãƒãƒƒãƒæ¨è«–ã«ãŠã„ã¦ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’æ‰‹å‹•ã§ãƒ‘ãƒƒãƒ‰/ã‚¢ãƒ³ãƒ‘ãƒƒãƒ‰ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã€ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å«ã‚€ãƒãƒƒãƒç”Ÿæˆã®å¤§å¹…ãªé…å»¶ãŒç™ºç”Ÿã—ã¾ã™ã€‚

ã“ã‚Œã‚’å…‹æœã™ã‚‹ãŸã‚ã«ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨ã›ãšã«Flash Attentionã‚’ä½¿ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼ˆãŸã¨ãˆã°ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ‘ãƒƒã‚¯ã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã«é”ã™ã‚‹ã¾ã§é€£çµã™ã‚‹ã“ã¨ãªã©ï¼‰ã€‚ã“ã“ã«[ä¾‹](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py#L516)ãŒæä¾›ã•ã‚Œã¦ã„ã¾ã™ã€‚

ä»¥ä¸‹ã¯ã€ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®ãªã„å ´åˆã«ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ãŒ4096ã®[tiiuae/falcon-7b](https://hf.co/tiiuae/falcon-7b)ã«å¯¾ã™ã‚‹å˜ç´”ãªãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã®äºˆæƒ³ã•ã‚Œã‚‹é«˜é€ŸåŒ–ã§ã™ã€‚ã•ã¾ã–ã¾ãªãƒãƒƒãƒã‚µã‚¤ã‚ºãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ï¼š

<div style="text-align: center">
<img src="https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/falcon-7b-inference-large-seqlen.png">
</div>

ä»¥ä¸‹ã¯ã€ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®ãªã„å ´åˆã«ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ãŒ4096ã®[`meta-llama/Llama-7b-hf`](https://hf.co/meta-llama/Llama-7b-hf)ã«å¯¾ã™ã‚‹å˜ç´”ãªãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã®äºˆæƒ³ã•ã‚Œã‚‹é«˜é€ŸåŒ–ã§ã™ã€‚ã•ã¾ã–ã¾ãªãƒãƒƒãƒã‚µã‚¤ã‚ºãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ï¼š

<div style="text-align: center">
<img src="https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/llama-7b-inference-large-seqlen.png">
</div>

ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å«ã‚€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ï¼ˆãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¾ãŸã¯ç”Ÿæˆã™ã‚‹ï¼‰ã®å ´åˆã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’æ­£ã—ãè¨ˆç®—ã™ã‚‹ãŸã‚ã«å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ã‚¢ãƒ³ãƒ‘ãƒƒãƒ‰/ãƒ‘ãƒƒãƒ‰ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚æ¯”è¼ƒçš„å°ã•ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã®å ´åˆã€ç´”ç²‹ãªãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã§ã¯ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ãŒ30%æœªæº€ã—ã‹åŸ‹ã‚ã‚‰ã‚Œã¦ã„ãªã„ãŸã‚ã€ã“ã‚Œã¯ã‚ãšã‹ãªé«˜é€ŸåŒ–ã‚’ã‚‚ãŸã‚‰ã—ã¾ã™ã€‚

<div style="text-align: center">
<img src="https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/llama-2-small-seqlen-padding.png">
</div>

ã—ã‹ã—ã€å¤§ããªã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã®å ´åˆã€ç´”ç²‹ãªæ¨è«–ï¼ˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚‚å«ã‚€ï¼‰ã«ã¯èˆˆå‘³æ·±ã„é«˜é€ŸåŒ–ãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚

Flash Attentionã¯ã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¨ˆç®—ã‚’ã‚ˆã‚Šãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®è‰¯ã„ã‚‚ã®ã«ã—ã€å¤§ããªã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã§ã®CUDA OOMã®å•é¡Œã‚’å›é¿ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚å¤§ããªã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã«å¯¾ã—ã¦æœ€å¤§20ã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã‚’ã‚‚ãŸã‚‰ã™ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚è©³ç´°ã«ã¤ã„ã¦ã¯ã€[å…¬å¼ã®Flash Attentionãƒªãƒã‚¸ãƒˆãƒª](https://github.com/Dao-AILab/flash-attention)ã‚’ã”è¦§ãã ã•ã„ã€‚

<div style="text-align: center">
<img src="https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/llama-2-large-seqlen-padding.png">
</div>


### Advanced usage

ã“ã®æ©Ÿèƒ½ã‚’ãƒ¢ãƒ‡ãƒ«ã®æœ€é©åŒ–ã«å¤šãã®æ—¢å­˜ã®æ©Ÿèƒ½ã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ä»¥ä¸‹ã«ã„ãã¤ã‹ã®ä¾‹ã‚’ç¤ºã—ã¾ã™ï¼š

### Combining Flash Attention 2 and 8-bit models

ã“ã®æ©Ÿèƒ½ã‚’8ãƒ“ãƒƒãƒˆã®é‡å­åŒ–ã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼š

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM

model_id = "tiiuae/falcon-7b"
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(
    model_id, 
    load_in_8bit=True,
    attn_implementation="flash_attention_2",
)
```

### Combining Flash Attention 2 and 4-bit models

ã“ã®æ©Ÿèƒ½ã‚’ 4 ãƒ“ãƒƒãƒˆã®é‡å­åŒ–ã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼š

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM

model_id = "tiiuae/falcon-7b"
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(
    model_id, 
    load_in_4bit=True,
    attn_implementation="flash_attention_2",
)
```

### Combining Flash Attention 2 and PEFT

ã“ã®æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¦ã€Flash Attention 2ã‚’ãƒ™ãƒ¼ã‚¹ã«ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹éš›ã«PEFTã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM
from peft import LoraConfig

model_id = "tiiuae/falcon-7b"
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(
    model_id, 
    load_in_4bit=True,
    attn_implementation="flash_attention_2",
)

lora_config = LoraConfig(
    r=8,
    task_type="CAUSAL_LM"
)

model.add_adapter(lora_config)

... # train your model
```

## BetterTransformer

[BetterTransformer](https://huggingface.co/docs/optimum/bettertransformer/overview)ã¯ã€ğŸ¤— Transformersãƒ¢ãƒ‡ãƒ«ã‚’PyTorchãƒã‚¤ãƒ†ã‚£ãƒ–ã®é«˜é€Ÿãƒ‘ã‚¹å®Ÿè¡Œã«å¤‰æ›ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€Flash Attentionãªã©ã®æœ€é©åŒ–ã•ã‚ŒãŸã‚«ãƒ¼ãƒãƒ«ãŒå†…éƒ¨ã§å‘¼ã³å‡ºã•ã‚Œã¾ã™ã€‚

BetterTransformerã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€ãŠã‚ˆã³ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ¢ãƒ‡ãƒ«ã®å˜ä¸€ãŠã‚ˆã³ãƒãƒ«ãƒGPUã§ã®é«˜é€Ÿãªæ¨è«–ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚

<Tip>

Flash Attentionã¯ã€fp16ã¾ãŸã¯bf16ã®dtypeã‚’ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã«ã®ã¿ä½¿ç”¨ã§ãã¾ã™ã€‚BetterTransformerã‚’ä½¿ç”¨ã™ã‚‹å‰ã«ã€ãƒ¢ãƒ‡ãƒ«ã‚’é©åˆ‡ãªdtypeã«ã‚­ãƒ£ã‚¹ãƒˆã—ã¦ãã ã•ã„ã€‚
  
</Tip>

### Encoder models

PyTorchãƒã‚¤ãƒ†ã‚£ãƒ–ã®[`nn.MultiHeadAttention`](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/)ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é«˜é€Ÿãƒ‘ã‚¹ã€BetterTransformerã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã¯ã€[ğŸ¤— Optimumãƒ©ã‚¤ãƒ–ãƒ©ãƒª](https://huggingface.co/docs/optimum/bettertransformer/overview)ã®çµ±åˆã‚’é€šã˜ã¦Transformersã¨ä¸€ç·’ã«ä½¿ç”¨ã§ãã¾ã™ã€‚

PyTorchã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é«˜é€Ÿãƒ‘ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€ã‚«ãƒ¼ãƒãƒ«ãƒ•ãƒ¥ãƒ¼ã‚¸ãƒ§ãƒ³ã¨[ãƒã‚¹ãƒˆã•ã‚ŒãŸãƒ†ãƒ³ã‚½ãƒ«](https://pytorch.org/docs/stable/nested.html)ã®ä½¿ç”¨ã«ã‚ˆã‚Šã€æ¨è«–ã‚’é«˜é€ŸåŒ–ã§ãã¾ã™ã€‚è©³ç´°ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æƒ…å ±ã¯[ã“ã®ãƒ–ãƒ­ã‚°è¨˜äº‹](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2)ã«ã‚ã‚Šã¾ã™ã€‚

[`optimum`](https://github.com/huggingface/optimum)ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ãŸå¾Œã€æ¨è«–ä¸­ã«Better Transformerã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€é–¢é€£ã™ã‚‹å†…éƒ¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã™ã“ã¨ã§ç½®ãæ›ãˆã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™[`~PreTrainedModel.to_bettertransformer`]:


```python
model = model.to_bettertransformer()
```

ãƒ¡ã‚½ãƒƒãƒ‰ [`~PreTrainedModel.reverse_bettertransformer`] ã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹å‰ã«ä½¿ç”¨ã™ã¹ãã§ã€æ¨™æº–ã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã®ã‚‚ã®ã§ã™ï¼š

```python
model = model.reverse_bettertransformer()
model.save_pretrained("saved_model")
```

BetterTransformer APIã‚’ä½¿ã£ãŸã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¢ãƒ‡ãƒ«ã®å¯èƒ½æ€§ã«ã¤ã„ã¦è©³ã—ãçŸ¥ã‚‹ã«ã¯ã€[ã“ã®ãƒ–ãƒ­ã‚°ãƒã‚¹ãƒˆ](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2)ã‚’ã”è¦§ãã ã•ã„ã€‚

### Decoder models

ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã€ç‰¹ã«ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆGPTã€T5ã€Llamaãªã©ï¼‰ã«ã¨ã£ã¦ã€BetterTransformer APIã¯ã™ã¹ã¦ã®æ³¨æ„æ“ä½œã‚’[`torch.nn.functional.scaled_dot_product_attention`ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention)ï¼ˆSDPAï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«å¤‰æ›ã—ã¾ã™ã€‚ã“ã®ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼ã¯PyTorch 2.0ä»¥é™ã§ã®ã¿åˆ©ç”¨å¯èƒ½ã§ã™ã€‚

ãƒ¢ãƒ‡ãƒ«ã‚’BetterTransformerã«å¤‰æ›ã™ã‚‹ã«ã¯ã€ä»¥ä¸‹ã®æ‰‹é †ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š

```python
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m")
# convert the model to BetterTransformer
model.to_bettertransformer()

# Use it for training or inference
```

SDPAã¯ã€ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚„å•é¡Œã®ã‚µã‚¤ã‚ºã«å¿œã˜ã¦[Flash Attention](https://arxiv.org/abs/2205.14135)ã‚«ãƒ¼ãƒãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚Flash Attentionã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹ã€ç‰¹å®šã®è¨­å®šï¼ˆãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã€å•é¡Œã‚µã‚¤ã‚ºï¼‰ã§ä½¿ç”¨å¯èƒ½ã‹ã©ã†ã‹ã‚’ç¢ºèªã™ã‚‹ã«ã¯ã€[`torch.backends.cuda.sdp_kernel`](https://pytorch.org/docs/master/backends.html#torch.backends.cuda.sdp_kernel)ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚


```diff
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")
model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", torch_dtype=torch.float16).to("cuda")
# convert the model to BetterTransformer
model.to_bettertransformer()

input_text = "Hello my dog is cute and"
inputs = tokenizer(input_text, return_tensors="pt").to("cuda")

+ with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):
    outputs = model.generate(**inputs)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

ã‚‚ã—ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯ã«ãƒã‚°ãŒè¡¨ç¤ºã•ã‚ŒãŸå ´åˆ

```bash
RuntimeError: No available kernel.  Aborting execution.
```

Flash Attention ã®åºƒç¯„ãªã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’æŒã¤ã‹ã‚‚ã—ã‚Œãªã„ PyTorch ã®ãƒŠã‚¤ãƒˆãƒªãƒ¼ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’è©¦ã—ã¦ã¿ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚

```bash
pip3 install -U --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118
```

Or make sure your model is correctly casted in float16 or bfloat16

ãƒ¢ãƒ‡ãƒ«ãŒæ­£ã—ãfloat16ã¾ãŸã¯bfloat16ã«ã‚­ãƒ£ã‚¹ãƒˆã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

Have a look at [this detailed blogpost](https://pytorch.org/blog/out-of-the-box-acceleration/) to read more about what is possible to do with `BetterTransformer` + SDPA API.

`BetterTransformer` + SDPA APIã‚’ä½¿ç”¨ã—ã¦ä½•ãŒå¯èƒ½ã‹ã«ã¤ã„ã¦è©³ã—ãèª­ã‚€ã«ã¯ã€[ã“ã®è©³ç´°ãªãƒ–ãƒ­ã‚°ãƒã‚¹ãƒˆ](https://pytorch.org/blog/out-of-the-box-acceleration/)ã‚’ã”è¦§ãã ã•ã„ã€‚

## `bitsandbytes` integration for FP4 mixed-precision inference

FP4æ··åˆç²¾åº¦æ¨è«–ã®ãŸã‚ã®`bitsandbytes`çµ±åˆ

You can install `bitsandbytes` and benefit from easy model compression on GPUs. Using FP4 quantization you can expect to reduce up to 8x the model size compared to its native full precision version. Check out below how to get started.

`bitsandbytes`ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã€GPUã§ç°¡å˜ãªãƒ¢ãƒ‡ãƒ«ã®åœ§ç¸®ã‚’åˆ©ç”¨ã§ãã¾ã™ã€‚FP4é‡å­åŒ–ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€ãƒã‚¤ãƒ†ã‚£ãƒ–ã®ãƒ•ãƒ«ãƒ—ãƒ¬ã‚·ã‚¸ãƒ§ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨æ¯”è¼ƒã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’æœ€å¤§8å€å‰Šæ¸›ã§ãã‚‹ã“ã¨ãŒæœŸå¾…ã§ãã¾ã™ã€‚ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ã€ã©ã®ã‚ˆã†ã«å§‹ã‚ã‚‹ã‹ã‚’ã”è¦§ãã ã•ã„ã€‚

<Tip>

Note that this feature can also be used in a multi GPU setup.

ã“ã®æ©Ÿèƒ½ã¯ã€ãƒãƒ«ãƒGPUã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã§ã‚‚ä½¿ç”¨ã§ãã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚

</Tip>

### Requirements [[requirements-for-fp4-mixedprecision-inference]]

- Latest `bitsandbytes` library
`pip install bitsandbytes>=0.39.0`

- Install latest `accelerate` from source
`pip install git+https://github.com/huggingface/accelerate.git`

- Install latest `transformers` from source
`pip install git+https://github.com/huggingface/transformers.git`


### Running FP4 models - single GPU setup - Quickstart

ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ã§ã€ç°¡å˜ã«å˜ä¸€ã®GPUã§FP4ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã§ãã¾ã™:


```py
from transformers import AutoModelForCausalLM

model_name = "bigscience/bloom-2b5"
model_4bit = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_4bit=True)
```

æ³¨æ„: `device_map`ã¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã™ãŒã€æ¨è«–æ™‚ã« `device_map = 'auto'` ã‚’è¨­å®šã™ã‚‹ã“ã¨ãŒæ¨å¥¨ã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€åˆ©ç”¨å¯èƒ½ãªãƒªã‚½ãƒ¼ã‚¹ã«åŠ¹ç‡çš„ã«ãƒ¢ãƒ‡ãƒ«ãŒãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã•ã‚Œã¾ã™ã€‚

### Running FP4 models - multi GPU setup

æ··åˆ4ãƒ“ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã‚’è¤‡æ•°ã®GPUã«ãƒ­ãƒ¼ãƒ‰ã™ã‚‹æ–¹æ³•ã¯ã€å˜ä¸€GPUã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¨åŒã˜ã§ã™ï¼ˆå˜ä¸€GPUã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¨åŒã˜ã‚³ãƒãƒ³ãƒ‰ã§ã™ï¼‰ï¼š

```py
model_name = "bigscience/bloom-2b5"
model_4bit = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_4bit=True)
```

ã—ã‹ã—ã€`accelerate`ã‚’ä½¿ç”¨ã—ã¦ã€å„GPUã«å‰²ã‚Šå½“ã¦ã‚‹GPU RAMã‚’åˆ¶å¾¡ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ä»¥ä¸‹ã®ã‚ˆã†ã«ã€`max_memory`å¼•æ•°ã‚’ä½¿ç”¨ã—ã¾ã™ï¼š


```py
max_memory_mapping = {0: "600MB", 1: "1GB"}
model_name = "bigscience/bloom-3b"
model_4bit = AutoModelForCausalLM.from_pretrained(
    model_name, device_map="auto", load_in_4bit=True, max_memory=max_memory_mapping
)
```

ã“ã®ä¾‹ã§ã¯ã€æœ€åˆã®GPUã¯600MBã®ãƒ¡ãƒ¢ãƒªã‚’ä½¿ç”¨ã—ã€2ç•ªç›®ã®GPUã¯1GBã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

### Advanced usage

ã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã®ã•ã‚‰ãªã‚‹é«˜åº¦ãªä½¿ç”¨æ³•ã«ã¤ã„ã¦ã¯ã€[é‡å­åŒ–](main_classes/quantization)ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒšãƒ¼ã‚¸ã‚’ã”è¦§ãã ã•ã„ã€‚

## `bitsandbytes` integration for Int8 mixed-precision matrix decomposition

<Tip>

ã“ã®æ©Ÿèƒ½ã¯ã€ãƒãƒ«ãƒGPUç’°å¢ƒã§ã‚‚ä½¿ç”¨ã§ãã¾ã™ã€‚

</Tip>

è«–æ–‡[`LLM.int8()ï¼šã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªTransformerå‘ã‘ã®8ãƒ“ãƒƒãƒˆè¡Œåˆ—ä¹—ç®—`](https://arxiv.org/abs/2208.07339)ã«ã‚ˆã‚Œã°ã€Hugging Faceçµ±åˆãŒHubå†…ã®ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã§ã‚ãšã‹æ•°è¡Œã®ã‚³ãƒ¼ãƒ‰ã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¯ã€åŠç²¾åº¦ï¼ˆ`float16`ãŠã‚ˆã³`bfloat16`ï¼‰ã®é‡ã¿ã®å ´åˆã«`nn.Linear`ã‚µã‚¤ã‚ºã‚’2å€ã€å˜ç²¾åº¦ï¼ˆ`float32`ï¼‰ã®é‡ã¿ã®å ´åˆã¯4å€ã«ç¸®å°ã—ã€å¤–ã‚Œå€¤ã«å¯¾ã—ã¦ã»ã¨ã‚“ã©å½±éŸ¿ã‚’ä¸ãˆã¾ã›ã‚“ã€‚

![HFxbitsandbytes.png](https://cdn-uploads.huggingface.co/production/uploads/1659861207959-62441d1d9fdefb55a0b7d12c.png)

Int8æ··åˆç²¾åº¦è¡Œåˆ—åˆ†è§£ã¯ã€è¡Œåˆ—ä¹—ç®—ã‚’2ã¤ã®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã«åˆ†å‰²ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦å‹•ä½œã—ã¾ã™ï¼š(1) ã‚·ã‚¹ãƒ†ãƒãƒ†ã‚£ãƒƒã‚¯ãªç‰¹å¾´å¤–ã‚Œå€¤ã‚¹ãƒˆãƒªãƒ¼ãƒ ãŒfp16ã§è¡Œåˆ—ä¹—ç®—ï¼ˆ0.01%ï¼‰ã€(2) int8è¡Œåˆ—ä¹—ç®—ã®é€šå¸¸ã®ã‚¹ãƒˆãƒªãƒ¼ãƒ ï¼ˆ99.9%ï¼‰ã€‚ã“ã®æ–¹æ³•ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€éå¸¸ã«å¤§ããªãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦äºˆæ¸¬ã®åŠ£åŒ–ãªã—ã«int8æ¨è«–ãŒå¯èƒ½ã§ã™ã€‚
ã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€[è«–æ–‡](https://arxiv.org/abs/2208.07339)ã¾ãŸã¯[ã“ã®çµ±åˆã«é–¢ã™ã‚‹ãƒ–ãƒ­ã‚°è¨˜äº‹](https://huggingface.co/blog/hf-bitsandbytes-integration)ã‚’ã”ç¢ºèªãã ã•ã„ã€‚

![MixedInt8.gif](https://cdn-uploads.huggingface.co/production/uploads/1660567469965-62441d1d9fdefb55a0b7d12c.gif)

ãªãŠã€ã“ã®æ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯GPUãŒå¿…è¦ã§ã‚ã‚Šã€ã‚«ãƒ¼ãƒãƒ«ã¯GPUå°‚ç”¨ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã•ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã“ã®æ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹å‰ã«ã€ãƒ¢ãƒ‡ãƒ«ã®1/4ï¼ˆã¾ãŸã¯ãƒãƒ¼ãƒ•ç²¾åº¦ã®é‡ã¿ã®å ´åˆã¯1/2ï¼‰ã‚’ä¿å­˜ã™ã‚‹ã®ã«ååˆ†ãªGPUãƒ¡ãƒ¢ãƒªãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚
ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹éš›ã®ãƒ˜ãƒ«ãƒ—ã«é–¢ã™ã‚‹è©³ç´°ã¯ã€ä»¥ä¸‹ã®ãƒãƒ¼ãƒˆã‚’ã”è¦§ã„ãŸã ãã‹ã€[Google Colabã®ãƒ‡ãƒ¢](#colab-demos)ã‚’ã”è¦§ãã ã•ã„ã€‚

### Requirements [[requirements-for-int8-mixedprecision-matrix-decomposition]]

- `bitsandbytes<0.37.0`ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€NVIDIA GPUã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã€8ãƒ“ãƒƒãƒˆãƒ†ãƒ³ã‚½ãƒ«ã‚³ã‚¢ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼ˆTuringã€Ampereã€ã¾ãŸã¯ãã‚Œä»¥é™ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ¼ã€ä¾‹ï¼šT4ã€RTX20s RTX30sã€A40-A100ãªã©ï¼‰ã€‚`bitsandbytes>=0.37.0`ã®å ´åˆã€ã™ã¹ã¦ã®GPUãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã‚‹ã¯ãšã§ã™ã€‚
- æ­£ã—ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®`bitsandbytes`ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã«ã¯ã€æ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š
`pip install bitsandbytes>=0.31.5`
- `accelerate`ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ï¼š
`pip install accelerate>=0.12.0`


### Running mixed-Int8 models - single GPU setup

å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ãŸå¾Œã€ãƒŸãƒƒã‚¯ã‚¹ 8 ãƒ“ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€æ–¹æ³•ã¯æ¬¡ã®é€šã‚Šã§ã™ï¼š

```py
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

model_name = "bigscience/bloom-2b5"
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True))
```

ä»¥ä¸‹ã¯ã‚·ãƒ³ãƒ—ãƒ«ãªä¾‹ã§ã™ï¼š

* `pipeline()` é–¢æ•°ã®ä»£ã‚ã‚Šã«ã€ãƒ¢ãƒ‡ãƒ«ã® `generate()` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚`pipeline()` é–¢æ•°ã‚’ä½¿ç”¨ã—ã¦æ¨è«–ã™ã‚‹ã“ã¨ã¯å¯èƒ½ã§ã™ãŒã€æ··åˆ8ãƒ“ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã«æœ€é©åŒ–ã•ã‚Œã¦ãŠã‚‰ãšã€`generate()` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã‚Šã‚‚é…ããªã‚Šã¾ã™ã€‚ã¾ãŸã€ä¸€éƒ¨ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ï¼ˆä¾‹ï¼šãƒŒã‚¯ãƒ¬ã‚¦ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰ã¯ã€`pipeline()` é–¢æ•°ã§ã¯æ··åˆ8ãƒ“ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã§ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚
* ã™ã¹ã¦ã®å…¥åŠ›ã‚’ãƒ¢ãƒ‡ãƒ«ã¨åŒã˜ãƒ‡ãƒã‚¤ã‚¹ã«é…ç½®ã—ã¦ãã ã•ã„ã€‚


```py
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_name = "bigscience/bloom-2b5"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True))

prompt = "Hello, my llama is cute"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
generated_ids = model.generate(**inputs)
outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
```

### Running mixed-int8 models - multi GPU setup

è¤‡æ•°ã®GPUã«æ··åˆ8ãƒ“ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹æ–¹æ³•ã¯ã€æ¬¡ã®é€šã‚Šã§ã™ï¼ˆã‚·ãƒ³ã‚°ãƒ«GPUã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¨åŒã˜ã‚³ãƒãƒ³ãƒ‰ã§ã™ï¼‰ï¼š

```py
model_name = "bigscience/bloom-2b5"
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True))
```

`accelerate`ã‚’ä½¿ç”¨ã—ã¦å„GPUã«å‰²ã‚Šå½“ã¦ã‚‹GPU RAMã‚’åˆ¶å¾¡ã™ã‚‹éš›ã«ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«`max_memory`å¼•æ•°ã‚’ä½¿ç”¨ã—ã¾ã™ï¼š


```py
max_memory_mapping = {0: "1GB", 1: "2GB"}
model_name = "bigscience/bloom-3b"
model_8bit = AutoModelForCausalLM.from_pretrained(
    model_name, device_map="auto", load_in_8bit=True, max_memory=max_memory_mapping
)
```

In this example, the first GPU will use 1GB of memory and the second 2GB.

### Colab demos

ã“ã®æ–¹æ³•ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€ä»¥å‰ã®Google Colabã§ã¯æ¨è«–ã§ããªã‹ã£ãŸãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦æ¨è«–ã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚ä»¥ä¸‹ã¯ã€Google Colabã§8ãƒ“ãƒƒãƒˆé‡å­åŒ–ã‚’ä½¿ç”¨ã—ã¦T5-11bï¼ˆfp32ã§42GBï¼‰ã‚’å®Ÿè¡Œã™ã‚‹ãƒ‡ãƒ¢ã®ãƒªãƒ³ã‚¯ã§ã™ï¼š

[![Open In Colab: T5-11b demo](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1YORPWx4okIHXnjW7MSAidXN29mPVNT7F?usp=sharing)

ã¾ãŸã€BLOOM-3Bã®ãƒ‡ãƒ¢ã‚‚ã”è¦§ã„ãŸã ã‘ã¾ã™ï¼š

[![Open In Colab: BLOOM-3b demo](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1qOjXfQIAULfKvZqwCen8-MoWKGdSatZ4?usp=sharing)

## Advanced usage: mixing FP4 (or Int8) and BetterTransformer

ç•°ãªã‚‹æ–¹æ³•ã‚’çµ„ã¿åˆã‚ã›ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®æœ€é©ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ä¾‹ãˆã°ã€BetterTransformerã‚’ä½¿ç”¨ã—ã¦FP4ãƒŸãƒƒã‚¯ã‚¹ãƒ—ãƒ¬ã‚·ã‚¸ãƒ§ãƒ³æ¨è«–ã¨ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚


```py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)

tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")
model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", quantization_config=quantization_config)

input_text = "Hello my dog is cute and"
inputs = tokenizer(input_text, return_tensors="pt").to("cuda")

with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):
    outputs = model.generate(**inputs)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```