<!--
Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯Markdownå½¢å¼ã§ã™ãŒã€ç‰¹å®šã®MDXã«é¡ä¼¼ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ“ãƒ«ãƒ€ãƒ¼ã®æ§‹æ–‡ã‚’å«ã‚“ã§ãŠã‚Šã€
Markdownãƒ“ãƒ¥ãƒ¼ã‚¢ãƒ¼ã§æ­£ã—ãè¡¨ç¤ºã•ã‚Œãªã„ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚

-->

# Preprocess

[[open-in-colab]]

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹å‰ã«ã€ãã‚Œã‚’ãƒ¢ãƒ‡ãƒ«ã®æœŸå¾…ã™ã‚‹å…¥åŠ›å½¢å¼ã«å‰å‡¦ç†ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
ãƒ‡ãƒ¼ã‚¿ãŒãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€ã¾ãŸã¯ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã§ã‚ã‚‹ã‹ã©ã†ã‹ã«ã‹ã‹ã‚ã‚‰ãšã€ãã‚Œã‚‰ã¯ãƒ†ãƒ³ã‚½ãƒ«ã®ãƒãƒƒãƒã«å¤‰æ›ã—ã¦çµ„ã¿ç«‹ã¦ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
ğŸ¤— Transformersã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¢ãƒ‡ãƒ«ç”¨ã«æº–å‚™ã™ã‚‹ã®ã«å½¹ç«‹ã¤å‰å‡¦ç†ã‚¯ãƒ©ã‚¹ã®ã‚»ãƒƒãƒˆã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚
ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€æ¬¡ã®ã“ã¨ã‚’å­¦ã³ã¾ã™ï¼š

* ãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆã€[Tokenizer](./main_classes/tokenizer)ã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¤‰æ›ã—ã€ãƒˆãƒ¼ã‚¯ãƒ³ã®æ•°å€¤è¡¨ç¾ã‚’ä½œæˆã—ã€ãã‚Œã‚‰ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«çµ„ã¿ç«‹ã¦ã‚‹æ–¹æ³•ã€‚
* éŸ³å£°ã¨ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã®å ´åˆã€[Feature extractor](./main_classes/feature_extractor)ã‚’ä½¿ç”¨ã—ã¦ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªæ³¢å½¢ã‹ã‚‰é€£ç¶šçš„ãªç‰¹å¾´ã‚’æŠ½å‡ºã—ã€ãã‚Œã‚‰ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã™ã‚‹æ–¹æ³•ã€‚
* ç”»åƒå…¥åŠ›ã®å ´åˆã€[ImageProcessor](./main_classes/image)ã‚’ä½¿ç”¨ã—ã¦ç”»åƒã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã™ã‚‹æ–¹æ³•ã€‚
* ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›ã®å ´åˆã€[Processor](./main_classes/processors)ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã¨ç‰¹å¾´æŠ½å‡ºå™¨ã¾ãŸã¯ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’çµ„ã¿åˆã‚ã›ã‚‹æ–¹æ³•ã€‚

> [!TIP]
> `AutoProcessor`ã¯å¸¸ã«å‹•ä½œã—ã€ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã«é©åˆ‡ãªã‚¯ãƒ©ã‚¹ã‚’è‡ªå‹•çš„ã«é¸æŠã—ã¾ã™ã€‚
> ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã€ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã€ç‰¹å¾´æŠ½å‡ºå™¨ã€ã¾ãŸã¯ãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã‹ã«ã‹ã‹ã‚ã‚‰ãšã€å‹•ä½œã—ã¾ã™ã€‚

å§‹ã‚ã‚‹å‰ã«ã€ğŸ¤— Datasetsã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ã€ã„ãã¤ã‹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è©¦ã™ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ï¼š

```bash
pip install datasets
```

## Natural Language Processing

<Youtube id="Yffk5aydLzg"/>

ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã«ä½¿ç”¨ã™ã‚‹ä¸»è¦ãªãƒ„ãƒ¼ãƒ«ã¯ã€[ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶](main_classes/tokenizer)ã§ã™ã€‚ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã¯ã€ä¸€é€£ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’*ãƒˆãƒ¼ã‚¯ãƒ³*ã«åˆ†å‰²ã—ã¾ã™ã€‚ãƒˆãƒ¼ã‚¯ãƒ³ã¯æ•°å€¤ã«å¤‰æ›ã•ã‚Œã€ãã®å¾Œãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã•ã‚Œã€ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ã¨ãªã‚Šã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦ã¨ã™ã‚‹è¿½åŠ ã®å…¥åŠ›ã¯ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã«ã‚ˆã£ã¦è¿½åŠ ã•ã‚Œã¾ã™ã€‚

> [!TIP]
> äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹äºˆå®šã®å ´åˆã€é–¢é€£ã™ã‚‹äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ†ã‚­ã‚¹ãƒˆãŒäº‹å‰å­¦ç¿’ã‚³ãƒ¼ãƒ‘ã‚¹ã¨åŒã˜æ–¹æ³•ã§åˆ†å‰²ã•ã‚Œã€äº‹å‰å­¦ç¿’ä¸­ã«é€šå¸¸*ãƒœã‚­ãƒ£ãƒ–*ã¨ã—ã¦å‚ç…§ã•ã‚Œã‚‹å¯¾å¿œã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

[`AutoTokenizer.from_pretrained`]ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã€é–‹å§‹ã—ã¾ã—ã‚‡ã†ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ãŒäº‹å‰å­¦ç¿’ã•ã‚ŒãŸ*ãƒœã‚­ãƒ£ãƒ–*ãŒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã™ï¼š

```python
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")
```

æ¬¡ã«ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã«æ¸¡ã—ã¾ã™ï¼š

```py
>>> encoded_input = tokenizer("Do not meddle in the affairs of wizards, for they are subtle and quick to anger.")
>>> print(encoded_input)
{'input_ids': [101, 2079, 2025, 19960, 10362, 1999, 1996, 3821, 1997, 16657, 1010, 2005, 2027, 2024, 11259, 1998, 4248, 2000, 4963, 1012, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã¯ã€é‡è¦ãª3ã¤ã®é …ç›®ã‚’æŒã¤è¾æ›¸ã‚’è¿”ã—ã¾ã™ï¼š

* [input_ids](glossary#input-ids) ã¯æ–‡ä¸­ã®å„ãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾å¿œã™ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§ã™ã€‚
* [attention_mask](glossary#attention-mask) ã¯ãƒˆãƒ¼ã‚¯ãƒ³ãŒã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å—ã‘ã‚‹å¿…è¦ãŒã‚ã‚‹ã‹ã©ã†ã‹ã‚’ç¤ºã—ã¾ã™ã€‚
* [token_type_ids](glossary#token-type-ids) ã¯è¤‡æ•°ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãŒã‚ã‚‹å ´åˆã€ãƒˆãƒ¼ã‚¯ãƒ³ãŒã©ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å±ã—ã¦ã„ã‚‹ã‹ã‚’è­˜åˆ¥ã—ã¾ã™ã€‚

`input_ids` ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦å…¥åŠ›ã‚’è¿”ã—ã¾ã™ï¼š

```python
>>> tokenizer.decode(encoded_input["input_ids"])
'[CLS] é­”æ³•ä½¿ã„ã®äº‹ã«å¹²æ¸‰ã™ã‚‹ãªã€å½¼ã‚‰ã¯å¾®å¦™ã§æ€’ã‚Šã£ã½ã„ã€‚ [SEP]'
```

å¦‚ä½•ã«ãŠåˆ†ã‹ã‚Šã„ãŸã ã‘ã‚‹ã‹ã¨æ€ã„ã¾ã™ãŒã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã¯ã“ã®æ–‡ç« ã«2ã¤ã®ç‰¹åˆ¥ãªãƒˆãƒ¼ã‚¯ãƒ³ã€`CLS`ï¼ˆã‚¯ãƒ©ã‚·ãƒ•ã‚¡ã‚¤ã‚¢ï¼‰ã¨`SEP`ï¼ˆã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿ï¼‰ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚
ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ãŒç‰¹åˆ¥ãªãƒˆãƒ¼ã‚¯ãƒ³ã‚’å¿…è¦ã¨ã™ã‚‹ã‚ã‘ã§ã¯ã‚ã‚Šã¾ã›ã‚“ãŒã€å¿…è¦ãªå ´åˆã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã¯è‡ªå‹•çš„ã«ãã‚Œã‚‰ã‚’è¿½åŠ ã—ã¾ã™ã€‚

è¤‡æ•°ã®æ–‡ç« ã‚’å‰å‡¦ç†ã™ã‚‹å ´åˆã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã«ãƒªã‚¹ãƒˆã¨ã—ã¦æ¸¡ã—ã¦ãã ã•ã„ï¼š

```py
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_inputs = tokenizer(batch_sentences)
>>> print(encoded_inputs)
{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102],
               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],
               [101, 1327, 1164, 5450, 23434, 136, 102]],
 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0]],
 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1],
                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                    [1, 1, 1, 1, 1, 1, 1]]}
```

### Pad

æ–‡ç« ã¯å¸¸ã«åŒã˜é•·ã•ã§ã¯ãªã„ã“ã¨ãŒã‚ã‚Šã€ã“ã‚Œã¯ãƒ†ãƒ³ã‚½ãƒ«ï¼ˆãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ï¼‰ãŒå‡ä¸€ãªå½¢çŠ¶ã‚’æŒã¤å¿…è¦ãŒã‚ã‚‹ãŸã‚å•é¡Œã¨ãªã‚Šã¾ã™ã€‚
ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã¯ã€çŸ­ã„æ–‡ã«ç‰¹åˆ¥ãªã€Œãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã€ã‚’è¿½åŠ ã—ã¦ã€ãƒ†ãƒ³ã‚½ãƒ«ã‚’é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«åˆã‚ã›ã‚‹ãŸã‚ã®æˆ¦ç•¥ã§ã™ã€‚

ãƒãƒƒãƒå†…ã®çŸ­ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’æœ€é•·ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«åˆã‚ã›ã‚‹ãŸã‚ã«ã€`padding`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’`True`ã«è¨­å®šã—ã¾ã™ï¼š

```py
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_input = tokenizer(batch_sentences, padding=True)
>>> print(encoded_input)
{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],
               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],
               [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],
 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                    [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}
```

1ç•ªç›®ã¨3ç•ªç›®ã®æ–‡ã¯ã€çŸ­ã„ãŸã‚ã«`0`ã§ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã•ã‚Œã¦ã„ã¾ã™ã€‚

### Truncation

é€†ã®ã‚¹ãƒšã‚¯ãƒˆãƒ«ã§ã¯ã€æ™‚æŠ˜ã€ãƒ¢ãƒ‡ãƒ«ãŒå‡¦ç†ã™ã‚‹ã®ã«é•·ã™ãã‚‹ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ã“ã®å ´åˆã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’çŸ­ç¸®ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

ãƒ¢ãƒ‡ãƒ«ãŒå—ã‘å…¥ã‚Œã‚‹æœ€å¤§ã®é•·ã•ã«ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’åˆ‡ã‚Šè©°ã‚ã‚‹ã«ã¯ã€`truncation`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’`True`ã«è¨­å®šã—ã¾ã™ï¼š

```py
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True)
>>> print(encoded_input)
{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],
               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],
               [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],
 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                    [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}
```

> [!TIP]
> ç•°ãªã‚‹ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã¨åˆ‡ã‚Šè©°ã‚ã®å¼•æ•°ã«ã¤ã„ã¦è©³ã—ãã¯ã€[ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã¨åˆ‡ã‚Šè©°ã‚](./pad_truncation)ã®ã‚³ãƒ³ã‚»ãƒ—ãƒˆã‚¬ã‚¤ãƒ‰ã‚’ã”è¦§ãã ã•ã„ã€‚

### Build tensors

æœ€å¾Œã«ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãŒãƒ¢ãƒ‡ãƒ«ã«ä¾›çµ¦ã•ã‚Œã‚‹å®Ÿéš›ã®ãƒ†ãƒ³ã‚½ãƒ«ã‚’è¿”ã™ã‚ˆã†ã«è¨­å®šã—ã¾ã™ã€‚

`return_tensors`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’`pt`ï¼ˆPyTorchç”¨ï¼‰ã¾ãŸã¯`tf`ï¼ˆTensorFlowç”¨ï¼‰ã«è¨­å®šã—ã¾ã™ï¼š


```py
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors="pt")
>>> print(encoded_input)
{'input_ids': tensor([[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],
                      [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],
                      [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]]),
 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),
 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
                           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                           [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}
```

## Audio

ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚¿ã‚¹ã‚¯ã®å ´åˆã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ¢ãƒ‡ãƒ«ç”¨ã«æº–å‚™ã™ã‚‹ãŸã‚ã«[ç‰¹å¾´æŠ½å‡ºå™¨](main_classes/feature_extractor)ãŒå¿…è¦ã§ã™ã€‚
ç‰¹å¾´æŠ½å‡ºå™¨ã¯ç”Ÿã®ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´ã‚’æŠ½å‡ºã—ã€ãã‚Œã‚‰ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã™ã‚‹ãŸã‚ã«è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚

[PolyAI/minds14](https://huggingface.co/datasets/PolyAI/minds14)ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ãƒ¼ãƒ‰æ–¹æ³•ã®è©³ç´°ã«ã¤ã„ã¦ã¯ğŸ¤— [Datasetsãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](https://huggingface.co/docs/datasets/load_hub)ã‚’å‚ç…§ï¼‰ã€
ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ç‰¹å¾´æŠ½å‡ºå™¨ã‚’ã©ã®ã‚ˆã†ã«ä½¿ç”¨ã§ãã‚‹ã‹ã‚’ç¢ºèªã—ã¦ã¿ã¾ã—ã‚‡ã†ï¼š

```python
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")
```

ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦`audio`åˆ—ã®æœ€åˆã®è¦ç´ ã‚’ç¢ºèªã—ã¾ã™ã€‚`audio`åˆ—ã‚’å‘¼ã³å‡ºã™ã¨ã€è‡ªå‹•çš„ã«ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ã‚¡ã‚¤ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã€ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚Œã¾ã™ï¼š

```py
>>> dataset[0]["audio"]
{'array': array([ 0.        ,  0.00024414, -0.00024414, ..., -0.00024414,
         0.        ,  0.        ], dtype=float32),
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',
 'sampling_rate': 8000}
```

ã“ã‚Œã«ã‚ˆã‚Šã€3ã¤ã®ã‚¢ã‚¤ãƒ†ãƒ ãŒè¿”ã•ã‚Œã¾ã™ï¼š

* `array` ã¯èª­ã¿è¾¼ã¾ã‚ŒãŸéŸ³å£°ä¿¡å·ã§ã€1Dã®é…åˆ—ã¨ã—ã¦èª­ã¿è¾¼ã¾ã‚Œã¾ã™ã€‚å¿…è¦ã«å¿œã˜ã¦ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚Œã‚‹ã“ã¨ã‚‚ã‚ã‚Šã¾ã™ã€‚
* `path` ã¯éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´æ‰€ã‚’æŒ‡ã—ã¾ã™ã€‚
* `sampling_rate` ã¯éŸ³å£°ä¿¡å·å†…ã®ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆãŒ1ç§’é–“ã«ã„ãã¤æ¸¬å®šã•ã‚Œã‚‹ã‹ã‚’ç¤ºã—ã¾ã™ã€‚

ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€[Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base)ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰ã‚’ç¢ºèªã™ã‚‹ã¨ã€Wav2Vec2ãŒ16kHzã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸéŸ³å£°ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã§äº‹å‰å­¦ç¿’ã•ã‚Œã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚
ãƒ¢ãƒ‡ãƒ«ã®äº‹å‰å­¦ç¿’ã«ä½¿ç”¨ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã¨ã€ã‚ãªãŸã®ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆãŒä¸€è‡´ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚
ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆãŒç•°ãªã‚‹å ´åˆã€ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

1. ğŸ¤— Datasetsã® [`~datasets.Dataset.cast_column`] ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã‚’16kHzã«ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¾ã™ï¼š

```py
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=16_000))
```

2. å†ã³ `audio` åˆ—ã‚’å‘¼ã³å‡ºã—ã¦ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒªã‚µãƒ³ãƒ—ãƒ«ã—ã¾ã™ï¼š

```py
>>> dataset[0]["audio"]
{'array': array([ 2.3443763e-05,  2.1729663e-04,  2.2145823e-04, ...,
         3.8356509e-05, -7.3497440e-06, -2.1754686e-05], dtype=float32),
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',
 'sampling_rate': 16000}
```

æ¬¡ã«ã€å…¥åŠ›ã‚’æ­£è¦åŒ–ã—ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹ãŸã‚ã«ç‰¹å¾´æŠ½å‡ºå™¨ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹å ´åˆã€çŸ­ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«ã¯ `0` ãŒè¿½åŠ ã•ã‚Œã¾ã™ã€‚åŒã˜è€ƒãˆæ–¹ãŒã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ‡ãƒ¼ã‚¿ã«ã‚‚é©ç”¨ã•ã‚Œã¾ã™ã€‚ç‰¹å¾´æŠ½å‡ºå™¨ã¯ `array` ã« `0` ã‚’è¿½åŠ ã—ã¾ã™ï¼ˆã“ã‚Œã¯ç„¡éŸ³ã¨ã—ã¦è§£é‡ˆã•ã‚Œã¾ã™ï¼‰ã€‚

[`AutoFeatureExtractor.from_pretrained`]ã‚’ä½¿ç”¨ã—ã¦ç‰¹å¾´æŠ½å‡ºå™¨ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ï¼š

```python
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base")
```

ã‚ªãƒ¼ãƒ‡ã‚£ã‚ª `array` ã‚’ç‰¹å¾´æŠ½å‡ºå™¨ã«æ¸¡ã—ã¾ã™ã€‚ç‰¹å¾´æŠ½å‡ºå™¨ã§ç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ç„¡éŸ³ã‚¨ãƒ©ãƒ¼ã‚’ã‚ˆã‚Šè‰¯ããƒ‡ãƒãƒƒã‚°ã™ã‚‹ãŸã‚ã«ã€ç‰¹å¾´æŠ½å‡ºå™¨ã« `sampling_rate` å¼•æ•°ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚

```python
>>> audio_input = [dataset[0]["audio"]["array"]]
>>> feature_extractor(audio_input, sampling_rate=16000)
{'input_values': [array([ 3.8106556e-04,  2.7506407e-03,  2.8015103e-03, ...,
        5.6335266e-04,  4.6588284e-06, -1.7142107e-04], dtype=float32)]}
```

åŒæ§˜ã«ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã¨åŒæ§˜ã«ã€ãƒãƒƒãƒå†…ã®å¯å¤‰ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å‡¦ç†ã™ã‚‹ãŸã‚ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã¾ãŸã¯åˆ‡ã‚Šè©°ã‚ã‚’é©ç”¨ã§ãã¾ã™ã€‚æ¬¡ã«ã€ã“ã‚Œã‚‰ã®2ã¤ã®ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚µãƒ³ãƒ—ãƒ«ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’ç¢ºèªã—ã¦ã¿ã¾ã—ã‚‡ã†ï¼š

```python
>>> dataset[0]["audio"]["array"].shape
(173398,)

>>> dataset[1]["audio"]["array"].shape
(106496,)
```

ã“ã®é–¢æ•°ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å‰å‡¦ç†ã—ã¦ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚µãƒ³ãƒ—ãƒ«ã®é•·ã•ã‚’åŒã˜ã«ã™ã‚‹ãŸã‚ã®ã‚‚ã®ã§ã™ã€‚æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«é•·ã‚’æŒ‡å®šã—ã€ç‰¹å¾´æŠ½å‡ºå™¨ã¯ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ãã‚Œã«åˆã‚ã›ã¦ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã¾ãŸã¯åˆ‡ã‚Šè©°ã‚ã¾ã™ã€‚

```py
>>> def preprocess_function(examples):
...     audio_arrays = [x["array"] for x in examples["audio"]]
...     inputs = feature_extractor(
...         audio_arrays,
...         sampling_rate=16000,
...         padding=True,
...         max_length=100000,
...         truncation=True,
...     )
...     return inputs
```

`preprocess_function`ã‚’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æœ€åˆã®æ•°ä¾‹ã«é©ç”¨ã—ã¾ã™ï¼š

```python
>>> processed_dataset = preprocess_function(dataset[:5])
```

ã‚µãƒ³ãƒ—ãƒ«ã®é•·ã•ã¯ç¾åœ¨åŒã˜ã§ã€æŒ‡å®šã•ã‚ŒãŸæœ€å¤§é•·ã¨ä¸€è‡´ã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã§å‡¦ç†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã™ã“ã¨ãŒã§ãã¾ã™ï¼

```py
>>> processed_dataset["input_values"][0].shape
(100000,)

>>> processed_dataset["input_values"][1].shape
(100000,)
```

## Computer Vision

ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™ã™ã‚‹ãŸã‚ã®[ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µ](main_classes/image_processor)ãŒå¿…è¦ã§ã™ã€‚
ç”»åƒã®å‰å‡¦ç†ã«ã¯ã€ç”»åƒã‚’ãƒ¢ãƒ‡ãƒ«ãŒæœŸå¾…ã™ã‚‹å…¥åŠ›å½¢å¼ã«å¤‰æ›ã™ã‚‹ãŸã‚ã®ã„ãã¤ã‹ã®ã‚¹ãƒ†ãƒƒãƒ—ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚Œã‚‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã«ã¯ã€ãƒªã‚µã‚¤ã‚ºã€æ­£è¦åŒ–ã€ã‚«ãƒ©ãƒ¼ãƒãƒ£ãƒãƒ«ã®è£œæ­£ã€ãŠã‚ˆã³ç”»åƒã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã™ã‚‹ãªã©ãŒå«ã¾ã‚Œã¾ã™ã€‚

> [!TIP]
> ç”»åƒã®å‰å‡¦ç†ã¯ã€é€šå¸¸ã€ç”»åƒã®å¢—å¼·ã®å½¢å¼ã«å¾“ã„ã¾ã™ã€‚ç”»åƒã®å‰å‡¦ç†ã¨ç”»åƒã®å¢—å¼·ã®ä¸¡æ–¹ã¯ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›ã—ã¾ã™ãŒã€ç•°ãªã‚‹ç›®çš„ãŒã‚ã‚Šã¾ã™ï¼š
>
> * ç”»åƒã®å¢—å¼·ã¯ã€éå­¦ç¿’ã‚’é˜²ãã€ãƒ¢ãƒ‡ãƒ«ã®å …ç‰¢æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ã®ã«å½¹ç«‹ã¤æ–¹æ³•ã§ç”»åƒã‚’å¤‰æ›´ã—ã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚’å¢—å¼·ã™ã‚‹æ–¹æ³•ã¯ç„¡é™ã§ã€æ˜ã‚‹ã•ã‚„è‰²ã®èª¿æ•´ã€ã‚¯ãƒ­ãƒƒãƒ—ã€å›è»¢ã€ãƒªã‚µã‚¤ã‚ºã€ã‚ºãƒ¼ãƒ ãªã©ã€æ§˜ã€…ãªæ–¹æ³•ãŒã‚ã‚Šã¾ã™ã€‚ãŸã ã—ã€å¢—å¼·æ“ä½œã«ã‚ˆã£ã¦ç”»åƒã®æ„å‘³ãŒå¤‰ã‚ã‚‰ãªã„ã‚ˆã†ã«æ³¨æ„ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
> * ç”»åƒã®å‰å‡¦ç†ã¯ã€ç”»åƒãŒãƒ¢ãƒ‡ãƒ«ã®æœŸå¾…ã™ã‚‹å…¥åŠ›å½¢å¼ã¨ä¸€è‡´ã™ã‚‹ã“ã¨ã‚’ä¿è¨¼ã—ã¾ã™ã€‚ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹å ´åˆã€ç”»åƒã¯ãƒ¢ãƒ‡ãƒ«ãŒæœ€åˆã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸã¨ãã¨ã¾ã£ãŸãåŒã˜æ–¹æ³•ã§å‰å‡¦ç†ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
>
> ç”»åƒã®å¢—å¼·ã«ã¯ä»»æ„ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚ç”»åƒã®å‰å‡¦ç†ã«ã¯ã€ãƒ¢ãƒ‡ãƒ«ã«é–¢é€£ä»˜ã‘ã‚‰ã‚ŒãŸ`ImageProcessor`ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’ä½¿ç”¨ã™ã‚‹æ–¹æ³•ã‚’ç¤ºã™ãŸã‚ã«ã€[food101](https://huggingface.co/datasets/food101)ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ãƒ¼ãƒ‰æ–¹æ³•ã®è©³ç´°ã«ã¤ã„ã¦ã¯ğŸ¤—[Datasetsãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](https://huggingface.co/docs/datasets/load_hub)ã‚’å‚ç…§ï¼‰ï¼š

> [!TIP]
> ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒã‹ãªã‚Šå¤§ãã„ãŸã‚ã€ğŸ¤— Datasetsã®`split`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®å°ã•ãªã‚µãƒ³ãƒ—ãƒ«ã®ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ï¼

```python
>>> from datasets import load_dataset

>>> dataset = load_dataset("food101", split="train[:100]")
```

æ¬¡ã«ã€ğŸ¤— Datasetsã® [`Image`](https://huggingface.co/docs/datasets/package_reference/main_classes?highlight=image#datasets.Image) æ©Ÿèƒ½ã§ç”»åƒã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ï¼š

```python
>>> dataset[0]["image"]
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/vision-preprocess-tutorial.png"/>
</div>

AutoImageProcessorã‚’[`AutoImageProcessor.from_pretrained`]ã‚’ä½¿ç”¨ã—ã¦ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ï¼š

```py
>>> from transformers import AutoImageProcessor

>>> image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
```

1. ã¾ãšã€ç”»åƒã®æ‹¡å¼µã‚’è¿½åŠ ã—ã¾ã—ã‚‡ã†ã€‚å¥½ããªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã§ãã¾ã™ãŒã€ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯torchvisionã®[`transforms`](https://pytorch.org/vision/stable/transforms.html)ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚åˆ¥ã®ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ãŸã„å ´åˆã¯ã€[Albumentations](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb)ã¾ãŸã¯[Kornia notebooks](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_kornia.ipynb)ã§è©³ç´°ã‚’å­¦ã¶ã“ã¨ãŒã§ãã¾ã™ã€‚

   ã“ã“ã§ã¯ã€[`Compose`](https://pytorch.org/vision/master/generated/torchvision.transforms.Compose.html)ã‚’ä½¿ç”¨ã—ã¦ã„ãã¤ã‹ã®å¤‰æ›ã‚’é€£é–ã•ã›ã¾ã™ - [`RandomResizedCrop`](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html)ã¨[`ColorJitter`](https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html)ã€‚
   ã‚µã‚¤ã‚ºã®å¤‰æ›´ã«é–¢ã—ã¦ã¯ã€`image_processor`ã‹ã‚‰ç”»åƒã‚µã‚¤ã‚ºã®è¦ä»¶ã‚’å–å¾—ã§ãã¾ã™ã€‚
   ä¸€éƒ¨ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€æ­£ç¢ºãªé«˜ã•ã¨å¹…ãŒå¿…è¦ã§ã™ãŒã€ä»–ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯`shortest_edge`ã®ã¿ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã™ã€‚

```py
>>> from torchvision.transforms import RandomResizedCrop, ColorJitter, Compose

>>> size = (
...     image_processor.size["shortest_edge"]
...     if "shortest_edge" in image_processor.size
...     else (image_processor.size["height"], image_processor.size["width"])
... )

>>> _transforms = Compose([RandomResizedCrop(size), ColorJitter(brightness=0.5, hue=0.5)])
```

2. ãƒ¢ãƒ‡ãƒ«ã¯[`pixel_values`](model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderModel.forward.pixel_values)ã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å–ã‚Šã¾ã™ã€‚
`ImageProcessor`ã¯ç”»åƒã®æ­£è¦åŒ–ã¨é©åˆ‡ãªãƒ†ãƒ³ã‚½ãƒ«ã®ç”Ÿæˆã‚’å‡¦ç†ã§ãã¾ã™ã€‚
ä¸€é€£ã®ç”»åƒã«å¯¾ã™ã‚‹ç”»åƒæ‹¡å¼µã¨ç”»åƒå‰å‡¦ç†ã‚’çµ„ã¿åˆã‚ã›ã€`pixel_values`ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°ã‚’ä½œæˆã—ã¾ã™ï¼š

```python
>>> def transforms(examples):
...     images = [_transforms(img.convert("RGB")) for img in examples["image"]]
...     examples["pixel_values"] = image_processor(images, do_resize=False, return_tensors="pt")["pixel_values"]
...     return examples
```

> [!TIP]
> ä¸Šè¨˜ã®ä¾‹ã§ã¯ã€ç”»åƒã®ã‚µã‚¤ã‚ºå¤‰æ›´ã‚’æ—¢ã«ç”»åƒå¢—å¼·å¤‰æ›ã§è¡Œã£ã¦ã„ã‚‹ãŸã‚ã€`do_resize=False`ã‚’è¨­å®šã—ã¾ã—ãŸã€‚
> é©åˆ‡ãª `image_processor` ã‹ã‚‰ã® `size` å±æ€§ã‚’æ´»ç”¨ã—ã¦ã„ã¾ã™ã€‚ç”»åƒå¢—å¼·ä¸­ã«ç”»åƒã®ã‚µã‚¤ã‚ºå¤‰æ›´ã‚’è¡Œã‚ãªã„å ´åˆã¯ã€ã“ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’çœç•¥ã—ã¦ãã ã•ã„ã€‚
> ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€`ImageProcessor` ãŒã‚µã‚¤ã‚ºå¤‰æ›´ã‚’å‡¦ç†ã—ã¾ã™ã€‚
>
> ç”»åƒã‚’å¢—å¼·å¤‰æ›ã®ä¸€éƒ¨ã¨ã—ã¦æ­£è¦åŒ–ã—ãŸã„å ´åˆã¯ã€`image_processor.image_mean` ã¨ `image_processor.image_std` ã®å€¤ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

3. æ¬¡ã«ã€ğŸ¤— Datasetsã®[`set_transform`](https://huggingface.co/docs/datasets/process#format-transform)ã‚’ä½¿ç”¨ã—ã¦ã€å¤‰æ›ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§é©ç”¨ã—ã¾ã™ï¼š

```python
>>> dataset.set_transform(transforms)
```

4. ç”»åƒã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã¨ã€ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µãŒ `pixel_values` ã‚’è¿½åŠ ã—ãŸã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ã“ã‚Œã§å‡¦ç†æ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã™ã“ã¨ãŒã§ãã¾ã™ï¼

```python
>>> dataset[0].keys()
```

ä»¥ä¸‹ã¯ã€å¤‰æ›ãŒé©ç”¨ã•ã‚ŒãŸå¾Œã®ç”»åƒã®å¤–è¦³ã§ã™ã€‚ ç”»åƒã¯ãƒ©ãƒ³ãƒ€ãƒ ã«åˆ‡ã‚ŠæŠœã‹ã‚Œã€ãã®è‰²ã®ç‰¹æ€§ã‚‚ç•°ãªã‚Šã¾ã™ã€‚

```py
>>> import numpy as np
>>> import matplotlib.pyplot as plt

>>> img = dataset[0]["pixel_values"]
>>> plt.imshow(img.permute(1, 2, 0))
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/preprocessed_image.png"/>
</div>

> [!TIP]
> ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ¤œå‡ºã€æ„å‘³ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã€ãŠã‚ˆã³ãƒ‘ãƒãƒ—ãƒ†ã‚£ãƒƒã‚¯ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãªã©ã®ã‚¿ã‚¹ã‚¯ã®å ´åˆã€`ImageProcessor`ã¯
> ãƒã‚¹ãƒˆå‡¦ç†ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æä¾›ã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ç”Ÿã®å‡ºåŠ›ã‚’å¢ƒç•Œãƒœãƒƒã‚¯ã‚¹ã‚„ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒãƒƒãƒ—ãªã©ã®æ„å‘³ã®ã‚ã‚‹äºˆæ¸¬ã«å¤‰æ›ã—ã¾ã™ã€‚

### Pad

ä¸€éƒ¨ã®å ´åˆã€ãŸã¨ãˆã°ã€[DETR](./model_doc/detr)ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹å ´åˆã€ãƒ¢ãƒ‡ãƒ«ã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚ã«ã‚¹ã‚±ãƒ¼ãƒ«ã®å¤‰æ›´ã‚’é©ç”¨ã—ã¾ã™ã€‚
ã“ã‚Œã«ã‚ˆã‚Šã€ãƒãƒƒãƒå†…ã®ç”»åƒã®ã‚µã‚¤ã‚ºãŒç•°ãªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚[`DetrImageProcessor`]ã‹ã‚‰[`DetrImageProcessor.pad`]ã‚’ä½¿ç”¨ã—ã€
ã‚«ã‚¹ã‚¿ãƒ ã®`collate_fn`ã‚’å®šç¾©ã—ã¦ç”»åƒã‚’ä¸€ç·’ã«ãƒãƒƒãƒå‡¦ç†ã§ãã¾ã™ã€‚

```py
>>> def collate_fn(batch):
...     pixel_values = [item["pixel_values"] for item in batch]
...     encoding = image_processor.pad(pixel_values, return_tensors="pt")
...     labels = [item["labels"] for item in batch]
...     batch = {}
...     batch["pixel_values"] = encoding["pixel_values"]
...     batch["pixel_mask"] = encoding["pixel_mask"]
...     batch["labels"] = labels
...     return batch
```

## Multi Modal

ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›ã‚’ä½¿ç”¨ã™ã‚‹ã‚¿ã‚¹ã‚¯ã®å ´åˆã€ãƒ¢ãƒ‡ãƒ«ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™ã™ã‚‹ãŸã‚ã®[ãƒ—ãƒ­ã‚»ãƒƒã‚µ](main_classes/processors)ãŒå¿…è¦ã§ã™ã€‚ãƒ—ãƒ­ã‚»ãƒƒã‚µã¯ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚„ç‰¹å¾´é‡æŠ½å‡ºå™¨ãªã©ã®2ã¤ã®å‡¦ç†ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’çµåˆã—ã¾ã™ã€‚

è‡ªå‹•éŸ³å£°èªè­˜ï¼ˆASRï¼‰ã®ãŸã‚ã®ãƒ—ãƒ­ã‚»ãƒƒã‚µã®ä½¿ç”¨æ–¹æ³•ã‚’ç¤ºã™ãŸã‚ã«ã€[LJ Speech](https://huggingface.co/datasets/lj_speech)ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ãƒ¼ãƒ‰æ–¹æ³•ã®è©³ç´°ã«ã¤ã„ã¦ã¯ğŸ¤— [Datasets ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](https://huggingface.co/docs/datasets/load_hub)ã‚’å‚ç…§ï¼‰ï¼š

```python
>>> from datasets import load_dataset

>>> lj_speech = load_dataset("lj_speech", split="train")
```

ASRï¼ˆè‡ªå‹•éŸ³å£°èªè­˜ï¼‰ã®å ´åˆã€ä¸»ã« `audio` ã¨ `text` ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã‚‹ãŸã‚ã€ä»–ã®åˆ—ã‚’å‰Šé™¤ã§ãã¾ã™ï¼š

```python
>>> lj_speech = lj_speech.map(remove_columns=["file", "id", "normalized_text"])
```

æ¬¡ã«ã€`audio`ã¨`text`ã®åˆ—ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ï¼š

```python
>>> lj_speech[0]["audio"]
{'array': array([-7.3242188e-04, -7.6293945e-04, -6.4086914e-04, ...,
         7.3242188e-04,  2.1362305e-04,  6.1035156e-05], dtype=float32),
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/917ece08c95cf0c4115e45294e3cd0dee724a1165b7fc11798369308a465bd26/LJSpeech-1.1/wavs/LJ001-0001.wav',
 'sampling_rate': 22050}

>>> lj_speech[0]["text"]
'Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition'
```

å¸¸ã«ã€ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã‚’ã€ãƒ¢ãƒ‡ãƒ«ã®äº‹å‰å­¦ç¿’ã«ä½¿ç”¨ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã¨ä¸€è‡´ã•ã›ã‚‹ã‚ˆã†ã«[ãƒªã‚µãƒ³ãƒ—ãƒ«](preprocessing#audio)ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼

```py
>>> lj_speech = lj_speech.cast_column("audio", Audio(sampling_rate=16_000))
```

ãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’ [`AutoProcessor.from_pretrained`] ã‚’ä½¿ç”¨ã—ã¦ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ï¼š

```py
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")
```

1. `array`å†…ã«å«ã¾ã‚Œã‚‹ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ‡ãƒ¼ã‚¿ã‚’`input_values`ã«å‡¦ç†ã—ã€`text`ã‚’`labels`ã«ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã™ã‚‹é–¢æ•°ã‚’ä½œæˆã—ã¾ã™ï¼š

```py
>>> def prepare_dataset(example):
...     audio = example["audio"]

...     example.update(processor(audio=audio["array"], text=example["text"], sampling_rate=16000))

...     return example
```

2. ã‚µãƒ³ãƒ—ãƒ«ã«`prepare_dataset`é–¢æ•°ã‚’é©ç”¨ã—ã¾ã™ï¼š

```py
>>> prepare_dataset(lj_speech[0])
```
