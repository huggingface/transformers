<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Efficient Inference on a Multiple GPUs

ã“ã®æ–‡æ›¸ã«ã¯ã€è¤‡æ•°ã®GPUã§åŠ¹ç‡çš„ã«æ¨è«–ã‚’è¡Œã†æ–¹æ³•ã«é–¢ã™ã‚‹æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚
<Tip>

æ³¨æ„: è¤‡æ•°ã®GPUã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¯ã€[å˜ä¸€ã®GPUã‚»ã‚¯ã‚·ãƒ§ãƒ³](./perf_infer_gpu_one)ã§èª¬æ˜ã•ã‚Œã¦ã„ã‚‹ã»ã¨ã‚“ã©ã®æˆ¦ç•¥ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚ãŸã ã—ã€ã‚ˆã‚Šè‰¯ã„ä½¿ç”¨æ³•ã®ãŸã‚ã«ä½¿ç”¨ã§ãã‚‹ç°¡å˜ãªãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã«ã¤ã„ã¦ã‚‚èªè­˜ã—ã¦ãŠãå¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

</Tip>

## Flash Attention 2

Flash Attention 2ã®çµ±åˆã¯ã€è¤‡æ•°ã®GPUã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã§ã‚‚æ©Ÿèƒ½ã—ã¾ã™ã€‚è©³ç´°ã«ã¤ã„ã¦ã¯ã€[å˜ä¸€ã®GPUã‚»ã‚¯ã‚·ãƒ§ãƒ³](./perf_infer_gpu_one#Flash-Attention-2)ã®é©åˆ‡ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã”è¦§ãã ã•ã„ã€‚

## BetterTransformer

[BetterTransformer](https://huggingface.co/docs/optimum/bettertransformer/overview)ã¯ã€ğŸ¤— Transformersãƒ¢ãƒ‡ãƒ«ã‚’PyTorchãƒã‚¤ãƒ†ã‚£ãƒ–ã®é«˜é€Ÿå®Ÿè¡Œãƒ‘ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«å¤‰æ›ã—ã€ãã®ä¸‹ã§Flash Attentionãªã©ã®æœ€é©åŒ–ã•ã‚ŒãŸã‚«ãƒ¼ãƒãƒ«ã‚’å‘¼ã³å‡ºã—ã¾ã™ã€‚

BetterTransformerã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€éŸ³å£°ãƒ¢ãƒ‡ãƒ«ã®å˜ä¸€GPUãŠã‚ˆã³è¤‡æ•°GPUã§ã®é«˜é€Ÿæ¨è«–ã‚‚ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚
<Tip>

Flash Attentionã¯ã€fp16ã¾ãŸã¯bf16 dtypeã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã«ã®ã¿ä½¿ç”¨ã§ãã¾ã™ã€‚BetterTransformerã‚’ä½¿ç”¨ã™ã‚‹å‰ã«ã€ãƒ¢ãƒ‡ãƒ«ã‚’é©åˆ‡ãªdtypeã«ã‚­ãƒ£ã‚¹ãƒˆã—ã¦ãã ã•ã„ã€‚
  
</Tip>

### Decoder models

ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã€ç‰¹ã«ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆGPTã€T5ã€Llamaãªã©ï¼‰ã®å ´åˆã€BetterTransformer APIã¯ã™ã¹ã¦ã®æ³¨æ„æ“ä½œã‚’[`torch.nn.functional.scaled_dot_product_attention`ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention)ï¼ˆSDPAï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«å¤‰æ›ã—ã¾ã™ã€‚ã“ã‚Œã¯PyTorch 2.0ä»¥é™ã§ã®ã¿ä½¿ç”¨å¯èƒ½ã§ã™ã€‚

ãƒ¢ãƒ‡ãƒ«ã‚’BetterTransformerã«å¤‰æ›ã™ã‚‹ã«ã¯ï¼š

```python
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m")
# convert the model to BetterTransformer
model.to_bettertransformer()

# Use it for training or inference
```

SDPAã¯ã€ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚„å•é¡Œã®ã‚µã‚¤ã‚ºãªã©ã®ç‰¹å®šã®è¨­å®šã§[Flash Attention](https://arxiv.org/abs/2205.14135)ã‚«ãƒ¼ãƒãƒ«ã‚’å‘¼ã³å‡ºã™ã“ã¨ã‚‚ã§ãã¾ã™ã€‚Flash Attentionã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹ã€ç‰¹å®šã®è¨­å®šï¼ˆãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã€å•é¡Œã®ã‚µã‚¤ã‚ºï¼‰ã§åˆ©ç”¨å¯èƒ½ã‹ã‚’ç¢ºèªã™ã‚‹ã«ã¯ã€[`torch.backends.cuda.sdp_kernel`](https://pytorch.org/docs/master/backends.html#torch.backends.cuda.sdp_kernel)ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚


```diff
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")
model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m").to("cuda")
# convert the model to BetterTransformer
model.to_bettertransformer()

input_text = "Hello my dog is cute and"
inputs = tokenizer(input_text, return_tensors="pt").to("cuda")

+ with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):
    outputs = model.generate(**inputs)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

ã‚‚ã—ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯ã§æ¬¡ã®ã‚ˆã†ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè¡¨ç¤ºã•ã‚ŒãŸå ´åˆï¼š


```bash
RuntimeError: No available kernel.  Aborting execution.
```

å½“æ—¥ã€Flash Attentionã®ã‚«ãƒãƒ¬ãƒƒã‚¸ãŒåºƒç¯„å›²ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹PyTorch Nightlyãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’è©¦ã™ã‚ˆã†ã«ãŠå‹§ã‚ã—ã¾ã™ã€‚

```bash
pip3 install -U --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118
```

[ã“ã®ãƒ–ãƒ­ã‚°æŠ•ç¨¿](https://pytorch.org/blog/out-of-the-box-acceleration/)ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã€BetterTransformer + SDPA APIã§å¯èƒ½ãªã“ã¨ã«ã¤ã„ã¦è©³ã—ãå­¦ã³ã¾ã—ã‚‡ã†ã€‚

### Encoder Models

æ¨è«–ä¸­ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€BetterTransformerã¯ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®forwardå‘¼ã³å‡ºã—ã‚’ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®[`torch.nn.TransformerEncoderLayer`](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html)ã®ç›¸å½“ã™ã‚‹ã‚‚ã®ã«ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®é«˜é€Ÿå®Ÿè£…ãŒå®Ÿè¡Œã•ã‚Œã¾ã™ã€‚

`torch.nn.TransformerEncoderLayer`ã®é«˜é€Ÿå®Ÿè£…ã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ãªã„ãŸã‚ã€ä»£ã‚ã‚Šã«`torch.nn.functional.scaled_dot_product_attention`ã«ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã•ã‚Œã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒã‚¹ãƒˆã•ã‚ŒãŸãƒ†ãƒ³ã‚½ãƒ«ã‚’æ´»ç”¨ã—ãªã„Flash Attentionã¾ãŸã¯Memory-Efficient Attentionã®èåˆã‚«ãƒ¼ãƒãƒ«ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚

BetterTransformerã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€ã“ã®[ãƒ–ãƒ­ã‚°æŠ•ç¨¿](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2)ã‚’ã”è¦§ã„ãŸã ã‘ã¾ã™ã€‚ã¾ãŸã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¢ãƒ‡ãƒ«ç”¨ã®BetterTransformerã«ã¤ã„ã¦ã¯ã€ã“ã®[ãƒ–ãƒ­ã‚°](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/)ã§è©³ã—ãå­¦ã¶ã“ã¨ãŒã§ãã¾ã™ã€‚


## Advanced usage: mixing FP4 (or Int8) and BetterTransformer

ãƒ¢ãƒ‡ãƒ«ã®æœ€è‰¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å¾—ã‚‹ãŸã‚ã«ã€ä¸Šè¨˜ã§èª¬æ˜ã—ãŸç•°ãªã‚‹æ–¹æ³•ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ä¾‹ãˆã°ã€FP4ãƒŸãƒƒã‚¯ã‚¹ãƒ—ãƒ¬ã‚·ã‚¸ãƒ§ãƒ³æ¨è«–+Flash Attentionã‚’ä½¿ç”¨ã—ãŸBetterTransformerã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚


```py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)

tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")
model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", quantization_config=quantization_config)

input_text = "Hello my dog is cute and"
inputs = tokenizer(input_text, return_tensors="pt").to("cuda")

with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):
    outputs = model.generate(**inputs)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```