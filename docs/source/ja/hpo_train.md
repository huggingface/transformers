<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Hyperparameter Search using Trainer API

ğŸ¤— Transformersã¯ã€ğŸ¤— Transformersãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’æœ€é©åŒ–ã™ã‚‹[`Trainer`]ã‚¯ãƒ©ã‚¹ã‚’æä¾›ã—ã€ç‹¬è‡ªã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã‚’æ‰‹å‹•ã§è¨˜è¿°ã›ãšã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹ã®ãŒç°¡å˜ã«ãªã‚Šã¾ã™ã€‚[`Trainer`]ã¯ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼æ¤œç´¢ã®APIã‚‚æä¾›ã—ã¦ã„ã¾ã™ã€‚ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ã€ãã‚Œã‚’ä¾‹ç¤ºã—ã¾ã™ã€‚

## Hyperparameter Search backend

[`Trainer`]ã¯ç¾åœ¨ã€4ã¤ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼æ¤œç´¢ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ï¼š
[optuna](https://optuna.org/)ã€[sigopt](https://sigopt.com/)ã€[raytune](https://docs.ray.io/en/latest/tune/index.html)ã€ãŠã‚ˆã³[wandb](https://wandb.ai/site/sweeps)ã€‚

ã“ã‚Œã‚‰ã‚’ä½¿ç”¨ã™ã‚‹å‰ã«ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼æ¤œç´¢ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
```bash
pip install optuna/sigopt/wandb/ray[tune] 
```

## How to enable Hyperparameter search in example

ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¤œç´¢ã‚¹ãƒšãƒ¼ã‚¹ã‚’å®šç¾©ã—ã€ç•°ãªã‚‹ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã«ã¯ç•°ãªã‚‹ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãŒå¿…è¦ã§ã™ã€‚

Sigoptã®å ´åˆã€sigopt [object_parameter](https://docs.sigopt.com/ai-module-api-references/api_reference/objects/object_parameter) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚ãã‚Œã¯ä»¥ä¸‹ã®ã‚ˆã†ãªã‚‚ã®ã§ã™ï¼š
```py
>>> def sigopt_hp_space(trial):
...     return [
...         {"bounds": {"min": 1e-6, "max": 1e-4}, "name": "learning_rate", "type": "double"},
...         {
...             "categorical_values": ["16", "32", "64", "128"],
...             "name": "per_device_train_batch_size",
...             "type": "categorical",
...         },
...     ]
```


Optunaã«é–¢ã—ã¦ã¯ã€[object_parameter](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/002_configurations.html#sphx-glr-tutorial-10-key-features-002-configurations-py)ã‚’ã”è¦§ãã ã•ã„ã€‚ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š


```py
>>> def optuna_hp_space(trial):
...     return {
...         "learning_rate": trial.suggest_float("learning_rate", 1e-6, 1e-4, log=True),
...         "per_device_train_batch_size": trial.suggest_categorical("per_device_train_batch_size", [16, 32, 64, 128]),
...     }
```

Optunaã¯ã€å¤šç›®çš„ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ï¼ˆHPOï¼‰ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚ `hyperparameter_search` ã§ `direction` ã‚’æ¸¡ã—ã€è¤‡æ•°ã®ç›®çš„é–¢æ•°å€¤ã‚’è¿”ã™ãŸã‚ã®ç‹¬è‡ªã® `compute_objective` ã‚’å®šç¾©ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ Pareto Frontï¼ˆ`List[BestRun]`ï¼‰ã¯ `hyperparameter_search` ã§è¿”ã•ã‚Œã€[test_trainer](https://github.com/huggingface/transformers/blob/main/tests/trainer/test_trainer.py) ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ `TrainerHyperParameterMultiObjectOptunaIntegrationTest` ã‚’å‚ç…§ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚


```py
>>> best_trials = trainer.hyperparameter_search(
...     direction=["minimize", "maximize"],
...     backend="optuna",
...     hp_space=optuna_hp_space,
...     n_trials=20,
...     compute_objective=compute_objective,
... )
```

Ray Tuneã«é–¢ã—ã¦ã€[object_parameter](https://docs.ray.io/en/latest/tune/api/search_space.html)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š


```py
>>> def ray_hp_space(trial):
...     return {
...         "learning_rate": tune.loguniform(1e-6, 1e-4),
...         "per_device_train_batch_size": tune.choice([16, 32, 64, 128]),
...     }
```

Wandbã«ã¤ã„ã¦ã¯ã€[object_parameter](https://docs.wandb.ai/guides/sweeps/configuration)ã‚’ã”è¦§ãã ã•ã„ã€‚ã“ã‚Œã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š

```py
>>> def wandb_hp_space(trial):
...     return {
...         "method": "random",
...         "metric": {"name": "objective", "goal": "minimize"},
...         "parameters": {
...             "learning_rate": {"distribution": "uniform", "min": 1e-6, "max": 1e-4},
...             "per_device_train_batch_size": {"values": [16, 32, 64, 128]},
...         },
...     }
```

`model_init` é–¢æ•°ã‚’å®šç¾©ã—ã€ãã‚Œã‚’ [`Trainer`] ã«æ¸¡ã™ä¾‹ã‚’ç¤ºã—ã¾ã™ï¼š


```py
>>> def model_init(trial):
...     return AutoModelForSequenceClassification.from_pretrained(
...         model_args.model_name_or_path,
...         from_tf=bool(".ckpt" in model_args.model_name_or_path),
...         config=config,
...         cache_dir=model_args.cache_dir,
...         revision=model_args.model_revision,
...         use_auth_token=True if model_args.use_auth_token else None,
...     )
```

[`Trainer`] ã‚’ `model_init` é–¢æ•°ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¼•æ•°ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ãŠã‚ˆã³è©•ä¾¡é–¢æ•°ã¨å…±ã«ä½œæˆã—ã¦ãã ã•ã„:


```py
>>> trainer = Trainer(
...     model=None,
...     args=training_args,
...     train_dataset=small_train_dataset,
...     eval_dataset=small_eval_dataset,
...     compute_metrics=compute_metrics,
...     tokenizer=tokenizer,
...     model_init=model_init,
...     data_collator=data_collator,
... )
```

ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®æ¢ç´¢ã‚’å‘¼ã³å‡ºã—ã€æœ€è‰¯ã®ãƒˆãƒ©ã‚¤ã‚¢ãƒ« ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’å–å¾—ã—ã¾ã™ã€‚ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¯ `"optuna"` / `"sigopt"` / `"wandb"` / `"ray"` ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚æ–¹å‘ã¯ `"minimize"` ã¾ãŸã¯ `"maximize"` ã§ã‚ã‚Šã€ç›®æ¨™ã‚’ã‚ˆã‚Šå¤§ããã™ã‚‹ã‹å°ã•ãã™ã‚‹ã‹ã‚’ç¤ºã—ã¾ã™ã€‚

`compute_objective` é–¢æ•°ã‚’ç‹¬è‡ªã«å®šç¾©ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚å®šç¾©ã•ã‚Œã¦ã„ãªã„å ´åˆã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã® `compute_objective` ãŒå‘¼ã³å‡ºã•ã‚Œã€F1ãªã©ã®è©•ä¾¡ãƒ¡ãƒˆãƒªãƒƒã‚¯ã®åˆè¨ˆãŒç›®æ¨™å€¤ã¨ã—ã¦è¿”ã•ã‚Œã¾ã™ã€‚


```py
>>> best_trial = trainer.hyperparameter_search(
...     direction="maximize",
...     backend="optuna",
...     hp_space=optuna_hp_space,
...     n_trials=20,
...     compute_objective=compute_objective,
... )
```

## Hyperparameter search For DDP finetune
ç¾åœ¨ã€DDPï¼ˆDistributed Data Parallelï¼‰ã®ãŸã‚ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼æ¤œç´¢ã¯ã€Optuna ã¨ SigOpt ã«å¯¾ã—ã¦æœ‰åŠ¹ã«ãªã£ã¦ã„ã¾ã™ã€‚ãƒ©ãƒ³ã‚¯ã‚¼ãƒ­ãƒ—ãƒ­ã‚»ã‚¹ã®ã¿ãŒæ¤œç´¢ãƒˆãƒ©ã‚¤ã‚¢ãƒ«ã‚’ç”Ÿæˆã—ã€ä»–ã®ãƒ©ãƒ³ã‚¯ã«å¼•æ•°ã‚’æ¸¡ã—ã¾ã™ã€‚






