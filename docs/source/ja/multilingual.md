<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# æ¨è«–ã®ãŸã‚ã®å¤šè¨€èªãƒ¢ãƒ‡ãƒ«

[[open-in-colab]]

ğŸ¤— Transformers ã«ã¯ã„ãã¤ã‹ã®å¤šè¨€èªãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã€ãã‚Œã‚‰ã®æ¨è«–ã®ä½¿ç”¨æ–¹æ³•ã¯å˜ä¸€è¨€èªãƒ¢ãƒ‡ãƒ«ã¨ã¯ç•°ãªã‚Šã¾ã™ã€‚ãŸã ã—ã€å¤šè¨€èªãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨æ–¹æ³•ãŒã™ã¹ã¦ç•°ãªã‚‹ã‚ã‘ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ [google-bert/bert-base-multilingual-uncased](https://huggingface.co/google-bert/bert-base-multilingual-uncased) ãªã©ã®ä¸€éƒ¨ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€å˜ä¸€è¨€èªãƒ¢ãƒ‡ãƒ«ã¨åŒæ§˜ã«ä½¿ç”¨ã§ãã¾ã™ã€‚ ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€æ¨è«–ã®ãŸã‚ã«ä½¿ç”¨æ–¹æ³•ãŒç•°ãªã‚‹å¤šè¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ã©ã®ã‚ˆã†ã«ä½¿ã†ã‹ã‚’ç¤ºã—ã¾ã™ã€‚

## XLM

XLM ã«ã¯10ã®ç•°ãªã‚‹ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒã‚ã‚Šã€ãã®ã†ã¡ã®1ã¤ã ã‘ãŒå˜ä¸€è¨€èªã§ã™ã€‚ æ®‹ã‚Šã®9ã¤ã®ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¯ã€è¨€èªåŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨ã™ã‚‹ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¨ä½¿ç”¨ã—ãªã„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®2ã¤ã®ã‚«ãƒ†ã‚´ãƒªã«åˆ†ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

### è¨€èªã®åŸ‹ã‚è¾¼ã¿ãŒã‚ã‚‹ XLM

æ¬¡ã® XLM ãƒ¢ãƒ‡ãƒ«ã¯ã€è¨€èªã®åŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨ã—ã¦ã€æ¨è«–ã§ä½¿ç”¨ã•ã‚Œã‚‹è¨€èªã‚’æŒ‡å®šã—ã¾ã™ã€‚

- `FacebookAI/xlm-mlm-ende-1024` (ãƒã‚¹ã‚¯åŒ–ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€è‹±èª-ãƒ‰ã‚¤ãƒ„èª)
- `FacebookAI/xlm-mlm-enfr-1024` (ãƒã‚¹ã‚¯åŒ–ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€è‹±èª-ãƒ•ãƒ©ãƒ³ã‚¹èª)
- `FacebookAI/xlm-mlm-enro-1024` (ãƒã‚¹ã‚¯åŒ–ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€è‹±èª-ãƒ«ãƒ¼ãƒãƒ‹ã‚¢èª)
- `FacebookAI/xlm-mlm-xnli15-1024` (ãƒã‚¹ã‚¯åŒ–ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€XNLI è¨€èª)
- `FacebookAI/xlm-mlm-tlm-xnli15-1024` (ãƒã‚¹ã‚¯åŒ–ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚° + ç¿»è¨³ + XNLI è¨€èª)
- `FacebookAI/xlm-clm-enfr-1024` (å› æœè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€è‹±èª-ãƒ•ãƒ©ãƒ³ã‚¹èª)
- `FacebookAI/xlm-clm-ende-1024` (å› æœè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€è‹±èª-ãƒ‰ã‚¤ãƒ„èª)

è¨€èªã®åŸ‹ã‚è¾¼ã¿ã¯ã€ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã•ã‚Œã‚‹ `input_ids` ã¨åŒã˜å½¢çŠ¶ã®ãƒ†ãƒ³ã‚½ãƒ«ã¨ã—ã¦è¡¨ã•ã‚Œã¾ã™ã€‚ ã“ã‚Œã‚‰ã®ãƒ†ãƒ³ã‚½ãƒ«ã®å€¤ã¯ã€ä½¿ç”¨ã•ã‚Œã‚‹è¨€èªã«ä¾å­˜ã—ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã® `lang2id` ãŠã‚ˆã³ `id2lang` å±æ€§ã«ã‚ˆã£ã¦è­˜åˆ¥ã•ã‚Œã¾ã™ã€‚

ã“ã®ä¾‹ã§ã¯ã€`FacebookAI/xlm-clm-enfr-1024` ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ (å› æœè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€è‹±èª-ãƒ•ãƒ©ãƒ³ã‚¹èª)ã€‚

```py
>>> import torch
>>> from transformers import XLMTokenizer, XLMWithLMHeadModel

>>> tokenizer = XLMTokenizer.from_pretrained("FacebookAI/xlm-clm-enfr-1024")
>>> model = XLMWithLMHeadModel.from_pretrained("FacebookAI/xlm-clm-enfr-1024")
```

ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã® `lang2id` å±æ€§ã¯ã€ã“ã®ãƒ¢ãƒ‡ãƒ«ã®è¨€èªã¨ãã® ID ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚

```py
>>> print(tokenizer.lang2id)
{'en': 0, 'fr': 1}
```

æ¬¡ã«ã€å…¥åŠ›ä¾‹ã‚’ä½œæˆã—ã¾ã™ã€‚

```py
>>> input_ids = torch.tensor([tokenizer.encode("Wikipedia was used to")])  # batch size of 1
```

è¨€èª ID ã‚’ `en` ã«è¨­å®šã—ã€ãã‚Œã‚’ä½¿ç”¨ã—ã¦è¨€èªã®åŸ‹ã‚è¾¼ã¿ã‚’å®šç¾©ã—ã¾ã™ã€‚ è¨€èªã®åŸ‹ã‚è¾¼ã¿ã¯ã€è‹±èªã®è¨€èª ID ã§ã‚ã‚‹ãŸã‚ã€`0` ã§åŸ‹ã‚ã‚‰ã‚ŒãŸãƒ†ãƒ³ã‚½ãƒ«ã§ã™ã€‚ ã“ã®ãƒ†ãƒ³ã‚½ãƒ«ã¯ `input_ids` ã¨åŒã˜ã‚µã‚¤ã‚ºã«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

```py
>>> language_id = tokenizer.lang2id["en"]  # 0
>>> langs = torch.tensor([language_id] * input_ids.shape[1])  # torch.tensor([0, 0, 0, ..., 0])

>>> # We reshape it to be of size (batch_size, sequence_length)
>>> langs = langs.view(1, -1)  # is now of shape [1, sequence_length] (we have a batch size of 1)
```

ã“ã‚Œã§ã€`input_ids` ã¨è¨€èªã®åŸ‹ã‚è¾¼ã¿ã‚’ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã™ã“ã¨ãŒã§ãã¾ã™ã€‚

```py
>>> outputs = model(input_ids, langs=langs)
```

[run_generation.py](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-generation/run_generation.py) ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€`xlm-clm` ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦ã€è¨€èªãŒåŸ‹ã‚è¾¼ã¾ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã§ãã¾ã™ã€‚

### è¨€èªã®åŸ‹ã‚è¾¼ã¿ãŒãªã„XLM

æ¬¡ã® XLM ãƒ¢ãƒ‡ãƒ«ã¯ã€æ¨è«–ä¸­ã«è¨€èªã®åŸ‹ã‚è¾¼ã¿ã‚’å¿…è¦ã¨ã—ã¾ã›ã‚“ã€‚

- `FacebookAI/xlm-mlm-17-1280` (ãƒã‚¹ã‚¯åŒ–ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€17ã®è¨€èª)
- `FacebookAI/xlm-mlm-100-1280` (ãƒã‚¹ã‚¯åŒ–ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€100ã®è¨€èª)

ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ä»¥å‰ã® XLM ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¨ã¯ç•°ãªã‚Šã€ä¸€èˆ¬çš„ãªæ–‡ã®è¡¨ç¾ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

## BERT

ä»¥ä¸‹ã® BERT ãƒ¢ãƒ‡ãƒ«ã¯ã€å¤šè¨€èªã‚¿ã‚¹ã‚¯ã«ä½¿ç”¨ã§ãã¾ã™ã€‚

- `google-bert/bert-base-multilingual-uncased` (ãƒã‚¹ã‚¯åŒ–ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚° + æ¬¡ã®æ–‡ã®äºˆæ¸¬ã€102ã®è¨€èª)
- `google-bert/bert-base-multilingual-cased` (ãƒã‚¹ã‚¯åŒ–ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚° + æ¬¡ã®æ–‡ã®äºˆæ¸¬ã€104ã®è¨€èª)

ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€æ¨è«–ä¸­ã«è¨€èªã®åŸ‹ã‚è¾¼ã¿ã‚’å¿…è¦ã¨ã—ã¾ã›ã‚“ã€‚ æ–‡è„ˆã‹ã‚‰è¨€èªã‚’è­˜åˆ¥ã—ã€ãã‚Œã«å¿œã˜ã¦æ¨æ¸¬ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

## XLM-RoBERTa

æ¬¡ã® XLM-RoBERTa ãƒ¢ãƒ‡ãƒ«ã¯ã€å¤šè¨€èªã‚¿ã‚¹ã‚¯ã«ä½¿ç”¨ã§ãã¾ã™ã€‚

- `FacebookAI/xlm-roberta-base` (ãƒã‚¹ã‚¯åŒ–ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€100ã®è¨€èª)
- `FacebookAI/xlm-roberta-large` (ãƒã‚¹ã‚¯åŒ–ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€100ã®è¨€èª)

XLM-RoBERTa ã¯ã€100ã®è¨€èªã§æ–°ã—ãä½œæˆãŠã‚ˆã³ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸ2.5 TB ã® CommonCrawl ãƒ‡ãƒ¼ã‚¿ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¾ã—ãŸã€‚ ã“ã‚Œã¯ã€åˆ†é¡ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ãƒ©ãƒ™ãƒ«ä»˜ã‘ã€è³ªå•å¿œç­”ãªã©ã®ãƒ€ã‚¦ãƒ³ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚¿ã‚¹ã‚¯ã§ã€mBERT ã‚„ XLM ãªã©ã®ä»¥å‰ã«ãƒªãƒªãƒ¼ã‚¹ã•ã‚ŒãŸå¤šè¨€èªãƒ¢ãƒ‡ãƒ«ã‚’å¤§å¹…ã«æ”¹å–„ã—ã¾ã™ã€‚

## M2M100

æ¬¡ã® M2M100 ãƒ¢ãƒ‡ãƒ«ã¯ã€å¤šè¨€èªç¿»è¨³ã«ä½¿ç”¨ã§ãã¾ã™ã€‚

- `facebook/m2m100_418M` (ç¿»è¨³)
- `facebook/m2m100_1.2B` (ç¿»è¨³)

ã“ã®ä¾‹ã§ã¯ã€`facebook/m2m100_418M` ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã€ä¸­å›½èªã‹ã‚‰è‹±èªã«ç¿»è¨³ã—ã¾ã™ã€‚ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã§ã‚½ãƒ¼ã‚¹è¨€èªã‚’è¨­å®šã§ãã¾ã™ã€‚

```py
>>> from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer

>>> en_text = "Do not meddle in the affairs of wizards, for they are subtle and quick to anger."
>>> chinese_text = "ä¸è¦æ’æ‰‹å·«å¸«çš„äº‹å‹™, å› ç‚ºä»–å€‘æ˜¯å¾®å¦™çš„, å¾ˆå¿«å°±æœƒç™¼æ€’."

>>> tokenizer = M2M100Tokenizer.from_pretrained("facebook/m2m100_418M", src_lang="zh")
>>> model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_418M")
```

ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¾ã™ã€‚

```py
>>> encoded_zh = tokenizer(chinese_text, return_tensors="pt")
```

M2M100 ã¯ã€æœ€åˆã«ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨€èª ID ã‚’å¼·åˆ¶çš„ã«ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨€èªã«ç¿»è¨³ã—ã¾ã™ã€‚ è‹±èªã«ç¿»è¨³ã™ã‚‹ã«ã¯ã€`generate` ãƒ¡ã‚½ãƒƒãƒ‰ã§ `forced_bos_token_id` ã‚’ `en` ã«è¨­å®šã—ã¾ã™ã€‚

```py
>>> generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id("en"))
>>> tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
'Do not interfere with the matters of the witches, because they are delicate and will soon be angry.'
```

## MBart

å¤šè¨€èªç¿»è¨³ã«ã¯ã€æ¬¡ã® MBart ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚

- `facebook/mbart-large-50-one-to-many-mmt` (One-to-many multilingual machine translation, 50 languages)
- `facebook/mbart-large-50-many-to-many-mmt` (Many-to-many multilingual machine translation, 50 languages)
- `facebook/mbart-large-50-many-to-one-mmt` (Many-to-one multilingual machine translation, 50 languages)
- `facebook/mbart-large-50` (Multilingual translation, 50 languages)
- `facebook/mbart-large-cc25`

ã“ã®ä¾‹ã§ã¯ã€`facebook/mbart-large-50-many-to-many-mmt` ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã€ãƒ•ã‚£ãƒ³ãƒ©ãƒ³ãƒ‰èªã‚’è‹±èªã«ç¿»è¨³ã—ã¾ã™ã€‚ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã§ã‚½ãƒ¼ã‚¹è¨€èªã‚’è¨­å®šã§ãã¾ã™ã€‚

```py
>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

>>> en_text = "Do not meddle in the affairs of wizards, for they are subtle and quick to anger."
>>> fi_text = "Ã„lÃ¤ sekaannu velhojen asioihin, sillÃ¤ ne ovat hienovaraisia ja nopeasti vihaisia."

>>> tokenizer = AutoTokenizer.from_pretrained("facebook/mbart-large-50-many-to-many-mmt", src_lang="fi_FI")
>>> model = AutoModelForSeq2SeqLM.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")
```

ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¾ã™ã€‚

```py
>>> encoded_en = tokenizer(en_text, return_tensors="pt")
```

MBart ã¯ã€æœ€åˆã«ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨€èª ID ã‚’å¼·åˆ¶çš„ã«ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨€èªã«ç¿»è¨³ã—ã¾ã™ã€‚ è‹±èªã«ç¿»è¨³ã™ã‚‹ã«ã¯ã€`generate` ãƒ¡ã‚½ãƒƒãƒ‰ã§ `forced_bos_token_id` ã‚’ `en` ã«è¨­å®šã—ã¾ã™ã€‚

```py
>>> generated_tokens = model.generate(**encoded_en, forced_bos_token_id=tokenizer.lang_code_to_id("en_XX"))
>>> tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
"Don't interfere with the wizard's affairs, because they are subtle, will soon get angry."
```

`facebook/mbart-large-50-many-to-one-mmt` ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã€æœ€åˆã«ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨€èª ID ã‚’å¼·åˆ¶ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ãã‚Œä»¥å¤–ã®å ´åˆã€ä½¿ç”¨æ–¹æ³•ã¯åŒã˜ã§ã™ã€‚