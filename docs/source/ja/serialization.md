<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Export to ONNX

ğŸ¤— Transformersãƒ¢ãƒ‡ãƒ«ã‚’æœ¬ç•ªç’°å¢ƒã«å±•é–‹ã™ã‚‹éš›ã«ã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚’ç‰¹æ®Šãªãƒ©ãƒ³ã‚¿ã‚¤ãƒ ãŠã‚ˆã³ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã§èª­ã¿è¾¼ã¿ã€å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã€ãƒ¢ãƒ‡ãƒ«ã‚’ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºã•ã‚ŒãŸå½¢å¼ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ã“ã¨ãŒå¿…è¦ã§ã‚ã‚‹ã‹ã€ãã®æ©æµã‚’å—ã‘ã‚‹ã“ã¨ãŒã§ãã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚

ğŸ¤— Optimumã¯ã€Transformersã®æ‹¡å¼µæ©Ÿèƒ½ã§ã‚ã‚Šã€PyTorchã¾ãŸã¯TensorFlowã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ONNXã‚„TFLiteãªã©ã®ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºã•ã‚ŒãŸå½¢å¼ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ã“ã¨ã‚’å¯èƒ½ã«ã™ã‚‹ã€Œexportersã€ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚ã¾ãŸã€ğŸ¤— Optimumã¯ã€æœ€å¤§ã®åŠ¹ç‡ã§ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãƒ„ãƒ¼ãƒ«ã‚‚æä¾›ã—ã¦ã„ã¾ã™ã€‚

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€ğŸ¤— Transformersãƒ¢ãƒ‡ãƒ«ã‚’ğŸ¤— Optimumã‚’ä½¿ç”¨ã—ã¦ONNXã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹æ–¹æ³•ã‚’ç¤ºã—ã¦ãŠã‚Šã€ãƒ¢ãƒ‡ãƒ«ã‚’TFLiteã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹æ–¹æ³•ã«ã¤ã„ã¦ã¯[Export to TFLiteãƒšãƒ¼ã‚¸](tflite)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

## Export to ONNX 

[ONNXï¼ˆOpen Neural Network eXchangeï¼‰](http://onnx.ai)ã¯ã€PyTorchãŠã‚ˆã³TensorFlowã‚’å«ã‚€ã•ã¾ã–ã¾ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’è¡¨ç¾ã™ã‚‹ãŸã‚ã®å…±é€šã®ä¸€é€£ã®æ¼”ç®—å­ã¨ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚’å®šç¾©ã™ã‚‹ã‚ªãƒ¼ãƒ—ãƒ³ã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ãŒONNXå½¢å¼ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã•ã‚Œã‚‹ã¨ã€ã“ã‚Œã‚‰ã®æ¼”ç®—å­ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä»‹ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®æµã‚Œã‚’è¡¨ã™è¨ˆç®—ã‚°ãƒ©ãƒ•ï¼ˆä¸€èˆ¬çš„ã«ã¯ã€Œä¸­é–“è¡¨ç¾ã€ã¨å‘¼ã°ã‚Œã‚‹ï¼‰ã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

æ¨™æº–åŒ–ã•ã‚ŒãŸæ¼”ç®—å­ã¨ãƒ‡ãƒ¼ã‚¿å‹ã‚’å‚™ãˆãŸã‚°ãƒ©ãƒ•ã‚’å…¬é–‹ã™ã‚‹ã“ã¨ã§ã€ONNXã¯ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯é–“ã®åˆ‡ã‚Šæ›¿ãˆã‚’å®¹æ˜“ã«ã—ã¾ã™ã€‚ãŸã¨ãˆã°ã€PyTorchã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯ONNXå½¢å¼ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã€ãã‚Œã‚’TensorFlowã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼ˆé€†ã‚‚åŒæ§˜ã§ã™ï¼‰ã€‚

ONNXå½¢å¼ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«ä½¿ç”¨ã§ãã¾ã™ï¼š
- [ã‚°ãƒ©ãƒ•æœ€é©åŒ–](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/optimization)ã‚„[é‡å­åŒ–](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/quantization)ãªã©ã®ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã‚’ä½¿ç”¨ã—ã¦æ¨è«–ã®ãŸã‚ã«æœ€é©åŒ–ã€‚
- [`ORTModelForXXX`ã‚¯ãƒ©ã‚¹](https://huggingface.co/docs/optimum/onnxruntime/package_reference/modeling_ort)ã‚’ä»‹ã—ã¦ONNX Runtimeã§å®Ÿè¡Œã—ã€ğŸ¤— Transformersã§ãŠãªã˜ã¿ã®`AutoModel` APIã«å¾“ã„ã¾ã™ã€‚
- [æœ€é©åŒ–ã•ã‚ŒãŸæ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³](https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/pipelines)ã‚’ä»‹ã—ã¦å®Ÿè¡Œã—ã€ğŸ¤— Transformersã®[`pipeline`]é–¢æ•°ã¨åŒã˜APIã‚’æŒã£ã¦ã„ã¾ã™ã€‚

ğŸ¤— Optimumã¯ã€è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ´»ç”¨ã—ã¦ONNXã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ãŠã‚Šã€ã“ã‚Œã‚‰ã®è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¯å¤šãã®ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ç”¨ã«äº‹å‰ã«ä½œæˆã•ã‚Œã¦ãŠã‚Šã€ä»–ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã‚‚ç°¡å˜ã«æ‹¡å¼µã§ãã‚‹ã‚ˆã†ã«è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚

äº‹å‰ã«ä½œæˆã•ã‚ŒãŸè¨­å®šã®ãƒªã‚¹ãƒˆã«ã¤ã„ã¦ã¯ã€[ğŸ¤— Optimumãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://huggingface.co/docs/optimum/exporters/onnx/overview)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

ğŸ¤— Transformersãƒ¢ãƒ‡ãƒ«ã‚’ONNXã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹æ–¹æ³•ã¯2ã¤ã‚ã‚Šã¾ã™ã€‚ä»¥ä¸‹ã§ã¯ä¸¡æ–¹ã®æ–¹æ³•ã‚’ç¤ºã—ã¾ã™ï¼š

- export with ğŸ¤— Optimum via CLI.
- export with ğŸ¤— Optimum with `optimum.onnxruntime`.

### Exporting a ğŸ¤— Transformers model to ONNX with CLI

ğŸ¤— Transformersãƒ¢ãƒ‡ãƒ«ã‚’ONNXã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ã«ã¯ã€ã¾ãšè¿½åŠ ã®ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ï¼š

```bash
pip install optimum-onnx
```

ã™ã¹ã¦ã®åˆ©ç”¨å¯èƒ½ãªå¼•æ•°ã‚’ç¢ºèªã™ã‚‹ã«ã¯ã€[ğŸ¤— Optimumãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model#exporting-a-model-to-onnx-using-the-cli)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚ã¾ãŸã¯ã€ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã§ãƒ˜ãƒ«ãƒ—ã‚’è¡¨ç¤ºã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ï¼š


```bash
optimum-cli export onnx --help
```

ğŸ¤— Hubã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ã«ã¯ã€ä¾‹ãˆã° `distilbert/distilbert-base-uncased-distilled-squad` ã‚’ä½¿ã„ãŸã„å ´åˆã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š

```bash
optimum-cli export onnx --model distilbert/distilbert-base-uncased-distilled-squad distilbert_base_uncased_squad_onnx/
```

é€²è¡ŒçŠ¶æ³ã‚’ç¤ºã—ã€çµæœã® `model.onnx` ãŒä¿å­˜ã•ã‚Œã‚‹å ´æ‰€ã‚’è¡¨ç¤ºã™ã‚‹ãƒ­ã‚°ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«è¡¨ç¤ºã•ã‚Œã‚‹ã¯ãšã§ã™ï¼š


```bash
Validating ONNX model distilbert_base_uncased_squad_onnx/model.onnx...
	-[âœ“] ONNX model output names match reference model (start_logits, end_logits)
	- Validating ONNX Model output "start_logits":
		-[âœ“] (2, 16) matches (2, 16)
		-[âœ“] all values close (atol: 0.0001)
	- Validating ONNX Model output "end_logits":
		-[âœ“] (2, 16) matches (2, 16)
		-[âœ“] all values close (atol: 0.0001)
The ONNX export succeeded and the exported model was saved at: distilbert_base_uncased_squad_onnx
```

ä¸Šè¨˜ã®ä¾‹ã¯ğŸ¤— Hubã‹ã‚‰ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹å ´åˆã€ã¾ãšãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆ`local_path`ï¼‰ã«ä¿å­˜ã—ã¦ãã ã•ã„ã€‚CLIã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€ğŸ¤— Hubã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆåã®ä»£ã‚ã‚Šã«`model`å¼•æ•°ã«`local_path`ã‚’æ¸¡ã—ã€`--task`å¼•æ•°ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚[ğŸ¤— Optimumãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://huggingface.co/docs/optimum/exporters/task_manager)ã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã‚¿ã‚¹ã‚¯ã®ãƒªã‚¹ãƒˆã‚’ç¢ºèªã§ãã¾ã™ã€‚`task`å¼•æ•°ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã€ã‚¿ã‚¹ã‚¯å›ºæœ‰ã®ãƒ˜ãƒƒãƒ‰ã‚’æŒãŸãªã„ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§é¸æŠã•ã‚Œã¾ã™ã€‚


```bash
optimum-cli export onnx --model local_path --task question-answering distilbert_base_uncased_squad_onnx/
```

ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã•ã‚ŒãŸ `model.onnx` ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€ONNXæ¨™æº–ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹[å¤šãã®ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚¿](https://onnx.ai/supported-tools.html#deployModel)ã®1ã¤ã§å®Ÿè¡Œã§ãã¾ã™ã€‚ãŸã¨ãˆã°ã€[ONNX Runtime](https://onnxruntime.ai/)ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€å®Ÿè¡Œã™ã‚‹æ–¹æ³•ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š


```python
>>> from transformers import AutoTokenizer
>>> from optimum.onnxruntime import ORTModelForQuestionAnswering

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert_base_uncased_squad_onnx")
>>> model = ORTModelForQuestionAnswering.from_pretrained("distilbert_base_uncased_squad_onnx")
>>> inputs = tokenizer("What am I using?", "Using DistilBERT with ONNX Runtime!", return_tensors="pt")
>>> outputs = model(**inputs)
```

ğŸ¤— Hubã‹ã‚‰TensorFlowã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã¯ã€åŒæ§˜ã§ã™ã€‚ä¾‹ãˆã°ã€[Keras organization](https://huggingface.co/keras-io)ã‹ã‚‰ç´”ç²‹ãªTensorFlowã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹æ–¹æ³•ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š


```bash
optimum-cli export onnx --model keras-io/transformers-qa distilbert_base_cased_squad_onnx/
```

### Exporting a ğŸ¤— Transformers model to ONNX with `optimum.onnxruntime`

CLIã®ä»£ã‚ã‚Šã«ã€ğŸ¤— Transformersãƒ¢ãƒ‡ãƒ«ã‚’ONNXã«ãƒ—ãƒ­ã‚°ãƒ©ãƒ çš„ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ä»¥ä¸‹ã®ã‚ˆã†ã«è¡Œã„ã¾ã™ï¼š

```python
>>> from optimum.onnxruntime import ORTModelForSequenceClassification
>>> from transformers import AutoTokenizer

>>> model_checkpoint = "distilbert_base_uncased_squad"
>>> save_directory = "onnx/"

>>> # Load a model from transformers and export it to ONNX
>>> ort_model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, export=True)
>>> tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

>>> # Save the onnx model and tokenizer
>>> ort_model.save_pretrained(save_directory)
>>> tokenizer.save_pretrained(save_directory)
```

### Exporting a model for an unsupported architecture

ç¾åœ¨ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã§ããªã„ãƒ¢ãƒ‡ãƒ«ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ãŸã‚ã«è²¢çŒ®ã—ãŸã„å ´åˆã€ã¾ãš[`optimum.exporters.onnx`](https://huggingface.co/docs/optimum/exporters/onnx/overview)ã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’ç¢ºèªã—ã€ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„å ´åˆã¯[ğŸ¤— Optimumã«è²¢çŒ®](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/contribute)ã—ã¦ãã ã•ã„ã€‚
