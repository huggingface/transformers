<!--ç‰ˆæƒæ‰€æœ‰ 2022 å¹´ HuggingFace å›¢é˜Ÿã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚
æ ¹æ® Apache è®¸å¯è¯ç¬¬ 2.0 ç‰ˆï¼ˆâ€œè®¸å¯è¯â€ï¼‰è·å¾—è®¸å¯ï¼›é™¤éç¬¦åˆè®¸å¯è¯çš„è§„å®šï¼Œå¦åˆ™æ‚¨ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ä½ç½®è·å–è®¸å¯è¯çš„å‰¯æœ¬
http://www.apache.org/licenses/LICENSE-2.0
é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼ŒæŒ‰â€œåŸæ ·â€åˆ†å‘çš„è½¯ä»¶æ ¹æ®è®¸å¯è¯åˆ†å‘ï¼Œå¹¶ä¸”æ²¡æœ‰ä»»ä½•å½¢å¼çš„æ‹…ä¿æˆ–æ¡ä»¶ã€‚è¯·å‚é˜…è®¸å¯è¯ä»¥äº†è§£ç‰¹å®šè¯­è¨€ä¸‹çš„æƒé™å’Œé™åˆ¶ã€‚å…·ä½“è¯­è¨€ä¸‹çš„æƒé™å’Œé™åˆ¶ã€‚
âš ï¸è¯·æ³¨æ„ï¼Œæ­¤æ–‡ä»¶æ˜¯ä½¿ç”¨ Markdown ç¼–å†™çš„ï¼Œä½†åŒ…å«æˆ‘ä»¬çš„æ–‡æ¡£ç”Ÿæˆå™¨ï¼ˆç±»ä¼¼äº MDXï¼‰çš„ç‰¹å®šè¯­æ³•ï¼Œå¯èƒ½æ— æ³•åœ¨æ‚¨çš„ Markdown æŸ¥çœ‹å™¨ä¸­æ­£ç¡®å‘ˆç°ã€‚
-->

# Fine-tune a pretrained model

[[open-in-colab]]

ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æœ‰å¾ˆå¤šå¥½å¤„ã€‚å®ƒé™ä½äº†è®¡ç®—æˆæœ¬å’Œç¢³æ’æ”¾é‡ï¼Œå¹¶ä¸”å¯ä»¥è®©æ‚¨ä½¿ç”¨æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œè€Œæ— éœ€ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªæ¨¡å‹ã€‚ğŸ¤— Transformers æä¾›äº†æ•°åƒä¸ªé’ˆå¯¹å„ç§ä»»åŠ¡çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚å½“æ‚¨ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æ—¶ï¼Œæ‚¨ä¼šåœ¨ä¸æ‚¨çš„ä»»åŠ¡ç›¸å…³çš„æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒã€‚è¿™è¢«ç§°ä¸ºå¾®è°ƒï¼Œæ˜¯ä¸€ç§éå¸¸å¼ºå¤§çš„è®­ç»ƒæŠ€æœ¯ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæ‚¨å°†ä½¿ç”¨æ‚¨é€‰æ‹©çš„æ·±åº¦å­¦ä¹ æ¡†æ¶å¯¹ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼š

* ä½¿ç”¨ğŸ¤— Transformers çš„ [`Trainer`] å¯¹ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚
* åœ¨ TensorFlow ä¸­ä½¿ç”¨ Keras å¯¹ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚
* åœ¨åŸç”Ÿ PyTorch ä¸­å¯¹ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚

<a id='data-processing'> </a>

## å‡†å¤‡æ•°æ®é›†

<Youtube id="_BZearw7f0w"/>

åœ¨æ‚¨å¯¹ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒä¹‹å‰ï¼Œéœ€è¦ä¸‹è½½ä¸€ä¸ªæ•°æ®é›†å¹¶ä¸ºå…¶è¿›è¡Œå‡†å¤‡ä»¥ç”¨äºè®­ç»ƒã€‚ä¹‹å‰çš„æ•™ç¨‹å‘æ‚¨å±•ç¤ºäº†å¦‚ä½•å¤„ç†ç”¨äºè®­ç»ƒçš„æ•°æ®ï¼Œç°åœ¨æ‚¨æœ‰æœºä¼šå°†è¿™äº›æŠ€å·§ä»˜è¯¸å®è·µï¼

é¦–å…ˆåŠ è½½ [Yelp Reviews](https://huggingface.co/datasets/yelp_review_full) æ•°æ®é›†ï¼š

```py
>>> from datasets import load_dataset

>>> dataset = load_dataset("yelp_review_full")
>>> dataset["train"][100]
{'label': 0,
 'text': 'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\nThe cashier took my friends\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\"serving off their orders\\" when they didn\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\nThe manager was rude when giving me my order. She didn\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\nI\'ve eaten at various McDonalds restaurants for over 30 years. I\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'}
```

æ­£å¦‚æ‚¨ç°åœ¨æ‰€çŸ¥é“çš„ï¼Œæ‚¨éœ€è¦ä¸€ä¸ªåˆ†è¯å™¨æ¥å¤„ç†æ–‡æœ¬ï¼Œå¹¶åŒ…æ‹¬å¡«å……å’Œæˆªæ–­ç­–ç•¥ä»¥å¤„ç†ä»»æ„é•¿åº¦çš„åºåˆ—ã€‚ä¸ºäº†ä¸€æ¬¡å¤„ç†æ•´ä¸ªæ•°æ®é›†ï¼Œä½¿ç”¨ğŸ¤— Datasets çš„ [`map`](https://huggingface.co/docs/datasets/process.html#map) æ–¹æ³•åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåº”ç”¨ä¸€ä¸ªé¢„å¤„ç†å‡½æ•°ï¼š

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")


>>> def tokenize_function(examples):
...     return tokenizer(examples["text"], padding="max_length", truncation=True)


>>> tokenized_datasets = dataset.map(tokenize_function, batched=True)
```

å¦‚æœæ„¿æ„çš„è¯ï¼Œæ‚¨å¯ä»¥åˆ›å»ºä¸€ä¸ªè¾ƒå°çš„å­é›†æ¥è¿›è¡Œå¾®è°ƒï¼Œä»¥å‡å°‘æ‰€éœ€çš„æ—¶é—´ï¼š
```py
>>> small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
>>> small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))
```

<a id='trainer'> </a>

## Train

æ­¤æ—¶ï¼Œæ‚¨åº”è¯¥æŒ‰ç…§æ‚¨æƒ³è¦ä½¿ç”¨çš„æ¡†æ¶å¯¹åº”çš„éƒ¨åˆ†è¿›è¡Œæ“ä½œã€‚æ‚¨å¯ä»¥ä½¿ç”¨å³ä¾§è¾¹æ çš„é“¾æ¥è·³è½¬åˆ°æ‚¨æƒ³è¦çš„éƒ¨åˆ† - å¦‚æœæ‚¨å¸Œæœ›éšè—ç‰¹å®šæ¡†æ¶çš„å…¨éƒ¨å†…å®¹ï¼Œåªéœ€ä½¿ç”¨è¯¥æ¡†æ¶å—é¡¶éƒ¨å³ä¾§çš„æŒ‰é’®ï¼

<frameworkcontent>
<pt>
<Youtube id="nvBXf7s7vTI"/>

## ä½¿ç”¨ PyTorch Trainer è¿›è¡Œè®­ç»ƒ

ğŸ¤— Transformers æä¾›äº†ä¸€ä¸ªé’ˆå¯¹è®­ç»ƒğŸ¤— Transformers æ¨¡å‹è¿›è¡Œä¼˜åŒ–çš„ [`Trainer`] ç±»ï¼Œä½¿å¾—æ— éœ€æ‰‹åŠ¨ç¼–å†™è®­ç»ƒå¾ªç¯å³å¯è½»æ¾å¼€å§‹è®­ç»ƒã€‚[` Trainer`] API æ”¯æŒå¹¿æ³›çš„è®­ç»ƒé€‰é¡¹å’ŒåŠŸèƒ½ï¼Œå¦‚æ—¥å¿—è®°å½•ã€æ¢¯åº¦ç´¯ç§¯å’Œæ··åˆç²¾åº¦ã€‚

é¦–å…ˆåŠ è½½æ¨¡å‹å¹¶æŒ‡å®šæœŸæœ›çš„æ ‡ç­¾æ•°é‡ã€‚æ ¹æ® Yelp Review [æ•°æ®é›†å¡ç‰‡](https://huggingface.co/datasets/yelp_review_full#data-fields)ï¼Œæ‚¨çŸ¥é“æœ‰äº”ä¸ªæ ‡ç­¾ï¼š

```py
>>> from transformers import AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=5)
```

<Tip>

æ‚¨ä¼šçœ‹åˆ°æœ‰å…³æŸäº›é¢„è®­ç»ƒæƒé‡æœªè¢«ä½¿ç”¨å’ŒæŸäº›æƒé‡éšæœºåˆå§‹åŒ–çš„è­¦å‘Šã€‚ä¸è¦æ‹…å¿ƒï¼Œè¿™æ˜¯å®Œå…¨æ­£å¸¸çš„ï¼BERT æ¨¡å‹çš„é¢„è®­ç»ƒå¤´éƒ¨è¢«ä¸¢å¼ƒï¼Œå¹¶ç”¨éšæœºåˆå§‹åŒ–çš„åˆ†ç±»å¤´éƒ¨æ›¿æ¢ã€‚æ‚¨å°†åœ¨æ‚¨çš„åºåˆ—åˆ†ç±»ä»»åŠ¡ä¸Šå¯¹è¿™ä¸ªæ–°çš„æ¨¡å‹å¤´éƒ¨è¿›è¡Œå¾®è°ƒï¼Œå°†é¢„è®­ç»ƒæ¨¡å‹çš„çŸ¥è¯†è½¬ç§»åˆ°å®ƒä¸Šé¢ã€‚

</Tip>

### è®­ç»ƒè¶…å‚æ•°

æ¥ä¸‹æ¥ï¼Œåˆ›å»ºä¸€ä¸ª [`TrainingArguments`] ç±»ï¼Œå…¶ä¸­åŒ…å«äº†æ‚¨å¯ä»¥è°ƒèŠ‚çš„æ‰€æœ‰è¶…å‚æ•°ï¼Œä»¥åŠç”¨äºæ¿€æ´»ä¸åŒè®­ç»ƒé€‰é¡¹çš„æ ‡å¿—ã€‚å¯¹äºæœ¬æ•™ç¨‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨é»˜è®¤çš„è®­ç»ƒ [è¶…å‚æ•°](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)ï¼Œä½†è¯·éšæ„å°è¯•ä¸åŒçš„è®¾ç½®ï¼Œæ‰¾åˆ°æœ€ä½³çš„é…ç½®ã€‚

æŒ‡å®šä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹çš„ä½ç½®ï¼š

```py
>>> from transformers import TrainingArguments

>>> training_args = TrainingArguments(output_dir="test_trainer")
```

### Evaluate

[`Trainer`] åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸ä¼šè‡ªåŠ¨è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚æ‚¨éœ€è¦ä¸º [`Trainer`] ä¼ é€’ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—å’ŒæŠ¥å‘ŠæŒ‡æ ‡ã€‚[ğŸ¤— Evaluate](https://huggingface.co/docs/evaluate/index) åº“æä¾›äº†ä¸€ä¸ªç®€å•çš„ [`accuracy`](https://huggingface.co/spaces/evaluate-metric/accuracy) å‡½æ•°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ [`evaluate.load`] å‡½æ•°åŠ è½½ï¼ˆæœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…æ­¤ [å¿«é€Ÿå…¥é—¨](https://huggingface.co/docs/evaluate/a_quick_tour)ï¼‰ï¼š

```py
>>> import numpy as np
>>> import evaluate

>>> metric = evaluate.load("accuracy")
```

åœ¨ `metric` ä¸Šè°ƒç”¨ [`~evaluate.compute`] è®¡ç®—æ‚¨çš„é¢„æµ‹çš„å‡†ç¡®ç‡ã€‚åœ¨å°†é¢„æµ‹ä¼ é€’ç»™ `compute` ä¹‹å‰ï¼Œæ‚¨éœ€è¦å°†é¢„æµ‹è½¬æ¢ä¸º logitsï¼ˆè®°ä½ï¼Œæ‰€æœ‰ğŸ¤— Transformers æ¨¡å‹è¿”å›çš„éƒ½æ˜¯ logitsï¼‰ï¼š

```py
>>> def compute_metrics(eval_pred):
...     logits, labels = eval_pred
...     predictions = np.argmax(logits, axis=-1)
...     return metric.compute(predictions=predictions, references=labels)
```

å¦‚æœæ‚¨å¸Œæœ›åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ç›‘æ§è¯„ä¼°æŒ‡æ ‡ï¼Œè¯·åœ¨è®­ç»ƒå‚æ•°ä¸­æŒ‡å®š `evaluation_strategy` å‚æ•°ï¼Œä»¥åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶æŠ¥å‘Šè¯„ä¼°æŒ‡æ ‡ï¼š

```py
>>> from transformers import TrainingArguments, Trainer

>>> training_args = TrainingArguments(output_dir="test_trainer", evaluation_strategy="epoch")
```

### Trainer

ä½¿ç”¨æ‚¨çš„æ¨¡å‹ã€è®­ç»ƒå‚æ•°ã€è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ä»¥åŠè¯„ä¼°å‡½æ•°åˆ›å»ºä¸€ä¸ª [`Trainer`] å¯¹è±¡ï¼š

```py
>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=small_train_dataset,
...     eval_dataset=small_eval_dataset,
...     compute_metrics=compute_metrics,
... )
```

ç„¶åé€šè¿‡è°ƒç”¨ [`~transformers.Trainer.train`] å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼š

```py
>>> trainer.train()
```
</pt>
<tf>
<a id='keras'> </a>

<Youtube id="rnTGBy2ax1c"/>

## ä½¿ç”¨ Keras è®­ç»ƒ TensorFlow æ¨¡å‹

æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ Keras API è®­ç»ƒğŸ¤— Transformers æ¨¡å‹ï¼

### ä¸º Keras åŠ è½½æ•°æ®

å½“æ‚¨æƒ³è¦ä½¿ç”¨ Keras API è®­ç»ƒä¸€ä¸ªğŸ¤— Transformers æ¨¡å‹æ—¶ï¼Œæ‚¨éœ€è¦å°†æ•°æ®é›†è½¬æ¢ä¸º Keras å¯ä»¥ç†è§£çš„æ ¼å¼ã€‚å¦‚æœæ‚¨çš„æ•°æ®é›†å¾ˆå°ï¼Œæ‚¨å¯ä»¥å°†æ•´ä¸ªæ•°æ®é›†è½¬æ¢ä¸º NumPy æ•°ç»„å¹¶å°†å…¶ä¼ é€’ç»™ Kerasã€‚åœ¨æ›´å¤æ‚çš„æ“ä½œä¹‹å‰ï¼Œè®©æˆ‘ä»¬é¦–å…ˆå°è¯•è¿™æ ·åšã€‚

é¦–å…ˆåŠ è½½ä¸€ä¸ªæ•°æ®é›†ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ [GLUE åŸºå‡†](https://huggingface.co/datasets/glue) ä¸­çš„ CoLA æ•°æ®é›†ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªç®€å•çš„äºŒåˆ†ç±»æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œç°åœ¨åªéœ€å–è®­ç»ƒé›†ã€‚

```py
from datasets import load_dataset

dataset = load_dataset("glue", "cola")
dataset = dataset["train"]  # ç°åœ¨åªå–è®­ç»ƒé›†
```

æ¥ä¸‹æ¥ï¼ŒåŠ è½½ä¸€ä¸ªåˆ†è¯å™¨å¹¶å°†æ•°æ®è¿›è¡Œåˆ†è¯ï¼Œå¾—åˆ° NumPy æ•°ç»„ã€‚è¯·æ³¨æ„ï¼Œæ ‡ç­¾å·²ç»æ˜¯ä¸€ä¸ªç”± 0 å’Œ 1 ç»„æˆçš„åˆ—è¡¨ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ç›´æ¥å°†å…¶è½¬æ¢ä¸º NumPy æ•°ç»„è€Œæ— éœ€è¿›è¡Œåˆ†è¯ï¼

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
tokenized_data = tokenizer(dataset["sentence"], return_tensors="np", padding=True)
# åˆ†è¯å™¨è¿”å›BatchEncodingï¼Œä½†æˆ‘ä»¬å°†å…¶è½¬æ¢ä¸ºKerasçš„dictæ ¼å¼
tokenized_data = dict(tokenized_data)

labels = np.array(dataset["label"])  # æ ‡ç­¾å·²ç»æ˜¯ä¸€ä¸ªç”±0å’Œ1ç»„æˆçš„æ•°ç»„
```

æœ€åï¼ŒåŠ è½½ã€[`compile`](https://keras.io/api/models/model_training_apis/#compile-method) å’Œ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) æ¨¡å‹ã€‚è¯·æ³¨æ„ï¼ŒğŸ¤— Transformers æ¨¡å‹éƒ½æœ‰ä¸€ä¸ªé»˜è®¤çš„ä¸ä»»åŠ¡ç›¸å…³çš„æŸå¤±å‡½æ•°ï¼Œå› æ­¤é™¤éæ‚¨æƒ³è¦æŒ‡å®šä¸€ä¸ªæŸå¤±å‡½æ•°ï¼Œå¦åˆ™ä¸éœ€è¦æŒ‡å®šï¼š

```py
from transformers import TFAutoModelForSequenceClassification
from tensorflow.keras.optimizers import Adam

# Load and compile our model
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased")
# Lower learning rates are often better for fine-tuning transformers
model.compile(optimizer=Adam(3e-5))  # No loss argument!

model.fit(tokenized_data, labels)
```

<Tip>

å½“æ‚¨è°ƒç”¨ `compile()` ç¼–è¯‘æ¨¡å‹æ—¶ï¼Œä¸éœ€è¦ä¸ºæ¨¡å‹ä¼ é€’æŸå¤±å‚æ•°ï¼å¦‚æœå°†è¯¥å‚æ•°ç•™ç©ºï¼ŒHugging Face æ¨¡å‹ä¼šè‡ªåŠ¨é€‰æ‹©é€‚åˆä»»åŠ¡å’Œæ¨¡å‹æ¶æ„çš„æŸå¤±å‡½æ•°ã€‚å¦‚æœæ‚¨å¸Œæœ›è‡ªè¡ŒæŒ‡å®šæŸå¤±å‡½æ•°ï¼Œæ‚¨å§‹ç»ˆå¯ä»¥è¦†ç›–æ­¤è¡Œä¸ºï¼

</Tip>

è¿™ç§æ–¹æ³•åœ¨è¾ƒå°çš„æ•°æ®é›†ä¸Šæ•ˆæœå¾ˆå¥½ï¼Œä½†å¯¹äºè¾ƒå¤§çš„æ•°æ®é›†ï¼Œæ‚¨å¯èƒ½ä¼šå‘ç°å®ƒå¼€å§‹æˆä¸ºä¸€ä¸ªé—®é¢˜ã€‚ä¸ºä»€ä¹ˆå‘¢ï¼Ÿå› ä¸ºåˆ†è¯åçš„æ•°ç»„å’Œæ ‡ç­¾éœ€è¦å®Œå…¨åŠ è½½åˆ°å†…å­˜ä¸­ï¼Œå¹¶ä¸”å› ä¸º NumPy ä¸èƒ½å¤„ç†â€œjaggedâ€æ•°ç»„ï¼Œæ‰€ä»¥æ¯ä¸ªåˆ†è¯åçš„æ ·æœ¬éƒ½å¿…é¡»å¡«å……åˆ°æ•´ä¸ªæ•°æ®é›†ä¸­æœ€é•¿æ ·æœ¬çš„é•¿åº¦ã€‚è¿™ä¼šä½¿æ‚¨çš„æ•°ç»„å˜å¾—æ›´å¤§ï¼Œå¹¶ä¸”æ‰€æœ‰è¿™äº›å¡«å……ä»¤ç‰Œä¹Ÿä¼šå‡æ…¢è®­ç»ƒé€Ÿåº¦ï¼

### å°†æ•°æ®åŠ è½½ä¸º tf.data.Dataset

å¦‚æœæ‚¨æƒ³é¿å…è®­ç»ƒé€Ÿåº¦å˜æ…¢ï¼Œæ‚¨å¯ä»¥å°†æ•°æ®åŠ è½½ä¸º `tf.data.Dataset`ã€‚è™½ç„¶æ‚¨å¯ä»¥è‡ªå·±ç¼–å†™ `tf.data` æµæ°´çº¿ï¼Œä½†æˆ‘ä»¬æœ‰ä¸¤ç§æ–¹ä¾¿çš„æ–¹æ³•å¯ä»¥å®ç°è¿™ä¸€ç‚¹ï¼š
- [`~TFPreTrainedModel.prepare_tf_dataset`]: è¿™æ˜¯æˆ‘ä»¬åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹æ¨èçš„æ–¹æ³•ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªæ–¹æ³•åœ¨æ‚¨çš„æ¨¡å‹ä¸Šï¼Œå®ƒå¯ä»¥æ£€æŸ¥æ¨¡å‹ä»¥è‡ªåŠ¨ç¡®å®šå“ªäº›åˆ—å¯ç”¨ä½œæ¨¡å‹è¾“å…¥ï¼Œå¹¶ä¸¢å¼ƒå…¶ä»–åˆ—ä»¥åˆ›å»ºä¸€ä¸ªæ›´ç®€å•ã€æ›´é«˜æ€§èƒ½çš„æ•°æ®é›†ã€‚
- [`~datasets.Dataset.to_tf_dataset`]: è¿™ç§æ–¹æ³•æ›´åº•å±‚ï¼Œé€‚ç”¨äºæ‚¨æƒ³è¦ç²¾ç¡®æ§åˆ¶æ•°æ®é›†åˆ›å»ºæ–¹å¼çš„æƒ…å†µï¼Œé€šè¿‡æŒ‡å®šè¦åŒ…æ‹¬çš„`columns`å’Œ`label_cols`ã€‚

åœ¨ä½¿ç”¨ [`~TFPreTrainedModel.prepare_tf_dataset`] ä¹‹å‰ï¼Œæ‚¨éœ€è¦å°†åˆ†è¯å™¨çš„è¾“å‡ºæ·»åŠ åˆ°æ•°æ®é›†ä¸­ä½œä¸ºåˆ—ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼šä»¥ä¸‹ä»£ç ç¤ºä¾‹ï¼š

```py
def tokenize_dataset(data):
    # Keys of the returned dictionary will be added to the dataset as columns
    return tokenizer(data["text"])


dataset = dataset.map(tokenize_dataset)
```

è¯·è®°ä½ï¼ŒHugging Face æ•°æ®é›†é»˜è®¤å­˜å‚¨åœ¨ç£ç›˜ä¸Šï¼Œå› æ­¤è¿™ä¸ä¼šå¢åŠ å†…å­˜ä½¿ç”¨é‡ï¼ä¸€æ—¦åˆ—è¢«æ·»åŠ ï¼Œæ‚¨å¯ä»¥ä»æ•°æ®é›†ä¸­æµå¼ä¼ è¾“æ‰¹æ¬¡å¹¶ä¸ºæ¯ä¸ªæ‰¹æ¬¡æ·»åŠ å¡«å……ï¼Œè¿™å¤§å¤§å‡å°‘äº†ä¸å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œå¡«å……ç›¸æ¯”çš„å¡«å……æ ‡è®°æ•°é‡ã€‚

```py
>>> tf_dataset = model.prepare_tf_dataset(dataset["train"], batch_size=16, shuffle=True, tokenizer=tokenizer)
```

è¯·æ³¨æ„ï¼Œåœ¨ä¸Šé¢çš„ä»£ç ç¤ºä¾‹ä¸­ï¼Œæ‚¨éœ€è¦å°†åˆ†è¯å™¨ä¼ é€’ç»™ `prepare_tf_dataset`ï¼Œä»¥ä¾¿åœ¨åŠ è½½æ‰¹æ¬¡æ—¶æ­£ç¡®å¡«å……æ‰¹æ¬¡ã€‚å¦‚æœæ•°æ®é›†ä¸­çš„æ‰€æœ‰æ ·æœ¬é•¿åº¦ç›¸åŒä¸”ä¸éœ€è¦å¡«å……ï¼Œåˆ™å¯ä»¥è·³è¿‡æ­¤å‚æ•°ã€‚å¦‚æœæ‚¨éœ€è¦æ‰§è¡Œæ¯”å¡«å……æ ·æœ¬æ›´å¤æ‚çš„æ“ä½œï¼ˆä¾‹å¦‚å¯¹é®è”½è¯­è¨€è¿›è¡Œç ´åï¼‰å»ºæ¨¡ï¼‰ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ `collate_fn` å‚æ•°å°†è°ƒç”¨ä¸€ä¸ªå‡½æ•°æ¥å°†æ ·æœ¬åˆ—è¡¨è½¬æ¢ä¸ºæ‰¹æ¬¡å¹¶åº”ç”¨ä»»ä½•é¢„å¤„ç†ã€‚è¯·å‚é˜…æˆ‘ä»¬çš„ [ç¤ºä¾‹](https://github.com/huggingface/transformers/tree/main/examples) æˆ– [ç¬”è®°æœ¬](https://huggingface.co/docs/transformers/notebooks) ä»¥æŸ¥çœ‹æ­¤æ–¹æ³•çš„å®é™…æ“ä½œã€‚

åˆ›å»ºäº†ä¸€ä¸ª `tf.data.Dataset` åï¼Œæ‚¨å¯ä»¥åƒä»¥å‰ä¸€æ ·ç¼–è¯‘å’Œè®­ç»ƒæ¨¡å‹ï¼š
```py
model.compile(optimizer=Adam(3e-5))  # No loss argument!

model.fit(tf_dataset)
```

</tf>
</frameworkcontent>
<a id='pytorch_native'> </a>

## åœ¨åŸç”Ÿ PyTorch ä¸­è®­ç»ƒ

<frameworkcontent> <pt>
<Youtube id="Dh9CL8fyG80"/>

[`Trainer`] è´Ÿè´£è®­ç»ƒå¾ªç¯ï¼Œå¹¶å…è®¸æ‚¨é€šè¿‡ä¸€è¡Œä»£ç è¿›è¡Œæ¨¡å‹å¾®è°ƒã€‚å¯¹äºå–œæ¬¢ç¼–å†™è‡ªå·±è®­ç»ƒå¾ªç¯çš„ç”¨æˆ·ï¼Œæ‚¨è¿˜å¯ä»¥åœ¨åŸç”Ÿ PyTorch ä¸­å¾®è°ƒğŸ¤— Transformers æ¨¡å‹ã€‚

æ­¤æ—¶ï¼Œæ‚¨å¯èƒ½éœ€è¦é‡æ–°å¯åŠ¨ç¬”è®°æœ¬æˆ–æ‰§è¡Œä»¥ä¸‹ä»£ç ä»¥é‡Šæ”¾ä¸€äº›å†…å­˜ï¼š

```py
del model
del trainer
torch.cuda.empty_cache()
```

æ¥ä¸‹æ¥ï¼Œæ‰‹åŠ¨å¯¹ `tokenized_dataset` è¿›è¡Œåå¤„ç†ï¼Œä»¥å‡†å¤‡è¿›è¡Œè®­ç»ƒã€‚
1. åˆ é™¤ `text` åˆ—ï¼Œå› ä¸ºæ¨¡å‹ä¸æ¥å—åŸå§‹æ–‡æœ¬ä½œä¸ºè¾“å…¥ï¼š
    ```py
    >>> tokenized_datasets = tokenized_datasets.remove_columns(["text"])
    ```
2. Rename the `label` column to `labels` because the model expects the argument to be named `labels`:


    ```py
    >>> tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
    ```
3. å°†æ•°æ®é›†çš„æ ¼å¼è®¾ç½®ä¸ºè¿”å› PyTorch å¼ é‡è€Œä¸æ˜¯åˆ—è¡¨ï¼š

```py
>>> tokenized_datasets.set_format("torch")
```
ç„¶åæŒ‰ç…§ä¹‹å‰å±•ç¤ºçš„æ–¹å¼åˆ›å»ºä¸€ä¸ªè¾ƒå°çš„æ•°æ®é›†å­é›†ï¼Œä»¥åŠ å¿«å¾®è°ƒçš„é€Ÿåº¦ï¼š

```py
>>> small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
>>> small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))
```

### DataLoader

ä¸ºæ‚¨çš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†åˆ›å»ºä¸€ä¸ª `DataLoader`ï¼Œä»¥ä¾¿æ‚¨å¯ä»¥è¿­ä»£å¤„ç†æ‰¹é‡æ•°æ®ï¼š

```py
>>> from torch.utils.data import DataLoader

>>> train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)
>>> eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)
```
ä½¿ç”¨é¢„æœŸæ ‡ç­¾æ•°é‡åŠ è½½æ‚¨çš„æ¨¡å‹ï¼š

```py
>>> from transformers import AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=5)
```

### Optimizer and learning rate scheduler

åˆ›å»ºä¸€ä¸ªä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨æ¥å¾®è°ƒæ¨¡å‹ã€‚è®©æˆ‘ä»¬ä½¿ç”¨ PyTorch ä¸­çš„ [`AdamW`](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) ä¼˜åŒ–å™¨ï¼š
```py
>>> from torch.optim import AdamW

>>> optimizer = AdamW(model.parameters(), lr=5e-5)
```

Create the default learning rate scheduler from [`Trainer`]:

```py
>>> from transformers import get_scheduler

>>> num_epochs = 3
>>> num_training_steps = num_epochs * len(train_dataloader)
>>> lr_scheduler = get_scheduler(
...     name="linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps
... )
```

æœ€åï¼Œå¦‚æœæ‚¨å¯ä»¥è®¿é—® GPUï¼Œè¯·æŒ‡å®š `device` ä»¥ä½¿ç”¨ GPUã€‚å¦åˆ™ï¼Œåœ¨ CPU ä¸Šè¿›è¡Œè®­ç»ƒå¯èƒ½éœ€è¦æ•°å°æ—¶è€Œä¸æ˜¯å‡ åˆ†é’Ÿã€‚

```py
>>> import torch

>>> device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
>>> model.to(device)
```

<Tip>

å¦‚æœæ‚¨æ²¡æœ‰ GPUï¼Œå¯ä»¥é€šè¿‡ä½¿ç”¨åƒ [Colaboratory](https://colab.research.google.com/) æˆ– [SageMaker StudioLab](https://studiolab.sagemaker.aws/) è¿™æ ·çš„åœ¨çº¿ç¬”è®°æœ¬æ¥è·å¾—å…è´¹çš„äº‘ GPU è®¿é—®ã€‚

</Tip>

å¤ªæ£’äº†ï¼Œç°åœ¨æ‚¨å·²ç»å‡†å¤‡å¥½å¼€å§‹è®­ç»ƒäº†ï¼ğŸ¥³

### è®­ç»ƒå¾ªç¯

ä¸ºäº†è·Ÿè¸ªè®­ç»ƒè¿›åº¦ï¼Œä½¿ç”¨ [tqdm](https://tqdm.github.io/) åº“åœ¨è®­ç»ƒæ­¥éª¤æ•°é‡ä¸Šæ·»åŠ ä¸€ä¸ªè¿›åº¦æ¡ï¼š

```py
>>> from tqdm.auto import tqdm

>>> progress_bar = tqdm(range(num_training_steps))

>>> model.train()
>>> for epoch in range(num_epochs):
...     for batch in train_dataloader:
...         batch = {k: v.to(device) for k, v in batch.items()}
...         outputs = model(**batch)
...         loss = outputs.loss
...         loss.backward()

...         optimizer.step()
...         lr_scheduler.step()
...         optimizer.zero_grad()
...         progress_bar.update(1)
```

### Evaluate

å°±åƒæ‚¨åœ¨ [`Trainer`] ä¸­æ·»åŠ äº†ä¸€ä¸ªè¯„ä¼°å‡½æ•°ä¸€æ ·ï¼Œå½“æ‚¨ç¼–å†™è‡ªå·±çš„è®­ç»ƒå¾ªç¯æ—¶ï¼Œæ‚¨éœ€è¦åšåŒæ ·çš„äº‹æƒ…ã€‚ä½†æ˜¯ï¼Œä¸åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶è®¡ç®—å’ŒæŠ¥å‘ŠæŒ‡æ ‡ä¸åŒï¼Œè¿™ä¸€æ¬¡æ‚¨å°†ä½¿ç”¨ [`~evaluate.add_batch`] æ¥ç´¯ç§¯æ‰€æœ‰æ‰¹æ¬¡ï¼Œå¹¶åœ¨æœ€åè®¡ç®—æŒ‡æ ‡ã€‚

```py
>>> import evaluate

>>> metric = evaluate.load("accuracy")
>>> model.eval()
>>> for batch in eval_dataloader:
...     batch = {k: v.to(device) for k, v in batch.items()}
...     with torch.no_grad():
...         outputs = model(**batch)

...     logits = outputs.logits
...     predictions = torch.argmax(logits, dim=-1)
...     metric.add_batch(predictions=predictions, references=batch["labels"])

>>> metric.compute()
```

</pt>
</frameworkcontent>

<a id='additional-resources'> </a>

## å…¶ä»–èµ„æº

è¦è·å–æ›´å¤šå¾®è°ƒç¤ºä¾‹ï¼Œè¯·å‚è€ƒä»¥ä¸‹èµ„æºï¼š

- [ğŸ¤— Transformers ç¤ºä¾‹ä»£ç åº“](https://github.com/huggingface/transformers/tree/main/examples)ï¼šåŒ…å«äº†ä½¿ç”¨ PyTorch å’Œ TensorFlow è®­ç»ƒå¸¸è§ NLP ä»»åŠ¡çš„ç¤ºä¾‹è„šæœ¬ã€‚
- [ğŸ¤— Transformers ç¬”è®°æœ¬](notebooks)ï¼šåŒ…å«äº†ä½¿ç”¨ PyTorch å’Œ TensorFlow ä¸ºç‰¹å®šä»»åŠ¡å¾®è°ƒæ¨¡å‹çš„å„ç§ç¬”è®°æœ¬ã€‚
