<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# ç”¨äº TensorFlow æ¨¡å‹çš„ XLA é›†æˆ

[[open-in-colab]]

åŠ é€Ÿçº¿æ€§ä»£æ•°ï¼Œä¹Ÿç§°ä¸ºXLAï¼Œæ˜¯ä¸€ä¸ªç”¨äºåŠ é€ŸTensorFlowæ¨¡å‹è¿è¡Œæ—¶é—´çš„ç¼–è¯‘å™¨ã€‚ä»[å®˜æ–¹æ–‡æ¡£](https://www.tensorflow.org/xla)ä¸­å¯ä»¥çœ‹åˆ°ï¼š

XLAï¼ˆåŠ é€Ÿçº¿æ€§ä»£æ•°ï¼‰æ˜¯ä¸€ç§é’ˆå¯¹çº¿æ€§ä»£æ•°çš„ç‰¹å®šé¢†åŸŸç¼–è¯‘å™¨ï¼Œå¯ä»¥åœ¨å¯èƒ½ä¸éœ€è¦æ›´æ”¹æºä»£ç çš„æƒ…å†µä¸‹åŠ é€ŸTensorFlowæ¨¡å‹ã€‚

åœ¨TensorFlowä¸­ä½¿ç”¨XLAéå¸¸ç®€å•â€”â€”å®ƒåŒ…å«åœ¨`tensorflow`åº“ä¸­ï¼Œå¹¶ä¸”å¯ä»¥ä½¿ç”¨ä»»ä½•å›¾åˆ›å»ºå‡½æ•°ä¸­çš„`jit_compile`å‚æ•°æ¥è§¦å‘ï¼Œä¾‹å¦‚[`tf.function`](https://www.tensorflow.org/guide/intro_to_graphs)ã€‚åœ¨ä½¿ç”¨Kerasæ–¹æ³•å¦‚`fit()`å’Œ`predict()`æ—¶ï¼Œåªéœ€å°†`jit_compile`å‚æ•°ä¼ é€’ç»™`model.compile()`å³å¯å¯ç”¨XLAã€‚ç„¶è€Œï¼ŒXLAä¸ä»…é™äºè¿™äº›æ–¹æ³• - å®ƒè¿˜å¯ä»¥ç”¨äºåŠ é€Ÿä»»ä½•ä»»æ„çš„`tf.function`ã€‚

åœ¨ğŸ¤— Transformersä¸­ï¼Œå‡ ä¸ªTensorFlowæ–¹æ³•å·²ç»è¢«é‡å†™ä¸ºä¸XLAå…¼å®¹ï¼ŒåŒ…æ‹¬[GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)ã€[T5](https://huggingface.co/docs/transformers/model_doc/t5)å’Œ[OPT](https://huggingface.co/docs/transformers/model_doc/opt)ç­‰æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œä»¥åŠ[Whisper](https://huggingface.co/docs/transformers/model_doc/whisper)ç­‰è¯­éŸ³å¤„ç†æ¨¡å‹ã€‚

è™½ç„¶ç¡®åˆ‡çš„åŠ é€Ÿå€æ•°å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºæ¨¡å‹ï¼Œä½†å¯¹äºğŸ¤— Transformersä¸­çš„TensorFlowæ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°é€Ÿåº¦æé«˜äº†çº¦100å€ã€‚æœ¬æ–‡æ¡£å°†è§£é‡Šå¦‚ä½•åœ¨è¿™äº›æ¨¡å‹ä¸Šä½¿ç”¨XLAè·å¾—æœ€å¤§çš„æ€§èƒ½ã€‚å¦‚æœæ‚¨æœ‰å…´è¶£äº†è§£æ›´å¤šå…³äºåŸºå‡†æµ‹è¯•å’Œæˆ‘ä»¬åœ¨XLAé›†æˆèƒŒåçš„è®¾è®¡å“²å­¦çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬è¿˜å°†æä¾›é¢å¤–çš„èµ„æºé“¾æ¥ã€‚


## ä½¿ç”¨ XLA è¿è¡Œ TensorFlow å‡½æ•°

è®©æˆ‘ä»¬è€ƒè™‘ä»¥ä¸‹TensorFlow ä¸­çš„æ¨¡å‹ï¼š

```py
import tensorflow as tf

model = tf.keras.Sequential(
    [tf.keras.layers.Dense(10, input_shape=(10,), activation="relu"), tf.keras.layers.Dense(5, activation="softmax")]
)
```

ä¸Šè¿°æ¨¡å‹æ¥å—ç»´åº¦ä¸º `(10,)` çš„è¾“å…¥ã€‚æˆ‘ä»¬å¯ä»¥åƒä¸‹é¢è¿™æ ·ä½¿ç”¨æ¨¡å‹è¿›è¡Œå‰å‘ä¼ æ’­ï¼š

```py
# Generate random inputs for the model.
batch_size = 16
input_vector_dim = 10
random_inputs = tf.random.normal((batch_size, input_vector_dim))

# Run a forward pass.
_ = model(random_inputs)
```

ä¸ºäº†ä½¿ç”¨ XLA ç¼–è¯‘çš„å‡½æ•°è¿è¡Œå‰å‘ä¼ æ’­ï¼Œæˆ‘ä»¬éœ€è¦æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

```py
xla_fn = tf.function(model, jit_compile=True)
_ = xla_fn(random_inputs)
```

`model`çš„é»˜è®¤`call()`å‡½æ•°ç”¨äºç¼–è¯‘XLAå›¾ã€‚ä½†å¦‚æœä½ æƒ³å°†å…¶ä»–æ¨¡å‹å‡½æ•°ç¼–è¯‘æˆXLAï¼Œä¹Ÿæ˜¯å¯ä»¥çš„ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```py
my_xla_fn = tf.function(model.my_xla_fn, jit_compile=True)
```

## åœ¨ğŸ¤— Transformersåº“ä¸­ä½¿ç”¨XLAè¿è¡ŒTensorFlowæ–‡æœ¬ç”Ÿæˆæ¨¡å‹

è¦åœ¨ğŸ¤— Transformersä¸­å¯ç”¨XLAåŠ é€Ÿç”Ÿæˆï¼Œæ‚¨éœ€è¦å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„`transformers`ã€‚æ‚¨å¯ä»¥é€šè¿‡è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥å®‰è£…å®ƒï¼š

```bash
pip install transformers --upgrade
```

ç„¶åæ‚¨å¯ä»¥è¿è¡Œä»¥ä¸‹ä»£ç ï¼š

```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForCausalLM

# Will error if the minimal version of Transformers is not installed.
from transformers.utils import check_min_version

check_min_version("4.21.0")


tokenizer = AutoTokenizer.from_pretrained("openai-community/gpt2", padding_side="left", pad_token="</s>")
model = TFAutoModelForCausalLM.from_pretrained("openai-community/gpt2")
input_string = ["TensorFlow is"]

# One line to create an XLA generation function
xla_generate = tf.function(model.generate, jit_compile=True)

tokenized_input = tokenizer(input_string, return_tensors="tf")
generated_tokens = xla_generate(**tokenized_input, num_beams=2)

decoded_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)
print(f"Generated -- {decoded_text}")
# Generated -- TensorFlow is an open-source, open-source, distributed-source application # framework for the
```

æ­£å¦‚æ‚¨æ‰€æ³¨æ„åˆ°çš„ï¼Œåœ¨`generate()`ä¸Šå¯ç”¨XLAåªéœ€è¦ä¸€è¡Œä»£ç ã€‚å…¶ä½™éƒ¨åˆ†ä»£ç ä¿æŒä¸å˜ã€‚ç„¶è€Œï¼Œä¸Šé¢çš„ä»£ç ç‰‡æ®µä¸­æœ‰ä¸€äº›ä¸XLAç›¸å…³çš„æ³¨æ„äº‹é¡¹ã€‚æ‚¨éœ€è¦äº†è§£è¿™äº›æ³¨æ„äº‹é¡¹ï¼Œä»¥å……åˆ†åˆ©ç”¨XLAå¯èƒ½å¸¦æ¥çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹é¢çš„éƒ¨åˆ†è®¨è®ºè¿™äº›å†…å®¹ã€‚

## éœ€è¦å…³æ³¨çš„æ³¨æ„äº‹é¡¹

å½“æ‚¨é¦–æ¬¡æ‰§è¡Œå¯ç”¨XLAçš„å‡½æ•°ï¼ˆå¦‚ä¸Šé¢çš„`xla_generate()`ï¼‰æ—¶ï¼Œå®ƒå°†åœ¨å†…éƒ¨å°è¯•æ¨æ–­è®¡ç®—å›¾ï¼Œè¿™æ˜¯ä¸€ä¸ªè€—æ—¶çš„è¿‡ç¨‹ã€‚è¿™ä¸ªè¿‡ç¨‹è¢«ç§°ä¸º[â€œtracingâ€](https://www.tensorflow.org/guide/intro_to_graphs#when_is_a_function_tracing)ã€‚

æ‚¨å¯èƒ½ä¼šæ³¨æ„åˆ°ç”Ÿæˆæ—¶é—´å¹¶ä¸å¿«ã€‚è¿ç»­è°ƒç”¨`xla_generate()`ï¼ˆæˆ–ä»»ä½•å…¶ä»–å¯ç”¨äº†XLAçš„å‡½æ•°ï¼‰ä¸éœ€è¦å†æ¬¡æ¨æ–­è®¡ç®—å›¾ï¼Œåªè¦å‡½æ•°çš„è¾“å…¥ä¸æœ€åˆæ„å»ºè®¡ç®—å›¾æ—¶çš„å½¢çŠ¶ç›¸åŒ¹é…ã€‚å¯¹äºå…·æœ‰å›ºå®šè¾“å…¥å½¢çŠ¶çš„æ¨¡æ€ï¼ˆä¾‹å¦‚å›¾åƒï¼‰ï¼Œè¿™ä¸æ˜¯é—®é¢˜ï¼Œä½†å¦‚æœæ‚¨æ­£åœ¨å¤„ç†å…·æœ‰å¯å˜è¾“å…¥å½¢çŠ¶çš„æ¨¡æ€ï¼ˆä¾‹å¦‚æ–‡æœ¬ï¼‰ï¼Œåˆ™å¿…é¡»æ³¨æ„ã€‚

ä¸ºäº†ç¡®ä¿`xla_generate()`å§‹ç»ˆä½¿ç”¨ç›¸åŒçš„è¾“å…¥å½¢çŠ¶ï¼Œæ‚¨å¯ä»¥åœ¨è°ƒç”¨`tokenizer`æ—¶æŒ‡å®š`padding`å‚æ•°ã€‚

```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("openai-community/gpt2", padding_side="left", pad_token="</s>")
model = TFAutoModelForCausalLM.from_pretrained("openai-community/gpt2")
input_string = ["TensorFlow is"]

xla_generate = tf.function(model.generate, jit_compile=True)

# Here, we call the tokenizer with padding options.
tokenized_input = tokenizer(input_string, pad_to_multiple_of=8, padding=True, return_tensors="tf")

generated_tokens = xla_generate(**tokenized_input, num_beams=2)
decoded_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)
print(f"Generated -- {decoded_text}")
```

é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ‚¨å¯ä»¥ç¡®ä¿`xla_generate()`çš„è¾“å…¥å§‹ç»ˆå…·æœ‰å®ƒè·Ÿè¸ªçš„å½¢çŠ¶ï¼Œä»è€ŒåŠ é€Ÿç”Ÿæˆæ—¶é—´ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç æ¥éªŒè¯è¿™ä¸€ç‚¹ï¼š

```py
import time
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("openai-community/gpt2", padding_side="left", pad_token="</s>")
model = TFAutoModelForCausalLM.from_pretrained("openai-community/gpt2")

xla_generate = tf.function(model.generate, jit_compile=True)

for input_string in ["TensorFlow is", "TensorFlow is a", "TFLite is a"]:
    tokenized_input = tokenizer(input_string, pad_to_multiple_of=8, padding=True, return_tensors="tf")
    start = time.time_ns()
    generated_tokens = xla_generate(**tokenized_input, num_beams=2)
    end = time.time_ns()
    print(f"Execution time -- {(end - start) / 1e6:.1f} ms\n")
```

åœ¨Tesla T4 GPUä¸Šï¼Œæ‚¨å¯ä»¥æœŸæœ›å¦‚ä¸‹çš„è¾“å‡ºï¼š

```bash
Execution time -- 30819.6 ms

Execution time -- 79.0 ms

Execution time -- 78.9 ms
```

ç¬¬ä¸€æ¬¡è°ƒç”¨`xla_generate()`ä¼šå› ä¸º`tracing`è€Œè€—æ—¶ï¼Œä½†åç»­çš„è°ƒç”¨ä¼šå¿«å¾—å¤šã€‚è¯·æ³¨æ„ï¼Œä»»ä½•æ—¶å€™å¯¹ç”Ÿæˆé€‰é¡¹çš„æ›´æ”¹éƒ½ä¼šè§¦å‘é‡æ–°`tracing`ï¼Œä»è€Œå¯¼è‡´ç”Ÿæˆæ—¶é—´å‡æ…¢ã€‚

åœ¨æœ¬æ–‡æ¡£ä¸­ï¼Œæˆ‘ä»¬æ²¡æœ‰æ¶µç›–ğŸ¤— Transformersæä¾›çš„æ‰€æœ‰æ–‡æœ¬ç”Ÿæˆé€‰é¡¹ã€‚æˆ‘ä»¬é¼“åŠ±æ‚¨é˜…è¯»æ–‡æ¡£ä»¥äº†è§£é«˜çº§ç”¨ä¾‹ã€‚

## é™„åŠ èµ„æº

ä»¥ä¸‹æ˜¯ä¸€äº›é™„åŠ èµ„æºï¼Œå¦‚æœæ‚¨æƒ³æ·±å…¥äº†è§£åœ¨ğŸ¤— Transformerså’Œå…¶ä»–åº“ä¸‹ä½¿ç”¨XLAï¼š

* [è¿™ä¸ªColab Notebook](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/91_tf_xla_generate.ipynb) æä¾›äº†ä¸€ä¸ªäº’åŠ¨æ¼”ç¤ºï¼Œè®©æ‚¨å¯ä»¥å°è¯•ä½¿ç”¨XLAå…¼å®¹çš„ç¼–ç å™¨-è§£ç å™¨ï¼ˆä¾‹å¦‚[T5](https://huggingface.co/docs/transformers/model_doc/t5)ï¼‰å’Œä»…è§£ç å™¨ï¼ˆä¾‹å¦‚[GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)ï¼‰æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ã€‚

* [è¿™ç¯‡åšå®¢æ–‡ç« ](https://huggingface.co/blog/tf-xla-generate) æä¾›äº†XLAå…¼å®¹æ¨¡å‹çš„æ¯”è¾ƒåŸºå‡†æ¦‚è¿°ï¼Œä»¥åŠå…³äºåœ¨TensorFlowä¸­ä½¿ç”¨XLAçš„å‹å¥½ä»‹ç»ã€‚

* [è¿™ç¯‡åšå®¢æ–‡ç« ](https://blog.tensorflow.org/2022/11/how-hugging-face-improved-text-generation-performance-with-xla.html) è®¨è®ºäº†æˆ‘ä»¬åœ¨ğŸ¤— Transformersä¸­ä¸ºTensorFlowæ¨¡å‹æ·»åŠ XLAæ”¯æŒçš„è®¾è®¡ç†å¿µã€‚

* æ¨èç”¨äºæ›´å¤šå­¦ä¹ XLAå’ŒTensorFlowå›¾çš„èµ„æºï¼š
    * [XLAï¼šé¢å‘æœºå™¨å­¦ä¹ çš„ä¼˜åŒ–ç¼–è¯‘å™¨](https://www.tensorflow.org/xla)
    * [å›¾å’Œtf.functionç®€ä»‹](https://www.tensorflow.org/guide/intro_to_graphs)
    * [ä½¿ç”¨tf.functionè·å¾—æ›´å¥½çš„æ€§èƒ½](https://www.tensorflow.org/guide/function)