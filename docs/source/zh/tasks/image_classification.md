<!--ç‰ˆæƒæ‰€æœ‰ 2022 å¹´ HuggingFace å›¢é˜Ÿä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚
æ ¹æ® Apache è®¸å¯è¯ç¬¬ 2.0 ç‰ˆï¼ˆâ€œè®¸å¯è¯â€ï¼‰è·å¾—è®¸å¯ï¼›é™¤éç¬¦åˆè®¸å¯è¯çš„è§„å®šï¼Œå¦åˆ™ä¸å¾—ä½¿ç”¨æœ¬æ–‡ä»¶ã€‚æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å–è®¸å¯è¯çš„å‰¯æœ¬
http://www.apache.org/licenses/LICENSE-2.0
é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œæ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯åŸºäºâ€œæŒ‰åŸæ ·â€åˆ†å‘çš„ï¼Œä¸é™„å¸¦ä»»ä½•å½¢å¼çš„æ‹…ä¿æˆ–æ¡ä»¶ã€‚è¯·å‚é˜…è®¸å¯è¯ä»¥è·å–ç‰¹å®šè¯­è¨€ä¸‹çš„æƒé™å’Œé™åˆ¶ã€‚ç‰¹åˆ«æ³¨æ„ï¼Œæ­¤æ–‡ä»¶æ˜¯ Markdown æ ¼å¼ï¼Œä½†åŒ…å«æˆ‘ä»¬çš„æ–‡æ¡£æ„å»ºå™¨ï¼ˆç±»ä¼¼äº MDXï¼‰çš„ç‰¹å®šè¯­æ³•ï¼Œå¯èƒ½æ— æ³•åœ¨ Markdown æŸ¥çœ‹å™¨ä¸­æ­£ç¡®
æ˜¾ç¤ºã€‚-->


# å›¾åƒåˆ†ç±»

[[åœ¨ Colab ä¸­æ‰“å¼€]]
<Youtube id="tjAIM7BOYhw"/>

å›¾åƒåˆ†ç±»æ˜¯å°†æ ‡ç­¾æˆ–ç±»åˆ«åˆ†é…ç»™å›¾åƒçš„è¿‡ç¨‹ã€‚ä¸æ–‡æœ¬æˆ–éŸ³é¢‘åˆ†ç±»ä¸åŒï¼Œè¾“å…¥æ˜¯æ„æˆå›¾åƒçš„åƒç´ å€¼ã€‚å›¾åƒåˆ†ç±»æœ‰è®¸å¤šåº”ç”¨ï¼Œä¾‹å¦‚åœ¨è‡ªç„¶ç¾å®³åæ£€æµ‹æŸåæƒ…å†µã€ç›‘æµ‹ä½œç‰©å¥åº·çŠ¶å†µæˆ–å¸®åŠ©ç­›æŸ¥åŒ»å­¦å›¾åƒä¸­çš„ç–¾ç—…è¿¹è±¡ã€‚æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š


1. åœ¨ [Food-101](https://huggingface.co/datasets/food101) æ•°æ®é›†ä¸Šå¾®è°ƒ [ViT](model_doc/vit) æ¨¡å‹ï¼Œä»¥å¯¹å›¾åƒä¸­çš„é£Ÿç‰©è¿›è¡Œåˆ†ç±»ã€‚2. ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚

<Tip> 

æœ¬æ•™ç¨‹å±•ç¤ºçš„ä»»åŠ¡ç”±ä»¥ä¸‹æ¨¡å‹æ¶æ„æ”¯æŒï¼š
<!--æ­¤æç¤ºç”±`make fix-copies`è‡ªåŠ¨ç”Ÿæˆï¼Œè¯·å‹¿æ‰‹åŠ¨å¡«å†™ï¼-->
[BEiT](../model_doc/beit), [BiT](../model_doc/bit), [ConvNeXT](../model_doc/convnext), [ConvNeXTV2](../model_doc/convnextv2), [CvT](../model_doc/cvt), [Data2VecVision](../model_doc/data2vec-vision), [DeiT](../model_doc/deit), [DiNAT](../model_doc/dinat), [EfficientFormer](../model_doc/efficientformer), [EfficientNet](../model_doc/efficientnet), [FocalNet](../model_doc/focalnet), [ImageGPT](../model_doc/imagegpt), [LeViT](../model_doc/levit), [MobileNetV1](../model_doc/mobilenet_v1), [MobileNetV2](../model_doc/mobilenet_v2), [MobileViT](../model_doc/mobilevit), [MobileViTV2](../model_doc/mobilevitv2), [NAT](../model_doc/nat), [Perceiver](../model_doc/perceiver), [PoolFormer](../model_doc/poolformer), [RegNet](../model_doc/regnet), [ResNet](../model_doc/resnet), [SegFormer](../model_doc/segformer), [SwiftFormer](../model_doc/swiftformer), [Swin Transformer](../model_doc/swin), [Swin Transformer V2](../model_doc/swinv2), [VAN](../model_doc/van), [ViT](../model_doc/vit), [ViT Hybrid](../model_doc/vit_hybrid), [ViTMSN](../model_doc/vit_msn) 
<!--ç”Ÿæˆæç¤ºçš„æœ«å°¾-->
</Tip>

å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š
```bash
pip install transformers datasets evaluate
```

æˆ‘ä»¬é¼“åŠ±æ‚¨ç™»å½• Hugging Face å¸æˆ·ï¼Œä¸ç¤¾åŒºå…±äº«å’Œä¸Šä¼ æ‚¨çš„æ¨¡å‹ã€‚å½“æç¤ºæ—¶ï¼Œè¯·è¾“å…¥æ‚¨çš„ä»¤ç‰Œä»¥ç™»å½•ï¼š
```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## åŠ è½½ Food-101 æ•°æ®é›†

é¦–å…ˆï¼Œä»ğŸ¤— Datasets åº“ä¸­åŠ è½½ Food-101 æ•°æ®é›†çš„è¾ƒå°å­é›†ã€‚è¿™æ ·å¯ä»¥è®©æ‚¨æœ‰æœºä¼šåœ¨è®­ç»ƒå®Œæ•´æ•°æ®é›†ä¹‹å‰è¿›è¡Œå®éªŒå’Œç¡®ä¿ä¸€åˆ‡æ­£å¸¸ã€‚ä½¿ç”¨ [`~datasets.Dataset.train_test_split`] æ–¹æ³•å°†æ•°æ®é›†çš„â€œtrainâ€éƒ¨åˆ†æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š
```py
>>> from datasets import load_dataset

>>> food = load_dataset("food101", split="train[:5000]")
```

ç„¶åï¼ŒæŸ¥çœ‹ä¸€ä¸ªç¤ºä¾‹ï¼š
```py
>>> food = food.train_test_split(test_size=0.2)
```

Then take a look at an example:

```py
>>> food["train"][0]
{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x512 at 0x7F52AFC8AC50>,
 'label': 79}
```

æ•°æ®é›†ä¸­çš„æ¯ä¸ªç¤ºä¾‹éƒ½åŒ…å«ä¸¤ä¸ªå­—æ®µï¼š

- `image`ï¼šé£Ÿç‰©é¡¹ç›®çš„ PIL å›¾åƒ
- `label`ï¼šé£Ÿç‰©é¡¹ç›®çš„æ ‡ç­¾ç±»åˆ«

ä¸ºäº†ä½¿æ¨¡å‹èƒ½å¤Ÿä»æ ‡ç­¾ ID è·å–æ ‡ç­¾åç§°ï¼Œåˆ›å»ºä¸€ä¸ªå°†æ ‡ç­¾åç§°æ˜ å°„åˆ°æ•´æ•°åŠå…¶åå‘æ˜ å°„çš„å­—å…¸ï¼š
```py
>>> labels = food["train"].features["label"].names
>>> label2id, id2label = dict(), dict()
>>> for i, label in enumerate(labels):
...     label2id[label] = str(i)
...     id2label[str(i)] = label
```

ç°åœ¨ï¼Œæ‚¨å¯ä»¥å°†æ ‡ç­¾ ID è½¬æ¢ä¸ºæ ‡ç­¾åç§°ï¼š
```py
>>> id2label[str(79)]
'prime_rib'
```

## é¢„å¤„ç†

æ¥ä¸‹æ¥ï¼ŒåŠ è½½ä¸€ä¸ª ViT å›¾åƒå¤„ç†å™¨ (Image Processor)ï¼Œå°†å›¾åƒå¤„ç†ä¸ºå¼ é‡ï¼š
```py
>>> from transformers import AutoImageProcessor

>>> checkpoint = "google/vit-base-patch16-224-in21k"
>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint)
```


<frameworkcontent> 
<pt> 

 å¯¹å›¾åƒåº”ç”¨ä¸€äº›å˜æ¢ï¼Œä½¿æ¨¡å‹å¯¹è¿‡åº¦æ‹Ÿåˆæ›´å…·é²æ£’æ€§ã€‚è¿™é‡Œä½¿ç”¨ torchvision çš„ [`transforms`](https://pytorch.org/vision/stable/transforms.html) æ¨¡å—ï¼Œä½†æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨å…¶ä»–å–œæ¬¢çš„å›¾åƒåº“ã€‚

éšæœºè£å‰ªå›¾åƒçš„ä¸€éƒ¨åˆ†ï¼Œè°ƒæ•´å¤§å°ï¼Œå¹¶ä½¿ç”¨å›¾åƒçš„å¹³å‡å€¼å’Œæ ‡å‡†å·®è¿›è¡Œå½’ä¸€åŒ–ï¼š
```py
>>> from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor

>>> normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)
>>> size = (
...     image_processor.size["shortest_edge"]
...     if "shortest_edge" in image_processor.size
...     else (image_processor.size["height"], image_processor.size["width"])
... )
>>> _transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])
```

ç„¶ååˆ›å»ºä¸€ä¸ªé¢„å¤„ç†å‡½æ•°ï¼Œåº”ç”¨å˜æ¢å¹¶è¿”å›å›¾åƒçš„ `pixel_values`ï¼ˆæ¨¡å‹çš„è¾“å…¥ï¼‰ï¼š
```py
>>> def transforms(examples):
...     examples["pixel_values"] = [_transforms(img.convert("RGB")) for img in examples["image"]]
...     del examples["image"]
...     return examples
```

è¦åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåº”ç”¨é¢„å¤„ç†å‡½æ•°ï¼Œä½¿ç”¨ğŸ¤— Datasets çš„ [`~datasets.Dataset.with_transform`] æ–¹æ³•ã€‚

å½“æ‚¨åŠ è½½æ•°æ®é›†çš„å…ƒç´ æ—¶ï¼Œå˜æ¢å°†åœ¨

```py
>>> food = food.with_transform(transforms)
```

åŠ è½½æ—¶åŠ¨æ€åº”ç”¨ã€‚
```py
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator()
```
</pt> </frameworkcontent>

<frameworkcontent> 
<tf>

ä¸ºäº†é¿å…è¿‡æ‹Ÿåˆå¹¶ä½¿æ¨¡å‹æ›´å…·é²æ£’æ€§ï¼Œåœ¨è®­ç»ƒæ•°æ®é›†ä¸­æ·»åŠ ä¸€äº›æ•°æ®å¢å¼ºã€‚è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ Keras é¢„å¤„ç†å±‚æ¥å®šä¹‰è®­ç»ƒæ•°æ®çš„å˜æ¢ï¼ˆåŒ…æ‹¬æ•°æ®å¢å¼ºï¼‰ï¼Œä»¥åŠéªŒè¯æ•°æ®çš„å˜æ¢ï¼ˆä»…åŒ…æ‹¬ä¸­å¿ƒè£å‰ªã€è°ƒæ•´å¤§å°å’Œå½’ä¸€åŒ–ï¼‰ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ `tf.image` æˆ–å…¶ä»–æ‚¨å–œæ¬¢çš„åº“ã€‚
```py
>>> from tensorflow import keras
>>> from tensorflow.keras import layers

>>> size = (image_processor.size["height"], image_processor.size["width"])

>>> train_data_augmentation = keras.Sequential(
...     [
...         layers.RandomCrop(size[0], size[1]),
...         layers.Rescaling(scale=1.0 / 127.5, offset=-1),
...         layers.RandomFlip("horizontal"),
...         layers.RandomRotation(factor=0.02),
...         layers.RandomZoom(height_factor=0.2, width_factor=0.2),
...     ],
...     name="train_data_augmentation",
... )

>>> val_data_augmentation = keras.Sequential(
...     [
...         layers.CenterCrop(size[0], size[1]),
...         layers.Rescaling(scale=1.0 / 127.5, offset=-1),
...     ],
...     name="val_data_augmentation",
... )
```

æ¥ä¸‹æ¥ï¼Œåˆ›å»ºæ‰¹é‡å›¾åƒçš„é€‚å½“å˜æ¢å‡½æ•°ï¼Œè€Œä¸æ˜¯é€ä¸ªå›¾åƒè¿›è¡Œå˜æ¢ã€‚
```py
>>> import numpy as np
>>> import tensorflow as tf
>>> from PIL import Image


>>> def convert_to_tf_tensor(image: Image):
...     np_image = np.array(image)
...     tf_image = tf.convert_to_tensor(np_image)
...     # `expand_dims()` is used to add a batch dimension since
...     # the TF augmentation layers operates on batched inputs.
...     return tf.expand_dims(tf_image, 0)


>>> def preprocess_train(example_batch):
...     """Apply train_transforms across a batch."""
...     images = [
...         train_data_augmentation(convert_to_tf_tensor(image.convert("RGB"))) for image in example_batch["image"]
...     ]
...     example_batch["pixel_values"] = [tf.transpose(tf.squeeze(image)) for image in images]
...     return example_batch


... def preprocess_val(example_batch):
...     """Apply val_transforms across a batch."""
...     images = [
...         val_data_augmentation(convert_to_tf_tensor(image.convert("RGB"))) for image in example_batch["image"]
...     ]
...     example_batch["pixel_values"] = [tf.transpose(tf.squeeze(image)) for image in images]
...     return example_batch
```

ä½¿ç”¨ğŸ¤— Datasets çš„ [`~datasets.Dataset.set_transform`] æ–¹æ³•ï¼ŒåŠ¨æ€åº”ç”¨å˜æ¢ï¼š
```py
food["train"].set_transform(preprocess_train)
food["test"].set_transform(preprocess_val)
```

ä½œä¸ºæœ€åé¢„å¤„ç†æ­¥éª¤ï¼Œä½¿ç”¨ `DefaultDataCollator` åˆ›å»ºä¸€ä¸ªç¤ºä¾‹æ‰¹æ¬¡ã€‚ä¸ğŸ¤— Transformers ä¸­çš„å…¶ä»–æ•°æ®æ•´ç†å™¨ä¸åŒï¼Œ`DefaultDataCollator` ä¸ä¼šåº”ç”¨é¢å¤–çš„é¢„å¤„ç†ï¼Œå¦‚å¡«å……ã€‚</tf>
```py
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator(return_tensors="tf")
```
</tf>
</frameworkcontent>

## è¯„ä¼°

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒ…å«åº¦é‡æŒ‡æ ‡é€šå¸¸æœ‰åŠ©äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) åº“å¿«é€ŸåŠ è½½ä¸€ä¸ªè¯„ä¼°æ–¹æ³•ã€‚

å¯¹äºæ­¤ä»»åŠ¡ï¼Œè¯·åŠ è½½ [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy) åº¦é‡æŒ‡æ ‡ï¼ˆè¯·å‚é˜…ğŸ¤— Evaluate [å¿«é€Ÿå¯¼è§ˆ](https://huggingface.co/docs/evaluate/a_quick_tour) ä»¥äº†è§£æœ‰å…³å¦‚ä½•åŠ è½½å’Œè®¡ç®—åº¦é‡æŒ‡æ ‡çš„æ›´å¤šä¿¡æ¯ï¼‰ï¼š

```py
>>> import evaluate

>>> accuracy = evaluate.load("accuracy")
```

ç„¶ååˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œå°†æ‚¨çš„é¢„æµ‹å’Œæ ‡ç­¾ä¼ é€’ç»™ [`~evaluate.EvaluationModule.compute`] ä»¥è®¡ç®—å‡†ç¡®åº¦ï¼š
```py
>>> import numpy as np


>>> def compute_metrics(eval_pred):
...     predictions, labels = eval_pred
...     predictions = np.argmax(predictions, axis=1)
...     return accuracy.compute(predictions=predictions, references=labels)
```

æ‚¨çš„ `compute_metrics` å‡½æ•°å·²å‡†å¤‡å°±ç»ªï¼Œå½“è®¾ç½®è®­ç»ƒæ—¶ï¼Œæ‚¨å°†è¿”å›å®ƒã€‚

## è®­ç»ƒ

<frameworkcontent>
<pt>
<Tip>

å¦‚æœæ‚¨å¯¹ä½¿ç”¨ [`Trainer`] å¾®è°ƒæ¨¡å‹ä¸ç†Ÿæ‚‰ï¼Œè¯·å‚é˜…æ­¤å¤„çš„åŸºæœ¬æ•™ç¨‹ [here](../training#train-with-pytorch-trainer)ï¼
</Tip>

ç°åœ¨ï¼Œæ‚¨å¯ä»¥å¼€å§‹è®­ç»ƒæ¨¡å‹äº†ï¼ä½¿ç”¨ [`AutoModelForImageClassification`] åŠ è½½ ViTã€‚æŒ‡å®šæ ‡ç­¾æ•°å’Œé¢„æœŸæ ‡ç­¾æ•°ä»¥åŠæ ‡ç­¾æ˜ å°„ï¼š
```py
>>> from transformers import AutoModelForImageClassification, TrainingArguments, Trainer

>>> model = AutoModelForImageClassification.from_pretrained(
...     checkpoint,
...     num_labels=len(labels),
...     id2label=id2label,
...     label2id=label2id,
... )
```

æ­¤æ—¶ï¼Œåªå‰©ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š

1. åœ¨ [`TrainingArguments`] ä¸­å®šä¹‰æ‚¨çš„è®­ç»ƒè¶…å‚æ•°ã€‚é‡è¦çš„æ˜¯ä¸è¦ç§»é™¤æœªä½¿ç”¨çš„åˆ—ï¼Œå› ä¸ºé‚£æ ·ä¼šåˆ é™¤ `image` åˆ—ã€‚æ²¡æœ‰ `image` åˆ—ï¼Œæ‚¨å°±æ— æ³•åˆ›å»º `pixel_values`ã€‚è®¾ç½® `remove_unused_columns=False` ä»¥é˜»æ­¢æ­¤è¡Œä¸ºï¼å”¯ä¸€å…¶ä»–å¿…éœ€çš„å‚æ•°æ˜¯ `output_dir`ï¼Œå®ƒæŒ‡å®šäº†ä¿å­˜æ¨¡å‹çš„ä½ç½®ã€‚æ‚¨å¯ä»¥é€šè¿‡è®¾ç½® `push_to_hub=True` å°†æ­¤æ¨¡å‹æ¨é€åˆ° Hubï¼ˆæ‚¨éœ€è¦ç™»å½• Hugging Face æ‰èƒ½ä¸Šä¼ æ¨¡å‹ï¼‰ã€‚åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶ï¼Œ[`Trainer`] å°†è¯„ä¼°å‡†ç¡®æ€§å¹¶ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹ã€‚
2. å°†è®­ç»ƒå‚æ•°ä¸æ¨¡å‹ã€æ•°æ®é›†ã€åˆ†è¯å™¨ (Tokenizer)ã€æ•°æ®æ•´ç†å™¨å’Œ `compute_metrics` å‡½æ•°ä¸€èµ·ä¼ é€’ç»™ [`Trainer`]ã€‚
3. è°ƒç”¨ [`~Trainer.train`] æ¥å¾®è°ƒæ‚¨çš„æ¨¡å‹ã€‚

```py
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_food_model",
...     remove_unused_columns=False,
...     evaluation_strategy="epoch",
...     save_strategy="epoch",
...     learning_rate=5e-5,
...     per_device_train_batch_size=16,
...     gradient_accumulation_steps=4,
...     per_device_eval_batch_size=16,
...     num_train_epochs=3,
...     warmup_ratio=0.1,
...     logging_steps=10,
...     load_best_model_at_end=True,
...     metric_for_best_model="accuracy",
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     data_collator=data_collator,
...     train_dataset=food["train"],
...     eval_dataset=food["test"],
...     tokenizer=image_processor,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨ [`~transformers.Trainer.push_to_hub`] æ–¹æ³•å°†æ‚¨çš„æ¨¡å‹å…±äº«åˆ° Hubï¼Œè®©æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨æ‚¨çš„æ¨¡å‹ï¼š
```py
>>> trainer.push_to_hub()
```
</pt>
</frameworkcontent>

<frameworkcontent>
<tf>

<Tip>

å¦‚æœæ‚¨å¯¹ä½¿ç”¨ Keras è¿›è¡Œæ¨¡å‹å¾®è°ƒä¸ç†Ÿæ‚‰ï¼Œè¯·å…ˆæŸ¥çœ‹ [åŸºæœ¬æ•™ç¨‹](./training#train-a-tensorflow-model-with-keras)ï¼
</Tip>

åœ¨ TensorFlow ä¸­è¿›è¡Œæ¨¡å‹å¾®è°ƒï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œï¼š

1. å®šä¹‰è®­ç»ƒè¶…å‚æ•°ï¼Œå¹¶è®¾ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦ã€‚
2. å®ä¾‹åŒ–ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹ã€‚
3. å°†ğŸ¤—æ•°æ®é›†è½¬æ¢ä¸º `tf.data.Dataset`ã€‚
4. ç¼–è¯‘æ‚¨çš„æ¨¡å‹ã€‚
5. æ·»åŠ å›è°ƒå‡½æ•°ï¼Œå¹¶ä½¿ç”¨ `fit()` æ–¹æ³•è¿è¡Œè®­ç»ƒã€‚
6. å°†æ¨¡å‹ä¸Šä¼ åˆ°ğŸ¤— Hub ä¸ç¤¾åŒºå…±äº«ã€‚

é¦–å…ˆï¼Œå®šä¹‰è¶…å‚æ•°ã€ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦ï¼š

```py
>>> from transformers import create_optimizer

>>> batch_size = 16
>>> num_epochs = 5
>>> num_train_steps = len(food["train"]) * num_epochs
>>> learning_rate = 3e-5
>>> weight_decay_rate = 0.01

>>> optimizer, lr_schedule = create_optimizer(
...     init_lr=learning_rate,
...     num_train_steps=num_train_steps,
...     weight_decay_rate=weight_decay_rate,
...     num_warmup_steps=0,
... )
```

ç„¶åï¼Œä½¿ç”¨ [`TFAutoModelForImageClassification`] åŠ è½½ ViT ä»¥åŠæ ‡ç­¾æ˜ å°„ï¼š
```py
>>> from transformers import TFAutoModelForImageClassification

>>> model = TFAutoModelForImageClassification.from_pretrained(
...     checkpoint,
...     id2label=id2label,
...     label2id=label2id,
... )
```

ä½¿ç”¨ [`~datasets.Dataset.to_tf_dataset`] å’Œæ‚¨çš„ `data_collator` å°†æ•°æ®é›†è½¬æ¢ä¸º `tf.data.Dataset` æ ¼å¼ï¼š
```py
>>> # converting our train dataset to tf.data.Dataset
>>> tf_train_dataset = food["train"].to_tf_dataset(
...     columns="pixel_values", label_cols="label", shuffle=True, batch_size=batch_size, collate_fn=data_collator
... )

>>> # converting our test dataset to tf.data.Dataset
>>> tf_eval_dataset = food["test"].to_tf_dataset(
...     columns="pixel_values", label_cols="label", shuffle=True, batch_size=batch_size, collate_fn=data_collator
... )
```

ä½¿ç”¨ `compile()` é…ç½®æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼š
```py
>>> from tensorflow.keras.losses import SparseCategoricalCrossentropy

>>> loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
>>> model.compile(optimizer=optimizer, loss=loss)
```

è¦ä»é¢„æµ‹ä¸­è®¡ç®—å‡†ç¡®æ€§å¹¶å°†æ¨¡å‹æ¨é€åˆ°ğŸ¤— Hubï¼Œä½¿ç”¨ [Keras å›è°ƒ](../main_classes/keras_callbacks)ã€‚å°†æ‚¨çš„ `compute_metrics` å‡½æ•°ä¼ é€’ç»™ [KerasMetricCallback](../main_classes/keras_callbacks#transformers.KerasMetricCallback)ï¼Œå¹¶ä½¿ç”¨ [PushToHubCallback](../main_classes/keras_callbacks#transformers.PushToHubCallback) ä¸Šä¼ æ¨¡å‹ï¼š
```py
>>> from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback

>>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_eval_dataset)
>>> push_to_hub_callback = PushToHubCallback(
...     output_dir="food_classifier",
...     tokenizer=image_processor,
...     save_strategy="no",
... )
>>> callbacks = [metric_callback, push_to_hub_callback]
```

æœ€åï¼Œæ‚¨å·²ç»å‡†å¤‡å¥½è®­ç»ƒæ¨¡å‹äº†ï¼ä½¿ç”¨æ‚¨çš„è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ã€epoch æ•°é‡å’Œå›è°ƒå‡½æ•°æ¥å¾®è°ƒæ¨¡å‹ï¼šå’Œä½ çš„å›è°ƒå‡½æ•°å¾®è°ƒæ¨¡å‹ï¼š
```py
>>> model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=num_epochs, callbacks=callbacks)
Epoch 1/5
250/250 [==============================] - 313s 1s/step - loss: 2.5623 - val_loss: 1.4161 - accuracy: 0.9290
Epoch 2/5
250/250 [==============================] - 265s 1s/step - loss: 0.9181 - val_loss: 0.6808 - accuracy: 0.9690
Epoch 3/5
250/250 [==============================] - 252s 1s/step - loss: 0.3910 - val_loss: 0.4303 - accuracy: 0.9820
Epoch 4/5
250/250 [==============================] - 251s 1s/step - loss: 0.2028 - val_loss: 0.3191 - accuracy: 0.9900
Epoch 5/5
250/250 [==============================] - 238s 949ms/step - loss: 0.1232 - val_loss: 0.3259 - accuracy: 0.9890
```

æ­å–œï¼æ‚¨å·²ç»å¯¹æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œå¹¶åœ¨ğŸ¤— Hub ä¸Šå…±äº«äº†å®ƒã€‚ç°åœ¨æ‚¨å¯ä»¥ç”¨å®ƒè¿›è¡Œæ¨ç†äº†ï¼
</tf>
 </frameworkcontent>

<Tip>

æœ‰å…³å¦‚ä½•ä¸ºå›¾åƒåˆ†ç±»å¾®è°ƒæ¨¡å‹çš„æ›´è¯¦ç»†ç¤ºä¾‹ï¼Œè¯·å‚é˜…ç›¸åº”çš„ [PyTorch ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)ã€‚
</Tip>

## æ¨ç†

å¤ªæ£’äº†ï¼Œç°åœ¨æ‚¨å·²ç»å¯¹æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œå¯ä»¥ç”¨å®ƒè¿›è¡Œæ¨ç†äº†ï¼

åŠ è½½è¦è¿›è¡Œæ¨ç†çš„å›¾åƒï¼š

```py
>>> ds = load_dataset("food101", split="validation[:10]")
>>> image = ds["image"][0]
```

<div class="flex justify-center">    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png" alt="image of beignets"/> </div>

å°è¯•ä½¿ç”¨ [`pipeline`] å¯¹å¾®è°ƒçš„æ¨¡å‹è¿›è¡Œæ¨ç†æ˜¯æœ€ç®€å•çš„æ–¹æ³•ã€‚ä½¿ç”¨æ‚¨çš„æ¨¡å‹å®ä¾‹åŒ–ä¸€ä¸ªå›¾åƒåˆ†ç±»çš„ `pipeline`ï¼Œå¹¶å°†å›¾åƒä¼ é€’ç»™å®ƒï¼š

```py
>>> from transformers import pipeline

>>> classifier = pipeline("image-classification", model="my_awesome_food_model")
>>> classifier(image)
[{'score': 0.31856709718704224, 'label': 'beignets'},
 {'score': 0.015232225880026817, 'label': 'bruschetta'},
 {'score': 0.01519392803311348, 'label': 'chicken_wings'},
 {'score': 0.013022331520915031, 'label': 'pork_chop'},
 {'score': 0.012728818692266941, 'label': 'prime_rib'}]
```

å¦‚æœæ‚¨æ„¿æ„ï¼Œä¹Ÿå¯ä»¥æ‰‹åŠ¨å¤åˆ¶ `pipeline` çš„ç»“æœï¼š

<frameworkcontent> 
<pt> 

 åŠ è½½å›¾åƒå¤„ç†å™¨ (Image Processor)ä»¥é¢„å¤„ç†å›¾åƒå¹¶å°† `input` è¿”å›ä¸º PyTorch å¼ é‡ï¼š

```py
>>> from transformers import AutoImageProcessor
>>> import torch

>>> image_processor = AutoImageProcessor.from_pretrained("my_awesome_food_model")
>>> inputs = image_processor(image, return_tensors="pt")
```

å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å¹¶è¿”å› logitsï¼š
```py
>>> from transformers import AutoModelForImageClassification

>>> model = AutoModelForImageClassification.from_pretrained("my_awesome_food_model")
>>> with torch.no_grad():
...     logits = model(**inputs).logits
```

è·å–æ¦‚ç‡æœ€é«˜çš„é¢„æµ‹æ ‡ç­¾ï¼Œå¹¶ä½¿ç”¨æ¨¡å‹çš„ `id2label` æ˜ å°„å°†å…¶è½¬æ¢ä¸ºæ ‡ç­¾ï¼š
```py
>>> predicted_label = logits.argmax(-1).item()
>>> model.config.id2label[predicted_label]
'beignets'
```
</pt> 
</frameworkcontent>
<frameworkcontent> 
<tf> 

åŠ è½½å›¾åƒå¤„ç†å™¨ (Image Processor)ä»¥é¢„å¤„ç†å›¾åƒå¹¶å°† `input` è¿”å›ä¸º TensorFlow å¼ é‡ï¼š

```py
>>> from transformers import AutoImageProcessor

>>> image_processor = AutoImageProcessor.from_pretrained("MariaK/food_classifier")
>>> inputs = image_processor(image, return_tensors="tf")
```

å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å¹¶è¿”å› logitsï¼š
```py
>>> from transformers import TFAutoModelForImageClassification

>>> model = TFAutoModelForImageClassification.from_pretrained("MariaK/food_classifier")
>>> logits = model(**inputs).logits
```

è·å–æ¦‚ç‡æœ€é«˜çš„é¢„æµ‹æ ‡ç­¾ï¼Œå¹¶ä½¿ç”¨æ¨¡å‹çš„ `id2label` æ˜ å°„å°†å…¶è½¬æ¢ä¸ºæ ‡ç­¾ï¼š
```py
>>> predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])
>>> model.config.id2label[predicted_class_id]
'beignets'
```

</tf>
</frameworkcontent>

