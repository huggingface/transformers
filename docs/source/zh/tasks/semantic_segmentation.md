<!--ç‰ˆæƒæ‰€æœ‰2022å¹´HuggingFaceå›¢é˜Ÿä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚-->
æ ¹æ® Apache è®¸å¯è¯ç¬¬ 2.0 ç‰ˆï¼ˆâ€œè®¸å¯è¯â€ï¼‰è·å¾—è®¸å¯ï¼›é™¤ééµå®ˆè®¸å¯è¯ï¼Œå¦åˆ™ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚æ‚¨å¯ä»¥åœ¨è®¸å¯è¯ä¸‹è·å–è®¸å¯è¯çš„å‰¯æœ¬ã€‚
http://www.apache.org/licenses/LICENSE-2.0
é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œæ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯åŸºäºâ€œæŒ‰åŸæ ·â€ BASISï¼Œä¸æä¾›ä»»ä½•å½¢å¼çš„æ‹…ä¿æˆ–æ¡ä»¶ï¼Œæ— è®ºæ˜¯æ˜ç¤ºè¿˜æ˜¯æš—ç¤ºã€‚æœ‰å…³è®¸å¯è¯ä¸‹çš„ç‰¹å®šè¯­è¨€çš„æƒé™å’Œé™åˆ¶ï¼Œè¯·å‚é˜…è®¸å¯è¯ã€‚æ³¨æ„ï¼šæ­¤æ–‡ä»¶ä¸º Markdown æ–‡ä»¶ï¼Œä½†åŒ…å«æˆ‘ä»¬çš„ doc-builderï¼ˆç±»ä¼¼äº MDXï¼‰çš„ç‰¹å®šè¯­æ³•ï¼Œå¯èƒ½æ— æ³•åœ¨ Markdown æŸ¥çœ‹å™¨ä¸­æ­£ç¡®å‘ˆç°ã€‚-->

# è¯­ä¹‰åˆ†å‰²

[[åœ¨ Colab ä¸­æ‰“å¼€]]

<Youtube id="dKE8SIt9C-w"/>

è¯­ä¹‰åˆ†å‰²å°†æ ‡ç­¾æˆ–ç±»åˆ«åˆ†é…ç»™å›¾åƒçš„æ¯ä¸ªåƒç´ ã€‚è¯­ä¹‰åˆ†å‰²æœ‰å‡ ç§ç±»å‹ï¼Œåœ¨è¯­ä¹‰åˆ†å‰²çš„æƒ…å†µä¸‹ï¼Œä¸åŒºåˆ†åŒä¸€å¯¹è±¡çš„å”¯ä¸€å®ä¾‹ã€‚

ä¸¤ä¸ªå¯¹è±¡éƒ½è¢«èµ‹äºˆç›¸åŒçš„æ ‡ç­¾ï¼ˆä¾‹å¦‚ï¼Œâ€œæ±½è½¦â€è€Œä¸æ˜¯â€œæ±½è½¦-1â€å’Œâ€œæ±½è½¦-2â€ï¼‰ã€‚è¯­ä¹‰åˆ†å‰²çš„å¸¸è§å®é™…åº”ç”¨åŒ…æ‹¬åŸ¹è®­è‡ªåŠ¨é©¾é©¶æ±½è½¦è¯†åˆ«è¡Œäººå’Œé‡è¦äº¤é€šä¿¡æ¯ï¼Œè¯†åˆ«åŒ»å­¦å›¾åƒä¸­çš„ç»†èƒå’Œå¼‚å¸¸ï¼Œä»¥åŠç›‘æµ‹å«æ˜Ÿå›¾åƒä¸­çš„ç¯å¢ƒå˜åŒ–ã€‚

æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š
1. åœ¨ [SceneParse150](https://huggingface.co/datasets/scene_parse_150) æ•°æ®é›†ä¸Šå¾®è°ƒ [SegFormer](https://huggingface.co/docs/transformers/main/en/model_doc/segformer#segformer) æ¨¡å‹ã€‚
2. ä½¿ç”¨æ‚¨å¾®è°ƒçš„æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚

<Tip> 

æœ¬æ•™ç¨‹ä¸­æ‰€ç¤ºçš„ä»»åŠ¡æ”¯æŒä»¥ä¸‹æ¨¡å‹æ¶æ„ï¼š
<!--æ­¤æç¤ºç”±'make fix-copies'è‡ªåŠ¨ç”Ÿæˆï¼Œè¯·å‹¿æ‰‹åŠ¨å¡«å†™ï¼-->
[BEiT](../model_doc/beit), [Data2VecVision](../model_doc/data2vec-vision), [DPT](../model_doc/dpt), [MobileNetV2](../model_doc/mobilenet_v2), [MobileViT](../model_doc/mobilevit), [MobileViTV2](../model_doc/mobilevitv2), [SegFormer](../model_doc/segformer), [UPerNet](../model_doc/upernet)
<!--ç”Ÿæˆæç¤ºçš„ç»“å°¾-->
</Tip>

å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š
```bash
pip install -q datasets transformers evaluate
```

æˆ‘ä»¬é¼“åŠ±æ‚¨ç™»å½• Hugging Face å¸æˆ·ï¼Œä»¥ä¾¿ä¸ç¤¾åŒºä¸Šä¼ å’Œå…±äº«æ‚¨çš„æ¨¡å‹ã€‚åœ¨æç¤ºæ—¶ï¼Œè¾“å…¥æ‚¨çš„ä»¤ç‰Œä»¥ç™»å½•ï¼š
```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## åŠ è½½ SceneParse150 æ•°æ®é›†

é¦–å…ˆä»ğŸ¤— Datasets åº“ä¸­åŠ è½½ SceneParse150 æ•°æ®é›†çš„è¾ƒå°å­é›†ã€‚è¿™æ ·æ‚¨å°±æœ‰æœºä¼šåœ¨åœ¨å®Œæ•´æ•°æ®é›†ä¸Šè¿›è¡Œæ›´å¤šæ—¶é—´çš„è®­ç»ƒä¹‹å‰è¿›è¡Œå®éªŒå’Œç¡®ä¿ä¸€åˆ‡æ­£å¸¸ã€‚
```py
>>> from datasets import load_dataset

>>> ds = load_dataset("scene_parse_150", split="train[:50]")
```

ä½¿ç”¨ [`~datasets.Dataset.train_test_split`] æ–¹æ³•å°†æ•°æ®é›†çš„ `train` æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š
```py
>>> ds = ds.train_test_split(test_size=0.2)
>>> train_ds = ds["train"]
>>> test_ds = ds["test"]
```

ç„¶åæŸ¥çœ‹ä¸€ä¸ªç¤ºä¾‹ï¼š
```py
>>> train_ds[0]
{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x683 at 0x7F9B0C201F90>,
 'annotation': <PIL.PngImagePlugin.PngImageFile image mode=L size=512x683 at 0x7F9B0C201DD0>,
 'scene_category': 368}
```

- `image`ï¼šåœºæ™¯çš„ PIL å›¾åƒã€‚
- `annotation`ï¼šåˆ†å‰²åœ°å›¾çš„ PIL å›¾åƒï¼Œä¹Ÿæ˜¯æ¨¡å‹çš„ç›®æ ‡ã€‚
- `scene_category`ï¼šæè¿°å›¾åƒåœºæ™¯çš„ç±»åˆ« IDï¼Œå¦‚â€œå¨æˆ¿â€æˆ–â€œåŠå…¬å®¤â€ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæ‚¨åªéœ€è¦ `image` å’Œ `annotation`ï¼Œè¿™ä¸¤è€…éƒ½æ˜¯ PIL å›¾åƒã€‚

æ‚¨è¿˜éœ€è¦åˆ›å»ºä¸€ä¸ªå°†æ ‡ç­¾ ID æ˜ å°„åˆ°æ ‡ç­¾ç±»çš„å­—å…¸ï¼Œåœ¨ç¨åè®¾ç½®æ¨¡å‹æ—¶å°†éå¸¸æœ‰ç”¨ã€‚

ä» Hub ä¸‹è½½æ˜ å°„å¹¶åˆ›å»º `id2label` å’Œ `label2id` å­—å…¸ï¼š

```py
>>> import json
>>> from huggingface_hub import cached_download, hf_hub_url

>>> repo_id = "huggingface/label-files"
>>> filename = "ade20k-id2label.json"
>>> id2label = json.load(open(cached_download(hf_hub_url(repo_id, filename, repo_type="dataset")), "r"))
>>> id2label = {int(k): v for k, v in id2label.items()}
>>> label2id = {v: k for k, v in id2label.items()}
>>> num_labels = len(id2label)
```

## é¢„å¤„ç†

ä¸‹ä¸€æ­¥æ˜¯åŠ è½½ SegFormer å›¾åƒå¤„ç†å™¨ (Image Processor)ï¼Œä¸ºæ¨¡å‹å‡†å¤‡å›¾åƒå’Œæ³¨é‡Šã€‚æŸäº›æ•°æ®é›†ï¼ˆå¦‚æ­¤æ•°æ®é›†ï¼‰ä½¿ç”¨é›¶ç´¢å¼•ä½œä¸ºèƒŒæ™¯ç±»ã€‚ä½†æ˜¯ï¼Œå®é™…ä¸Šï¼ŒèƒŒæ™¯ç±»å¹¶ä¸åŒ…å«åœ¨ 150 ä¸ªç±»åˆ«ä¸­ï¼Œå› æ­¤æ‚¨éœ€è¦å°† `reduce_labels=True` è®¾ç½®ä¸ºä»æ‰€æœ‰æ ‡ç­¾ä¸­å‡å» 1ã€‚é›¶ç´¢å¼•å°†è¢«æ›¿æ¢ä¸º `255`ï¼Œå› æ­¤ SegFormer çš„æŸå¤±å‡½æ•°å°†å¿½ç•¥å®ƒï¼š
```py
>>> from transformers import AutoImageProcessor

>>> checkpoint = "nvidia/mit-b0"
>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint, reduce_labels=True)
```


<frameworkcontent> 
<pt> 

é€šå¸¸ä¼šå¯¹å›¾åƒæ•°æ®é›†åº”ç”¨ä¸€äº›æ•°æ®å¢å¼ºä»¥ä½¿æ¨¡å‹å¯¹è¿‡æ‹Ÿåˆæ›´å…·é²æ£’æ€§ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæ‚¨å°†ä½¿ç”¨æ¥è‡ª [torchvision](https://pytorch.org/vision/stable/index.html) çš„ [`ColorJitter`](https://pytorch.org/vision/stable/generated/torchvision.transforms.ColorJitter.html) å‡½æ•°éšæœºæ›´æ”¹å›¾åƒçš„é¢œè‰²å±æ€§ï¼Œä½†æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ä»»ä½•æ‚¨å–œæ¬¢çš„å›¾åƒåº“ã€‚
```py
>>> from torchvision.transforms import ColorJitter

>>> jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)
```

ç°åœ¨åˆ›å»ºä¸¤ä¸ªé¢„å¤„ç†å‡½æ•°ï¼Œä»¥å‡†å¤‡å›¾åƒå’Œæ³¨é‡Šä¾›æ¨¡å‹ä½¿ç”¨ã€‚è¿™äº›å‡½æ•°å°†å›¾åƒè½¬æ¢ä¸º `pixel_values`ï¼Œå°†æ³¨é‡Šè½¬æ¢ä¸º `labels`ã€‚å¯¹äºè®­ç»ƒé›†ï¼Œä¼šåœ¨å°†å›¾åƒæä¾›ç»™å›¾åƒå¤„ç†å™¨ (Image Processor)ä¹‹å‰åº”ç”¨ `jitter`ã€‚

å¯¹äºæµ‹è¯•é›†ï¼Œå›¾åƒå¤„ç†å™¨ (Image Processor)ä¼šè£å‰ªå’Œè§„èŒƒåŒ– `images`ï¼Œä»…è£å‰ª `labels`ï¼Œå› ä¸ºåœ¨æµ‹è¯•æœŸé—´ä¸åº”ç”¨ä»»ä½•æ•°æ®å¢å¼ºã€‚

```py
>>> def train_transforms(example_batch):
...     images = [jitter(x) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs


>>> def val_transforms(example_batch):
...     images = [x for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs
```

è¦åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåº”ç”¨ `jitter`ï¼Œè¯·ä½¿ç”¨ğŸ¤— Datasets çš„ [`~datasets.Dataset.set_transform`] å‡½æ•°ã€‚

è½¬æ¢æ˜¯åœ¨è¿è¡Œæ—¶åº”ç”¨çš„ï¼Œé€Ÿåº¦æ›´å¿«ï¼Œå ç”¨çš„ç£ç›˜ç©ºé—´æ›´å°‘ï¼š
```py
>>> train_ds.set_transform(train_transforms)
>>> test_ds.set_transform(val_transforms)
```

</pt>
</frameworkcontent>

<frameworkcontent>
<tf>

é€šå¸¸ä¼šå¯¹å›¾åƒæ•°æ®é›†åº”ç”¨ä¸€äº›æ•°æ®å¢å¼ºä»¥ä½¿æ¨¡å‹å¯¹è¿‡æ‹Ÿåˆæ›´å…·é²æ£’æ€§ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæ‚¨å°†ä½¿ç”¨ [`tf.image`](https://www.tensorflow.org/api_docs/python/tf/image) éšæœºæ›´æ”¹å›¾åƒçš„é¢œè‰²å±æ€§ï¼Œä½†æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ä»»ä½•å›¾åƒåº“ã€‚å®šä¹‰ä¸¤ä¸ªå•ç‹¬çš„è½¬æ¢å‡½æ•°ï¼š- è®­ç»ƒæ•°æ®è½¬æ¢ï¼ŒåŒ…æ‹¬å›¾åƒå¢å¼º- éªŒè¯æ•°æ®è½¬æ¢ï¼Œä»…è½¬ç½®å›¾åƒï¼Œå› ä¸ºğŸ¤— Transformers ä¸­çš„è®¡ç®—æœºè§†è§‰æ¨¡å‹æœŸæœ›é€šé“ä¼˜å…ˆå¸ƒå±€

```py
>>> import tensorflow as tf


>>> def aug_transforms(image):
...     image = tf.keras.utils.img_to_array(image)
...     image = tf.image.random_brightness(image, 0.25)
...     image = tf.image.random_contrast(image, 0.5, 2.0)
...     image = tf.image.random_saturation(image, 0.75, 1.25)
...     image = tf.image.random_hue(image, 0.1)
...     image = tf.transpose(image, (2, 0, 1))
...     return image


>>> def transforms(image):
...     image = tf.keras.utils.img_to_array(image)
...     image = tf.transpose(image, (2, 0, 1))
...     return image
```

æ¥ä¸‹æ¥ï¼Œåˆ›å»ºä¸¤ä¸ªé¢„å¤„ç†å‡½æ•°ï¼Œä»¥ä¸ºæ¨¡å‹å‡†å¤‡å›¾åƒå’Œæ³¨é‡Šçš„æ‰¹æ¬¡ã€‚è¿™äº›å‡½æ•°åº”ç”¨äº†å›¾åƒè½¬æ¢ï¼Œå¹¶ä½¿ç”¨å…ˆå‰åŠ è½½çš„ `image_processor` å°†å›¾åƒè½¬æ¢ä¸º `pixel_values` å’Œå°†æ³¨é‡Šè½¬æ¢ä¸º `labels`ã€‚

`ImageProcessor` è¿˜è´Ÿè´£è°ƒæ•´å›¾åƒçš„å°ºå¯¸å’Œè§„èŒƒåŒ–ã€‚

```py
>>> def train_transforms(example_batch):
...     images = [aug_transforms(x.convert("RGB")) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs


>>> def val_transforms(example_batch):
...     images = [transforms(x.convert("RGB")) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs
```

è¦åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåº”ç”¨é¢„å¤„ç†è½¬æ¢ï¼Œè¯·ä½¿ç”¨ğŸ¤— Datasets çš„ [`~datasets.Dataset.set_transform`] å‡½æ•°ã€‚

è½¬æ¢æ˜¯åœ¨è¿è¡Œæ—¶åº”ç”¨çš„ï¼Œé€Ÿåº¦æ›´å¿«ï¼Œå ç”¨çš„ç£ç›˜ç©ºé—´æ›´å°‘ï¼š

```py
>>> train_ds.set_transform(train_transforms)
>>> test_ds.set_transform(val_transforms)
```
</tf>
</frameworkcontent>


## è¯„ä¼°

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒ…å«åº¦é‡æ ‡å‡†é€šå¸¸æœ‰åŠ©äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) åº“å¿«é€ŸåŠ è½½è¯„ä¼°æ–¹æ³•ã€‚

å¯¹äºæ­¤ä»»åŠ¡ï¼ŒåŠ è½½ [mean Intersection over Union](https://huggingface.co/spaces/evaluate-metric/accuracy)ï¼ˆIoUï¼‰åº¦é‡æ ‡å‡†ï¼ˆè¯·å‚é˜…ğŸ¤— Evaluate [å¿«é€Ÿå¯¼è§ˆ](https://huggingface.co/docs/evaluate/a_quick_tour) ä»¥äº†è§£æœ‰å…³å¦‚ä½•åŠ è½½å’Œè®¡ç®—åº¦é‡æ ‡å‡†çš„æ›´å¤šä¿¡æ¯ï¼‰ï¼š

```py
>>> import evaluate

>>> metric = evaluate.load("mean_iou")
```

ç„¶ååˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥ [`~evaluate.EvaluationModule.compute`] è®¡ç®—åº¦é‡æ ‡å‡†ã€‚æ‚¨çš„é¢„æµ‹éœ€è¦é¦–å…ˆè½¬æ¢ä¸º logitsï¼Œç„¶åè°ƒæ•´å…¶å½¢çŠ¶ä»¥ä¸æ ‡ç­¾çš„å¤§å°ç›¸åŒ¹é…ï¼Œç„¶åæ‰èƒ½è°ƒç”¨ [`~evaluate.EvaluationModule.compute`]ï¼š

<frameworkcontent> 
<pt> 

```py
>>> def compute_metrics(eval_pred):
...     with torch.no_grad():
...         logits, labels = eval_pred
...         logits_tensor = torch.from_numpy(logits)
...         logits_tensor = nn.functional.interpolate(
...             logits_tensor,
...             size=labels.shape[-2:],
...             mode="bilinear",
...             align_corners=False,
...         ).argmax(dim=1)

...         pred_labels = logits_tensor.detach().cpu().numpy()
...         metrics = metric.compute(
...             predictions=pred_labels,
...             references=labels,
...             num_labels=num_labels,
...             ignore_index=255,
...             reduce_labels=False,
...         )
...         for key, value in metrics.items():
...             if type(value) is np.ndarray:
...                 metrics[key] = value.tolist()
...         return metrics
```

</pt>
</frameworkcontent>


<frameworkcontent>
<tf>

```py
>>> def compute_metrics(eval_pred):
...     logits, labels = eval_pred
...     logits = tf.transpose(logits, perm=[0, 2, 3, 1])
...     logits_resized = tf.image.resize(
...         logits,
...         size=tf.shape(labels)[1:],
...         method="bilinear",
...     )

...     pred_labels = tf.argmax(logits_resized, axis=-1)
...     metrics = metric.compute(
...         predictions=pred_labels,
...         references=labels,
...         num_labels=num_labels,
...         ignore_index=-1,
...         reduce_labels=image_processor.do_reduce_labels,
...     )

...     per_category_accuracy = metrics.pop("per_category_accuracy").tolist()
...     per_category_iou = metrics.pop("per_category_iou").tolist()

...     metrics.update({f"accuracy_{id2label[i]}": v for i, v in enumerate(per_category_accuracy)})
...     metrics.update({f"iou_{id2label[i]}": v for i, v in enumerate(per_category_iou)})
...     return {"val_" + k: v for k, v in metrics.items()}
```

</tf>
</frameworkcontent>

æ‚¨çš„ `compute_metrics` å‡½æ•°å·²å‡†å¤‡å°±ç»ªï¼Œå½“æ‚¨è®¾ç½®è®­ç»ƒæ—¶å°†è¿”å›è¯¥å‡½æ•°ã€‚

## Train
<frameworkcontent>
<pt>
<Tip>

å¦‚æœæ‚¨å¯¹ä½¿ç”¨ [`Trainer`] å¾®è°ƒæ¨¡å‹ä¸ç†Ÿæ‚‰ï¼Œè¯·æŸ¥çœ‹åŸºæœ¬æ•™ç¨‹ [æ­¤å¤„](../training#finetune-with-trainer)ï¼
</Tip>

æ‚¨ç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒæ¨¡å‹äº†ï¼ä½¿ç”¨ [`AutoModelForSemanticSegmentation`] åŠ è½½ SegFormerï¼Œå¹¶å°†æ ‡ç­¾ ID ä¸æ ‡ç­¾ç±»ä¹‹é—´çš„æ˜ å°„ä¼ é€’ç»™æ¨¡å‹ï¼š
```py
>>> from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer

>>> model = AutoModelForSemanticSegmentation.from_pretrained(checkpoint, id2label=id2label, label2id=label2id)
```

æ­¤æ—¶ï¼Œåªå‰©ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š

1. åœ¨ [`TrainingArguments`] ä¸­å®šä¹‰æ‚¨çš„è®­ç»ƒè¶…å‚æ•°ã€‚è¯·åŠ¡å¿…ä¸è¦åˆ é™¤æœªä½¿ç”¨çš„åˆ—ï¼Œå› ä¸ºè¿™ä¼šåˆ é™¤ `image` åˆ—ã€‚æ²¡æœ‰ `image` åˆ—ï¼Œæ‚¨æ— æ³•åˆ›å»º `pixel_values`ã€‚å°† `remove_unused_columns=False` ä»¥é˜²æ­¢æ­¤è¡Œä¸ºï¼å¦ä¸€ä¸ªå¿…éœ€çš„å‚æ•°æ˜¯ `output_dir`ï¼Œå®ƒæŒ‡å®šäº†ä¿å­˜æ¨¡å‹çš„ä½ç½®ã€‚é€šè¿‡è®¾ç½® `push_to_hub=True` å°†æ­¤æ¨¡å‹æ¨é€åˆ° Hub ä¸Šï¼ˆæ‚¨éœ€è¦ç™»å½•åˆ° Hugging Face æ‰èƒ½ä¸Šä¼ æ¨¡å‹ï¼‰ã€‚åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶ï¼Œ[`Trainer`] å°†è¯„ä¼° IoU æŒ‡æ ‡å¹¶ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹ã€‚

2. å°†è®­ç»ƒå‚æ•°ä¸æ¨¡å‹ã€æ•°æ®é›†ã€tokenizerã€æ•°æ®æ•´ç†å™¨å’Œ `compute_metrics` å‡½æ•°ä¸€èµ·ä¼ é€’ç»™ [`Trainer`]ã€‚

3. è°ƒç”¨ [`~Trainer.train`] æ¥å¾®è°ƒæ‚¨çš„æ¨¡å‹ã€‚

```py
>>> training_args = TrainingArguments(
...     output_dir="segformer-b0-scene-parse-150",
...     learning_rate=6e-5,
...     num_train_epochs=50,
...     per_device_train_batch_size=2,
...     per_device_eval_batch_size=2,
...     save_total_limit=3,
...     evaluation_strategy="steps",
...     save_strategy="steps",
...     save_steps=20,
...     eval_steps=20,
...     logging_steps=1,
...     eval_accumulation_steps=5,
...     remove_unused_columns=False,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=train_ds,
...     eval_dataset=test_ds,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨ [`~transformers.Trainer.push_to_hub`] æ–¹æ³•å°†æ‚¨çš„æ¨¡å‹å…±äº«åˆ° Hub ä¸Šï¼Œä»¥ä¾¿æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨æ‚¨çš„æ¨¡å‹ï¼š
```py
>>> trainer.push_to_hub()
```

</pt> </frameworkcontent>
<frameworkcontent> <tf> <Tip>

å¦‚æœæ‚¨å¯¹ä½¿ç”¨ Keras è¿›è¡Œæ¨¡å‹å¾®è°ƒä¸ç†Ÿæ‚‰ï¼Œè¯·å…ˆæŸ¥çœ‹ [åŸºæœ¬æ•™ç¨‹](./training#train-a-tensorflow-model-with-keras)ï¼

</Tip>

è¦åœ¨ TensorFlow ä¸­è¿›è¡Œæ¨¡å‹å¾®è°ƒï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œï¼š
1. å®šä¹‰è®­ç»ƒè¶…å‚æ•°ï¼Œå¹¶è®¾ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚
2. å®ä¾‹åŒ–é¢„è®­ç»ƒæ¨¡å‹ã€‚
3. å°†ğŸ¤—æ•°æ®é›†è½¬æ¢ä¸º `tf.data.Dataset`ã€‚
4. ç¼–è¯‘æ‚¨çš„æ¨¡å‹ã€‚
5. æ·»åŠ å›è°ƒå‡½æ•°ä»¥è®¡ç®—æŒ‡æ ‡å¹¶ä¸Šä¼ æ¨¡å‹åˆ°ğŸ¤— Hubã€‚
6. ä½¿ç”¨ `fit()` æ–¹æ³•è¿è¡Œè®­ç»ƒã€‚

é¦–å…ˆï¼Œå®šä¹‰è¶…å‚æ•°ã€ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼š
```py
>>> from transformers import create_optimizer

>>> batch_size = 2
>>> num_epochs = 50
>>> num_train_steps = len(train_ds) * num_epochs
>>> learning_rate = 6e-5
>>> weight_decay_rate = 0.01

>>> optimizer, lr_schedule = create_optimizer(
...     init_lr=learning_rate,
...     num_train_steps=num_train_steps,
...     weight_decay_rate=weight_decay_rate,
...     num_warmup_steps=0,
... )
```

ç„¶åï¼Œä½¿ç”¨ [`TFAutoModelForSemanticSegmentation`] åŠ è½½ SegFormerï¼Œå¹¶ä¸æ ‡ç­¾æ˜ å°„ä¸€èµ·ç¼–è¯‘å®ƒçš„ä¼˜åŒ–å™¨ã€‚

è¯·æ³¨æ„ï¼ŒTransformers æ¨¡å‹éƒ½æœ‰ä¸€ä¸ªé»˜è®¤çš„ä»»åŠ¡ç›¸å…³æŸå¤±å‡½æ•°ï¼Œå› æ­¤æ‚¨æ— éœ€æŒ‡å®šï¼Œé™¤éæ‚¨æƒ³è¦è‡ªå®šä¹‰ï¼šä½¿ç”¨ [`~datasets.Dataset.to_tf_dataset`] å’Œ [`DefaultDataCollator`] å°†æ•°æ®é›†è½¬æ¢ä¸º `tf.data.Dataset` æ ¼å¼ï¼š

```py
>>> from transformers import TFAutoModelForSemanticSegmentation

>>> model = TFAutoModelForSemanticSegmentation.from_pretrained(
...     checkpoint,
...     id2label=id2label,
...     label2id=label2id,
... )
>>> model.compile(optimizer=optimizer)  # No loss argument!
```

ä½¿ç”¨ [`~datasets.Dataset.to_tf_dataset`] å’Œ [`DefaultDataCollator`] å°†æ•°æ®é›†è½¬æ¢ä¸º `tf.data.Dataset` æ ¼å¼ï¼š
```py
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator(return_tensors="tf")

>>> tf_train_dataset = train_ds.to_tf_dataset(
...     columns=["pixel_values", "label"],
...     shuffle=True,
...     batch_size=batch_size,
...     collate_fn=data_collator,
... )

>>> tf_eval_dataset = test_ds.to_tf_dataset(
...     columns=["pixel_values", "label"],
...     shuffle=True,
...     batch_size=batch_size,
...     collate_fn=data_collator,
... )
```

ä½¿ç”¨ [Keras å›è°ƒ](../main_classes/keras_callbacks) è®¡ç®—å‡†ç¡®æ€§å¹¶å°†æ¨¡å‹æ¨é€åˆ°ğŸ¤— Hubï¼Œä»¥è®¡ç®—é¢„æµ‹ç»“æœçš„å‡†ç¡®æ€§ï¼šå°†æ‚¨çš„ `compute_metrics` å‡½æ•°ä¼ é€’ç»™ [`KerasMetricCallback`]ï¼Œå¹¶ä½¿ç”¨ [`PushToHubCallback`] ä¸Šä¼ æ¨¡å‹ï¼š
```py
>>> from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback

>>> metric_callback = KerasMetricCallback(
...     metric_fn=compute_metrics, eval_dataset=tf_eval_dataset, batch_size=batch_size, label_cols=["labels"]
... )

>>> push_to_hub_callback = PushToHubCallback(output_dir="scene_segmentation", tokenizer=image_processor)

>>> callbacks = [metric_callback, push_to_hub_callback]
```

æœ€åï¼Œæ‚¨å·²å‡†å¤‡å¥½è®­ç»ƒæ¨¡å‹äº†ï¼ä½¿ç”¨æ‚¨çš„è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ã€epoch æ•°é‡å’Œå›è°ƒå‡½æ•°æ¥å¾®è°ƒæ¨¡å‹ï¼š</tf>
```py
>>> model.fit(
...     tf_train_dataset,
...     validation_data=tf_eval_dataset,
...     callbacks=callbacks,
...     epochs=num_epochs,
... )
```

æ­å–œï¼æ‚¨å·²ç»å¾®è°ƒäº†æ¨¡å‹å¹¶åœ¨ğŸ¤— Hub ä¸Šå…±äº«äº†å®ƒã€‚ç°åœ¨æ‚¨å¯ä»¥ç”¨å®ƒè¿›è¡Œæ¨ç†ï¼</tf> </frameworkcontent>

## æ¨ç†

å¤ªæ£’äº†ï¼Œç°åœ¨æ‚¨å·²ç»å¾®è°ƒäº†æ¨¡å‹ï¼Œå¯ä»¥ç”¨å®ƒè¿›è¡Œæ¨ç†äº†ï¼
åŠ è½½ä¸€å¼ å›¾ç‰‡è¿›è¡Œæ¨ç†ï¼š

```py
>>> image = ds[0]["image"]
>>> image
```

<div class="flex justify-center">    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-image.png" alt="å§å®¤å›¾ç‰‡"/> </div>

<frameworkcontent> 
<pt> 

 å°è¯•ä½¿ç”¨ [`pipeline`] å¯¹æ‚¨çš„å¾®è°ƒæ¨¡å‹è¿›è¡Œæ¨ç†æ˜¯æœ€ç®€å•çš„æ–¹æ³•ã€‚ä½¿ç”¨æ‚¨çš„æ¨¡å‹å®ä¾‹åŒ–ä¸€ä¸ªç”¨äºå›¾åƒåˆ†å‰²çš„ `pipeline`ï¼Œå¹¶å°†å›¾ç‰‡ä¼ é€’ç»™å®ƒï¼š

```py
>>> from transformers import pipeline

>>> segmenter = pipeline("image-segmentation", model="my_awesome_seg_model")
>>> segmenter(image)
[{'score': None,
  'label': 'wall',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062690>},
 {'score': None,
  'label': 'sky',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062A50>},
 {'score': None,
  'label': 'floor',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062B50>},
 {'score': None,
  'label': 'ceiling',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062A10>},
 {'score': None,
  'label': 'bed ',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062E90>},
 {'score': None,
  'label': 'windowpane',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062390>},
 {'score': None,
  'label': 'cabinet',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062550>},
 {'score': None,
  'label': 'chair',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062D90>},
 {'score': None,
  'label': 'armchair',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062E10>}]
```

å¦‚æœæ‚¨æ„¿æ„ï¼Œä¹Ÿå¯ä»¥æ‰‹åŠ¨å¤åˆ¶ `pipeline` çš„ç»“æœã€‚ä½¿ç”¨å›¾åƒå¤„ç†å™¨ (Image Processor)å¤„ç†å›¾åƒï¼Œå¹¶å°† `pixel_values` æ”¾åœ¨ GPU ä¸Šï¼š
```py
>>> device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # use GPU if available, otherwise use a CPU
>>> encoding = image_processor(image, return_tensors="pt")
>>> pixel_values = encoding.pixel_values.to(device)
```

å°†æ‚¨çš„è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å¹¶è¿”å› `logits`ï¼š
```py
>>> outputs = model(pixel_values=pixel_values)
>>> logits = outputs.logits.cpu()
```

æ¥ä¸‹æ¥ï¼Œå°† `logits` é‡æ–°ç¼©æ”¾ä¸ºåŸå§‹å›¾åƒå¤§å°ï¼š
```py
>>> upsampled_logits = nn.functional.interpolate(
...     logits,
...     size=image.size[::-1],
...     mode="bilinear",
...     align_corners=False,
... )

>>> pred_seg = upsampled_logits.argmax(dim=1)[0]
```
</pt>
</frameworkcontent>

<frameworkcontent>
<tf>

åŠ è½½å›¾åƒå¤„ç†å™¨ (Image Processor)ä»¥é¢„å¤„ç†å›¾åƒå¹¶å°†è¾“å…¥è¿”å›ä¸º TensorFlow å¼ é‡ï¼š
```py
>>> from transformers import AutoImageProcessor

>>> image_processor = AutoImageProcessor.from_pretrained("MariaK/scene_segmentation")
>>> inputs = image_processor(image, return_tensors="tf")
```

å°†æ‚¨çš„è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å¹¶è¿”å› `logits`ï¼š
```py
>>> from transformers import TFAutoModelForSemanticSegmentation

>>> model = TFAutoModelForSemanticSegmentation.from_pretrained("MariaK/scene_segmentation")
>>> logits = model(**inputs).logits
```

æ¥ä¸‹æ¥ï¼Œå°† `logits` é‡æ–°ç¼©æ”¾ä¸ºåŸå§‹å›¾åƒå¤§å°ï¼Œå¹¶åœ¨ç±»ç»´åº¦ä¸Šåº”ç”¨ argmaxï¼š
```py
>>> logits = tf.transpose(logits, [0, 2, 3, 1])

>>> upsampled_logits = tf.image.resize(
...     logits,
...     # We reverse the shape of `image` because `image.size` returns width and height.
...     image.size[::-1],
... )

>>> pred_seg = tf.math.argmax(upsampled_logits, axis=-1)[0]
```

</tf>
</frameworkcontent>


è¦å¯è§†åŒ–ç»“æœï¼Œè¯·åŠ è½½ [æ•°æ®é›†é¢œè‰²è°ƒè‰²æ¿](https://github.com/tensorflow/models/blob/3f1ca33afe3c1631b733ea7e40c294273b9e406d/research/deeplab/utils/get_dataset_colormap.py#L51) ä½œä¸º `ade_palette()`ï¼Œå°†æ¯ä¸ªç±»åˆ«æ˜ å°„åˆ°å…¶ RGB å€¼ã€‚
ç„¶åï¼Œæ‚¨å¯ä»¥ç»„åˆå¹¶ç»˜åˆ¶å›¾åƒå’Œé¢„æµ‹çš„åˆ†å‰²å›¾ï¼š

```py
>>> import matplotlib.pyplot as plt
>>> import numpy as np

>>> color_seg = np.zeros((pred_seg.shape[0], pred_seg.shape[1], 3), dtype=np.uint8)
>>> palette = np.array(ade_palette())
>>> for label, color in enumerate(palette):
...     color_seg[pred_seg == label, :] = color
>>> color_seg = color_seg[..., ::-1]  # convert to BGR

>>> img = np.array(image) * 0.5 + color_seg * 0.5  # plot the image with the segmentation map
>>> img = img.astype(np.uint8)

>>> plt.figure(figsize=(15, 10))
>>> plt.imshow(img)
>>> plt.show()
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-preds.png" alt="Image of bedroom overlaid with segmentation map"/>
</div>