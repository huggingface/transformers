<!--ç‰ˆæƒæ‰€æœ‰ 2023 å¹´ The HuggingFace å›¢é˜Ÿã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚
æ ¹æ® Apache è®¸å¯è¯ç¬¬ 2.0 ç‰ˆï¼ˆâ€œè®¸å¯è¯â€ï¼‰çš„è§„å®šï¼Œæ‚¨é™¤ééµå®ˆè®¸å¯è¯ï¼Œå¦åˆ™ä¸å¾—ä½¿ç”¨æœ¬æ–‡ä»¶ã€‚æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å¾—è®¸å¯è¯çš„å‰¯æœ¬
http://www.apache.org/licenses/LICENSE-2.0
é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œæ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯æŒ‰ç…§â€œåŸæ ·â€åˆ†å‘çš„ï¼Œä¸é™„å¸¦ä»»ä½•å½¢å¼çš„ä¿è¯æˆ–æ¡ä»¶ã€‚è¯·å‚é˜…è®¸å¯è¯ä»¥è·å¾—ç‰¹å®šè¯­è¨€ä¸‹çš„æƒé™å’Œé™åˆ¶ã€‚ç‰¹åˆ«æ³¨æ„ï¼Œæ­¤æ–‡ä»¶ä»¥ Markdown æ ¼å¼ç¼–å†™ï¼Œä½†åŒ…å«æˆ‘ä»¬æ–‡æ¡£æ„å»ºå™¨ï¼ˆç±»ä¼¼äº MDXï¼‰çš„ç‰¹å®šè¯­æ³•ï¼Œå¯èƒ½æ— æ³•åœ¨æ‚¨çš„ Markdown æŸ¥çœ‹å™¨ä¸­æ­£ç¡®æ˜¾ç¤ºã€‚
-->


# å›¾åƒå­—å¹•

[[åœ¨ Colab ä¸­æ‰“å¼€]]

å›¾åƒå­—å¹•æ˜¯é¢„æµ‹ç»™å®šå›¾åƒçš„å­—å¹•çš„ä»»åŠ¡ã€‚å…¶å¸¸è§çš„å®é™…åº”ç”¨åŒ…æ‹¬å¸®åŠ©è§†éšœäººå£«åœ¨ä¸åŒæƒ…å†µä¸‹è¿›è¡Œå¯¼èˆªã€‚å› æ­¤ï¼Œå›¾åƒå­—å¹•é€šè¿‡ä¸ºäººä»¬æè¿°å›¾åƒæ¥æé«˜å†…å®¹çš„å¯è®¿é—®æ€§ã€‚

æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š

* å¯¹å›¾åƒå­—å¹•æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚
* ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚

å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š
```bash
pip install transformers datasets evaluate -q
pip install jiwer -q
```

æˆ‘ä»¬é¼“åŠ±æ‚¨ç™»å½•æ‚¨çš„ Hugging Face è´¦æˆ·ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥ä¸ç¤¾åŒºå…±äº«æ‚¨çš„æ¨¡å‹ã€‚åœ¨æç¤ºæ—¶ï¼Œè¾“å…¥æ‚¨çš„ä»¤ç‰Œä»¥ç™»å½•ï¼š

```python
from huggingface_hub import notebook_login

notebook_login()
```

## åŠ è½½ Pok Ã© mon BLIP å­—å¹•æ•°æ®é›†

ä½¿ç”¨ğŸ¤—æ•°æ®é›†åº“åŠ è½½ç”±{image-caption}å¯¹ç»„æˆçš„æ•°æ®é›†ã€‚è¦åœ¨ PyTorch ä¸­åˆ›å»ºè‡ªå·±çš„å›¾åƒå­—å¹•æ•°æ®é›†ï¼Œå¯ä»¥å‚è€ƒ [æ­¤ç¬”è®°æœ¬](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/GIT/Fine_tune_GIT_on_an_image_captioning_dataset.ipynb)ã€‚

```python
from datasets import load_dataset

ds = load_dataset("lambdalabs/pokemon-blip-captions")
ds
```
```bash
DatasetDict({
    train: Dataset({
        features: ['image', 'text'],
        num_rows: 833
    })
})
```

æ•°æ®é›†æœ‰ä¸¤ä¸ªç‰¹å¾ï¼Œ`image` å’Œ `text`ã€‚
<Tip>
è®¸å¤šå›¾åƒå­—å¹•æ•°æ®é›†æ¯ä¸ªå›¾åƒéƒ½æœ‰å¤šä¸ªå­—å¹•ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸€ç§å¸¸è§çš„ç­–ç•¥æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åœ¨å¯ç”¨å­—å¹•ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªå­—å¹•ã€‚
</Tip>
ä½¿ç”¨ [~datasets.Dataset.train_test_split] æ–¹æ³•å°†æ•°æ®é›†çš„è®­ç»ƒé›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š

```python
ds = ds["train"].train_test_split(test_size=0.1)
train_ds = ds["train"]
test_ds = ds["test"]
```

è®©æˆ‘ä»¬ä»è®­ç»ƒé›†ä¸­å¯è§†åŒ–å‡ ä¸ªæ ·æœ¬ã€‚

```python
from textwrap import wrap
import matplotlib.pyplot as plt
import numpy as np


def plot_images(images, captions):
    plt.figure(figsize=(20, 20))
    for i in range(len(images)):
        ax = plt.subplot(1, len(images), i + 1)
        caption = captions[i]
        caption = "\n".join(wrap(caption, 12))
        plt.title(caption)
        plt.imshow(images[i])
        plt.axis("off")


sample_images_to_visualize = [np.array(train_ds[i]["image"]) for i in range(5)]
sample_captions = [train_ds[i]["text"] for i in range(5)]
plot_images(sample_images_to_visualize, sample_captions)
```
    
<div class="flex justify-center">    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_training_images_image_cap.png" alt="Sample training images"/> </div>

## é¢„å¤„ç†æ•°æ®é›†

ç”±äºæ•°æ®é›†æœ‰ä¸¤ç§æ¨¡æ€ï¼ˆå›¾åƒå’Œæ–‡æœ¬ï¼‰ï¼Œé¢„å¤„ç†æµç¨‹å°†å¯¹å›¾åƒå’Œå­—å¹•è¿›è¡Œé¢„å¤„ç†ã€‚

ä¸ºæ­¤ï¼ŒåŠ è½½ä¸å³å°†è¿›è¡Œå¾®è°ƒçš„æ¨¡å‹ç›¸å…³è”çš„å¤„ç†å™¨ç±»ã€‚
```python
from transformers import AutoProcessor

checkpoint = "microsoft/git-base"
processor = AutoProcessor.from_pretrained(checkpoint)
```

å¤„ç†å™¨å°†åœ¨å†…éƒ¨é¢„å¤„ç†å›¾åƒï¼ˆåŒ…æ‹¬è°ƒæ•´å¤§å°å’Œåƒç´ ç¼©æ”¾ï¼‰å¹¶å¯¹å­—å¹•è¿›è¡Œæ ‡è®°åŒ–ã€‚
```python
def transforms(example_batch):
    images = [x for x in example_batch["image"]]
    captions = [x for x in example_batch["text"]]
    inputs = processor(images=images, text=captions, padding="max_length")
    inputs.update({"labels": inputs["input_ids"]})
    return inputs


train_ds.set_transform(transforms)
test_ds.set_transform(transforms)
```

æ•°æ®é›†å‡†å¤‡å°±ç»ªåï¼Œæ‚¨ç°åœ¨å¯ä»¥ä¸ºå¾®è°ƒè®¾ç½®æ¨¡å‹ã€‚

## åŠ è½½åŸºç¡€æ¨¡å‹

å°† ["microsoft/git-base"](https://huggingface.co/microsoft/git-base) åŠ è½½åˆ° [`AutoModelForCausalLM`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM) å¯¹è±¡ä¸­ã€‚


```python
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(checkpoint)
```

## è¯„ä¼°

å›¾åƒå­—å¹•æ¨¡å‹é€šå¸¸ä½¿ç”¨ [Rouge åˆ†æ•°](https://huggingface.co/spaces/evaluate-metric/rouge) æˆ– [è¯é”™è¯¯ç‡](https://huggingface.co/spaces/evaluate-metric/wer) è¿›è¡Œè¯„ä¼°ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæ‚¨å°†ä½¿ç”¨è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚

æˆ‘ä»¬ä½¿ç”¨ğŸ¤—è¯„ä¼°åº“æ¥è¿›è¡Œè¯„ä¼°ã€‚æœ‰å…³ WER çš„æ½œåœ¨é™åˆ¶å’Œå…¶ä»–æ³¨æ„äº‹é¡¹ï¼Œè¯·å‚é˜… [æ­¤æŒ‡å—](https://huggingface.co/spaces/evaluate-metric/wer)ã€‚

```python
from evaluate import load
import torch

wer = load("wer")


def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predicted = logits.argmax(-1)
    decoded_labels = processor.batch_decode(labels, skip_special_tokens=True)
    decoded_predictions = processor.batch_decode(predicted, skip_special_tokens=True)
    wer_score = wer.compute(predictions=decoded_predictions, references=decoded_labels)
    return {"wer_score": wer_score}
```

## è®­ç»ƒï¼
ç°åœ¨ï¼Œæ‚¨å·²å‡†å¤‡å¥½å¼€å§‹å¾®è°ƒæ¨¡å‹äº†ã€‚æ‚¨å°†ä½¿ç”¨ğŸ¤— [`Trainer`] è¿›è¡Œæ­¤æ“ä½œã€‚

é¦–å…ˆï¼Œä½¿ç”¨ [`TrainingArguments`] å®šä¹‰è®­ç»ƒå‚æ•°ã€‚

```python
from transformers import TrainingArguments, Trainer

model_name = checkpoint.split("/")[1]

training_args = TrainingArguments(
    output_dir=f"{model_name}-pokemon",
    learning_rate=5e-5,
    num_train_epochs=50,
    fp16=True,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    gradient_accumulation_steps=2,
    save_total_limit=3,
    evaluation_strategy="steps",
    eval_steps=50,
    save_strategy="steps",
    save_steps=50,
    logging_steps=50,
    remove_unused_columns=False,
    push_to_hub=True,
    label_names=["labels"],
    load_best_model_at_end=True,
)
```

ç„¶åå°†å‚æ•°ä¸æ•°æ®é›†å’Œæ¨¡å‹ä¸€èµ·ä¼ é€’ç»™ğŸ¤— Trainerã€‚
```python
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=test_ds,
    compute_metrics=compute_metrics,
)
```

è¦å¼€å§‹è®­ç»ƒï¼Œåªéœ€åœ¨ [`Trainer`] å¯¹è±¡ä¸Šè°ƒç”¨ [`~Trainer.train`]ã€‚
```python 
trainer.train()
```

æ‚¨åº”è¯¥çœ‹åˆ°è®­ç»ƒæŸå¤±éšç€è®­ç»ƒçš„è¿›è¡Œè€Œå¹³æ»‘ä¸‹é™ã€‚
è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨ [`~Trainer.push_to_hub`] æ–¹æ³•å°†æ‚¨çš„æ¨¡å‹å…±äº«åˆ° Hubï¼Œä»¥ä¾¿æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨æ‚¨çš„æ¨¡å‹ï¼š

```python
trainer.push_to_hub()
```

## æ¨ç†
ä» `test_ds` ä¸­è·å–ä¸€ä¸ªæ ·æœ¬å›¾åƒä»¥æµ‹è¯•æ¨¡å‹ã€‚

```python
from PIL import Image
import requests

url = "https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/pokemon.png"
image = Image.open(requests.get(url, stream=True).raw)
image
```

<div class="flex justify-center">    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/test_image_image_cap.png" alt="Test image"/> </div>    

ä¸ºæ¨¡å‹å‡†å¤‡å›¾åƒã€‚
```python
device = "cuda" if torch.cuda.is_available() else "cpu"

inputs = processor(images=image, return_tensors="pt").to(device)
pixel_values = inputs.pixel_values
```

è°ƒç”¨ [`generate`] å¹¶è§£ç é¢„æµ‹ç»“æœã€‚
```python
generated_ids = model.generate(pixel_values=pixel_values, max_length=50)
generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(generated_caption)
```
```bash
a drawing of a pink and blue pokemon
```

çœ‹èµ·æ¥å¾®è°ƒåçš„æ¨¡å‹ç”Ÿæˆäº†ä¸€ä¸ªéå¸¸å¥½çš„å­—å¹•ï¼