<!--ç‰ˆæƒæ‰€æœ‰2022å¹´çš„HuggingFaceå›¢é˜Ÿã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚
æ ¹æ®Apacheè®¸å¯è¯ç¬¬2ç‰ˆï¼ˆâ€œè®¸å¯è¯â€ï¼‰è·å¾—è®¸å¯ï¼›é™¤éç¬¦åˆè®¸å¯è¯ï¼Œå¦åˆ™æ‚¨ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ä½ç½®è·å–è®¸å¯è¯å‰¯æœ¬
http://www.apache.org/licenses/LICENSE-2.0
é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œæ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯æŒ‰ç…§â€œæŒ‰åŸæ ·â€åˆ†å‘ï¼Œä¸é™„å¸¦ä»»ä½•å½¢å¼çš„æ‹…ä¿æˆ–æ¡ä»¶ï¼Œæ— è®ºæ˜¯æ˜ç¤ºè¿˜æ˜¯æš—ç¤ºã€‚è¯·æŸ¥çœ‹è®¸å¯è¯äº†è§£å…·ä½“çš„æƒé™å’Œé™åˆ¶ã€‚
âš ï¸ è¯·æ³¨æ„ï¼Œè¯¥æ–‡ä»¶æ˜¯Markdownæ ¼å¼ï¼Œä½†åŒ…å«æˆ‘ä»¬çš„æ–‡æ¡£æ„å»ºå™¨ï¼ˆç±»ä¼¼äºMDXï¼‰çš„ç‰¹å®šè¯­æ³•ï¼Œå¯èƒ½æ— æ³•åœ¨æ‚¨çš„MarkdownæŸ¥çœ‹å™¨ä¸­æ­£ç¡®å‘ˆç°ã€‚
-->

# ç¿»è¯‘

[[open-in-colab]]
<Youtube id="1JvfrvZgi6c"/>

ç¿»è¯‘å°†ä¸€ä¸ªæ–‡æœ¬åºåˆ—ä»ä¸€ç§è¯­è¨€è½¬æ¢ä¸ºå¦ä¸€ç§è¯­è¨€ã€‚å®ƒæ˜¯å¤šä¸ªä»»åŠ¡ä¹‹ä¸€ï¼Œå¯ä»¥å°†å…¶è¡¨è¿°ä¸ºåºåˆ—åˆ°åºåˆ—é—®é¢˜ï¼Œè¿™æ˜¯ä¸€ä¸ªä»è¾“å…¥è¿”å›æŸäº›è¾“å‡ºçš„å¼ºå¤§æ¡†æ¶ï¼Œä¾‹å¦‚ç¿»è¯‘æˆ–æ‘˜è¦ã€‚ç¿»è¯‘ç³»ç»Ÿé€šå¸¸ç”¨äºä¸åŒè¯­è¨€æ–‡æœ¬ä¹‹é—´çš„ç¿»è¯‘ï¼Œä½†ä¹Ÿå¯ä»¥ç”¨äºè¯­éŸ³æˆ–æ–‡æœ¬åˆ°è¯­éŸ³æˆ–è¯­éŸ³åˆ°æ–‡æœ¬ä¹‹é—´çš„æŸç§ç»„åˆã€‚
æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š

1. åœ¨[OPUSå›¾ä¹¦](https://huggingface.co/datasets/opus_books)æ•°æ®é›†çš„è‹±æ³•å­é›†ä¸Šå¯¹[T5](https://huggingface.co/t5-small)è¿›è¡Œå¾®è°ƒï¼Œä»¥å°†è‹±æ–‡æ–‡æœ¬ç¿»è¯‘æˆæ³•è¯­ã€‚2. ä½¿ç”¨æ‚¨å¾®è°ƒçš„æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚

<Tip>
æœ¬æ•™ç¨‹ä¸­çš„ä»»åŠ¡å—ä»¥ä¸‹æ¨¡å‹æ¶æ„çš„æ”¯æŒï¼š
<!--æ­¤æç¤ºç”±`make fix-copies`è‡ªåŠ¨ç”Ÿæˆï¼Œè¯·å‹¿æ‰‹åŠ¨å¡«å†™ï¼-->
[BART](../model_doc/bart)ï¼Œ[BigBird-Pegasus](../model_doc/bigbird_pegasus)ï¼Œ[Blenderbot](../model_doc/blenderbot)ï¼Œ[BlenderbotSmall](../model_doc/blenderbot-small)ï¼Œ[Encoder decoder](../model_doc/encoder-decoder)ï¼Œ[FairSeq Machine-Translation](../model_doc/fsmt)ï¼Œ[GPTSAN-japanese](../model_doc/gptsan-japanese)ï¼Œ[LED](../model_doc/led)ï¼Œ[LongT5](../model_doc/longt5)ï¼Œ[M2M100](../model_doc/m2m_100)ï¼Œ[Marian](../model_doc/marian)ï¼Œ[mBART](../model_doc/mbart)ï¼Œ[MT5](../model_doc/mt5)ï¼Œ[MVP](../model_doc/mvp)ï¼Œ[NLLB](../model_doc/nllb)ï¼Œ[NLLB-MOE](../model_doc/nllb-moe)ï¼Œ[Pegasus](../model_doc/pegasus)ï¼Œ[PEGASUS-X](../model_doc/pegasus_x)ï¼Œ[PLBart](../model_doc/plbart)ï¼Œ[ProphetNet](../model_doc/prophetnet)ï¼Œ[SwitchTransformers](../model_doc/switch_transformers)ï¼Œ[T5](../model_doc/t5)ï¼Œ[XLM-ProphetNet](../model_doc/xlm-prophetnet)
<!--ç”Ÿæˆæç¤ºçš„æœ«å°¾-->
</Tip>

å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š
```bash
pip install transformers datasets evaluate sacrebleu
```

æˆ‘ä»¬é¼“åŠ±æ‚¨ç™»å½•æ‚¨çš„Hugging Faceè´¦æˆ·ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥ä¸Šä¼ å’Œå…±äº«æ‚¨çš„æ¨¡å‹ã€‚å½“æç¤ºæ—¶ï¼Œè¯·è¾“å…¥æ‚¨çš„ä»¤ç‰Œä»¥ç™»å½•ï¼š
```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## åŠ è½½OPUSå›¾ä¹¦æ•°æ®é›†

é¦–å…ˆï¼Œä½¿ç”¨ğŸ¤— Datasetsåº“åŠ è½½[OPUSå›¾ä¹¦](https://huggingface.co/datasets/opus_books)æ•°æ®é›†çš„è‹±æ³•å­é›†ï¼š
```py
>>> from datasets import load_dataset

>>> books = load_dataset("opus_books", "en-fr")
```

ä½¿ç”¨[`~datasets.Dataset.train_test_split`]æ–¹æ³•å°†æ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š
```py
>>> books = books["train"].train_test_split(test_size=0.2)
```

ç„¶åçœ‹ä¸€ä¸ªä¾‹å­ï¼š
```py
>>> books["train"][0]
{'id': '90560',
 'translation': {'en': 'But this lofty plateau measured only a few fathoms, and soon we reentered Our Element.',
  'fr': 'Mais ce plateau Ã©levÃ© ne mesurait que quelques toises, et bientÃ´t nous fÃ»mes rentrÃ©s dans notre Ã©lÃ©ment.'}}
```

`translation`ï¼šæ–‡æœ¬çš„è‹±æ–‡å’Œæ³•æ–‡ç¿»è¯‘ã€‚
## é¢„å¤„ç†
<Youtube id="XAR8jnZZuUs"/>
ä¸‹ä¸€æ­¥æ˜¯åŠ è½½T5åˆ†è¯å™¨æ¥å¤„ç†è‹±æ³•è¯­è¨€å¯¹ï¼š
```py
>>> from transformers import AutoTokenizer

>>> checkpoint = "t5-small"
>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

æ‚¨è¦åˆ›å»ºçš„é¢„å¤„ç†å‡½æ•°éœ€è¦æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
1. ä¸ºè¾“å…¥æ·»åŠ æç¤ºï¼Œä»¥ä¾¿T5çŸ¥é“è¿™æ˜¯ä¸€ä¸ªç¿»è¯‘ä»»åŠ¡ã€‚æŸäº›èƒ½å¤Ÿæ‰§è¡Œå¤šä¸ªNLPä»»åŠ¡çš„æ¨¡å‹éœ€è¦ä¸ºç‰¹å®šä»»åŠ¡æä¾›æç¤ºã€‚
2. å°†è¾“å…¥ï¼ˆè‹±æ–‡ï¼‰å’Œç›®æ ‡ï¼ˆæ³•æ–‡ï¼‰åˆ†åˆ«è¿›è¡Œæ ‡è®°åŒ–ï¼Œå› ä¸ºæ— æ³•ä½¿ç”¨åœ¨è‹±æ–‡è¯æ±‡ä¸Šé¢„è®­ç»ƒçš„æ ‡è®°å™¨å¯¹æ³•æ–‡æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–ã€‚
3. å°†åºåˆ—æˆªæ–­ä¸ºç”±`max_length`å‚æ•°è®¾ç½®çš„æœ€å¤§é•¿åº¦ã€‚
```py
>>> source_lang = "en"
>>> target_lang = "fr"
>>> prefix = "translate English to French: "


>>> def preprocess_function(examples):
...     inputs = [prefix + example[source_lang] for example in examples["translation"]]
...     targets = [example[target_lang] for example in examples["translation"]]
...     model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)
...     return model_inputs
```

è¦åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåº”ç”¨é¢„å¤„ç†å‡½æ•°ï¼Œä½¿ç”¨ğŸ¤— Datasetsçš„[`~datasets.Dataset.map`]æ–¹æ³•ã€‚é€šè¿‡å°†`batched=True`è®¾ç½®ä¸ºä¸€æ¬¡å¤„ç†æ•°æ®é›†çš„å¤šä¸ªå…ƒç´ ï¼Œæ‚¨å¯ä»¥åŠ é€Ÿ`map`å‡½æ•°çš„å¤„ç†é€Ÿåº¦ï¼š
```py
>>> tokenized_books = books.map(preprocess_function, batched=True)
```

ä½¿ç”¨[`DataCollatorForSeq2Seq`]åˆ›å»ºä¸€æ‰¹ç¤ºä¾‹ã€‚
åœ¨æ•´ç†è¿‡ç¨‹ä¸­ï¼Œå°†å¥å­åŠ¨æ€å¡«å……åˆ°æ‰¹ä¸­çš„æœ€é•¿é•¿åº¦ï¼Œè€Œä¸æ˜¯å°†æ•´ä¸ªæ•°æ®é›†å¡«å……åˆ°æœ€å¤§é•¿åº¦ã€‚
<frameworkcontent>
<pt>

```py
>>> from transformers import DataCollatorForSeq2Seq

>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)
```
</pt>
<tf>
```py
>>> from transformers import DataCollatorForSeq2Seq

>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors="tf")
```
</tf>
</frameworkcontent>

## è¯„ä¼°

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒ…å«åº¦é‡æ ‡å‡†é€šå¸¸æœ‰åŠ©äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index)åº“å¿«é€ŸåŠ è½½è¯„ä¼°æ–¹æ³•ã€‚å¯¹äºæ­¤ä»»åŠ¡ï¼ŒåŠ è½½[SacreBLEU](https://huggingface.co/spaces/evaluate-metric/sacrebleu)åº¦é‡æ ‡å‡†ï¼ˆè¯·å‚é˜…ğŸ¤— Evaluate [å¿«é€Ÿå¯¼è§ˆ](https://huggingface.co/docs/evaluate/a_quick_tour)ä»¥äº†è§£æœ‰å…³å¦‚ä½•åŠ è½½å’Œè®¡ç®—åº¦é‡æ ‡å‡†çš„æ›´å¤šä¿¡æ¯ï¼‰ï¼š
```py
>>> import evaluate

>>> metric = evaluate.load("sacrebleu")
```

ç„¶ååˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œå°†æ‚¨çš„é¢„æµ‹å’Œæ ‡ç­¾ä¼ é€’ç»™[`~evaluate.EvaluationModule.compute`]ä»¥è®¡ç®—SacreBLEUåˆ†æ•°ï¼š
```py
>>> import numpy as np


>>> def postprocess_text(preds, labels):
...     preds = [pred.strip() for pred in preds]
...     labels = [[label.strip()] for label in labels]

...     return preds, labels


>>> def compute_metrics(eval_preds):
...     preds, labels = eval_preds
...     if isinstance(preds, tuple):
...         preds = preds[0]
...     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

...     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
...     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

...     decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

...     result = metric.compute(predictions=decoded_preds, references=decoded_labels)
...     result = {"bleu": result["score"]}

...     prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
...     result["gen_len"] = np.mean(prediction_lens)
...     result = {k: round(v, 4) for k, v in result.items()}
...     return result
```

æ‚¨çš„`compute_metrics`å‡½æ•°ç°åœ¨å·²ç»å‡†å¤‡å¥½äº†ï¼Œåœ¨è®¾ç½®è®­ç»ƒæ—¶ä¼šè¿”å›åˆ°å®ƒã€‚

## è®­ç»ƒ

<frameworkcontent><pt>
<Tip>
å¦‚æœæ‚¨å¯¹ä½¿ç”¨[`Trainer`]è¿›è¡Œæ¨¡å‹å¾®è°ƒä¸ç†Ÿæ‚‰ï¼Œè¯·æŸ¥çœ‹åŸºæœ¬æ•™ç¨‹[æ­¤å¤„](../training#train-with-pytorch-trainer)ï¼
</Tip>

ç°åœ¨ï¼Œæ‚¨å·²å‡†å¤‡å¥½å¼€å§‹è®­ç»ƒæ‚¨çš„æ¨¡å‹äº†ï¼ä½¿ç”¨[`AutoModelForSeq2SeqLM`]åŠ è½½T5ï¼š
```py
>>> from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer

>>> model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)
```

æ­¤æ—¶ï¼Œåªå‰©ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š

1. åœ¨[`Seq2SeqTrainingArguments`]ä¸­å®šä¹‰æ‚¨çš„è®­ç»ƒè¶…å‚æ•°ã€‚å”¯ä¸€éœ€è¦çš„å‚æ•°æ˜¯`output_dir`ï¼Œå®ƒæŒ‡å®šä¿å­˜æ¨¡å‹çš„ä½ç½®ã€‚é€šè¿‡è®¾ç½®`push_to_hub=True`å°†æ­¤æ¨¡å‹æ¨é€åˆ°Hubï¼ˆéœ€è¦ç™»å½•Hugging Faceä»¥ä¸Šä¼ æ‚¨çš„æ¨¡å‹ï¼‰ã€‚åœ¨æ¯ä¸ªæ—¶æœŸç»“æŸæ—¶ï¼Œ[`Trainer`]å°†è¯„ä¼°SacreBLEUåº¦é‡æ ‡å‡†å¹¶ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹ã€‚
2. å°†è®­ç»ƒå‚æ•°ä¸æ¨¡å‹ã€æ•°æ®é›†ã€æ ‡è®°å™¨ã€æ•°æ®æ•´ç†å™¨å’Œ`compute_metrics`å‡½æ•°ä¸€èµ·ä¼ é€’ç»™[`Seq2SeqTrainer`]ã€‚
3. è°ƒç”¨[`~Trainer.train`]æ¥å¾®è°ƒæ‚¨çš„æ¨¡å‹ã€‚

```py
>>> training_args = Seq2SeqTrainingArguments(
...     output_dir="my_awesome_opus_books_model",
...     evaluation_strategy="epoch",
...     learning_rate=2e-5,
...     per_device_train_batch_size=16,
...     per_device_eval_batch_size=16,
...     weight_decay=0.01,
...     save_total_limit=3,
...     num_train_epochs=2,
...     predict_with_generate=True,
...     fp16=True,
...     push_to_hub=True,
... )

>>> trainer = Seq2SeqTrainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_books["train"],
...     eval_dataset=tokenized_books["test"],
...     tokenizer=tokenizer,
...     data_collator=data_collator,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
````

å®Œæˆè®­ç»ƒåï¼Œä½¿ç”¨[`~transformers.Trainer.push_to_hub`]æ–¹æ³•å°†æ‚¨çš„æ¨¡å‹å…±äº«åˆ°Hubï¼Œä»¥ä¾¿æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨æ‚¨çš„æ¨¡å‹ï¼š
```py
>>> trainer.push_to_hub()
```

</pt>
<tf>
<Tip>

å¦‚æœæ‚¨å¯¹ä½¿ç”¨Kerasè¿›è¡Œæ¨¡å‹å¾®è°ƒä¸ç†Ÿæ‚‰ï¼Œè¯·æŸ¥çœ‹åŸºæœ¬æ•™ç¨‹[æ­¤å¤„](../training#train-a-tensorflow-model-with-keras)ï¼
</Tip>

è¦åœ¨TensorFlowä¸­å¾®è°ƒæ¨¡å‹ï¼Œè¯·é¦–å…ˆè®¾ç½®ä¼˜åŒ–å™¨å‡½æ•°ã€å­¦ä¹ ç‡è®¡åˆ’å’Œä¸€äº›è®­ç»ƒè¶…å‚æ•°ï¼š

```py
>>> from transformers import AdamWeightDecay

>>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)
```

ç„¶åï¼Œä½¿ç”¨[`TFAutoModelForSeq2SeqLM`]åŠ è½½T5ï¼š
```py
>>> from transformers import TFAutoModelForSeq2SeqLM

>>> model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)
```

ä½¿ç”¨[`~transformers.TFPreTrainedModel.prepare_tf_dataset`]å°†æ•°æ®é›†è½¬æ¢ä¸º`tf.data.Dataset`æ ¼å¼ï¼š
```py
>>> tf_train_set = model.prepare_tf_dataset(
...     tokenized_books["train"],
...     shuffle=True,
...     batch_size=16,
...     collate_fn=data_collator,
... )

>>> tf_test_set = model.prepare_tf_dataset(
...     tokenized_books["test"],
...     shuffle=False,
...     batch_size=16,
...     collate_fn=data_collator,
... )
```

ä½¿ç”¨[`compile`](https://keras.io/api/models/model_training_apis/#compile-method)ä¸ºè®­ç»ƒé…ç½®æ¨¡å‹ã€‚è¯·æ³¨æ„ï¼ŒTransformersæ¨¡å‹éƒ½å…·æœ‰é»˜è®¤çš„ä¸ä»»åŠ¡ç›¸å…³çš„æŸå¤±å‡½æ•°ï¼Œå› æ­¤æ‚¨æ— éœ€æŒ‡å®šæŸå¤±å‡½æ•°ï¼Œé™¤éæ‚¨å¸Œæœ›ä½¿ç”¨å…¶ä»–æŸå¤±å‡½æ•°ï¼š
```py
>>> import tensorflow as tf

>>> model.compile(optimizer=optimizer)  # No loss argument!
```

åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œæœ€åä¸¤ä»¶è¦è®¾ç½®çš„äº‹æƒ…æ˜¯ä»é¢„æµ‹ä¸­è®¡ç®—SacreBLEUåº¦é‡æ ‡å‡†ï¼Œå¹¶æä¾›ä¸€ç§å°†æ¨¡å‹æ¨é€åˆ°Hubçš„æ–¹å¼ã€‚è¿™ä¸¤ä¸ªä»»åŠ¡éƒ½å¯ä»¥é€šè¿‡ä½¿ç”¨[Keraså›è°ƒ](../main_classes/keras_callbacks)æ¥å®Œæˆã€‚
å°†æ‚¨çš„`compute_metrics`å‡½æ•°ä¼ é€’ç»™[`~transformers.KerasMetricCallback`]ï¼š
```py
>>> from transformers.keras_callbacks import KerasMetricCallback

>>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)
```

åœ¨[`~transformers.PushToHubCallback`]ä¸­æŒ‡å®šè¦æ¨é€æ¨¡å‹å’Œåˆ†è¯å™¨çš„ä½ç½®ï¼š
```py
>>> from transformers.keras_callbacks import PushToHubCallback

>>> push_to_hub_callback = PushToHubCallback(
...     output_dir="my_awesome_opus_books_model",
...     tokenizer=tokenizer,
... )
```

ç„¶åå°†å›è°ƒå‡½æ•°æ†ç»‘åœ¨ä¸€èµ·ï¼š
```py
>>> callbacks = [metric_callback, push_to_hub_callback]
```

æœ€åï¼Œæ‚¨å¯ä»¥å¼€å§‹è®­ç»ƒæ¨¡å‹äº†ï¼ä½¿ç”¨[`fit`](https://keras.io/api/models/model_training_apis/#fit-method)æ–¹æ³•ï¼Œä¼ å…¥è®­ç»ƒæ•°æ®é›†ã€éªŒè¯æ•°æ®é›†ã€è®­ç»ƒè½®æ•°å’Œå›è°ƒå‡½æ•°æ¥å¾®è°ƒæ¨¡å‹ï¼š
```py
>>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=callbacks)
```

è®­ç»ƒå®Œæˆåï¼Œæ‚¨çš„æ¨¡å‹å°†è‡ªåŠ¨ä¸Šä¼ åˆ°Hubï¼Œè¿™æ ·æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨å®ƒï¼</tf></frameworkcontent>
<Tip>
å¦‚æœæ‚¨æƒ³äº†è§£æœ‰å…³å¦‚ä½•ä¸ºç¿»è¯‘å¾®è°ƒæ¨¡å‹çš„æ›´è¯¦ç»†ç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹ç›¸åº”çš„[PyTorchç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb)æˆ–è€…[TensorFlowç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb)ã€‚

</Tip>

## æ¨æ–­

å¤ªæ£’äº†ï¼Œç°åœ¨æ‚¨å·²ç»å¾®è°ƒäº†ä¸€ä¸ªæ¨¡å‹ï¼Œå¯ä»¥ç”¨å®ƒè¿›è¡Œæ¨æ–­äº†ï¼
å‡†å¤‡ä¸€äº›æ‚¨æƒ³è¦ç¿»è¯‘æˆå…¶ä»–è¯­è¨€çš„æ–‡æœ¬ã€‚å¯¹äºT5æ¨¡å‹ï¼Œæ‚¨éœ€è¦æ ¹æ®æ­£åœ¨å¤„ç†çš„ä»»åŠ¡ä¸ºè¾“å…¥æ·»åŠ å‰ç¼€ã€‚ä¾‹å¦‚ï¼Œè¦å°†è‹±æ–‡ç¿»è¯‘ä¸ºæ³•æ–‡ï¼Œæ‚¨åº”è¯¥åƒä¸‹é¢è¿™æ ·ä¸ºè¾“å…¥æ·»åŠ å‰ç¼€ï¼š

```py
>>> text = "translate English to French: Legumes share resources with nitrogen-fixing bacteria."
```

å°è¯•ä½¿ç”¨æ‚¨å¾®è°ƒçš„æ¨¡å‹è¿›è¡Œæ¨æ–­çš„æœ€ç®€å•æ–¹æ³•æ˜¯åœ¨[`pipeline`]ä¸­ä½¿ç”¨å®ƒã€‚å®ä¾‹åŒ–ä¸€ä¸ªç”¨äºç¿»è¯‘çš„`pipeline`ï¼Œå¹¶å°†æ–‡æœ¬ä¼ é€’ç»™å®ƒï¼š
```py
>>> from transformers import pipeline

>>> translator = pipeline("translation", model="my_awesome_opus_books_model")
>>> translator(text)
[{'translation_text': 'Legumes partagent des ressources avec des bactÃ©ries azotantes.'}]
```

å¦‚æœæ‚¨æƒ³è¦æ‰‹åŠ¨å¤åˆ¶`pipeline`çš„ç»“æœï¼Œä¹Ÿå¯ä»¥è¿™æ ·åšï¼š
<frameworkcontent><pt>å°†æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–ï¼Œå¹¶å°†`input_ids`è¿”å›ä¸ºPyTorchå¼ é‡ï¼š
```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("my_awesome_opus_books_model")
>>> inputs = tokenizer(text, return_tensors="pt").input_ids
```

ä½¿ç”¨[`~transformers.generation_utils.GenerationMixin.generate`]æ–¹æ³•ç”Ÿæˆç¿»è¯‘ç»“æœã€‚

æœ‰å…³ä¸åŒæ–‡æœ¬ç”Ÿæˆç­–ç•¥å’Œæ§åˆ¶ç”Ÿæˆçš„å‚æ•°çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[Text Generation](../main_classes/text_generation) APIã€‚

```py
>>> from transformers import AutoModelForSeq2SeqLM

>>> model = AutoModelForSeq2SeqLM.from_pretrained("my_awesome_opus_books_model")
>>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)
```

å°†ç”Ÿæˆçš„æ ‡è®°IDè§£ç ä¸ºæ–‡æœ¬ï¼š
```py
>>> tokenizer.decode(outputs[0], skip_special_tokens=True)
'Les lignÃ©es partagent des ressources avec des bactÃ©ries enfixant l'azote.'
```
</pt><tf>å°†æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–ï¼Œå¹¶å°†`input_ids`è¿”å›ä¸ºTensorFlowå¼ é‡ï¼š
```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("my_awesome_opus_books_model")
>>> inputs = tokenizer(text, return_tensors="tf").input_ids
```

ä½¿ç”¨[`~transformers.generation_tf_utils.TFGenerationMixin.generate`]æ–¹æ³•ç”Ÿæˆç¿»è¯‘ç»“æœã€‚

æœ‰å…³ä¸åŒæ–‡æœ¬ç”Ÿæˆç­–ç•¥å’Œæ§åˆ¶ç”Ÿæˆçš„å‚æ•°çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[Text Generation](../main_classes/text_generation) APIã€‚

```py
>>> from transformers import TFAutoModelForSeq2SeqLM

>>> model = TFAutoModelForSeq2SeqLM.from_pretrained("my_awesome_opus_books_model")
>>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)
```

å°†ç”Ÿæˆçš„æ ‡è®°IDè§£ç ä¸ºæ–‡æœ¬ï¼š
```py
>>> tokenizer.decode(outputs[0], skip_special_tokens=True)
'Les lugumes partagent les ressources avec des bactÃ©ries fixatrices d'azote.'
```
</tf>
</frameworkcontent>