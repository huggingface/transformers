<!--
Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.
-->

# è‡ªåŠ¨è¯­éŸ³è¯†åˆ«

[[open-in-colab]]

<Youtube id="TksaY_FDgnk"/>

è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å°†è¯­éŸ³ä¿¡å·è½¬æ¢ä¸ºæ–‡æœ¬ï¼Œå°†ä¸€ç³»åˆ—éŸ³é¢‘è¾“å…¥æ˜ å°„åˆ°æ–‡æœ¬è¾“å‡ºã€‚
Siri å’Œ Alexa è¿™ç±»è™šæ‹ŸåŠ©æ‰‹ä½¿ç”¨ ASR æ¨¡å‹æ¥å¸®åŠ©ç”¨æˆ·æ—¥å¸¸ç”Ÿæ´»ï¼Œè¿˜æœ‰è®¸å¤šå…¶ä»–é¢å‘ç”¨æˆ·çš„æœ‰ç”¨åº”ç”¨ï¼Œå¦‚ä¼šè®®å®æ—¶å­—å¹•å’Œä¼šè®®çºªè¦ã€‚

æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š

1. åœ¨ [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) æ•°æ®é›†ä¸Šå¯¹
   [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base) è¿›è¡Œå¾®è°ƒï¼Œä»¥å°†éŸ³é¢‘è½¬å½•ä¸ºæ–‡æœ¬ã€‚
2. ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨æ–­ã€‚

<Tip>

æœ¬æ•™ç¨‹ä¸­å±•ç¤ºçš„ä»»åŠ¡å—ä»¥ä¸‹æ¨¡å‹æ¶æ„çš„æ”¯æŒï¼š

<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->

[Data2VecAudio](../model_doc/data2vec-audio), [Hubert](../model_doc/hubert), [M-CTC-T](../model_doc/mctct), [SEW](../model_doc/sew), [SEW-D](../model_doc/sew-d), [UniSpeech](../model_doc/unispeech), [UniSpeechSat](../model_doc/unispeech-sat), [Wav2Vec2](../model_doc/wav2vec2), [Wav2Vec2-BERT](../model_doc/wav2vec2-bert), [Wav2Vec2-Conformer](../model_doc/wav2vec2-conformer), [WavLM](../model_doc/wavlm)

<!--End of the generated tip-->

</Tip>

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š

```bash
pip install transformers datasets evaluate jiwer
```

æˆ‘ä»¬é¼“åŠ±æ‚¨ç™»å½•è‡ªå·±çš„ Hugging Face è´¦æˆ·ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥ä¸Šä¼ å¹¶ä¸ç¤¾åŒºåˆ†äº«æ‚¨çš„æ¨¡å‹ã€‚
å‡ºç°æç¤ºæ—¶ï¼Œè¾“å…¥æ‚¨çš„ä»¤ç‰Œç™»å½•ï¼š

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## åŠ è½½ MInDS-14 æ•°æ®é›†

é¦–å…ˆä»ğŸ¤— Datasets åº“ä¸­åŠ è½½ [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14)
æ•°æ®é›†çš„ä¸€ä¸ªè¾ƒå°å­é›†ã€‚è¿™å°†è®©æ‚¨æœ‰æœºä¼šå…ˆè¿›è¡Œå®éªŒï¼Œç¡®ä¿ä¸€åˆ‡æ­£å¸¸ï¼Œç„¶åå†èŠ±æ›´å¤šæ—¶é—´åœ¨å®Œæ•´æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚

```py
>>> from datasets import load_dataset, Audio

>>> minds = load_dataset("PolyAI/minds14", name="en-US", split="train[:100]")
```

ä½¿ç”¨ [`~Dataset.train_test_split`] æ–¹æ³•å°†æ•°æ®é›†çš„ `train` æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š

```py
>>> minds = minds.train_test_split(test_size=0.2)
```

ç„¶åçœ‹çœ‹æ•°æ®é›†ï¼š

```py
>>> minds
DatasetDict({
    train: Dataset({
        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],
        num_rows: 16
    })
    test: Dataset({
        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],
        num_rows: 4
    })
})
```

è™½ç„¶æ•°æ®é›†åŒ…å« `lang_id `å’Œ `english_transcription` ç­‰è®¸å¤šæœ‰ç”¨çš„ä¿¡æ¯ï¼Œä½†åœ¨æœ¬æŒ‡å—ä¸­ï¼Œ
æ‚¨å°†ä¸“æ³¨äº `audio` å’Œ `transcription`ã€‚ä½¿ç”¨ [`~datasets.Dataset.remove_columns`] æ–¹æ³•åˆ é™¤å…¶ä»–åˆ—ï¼š

```py
>>> minds = minds.remove_columns(["english_transcription", "intent_class", "lang_id"])
```

å†çœ‹çœ‹ç¤ºä¾‹ï¼š

```py
>>> minds["train"][0]
{'audio': {'array': array([-0.00024414,  0.        ,  0.        , ...,  0.00024414,
          0.00024414,  0.00024414], dtype=float32),
  'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav',
  'sampling_rate': 8000},
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav',
 'transcription': "hi I'm trying to use the banking app on my phone and currently my checking and savings account balance is not refreshing"}
```

æœ‰ 2 ä¸ªå­—æ®µï¼š

- `audio`ï¼šç”±è¯­éŸ³ä¿¡å·å½¢æˆçš„ä¸€ç»´ `array`ï¼Œç”¨äºåŠ è½½å’Œé‡æ–°é‡‡æ ·éŸ³é¢‘æ–‡ä»¶ã€‚
- `transcription`ï¼šç›®æ ‡æ–‡æœ¬ã€‚

## é¢„å¤„ç†

ä¸‹ä¸€æ­¥æ˜¯åŠ è½½ä¸€ä¸ª Wav2Vec2 å¤„ç†å™¨æ¥å¤„ç†éŸ³é¢‘ä¿¡å·ï¼š

```py
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base")
```

MInDS-14 æ•°æ®é›†çš„é‡‡æ ·ç‡ä¸º 8000kHzï¼ˆæ‚¨å¯ä»¥åœ¨å…¶[æ•°æ®é›†å¡ç‰‡](https://huggingface.co/datasets/PolyAI/minds14)ä¸­æ‰¾åˆ°æ­¤ä¿¡æ¯ï¼‰ï¼Œ
è¿™æ„å‘³ç€æ‚¨éœ€è¦å°†æ•°æ®é›†é‡æ–°é‡‡æ ·ä¸º 16000kHz ä»¥ä½¿ç”¨é¢„è®­ç»ƒçš„ Wav2Vec2 æ¨¡å‹ï¼š

```py
>>> minds = minds.cast_column("audio", Audio(sampling_rate=16_000))
>>> minds["train"][0]
{'audio': {'array': array([-2.38064706e-04, -1.58618059e-04, -5.43987835e-06, ...,
          2.78103951e-04,  2.38446111e-04,  1.18740834e-04], dtype=float32),
  'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav',
  'sampling_rate': 16000},
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav',
 'transcription': "hi I'm trying to use the banking app on my phone and currently my checking and savings account balance is not refreshing"}
```

å¦‚æ‚¨åœ¨ä¸Šé¢çš„ `transcription` ä¸­æ‰€çœ‹åˆ°çš„ï¼Œæ–‡æœ¬åŒ…å«å¤§å°å†™å­—ç¬¦çš„æ··åˆã€‚
Wav2Vec2 åˆ†è¯å™¨ä»…è®­ç»ƒäº†å¤§å†™å­—ç¬¦ï¼Œå› æ­¤æ‚¨éœ€è¦ç¡®ä¿æ–‡æœ¬ä¸åˆ†è¯å™¨çš„è¯æ±‡è¡¨åŒ¹é…ï¼š

```py
>>> def uppercase(example):
...     return {"transcription": example["transcription"].upper()}


>>> minds = minds.map(uppercase)
```

ç°åœ¨åˆ›å»ºä¸€ä¸ªé¢„å¤„ç†å‡½æ•°ï¼Œè¯¥å‡½æ•°åº”è¯¥ï¼š

1. è°ƒç”¨ `audio` åˆ—ä»¥åŠ è½½å’Œé‡æ–°é‡‡æ ·éŸ³é¢‘æ–‡ä»¶ã€‚
2. ä»éŸ³é¢‘æ–‡ä»¶ä¸­æå– `input_values` å¹¶ä½¿ç”¨å¤„ç†å™¨å¯¹ `transcription` åˆ—æ‰§è¡Œ tokenizer æ“ä½œã€‚

```py
>>> def prepare_dataset(batch):
...     audio = batch["audio"]
...     batch = processor(audio["array"], sampling_rate=audio["sampling_rate"], text=batch["transcription"])
...     batch["input_length"] = len(batch["input_values"][0])
...     return batch
```

è¦åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåº”ç”¨é¢„å¤„ç†å‡½æ•°ï¼Œå¯ä»¥ä½¿ç”¨ğŸ¤— Datasets çš„ [`~datasets.Dataset.map`] å‡½æ•°ã€‚
æ‚¨å¯ä»¥é€šè¿‡å¢åŠ  `num_proc` å‚æ•°æ¥åŠ é€Ÿ `map` çš„å¤„ç†è¿›ç¨‹æ•°é‡ã€‚
ä½¿ç”¨ [`~datasets.Dataset.remove_columns`] æ–¹æ³•åˆ é™¤ä¸éœ€è¦çš„åˆ—ï¼š

```py
>>> encoded_minds = minds.map(prepare_dataset, remove_columns=minds.column_names["train"], num_proc=4)
```

ğŸ¤— Transformers æ²¡æœ‰ç”¨äº ASR çš„æ•°æ®æ•´ç†å™¨ï¼Œå› æ­¤æ‚¨éœ€è¦è°ƒæ•´ [`DataCollatorWithPadding`] æ¥åˆ›å»ºä¸€ä¸ªç¤ºä¾‹æ‰¹æ¬¡ã€‚
å®ƒè¿˜ä¼šåŠ¨æ€åœ°å°†æ‚¨çš„æ–‡æœ¬å’Œæ ‡ç­¾å¡«å……åˆ°å…¶æ‰¹æ¬¡ä¸­æœ€é•¿å…ƒç´ çš„é•¿åº¦ï¼ˆè€Œä¸æ˜¯æ•´ä¸ªæ•°æ®é›†ï¼‰ï¼Œä»¥ä½¿å®ƒä»¬å…·æœ‰ç»Ÿä¸€çš„é•¿åº¦ã€‚
è™½ç„¶å¯ä»¥é€šè¿‡åœ¨ `tokenizer` å‡½æ•°ä¸­è®¾ç½® `padding=True` æ¥å¡«å……æ–‡æœ¬ï¼Œä½†åŠ¨æ€å¡«å……æ›´æœ‰æ•ˆã€‚

ä¸å…¶ä»–æ•°æ®æ•´ç†å™¨ä¸åŒï¼Œè¿™ä¸ªç‰¹å®šçš„æ•°æ®æ•´ç†å™¨éœ€è¦å¯¹ `input_values` å’Œ `labels `åº”ç”¨ä¸åŒçš„å¡«å……æ–¹æ³•ï¼š

```py
>>> import torch

>>> from dataclasses import dataclass, field
>>> from typing import Any, Dict, List, Optional, Union


>>> @dataclass
... class DataCollatorCTCWithPadding:
...     processor: AutoProcessor
...     padding: Union[bool, str] = "longest"

...     def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
...         # split inputs and labels since they have to be of different lengths and need
...         # different padding methods
...         input_features = [{"input_values": feature["input_values"][0]} for feature in features]
...         label_features = [{"input_ids": feature["labels"]} for feature in features]

...         batch = self.processor.pad(input_features, padding=self.padding, return_tensors="pt")

...         labels_batch = self.processor.pad(labels=label_features, padding=self.padding, return_tensors="pt")

...         # replace padding with -100 to ignore loss correctly
...         labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

...         batch["labels"] = labels

...         return batch
```

ç°åœ¨å®ä¾‹åŒ–æ‚¨çš„ `DataCollatorForCTCWithPadding`ï¼š

```py
>>> data_collator = DataCollatorCTCWithPadding(processor=processor, padding="longest")
```

## è¯„ä¼°

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒ…å«ä¸€ä¸ªæŒ‡æ ‡é€šå¸¸æœ‰åŠ©äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚
æ‚¨å¯ä»¥é€šè¿‡ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) åº“å¿«é€ŸåŠ è½½ä¸€ä¸ªè¯„ä¼°æ–¹æ³•ã€‚
å¯¹äºè¿™ä¸ªä»»åŠ¡ï¼ŒåŠ è½½ [word error rate](https://huggingface.co/spaces/evaluate-metric/wer)ï¼ˆWERï¼‰æŒ‡æ ‡
ï¼ˆè¯·å‚é˜…ğŸ¤— Evaluate [å¿«é€Ÿä¸Šæ‰‹](https://huggingface.co/docs/evaluate/a_quick_tour)ä»¥äº†è§£å¦‚ä½•åŠ è½½å’Œè®¡ç®—æŒ‡æ ‡ï¼‰ï¼š

```py
>>> import evaluate

>>> wer = evaluate.load("wer")
```

ç„¶ååˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œå°†æ‚¨çš„é¢„æµ‹å’Œæ ‡ç­¾ä¼ é€’ç»™ [`~evaluate.EvaluationModule.compute`] æ¥è®¡ç®— WERï¼š

```py
>>> import numpy as np


>>> def compute_metrics(pred):
...     pred_logits = pred.predictions
...     pred_ids = np.argmax(pred_logits, axis=-1)

...     pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id

...     pred_str = processor.batch_decode(pred_ids)
...     label_str = processor.batch_decode(pred.label_ids, group_tokens=False)

...     wer = wer.compute(predictions=pred_str, references=label_str)

...     return {"wer": wer}
```

æ‚¨çš„ `compute_metrics` å‡½æ•°ç°åœ¨å·²ç»å‡†å¤‡å°±ç»ªï¼Œå½“æ‚¨è®¾ç½®å¥½è®­ç»ƒæ—¶å°†è¿”å›ç»™æ­¤å‡½æ•°ã€‚

## è®­ç»ƒ

<frameworkcontent>
<pt>
<Tip>

å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰ä½¿ç”¨[`Trainer`]å¾®è°ƒæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹è¿™é‡Œçš„åŸºæœ¬æ•™ç¨‹[here](../training#train-with-pytorch-trainer)ï¼

</Tip>

ç°åœ¨æ‚¨å·²ç»å‡†å¤‡å¥½å¼€å§‹è®­ç»ƒæ‚¨çš„æ¨¡å‹äº†ï¼ä½¿ç”¨ [`AutoModelForCTC`] åŠ è½½ Wav2Vec2ã€‚
ä½¿ç”¨ `ctc_loss_reduction` å‚æ•°æŒ‡å®šè¦åº”ç”¨çš„å‡å°‘æ–¹å¼ã€‚é€šå¸¸æœ€å¥½ä½¿ç”¨å¹³å‡å€¼è€Œä¸æ˜¯é»˜è®¤çš„æ±‚å’Œï¼š

```py
>>> from transformers import AutoModelForCTC, TrainingArguments, Trainer

>>> model = AutoModelForCTC.from_pretrained(
...     "facebook/wav2vec2-base",
...     ctc_loss_reduction="mean",
...     pad_token_id=processor.tokenizer.pad_token_id,
)
```

æ­¤æ—¶ï¼Œåªå‰©ä¸‹ 3 ä¸ªæ­¥éª¤ï¼š

1. åœ¨ [`TrainingArguments`] ä¸­å®šä¹‰æ‚¨çš„è®­ç»ƒå‚æ•°ã€‚å”¯ä¸€å¿…éœ€çš„å‚æ•°æ˜¯ `output_dir`ï¼Œç”¨äºæŒ‡å®šä¿å­˜æ¨¡å‹çš„ä½ç½®ã€‚
   æ‚¨å¯ä»¥é€šè¿‡è®¾ç½® `push_to_hub=True` å°†æ­¤æ¨¡å‹æ¨é€åˆ° Hubï¼ˆæ‚¨éœ€è¦ç™»å½•åˆ° Hugging Face æ‰èƒ½ä¸Šä¼ æ‚¨çš„æ¨¡å‹ï¼‰ã€‚
   åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶ï¼Œ[`Trainer`] å°†è¯„ä¼° WER å¹¶ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹ã€‚
2. å°†è®­ç»ƒå‚æ•°ä¸æ¨¡å‹ã€æ•°æ®é›†ã€åˆ†è¯å™¨ã€æ•°æ®æ•´ç†å™¨å’Œ `compute_metrics` å‡½æ•°ä¸€èµ·ä¼ é€’ç»™ [`Trainer`]ã€‚
3. è°ƒç”¨ [`~Trainer.train`] æ¥å¾®è°ƒæ‚¨çš„æ¨¡å‹ã€‚

```py
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_asr_mind_model",
...     per_device_train_batch_size=8,
...     gradient_accumulation_steps=2,
...     learning_rate=1e-5,
...     warmup_steps=500,
...     max_steps=2000,
...     gradient_checkpointing=True,
...     fp16=True,
...     group_by_length=True,
...     eval_strategy="steps",
...     per_device_eval_batch_size=8,
...     save_steps=1000,
...     eval_steps=1000,
...     logging_steps=25,
...     load_best_model_at_end=True,
...     metric_for_best_model="wer",
...     greater_is_better=False,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=encoded_minds["train"],
...     eval_dataset=encoded_minds["test"],
...     tokenizer=processor,
...     data_collator=data_collator,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨ [`~transformers.Trainer.push_to_hub`] æ–¹æ³•å°†æ‚¨çš„æ¨¡å‹åˆ†äº«åˆ° Hubï¼Œæ–¹ä¾¿å¤§å®¶ä½¿ç”¨æ‚¨çš„æ¨¡å‹ï¼š

```py
>>> trainer.push_to_hub()
```
</pt>
</frameworkcontent>

<Tip>

è¦æ·±å…¥äº†è§£å¦‚ä½•å¾®è°ƒæ¨¡å‹è¿›è¡Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼Œ
è¯·æŸ¥çœ‹è¿™ç¯‡åšå®¢[æ–‡ç« ](https://huggingface.co/blog/fine-tune-wav2vec2-english)ä»¥äº†è§£è‹±è¯­ ASRï¼Œ
è¿˜å¯ä»¥å‚é˜…[è¿™ç¯‡æ–‡ç« ](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2)ä»¥äº†è§£å¤šè¯­è¨€ ASRã€‚

</Tip>

## æ¨æ–­

å¾ˆå¥½ï¼Œç°åœ¨æ‚¨å·²ç»å¾®è°ƒäº†ä¸€ä¸ªæ¨¡å‹ï¼Œæ‚¨å¯ä»¥ç”¨å®ƒè¿›è¡Œæ¨æ–­äº†ï¼

åŠ è½½æ‚¨æƒ³è¦è¿è¡Œæ¨æ–­çš„éŸ³é¢‘æ–‡ä»¶ã€‚è¯·è®°ä½ï¼Œå¦‚æœéœ€è¦ï¼Œå°†éŸ³é¢‘æ–‡ä»¶çš„é‡‡æ ·ç‡é‡æ–°é‡‡æ ·ä¸ºä¸æ¨¡å‹åŒ¹é…çš„é‡‡æ ·ç‡ï¼

```py
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", "en-US", split="train")
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
>>> sampling_rate = dataset.features["audio"].sampling_rate
>>> audio_file = dataset[0]["audio"]["path"]
```

å°è¯•ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨æ–­çš„æœ€ç®€å•æ–¹æ³•æ˜¯ä½¿ç”¨ [`pipeline`]ã€‚
ä½¿ç”¨æ‚¨çš„æ¨¡å‹å®ä¾‹åŒ–ä¸€ä¸ªç”¨äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«çš„ `pipeline`ï¼Œå¹¶å°†æ‚¨çš„éŸ³é¢‘æ–‡ä»¶ä¼ é€’ç»™å®ƒï¼š

```py
>>> from transformers import pipeline

>>> transcriber = pipeline("automatic-speech-recognition", model="stevhliu/my_awesome_asr_minds_model")
>>> transcriber(audio_file)
{'text': 'I WOUD LIKE O SET UP JOINT ACOUNT WTH Y PARTNER'}
```

<Tip>

è½¬å½•ç»“æœè¿˜ä¸é”™ï¼Œä½†å¯ä»¥æ›´å¥½ï¼å°è¯•ç”¨æ›´å¤šç¤ºä¾‹å¾®è°ƒæ‚¨çš„æ¨¡å‹ï¼Œä»¥è·å¾—æ›´å¥½çš„ç»“æœï¼

</Tip>

å¦‚æœæ‚¨æ„¿æ„ï¼Œæ‚¨ä¹Ÿå¯ä»¥æ‰‹åŠ¨å¤åˆ¶ `pipeline` çš„ç»“æœï¼š

<frameworkcontent>
<pt>

åŠ è½½ä¸€ä¸ªå¤„ç†å™¨æ¥é¢„å¤„ç†éŸ³é¢‘æ–‡ä»¶å’Œè½¬å½•ï¼Œå¹¶å°† `input` è¿”å›ä¸º PyTorch å¼ é‡ï¼š

```py
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("stevhliu/my_awesome_asr_mind_model")
>>> inputs = processor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")
```

å°†æ‚¨çš„è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å¹¶è¿”å› logitsï¼š

```py
>>> from transformers import AutoModelForCTC

>>> model = AutoModelForCTC.from_pretrained("stevhliu/my_awesome_asr_mind_model")
>>> with torch.no_grad():
...     logits = model(**inputs).logits
```

è·å–å…·æœ‰æœ€é«˜æ¦‚ç‡çš„é¢„æµ‹ `input_ids`ï¼Œå¹¶ä½¿ç”¨å¤„ç†å™¨å°†é¢„æµ‹çš„ `input_ids` è§£ç å›æ–‡æœ¬ï¼š

```py
>>> import torch

>>> predicted_ids = torch.argmax(logits, dim=-1)
>>> transcription = processor.batch_decode(predicted_ids)
>>> transcription
['I WOUL LIKE O SET UP JOINT ACOUNT WTH Y PARTNER']
```
</pt>
</frameworkcontent>