<!--ç‰ˆæƒæ‰€æœ‰ 2022 å¹´ HuggingFace å›¢é˜Ÿã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚
æ ¹æ® Apache è®¸å¯è¯ç¬¬ 2.0 ç‰ˆï¼ˆâ€œè®¸å¯è¯â€ï¼‰è·å¾—è®¸å¯ï¼Œé™¤éç¬¦åˆè®¸å¯è¯çš„è§„å®šï¼Œå¦åˆ™æ‚¨ä¸èƒ½ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚æ‚¨å¯ä»¥åœ¨è®¸å¯è¯å¤„è·å¾—è®¸å¯è¯å‰¯æœ¬ã€‚
http://www.apache.org/licenses/LICENSE-2.0
é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œæ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶ä»¥ "æŒ‰åŸæ ·" çš„æ–¹å¼åˆ†å‘ï¼Œä¸é™„å¸¦ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„æ‹…ä¿æˆ–æ¡ä»¶ã€‚è¯·å‚é˜…è®¸å¯è¯ä»¥äº†è§£è®¸å¯è¯ä¸‹çš„ç‰¹å®šè¯­è¨€çš„æƒé™å’Œé™åˆ¶ã€‚ç‰¹å®šè¯­è¨€çš„æƒé™å’Œé™åˆ¶ã€‚ç‰¹å®šè¯­è¨€çš„æƒé™å’Œé™åˆ¶ã€‚
âš ï¸ è¯·æ³¨æ„ï¼Œæ­¤æ–‡ä»¶æ˜¯ Markdown æ ¼å¼ï¼Œä½†åŒ…å«æˆ‘ä»¬çš„æ–‡æ¡£ç”Ÿæˆå™¨çš„ç‰¹å®šè¯­æ³•ï¼ˆç±»ä¼¼äº MDXï¼‰ï¼Œå¯èƒ½æ— æ³•åœ¨æ‚¨çš„ Markdown æŸ¥çœ‹å™¨ä¸­æ­£ç¡®æ¸²æŸ“ã€‚
-->

# éŸ³é¢‘åˆ†ç±»

[[åœ¨ Colab ä¸­æ‰“å¼€]]
<Youtube id="KWwzcmG98Ds"/>

éŸ³é¢‘åˆ†ç±» - ä¸æ–‡æœ¬ç±»ä¼¼ - ä»è¾“å…¥æ•°æ®ä¸­è¾“å‡ºä¸€ä¸ªç±»åˆ«æ ‡ç­¾ã€‚å”¯ä¸€çš„åŒºåˆ«æ˜¯ï¼Œæ‚¨æ‹¥æœ‰çš„æ˜¯åŸå§‹éŸ³é¢‘æ³¢å½¢ï¼Œè€Œä¸æ˜¯æ–‡æœ¬è¾“å…¥ã€‚éŸ³é¢‘åˆ†ç±»çš„ä¸€äº›å®é™…åº”ç”¨åŒ…æ‹¬è¯†åˆ«è¯´è¯è€…æ„å›¾ã€è¯­è¨€åˆ†ç±»ï¼Œç”šè‡³é€šè¿‡å£°éŸ³è¯†åˆ«åŠ¨ç‰©ç‰©ç§ã€‚

æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š

1. åœ¨ [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) æ•°æ®é›†ä¸Šå¾®è°ƒ [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base) ä»¥åˆ†ç±»è¯´è¯è€…æ„å›¾ã€‚
2. ä½¿ç”¨æ‚¨å¾®è°ƒçš„æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚

<Tip> 
æœ¬æ•™ç¨‹ä¸­æ‰€ç¤ºçš„ä»»åŠ¡ç”±ä»¥ä¸‹æ¨¡å‹æ¶æ„æ”¯æŒï¼š
<!--æ­¤æç¤ºæ˜¯ç”±`make fix-copies`è‡ªåŠ¨ç”Ÿæˆçš„ï¼Œè¯·å‹¿æ‰‹åŠ¨å¡«å†™ï¼-->
[éŸ³é¢‘é¢‘è°±å˜æ¢å™¨](../model_doc/audio-spectrogram-transformer)ï¼Œ[Data2Vec éŸ³é¢‘](../model_doc/data2vec-audio)ï¼Œ[Hubert](../model_doc/hubert)ï¼Œ[SEW](../model_doc/sew)ï¼Œ[SEW-D](../model_doc/sew-d)ï¼Œ[UniSpeech](../model_doc/unispeech)ï¼Œ[UniSpeechSat](../model_doc/unispeech-sat)ï¼Œ[Wav2Vec2](../model_doc/wav2vec2)ï¼Œ[Wav2Vec2-Conformer](../model_doc/wav2vec2-conformer)ï¼Œ[WavLM](../model_doc/wavlm)ï¼Œ[Whisper](../model_doc/whisper)
<!--ç”Ÿæˆæç¤ºçš„æœ«å°¾-->
</Tip>

å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š
```bash
pip install transformers datasets evaluate
```

æˆ‘ä»¬é¼“åŠ±æ‚¨ç™»å½• Hugging Face å¸æˆ·ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥ä¸ç¤¾åŒºä¸Šä¼ å’Œåˆ†äº«æ‚¨çš„æ¨¡å‹ã€‚åœ¨æç¤ºæ—¶ï¼Œè¾“å…¥æ‚¨çš„ä»¤ç‰Œç™»å½•ï¼š
```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## åŠ è½½ MInDS-14 æ•°æ®é›†

é¦–å…ˆä»ğŸ¤— Datasets åº“ä¸­åŠ è½½ MInDS-14 æ•°æ®é›†ï¼š
```py
>>> from datasets import load_dataset, Audio

>>> minds = load_dataset("PolyAI/minds14", name="en-US", split="train")
```

ä½¿ç”¨ [`~datasets.Dataset.train_test_split`] æ–¹æ³•å°†æ•°æ®é›†çš„ `train` æ‹†åˆ†ä¸ºè¾ƒå°çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚è¿™æ ·æ‚¨å¯ä»¥åœ¨å¤„ç†å®Œæ•´æ•°æ®é›†ä¹‹å‰è¿›è¡Œå®éªŒå’Œç¡®ä¿ä¸€åˆ‡æ­£å¸¸ã€‚
```py
>>> minds = minds.train_test_split(test_size=0.2)
```

ç„¶åæŸ¥çœ‹æ•°æ®é›†ï¼š
```py
>>> minds
DatasetDict({
    train: Dataset({
        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],
        num_rows: 450
    })
    test: Dataset({
        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],
        num_rows: 113
    })
})
```

å°½ç®¡æ•°æ®é›†åŒ…å«è®¸å¤šæœ‰ç”¨çš„ä¿¡æ¯ï¼Œæ¯”å¦‚ `lang_id` å’Œ `english_transcription`ï¼Œä½†åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæ‚¨å°†å…³æ³¨ `audio` å’Œ `intent_class`ã€‚

ä½¿ç”¨ [`~datasets.Dataset.remove_columns`] æ–¹æ³•åˆ é™¤å…¶ä»–åˆ—ï¼š
```py
>>> minds = minds.remove_columns(["path", "transcription", "english_transcription", "lang_id"])
```

ç°åœ¨æ¥çœ‹ä¸€ä¸ªç¤ºä¾‹ï¼š
```py
>>> minds["train"][0]
{'audio': {'array': array([ 0.        ,  0.        ,  0.        , ..., -0.00048828,
         -0.00024414, -0.00024414], dtype=float32),
  'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602b9a5fbb1e6d0fbce91f52.wav',
  'sampling_rate': 8000},
 'intent_class': 2}
```

æœ‰ä¸¤ä¸ªå­—æ®µï¼š
- `audio`ï¼šè¡¨ç¤ºå¿…é¡»è°ƒç”¨ä»¥åŠ è½½å’Œé‡é‡‡æ ·éŸ³é¢‘æ–‡ä»¶çš„è¯­éŸ³ä¿¡å·çš„ä¸€ç»´ `array`ã€‚- `intent_class`ï¼šè¡¨ç¤ºè¯´è¯è€…æ„å›¾çš„ç±»åˆ« IDã€‚
ä¸ºäº†ä½¿æ¨¡å‹èƒ½å¤Ÿä»æ ‡ç­¾ ID è·å–æ ‡ç­¾åç§°ï¼Œåˆ›å»ºä¸€ä¸ªå°†æ ‡ç­¾åç§°æ˜ å°„åˆ°æ•´æ•°åŠå…¶ç›¸åçš„å­—å…¸ï¼š
```py
>>> labels = minds["train"].features["intent_class"].names
>>> label2id, id2label = dict(), dict()
>>> for i, label in enumerate(labels):
...     label2id[label] = str(i)
...     id2label[str(i)] = label
```

ç°åœ¨æ‚¨å¯ä»¥å°†æ ‡ç­¾ ID è½¬æ¢ä¸ºæ ‡ç­¾åç§°ï¼š
```py
>>> id2label[str(2)]
'app_error'
```

## é¢„å¤„ç†
ä¸‹ä¸€æ­¥æ˜¯åŠ è½½ Wav2Vec2 ç‰¹å¾æå–å™¨æ¥å¤„ç†éŸ³é¢‘ä¿¡å·ï¼š
```py
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base")
```

MInDS-14 æ•°æ®é›†çš„é‡‡æ ·ç‡ä¸º 8000kHzï¼ˆæ‚¨å¯ä»¥åœ¨å…¶ [æ•°æ®é›†å¡ç‰‡](https://huggingface.co/datasets/PolyAI/minds14) ä¸­æ‰¾åˆ°æ­¤ä¿¡æ¯ï¼‰ï¼Œè¿™æ„å‘³ç€æ‚¨éœ€è¦å°†æ•°æ®é›†é‡é‡‡æ ·ä¸º 16000kHz æ‰èƒ½ä½¿ç”¨é¢„è®­ç»ƒçš„ Wav2Vec2 æ¨¡å‹ï¼š
```py
>>> minds = minds.cast_column("audio", Audio(sampling_rate=16_000))
>>> minds["train"][0]
{'audio': {'array': array([ 2.2098757e-05,  4.6582241e-05, -2.2803260e-05, ...,
         -2.8419291e-04, -2.3305941e-04, -1.1425107e-04], dtype=float32),
  'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602b9a5fbb1e6d0fbce91f52.wav',
  'sampling_rate': 16000},
 'intent_class': 2}
```

ç°åœ¨åˆ›å»ºä¸€ä¸ªé¢„å¤„ç†å‡½æ•°ï¼Œå®ƒï¼š
1. è°ƒç”¨ `audio` åˆ—æ¥åŠ è½½éŸ³é¢‘æ–‡ä»¶ï¼Œå¹¶åœ¨å¿…è¦æ—¶å¯¹éŸ³é¢‘æ–‡ä»¶è¿›è¡Œé‡é‡‡æ ·ã€‚
2. æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶çš„é‡‡æ ·ç‡æ˜¯å¦ä¸æ¨¡å‹é¢„è®­ç»ƒæ—¶çš„éŸ³é¢‘æ•°æ®çš„é‡‡æ ·ç‡åŒ¹é…ã€‚æ‚¨å¯ä»¥åœ¨ Wav2Vec2 [æ¨¡å‹å¡ç‰‡](https://huggingface.co/facebook/wav2vec2-base) ä¸­æ‰¾åˆ°æ­¤ä¿¡æ¯ã€‚
3. è®¾ç½®æœ€å¤§è¾“å…¥é•¿åº¦ï¼Œä»¥æ‰¹å¤„ç†æ›´é•¿çš„è¾“å…¥è€Œä¸æˆªæ–­å®ƒä»¬ã€‚
```py
>>> def preprocess_function(examples):
...     audio_arrays = [x["array"] for x in examples["audio"]]
...     inputs = feature_extractor(
...         audio_arrays, sampling_rate=feature_extractor.sampling_rate, max_length=16000, truncation=True
...     )
...     return inputs
```

è¦å°†é¢„å¤„ç†å‡½æ•°åº”ç”¨äºæ•´ä¸ªæ•°æ®é›†ï¼Œè¯·ä½¿ç”¨ğŸ¤— Datasets [`~datasets.Dataset.map`] å‡½æ•°ã€‚é€šè¿‡å°† `batched=True` è®¾ç½®ä¸ºä¸€æ¬¡å¤„ç†æ•°æ®é›†çš„å¤šä¸ªå…ƒç´ ï¼Œå¯ä»¥åŠ å¿« `map` çš„é€Ÿåº¦ã€‚åˆ é™¤ä¸éœ€è¦çš„åˆ—ï¼Œå¹¶å°† `intent_class` é‡å‘½åä¸º `label`ï¼Œå› ä¸ºæ¨¡å‹æœŸæœ›çš„æ˜¯è¿™ä¸ªåç§°ï¼š
```py
>>> encoded_minds = minds.map(preprocess_function, remove_columns="audio", batched=True)
>>> encoded_minds = encoded_minds.rename_column("intent_class", "label")
```

## è¯„ä¼°

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒ…æ‹¬æŒ‡æ ‡é€šå¸¸æœ‰åŠ©äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) åº“å¿«é€ŸåŠ è½½è¯„ä¼°æ–¹æ³•ã€‚å¯¹äºæ­¤ä»»åŠ¡ï¼Œè¯·åŠ è½½ [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy) æŒ‡æ ‡ï¼ˆè¯·å‚é˜…ğŸ¤— Evaluate [å¿«é€Ÿå¯¼è§ˆ](https://huggingface.co/docs/evaluate/a_quick_tour) ä»¥äº†è§£æ›´å¤šå…³äºå¦‚ä½•åŠ è½½å’Œè®¡ç®—æŒ‡æ ‡çš„ä¿¡æ¯ï¼‰ï¼š

```py
>>> import evaluate

>>> accuracy = evaluate.load("accuracy")
```

ç„¶ååˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œå°†æ‚¨çš„é¢„æµ‹å’Œæ ‡ç­¾ä¼ é€’ç»™ [`~evaluate.EvaluationModule.compute`] ä»¥è®¡ç®—å‡†ç¡®æ€§ï¼š
```py
>>> import numpy as np


>>> def compute_metrics(eval_pred):
...     predictions = np.argmax(eval_pred.predictions, axis=1)
...     return accuracy.compute(predictions=predictions, references=eval_pred.label_ids)
```

ç°åœ¨æ‚¨çš„ `compute_metrics` å‡½æ•°å·²å‡†å¤‡å°±ç»ªï¼Œåœ¨è®¾ç½®åŸ¹è®­æ—¶å°†è¿”å›åˆ°å®ƒã€‚

## è®­ç»ƒ

<frameworkcontent> 
<pt> 

 <Tip>
å¦‚æœæ‚¨å¯¹ä½¿ç”¨ [`Trainer`] å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒä¸ç†Ÿæ‚‰ï¼Œè¯·æŸ¥çœ‹åŸºæœ¬æ•™ç¨‹ [æ­¤å¤„](../training#train-with-pytorch-trainer)ï¼
</Tip>
ç°åœ¨æ‚¨å·²ç»å‡†å¤‡å¥½å¼€å§‹è®­ç»ƒæ¨¡å‹äº†ï¼ä½¿ç”¨ [`AutoModelForAudioClassification`] åŠ è½½ Wav2Vec2ï¼ŒåŒæ—¶æŒ‡å®šé¢„æœŸçš„æ ‡ç­¾æ•°é‡å’Œæ ‡ç­¾æ˜ å°„ï¼š
```py
>>> from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer

>>> num_labels = len(id2label)
>>> model = AutoModelForAudioClassification.from_pretrained(
...     "facebook/wav2vec2-base", num_labels=num_labels, label2id=label2id, id2label=id2label
... )
```

æ­¤æ—¶ï¼Œåªå‰©ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š

1. åœ¨ [`TrainingArguments`] ä¸­å®šä¹‰æ‚¨çš„è®­ç»ƒè¶…å‚æ•°ã€‚å”¯ä¸€å¿…éœ€çš„å‚æ•°æ˜¯ `output_dir`ï¼ŒæŒ‡å®šè¦ä¿å­˜æ¨¡å‹çš„ä½ç½®ã€‚æ‚¨å¯ä»¥é€šè¿‡è®¾ç½® `push_to_hub=True` å°†æ­¤æ¨¡å‹ä¸Šä¼ åˆ° Hubï¼ˆéœ€è¦ç™»å½• Hugging Face æ‰èƒ½ä¸Šä¼ æ¨¡å‹ï¼‰ã€‚åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶ï¼Œ[`Trainer`] å°†è¯„ä¼°å‡†ç¡®æ€§å¹¶ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹ã€‚
2. å°†è®­ç»ƒå‚æ•°ä¸æ¨¡å‹ã€æ•°æ®é›†ã€åˆ†è¯å™¨ (Tokenizer)ã€æ•°æ®æ•´ç†å™¨å’Œ `compute_metrics` å‡½æ•°ä¸€èµ·ä¼ é€’ç»™ [`Trainer`]ã€‚
3. è°ƒç”¨ [`~Trainer.train`] æ¥å¾®è°ƒæ‚¨çš„æ¨¡å‹ã€‚


```py
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_mind_model",
...     evaluation_strategy="epoch",
...     save_strategy="epoch",
...     learning_rate=3e-5,
...     per_device_train_batch_size=32,
...     gradient_accumulation_steps=4,
...     per_device_eval_batch_size=32,
...     num_train_epochs=10,
...     warmup_ratio=0.1,
...     logging_steps=10,
...     load_best_model_at_end=True,
...     metric_for_best_model="accuracy",
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=encoded_minds["train"],
...     eval_dataset=encoded_minds["test"],
...     tokenizer=feature_extractor,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

å®Œæˆè®­ç»ƒåï¼Œä½¿ç”¨ [`~transformers.Trainer.push_to_hub`] æ–¹æ³•å°†æ‚¨çš„æ¨¡å‹å…±äº«åˆ° Hubï¼Œä»¥ä¾¿æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨æ‚¨çš„æ¨¡å‹ï¼š
```py
>>> trainer.push_to_hub()
```
</pt> 
</frameworkcontent>
<Tip>

æœ‰å…³å¦‚ä½•ä¸ºéŸ³é¢‘åˆ†ç±»å¾®è°ƒæ¨¡å‹çš„æ›´è¯¦ç»†ç¤ºä¾‹ï¼Œè¯·å‚é˜…ç›¸åº”çš„ [PyTorch ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb)ã€‚
</Tip>

## æ¨ç†

å¾ˆå¥½ï¼Œç°åœ¨æ‚¨å·²ç»å¾®è°ƒäº†æ¨¡å‹ï¼Œå¯ä»¥å°†å…¶ç”¨äºæ¨ç†ï¼
åŠ è½½è¦è¿è¡Œæ¨ç†çš„éŸ³é¢‘æ–‡ä»¶ã€‚å¦‚æœéœ€è¦ï¼Œè¯·è®°å¾—å°†éŸ³é¢‘æ–‡ä»¶çš„é‡‡æ ·ç‡è¿›è¡Œé‡é‡‡æ ·ï¼Œä»¥åŒ¹é…æ¨¡å‹çš„é‡‡æ ·ç‡ï¼

```py
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
>>> sampling_rate = dataset.features["audio"].sampling_rate
>>> audio_file = dataset[0]["audio"]["path"]
```

å°è¯•ä½¿ç”¨ [`pipeline`] åœ¨æ¨ç†ä¸­ä½¿ç”¨æ‚¨å¾®è°ƒçš„æ¨¡å‹æ˜¯æœ€ç®€å•çš„æ–¹æ³•ã€‚ä½¿ç”¨æ‚¨çš„æ¨¡å‹å®ä¾‹åŒ–ä¸€ä¸ªéŸ³é¢‘åˆ†ç±»çš„ `pipeline`ï¼Œå¹¶å°†éŸ³é¢‘æ–‡ä»¶ä¼ é€’ç»™å®ƒï¼š
```py
>>> from transformers import pipeline

>>> classifier = pipeline("audio-classification", model="stevhliu/my_awesome_minds_model")
>>> classifier(audio_file)
[
    {'score': 0.09766869246959686, 'label': 'cash_deposit'},
    {'score': 0.07998877018690109, 'label': 'app_error'},
    {'score': 0.0781070664525032, 'label': 'joint_account'},
    {'score': 0.07667109370231628, 'label': 'pay_bill'},
    {'score': 0.0755252093076706, 'label': 'balance'}
]
```

å¦‚æœæ„¿æ„ï¼Œæ‚¨ä¹Ÿå¯ä»¥æ‰‹åŠ¨å¤åˆ¶ `pipeline` çš„ç»“æœï¼š

<frameworkcontent> 
<pt> 

 åŠ è½½ä¸€ä¸ªç‰¹å¾æå–å™¨æ¥é¢„å¤„ç†éŸ³é¢‘æ–‡ä»¶ï¼Œå¹¶å°† `input` è¿”å›ä¸º PyTorch å¼ é‡ï¼š

```py
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("stevhliu/my_awesome_minds_model")
>>> inputs = feature_extractor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")
```

å°†æ‚¨çš„è¾“å…¥ä¼ é€’ç»™æ¨¡å‹ï¼Œå¹¶è¿”å›é€»è¾‘å€¼ï¼š
```py
>>> from transformers import AutoModelForAudioClassification

>>> model = AutoModelForAudioClassification.from_pretrained("stevhliu/my_awesome_minds_model")
>>> with torch.no_grad():
...     logits = model(**inputs).logits
```

è·å–å…·æœ‰æœ€é«˜æ¦‚ç‡çš„ç±»ï¼Œå¹¶ä½¿ç”¨æ¨¡å‹çš„ `id2label` æ˜ å°„å°†å…¶è½¬æ¢ä¸ºæ ‡ç­¾ï¼š
```py
>>> import torch

>>> predicted_class_ids = torch.argmax(logits).item()
>>> predicted_label = model.config.id2label[predicted_class_ids]
>>> predicted_label
'cash_deposit'
```
</pt> 
</frameworkcontent>