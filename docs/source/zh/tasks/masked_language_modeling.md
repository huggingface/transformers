<!--ç‰ˆæƒæ‰€æœ‰ 2023 å¹´ HuggingFace å›¢é˜Ÿã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚
æ ¹æ® Apache è®¸å¯è¯ç¬¬ 2.0 ç‰ˆè®¸å¯ï¼ˆâ€œè®¸å¯è¯â€ï¼‰ï¼Œæ‚¨é™¤éç¬¦åˆè®¸å¯è¯çš„è§„å®šï¼Œå¦åˆ™ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ä½ç½®è·å–è®¸å¯è¯çš„å‰¯æœ¬
http://www.apache.org/licenses/LICENSE-2.0
é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œæ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯æŒ‰åŸæ ·åˆ†å‘çš„ï¼Œä¸é™„å¸¦ä»»ä½•å½¢å¼çš„æ‹…ä¿æˆ–æ¡ä»¶ã€‚è¯·å‚é˜…è®¸å¯è¯ä»¥è·å–ç‰¹å®šè¯­è¨€ä¸­æœ‰å…³æƒé™å’Œé™åˆ¶çš„æƒé™ã€‚-->
âš ï¸è¯·æ³¨æ„ï¼Œæ­¤æ–‡ä»¶æ˜¯ Markdown æ ¼å¼çš„ï¼Œä½†åŒ…å«ç‰¹å®šäºæˆ‘ä»¬çš„æ–‡æ¡£æ„å»ºå™¨ï¼ˆç±»ä¼¼äº MDXï¼‰çš„è¯­æ³•ï¼Œå¯èƒ½æ— æ³•æ­£ç¡®åœ°åœ¨æ‚¨çš„ Markdown æŸ¥çœ‹å™¨ä¸­å‘ˆç°ã€‚
-->

# æ©ç è¯­è¨€å»ºæ¨¡ Masked language modeling

[[åœ¨ Google Colab ä¸­æ‰“å¼€]]
<Youtube id="mqElG5QJWUg"/>

æ©ç è¯­è¨€å»ºæ¨¡æ˜¯é¢„æµ‹åºåˆ—ä¸­çš„æ©ç æ ‡è®°ï¼Œæ¨¡å‹å¯ä»¥åŒå‘å…³æ³¨æ ‡è®°ã€‚è¿™æ„å‘³ç€æ¨¡å‹å¯ä»¥å®Œå…¨è®¿é—®å·¦å³çš„æ ‡è®°ã€‚æ©ç è¯­è¨€å»ºæ¨¡éå¸¸é€‚åˆéœ€è¦å¯¹æ•´ä¸ªåºåˆ—è¿›è¡Œè‰¯å¥½ä¸Šä¸‹æ–‡ç†è§£çš„ä»»åŠ¡ã€‚BERT å°±æ˜¯æ©ç è¯­è¨€æ¨¡å‹çš„ä¸€ä¸ªç¤ºä¾‹ã€‚æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š
1. åœ¨ [ELI5](https://huggingface.co/datasets/eli5) æ•°æ®é›†çš„ [r/askscience](https://www.reddit.com/r/askscience/) å­é›†ä¸Šå¾®è°ƒ [DistilRoBERTa](https://huggingface.co/distilroberta-base)ã€‚

2. ä½¿ç”¨æ‚¨å¾®è°ƒçš„æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚
<Tip> æ‚¨å¯ä»¥æŒ‰ç…§æœ¬æŒ‡å—ä¸­çš„ç›¸åŒæ­¥éª¤å¾®è°ƒå…¶ä»–æ¶æ„çš„æ©ç è¯­è¨€å»ºæ¨¡ã€‚
<Tip>

è¯·é€‰æ‹©ä»¥ä¸‹æ¶æ„ä¹‹ä¸€ï¼š
<!--æ­¤æç¤ºç”±`make fix-copies`è‡ªåŠ¨ç”Ÿæˆï¼Œè¯·å‹¿æ‰‹åŠ¨å¡«å†™ï¼-->
[ALBERT](../model_doc/albert), [BART](../model_doc/bart), [BERT](../model_doc/bert), [BigBird](../model_doc/big_bird), [CamemBERT](../model_doc/camembert), [ConvBERT](../model_doc/convbert), [Data2VecText](../model_doc/data2vec-text), [DeBERTa](../model_doc/deberta), [DeBERTa-v2](../model_doc/deberta-v2), [DistilBERT](../model_doc/distilbert), [ELECTRA](../model_doc/electra), [ERNIE](../model_doc/ernie), [ESM](../model_doc/esm), [FlauBERT](../model_doc/flaubert), [FNet](../model_doc/fnet), [Funnel Transformer](../model_doc/funnel), [I-BERT](../model_doc/ibert), [LayoutLM](../model_doc/layoutlm), [Longformer](../model_doc/longformer), [LUKE](../model_doc/luke), [mBART](../model_doc/mbart), [MEGA](../model_doc/mega), [Megatron-BERT](../model_doc/megatron-bert), [MobileBERT](../model_doc/mobilebert), [MPNet](../model_doc/mpnet), [MVP](../model_doc/mvp), [Nezha](../model_doc/nezha), [Nystr Ã¶ mformer](../model_doc/nystromformer), [Perceiver](../model_doc/perceiver), [QDQBert](../model_doc/qdqbert), [Reformer](../model_doc/reformer), [RemBERT](../model_doc/rembert), [RoBERTa](../model_doc/roberta), [RoBERTa-PreLayerNorm](../model_doc/roberta-prelayernorm), [RoCBert](../model_doc/roc_bert), [RoFormer](../model_doc/roformer), [SqueezeBERT](../model_doc/squeezebert), [TAPAS](../model_doc/tapas), [Wav2Vec2](../model_doc/wav2vec2), [XLM](../model_doc/xlm), [XLM-RoBERTa](../model_doc/xlm-roberta), [XLM-RoBERTa-XL](../model_doc/xlm-roberta-xl), [X-MOD](../model_doc/xmod), [YOSO](../model_doc/yoso)
<!--End of the generated tip-->

</Tip>

å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…éœ€çš„åº“ï¼š
```bash
pip install transformers datasets evaluate
```

æˆ‘ä»¬é¼“åŠ±æ‚¨ç™»å½•æ‚¨çš„ Hugging Face è´¦æˆ·ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥ä¸ç¤¾åŒºå…±äº«å’Œä¸Šä¼ æ‚¨çš„æ¨¡å‹ã€‚åœ¨æç¤ºæ—¶ï¼Œè¾“å…¥æ‚¨çš„ä»¤ç‰Œä»¥ç™»å½•ï¼š
```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## åŠ è½½ ELI5 æ•°æ®é›†

é¦–å…ˆä»ğŸ¤— Datasets åº“ä¸­åŠ è½½ r/askscience å­é›†çš„è¾ƒå°æ•°æ®é›†ã€‚è¿™æ ·å¯ä»¥è®©æ‚¨æœ‰æœºä¼šè¿›è¡Œå®éªŒï¼Œå¹¶ç¡®ä¿ä¸€åˆ‡æ­£å¸¸ï¼Œç„¶åå†åœ¨å®Œæ•´æ•°æ®é›†ä¸Šè¿›è¡Œæ›´é•¿æ—¶é—´çš„è®­ç»ƒã€‚ä½¿ç”¨ [`~datasets.Dataset.train_test_split`] æ–¹æ³•å°†æ•°æ®é›†çš„ `train_asks` æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š
```py
>>> from datasets import load_dataset

>>> eli5 = load_dataset("eli5", split="train_asks[:5000]")
```

ç„¶åæŸ¥çœ‹ä¸€ä¸ªç¤ºä¾‹ï¼š
```py
>>> eli5 = eli5.train_test_split(test_size=0.2)
```

Then take a look at an example:

```py
>>> eli5["train"][0]
{'answers': {'a_id': ['c3d1aib', 'c3d4lya'],
  'score': [6, 3],
  'text': ["The velocity needed to remain in orbit is equal to the square root of Newton's constant times the mass of earth divided by the distance from the center of the earth. I don't know the altitude of that specific mission, but they're usually around 300 km. That means he's going 7-8 km/s.\n\nIn space there are no other forces acting on either the shuttle or the guy, so they stay in the same position relative to each other. If he were to become unable to return to the ship, he would presumably run out of oxygen, or slowly fall into the atmosphere and burn up.",
   "Hope you don't mind me asking another question, but why aren't there any stars visible in this photo?"]},
 'answers_urls': {'url': []},
 'document': '',
 'q_id': 'nyxfp',
 'selftext': '_URL_0_\n\nThis was on the front page earlier and I have a few questions about it. Is it possible to calculate how fast the astronaut would be orbiting the earth? Also how does he stay close to the shuttle so that he can return safely, i.e is he orbiting at the same speed and can therefore stay next to it? And finally if his propulsion system failed, would he eventually re-enter the atmosphere and presumably die?',
 'selftext_urls': {'url': ['http://apod.nasa.gov/apod/image/1201/freeflyer_nasa_3000.jpg']},
 'subreddit': 'askscience',
 'title': 'Few questions about this space walk photograph.',
 'title_urls': {'url': []}}
```

è™½ç„¶è¿™çœ‹èµ·æ¥å¯èƒ½å¾ˆå¤šï¼Œä½†æ‚¨å®é™…ä¸Šåªå¯¹ `text` å­—æ®µæ„Ÿå…´è¶£ã€‚è¯­è¨€å»ºæ¨¡ä»»åŠ¡çš„æœ‰è¶£ä¹‹å¤„åœ¨äºæ‚¨ä¸éœ€è¦æ ‡ç­¾ï¼ˆä¹Ÿç§°ä¸ºæ— ç›‘ç£ä»»åŠ¡ï¼‰ï¼Œå› ä¸ºä¸‹ä¸€ä¸ªè¯å°±æ˜¯æ ‡ç­¾ã€‚

## é¢„å¤„ç†
<Youtube id="8PmhEIXhBvI"/>

å¯¹äºæ©ç è¯­è¨€å»ºæ¨¡ï¼Œä¸‹ä¸€æ­¥æ˜¯åŠ è½½ä¸€ä¸ª DistilRoBERTa tokenizer æ¥å¤„ç† `text` å­å­—æ®µï¼š
```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilroberta-base")
```

ä»ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œæ‚¨ä¼šæ³¨æ„åˆ° `text` å­—æ®µå®é™…ä¸Šæ˜¯åµŒå¥—åœ¨ `answers` ä¸­çš„ã€‚è¿™æ„å‘³ç€æ‚¨éœ€è¦ä½¿ç”¨ [`flatten`](https://huggingface.co/docs/datasets/process.html#flatten) æ–¹æ³•ä»åµŒå¥—ç»“æ„ä¸­æå– `text` å­å­—æ®µï¼š

```py
>>> eli5 = eli5.flatten()
>>> eli5["train"][0]
{'answers.a_id': ['c3d1aib', 'c3d4lya'],
 'answers.score': [6, 3],
 'answers.text': ["The velocity needed to remain in orbit is equal to the square root of Newton's constant times the mass of earth divided by the distance from the center of the earth. I don't know the altitude of that specific mission, but they're usually around 300 km. That means he's going 7-8 km/s.\n\nIn space there are no other forces acting on either the shuttle or the guy, so they stay in the same position relative to each other. If he were to become unable to return to the ship, he would presumably run out of oxygen, or slowly fall into the atmosphere and burn up.",
  "Hope you don't mind me asking another question, but why aren't there any stars visible in this photo?"],
 'answers_urls.url': [],
 'document': '',
 'q_id': 'nyxfp',
 'selftext': '_URL_0_\n\nThis was on the front page earlier and I have a few questions about it. Is it possible to calculate how fast the astronaut would be orbiting the earth? Also how does he stay close to the shuttle so that he can return safely, i.e is he orbiting at the same speed and can therefore stay next to it? And finally if his propulsion system failed, would he eventually re-enter the atmosphere and presumably die?',
 'selftext_urls.url': ['http://apod.nasa.gov/apod/image/1201/freeflyer_nasa_3000.jpg'],
 'subreddit': 'askscience',
 'title': 'Few questions about this space walk photograph.',
 'title_urls.url': []}
```

ç°åœ¨ï¼Œæ¯ä¸ªå­å­—æ®µéƒ½æˆä¸ºä¸€ä¸ªå•ç‹¬çš„åˆ—ï¼Œç”± `answers` å‰ç¼€æŒ‡ç¤ºï¼Œ`text` å­—æ®µç°åœ¨æ˜¯ä¸€ä¸ªåˆ—è¡¨ã€‚ä¸å•ç‹¬å¯¹æ¯ä¸ªå¥å­è¿›è¡Œæ ‡è®°åŒ–ä¸åŒï¼Œå°†åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œä»¥ä¾¿å¯ä»¥åŒæ—¶å¯¹å®ƒä»¬è¿›è¡Œæ ‡è®°åŒ–ã€‚

è¿™æ˜¯ä¸€ä¸ªç¬¬ä¸€ä¸ªé¢„å¤„ç†å‡½æ•°ï¼Œç”¨äºè¿æ¥æ¯ä¸ªç¤ºä¾‹çš„å­—ç¬¦ä¸²åˆ—è¡¨å¹¶å¯¹ç»“æœè¿›è¡Œæ ‡è®°åŒ–ï¼š
```py
>>> def preprocess_function(examples):
...     return tokenizer([" ".join(x) for x in examples["answers.text"]])
```

è¦åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåº”ç”¨æ­¤é¢„å¤„ç†å‡½æ•°ï¼Œä½¿ç”¨ğŸ¤— Datasets [`~datasets.Dataset.map`] æ–¹æ³•ã€‚æ‚¨å¯ä»¥é€šè¿‡å°† `batched=True` è®¾ç½®ä¸ºä¸€æ¬¡å¤„ç†æ•°æ®é›†çš„å¤šä¸ªå…ƒç´ ï¼Œå¹¶ä½¿ç”¨ `num_proc` å¢åŠ è¿›ç¨‹çš„æ•°é‡æ¥åŠ å¿« `map` å‡½æ•°çš„é€Ÿåº¦ã€‚åˆ é™¤æ‚¨ä¸éœ€è¦çš„ä»»ä½•åˆ—ï¼š
```py
>>> tokenized_eli5 = eli5.map(
...     preprocess_function,
...     batched=True,
...     num_proc=4,
...     remove_columns=eli5["train"].column_names,
... )
```

æ­¤æ•°æ®é›†åŒ…å«æ ‡è®°åºåˆ—ï¼Œä½†å…¶ä¸­ä¸€äº›åºåˆ—é•¿åº¦è¶…è¿‡äº†æ¨¡å‹çš„æœ€å¤§è¾“å…¥é•¿åº¦ã€‚

ç°åœ¨ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ç¬¬äºŒä¸ªé¢„å¤„ç†å‡½æ•°æ¥- è¿æ¥æ‰€æœ‰åºåˆ—- å°†è¿æ¥çš„åºåˆ—åˆ†å‰²ä¸º `block_size` å®šä¹‰çš„è¾ƒçŸ­å—ï¼Œè¯¥å—åº”æ—¢çŸ­äºæœ€å¤§è¾“å…¥é•¿åº¦åˆçŸ­åˆ°è¶³å¤Ÿé€‚åº”æ‚¨çš„ GPU RAMã€‚
```py
>>> block_size = 128


>>> def group_texts(examples):
...     # Concatenate all texts.
...     concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
...     total_length = len(concatenated_examples[list(examples.keys())[0]])
...     # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can
...     # customize this part to your needs.
...     if total_length >= block_size:
...         total_length = (total_length // block_size) * block_size
...     # Split by chunks of block_size.
...     result = {
...         k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
...         for k, t in concatenated_examples.items()
...     }
...     result["labels"] = result["input_ids"].copy()
...     return result
```

å¯¹æ•´ä¸ªæ•°æ®é›†åº”ç”¨ `group_texts` å‡½æ•°ï¼š
```py
>>> lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)
```

ç°åœ¨ä½¿ç”¨ [`DataCollatorForLanguageModeling`] åˆ›å»ºä¸€æ‰¹ç¤ºä¾‹ã€‚åœ¨æ•´ç†æœŸé—´ï¼Œå°†å¥å­åŠ¨æ€å¡«å……åˆ°æ‰¹æ¬¡ä¸­çš„æœ€é•¿é•¿åº¦ï¼Œè€Œä¸æ˜¯å°†æ•´ä¸ªæ•°æ®é›†å¡«å……åˆ°æœ€å¤§é•¿åº¦ã€‚

<frameworkcontent> 
<pt> 

ä½¿ç”¨ç»“æŸåºåˆ—æ ‡è®°ä½œä¸ºå¡«å……æ ‡è®°ï¼Œå¹¶æŒ‡å®š `mlm_probability` ä»¥åœ¨æ¯æ¬¡è¿­ä»£æ•°æ®æ—¶éšæœºå±è”½æ ‡è®°ï¼š
```py
>>> from transformers import DataCollatorForLanguageModeling

>>> tokenizer.pad_token = tokenizer.eos_token
>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)
```
</pt> <tf>

ä½¿ç”¨ç»“æŸåºåˆ—æ ‡è®°ä½œä¸ºå¡«å……æ ‡è®°ï¼Œå¹¶æŒ‡å®š `mlm_probability` ä»¥åœ¨æ¯æ¬¡è¿­ä»£æ•°æ®æ—¶éšæœºå±è”½æ ‡è®°ï¼š

```py
>>> from transformers import DataCollatorForLanguageModeling

>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors="tf")
```
</tf>
</frameworkcontent>


## è®­ç»ƒ

<frameworkcontent> 
<pt> 
<Tip>

å¦‚æœæ‚¨å¯¹ä½¿ç”¨ [`Trainer`] å¾®è°ƒæ¨¡å‹ä¸ç†Ÿæ‚‰ï¼Œè¯·æŸ¥çœ‹ [æ­¤å¤„](../training#train-with-pytorch-trainer) çš„åŸºæœ¬æ•™ç¨‹ï¼
</Tip>

ç°åœ¨ï¼Œæ‚¨å¯ä»¥å¼€å§‹è®­ç»ƒæ¨¡å‹äº†ï¼ä½¿ç”¨ [`AutoModelForMaskedLM`] åŠ è½½ DistilRoBERTaï¼š
```py
>>> from transformers import AutoModelForMaskedLM

>>> model = AutoModelForMaskedLM.from_pretrained("distilroberta-base")
```

æ­¤æ—¶ï¼Œåªå‰©ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š
1. åœ¨ [`TrainingArguments`] ä¸­å®šä¹‰æ‚¨çš„è®­ç»ƒè¶…å‚æ•°ã€‚å”¯ä¸€å¿…éœ€çš„å‚æ•°æ˜¯ `output_dir`ï¼Œç”¨äºæŒ‡å®šä¿å­˜æ¨¡å‹çš„ä½ç½®ã€‚é€šè¿‡è®¾ç½® `push_to_hub=True` å°†è¯¥æ¨¡å‹æ¨é€åˆ° Hubï¼ˆæ‚¨éœ€è¦ç™»å½• Hugging Face ä»¥ä¸Šä¼ æ¨¡å‹ï¼‰ã€‚
2. å°†è®­ç»ƒå‚æ•°ä¸æ¨¡å‹ã€æ•°æ®é›†å’Œæ•°æ®æ•´ç†å™¨ä¸€èµ·ä¼ é€’ç»™ [`Trainer`]ã€‚
3. è°ƒç”¨ [`~Trainer.train`] æ¥å¾®è°ƒæ‚¨çš„æ¨¡å‹ã€‚
```py
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_eli5_mlm_model",
...     evaluation_strategy="epoch",
...     learning_rate=2e-5,
...     num_train_epochs=3,
...     weight_decay=0.01,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=lm_dataset["train"],
...     eval_dataset=lm_dataset["test"],
...     data_collator=data_collator,
... )

>>> trainer.train()
```

è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨ [`~transformers.Trainer.evaluate`] æ–¹æ³•è¯„ä¼°æ¨¡å‹å¹¶è·å–å…¶å›°æƒ‘åº¦ï¼š
```py
>>> import math

>>> eval_results = trainer.evaluate()
>>> print(f"Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
Perplexity: 8.76
```

ç„¶åä½¿ç”¨ [`~transformers.Trainer.push_to_hub`] æ–¹æ³•å°†æ‚¨çš„æ¨¡å‹å…±äº«åˆ° Hub ä¸Šï¼Œè¿™æ ·æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨æ‚¨çš„æ¨¡å‹ï¼š
```py
>>> trainer.push_to_hub()
```
</pt> 
<tf> 

<Tip>
å¦‚æœæ‚¨å¯¹ä½¿ç”¨ Keras è¿›è¡Œæ¨¡å‹å¾®è°ƒä¸ç†Ÿæ‚‰ï¼Œè¯·æŸ¥çœ‹åŸºç¡€æ•™ç¨‹ [è¿™é‡Œ](../training#train-a-tensorflow-model-with-keras)ï¼
</Tip> è¦åœ¨ TensorFlow ä¸­å¾®è°ƒæ¨¡å‹ï¼Œè¯·é¦–å…ˆè®¾ç½®ä¼˜åŒ–å™¨å‡½æ•°ã€å­¦ä¹ ç‡è°ƒåº¦å’Œä¸€äº›è®­ç»ƒè¶…å‚æ•°ï¼š
```py
>>> from transformers import create_optimizer, AdamWeightDecay

>>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)
```

ç„¶åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ [`TFAutoModelForMaskedLM`] åŠ è½½ DistilRoBERTaï¼š
```py
>>> from transformers import TFAutoModelForMaskedLM

>>> model = TFAutoModelForMaskedLM.from_pretrained("distilroberta-base")
```

å°†æ‚¨çš„æ•°æ®é›†è½¬æ¢ä¸º `tf.data.Dataset` æ ¼å¼ï¼Œä½¿ç”¨ [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]ï¼š
```py
>>> tf_train_set = model.prepare_tf_dataset(
...     lm_dataset["train"],
...     shuffle=True,
...     batch_size=16,
...     collate_fn=data_collator,
... )

>>> tf_test_set = model.prepare_tf_dataset(
...     lm_dataset["test"],
...     shuffle=False,
...     batch_size=16,
...     collate_fn=data_collator,
... )
```

ä½¿ç”¨ [`compile`](https://keras.io/api/models/model_training_apis/#compile-method) é…ç½®æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚è¯·æ³¨æ„ï¼ŒTransformers æ¨¡å‹éƒ½æœ‰ä¸€ä¸ªé»˜è®¤çš„ä¸ä»»åŠ¡ç›¸å…³çš„æŸå¤±å‡½æ•°ï¼Œæ‰€ä»¥æ‚¨ä¸éœ€è¦æŒ‡å®šæŸå¤±å‡½æ•°ï¼Œé™¤éæ‚¨æƒ³è¦è‡ªå®šä¹‰ï¼š
```py
>>> import tensorflow as tf

>>> model.compile(optimizer=optimizer)  # No loss argument!
```

å¯ä»¥é€šè¿‡åœ¨ [`~transformers.PushToHubCallback`] ä¸­æŒ‡å®šæ¨¡å‹å’Œåˆ†è¯å™¨ (Tokenizer)çš„æ¨é€ä½ç½®æ¥å®ç°ï¼š
```py
>>> from transformers.keras_callbacks import PushToHubCallback

>>> callback = PushToHubCallback(
...     output_dir="my_awesome_eli5_mlm_model",
...     tokenizer=tokenizer,
... )
```

æœ€åï¼Œæ‚¨å¯ä»¥å¼€å§‹è®­ç»ƒæ‚¨çš„æ¨¡å‹äº†ï¼ä½¿ç”¨æ‚¨çš„è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ã€è®­ç»ƒå‘¨æœŸæ•°å’Œå›è°ƒå‡½æ•°è°ƒç”¨ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) æ¥å¾®è°ƒæ¨¡å‹ï¼š
```py
>>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=[callback])
```

è®­ç»ƒå®Œæˆåï¼Œæ‚¨çš„æ¨¡å‹ä¼šè‡ªåŠ¨ä¸Šä¼ åˆ° Hubï¼Œè¿™æ ·æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨å®ƒï¼
</tf>
</frameworkcontent>
<Tip>

æœ‰å…³å¦‚ä½•ä¸ºé®è”½è¯­è¨€å»ºæ¨¡å¾®è°ƒæ¨¡å‹çš„æ›´è¯¦ç»†ç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹ç›¸åº”çš„ [PyTorch ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb) æˆ–è€… [TensorFlow ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)ã€‚
</Tip>

## æ¨ç†

å¤ªæ£’äº†ï¼Œç°åœ¨æ‚¨å·²ç»å¾®è°ƒäº†ä¸€ä¸ªæ¨¡å‹ï¼Œå¯ä»¥ç”¨å®ƒè¿›è¡Œæ¨ç†äº†ï¼
æƒ³å‡ºä¸€äº›æ‚¨å¸Œæœ›æ¨¡å‹å¡«å……ç©ºç™½çš„æ–‡æœ¬ï¼Œå¹¶ä½¿ç”¨ç‰¹æ®Šçš„ `<mask>` æ ‡è®°æ¥è¡¨ç¤ºç©ºç™½å¤„ï¼š

```py
>>> text = "The Milky Way is a <mask> galaxy."
```

å°è¯•ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨ç†çš„æœ€ç®€å•æ–¹æ³•æ˜¯åœ¨ [`pipeline`] ä¸­ä½¿ç”¨å®ƒã€‚ä½¿ç”¨æ‚¨çš„æ¨¡å‹å®ä¾‹åŒ–ä¸€ä¸ªç”¨äºå¡«å……é®è”½çš„ `pipeline`ï¼Œå¹¶å°†æ–‡æœ¬ä¼ é€’ç»™å®ƒã€‚å¦‚æœéœ€è¦ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ `top_k` å‚æ•°æ¥æŒ‡å®šè¿”å›å¤šå°‘ä¸ªé¢„æµ‹ç»“æœï¼š
```py
>>> from transformers import pipeline

>>> mask_filler = pipeline("fill-mask", "stevhliu/my_awesome_eli5_mlm_model")
>>> mask_filler(text, top_k=3)
[{'score': 0.5150994658470154,
  'token': 21300,
  'token_str': ' spiral',
  'sequence': 'The Milky Way is a spiral galaxy.'},
 {'score': 0.07087188959121704,
  'token': 2232,
  'token_str': ' massive',
  'sequence': 'The Milky Way is a massive galaxy.'},
 {'score': 0.06434620916843414,
  'token': 650,
  'token_str': ' small',
  'sequence': 'The Milky Way is a small galaxy.'}]
```


<frameworkcontent> 
<pt> 

 å°†æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œå¹¶å°† `input_ids` è¿”å›ä¸º PyTorch å¼ é‡ã€‚æ‚¨è¿˜éœ€è¦æŒ‡å®š `<mask>` æ ‡è®°çš„ä½ç½®ï¼š
```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("stevhliu/my_awesome_eli5_mlm_model")
>>> inputs = tokenizer(text, return_tensors="pt")
>>> mask_token_index = torch.where(inputs["input_ids"] == tokenizer.mask_token_id)[1]
```

å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹ï¼Œå¹¶è¿”å›é®è”½æ ‡è®°çš„ `logits`ï¼š
```py
>>> from transformers import AutoModelForMaskedLM

>>> model = AutoModelForMaskedLM.from_pretrained("stevhliu/my_awesome_eli5_mlm_model")
>>> logits = model(**inputs).logits
>>> mask_token_logits = logits[0, mask_token_index, :]
```

ç„¶åè¿”å›æœ€é«˜æ¦‚ç‡çš„ä¸‰ä¸ªé®è”½æ ‡è®°ï¼Œå¹¶æ‰“å°å‡ºæ¥ï¼š
```py
>>> top_3_tokens = torch.topk(mask_token_logits, 3, dim=1).indices[0].tolist()

>>> for token in top_3_tokens:
...     print(text.replace(tokenizer.mask_token, tokenizer.decode([token])))
The Milky Way is a spiral galaxy.
The Milky Way is a massive galaxy.
The Milky Way is a small galaxy.
```
</pt> 
<tf> 

å°†æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œå¹¶å°† `input_ids` è¿”å›ä¸º TensorFlow å¼ é‡ã€‚æ‚¨è¿˜éœ€è¦æŒ‡å®š `<mask>` æ ‡è®°çš„ä½ç½®ï¼š

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("stevhliu/my_awesome_eli5_mlm_model")
>>> inputs = tokenizer(text, return_tensors="tf")
>>> mask_token_index = tf.where(inputs["input_ids"] == tokenizer.mask_token_id)[0, 1]
```

å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹ï¼Œå¹¶è¿”å›é®è”½æ ‡è®°çš„ `logits`ï¼š
```py
>>> from transformers import TFAutoModelForMaskedLM

>>> model = TFAutoModelForMaskedLM.from_pretrained("stevhliu/my_awesome_eli5_mlm_model")
>>> logits = model(**inputs).logits
>>> mask_token_logits = logits[0, mask_token_index, :]
```

ç„¶åè¿”å›æœ€é«˜æ¦‚ç‡çš„ä¸‰ä¸ªé®è”½æ ‡è®°ï¼Œå¹¶æ‰“å°å‡ºæ¥ï¼š
```py
>>> top_3_tokens = tf.math.top_k(mask_token_logits, 3).indices.numpy()

>>> for token in top_3_tokens:
...     print(text.replace(tokenizer.mask_token, tokenizer.decode([token])))
The Milky Way is a spiral galaxy.
The Milky Way is a massive galaxy.
The Milky Way is a small galaxy.
```
</tf>
</frameworkcontent>

