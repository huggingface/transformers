<!--ç‰ˆæƒ 2023 å¹´ HuggingFace å›¢é˜Ÿã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚
æ ¹æ® Apache è®¸å¯è¯ç¬¬ 2.0 ç‰ˆï¼ˆâ€œè®¸å¯è¯â€ï¼‰çš„è§„å®šï¼Œæ‚¨ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ï¼Œé™¤éç¬¦åˆè®¸å¯è¯ã€‚æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ä½ç½®è·å–è®¸å¯è¯çš„å‰¯æœ¬
http://www.apache.org/licenses/LICENSE-2.0
é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œæ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯æŒ‰ç…§â€œæŒ‰åŸæ ·â€åˆ†å‘çš„åŸºç¡€ï¼Œæ²¡æœ‰ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„æ‹…ä¿æˆ–æ¡ä»¶ã€‚è¯·å‚é˜…è®¸å¯è¯å…·ä½“è¯­è¨€ä¸‹çš„æƒé™å’Œé™åˆ¶ã€‚
âš ï¸è¯·æ³¨æ„ï¼Œæ­¤æ–‡ä»¶æ˜¯ Markdown æ ¼å¼ï¼Œä½†åŒ…å«æˆ‘ä»¬çš„æ–‡æ¡£æ„å»ºå™¨ï¼ˆç±»ä¼¼äº MDXï¼‰çš„ç‰¹å®šè¯­æ³•ï¼Œå¯èƒ½æ— æ³•åœ¨ Markdown æŸ¥çœ‹å™¨ä¸­æ­£ç¡®æ˜¾ç¤ºã€‚
-->

# æ–‡æ¡£é—®ç­”

[[åœ¨ Colab ä¸­æ‰“å¼€]]
æ–‡æ¡£é—®ç­”ï¼Œåˆç§°ä¸ºæ–‡æ¡£è§†è§‰é—®ç­”ï¼Œæ˜¯ä¸€é¡¹æ¶‰åŠæä¾›å…³äºæ–‡æ¡£å›¾åƒçš„é—®é¢˜çš„ç­”æ¡ˆçš„ä»»åŠ¡ã€‚æ”¯æŒæ­¤ä»»åŠ¡çš„æ¨¡å‹çš„è¾“å…¥é€šå¸¸æ˜¯å›¾åƒå’Œé—®é¢˜çš„ç»„åˆï¼Œè¾“å‡ºæ˜¯ç”¨è‡ªç„¶è¯­è¨€è¡¨ç¤ºçš„ç­”æ¡ˆã€‚è¿™äº›æ¨¡å‹åˆ©ç”¨å¤šç§æ¨¡æ€ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€å•è¯ä½ç½®ï¼ˆè¾¹ç•Œæ¡†ï¼‰å’Œå›¾åƒæœ¬èº«ã€‚

æœ¬æŒ‡å—è¯´æ˜äº†å¦‚ä½•ï¼š

- åœ¨ [LayoutLMv2](../model_doc/layoutlmv2) ä¸Šå¯¹ [DocVQA æ•°æ®é›†](https://huggingface.co/datasets/nielsr/docvqa_1200_examples_donut) è¿›è¡Œå¾®è°ƒã€‚- ä½¿ç”¨æ‚¨å¾®è°ƒçš„æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚

<Tip>

æœ¬æ•™ç¨‹æ¼”ç¤ºçš„ä»»åŠ¡ç”±ä»¥ä¸‹æ¨¡å‹æ¶æ„æ”¯æŒï¼š
<!--æ­¤æç¤ºç”±`make fix-copies`è‡ªåŠ¨ç”Ÿæˆï¼Œè¯·å‹¿æ‰‹åŠ¨å¡«å†™ï¼-->
[LayoutLM](../model_doc/layoutlm)ï¼Œ[LayoutLMv2](../model_doc/layoutlmv2)ï¼Œ[LayoutLMv3](../model_doc/layoutlmv3)
<!--ç”Ÿæˆæç¤ºçš„æœ«å°¾-->

</Tip>

LayoutLMv2 é€šè¿‡åœ¨ä»¤ç‰Œçš„æœ€ç»ˆéšè—çŠ¶æ€ä¹‹ä¸Šæ·»åŠ ä¸€ä¸ªé—®ç­”å¤´æ¥è§£å†³æ–‡æ¡£é—®ç­”ä»»åŠ¡ï¼Œä»¥é¢„æµ‹ç­”æ¡ˆçš„å¼€å§‹å’Œç»“æŸä»¤ç‰Œçš„ä½ç½®ã€‚æ¢å¥è¯è¯´ï¼Œé—®é¢˜è¢«è§†ä¸ºæŠ½å–æ€§é—®ç­”ï¼šç»™å®šä¸Šä¸‹æ–‡ï¼Œæå–å›ç­”é—®é¢˜çš„ä¿¡æ¯ç‰‡æ®µã€‚ä¸Šä¸‹æ–‡æ¥è‡ª OCR å¼•æ“çš„è¾“å‡ºï¼Œè¿™é‡Œä½¿ç”¨çš„æ˜¯ Google çš„ Tesseractã€‚å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ã€‚LayoutLMv2 ä¾èµ–äº detectron2ã€torchvision å’Œ tesseractã€‚

```bash
pip install -q transformers datasets
```

```bash
pip install 'git+https://github.com/facebookresearch/detectron2.git'
pip install torchvision
```

```bash
sudo apt install tesseract-ocr
pip install -q pytesseract
```

å®‰è£…å®Œæ‰€æœ‰ä¾èµ–é¡¹åï¼Œè¯·é‡æ–°å¯åŠ¨è¿è¡Œæ—¶ã€‚
æˆ‘ä»¬é¼“åŠ±æ‚¨ä¸ç¤¾åŒºåˆ†äº«æ‚¨çš„æ¨¡å‹ã€‚

ç™»å½•æ‚¨çš„ Hugging Face å¸æˆ·å°†å…¶ä¸Šä¼ åˆ°ğŸ¤— Hubã€‚æç¤ºæ—¶ï¼Œè¾“å…¥æ‚¨çš„ä»¤ç‰Œä»¥ç™»å½•ï¼š
```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

è®©æˆ‘ä»¬å®šä¹‰ä¸€äº›å…¨å±€å˜é‡ã€‚
```py
>>> model_checkpoint = "microsoft/layoutlmv2-base-uncased"
>>> batch_size = 4
```

## åŠ è½½æ•°æ®

åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªé¢„å¤„ç†è¿‡çš„ DocVQA çš„å°æ ·æœ¬ï¼Œæ‚¨å¯ä»¥åœ¨ğŸ¤— Hub ä¸Šæ‰¾åˆ°ã€‚

å¦‚æœæ‚¨æƒ³ä½¿ç”¨å®Œæ•´çš„ DocVQA æ•°æ®é›†ï¼Œå¯ä»¥åœ¨ [DocVQA ä¸»é¡µ](https://rrc.cvc.uab.es/?ch=17) ä¸Šæ³¨å†Œå¹¶ä¸‹è½½ã€‚å¦‚æœæ‚¨è¿™æ ·åšï¼Œè¦ç»§ç»­è¿›è¡Œæœ¬æŒ‡å—ï¼Œè¯·æŸ¥çœ‹ [å¦‚ä½•å°†æ–‡ä»¶åŠ è½½åˆ°ğŸ¤—æ•°æ®é›†](https://huggingface.co/docs/datasets/loading#local-and-remote-files)ã€‚

```py
>>> from datasets import load_dataset

>>> dataset = load_dataset("nielsr/docvqa_1200_examples")
>>> dataset
DatasetDict({
    train: Dataset({
        features: ['id', 'image', 'query', 'answers', 'words', 'bounding_boxes', 'answer'],
        num_rows: 1000
    })
    test: Dataset({
        features: ['id', 'image', 'query', 'answers', 'words', 'bounding_boxes', 'answer'],
        num_rows: 200
    })
})
```

æ­£å¦‚æ‚¨æ‰€è§ï¼Œæ•°æ®é›†å·²ç»åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚éšæœºæŸ¥çœ‹ä¸€ä¸ªç¤ºä¾‹ä»¥ç†Ÿæ‚‰å…¶ä¸­çš„ç‰¹å¾ã€‚
```py
>>> dataset["train"].features
```

ä»¥ä¸‹æ˜¯å„ä¸ªå­—æ®µçš„å«ä¹‰ï¼š* `id`ï¼šç¤ºä¾‹çš„ ID* `image`ï¼šåŒ…å«æ–‡æ¡£å›¾åƒçš„ PIL.Image.Image å¯¹è±¡ * `query`ï¼šé—®é¢˜å­—ç¬¦ä¸²-è‡ªç„¶è¯­è¨€æé—®ï¼Œä»¥å¤šç§è¯­è¨€æé—®* `answers`ï¼šç”±äººç±»æ ‡æ³¨è€…æä¾›çš„æ­£ç¡®ç­”æ¡ˆåˆ—è¡¨ * `words` å’Œ `bounding_boxes`ï¼šOCR çš„ç»“æœï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬ä¸ä¼šä½¿ç”¨* `answer`ï¼šç”±å…¶ä»–æ¨¡å‹åŒ¹é…çš„ç­”æ¡ˆï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬ä¸ä¼šä½¿ç”¨
è®©æˆ‘ä»¬ä»…ä¿ç•™è‹±æ–‡é—®é¢˜ï¼Œå¹¶åˆ é™¤ä¼¼ä¹åŒ…å«å¦ä¸€ä¸ªæ¨¡å‹é¢„æµ‹çš„ `answer` ç‰¹å¾ã€‚æˆ‘ä»¬è¿˜å°†ä»æ ‡æ³¨è€…æä¾›çš„ç­”æ¡ˆé›†ä¸­é€‰æ‹©ç¬¬ä¸€ä¸ªç­”æ¡ˆã€‚

æˆ–è€…ï¼Œæ‚¨å¯ä»¥éšæœºæŠ½æ ·ã€‚
```py
>>> updated_dataset = dataset.map(lambda example: {"question": example["query"]["en"]}, remove_columns=["query"])
>>> updated_dataset = updated_dataset.map(
...     lambda example: {"answer": example["answers"][0]}, remove_columns=["answer", "answers"]
... )
```

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬åœ¨æœ¬æŒ‡å—ä¸­ä½¿ç”¨çš„ LayoutLMv2 æ£€æŸ¥ç‚¹å·²ç»è¿›è¡Œäº† `max_position_embeddings = 512` çš„è®­ç»ƒï¼ˆæ‚¨å¯ä»¥åœ¨ [æ£€æŸ¥ç‚¹çš„ `config.json` æ–‡ä»¶](https://huggingface.co/microsoft/layoutlmv2-base-uncased/blob/main/config.json#L18) ä¸­æ‰¾åˆ°è¿™äº›ä¿¡æ¯ï¼‰ã€‚

æˆ‘ä»¬å¯ä»¥æˆªæ–­ç¤ºä¾‹ï¼Œä»¥é¿å…åµŒå…¥å¯èƒ½æ¯” 512 è¿˜é•¿çš„æƒ…å†µï¼Œåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†åˆ é™¤å‡ ä¸ªç¤ºä¾‹ï¼Œå…¶ä¸­åµŒå…¥å¯èƒ½è¶…è¿‡ 512ã€‚å¦‚æœæ‚¨çš„æ•°æ®é›†ä¸­çš„å¤§å¤šæ•°æ–‡æ¡£éƒ½å¾ˆé•¿ï¼Œæ‚¨å¯ä»¥å®æ–½æ»‘åŠ¨çª—å£ç­–ç•¥-è¯¦ç»†ä¿¡æ¯è¯·å‚è§ [æ­¤ç¬”è®°æœ¬](https://github.com/huggingface/notebooks/blob/main/examples/question_answering.ipynb)ã€‚

```py
>>> updated_dataset = updated_dataset.filter(lambda x: len(x["words"]) + len(x["question"].split()) < 512)
```

æ­¤æ—¶ï¼Œè®©æˆ‘ä»¬è¿˜ä»æ­¤æ•°æ®é›†ä¸­åˆ é™¤ OCR åŠŸèƒ½ã€‚è¿™äº›æ˜¯å¦ä¸€ä¸ªæ¨¡å‹çš„å¾®è°ƒçš„ OCR ç»“æœã€‚å¦‚æœæˆ‘ä»¬è¦ä½¿ç”¨å®ƒä»¬ï¼Œå®ƒä»¬ä»ç„¶éœ€è¦è¿›è¡Œä¸€äº›å¤„ç†ï¼Œå› ä¸ºå®ƒä»¬ä¸ç¬¦åˆæˆ‘ä»¬åœ¨æœ¬æŒ‡å—ä¸­ä½¿ç”¨çš„æ¨¡å‹çš„è¾“å…¥è¦æ±‚ã€‚ç›¸åï¼Œæˆ‘ä»¬å¯ä»¥åœ¨åŸå§‹æ•°æ®ä¸Šä½¿ç”¨ [`LayoutLMv2Processor`] è¿›è¡Œ OCR å’Œæ ‡è®°åŒ–å¤„ç†ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å°†è·å¾—ä¸æ¨¡å‹æœŸæœ›è¾“å…¥åŒ¹é…çš„è¾“å…¥ã€‚

å¦‚æœæ‚¨æƒ³æ‰‹åŠ¨å¤„ç†å›¾åƒï¼Œè¯·æŸ¥çœ‹ [`LayoutLMv2` æ¨¡å‹æ–‡æ¡£](../model_doc/layoutlmv2) ä»¥äº†è§£æ¨¡å‹æœŸæœ›çš„è¾“å…¥æ ¼å¼ã€‚

```py
>>> updated_dataset = updated_dataset.remove_columns("words")
>>> updated_dataset = updated_dataset.remove_columns("bounding_boxes")
```

æœ€åï¼Œå¦‚æœæˆ‘ä»¬ä¸æŸ¥çœ‹å›¾åƒç¤ºä¾‹ï¼Œæ•°æ®æ¢ç´¢å°†ä¸å®Œæ•´ã€‚
```py
>>> updated_dataset["train"][11]["image"]
```

<div class="flex justify-center">
     <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/docvqa_example.jpg" alt="DocVQA Image Example"/>
 </div>

## é¢„å¤„ç†æ•°æ®

æ–‡æ¡£é—®ç­”ä»»åŠ¡æ˜¯ä¸€é¡¹å¤šæ¨¡æ€ä»»åŠ¡ï¼Œæ‚¨éœ€è¦ç¡®ä¿æ¯ç§æ¨¡æ€çš„è¾“å…¥æŒ‰ç…§æ¨¡å‹çš„é¢„æœŸè¿›è¡Œé¢„å¤„ç†ã€‚è®©æˆ‘ä»¬ä»åŠ è½½ 
[`LayoutLMv2Processor`] å¼€å§‹ï¼Œè¯¥å¤„ç†å™¨åœ¨å†…éƒ¨ç»“åˆäº†å¯ä»¥å¤„ç†å›¾åƒæ•°æ®çš„å›¾åƒå¤„ç†å™¨ (Image Processor)å’Œå¯ä»¥ç¼–ç æ–‡æœ¬æ•°æ®çš„æ ‡è®°å™¨ã€‚
```py
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained(model_checkpoint)
```

### é¢„å¤„ç†æ–‡æ¡£å›¾åƒ

é¦–å…ˆï¼Œè®©æˆ‘ä»¬é€šè¿‡å¤„ç†å™¨ä¸­çš„ `image_processor` ä¸ºæ¨¡å‹å‡†å¤‡æ–‡æ¡£å›¾åƒã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå›¾åƒå¤„ç†å™¨ (Image Processor)å°†å›¾åƒè°ƒæ•´å¤§å°ä¸º 224x224ï¼Œç¡®ä¿å®ƒä»¬å…·æœ‰æ­£ç¡®çš„é¢œè‰²é€šé“é¡ºåºï¼Œä½¿ç”¨ tesseract åº”ç”¨ OCR ä»¥è·å–å•è¯å’Œè§„èŒƒåŒ–è¾¹ç•Œæ¡†ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæ‰€æœ‰è¿™äº›é»˜è®¤å€¼æ°å¥½ç¬¦åˆæˆ‘ä»¬çš„éœ€æ±‚ã€‚

ç¼–å†™ä¸€ä¸ªå°†é»˜è®¤å›¾åƒå¤„ç†åº”ç”¨äºä¸€æ‰¹å›¾åƒå¹¶è¿”å› OCR ç»“æœçš„å‡½æ•°ã€‚

```py
>>> image_processor = processor.image_processor


>>> def get_ocr_words_and_boxes(examples):
...     images = [image.convert("RGB") for image in examples["image"]]
...     encoded_inputs = image_processor(images)

...     examples["image"] = encoded_inputs.pixel_values
...     examples["words"] = encoded_inputs.words
...     examples["boxes"] = encoded_inputs.boxes

...     return examples
```

è¦å¿«é€Ÿå°†æ­¤é¢„å¤„ç†åº”ç”¨äºæ•´ä¸ªæ•°æ®é›†ï¼Œè¯·ä½¿ç”¨ [`~datasets.Dataset.map`]ã€‚
```py
>>> dataset_with_ocr = updated_dataset.map(get_ocr_words_and_boxes, batched=True, batch_size=2)
```

### é¢„å¤„ç†æ–‡æœ¬æ•°æ®

ä¸€æ—¦æˆ‘ä»¬å¯¹å›¾åƒåº”ç”¨äº† OCRï¼Œæˆ‘ä»¬å°±éœ€è¦å¯¹æ•°æ®é›†çš„æ–‡æœ¬éƒ¨åˆ†è¿›è¡Œç¼–ç ï¼Œä»¥å‡†å¤‡æ¨¡å‹è¾“å…¥ã€‚è¿™æ¶‰åŠå°†æˆ‘ä»¬åœ¨ä¸Šä¸€æ­¥ä¸­è·å¾—çš„å•è¯å’Œè¾¹ç•Œæ¡†è½¬æ¢ä¸ºä»¤ç‰Œçº§åˆ«çš„ `input_ids`ã€`attention_mask`ï¼Œ`token_type_ids` å’Œ `bbox`ã€‚å¯¹äºæ–‡æœ¬é¢„å¤„ç†ï¼Œæˆ‘ä»¬å°†éœ€è¦å¤„ç†å™¨ä¸­çš„ `tokenizer`ã€‚
```py
>>> tokenizer = processor.tokenizer
```

é™¤äº†ä¸Šè¿°é¢„å¤„ç†ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜éœ€è¦ä¸ºæ¨¡å‹æ·»åŠ æ ‡ç­¾ã€‚å¯¹äº `xxxForQuestionAnswering` æ¨¡å‹åœ¨ğŸ¤— Transformers ä¸­ï¼Œæ ‡ç­¾ç”± `start_positions` å’Œ `end_positions` ç»„æˆï¼Œè¡¨ç¤ºç­”æ¡ˆçš„èµ·å§‹å’Œç»“æŸçš„æ ‡è®°ã€‚è®©æˆ‘ä»¬ä»è¿™å¼€å§‹ã€‚å®šä¹‰ä¸€ä¸ªèƒ½åœ¨è¾ƒå¤§åˆ—è¡¨ï¼ˆè¯åˆ—è¡¨ï¼‰ä¸­æ‰¾åˆ°å­åˆ—è¡¨ï¼ˆç­”æ¡ˆæ‹†åˆ†ä¸ºå•è¯ï¼‰çš„è¾…åŠ©å‡½æ•°ã€‚

è¯¥å‡½æ•°å°†ä»¥ä¸¤ä¸ªåˆ—è¡¨ `words_list` å’Œ `answer_list` ä½œä¸ºè¾“å…¥ã€‚ç„¶åï¼Œå®ƒå°†éå† `words_list` å¹¶æ£€æŸ¥
å½“å‰è¯æ˜¯å¦ç­‰äº `words_list` ä¸­çš„ç¬¬ä¸€ä¸ªè¯ï¼ˆwords_list [i]ï¼‰ï¼Œå¹¶ä¸”é•¿åº¦ä¸ `answer_list` ç›¸ç­‰çš„ä»¥å½“å‰è¯ä¸ºèµ·å§‹çš„å­åˆ—è¡¨æ˜¯å¦ç­‰äº `answer_list`ã€‚å¦‚æœæ¡ä»¶æˆç«‹ï¼Œè¡¨ç¤ºæ‰¾åˆ°äº†åŒ¹é…é¡¹ï¼Œå‡½æ•°å°†è®°å½•åŒ¹é…é¡¹åŠå…¶èµ·å§‹ç´¢å¼•ï¼ˆidxï¼‰å’Œç»“æŸç´¢å¼•ï¼ˆidx + len(answer_list) - 1)ã€‚å¦‚æœæ‰¾åˆ°äº†å¤šä¸ªåŒ¹é…é¡¹ï¼Œåˆ™å‡½æ•°åªè¿”å›ç¬¬ä¸€ä¸ªã€‚å¦‚æœæœªæ‰¾åˆ°åŒ¹é…é¡¹ï¼Œå‡½æ•°è¿”å›(`None`ï¼Œ0 å’Œ 0)ã€‚If no match is found, the function returns (`None`, 0, and 0).

```py
>>> def subfinder(words_list, answer_list):
...     matches = []
...     start_indices = []
...     end_indices = []
...     for idx, i in enumerate(range(len(words_list))):
...         if words_list[i] == answer_list[0] and words_list[i : i + len(answer_list)] == answer_list:
...             matches.append(answer_list)
...             start_indices.append(idx)
...             end_indices.append(idx + len(answer_list) - 1)
...     if matches:
...         return matches[0], start_indices[0], end_indices[0]
...     else:
...         return None, 0, 0
```

ä¸ºäº†è¯´æ˜è¯¥å‡½æ•°å¦‚ä½•æ‰¾åˆ°ç­”æ¡ˆçš„ä½ç½®ï¼Œè®©æˆ‘ä»¬ä»¥ä¸€ä¸ªä¾‹å­æ¥ä½¿ç”¨å®ƒï¼š
```py
>>> example = dataset_with_ocr["train"][1]
>>> words = [word.lower() for word in example["words"]]
>>> match, word_idx_start, word_idx_end = subfinder(words, example["answer"].lower().split())
>>> print("Question: ", example["question"])
>>> print("Words:", words)
>>> print("Answer: ", example["answer"])
>>> print("start_index", word_idx_start)
>>> print("end_index", word_idx_end)
Question:  Who is in  cc in this letter?
Words: ['wie', 'baw', 'brown', '&', 'williamson', 'tobacco', 'corporation', 'research', '&', 'development', 'internal', 'correspondence', 'to:', 'r.', 'h.', 'honeycutt', 'ce:', 't.f.', 'riehl', 'from:', '.', 'c.j.', 'cook', 'date:', 'may', '8,', '1995', 'subject:', 'review', 'of', 'existing', 'brainstorming', 'ideas/483', 'the', 'major', 'function', 'of', 'the', 'product', 'innovation', 'graup', 'is', 'to', 'develop', 'marketable', 'nove!', 'products', 'that', 'would', 'be', 'profitable', 'to', 'manufacture', 'and', 'sell.', 'novel', 'is', 'defined', 'as:', 'of', 'a', 'new', 'kind,', 'or', 'different', 'from', 'anything', 'seen', 'or', 'known', 'before.', 'innovation', 'is', 'defined', 'as:', 'something', 'new', 'or', 'different', 'introduced;', 'act', 'of', 'innovating;', 'introduction', 'of', 'new', 'things', 'or', 'methods.', 'the', 'products', 'may', 'incorporate', 'the', 'latest', 'technologies,', 'materials', 'and', 'know-how', 'available', 'to', 'give', 'then', 'a', 'unique', 'taste', 'or', 'look.', 'the', 'first', 'task', 'of', 'the', 'product', 'innovation', 'group', 'was', 'to', 'assemble,', 'review', 'and', 'categorize', 'a', 'list', 'of', 'existing', 'brainstorming', 'ideas.', 'ideas', 'were', 'grouped', 'into', 'two', 'major', 'categories', 'labeled', 'appearance', 'and', 'taste/aroma.', 'these', 'categories', 'are', 'used', 'for', 'novel', 'products', 'that', 'may', 'differ', 'from', 'a', 'visual', 'and/or', 'taste/aroma', 'point', 'of', 'view', 'compared', 'to', 'canventional', 'cigarettes.', 'other', 'categories', 'include', 'a', 'combination', 'of', 'the', 'above,', 'filters,', 'packaging', 'and', 'brand', 'extensions.', 'appearance', 'this', 'category', 'is', 'used', 'for', 'novel', 'cigarette', 'constructions', 'that', 'yield', 'visually', 'different', 'products', 'with', 'minimal', 'changes', 'in', 'smoke', 'chemistry', 'two', 'cigarettes', 'in', 'cne.', 'emulti-plug', 'te', 'build', 'yaur', 'awn', 'cigarette.', 'eswitchable', 'menthol', 'or', 'non', 'menthol', 'cigarette.', '*cigarettes', 'with', 'interspaced', 'perforations', 'to', 'enable', 'smoker', 'to', 'separate', 'unburned', 'section', 'for', 'future', 'smoking.', 'Â«short', 'cigarette,', 'tobacco', 'section', '30', 'mm.', 'Â«extremely', 'fast', 'buming', 'cigarette.', 'Â«novel', 'cigarette', 'constructions', 'that', 'permit', 'a', 'significant', 'reduction', 'iretobacco', 'weight', 'while', 'maintaining', 'smoking', 'mechanics', 'and', 'visual', 'characteristics.', 'higher', 'basis', 'weight', 'paper:', 'potential', 'reduction', 'in', 'tobacco', 'weight.', 'Â«more', 'rigid', 'tobacco', 'column;', 'stiffing', 'agent', 'for', 'tobacco;', 'e.g.', 'starch', '*colored', 'tow', 'and', 'cigarette', 'papers;', 'seasonal', 'promotions,', 'e.g.', 'pastel', 'colored', 'cigarettes', 'for', 'easter', 'or', 'in', 'an', 'ebony', 'and', 'ivory', 'brand', 'containing', 'a', 'mixture', 'of', 'all', 'black', '(black', 'paper', 'and', 'tow)', 'and', 'ail', 'white', 'cigarettes.', '499150498']
Answer:  T.F. Riehl
start_index 17
end_index 18
```

ç„¶è€Œï¼Œä¸€æ—¦å¯¹ç¤ºä¾‹è¿›è¡Œç¼–ç ï¼Œå®ƒä»¬å°†å˜æˆè¿™æ ·ï¼š
```py
>>> encoding = tokenizer(example["question"], example["words"], example["boxes"])
>>> tokenizer.decode(encoding["input_ids"])
[CLS] who is in cc in this letter? [SEP] wie baw brown & williamson tobacco corporation research & development ...
```

æˆ‘ä»¬éœ€è¦æ‰¾åˆ°ç¼–ç è¾“å…¥ä¸­ç­”æ¡ˆçš„ä½ç½®ã€‚* `token_type_ids` å‘Šè¯‰æˆ‘ä»¬å“ªäº›æ ‡è®°å±äºé—®é¢˜ï¼Œå“ªäº›å±äºæ–‡æ¡£çš„è¯ã€‚* `tokenizer.cls_token_id` å°†å¸®åŠ©æ‰¾åˆ°è¾“å…¥å¼€å¤´çš„ç‰¹æ®Šæ ‡è®°ã€‚* `word_ids` å°†å¸®åŠ©å°†åŸå§‹ `words` ä¸­æ‰¾åˆ°çš„ç­”æ¡ˆä¸å®Œæ•´ç¼–ç è¾“å…¥ä¸­çš„ç›¸åŒç­”æ¡ˆåŒ¹é…ï¼Œå¹¶ç¡®å®šç¼–ç è¾“å…¥ä¸­ç­”æ¡ˆçš„èµ·å§‹/ç»“æŸä½ç½®ã€‚
æœ‰äº†è¿™ä¸ªæƒ³æ³•åï¼Œæˆ‘ä»¬æ¥åˆ›å»ºä¸€ä¸ªæ‰¹é‡ç¼–ç æ•°æ®é›†çš„å‡½æ•°ï¼š
```py
>>> def encode_dataset(examples, max_length=512):
...     questions = examples["question"]
...     words = examples["words"]
...     boxes = examples["boxes"]
...     answers = examples["answer"]

...     # encode the batch of examples and initialize the start_positions and end_positions
...     encoding = tokenizer(questions, words, boxes, max_length=max_length, padding="max_length", truncation=True)
...     start_positions = []
...     end_positions = []

...     # loop through the examples in the batch
...     for i in range(len(questions)):
...         cls_index = encoding["input_ids"][i].index(tokenizer.cls_token_id)

...         # find the position of the answer in example's words
...         words_example = [word.lower() for word in words[i]]
...         answer = answers[i]
...         match, word_idx_start, word_idx_end = subfinder(words_example, answer.lower().split())

...         if match:
...             # if match is found, use `token_type_ids` to find where words start in the encoding
...             token_type_ids = encoding["token_type_ids"][i]
...             token_start_index = 0
...             while token_type_ids[token_start_index] != 1:
...                 token_start_index += 1

...             token_end_index = len(encoding["input_ids"][i]) - 1
...             while token_type_ids[token_end_index] != 1:
...                 token_end_index -= 1

...             word_ids = encoding.word_ids(i)[token_start_index : token_end_index + 1]
...             start_position = cls_index
...             end_position = cls_index

...             # loop over word_ids and increase `token_start_index` until it matches the answer position in words
...             # once it matches, save the `token_start_index` as the `start_position` of the answer in the encoding
...             for id in word_ids:
...                 if id == word_idx_start:
...                     start_position = token_start_index
...                 else:
...                     token_start_index += 1

...             # similarly loop over `word_ids` starting from the end to find the `end_position` of the answer
...             for id in word_ids[::-1]:
...                 if id == word_idx_end:
...                     end_position = token_end_index
...                 else:
...                     token_end_index -= 1

...             start_positions.append(start_position)
...             end_positions.append(end_position)

...         else:
...             start_positions.append(cls_index)
...             end_positions.append(cls_index)

...     encoding["image"] = examples["image"]
...     encoding["start_positions"] = start_positions
...     encoding["end_positions"] = end_positions

...     return encoding
```

ç°åœ¨æˆ‘ä»¬æœ‰äº†è¿™ä¸ªé¢„å¤„ç†å‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œç¼–ç ï¼š
```py
>>> encoded_train_dataset = dataset_with_ocr["train"].map(
...     encode_dataset, batched=True, batch_size=2, remove_columns=dataset_with_ocr["train"].column_names
... )
>>> encoded_test_dataset = dataset_with_ocr["test"].map(
...     encode_dataset, batched=True, batch_size=2, remove_columns=dataset_with_ocr["test"].column_names
... )
```

è®©æˆ‘ä»¬çœ‹çœ‹ç¼–ç æ•°æ®é›†çš„ç‰¹å¾æ˜¯ä»€ä¹ˆæ ·çš„ï¼š
```py
>>> encoded_train_dataset.features
{'image': Sequence(feature=Sequence(feature=Sequence(feature=Value(dtype='uint8', id=None), length=-1, id=None), length=-1, id=None), length=-1, id=None),
 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),
 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),
 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),
 'bbox': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None),
 'start_positions': Value(dtype='int64', id=None),
 'end_positions': Value(dtype='int64', id=None)}
```

## è¯„ä¼°

æ–‡æ¡£é—®ç­”è¯„ä¼°éœ€è¦å¤§é‡çš„åå¤„ç†å·¥ä½œã€‚ä¸ºäº†èŠ‚çœæ‚¨çš„æ—¶é—´ï¼Œæœ¬æŒ‡å—è·³è¿‡äº†è¯„ä¼°æ­¥éª¤ã€‚[`Trainer`] ä»ç„¶ä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è®¡ç®—è¯„ä¼°æŸå¤±ï¼Œå› æ­¤æ‚¨ä¸ä¼šå¯¹æ¨¡å‹çš„æ€§èƒ½å®Œå…¨ä¸€æ— æ‰€çŸ¥ã€‚æŠ½å–å¼é—®ç­”é€šå¸¸ä½¿ç”¨ F1/å®Œå…¨åŒ¹é…è¿›è¡Œè¯„ä¼°ã€‚å¦‚æœæ‚¨å¸Œæœ›è‡ªå·±å®ç°å®ƒï¼Œè¯·æŸ¥çœ‹ [Hugging Face è¯¾ç¨‹çš„é—®ç­”ç« èŠ‚](https://huggingface.co/course/chapter7/7?fw=pt#postprocessing) ä»¥è·å–çµæ„Ÿã€‚

## è®­ç»ƒ

æ­å–œï¼æ‚¨å·²æˆåŠŸå®Œæˆæœ¬æŒ‡å—æœ€è‰°éš¾çš„éƒ¨åˆ†ï¼Œç°åœ¨æ‚¨å·²ç»å‡†å¤‡å¥½è®­ç»ƒè‡ªå·±çš„æ¨¡å‹äº†ã€‚

è®­ç»ƒåŒ…æ‹¬ä»¥ä¸‹æ­¥éª¤ï¼š
* ä½¿ç”¨ä¸é¢„å¤„ç†ç›¸åŒçš„æ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹ [`AutoModelForDocumentQuestionAnswering`]ã€‚
* åœ¨ [`TrainingArguments`] ä¸­å®šä¹‰æ‚¨çš„è®­ç»ƒè¶…å‚æ•°ã€‚* å®šä¹‰ä¸€ä¸ªå°†ç¤ºä¾‹æ‰¹å¤„ç†åœ¨ä¸€èµ·çš„å‡½æ•°ï¼Œè¿™é‡Œä½¿ç”¨ [`DefaultDataCollator`] å³å¯ã€‚
* å°†è®­ç»ƒå‚æ•°ä¸æ¨¡å‹ã€æ•°æ®é›†å’Œæ•°æ®æ”¶é›†å™¨ä¸€èµ·ä¼ é€’ç»™ [`Trainer`]ã€‚
* è°ƒç”¨ [`~Trainer.train`] æ¥å¾®è°ƒæ‚¨çš„æ¨¡å‹ã€‚


```py
>>> from transformers import AutoModelForDocumentQuestionAnswering

>>> model = AutoModelForDocumentQuestionAnswering.from_pretrained(model_checkpoint)
```

åœ¨ [`TrainingArguments`] ä¸­ä½¿ç”¨ `output_dir` æ¥æŒ‡å®šä¿å­˜æ¨¡å‹çš„ä½ç½®ï¼Œå¹¶æ ¹æ®éœ€è¦é…ç½®è¶…å‚æ•°ã€‚

å¦‚æœæ‚¨å¸Œæœ›ä¸ç¤¾åŒºåˆ†äº«æ‚¨çš„æ¨¡å‹ï¼Œè¯·å°† `push_to_hub` è®¾ç½®ä¸º `True`ï¼ˆæ‚¨å¿…é¡»ç™»å½• Hugging Face æ‰èƒ½ä¸Šä¼ æ¨¡å‹ï¼‰ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ`output_dir` ä¹Ÿå°†æ˜¯æ‚¨çš„æ¨¡å‹æ£€æŸ¥ç‚¹å°†è¢«æ¨é€åˆ°çš„ä»“åº“çš„åç§°ã€‚

```py
>>> from transformers import TrainingArguments

>>> # REPLACE THIS WITH YOUR REPO ID
>>> repo_id = "MariaK/layoutlmv2-base-uncased_finetuned_docvqa"

>>> training_args = TrainingArguments(
...     output_dir=repo_id,
...     per_device_train_batch_size=4,
...     num_train_epochs=20,
...     save_steps=200,
...     logging_steps=50,
...     evaluation_strategy="steps",
...     learning_rate=5e-5,
...     save_total_limit=2,
...     remove_unused_columns=False,
...     push_to_hub=True,
... )
```

å®šä¹‰ä¸€ä¸ªç®€å•çš„æ•°æ®æ”¶é›†å™¨ï¼Œå°†ç¤ºä¾‹æ‰¹å¤„ç†åœ¨ä¸€èµ·ã€‚
```py
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator()
```

æœ€åï¼Œå°†æ‰€æœ‰å†…å®¹æ•´åˆåœ¨ä¸€èµ·ï¼Œå¹¶è°ƒç”¨ [`~Trainer.train`]ï¼š
```py
>>> from transformers import Trainer

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     data_collator=data_collator,
...     train_dataset=encoded_train_dataset,
...     eval_dataset=encoded_test_dataset,
...     tokenizer=processor,
... )

>>> trainer.train()
```

è¦å°†æœ€ç»ˆæ¨¡å‹æ·»åŠ åˆ°ğŸ¤— Hubï¼Œè¯·åˆ›å»ºä¸€ä¸ªæ¨¡å‹å¡å¹¶è°ƒç”¨ `push_to_hub`ï¼š
```py
>>> trainer.create_model_card()
>>> trainer.push_to_hub()
```

## æ¨ç†

ç°åœ¨ï¼Œæ‚¨å·²ç»å¾®è°ƒäº†ä¸€ä¸ª LayoutLMv2 æ¨¡å‹ï¼Œå¹¶å°†å…¶ä¸Šä¼ åˆ°äº†ğŸ¤— Hubï¼Œæ‚¨å¯ä»¥ç”¨å®ƒè¿›è¡Œæ¨ç†ã€‚è¯•ç”¨æ‚¨å¾®è°ƒçš„æ¨¡å‹è¿›è¡Œæ¨ç†çš„æœ€ç®€å•çš„æ–¹æ³•æ˜¯åœ¨ [`Pipeline`] ä¸­ä½¿ç”¨å®ƒã€‚
è®©æˆ‘ä»¬æ¥çœ‹ä¸ªä¾‹å­ï¼š

```py
>>> example = dataset["test"][2]
>>> question = example["query"]["en"]
>>> image = example["image"]
>>> print(question)
>>> print(example["answers"])
'Who is â€˜presidingâ€™ TRRF GENERAL SESSION (PART 1)?'
['TRRF Vice President', 'lee a. waller']
```

æ¥ä¸‹æ¥ï¼Œä½¿ç”¨æ‚¨çš„æ¨¡å‹ä¸ºæ–‡æ¡£é—®ç­”å®ä¾‹åŒ–ä¸€ä¸ªç®¡é“ï¼Œå¹¶å°†å›¾åƒ+é—®é¢˜ç»„åˆä¼ é€’ç»™å®ƒã€‚
```py
>>> from transformers import pipeline

>>> qa_pipeline = pipeline("document-question-answering", model="MariaK/layoutlmv2-base-uncased_finetuned_docvqa")
>>> qa_pipeline(image, question)
[{'score': 0.9949808120727539,
  'answer': 'Lee A. Waller',
  'start': 55,
  'end': 57}]
```

å¦‚æœæ‚¨æ„¿æ„ï¼Œæ‚¨è¿˜å¯ä»¥æ‰‹åŠ¨å¤åˆ¶ç®¡é“çš„ç»“æœï¼š
1. å–ä¸€å¼ å›¾ç‰‡å’Œä¸€ä¸ªé—®é¢˜ï¼Œä½¿ç”¨æ‚¨æ¨¡å‹çš„å¤„ç†å™¨å°†å®ƒä»¬å‡†å¤‡å¥½ã€‚
2. å°†ç»“æœæˆ–å¤„ç†ç»“æœä¼ é€’ç»™æ¨¡å‹ã€‚
3. æ¨¡å‹è¿”å› `start_logits` å’Œ `end_logits`ï¼Œå®ƒä»¬è¡¨ç¤ºç­”æ¡ˆçš„èµ·å§‹æ ‡è®°å’Œç»“æŸæ ‡è®°ã€‚ä¸¤è€…çš„å½¢çŠ¶éƒ½æ˜¯ï¼ˆbatch_sizeï¼Œsequence_lengthï¼‰ã€‚
4. å¯¹ `start_logits` å’Œ `end_logits` çš„æœ€åä¸€ä¸ªç»´åº¦è¿›è¡Œ argmaxï¼Œå¾—åˆ°é¢„æµ‹çš„ `start_idx` å’Œ `end_idx`ã€‚
5. ä½¿ç”¨åˆ†è¯å™¨ (Tokenizer)è§£ç ç­”æ¡ˆã€‚
```py
>>> import torch
>>> from transformers import AutoProcessor
>>> from transformers import AutoModelForDocumentQuestionAnswering

>>> processor = AutoProcessor.from_pretrained("MariaK/layoutlmv2-base-uncased_finetuned_docvqa")
>>> model = AutoModelForDocumentQuestionAnswering.from_pretrained("MariaK/layoutlmv2-base-uncased_finetuned_docvqa")

>>> with torch.no_grad():
...     encoding = processor(image.convert("RGB"), question, return_tensors="pt")
...     outputs = model(**encoding)
...     start_logits = outputs.start_logits
...     end_logits = outputs.end_logits
...     predicted_start_idx = start_logits.argmax(-1).item()
...     predicted_end_idx = end_logits.argmax(-1).item()

>>> processor.tokenizer.decode(encoding.input_ids.squeeze()[predicted_start_idx : predicted_end_idx + 1])
'lee a. waller'
```