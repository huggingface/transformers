<!--ç‰ˆæƒæ‰€æœ‰ 2022 å¹´ HuggingFace å›¢é˜Ÿã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚
æ ¹æ® Apache Licenseï¼ŒVersion 2.0ï¼ˆå³â€œè®¸å¯è¯â€ï¼‰è·å¾—è®¸å¯ï¼›é™¤éç¬¦åˆè®¸å¯è¯çš„è§„å®šï¼Œå¦åˆ™æ‚¨ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ä½ç½®è·å–è®¸å¯è¯çš„å‰¯æœ¬
http://www.apache.org/licenses/LICENSE-2.0
é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œæ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯æŒ‰ "æŒ‰åŸæ ·" åŸºç¡€åˆ†å‘çš„ï¼Œä¸é™„å¸¦ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„ä¿è¯æˆ–æ¡ä»¶ã€‚æœ‰å…³è®¸å¯è¯çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è®¸å¯è¯ã€‚
âš ï¸ è¯·æ³¨æ„ï¼Œæ­¤æ–‡ä»¶æ˜¯ Markdown æ ¼å¼ï¼Œä½†åŒ…å«ç‰¹å®šäºæˆ‘ä»¬çš„ doc-builderï¼ˆç±»ä¼¼äº MDXï¼‰çš„è¯­æ³•ï¼Œå¯èƒ½æ— æ³•åœ¨æ‚¨çš„ Markdown æŸ¥çœ‹å™¨ä¸­æ­£ç¡®å‘ˆç°ã€‚æ¸²æŸ“ã€‚
-->


# åœ¨å•ä¸ª GPU ä¸Šè¿›è¡Œé«˜æ•ˆæ¨ç†

é™¤äº†æœ¬æŒ‡å—å¤–ï¼Œè¿˜å¯ä»¥åœ¨ [åœ¨å•ä¸ª GPU ä¸Šè¿›è¡Œè®­ç»ƒçš„æŒ‡å—](perf_train_gpu_one) å’Œ [åœ¨ CPU ä¸Šè¿›è¡Œæ¨ç†çš„æŒ‡å—](perf_infer_cpu) ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚
## æ›´å¥½çš„ Transformerï¼šåŸºäº PyTorch çš„ Transformer å¿«é€Ÿè·¯å¾„

åŸºäº PyTorch çš„ [`nn.MultiHeadAttention`](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/) æ³¨æ„åŠ›å¿«é€Ÿè·¯å¾„ï¼Œç§°ä¸º BetterTransformerï¼Œå¯ä»¥é€šè¿‡ [ğŸ¤— Optimum åº“](https://huggingface.co/docs/optimum/bettertransformer/overview) ä¸­çš„é›†æˆä¸ Transformers ä¸€èµ·ä½¿ç”¨ã€‚

PyTorch çš„æ³¨æ„åŠ›å¿«é€Ÿè·¯å¾„é€šè¿‡å†…æ ¸èåˆå’Œä½¿ç”¨ [åµŒå¥—å¼ é‡](https://pytorch.org/docs/stable/nested.html) æ¥åŠ é€Ÿæ¨ç†ã€‚è¯¦ç»†çš„åŸºå‡†æµ‹è¯•å¯ä»¥åœ¨ [æ­¤åšå®¢æ–‡ç« ](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2) ä¸­æ‰¾åˆ°ã€‚

åœ¨å®‰è£…äº† [`optimum`](https://github.com/huggingface/optimum) åŒ…ä¹‹åï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä½¿ç”¨ Better Transformerï¼Œå¯ä»¥é€šè¿‡è°ƒç”¨ [`~PreTrainedModel.to_bettertransformer`] æ¥æ›¿æ¢ç›¸å…³çš„å†…éƒ¨æ¨¡å—ï¼š
```python
model = model.to_bettertransformer()
```

æ–¹æ³• [`~PreTrainedModel.reverse_bettertransformer`] å…è®¸è¿”å›åŸå§‹å»ºæ¨¡ï¼Œåœ¨ä¿å­˜æ¨¡å‹ä¹‹å‰åº”ä½¿ç”¨è¯¥æ–¹æ³•ï¼Œä»¥ä½¿ç”¨è§„èŒƒçš„ transformers å»ºæ¨¡ï¼š
```python
model = model.reverse_bettertransformer()
model.save_pretrained("saved_model")
```

ä» PyTorch 2.0 å¼€å§‹ï¼Œæ³¨æ„åŠ›å¿«é€Ÿè·¯å¾„æ”¯æŒç¼–ç å™¨å’Œè§£ç å™¨ã€‚æ”¯æŒçš„æ¶æ„åˆ—è¡¨å¯ä»¥åœ¨ [è¿™é‡Œ](https://huggingface.co/docs/optimum/bettertransformer/overview#supported-models) æ‰¾åˆ°ã€‚

## `bitsandbytes` é›†æˆç”¨äº FP4 æ··åˆç²¾åº¦æ¨ç†
æ‚¨å¯ä»¥å®‰è£… `bitsandbytes` å¹¶ä»ä¸­å—ç›Šï¼Œä»¥ä¾¿åœ¨ GPU ä¸Šè½»æ¾å‹ç¼©æ¨¡å‹ã€‚ä½¿ç”¨ FP4 é‡åŒ–ï¼Œä¸å…¶æœ¬æœºå®Œå…¨ç²¾åº¦ç‰ˆæœ¬ç›¸æ¯”ï¼Œæ‚¨å¯ä»¥å°†æ¨¡å‹å¤§å°å‡å°é«˜è¾¾ 8 å€ã€‚è¯·æŸ¥çœ‹ä»¥ä¸‹å¦‚ä½•å…¥é—¨ã€‚
<Tip>

è¯·æ³¨æ„ï¼Œæ­¤åŠŸèƒ½ä¹Ÿå¯ä»¥åœ¨å¤š GPU è®¾ç½®ä¸­ä½¿ç”¨ã€‚
</Tip>

### è¦æ±‚

- æœ€æ–°çš„ `bitsandbytes` åº“ `pip install bitsandbytes>=0.39.0`
- ä»æºä»£ç å®‰è£…æœ€æ–°çš„ `accelerate` `pip install git+https://github.com/huggingface/accelerate.git`
- ä»æºä»£ç å®‰è£…æœ€æ–°çš„ `transformers` `pip install git+https://github.com/huggingface/transformers.git`

### è¿è¡Œ FP4 æ¨¡å‹ - å•ä¸ª GPU è®¾ç½® - å¿«é€Ÿå…¥é—¨

æ‚¨å¯ä»¥é€šè¿‡è¿è¡Œä»¥ä¸‹ä»£ç å¿«é€Ÿåœ¨å•ä¸ª GPU ä¸Šè¿è¡Œ FP4 æ¨¡å‹ï¼š
```py
from transformers import AutoModelForCausalLM

model_name = "bigscience/bloom-2b5"
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_4bit=True)
```

è¯·æ³¨æ„ï¼Œ`device_map` æ˜¯å¯é€‰çš„ï¼Œä½†åœ¨æ¨ç†ä¸­è®¾ç½® `device_map = 'auto'` æ˜¯é¦–é€‰ï¼Œå› ä¸ºå®ƒå°†é«˜æ•ˆåœ°å°†æ¨¡å‹åˆ†é…åˆ°å¯ç”¨çš„èµ„æºä¸Šã€‚

### è¿è¡Œ FP4 æ¨¡å‹ - å¤š GPU è®¾ç½®

å°†æ··åˆ 8 ä½æ¨¡å‹åŠ è½½åˆ°å¤šä¸ª GPU ä¸­çš„æ–¹æ³•å¦‚ä¸‹ï¼ˆä¸å•ä¸ª GPU è®¾ç½®ç›¸åŒï¼‰ï¼š

```py
model_name = "bigscience/bloom-2b5"
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_4bit=True)
```
ä½†æ˜¯ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ `accelerate` æ¥æ§åˆ¶è¦åœ¨æ¯ä¸ª GPU ä¸Šåˆ†é…çš„ GPU RAMã€‚ä½¿ç”¨ `max_memory` å‚æ•°å¦‚ä¸‹æ‰€ç¤ºï¼š
```py
max_memory_mapping = {0: "600MB", 1: "1GB"}
model_name = "bigscience/bloom-3b"
model_8bit = AutoModelForCausalLM.from_pretrained(
    model_name, device_map="auto", load_in_4bit=True, max_memory=max_memory_mapping
)
```

åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œç¬¬ä¸€ä¸ª GPU å°†ä½¿ç”¨ 600MB çš„å†…å­˜ï¼Œç¬¬äºŒä¸ª GPU å°†ä½¿ç”¨ 1GB çš„å†…å­˜ã€‚

### é«˜çº§ç”¨æ³•

æœ‰å…³æ­¤æ–¹æ³•çš„æ›´é«˜çº§ç”¨æ³•ï¼Œè¯·å‚é˜… [é‡åŒ–](main_classes/quantization) æ–‡æ¡£é¡µé¢ã€‚

## `bitsandbytes` é›†æˆç”¨äº Int8 æ··åˆç²¾åº¦çŸ©é˜µåˆ†è§£

<Tip>
è¯·æ³¨æ„ï¼Œæ­¤åŠŸèƒ½ä¹Ÿå¯ä»¥åœ¨å¤š GPU è®¾ç½®ä¸­ä½¿ç”¨ã€‚
ä»è®ºæ–‡ [`LLM.int8()ï¼šå¤§è§„æ¨¡Transformerçš„8ä½çŸ©é˜µä¹˜æ³•`](https://arxiv.org/abs/2208.07339) å¼€å§‹ï¼Œæˆ‘ä»¬æ”¯æŒ Hub ä¸­æ‰€æœ‰æ¨¡å‹çš„ Hugging Face é›†æˆï¼Œåªéœ€å‡ è¡Œä»£ç ã€‚

è¯¥æ–¹æ³•é€šè¿‡å°† `nn.Linear` å¤§å°å‡å° 2ï¼ˆå¯¹äº `float16` å’Œ `bfloat16` æƒé‡ï¼‰å’Œ 4ï¼ˆå¯¹äº `float32` æƒé‡ï¼‰æ¥æ“ä½œåŠç²¾åº¦ä¸­çš„ç¦»ç¾¤ç‚¹ï¼Œå‡ ä¹ä¸ä¼šå¯¹è´¨é‡äº§ç”Ÿå½±å“ã€‚
![HFxbitsandbytes.png](https://s3.amazonaws.com/moonup/production/uploads/1659861207959-62441d1d9fdefb55a0b7d12c.png)
Int8 æ··åˆç²¾åº¦çŸ©é˜µåˆ†è§£é€šè¿‡å°†çŸ©é˜µä¹˜æ³•åˆ†ä¸ºä¸¤ä¸ªæµï¼ˆ1ï¼‰åœ¨ fp16 ä¸­è¿›è¡ŒçŸ©é˜µä¹˜æ³•çš„ç³»ç»Ÿç‰¹å¾ç¦»ç¾¤ç‚¹æµï¼ˆ0.01%ï¼‰ï¼Œï¼ˆ2ï¼‰è¿›è¡Œ int8 çŸ©é˜µä¹˜æ³•çš„å¸¸è§„æµï¼ˆ99.9%ï¼‰æ¥å·¥ä½œã€‚ä½¿ç”¨æ­¤æ–¹æ³•ï¼Œå¯¹äºéå¸¸å¤§çš„æ¨¡å‹ï¼Œå¯ä»¥è¿›è¡Œ int8 æ¨ç†è€Œå‡ ä¹ä¸ä¼šäº§ç”Ÿé¢„æµ‹æ€§èƒ½ä¸‹é™ã€‚æœ‰å…³è¯¥æ–¹æ³•çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ [è®ºæ–‡](https://arxiv.org/abs/2208.07339) æˆ–æˆ‘ä»¬çš„ [å…³äºé›†æˆçš„åšå®¢æ–‡ç« ](https://huggingface.co/blog/hf-bitsandbytes-integration)ã€‚
![MixedInt8.gif](https://s3.amazonaws.com/moonup/production/uploads/1660567469965-62441d1d9fdefb55a0b7d12c.gif)

è¯·æ³¨æ„ï¼Œè¦è¿è¡Œæ··åˆ 8 ä½æ¨¡å‹ï¼Œæ‚¨éœ€è¦ä¸€ä¸ª GPUï¼Œå› ä¸ºå†…æ ¸ä»…é’ˆå¯¹ GPU è¿›è¡Œäº†ç¼–è¯‘ã€‚åœ¨ä½¿ç”¨æ­¤åŠŸèƒ½ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨æœ‰è¶³å¤Ÿçš„ GPU å†…å­˜æ¥å­˜å‚¨æ¨¡å‹çš„å››åˆ†ä¹‹ä¸€ï¼ˆæˆ–åŠç²¾åº¦çš„æƒé‡ï¼‰ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¸®åŠ©æ‚¨ä½¿ç”¨æ­¤æ¨¡å—çš„æ³¨æ„äº‹é¡¹ï¼Œæˆ–è€…å¯ä»¥åœ¨ [Google Colab çš„æ¼”ç¤º](#colab-demos) ä¸­æŸ¥çœ‹æ¼”ç¤ºã€‚

### è¦æ±‚

- å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯ `bitsandbytes<0.37.0`ï¼Œè¯·ç¡®ä¿åœ¨æ”¯æŒ 8 ä½å¼ é‡æ ¸å¿ƒï¼ˆå›¾çµï¼Œå®‰åŸ¹æˆ–æ›´æ–°çš„æ¶æ„ï¼Œä¾‹å¦‚ T4ï¼ŒRTX20s RTX30sï¼ŒA40-A100ï¼‰çš„ NVIDIA GPU ä¸Šè¿è¡Œã€‚å¯¹äº `bitsandbytes>=0.37.0`ï¼Œåº”æ”¯æŒæ‰€æœ‰ GPUã€‚- å®‰è£…æ­£ç¡®ç‰ˆæœ¬çš„ `bitsandbytes`ï¼Œè¯·è¿è¡Œï¼š`pip install bitsandbytes>=0.31.5`- å®‰è£… `accelerate` `pip install accelerate>=0.12.0`

### è¿è¡Œæ··åˆ Int8 æ¨¡å‹ - å•ä¸ª GPU è®¾ç½®

åœ¨å®‰è£…æ‰€éœ€çš„åº“ä¹‹åï¼ŒåŠ è½½æ··åˆ 8 ä½æ¨¡å‹çš„æ–¹æ³•å¦‚ä¸‹ï¼š

```py
from transformers import AutoModelForCausalLM

model_name = "bigscience/bloom-2b5"
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_8bit=True)
```

å¯¹äºæ–‡æœ¬ç”Ÿæˆï¼Œæˆ‘ä»¬å»ºè®®ï¼š
- ä½¿ç”¨æ¨¡å‹çš„ `generate()` æ–¹æ³•è€Œä¸æ˜¯ `pipeline()` å‡½æ•°ã€‚å°½ç®¡å¯ä»¥ä½¿ç”¨ `pipeline()` å‡½æ•°è¿›è¡Œæ¨ç†ï¼Œä½†å®ƒå¯¹äºæ··åˆ 8 ä½æ¨¡å‹æ¥è¯´å¹¶ä¸æ˜¯æœ€ä¼˜åŒ–çš„ï¼Œå¹¶ä¸”æ¯”ä½¿ç”¨ `generate()` æ–¹æ³•æ…¢ã€‚æ­¤å¤–ï¼ŒæŸäº›é‡‡æ ·ç­–ç•¥ï¼ˆå¦‚æ ¸å¿ƒé‡‡æ ·ï¼‰ä¸æ”¯æŒæ··åˆ 8 ä½æ¨¡å‹çš„ `pipeline()` å‡½æ•°ã€‚- å°†æ‰€æœ‰è¾“å…¥æ”¾ç½®åœ¨ä¸æ¨¡å‹ç›¸åŒçš„è®¾å¤‡ä¸Šã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹ï¼š

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "bigscience/bloom-2b5"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_8bit=True)

prompt = "Hello, my llama is cute"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
generated_ids = model.generate(**inputs)
outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
```


### è¿è¡Œæ··åˆ int8 æ¨¡å‹ - å¤š GPU è®¾ç½®

å°†æ··åˆ 8 ä½æ¨¡å‹åŠ è½½åˆ°å¤šä¸ª GPU ä¸­çš„æ–¹æ³•å¦‚ä¸‹ï¼ˆä¸å•ä¸ª GPU è®¾ç½®ç›¸åŒï¼‰ï¼š
```py
model_name = "bigscience/bloom-2b5"
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_8bit=True)
```

ä½†æ˜¯ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ `accelerate` æ¥æ§åˆ¶è¦åœ¨æ¯ä¸ª GPU ä¸Šåˆ†é…çš„ GPU RAMã€‚ä½¿ç”¨ `max_memory` å‚æ•°å¦‚ä¸‹æ‰€ç¤ºï¼š
```py
max_memory_mapping = {0: "1GB", 1: "2GB"}
model_name = "bigscience/bloom-3b"
model_8bit = AutoModelForCausalLM.from_pretrained(
    model_name, device_map="auto", load_in_8bit=True, max_memory=max_memory_mapping
)
```

åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œç¬¬ä¸€ä¸ª GPU å°†ä½¿ç”¨ 1GB çš„å†…å­˜ï¼Œç¬¬äºŒä¸ª GPU å°†ä½¿ç”¨ 2GB çš„å†…å­˜ã€‚

### Colab æ¼”ç¤º

ä½¿ç”¨æ­¤æ–¹æ³•æ‚¨å¯ä»¥å¯¹ä»¥å‰æ— æ³•åœ¨ Google Colab ä¸Šè¿›è¡Œæ¨ç†çš„æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚æŸ¥çœ‹åœ¨ Google Colab ä¸Šè¿è¡Œ T5-11bï¼ˆ42GB in fp32ï¼‰çš„æ¼”ç¤ºï¼ä½¿ç”¨ 8 ä½é‡åŒ–ï¼š
[![åœ¨ Colab ä¸­æ‰“å¼€ï¼šT5-11b æ¼”ç¤º](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1YORPWx4okIHXnjW7MSAidXN29mPVNT7F?usp=sharing)
æˆ–è€…ä½¿ç”¨ BLOOM-3B è¿›è¡Œæ¼”ç¤ºï¼š

[![åœ¨ Colab ä¸­æ‰“å¼€ï¼šBLOOM-3b æ¼”ç¤º](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1qOjXfQIAULfKvZqwCen8-MoWKGdSatZ4?usp=sharing)