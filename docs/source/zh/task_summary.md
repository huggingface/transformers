<!--ç‰ˆæƒæ‰€æœ‰ 2020 å¹´ HuggingFace å›¢é˜Ÿã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚
æ ¹æ® Apache è®¸å¯è¯ï¼Œç¬¬ 2.0 ç‰ˆï¼ˆâ€œè®¸å¯è¯â€ï¼‰ï¼Œæ‚¨ä¸å¾—åœ¨æœªéµå®ˆè®¸å¯è¯çš„æƒ…å†µä¸‹ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ä½ç½®è·å–è®¸å¯è¯çš„å‰¯æœ¬
http://www.apache.org/licenses/LICENSE-2.0
é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œæ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯åŸºäºâ€œæŒ‰åŸæ ·â€æä¾›çš„ï¼Œæ— è®ºæ˜¯æ˜ç¤ºçš„è¿˜æ˜¯æš—ç¤ºçš„ã€‚è¯·å‚é˜…è®¸å¯è¯ä»¥è·å–ç‰¹å®šè¯­è¨€ä¸‹çš„æƒé™å’Œé™åˆ¶çš„è¯¦ç»†ä¿¡æ¯ã€‚âš ï¸ è¯·æ³¨æ„ï¼Œæ­¤æ–‡ä»¶æ˜¯ Markdown æ ¼å¼çš„ï¼Œä½†åŒ…å«æˆ‘ä»¬çš„æ–‡æ¡£ç”Ÿæˆå™¨ï¼ˆç±»ä¼¼äº MDXï¼‰çš„ç‰¹å®šè¯­æ³•ï¼Œæ‚¨çš„ Markdown æŸ¥çœ‹å™¨å¯èƒ½æ— æ³•æ­£ç¡®æ¸²æŸ“ã€‚
-->

# ğŸ¤— Transformers å¯ä»¥åšä»€ä¹ˆ


ğŸ¤— Transformers æ˜¯ä¸€ä¸ªé¢„è®­ç»ƒçš„æœ€å…ˆè¿›æ¨¡å‹åº“ï¼Œç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ã€è®¡ç®—æœºè§†è§‰ã€éŸ³é¢‘å’Œè¯­éŸ³å¤„ç†ä»»åŠ¡ã€‚è¯¥åº“ä¸ä»…åŒ…å« Transformer æ¨¡å‹ï¼Œè¿˜åŒ…æ‹¬ç°ä»£å·ç§¯ç½‘ç»œç­‰é Transformer æ¨¡å‹ï¼Œç”¨äºè®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚å¦‚æœæ‚¨çœ‹ä¸€ä¸‹å½“ä»Šæœ€å—æ¬¢è¿çš„æ¶ˆè´¹äº§å“ï¼Œæ¯”å¦‚æ™ºèƒ½æ‰‹æœºã€åº”ç”¨ç¨‹åºå’Œç”µè§†ï¼Œå¾ˆå¯èƒ½å…¶ä¸­æŸç§å½¢å¼çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯èµ·åˆ°äº†é‡è¦çš„ä½œç”¨ã€‚æƒ³è¦ä»æ™ºèƒ½æ‰‹æœºæ‹æ‘„çš„ç…§ç‰‡ä¸­å»é™¤èƒŒæ™¯ç‰©ä½“ï¼Ÿè¿™æ˜¯ä¸€ä¸ªå…¨æ™¯åˆ†å‰²ä»»åŠ¡çš„ç¤ºä¾‹ï¼ˆå¦‚æœæ‚¨è¿˜ä¸çŸ¥é“è¿™æ˜¯ä»€ä¹ˆæ„æ€ï¼Œæˆ‘ä»¬å°†åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­è¿›è¡Œæè¿°ï¼ï¼‰ã€‚

æœ¬é¡µé¢æä¾›äº†ä½¿ç”¨ ğŸ¤— Transformers åº“åœ¨åªéœ€ä¸‰è¡Œä»£ç çš„æƒ…å†µä¸‹è§£å†³ä¸åŒè¯­éŸ³å’ŒéŸ³é¢‘ã€è®¡ç®—æœºè§†è§‰ä»¥åŠ NLP ä»»åŠ¡çš„æ¦‚è¿°ï¼



## éŸ³é¢‘

éŸ³é¢‘å’Œè¯­éŸ³å¤„ç†ä»»åŠ¡ä¸å…¶ä»–æ¨¡æ€æœ‰äº›ä¸åŒï¼Œä¸»è¦æ˜¯å› ä¸ºéŸ³é¢‘ä½œä¸ºè¾“å…¥æ˜¯ä¸€ä¸ªè¿ç»­ä¿¡å·ã€‚ä¸æ–‡æœ¬ä¸åŒï¼ŒåŸå§‹éŸ³é¢‘æ³¢å½¢æ— æ³•åƒå¥å­å¯ä»¥è¢«åˆ’åˆ†ä¸ºå•è¯é‚£æ ·è¢«æ•´é½åœ°åˆ†å‰²ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé€šå¸¸ä¼šä»¥å›ºå®šçš„é—´éš”å¯¹åŸå§‹éŸ³é¢‘ä¿¡å·è¿›è¡Œé‡‡æ ·ã€‚å¦‚æœåœ¨ä¸€ä¸ªé—´éš”å†…é‡‡æ ·æ›´å¤šçš„æ ·æœ¬ï¼Œé‡‡æ ·ç‡å°±æ›´é«˜ï¼ŒéŸ³é¢‘å°±æ›´æ¥è¿‘åŸå§‹éŸ³é¢‘æºã€‚

ä»¥å¾€çš„æ–¹æ³•æ˜¯å¯¹éŸ³é¢‘è¿›è¡Œé¢„å¤„ç†ï¼Œä»ä¸­æå–æœ‰ç”¨çš„ç‰¹å¾ã€‚ç°åœ¨æ›´å¸¸è§çš„åšæ³•æ˜¯ç›´æ¥å°†åŸå§‹éŸ³é¢‘æ³¢å½¢è¾“å…¥åˆ°ç‰¹å¾ç¼–ç å™¨ä¸­ï¼Œä»¥æå–éŸ³é¢‘è¡¨ç¤ºã€‚è¿™ç®€åŒ–äº†é¢„å¤„ç†æ­¥éª¤ï¼Œå¹¶å…è®¸æ¨¡å‹å­¦ä¹ æœ€é‡è¦çš„ç‰¹å¾ã€‚

### éŸ³é¢‘åˆ†ç±»
éŸ³é¢‘åˆ†ç±»æ˜¯å°†éŸ³é¢‘æ•°æ®æ ‡è®°ä¸ºé¢„å®šä¹‰ç±»åˆ«çš„ä»»åŠ¡ã€‚å®ƒæ˜¯ä¸€ä¸ªå¹¿æ³›çš„ç±»åˆ«ï¼Œæœ‰è®¸å¤šå…·ä½“çš„åº”ç”¨ï¼Œå…¶ä¸­ä¸€äº›åŒ…æ‹¬ï¼š

* å£°å­¦åœºæ™¯åˆ†ç±»ï¼šä¸ºéŸ³é¢‘æ ‡è®°åœºæ™¯æ ‡ç­¾ï¼ˆ"åŠå…¬å®¤"ï¼Œ"æµ·æ»©"ï¼Œ"ä½“è‚²åœº"ï¼‰* å£°å­¦äº‹ä»¶æ£€æµ‹ï¼šä¸ºéŸ³é¢‘æ ‡è®°å£°éŸ³äº‹ä»¶æ ‡ç­¾ï¼ˆ"æ±½è½¦å–‡å­"ï¼Œ"é²¸é±¼å‘¼å«"ï¼Œ"ç»ç’ƒç ´ç¢"ï¼‰
* æ ‡ç­¾åŒ–ï¼šä¸ºåŒ…å«å¤šä¸ªå£°éŸ³çš„éŸ³é¢‘æ ‡è®°æ ‡ç­¾ï¼ˆé¸Ÿé¸£å£°ï¼Œä¼šè®®ä¸­çš„å‘è¨€äººè¯†åˆ«ï¼‰
* éŸ³ä¹åˆ†ç±»ï¼šä¸ºéŸ³ä¹æ ‡è®°æµæ´¾æ ‡ç­¾ï¼ˆ"é‡‘å±"ï¼Œ"å˜»å“ˆ"ï¼Œ"ä¹¡æ‘"ï¼‰

```py
>>> from transformers import pipeline

>>> classifier = pipeline(task = "audio-classification", model = "superb/hubert-base-superb-er")
>>> preds = classifier("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
>>> preds = [{"score": round(pred["score"], 4), "label": pred ["label"]} for pred in preds]
>>> preds
[{'score': 0.4532, 'label': 'hap'},
 {'score': 0.3622, 'label': 'sad'},
 {'score': 0.0943, 'label': 'neu'},
 {'score': 0.0903, 'label': 'ang'}]
```

### è‡ªåŠ¨è¯­éŸ³è¯†åˆ«

è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å°†è¯­éŸ³è½¬å½•ä¸ºæ–‡æœ¬ã€‚ç”±äºè¯­éŸ³æ˜¯äººç±»äº¤æµçš„ä¸€ç§è‡ªç„¶å½¢å¼ï¼Œå®ƒæ˜¯æœ€å¸¸è§çš„éŸ³é¢‘ä»»åŠ¡ä¹‹ä¸€ã€‚å¦‚ä»Šï¼ŒASR ç³»ç»ŸåµŒå…¥åœ¨åƒæ‰¬å£°å™¨ã€æ‰‹æœºå’Œæ±½è½¦ç­‰â€œæ™ºèƒ½â€æŠ€æœ¯äº§å“ä¸­ã€‚æˆ‘ä»¬å¯ä»¥è¦æ±‚è™šæ‹ŸåŠ©æ‰‹æ’­æ”¾éŸ³ä¹ã€è®¾ç½®æé†’å¹¶å‘Šè¯‰æˆ‘ä»¬å¤©æ°”ã€‚

ä½† Transformer æ¶æ„å¸®åŠ©è§£å†³çš„å…³é”®æŒ‘æˆ˜ä¹‹ä¸€æ˜¯ä½èµ„æºè¯­è¨€ã€‚é€šè¿‡åœ¨å¤§é‡è¯­éŸ³æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä»…åœ¨ä½èµ„æºè¯­è¨€ä¸­ä½¿ç”¨ä¸€å°æ—¶æ ‡è®°çš„è¯­éŸ³æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»ç„¶å¯ä»¥äº§ç”Ÿä¸ä¹‹å‰ä½¿ç”¨ 100 å€æ ‡è®°æ•°æ®è®­ç»ƒçš„ ASR ç³»ç»Ÿç›¸æ¯”çš„é«˜è´¨é‡ç»“æœã€‚

``` py
>>> from transformers import pipeline

>>> transcriber = pipeline(task = "automatic-speech-recognition", model = "openai/whisper-small")
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
```

## è®¡ç®—æœºè§†è§‰

è®¡ç®—æœºè§†è§‰çš„ä¸€ä¸ªæœ€æ—©æˆåŠŸçš„ä»»åŠ¡ä¹‹ä¸€æ˜¯ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰è¯†åˆ«é‚®æ”¿ç¼–ç å·ç çš„å›¾åƒã€‚å›¾åƒç”±åƒç´ ç»„æˆï¼Œæ¯ä¸ªåƒç´ éƒ½æœ‰ä¸€ä¸ªæ•°å€¼ã€‚è¿™ä½¿å¾—å°†å›¾åƒè¡¨ç¤ºä¸ºåƒç´ å€¼çŸ©é˜µå˜å¾—å®¹æ˜“ã€‚åƒç´ å€¼çš„ç‰¹å®šç»„åˆæè¿°äº†å›¾åƒçš„é¢œè‰²ã€‚

è§£å†³è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„ä¸¤ç§ä¸€èˆ¬æ–¹æ³•æ˜¯ï¼š

1. ä½¿ç”¨å·ç§¯æ¥ä»ä½çº§ç‰¹å¾åˆ°é«˜çº§æŠ½è±¡äº‹ç‰©å­¦ä¹ å›¾åƒçš„å±‚æ¬¡ç‰¹å¾ã€‚
2. å°†å›¾åƒåˆ†æˆè¡¥ä¸ï¼Œå¹¶ä½¿ç”¨ Transformer é€æ¸å­¦ä¹ æ¯ä¸ªå›¾åƒè¡¥ä¸ä¹‹é—´çš„å…³ç³»ä»¥å½¢æˆå›¾åƒã€‚ä¸ CNN åçˆ±çš„è‡ªä¸‹è€Œä¸Šæ–¹æ³•ä¸åŒï¼Œè¿™æœ‰ç‚¹åƒä»æ¨¡ç³Šçš„å›¾åƒå¼€å§‹ï¼Œç„¶åé€æ¸ä½¿å…¶å˜å¾—æ¸…æ™°ã€‚

### å›¾åƒåˆ†ç±»

å›¾åƒåˆ†ç±»å°†æ•´ä¸ªå›¾åƒæ ‡è®°ä¸ºé¢„å®šä¹‰çš„ç±»åˆ«é›†ã€‚ä¸å¤§å¤šæ•°åˆ†ç±»ä»»åŠ¡ä¸€æ ·ï¼Œå›¾åƒåˆ†ç±»æœ‰è®¸å¤šå®é™…åº”ç”¨åœºæ™¯ï¼Œå…¶ä¸­ä¸€äº›åŒ…æ‹¬ï¼š
* åŒ»ç–—ä¿å¥ï¼šæ ‡è®°åŒ»å­¦å›¾åƒä»¥æ£€æµ‹ç–¾ç—…æˆ–ç›‘æµ‹æ‚£è€…å¥åº·
* ç¯å¢ƒï¼šæ ‡è®°å«æ˜Ÿå›¾åƒä»¥ç›‘æµ‹æ£®æ—ç ä¼ã€æŒ‡å¯¼é‡ç”ŸåŠ¨æ¤ç‰©ç®¡ç†æˆ–æ£€æµ‹é‡ç«
* å†œä¸šï¼šæ ‡è®°å†œä½œç‰©å›¾åƒä»¥ç›‘æµ‹æ¤ç‰©å¥åº·æˆ–ç”¨äºåœŸåœ°åˆ©ç”¨ç›‘æµ‹çš„å«æ˜Ÿå›¾åƒ
* ç”Ÿæ€å­¦ï¼šæ ‡è®°åŠ¨ç‰©æˆ–æ¤ç‰©ç‰©ç§å›¾åƒä»¥ç›‘æµ‹é‡ç”ŸåŠ¨ç‰©ç§ç¾¤æˆ–è·Ÿè¸ªæ¿’å±ç‰©ç§

``` py
>>> from transformers import pipeline

>>> classifier = pipeline(task = "image-classification")
>>> preds = classifier(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred ["label"]} for pred in preds]
>>> print(*preds, sep = "\n")
{'score': 0.4335, 'label': 'lynx, catamount'}
{'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}
{'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}
{'score': 0.0239, 'label': 'Egyptian cat'}
{'score': 0.0229, 'label': 'tiger cat'}
```

### ç‰©ä½“æ£€æµ‹

ä¸å›¾åƒåˆ†ç±»ä¸åŒï¼Œç‰©ä½“æ£€æµ‹å¯åœ¨å›¾åƒä¸­è¯†åˆ«å¤šä¸ªå¯¹è±¡åŠå…¶ä½ç½®ï¼ˆç”±è¾¹ç•Œæ¡†å®šä¹‰ï¼‰ã€‚ç‰©ä½“æ£€æµ‹çš„ä¸€äº›ç¤ºä¾‹åº”ç”¨åŒ…æ‹¬ï¼š
* è‡ªåŠ¨é©¾é©¶è½¦è¾†ï¼šæ£€æµ‹å…¶ä»–è½¦è¾†ã€è¡Œäººå’Œäº¤é€šä¿¡å·ç­‰æ—¥å¸¸äº¤é€šå¯¹è±¡ * é¥æ„Ÿï¼šç¾éš¾ç›‘æµ‹ã€åŸå¸‚è§„åˆ’å’Œå¤©æ°”é¢„æŠ¥* ç¼ºé™·æ£€æµ‹ï¼šæ£€æµ‹å»ºç­‘ç‰©ä¸­çš„è£‚ç¼æˆ–ç»“æ„æŸä¼¤ä»¥åŠåˆ¶é€ ç¼ºé™·

``` py
>>> from transformers import pipeline

>>> detector = pipeline(task = "object-detection")
>>> preds = detector(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred ["label"], "box": pred ["box"]} for pred in preds]
>>> preds
[{'score': 0.9865,
  'label': 'cat',
  'box': {'xmin': 178, 'ymin': 154, 'xmax': 882, 'ymax': 598}}]
```

### å›¾åƒåˆ†å‰²

å›¾åƒåˆ†å‰²æ˜¯ä¸€ä¸ªåƒç´ çº§çš„ä»»åŠ¡ï¼Œå®ƒå°†å›¾åƒä¸­çš„æ¯ä¸ªåƒç´ åˆ†é…ç»™ä¸€ä¸ªç±»åˆ«ã€‚å®ƒä¸ä½¿ç”¨è¾¹ç•Œæ¡†æ ‡è®°å’Œé¢„æµ‹å›¾åƒä¸­çš„å¯¹è±¡çš„å¯¹è±¡æ£€æµ‹ä¸åŒï¼Œå› ä¸ºåˆ†å‰²æ›´åŠ ç²¾ç»†ã€‚åˆ†å‰²å¯ä»¥åœ¨åƒç´ çº§åˆ«æ£€æµ‹å¯¹è±¡ã€‚å›¾åƒåˆ†å‰²æœ‰å‡ ç§ç±»å‹ï¼š

* å®ä¾‹åˆ†å‰²ï¼šé™¤äº†æ ‡è®°å¯¹è±¡çš„ç±»åˆ«å¤–ï¼Œè¿˜æ ‡è®°æ¯ä¸ªä¸åŒå®ä¾‹çš„å¯¹è±¡ï¼ˆ"ç‹—-1"ï¼Œ"ç‹—-2"ï¼‰

* å…¨æ™¯åˆ†å‰²ï¼šè¯­ä¹‰åˆ†å‰²å’Œå®ä¾‹åˆ†å‰²çš„ç»„åˆï¼›å®ƒä¸ºæ¯ä¸ªåƒç´ æ ‡è®°ä¸€ä¸ªè¯­ä¹‰ç±»åˆ« **å’Œ** æ¯ä¸ªä¸åŒå¯¹è±¡çš„å®ä¾‹

åˆ†å‰²ä»»åŠ¡åœ¨è‡ªåŠ¨é©¾é©¶è½¦è¾†ä¸­éå¸¸æœ‰ç”¨ï¼Œå¯ä»¥åˆ›å»ºä¸€ä¸ªåƒç´ çº§åˆ«çš„ä¸–ç•Œåœ°å›¾ï¼Œä»¥ä¾¿å®ƒä»¬å¯ä»¥å®‰å…¨åœ°é¿å¼€è¡Œäººå’Œå…¶ä»–è½¦è¾†ã€‚åœ¨åŒ»å­¦å½±åƒå­¦ä¸­ä¹Ÿå¾ˆæœ‰ç”¨ï¼Œä»»åŠ¡çš„æ›´ç»†ç²’åº¦å¯ä»¥å¸®åŠ©è¯†åˆ«å¼‚å¸¸ç»†èƒæˆ–å™¨å®˜ç‰¹å¾ã€‚

å›¾åƒåˆ†å‰²è¿˜å¯ä»¥åœ¨ç”µå­å•†åŠ¡ä¸­ä½¿ç”¨ï¼Œé€šè¿‡è™šæ‹Ÿè¯•ç©¿è¡£æœæˆ–é€šè¿‡ç›¸æœºåœ¨çœŸå®ä¸–ç•Œä¸­å åŠ å¯¹è±¡æ¥åˆ›å»ºå¢å¼ºç°å®ä½“éªŒã€‚

``` py
>>> from transformers import pipeline

>>> segmenter = pipeline(task = "image-segmentation")
>>> preds = segmenter(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred ["label"]} for pred in preds]
>>> print(*preds, sep = "\n")
{'score': 0.9879, 'label': 'LABEL_184'}
{'score': 0.9973, 'label': 'snow'}
{'score': 0.9972, 'label': 'cat'}
```

### æ·±åº¦ä¼°è®¡

æ·±åº¦ä¼°è®¡é¢„æµ‹å›¾åƒä¸­æ¯ä¸ªåƒç´ ä¸ç›¸æœºçš„è·ç¦»ã€‚è¿™é¡¹è®¡ç®—æœºè§†è§‰ä»»åŠ¡å¯¹äºåœºæ™¯ç†è§£å’Œé‡å»ºå°¤ä¸ºé‡è¦ã€‚ä¾‹å¦‚ï¼Œåœ¨è‡ªåŠ¨é©¾é©¶æ±½è½¦ä¸­ï¼Œè½¦è¾†éœ€è¦äº†è§£è¡Œäººã€äº¤é€šæ ‡å¿—å’Œå…¶ä»–è½¦è¾†ç­‰ç‰©ä½“çš„è·ç¦»ï¼Œä»¥é¿å…éšœç¢å’Œç¢°æ’ã€‚æ·±åº¦ä¿¡æ¯è¿˜æœ‰åŠ©äºä» 2D å›¾åƒæ„å»º 3D è¡¨ç¤ºï¼Œå¹¶å¯ç”¨äºåˆ›å»ºç”Ÿç‰©ç»“æ„æˆ–å»ºç­‘ç‰©çš„é«˜è´¨é‡ 3D è¡¨ç¤ºã€‚
æ·±åº¦ä¼°è®¡æœ‰ä¸¤ç§æ–¹æ³•ï¼š

* ç«‹ä½“è§†è§‰ï¼šé€šè¿‡æ¯”è¾ƒç¨å¾®ä¸åŒè§’åº¦æ‹æ‘„çš„åŒä¸€å›¾åƒæ¥ä¼°è®¡æ·±åº¦* å•ç›®è§†è§‰ï¼šé€šè¿‡å•å¼ å›¾åƒä¼°è®¡æ·±åº¦

``` py
>>> from transformers import pipeline

>>> depth_estimator = pipeline(task = "depth-estimation")
>>> preds = depth_estimator(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
```

## è‡ªç„¶è¯­è¨€å¤„ç†

NLP ä»»åŠ¡æ˜¯æœ€å¸¸è§çš„ä»»åŠ¡ç±»å‹ä¹‹ä¸€ï¼Œå› ä¸ºå¯¹æˆ‘ä»¬æ¥è¯´ï¼Œæ–‡æœ¬æ˜¯ä¸€ç§è‡ªç„¶çš„äº¤æµæ–¹å¼ã€‚ä¸ºäº†ä½¿æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«æ–‡æœ¬ï¼Œéœ€è¦å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯ã€‚è¿™æ„å‘³ç€å°†ä¸€ç³»åˆ—æ–‡æœ¬åˆ’åˆ†ä¸ºå•ç‹¬çš„å•è¯æˆ–å­è¯ï¼ˆæ ‡è®°ï¼‰ï¼Œç„¶åå°†è¿™äº›æ ‡è®°è½¬æ¢ä¸ºæ•°å­—ã€‚å› æ­¤ï¼Œæ‚¨å¯ä»¥å°†ä¸€ç³»åˆ—æ–‡æœ¬è¡¨ç¤ºä¸ºä¸€ç³»åˆ—æ•°å­—ï¼Œä¸€æ—¦æœ‰äº†ä¸€ç³»åˆ—æ•°å­—ï¼Œå°±å¯ä»¥å°†å…¶è¾“å…¥æ¨¡å‹ä»¥è§£å†³å„ç§ NLP ä»»åŠ¡ï¼

### æ–‡æœ¬åˆ†ç±»
ä¸ä»»ä½•å…¶ä»–æ¨¡æ€çš„åˆ†ç±»ä»»åŠ¡ä¸€æ ·ï¼Œæ–‡æœ¬åˆ†ç±»å°†æ–‡æœ¬åºåˆ—ï¼ˆå¯ä»¥æ˜¯å¥å­çº§åˆ«ã€æ®µè½æˆ–æ–‡æ¡£ï¼‰æ ‡è®°ä¸ºé¢„å®šä¹‰çš„ç±»åˆ«é›†åˆã€‚æ–‡æœ¬åˆ†ç±»æœ‰è®¸å¤šå®é™…åº”ç”¨ï¼Œå…¶ä¸­ä¸€äº›åŒ…æ‹¬ï¼š

* æƒ…æ„Ÿåˆ†æï¼šæ ¹æ®æŸç§ææ€§ï¼ˆå¦‚â€œç§¯æâ€æˆ–â€œæ¶ˆæâ€ï¼‰æ ‡è®°æ–‡æœ¬ï¼Œå¯åœ¨æ”¿æ²»ã€é‡‘èå’Œå¸‚åœºè¥é”€ç­‰é¢†åŸŸæ”¯æŒå†³ç­–
* å†…å®¹åˆ†ç±»ï¼šæ ¹æ®æŸä¸ªä¸»é¢˜æ ‡è®°æ–‡æœ¬ï¼Œä»¥å¸®åŠ©ç»„ç»‡å’Œè¿‡æ»¤æ–°é—»å’Œç¤¾äº¤åª’ä½“ä¿¡æ¯ï¼ˆâ€œå¤©æ°”â€ã€â€œä½“è‚²â€ã€â€œé‡‘èâ€ç­‰ï¼‰

``` py
>>> from transformers import pipeline

>>> classifier = pipeline(task = "sentiment-analysis")
>>> preds = classifier("Hugging Face is the best thing since sliced bread!")
>>> preds = [{"score": round(pred["score"], 4), "label": pred ["label"]} for pred in preds]
>>> preds
[{'score': 0.9991, 'label': 'POSITIVE'}]
```

### æ ‡è®°åˆ†ç±»

åœ¨ä»»ä½• NLP ä»»åŠ¡ä¸­ï¼Œæ–‡æœ¬éƒ½ä¼šç»è¿‡é¢„å¤„ç†ï¼Œå°†æ–‡æœ¬åºåˆ—åˆ†éš”ä¸ºå•ä¸ªå•è¯æˆ–å­è¯ã€‚

è¿™äº›è¢«ç§°ä¸º [token](/glossary#token)ã€‚æ ‡è®°åˆ†ç±»ä¸ºæ¯ä¸ªæ ‡è®°åˆ†é…æ¥è‡ªé¢„å®šä¹‰çš„ç±»åˆ«é›†åˆçš„æ ‡ç­¾ã€‚

æ ‡è®°åˆ†ç±»çš„ä¸¤ç§å¸¸è§ç±»å‹æ˜¯ï¼š

* å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ï¼šæ ¹æ®å®ä½“ç±»åˆ«ï¼ˆå¦‚ç»„ç»‡ã€äººå‘˜ã€åœ°ç‚¹æˆ–æ—¥æœŸï¼‰æ ‡è®°æ ‡è®°ã€‚NER åœ¨ç”Ÿç‰©åŒ»å­¦ç¯å¢ƒä¸­ç‰¹åˆ«æµè¡Œï¼Œå¯ä»¥æ ‡è®°åŸºå› ã€è›‹ç™½è´¨å’Œè¯ç‰©åç§°ã€‚* è¯æ€§æ ‡æ³¨ï¼ˆPOSï¼‰ï¼šæ ¹æ®å…¶è¯æ€§ï¼ˆåè¯ã€åŠ¨è¯æˆ–å½¢å®¹è¯ï¼‰æ ‡è®°æ ‡è®°ã€‚POS ç”¨äºå¸®åŠ©ç¿»è¯‘ç³»ç»Ÿç†è§£ä¸¤ä¸ªç›¸åŒå•è¯åœ¨è¯­æ³•ä¸Šçš„åŒºåˆ«ï¼ˆä½œä¸ºåè¯çš„é“¶è¡Œä¸ä½œä¸ºåŠ¨è¯çš„é“¶è¡Œï¼‰ã€‚

``` py
>>> from transformers import pipeline

>>> classifier = pipeline(task = "ner")
>>> preds = classifier("Hugging Face is a French company based in New York City.")
>>> preds = [
...     {
...         "entity": pred ["entity"],
...         "score": round(pred ["score"], 4),
...         "index": pred ["index"],
...         "word": pred ["word"],
...         "start": pred ["start"],
...         "end": pred ["end"],
...     }
...     for pred in preds
... ]
>>> print(*preds, sep = "\n")
{'entity': 'I-ORG', 'score': 0.9968, 'index': 1, 'word': 'Hu', 'start': 0, 'end': 2}
{'entity': 'I-ORG', 'score': 0.9293, 'index': 2, 'word': '##gging', 'start': 2, 'end': 7}
{'entity': 'I-ORG', 'score': 0.9763, 'index': 3, 'word': 'Face', 'start': 8, 'end': 12}
{'entity': 'I-MISC', 'score': 0.9983, 'index': 6, 'word': 'French', 'start': 18, 'end': 24}
{'entity': 'I-LOC', 'score': 0.999, 'index': 10, 'word': 'New', 'start': 42, 'end': 45}
{'entity': 'I-LOC', 'score': 0.9987, 'index': 11, 'word': 'York', 'start': 46, 'end': 50}
{'entity': 'I-LOC', 'score': 0.9992, 'index': 12, 'word': 'City', 'start': 51, 'end': 55}
```

### é—®ç­”

é—®ç­”æ˜¯å¦ä¸€ç§åŸºäºæ ‡è®°çš„ä»»åŠ¡ï¼Œå®ƒå›ç­”é—®é¢˜ï¼Œæœ‰æ—¶æä¾›ä¸Šä¸‹æ–‡ï¼ˆå¼€æ”¾åŸŸï¼‰ï¼Œæœ‰æ—¶æ²¡æœ‰ä¸Šä¸‹æ–‡ï¼ˆå°é—­åŸŸï¼‰ã€‚å½“æˆ‘ä»¬å‘è™šæ‹ŸåŠ©æ‰‹æé—®é¤å…æ˜¯å¦è¥ä¸šæ—¶ï¼Œå°±ä¼šå‘ç”Ÿè¿™ä¸ªä»»åŠ¡ã€‚å®ƒè¿˜å¯ä»¥æä¾›å®¢æˆ·æˆ–æŠ€æœ¯æ”¯æŒï¼Œå¹¶å¸®åŠ©æœç´¢å¼•æ“æ£€ç´¢æ‚¨æ‰€è¦æ±‚çš„ç›¸å…³ä¿¡æ¯ã€‚

é—®ç­”æœ‰ä¸¤ç§å¸¸è§ç±»å‹ï¼š

* æŠ½å–å¼ï¼šç»™å®šä¸€ä¸ªé—®é¢˜å’Œä¸€äº›ä¸Šä¸‹æ–‡ï¼Œç­”æ¡ˆæ˜¯æ¨¡å‹ä»ä¸Šä¸‹æ–‡ä¸­æå–çš„ä¸€æ®µæ–‡æœ¬

* æŠ½è±¡å¼ï¼šç»™å®šä¸€ä¸ªé—®é¢˜å’Œä¸€äº›ä¸Šä¸‹æ–‡ï¼Œç­”æ¡ˆæ˜¯ä»ä¸Šä¸‹æ–‡ä¸­ç”Ÿæˆçš„ï¼›æ­¤æ–¹æ³•ç”± [`Text2TextGenerationPipeline`] å¤„ç†ï¼Œè€Œä¸æ˜¯ä¸‹é¢æ˜¾ç¤ºçš„ [`QuestionAnsweringPipeline`]


``` py
>>> from transformers import pipeline

>>> question_answerer = pipeline(task = "question-answering")
>>> preds = question_answerer(
...     question = "What is the name of the repository?",
...     context = "The name of the repository is huggingface/transformers",
... )
>>> print(
...     f "score: {round(preds ['score'], 4)}, start: {preds ['start']}, end: {preds ['end']}, answer: {preds ['answer']}"
... )
score: 0.9327, start: 30, end: 54, answer: huggingface/transformers
```

### æ‘˜è¦

æ‘˜è¦ä»è¾ƒé•¿çš„æ–‡æœ¬ä¸­åˆ›å»ºä¸€ä¸ªè¾ƒçŸ­çš„ç‰ˆæœ¬ï¼ŒåŒæ—¶å°½é‡ä¿ç•™åŸå§‹æ–‡æ¡£çš„å¤§éƒ¨åˆ†å«ä¹‰ã€‚æ‘˜è¦æ˜¯ä¸€ä¸ªåºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡ï¼›å®ƒè¾“å‡ºä¸€ä¸ªæ¯”è¾“å…¥æ›´çŸ­çš„æ–‡æœ¬åºåˆ—ã€‚æœ‰è®¸å¤šé•¿ç¯‡æ–‡æ¡£å¯ä»¥æ‘˜è¦ï¼Œä»¥å¸®åŠ©è¯»è€…å¿«é€Ÿäº†è§£ä¸»è¦è¦ç‚¹ã€‚ç«‹æ³•æ³•æ¡ˆã€æ³•å¾‹å’Œé‡‘èæ–‡ä»¶ã€ä¸“åˆ©å’Œç§‘å­¦è®ºæ–‡æ˜¯å¯ä»¥è¿›è¡Œæ‘˜è¦çš„ä¸€äº›ä¾‹å­ï¼Œä»¥èŠ‚çœè¯»è€…çš„æ—¶é—´å¹¶ä½œä¸ºé˜…è¯»è¾…åŠ©å·¥å…·ã€‚

ä¸é—®ç­”ç±»ä¼¼ï¼Œæ‘˜è¦æœ‰ä¸¤ç§ç±»å‹ï¼š
* æŠ½å–å¼ï¼šä»åŸå§‹æ–‡æœ¬ä¸­è¯†åˆ«å¹¶æå–æœ€é‡è¦çš„å¥å­
* æŠ½è±¡å¼ï¼šä»åŸå§‹æ–‡æœ¬ç”Ÿæˆç›®æ ‡æ‘˜è¦ï¼ˆå¯èƒ½åŒ…å«è¾“å…¥æ–‡æ¡£ä¸­æ²¡æœ‰çš„æ–°è¯ï¼‰ï¼›[`SummarizationPipeline`] ä½¿ç”¨æŠ½è±¡æ–¹æ³•

``` py
>>> from transformers import pipeline

>>> summarizer = pipeline(task = "summarization")
>>> summarizer(
...     "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles."
... )
[{'summary_text': ' The Transformer is the first sequence transduction model based entirely on attention . It replaces the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention . For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .'}]
```

### ç¿»è¯‘

ç¿»è¯‘å°†ä¸€ç§è¯­è¨€çš„æ–‡æœ¬åºåˆ—è½¬æ¢ä¸ºå¦ä¸€ç§è¯­è¨€ã€‚

å®ƒåœ¨å¸®åŠ©æ¥è‡ªä¸åŒèƒŒæ™¯çš„äººä»¬ç›¸äº’äº¤æµæ–¹é¢éå¸¸é‡è¦ï¼Œå¯ä»¥å¸®åŠ©ç¿»è¯‘å†…å®¹è§¦è¾¾æ›´å¹¿æ³›çš„å—ä¼—ï¼Œç”šè‡³å¯ä»¥ä½œä¸ºå­¦ä¹ å·¥å…·å¸®åŠ©äººä»¬å­¦ä¹ ä¸€é—¨æ–°è¯­è¨€ã€‚é™¤äº†æ‘˜è¦ä¹‹å¤–ï¼Œç¿»è¯‘ä¹Ÿæ˜¯ä¸€ä¸ªåºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡ï¼Œå³æ¨¡å‹æ¥æ”¶ä¸€ä¸ªè¾“å…¥åºåˆ—å¹¶è¿”å›ä¸€ä¸ªç›®æ ‡è¾“å‡ºåºåˆ—ã€‚

åœ¨æ—©æœŸï¼Œç¿»è¯‘æ¨¡å‹ä¸»è¦æ˜¯å•è¯­è¨€çš„ï¼Œä½†æœ€è¿‘å¯¹å¯ä»¥åœ¨è®¸å¤šè¯­è¨€å¯¹ä¹‹é—´è¿›è¡Œç¿»è¯‘çš„å¤šè¯­è¨€æ¨¡å‹è¶Šæ¥è¶Šæ„Ÿå…´è¶£ã€‚

``` py
>>> from transformers import pipeline

>>> text = "translate English to French: Hugging Face is a community-based open-source platform for machine learning."
>>> translator = pipeline(task = "translation", model = "t5-small")
>>> translator(text)
[{'translation_text': "Hugging Face est une tribune communautaire de l'apprentissage des machines."}]
```

### è¯­è¨€å»ºæ¨¡

è¯­è¨€å»ºæ¨¡æ˜¯ä¸€ç§é¢„æµ‹æ–‡æœ¬åºåˆ—ä¸­çš„å•è¯çš„ä»»åŠ¡ã€‚å®ƒå·²æˆä¸ºéå¸¸æµè¡Œçš„ NLP ä»»åŠ¡ï¼Œå› ä¸ºé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹å¯ä»¥ç”¨äºè®¸å¤šå…¶ä»–ä¸‹æ¸¸ä»»åŠ¡çš„å¾®è°ƒã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼•èµ·äº†å¾ˆå¤§çš„å…³æ³¨ï¼Œå®ƒä»¬å±•ç¤ºäº†é›¶æˆ–å°‘è®¸æ ·æœ¬å­¦ä¹ çš„èƒ½åŠ›ã€‚è¿™æ„å‘³ç€æ¨¡å‹å¯ä»¥è§£å†³å…¶æœªç»æ˜ç¡®è®­ç»ƒçš„ä»»åŠ¡ï¼è¯­è¨€æ¨¡å‹å¯ä»¥ç”¨äºç”Ÿæˆæµç•…è€Œä»¤äººä¿¡æœçš„æ–‡æœ¬ï¼Œä½†éœ€è¦å°å¿ƒï¼Œå› ä¸ºæ–‡æœ¬å¯èƒ½ä¸æ€»æ˜¯å‡†ç¡®çš„ã€‚

è¯­è¨€å»ºæ¨¡æœ‰ä¸¤ç§ç±»å‹ï¼š

* å› æœï¼šæ¨¡å‹çš„ç›®æ ‡æ˜¯é¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªæ ‡è®°ï¼Œæœªæ¥æ ‡è®°è¢«æ©ç›–
    ```py
    >>> from transformers import pipeline

    >>> prompt = "Hugging Face is a community-based open-source platform for machine learning."
    >>> generator = pipeline(task="text-generation")
    >>> generator(prompt)  # doctest: +SKIP
    ```
* maskedï¼šæ¨¡å‹çš„ç›®æ ‡æ˜¯åœ¨å®Œå…¨è®¿é—®åºåˆ—ä¸­çš„æ ‡è®°æ—¶é¢„æµ‹è¢«æ©ç›–çš„æ ‡è®°ã€‚
    
    ```py
    >>> text = "Hugging Face is a community-based open-source <mask> for machine learning."
    >>> fill_mask = pipeline(task="fill-mask")
    >>> preds = fill_mask(text, top_k=1)
    >>> preds = [
    ...     {
    ...         "score": round(pred["score"], 4),
    ...         "token": pred["token"],
    ...         "token_str": pred["token_str"],
    ...         "sequence": pred["sequence"],
    ...     }
    ...     for pred in preds
    ... ]
    >>> preds
    [{'score': 0.2236,
      'token': 1761,
      'token_str': ' platform',
      'sequence': 'Hugging Face is a community-based open-source platform for machine learning.'}]
  ```

å¸Œæœ›æœ¬é¡µé¢ä¸ºæ‚¨æä¾›äº†å…³äºæ¯ç§æ¨¡æ€ä»»åŠ¡ç±»å‹åŠå…¶å®é™…é‡è¦æ€§çš„æ›´å¤šèƒŒæ™¯ä¿¡æ¯ã€‚åœ¨æ¥ä¸‹æ¥çš„ [éƒ¨åˆ†](tasks_explained) ä¸­ï¼Œæ‚¨å°†äº†è§£ğŸ¤— Transformers å¦‚ä½•è§£å†³è¿™äº›ä»»åŠ¡ã€‚