<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# ğŸ¤— Transformers èƒ½åšä»€ä¹ˆ

ğŸ¤— Transformersæ˜¯ä¸€ä¸ªç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ã€è®¡ç®—æœºè§†è§‰å’ŒéŸ³é¢‘å’Œè¯­éŸ³å¤„ç†ä»»åŠ¡çš„é¢„è®­ç»ƒæ¨¡å‹åº“ã€‚è¯¥åº“ä¸ä»…åŒ…å«Transformeræ¨¡å‹ï¼Œè¿˜åŒ…æ‹¬ç”¨äºè®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„ç°ä»£å·ç§¯ç½‘ç»œç­‰éTransformeræ¨¡å‹ã€‚å¦‚æœæ‚¨çœ‹çœ‹ä»Šå¤©æœ€å—æ¬¢è¿çš„ä¸€äº›æ¶ˆè´¹äº§å“ï¼Œæ¯”å¦‚æ™ºèƒ½æ‰‹æœºã€åº”ç”¨ç¨‹åºå’Œç”µè§†ï¼Œå¾ˆå¯èƒ½èƒŒåéƒ½æœ‰æŸç§æ·±åº¦å­¦ä¹ æŠ€æœ¯çš„æ”¯æŒã€‚æƒ³è¦ä»æ‚¨æ™ºèƒ½æ‰‹æœºæ‹æ‘„çš„ç…§ç‰‡ä¸­åˆ é™¤èƒŒæ™¯å¯¹è±¡å—ï¼Ÿè¿™é‡Œæ˜¯ä¸€ä¸ªå…¨æ™¯åˆ†å‰²ä»»åŠ¡çš„ä¾‹å­ï¼ˆå¦‚æœæ‚¨è¿˜ä¸äº†è§£è¿™æ˜¯ä»€ä¹ˆæ„æ€ï¼Œæˆ‘ä»¬å°†åœ¨ä»¥ä¸‹éƒ¨åˆ†è¿›è¡Œæè¿°ï¼ï¼‰ã€‚

æœ¬é¡µé¢æä¾›äº†ä½¿ç”¨ğŸ¤— Transformersåº“ä»…ç”¨ä¸‰è¡Œä»£ç è§£å†³ä¸åŒçš„è¯­éŸ³å’ŒéŸ³é¢‘ã€è®¡ç®—æœºè§†è§‰å’ŒNLPä»»åŠ¡çš„æ¦‚è¿°ï¼


## éŸ³é¢‘
éŸ³é¢‘å’Œè¯­éŸ³å¤„ç†ä»»åŠ¡ä¸å…¶ä»–æ¨¡æ€ç•¥æœ‰ä¸åŒï¼Œä¸»è¦æ˜¯å› ä¸ºéŸ³é¢‘ä½œä¸ºè¾“å…¥æ˜¯ä¸€ä¸ªè¿ç»­çš„ä¿¡å·ã€‚ä¸æ–‡æœ¬ä¸åŒï¼ŒåŸå§‹éŸ³é¢‘æ³¢å½¢ä¸èƒ½åƒå¥å­å¯ä»¥è¢«åˆ’åˆ†ä¸ºå•è¯é‚£æ ·è¢«æ•´é½åœ°åˆ†å‰²æˆç¦»æ•£çš„å—ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé€šå¸¸åœ¨å›ºå®šçš„æ—¶é—´é—´éš”å†…å¯¹åŸå§‹éŸ³é¢‘ä¿¡å·è¿›è¡Œé‡‡æ ·ã€‚å¦‚æœåœ¨æ¯ä¸ªæ—¶é—´é—´éš”å†…é‡‡æ ·æ›´å¤šæ ·æœ¬ï¼Œé‡‡æ ·ç‡å°±ä¼šæ›´é«˜ï¼ŒéŸ³é¢‘æ›´æ¥è¿‘åŸå§‹éŸ³é¢‘æºã€‚

ä»¥å‰çš„æ–¹æ³•æ˜¯é¢„å¤„ç†éŸ³é¢‘ä»¥ä»ä¸­æå–æœ‰ç”¨çš„ç‰¹å¾ã€‚ç°åœ¨æ›´å¸¸è§çš„åšæ³•æ˜¯ç›´æ¥å°†åŸå§‹éŸ³é¢‘æ³¢å½¢è¾“å…¥åˆ°ç‰¹å¾ç¼–ç å™¨ä¸­ï¼Œä»¥æå–éŸ³é¢‘è¡¨ç¤ºã€‚è¿™æ ·å¯ä»¥ç®€åŒ–é¢„å¤„ç†æ­¥éª¤ï¼Œå¹¶å…è®¸æ¨¡å‹å­¦ä¹ æœ€é‡è¦çš„ç‰¹å¾ã€‚

### éŸ³é¢‘åˆ†ç±»

éŸ³é¢‘åˆ†ç±»æ˜¯ä¸€é¡¹å°†éŸ³é¢‘æ•°æ®ä»é¢„å®šä¹‰çš„ç±»åˆ«é›†åˆä¸­è¿›è¡Œæ ‡è®°çš„ä»»åŠ¡ã€‚è¿™æ˜¯ä¸€ä¸ªå¹¿æ³›çš„ç±»åˆ«ï¼Œå…·æœ‰è®¸å¤šå…·ä½“çš„åº”ç”¨ï¼Œå…¶ä¸­ä¸€äº›åŒ…æ‹¬ï¼š

* å£°å­¦åœºæ™¯åˆ†ç±»ï¼šä½¿ç”¨åœºæ™¯æ ‡ç­¾ï¼ˆ"åŠå…¬å®¤"ã€"æµ·æ»©"ã€"ä½“è‚²åœº"ï¼‰å¯¹éŸ³é¢‘è¿›è¡Œæ ‡è®°ã€‚
* å£°å­¦äº‹ä»¶æ£€æµ‹ï¼šä½¿ç”¨å£°éŸ³äº‹ä»¶æ ‡ç­¾ï¼ˆ"æ±½è½¦å–‡å­å£°"ã€"é²¸é±¼å«å£°"ã€"ç»ç’ƒç ´ç¢å£°"ï¼‰å¯¹éŸ³é¢‘è¿›è¡Œæ ‡è®°ã€‚
* æ ‡è®°ï¼šå¯¹åŒ…å«å¤šç§å£°éŸ³çš„éŸ³é¢‘è¿›è¡Œæ ‡è®°ï¼ˆé¸Ÿé¸£ã€ä¼šè®®ä¸­çš„è¯´è¯äººè¯†åˆ«ï¼‰ã€‚
* éŸ³ä¹åˆ†ç±»ï¼šä½¿ç”¨æµæ´¾æ ‡ç­¾ï¼ˆ"é‡‘å±"ã€"å˜»å“ˆ"ã€"ä¹¡æ‘"ï¼‰å¯¹éŸ³ä¹è¿›è¡Œæ ‡è®°ã€‚

```py
>>> from transformers import pipeline

>>> classifier = pipeline(task="audio-classification", model="superb/hubert-base-superb-er")
>>> preds = classifier("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> preds
[{'score': 0.4532, 'label': 'hap'},
 {'score': 0.3622, 'label': 'sad'},
 {'score': 0.0943, 'label': 'neu'},
 {'score': 0.0903, 'label': 'ang'}]
```

### è‡ªåŠ¨è¯­éŸ³è¯†åˆ«

è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å°†è¯­éŸ³è½¬å½•ä¸ºæ–‡æœ¬ã€‚è¿™æ˜¯æœ€å¸¸è§çš„éŸ³é¢‘ä»»åŠ¡ä¹‹ä¸€ï¼Œéƒ¨åˆ†åŸå› æ˜¯å› ä¸ºè¯­éŸ³æ˜¯äººç±»äº¤æµçš„è‡ªç„¶å½¢å¼ã€‚å¦‚ä»Šï¼ŒASRç³»ç»ŸåµŒå…¥åœ¨æ™ºèƒ½æŠ€æœ¯äº§å“ä¸­ï¼Œå¦‚æ‰¬å£°å™¨ã€ç”µè¯å’Œæ±½è½¦ã€‚æˆ‘ä»¬å¯ä»¥è¦æ±‚è™šæ‹ŸåŠ©æ‰‹æ’­æ”¾éŸ³ä¹ã€è®¾ç½®æé†’å’Œå‘Šè¯‰æˆ‘ä»¬å¤©æ°”ã€‚

ä½†æ˜¯ï¼ŒTransformeræ¶æ„å¸®åŠ©è§£å†³çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯ä½èµ„æºè¯­è¨€ã€‚é€šè¿‡åœ¨å¤§é‡è¯­éŸ³æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä»…åœ¨ä¸€ä¸ªä½èµ„æºè¯­è¨€çš„ä¸€å°æ—¶æ ‡è®°è¯­éŸ³æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä»ç„¶å¯ä»¥äº§ç”Ÿä¸ä»¥å‰åœ¨100å€æ›´å¤šæ ‡è®°æ•°æ®ä¸Šè®­ç»ƒçš„ASRç³»ç»Ÿç›¸æ¯”é«˜è´¨é‡çš„ç»“æœã€‚

```py
>>> from transformers import pipeline

>>> transcriber = pipeline(task="automatic-speech-recognition", model="openai/whisper-small")
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
```

## è®¡ç®—æœºè§†è§‰

è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­æœ€æ—©æˆåŠŸä¹‹ä¸€æ˜¯ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆ[CNN](glossary#convolution)ï¼‰è¯†åˆ«é‚®æ”¿ç¼–ç æ•°å­—å›¾åƒã€‚å›¾åƒç”±åƒç´ ç»„æˆï¼Œæ¯ä¸ªåƒç´ éƒ½æœ‰ä¸€ä¸ªæ•°å€¼ã€‚è¿™ä½¿å¾—å°†å›¾åƒè¡¨ç¤ºä¸ºåƒç´ å€¼çŸ©é˜µå˜å¾—å®¹æ˜“ã€‚æ¯ä¸ªåƒç´ å€¼ç»„åˆæè¿°äº†å›¾åƒçš„é¢œè‰²ã€‚

è®¡ç®—æœºè§†è§‰ä»»åŠ¡å¯ä»¥é€šè¿‡ä»¥ä¸‹ä¸¤ç§é€šç”¨æ–¹å¼è§£å†³ï¼š

1. ä½¿ç”¨å·ç§¯æ¥å­¦ä¹ å›¾åƒçš„å±‚æ¬¡ç‰¹å¾ï¼Œä»ä½çº§ç‰¹å¾åˆ°é«˜çº§æŠ½è±¡ç‰¹å¾ã€‚
2. å°†å›¾åƒåˆ†æˆå—ï¼Œå¹¶ä½¿ç”¨Transformeré€æ­¥å­¦ä¹ æ¯ä¸ªå›¾åƒå—å¦‚ä½•ç›¸äº’å…³è”ä»¥å½¢æˆå›¾åƒã€‚ä¸CNNåå¥½çš„è‡ªåº•å‘ä¸Šæ–¹æ³•ä¸åŒï¼Œè¿™ç§æ–¹æ³•æœ‰ç‚¹åƒä»ä¸€ä¸ªæ¨¡ç³Šçš„å›¾åƒå¼€å§‹ï¼Œç„¶åé€æ¸å°†å…¶èšç„¦æ¸…æ™°ã€‚

### å›¾åƒåˆ†ç±»

å›¾åƒåˆ†ç±»å°†æ•´ä¸ªå›¾åƒä»é¢„å®šä¹‰çš„ç±»åˆ«é›†åˆä¸­è¿›è¡Œæ ‡è®°ã€‚åƒå¤§å¤šæ•°åˆ†ç±»ä»»åŠ¡ä¸€æ ·ï¼Œå›¾åƒåˆ†ç±»æœ‰è®¸å¤šå®é™…ç”¨ä¾‹ï¼Œå…¶ä¸­ä¸€äº›åŒ…æ‹¬ï¼š

* åŒ»ç–—ä¿å¥ï¼šæ ‡è®°åŒ»å­¦å›¾åƒä»¥æ£€æµ‹ç–¾ç—…æˆ–ç›‘æµ‹æ‚£è€…å¥åº·çŠ¶å†µ
* ç¯å¢ƒï¼šæ ‡è®°å«æ˜Ÿå›¾åƒä»¥ç›‘æµ‹æ£®æ—ç ä¼ã€æä¾›é‡å¤–ç®¡ç†ä¿¡æ¯æˆ–æ£€æµ‹é‡ç«
* å†œä¸šï¼šæ ‡è®°å†œä½œç‰©å›¾åƒä»¥ç›‘æµ‹æ¤ç‰©å¥åº·æˆ–ç”¨äºåœŸåœ°ä½¿ç”¨ç›‘æµ‹çš„å«æ˜Ÿå›¾åƒ
* ç”Ÿæ€å­¦ï¼šæ ‡è®°åŠ¨ç‰©æˆ–æ¤ç‰©ç‰©ç§çš„å›¾åƒä»¥ç›‘æµ‹é‡ç”ŸåŠ¨ç‰©ç§ç¾¤æˆ–è·Ÿè¸ªæ¿’å±ç‰©ç§

```py
>>> from transformers import pipeline

>>> classifier = pipeline(task="image-classification")
>>> preds = classifier(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> print(*preds, sep="\n")
{'score': 0.4335, 'label': 'lynx, catamount'}
{'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}
{'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}
{'score': 0.0239, 'label': 'Egyptian cat'}
{'score': 0.0229, 'label': 'tiger cat'}
```

### ç›®æ ‡æ£€æµ‹

ä¸å›¾åƒåˆ†ç±»ä¸åŒï¼Œç›®æ ‡æ£€æµ‹åœ¨å›¾åƒä¸­è¯†åˆ«å¤šä¸ªå¯¹è±¡ä»¥åŠè¿™äº›å¯¹è±¡åœ¨å›¾åƒä¸­çš„ä½ç½®ï¼ˆç”±è¾¹ç•Œæ¡†å®šä¹‰ï¼‰ã€‚ç›®æ ‡æ£€æµ‹çš„ä¸€äº›ç¤ºä¾‹åº”ç”¨åŒ…æ‹¬ï¼š

* è‡ªåŠ¨é©¾é©¶è½¦è¾†ï¼šæ£€æµ‹æ—¥å¸¸äº¤é€šå¯¹è±¡ï¼Œå¦‚å…¶ä»–è½¦è¾†ã€è¡Œäººå’Œçº¢ç»¿ç¯
* é¥æ„Ÿï¼šç¾å®³ç›‘æµ‹ã€åŸå¸‚è§„åˆ’å’Œå¤©æ°”é¢„æŠ¥
* ç¼ºé™·æ£€æµ‹ï¼šæ£€æµ‹å»ºç­‘ç‰©ä¸­çš„è£‚ç¼æˆ–ç»“æ„æŸåï¼Œä»¥åŠåˆ¶é€ ä¸šäº§å“ç¼ºé™·


```py
>>> from transformers import pipeline

>>> detector = pipeline(task="object-detection")
>>> preds = detector(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"], "box": pred["box"]} for pred in preds]
>>> preds
[{'score': 0.9865,
  'label': 'cat',
  'box': {'xmin': 178, 'ymin': 154, 'xmax': 882, 'ymax': 598}}]
```

### å›¾åƒåˆ†å‰²

å›¾åƒåˆ†å‰²æ˜¯ä¸€é¡¹åƒç´ çº§ä»»åŠ¡ï¼Œå°†å›¾åƒä¸­çš„æ¯ä¸ªåƒç´ åˆ†é…ç»™ä¸€ä¸ªç±»åˆ«ã€‚å®ƒä¸ä½¿ç”¨è¾¹ç•Œæ¡†æ ‡è®°å’Œé¢„æµ‹å›¾åƒä¸­çš„å¯¹è±¡çš„ç›®æ ‡æ£€æµ‹ä¸åŒï¼Œå› ä¸ºåˆ†å‰²æ›´åŠ ç²¾ç»†ã€‚åˆ†å‰²å¯ä»¥åœ¨åƒç´ çº§åˆ«æ£€æµ‹å¯¹è±¡ã€‚æœ‰å‡ ç§ç±»å‹çš„å›¾åƒåˆ†å‰²ï¼š

* å®ä¾‹åˆ†å‰²ï¼šé™¤äº†æ ‡è®°å¯¹è±¡çš„ç±»åˆ«å¤–ï¼Œè¿˜æ ‡è®°æ¯ä¸ªå¯¹è±¡çš„ä¸åŒå®ä¾‹ï¼ˆâ€œdog-1â€ï¼Œâ€œdog-2â€ï¼‰
* å…¨æ™¯åˆ†å‰²ï¼šè¯­ä¹‰åˆ†å‰²å’Œå®ä¾‹åˆ†å‰²çš„ç»„åˆï¼› å®ƒä½¿ç”¨è¯­ä¹‰ç±»ä¸ºæ¯ä¸ªåƒç´ æ ‡è®°å¹¶æ ‡è®°æ¯ä¸ªå¯¹è±¡çš„ä¸åŒå®ä¾‹

åˆ†å‰²ä»»åŠ¡å¯¹äºè‡ªåŠ¨é©¾é©¶è½¦è¾†å¾ˆæœ‰å¸®åŠ©ï¼Œå¯ä»¥åˆ›å»ºå‘¨å›´ä¸–ç•Œçš„åƒç´ çº§åœ°å›¾ï¼Œä»¥ä¾¿å®ƒä»¬å¯ä»¥åœ¨è¡Œäººå’Œå…¶ä»–è½¦è¾†å‘¨å›´å®‰å…¨å¯¼èˆªã€‚å®ƒè¿˜é€‚ç”¨äºåŒ»å­¦æˆåƒï¼Œå…¶ä¸­ä»»åŠ¡çš„æ›´ç²¾ç»†ç²’åº¦å¯ä»¥å¸®åŠ©è¯†åˆ«å¼‚å¸¸ç»†èƒæˆ–å™¨å®˜ç‰¹å¾ã€‚å›¾åƒåˆ†å‰²ä¹Ÿå¯ä»¥ç”¨äºç”µå­å•†åŠ¡ï¼Œé€šè¿‡æ‚¨çš„ç›¸æœºåœ¨ç°å®ä¸–ç•Œä¸­è¦†ç›–ç‰©ä½“æ¥è™šæ‹Ÿè¯•ç©¿è¡£æœæˆ–åˆ›å»ºå¢å¼ºç°å®ä½“éªŒã€‚

```py
>>> from transformers import pipeline

>>> segmenter = pipeline(task="image-segmentation")
>>> preds = segmenter(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> print(*preds, sep="\n")
{'score': 0.9879, 'label': 'LABEL_184'}
{'score': 0.9973, 'label': 'snow'}
{'score': 0.9972, 'label': 'cat'}
```

### æ·±åº¦ä¼°è®¡

æ·±åº¦ä¼°è®¡é¢„æµ‹å›¾åƒä¸­æ¯ä¸ªåƒç´ åˆ°ç›¸æœºçš„è·ç¦»ã€‚è¿™ä¸ªè®¡ç®—æœºè§†è§‰ä»»åŠ¡å¯¹äºåœºæ™¯ç†è§£å’Œé‡å»ºå°¤ä¸ºé‡è¦ã€‚ä¾‹å¦‚ï¼Œåœ¨è‡ªåŠ¨é©¾é©¶æ±½è½¦ä¸­ï¼Œè½¦è¾†éœ€è¦äº†è§£è¡Œäººã€äº¤é€šæ ‡å¿—å’Œå…¶ä»–è½¦è¾†ç­‰ç‰©ä½“çš„è·ç¦»ï¼Œä»¥é¿å…éšœç¢ç‰©å’Œç¢°æ’ã€‚æ·±åº¦ä¿¡æ¯è¿˜æœ‰åŠ©äºä»2Då›¾åƒæ„å»º3Dè¡¨ç¤ºï¼Œå¹¶å¯ç”¨äºåˆ›å»ºç”Ÿç‰©ç»“æ„æˆ–å»ºç­‘ç‰©çš„é«˜è´¨é‡3Dè¡¨ç¤ºã€‚

æœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥è¿›è¡Œæ·±åº¦ä¼°è®¡ï¼š

* stereoï¼ˆç«‹ä½“ï¼‰ï¼šé€šè¿‡æ¯”è¾ƒåŒä¸€å›¾åƒçš„ä¸¤ä¸ªç•¥å¾®ä¸åŒè§’åº¦çš„å›¾åƒæ¥ä¼°è®¡æ·±åº¦
* monocularï¼ˆå•ç›®ï¼‰ï¼šä»å•ä¸ªå›¾åƒä¸­ä¼°è®¡æ·±åº¦


```py
>>> from transformers import pipeline

>>> depth_estimator = pipeline(task="depth-estimation")
>>> preds = depth_estimator(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
```

## è‡ªç„¶è¯­è¨€å¤„ç†

NLPä»»åŠ¡æ˜¯æœ€å¸¸è§çš„ç±»å‹ä¹‹ä¸€ï¼Œå› ä¸ºæ–‡æœ¬æ˜¯æˆ‘ä»¬è¿›è¡Œäº¤æµçš„è‡ªç„¶æ–¹å¼ã€‚ä¸ºäº†è®©æ–‡æœ¬å˜æˆæ¨¡å‹è¯†åˆ«çš„æ ¼å¼ï¼Œéœ€è¦å¯¹å…¶è¿›è¡Œåˆ†è¯ã€‚è¿™æ„å‘³ç€å°†ä¸€æ®µæ–‡æœ¬åˆ†æˆå•ç‹¬çš„å•è¯æˆ–å­è¯ï¼ˆ`tokens`ï¼‰ï¼Œç„¶åå°†è¿™äº›`tokens`è½¬æ¢ä¸ºæ•°å­—ã€‚å› æ­¤ï¼Œå¯ä»¥å°†ä¸€æ®µæ–‡æœ¬è¡¨ç¤ºä¸ºä¸€ç³»åˆ—æ•°å­—ï¼Œä¸€æ—¦æœ‰äº†ä¸€ç³»åˆ—çš„æ•°å­—ï¼Œå°±å¯ä»¥å°†å…¶è¾“å…¥åˆ°æ¨¡å‹ä¸­ä»¥è§£å†³å„ç§NLPä»»åŠ¡ï¼

### æ–‡æœ¬åˆ†ç±»

åƒä»»ä½•æ¨¡æ€çš„åˆ†ç±»ä»»åŠ¡ä¸€æ ·ï¼Œæ–‡æœ¬åˆ†ç±»å°†ä¸€æ®µæ–‡æœ¬ï¼ˆå¯ä»¥æ˜¯å¥å­çº§åˆ«ã€æ®µè½æˆ–æ–‡æ¡£ï¼‰ä»é¢„å®šä¹‰çš„ç±»åˆ«é›†åˆä¸­è¿›è¡Œæ ‡è®°ã€‚æ–‡æœ¬åˆ†ç±»æœ‰è®¸å¤šå®é™…åº”ç”¨ï¼Œå…¶ä¸­ä¸€äº›åŒ…æ‹¬ï¼š

* æƒ…æ„Ÿåˆ†æï¼šæ ¹æ®æŸäº›ææ€§ï¼ˆå¦‚`ç§¯æ`æˆ–`æ¶ˆæ`ï¼‰å¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°ï¼Œå¯ä»¥æ”¯æŒæ”¿æ²»ã€é‡‘èå’Œè¥é”€ç­‰é¢†åŸŸçš„å†³ç­–åˆ¶å®š
* å†…å®¹åˆ†ç±»ï¼šæ ¹æ®æŸäº›ä¸»é¢˜å¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°ï¼Œæœ‰åŠ©äºç»„ç»‡å’Œè¿‡æ»¤æ–°é—»å’Œç¤¾äº¤åª’ä½“æè¦ä¸­çš„ä¿¡æ¯ï¼ˆ`å¤©æ°”`ã€`ä½“è‚²`ã€`é‡‘è`ç­‰ï¼‰


```py
>>> from transformers import pipeline

>>> classifier = pipeline(task="sentiment-analysis")
>>> preds = classifier("Hugging Face is the best thing since sliced bread!")
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> preds
[{'score': 0.9991, 'label': 'POSITIVE'}]
```

### Tokenåˆ†ç±»

åœ¨ä»»ä½•NLPä»»åŠ¡ä¸­ï¼Œæ–‡æœ¬éƒ½ç»è¿‡é¢„å¤„ç†ï¼Œå°†æ–‡æœ¬åºåˆ—åˆ†æˆå•ä¸ªå•è¯æˆ–å­è¯ã€‚è¿™äº›è¢«ç§°ä¸º[tokens](/glossary#token)ã€‚Tokenåˆ†ç±»å°†æ¯ä¸ª`token`åˆ†é…ä¸€ä¸ªæ¥è‡ªé¢„å®šä¹‰ç±»åˆ«é›†çš„æ ‡ç­¾ã€‚

ä¸¤ç§å¸¸è§çš„Tokenåˆ†ç±»æ˜¯ï¼š

* å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ï¼šæ ¹æ®å®ä½“ç±»åˆ«ï¼ˆå¦‚ç»„ç»‡ã€äººå‘˜ã€ä½ç½®æˆ–æ—¥æœŸï¼‰å¯¹`token`è¿›è¡Œæ ‡è®°ã€‚NERåœ¨ç”Ÿç‰©åŒ»å­¦è®¾ç½®ä¸­ç‰¹åˆ«å—æ¬¢è¿ï¼Œå¯ä»¥æ ‡è®°åŸºå› ã€è›‹ç™½è´¨å’Œè¯ç‰©åç§°ã€‚
* è¯æ€§æ ‡æ³¨ï¼ˆPOSï¼‰ï¼šæ ¹æ®å…¶è¯æ€§ï¼ˆå¦‚åè¯ã€åŠ¨è¯æˆ–å½¢å®¹è¯ï¼‰å¯¹æ ‡è®°è¿›è¡Œæ ‡è®°ã€‚POSå¯¹äºå¸®åŠ©ç¿»è¯‘ç³»ç»Ÿäº†è§£ä¸¤ä¸ªç›¸åŒçš„å•è¯å¦‚ä½•åœ¨è¯­æ³•ä¸Šä¸åŒå¾ˆæœ‰ç”¨ï¼ˆä½œä¸ºåè¯çš„é“¶è¡Œä¸ä½œä¸ºåŠ¨è¯çš„é“¶è¡Œï¼‰ã€‚

```py
>>> from transformers import pipeline

>>> classifier = pipeline(task="ner")
>>> preds = classifier("Hugging Face is a French company based in New York City.")
>>> preds = [
...     {
...         "entity": pred["entity"],
...         "score": round(pred["score"], 4),
...         "index": pred["index"],
...         "word": pred["word"],
...         "start": pred["start"],
...         "end": pred["end"],
...     }
...     for pred in preds
... ]
>>> print(*preds, sep="\n")
{'entity': 'I-ORG', 'score': 0.9968, 'index': 1, 'word': 'Hu', 'start': 0, 'end': 2}
{'entity': 'I-ORG', 'score': 0.9293, 'index': 2, 'word': '##gging', 'start': 2, 'end': 7}
{'entity': 'I-ORG', 'score': 0.9763, 'index': 3, 'word': 'Face', 'start': 8, 'end': 12}
{'entity': 'I-MISC', 'score': 0.9983, 'index': 6, 'word': 'French', 'start': 18, 'end': 24}
{'entity': 'I-LOC', 'score': 0.999, 'index': 10, 'word': 'New', 'start': 42, 'end': 45}
{'entity': 'I-LOC', 'score': 0.9987, 'index': 11, 'word': 'York', 'start': 46, 'end': 50}
{'entity': 'I-LOC', 'score': 0.9992, 'index': 12, 'word': 'City', 'start': 51, 'end': 55}
```

### é—®ç­”

é—®ç­”æ˜¯å¦ä¸€ä¸ª`token-level`çš„ä»»åŠ¡ï¼Œè¿”å›ä¸€ä¸ªé—®é¢˜çš„ç­”æ¡ˆï¼Œæœ‰æ—¶å¸¦æœ‰ä¸Šä¸‹æ–‡ï¼ˆå¼€æ”¾é¢†åŸŸï¼‰ï¼Œæœ‰æ—¶ä¸å¸¦ä¸Šä¸‹æ–‡ï¼ˆå°é—­é¢†åŸŸï¼‰ã€‚æ¯å½“æˆ‘ä»¬å‘è™šæ‹ŸåŠ©æ‰‹æå‡ºé—®é¢˜æ—¶ï¼Œä¾‹å¦‚è¯¢é—®ä¸€å®¶é¤å…æ˜¯å¦è¥ä¸šï¼Œå°±ä¼šå‘ç”Ÿè¿™ç§æƒ…å†µã€‚å®ƒè¿˜å¯ä»¥æä¾›å®¢æˆ·æˆ–æŠ€æœ¯æ”¯æŒï¼Œå¹¶å¸®åŠ©æœç´¢å¼•æ“æ£€ç´¢æ‚¨è¦æ±‚çš„ç›¸å…³ä¿¡æ¯ã€‚

æœ‰ä¸¤ç§å¸¸è§çš„é—®ç­”ç±»å‹ï¼š

* æå–å¼ï¼šç»™å®šä¸€ä¸ªé—®é¢˜å’Œä¸€äº›ä¸Šä¸‹æ–‡ï¼Œç­”æ¡ˆæ˜¯ä»æ¨¡å‹å¿…é¡»æå–çš„ä¸Šä¸‹æ–‡ä¸­çš„ä¸€æ®µæ–‡æœ¬è·¨åº¦ã€‚
* æŠ½è±¡å¼ï¼šç»™å®šä¸€ä¸ªé—®é¢˜å’Œä¸€äº›ä¸Šä¸‹æ–‡ï¼Œç­”æ¡ˆä»ä¸Šä¸‹æ–‡ä¸­ç”Ÿæˆï¼›è¿™ç§æ–¹æ³•ç”±[`Text2TextGenerationPipeline`]å¤„ç†ï¼Œè€Œä¸æ˜¯ä¸‹é¢æ˜¾ç¤ºçš„[`QuestionAnsweringPipeline`]ã€‚


```py
>>> from transformers import pipeline

>>> question_answerer = pipeline(task="question-answering")
>>> preds = question_answerer(
...     question="What is the name of the repository?",
...     context="The name of the repository is huggingface/transformers",
... )
>>> print(
...     f"score: {round(preds['score'], 4)}, start: {preds['start']}, end: {preds['end']}, answer: {preds['answer']}"
... )
score: 0.9327, start: 30, end: 54, answer: huggingface/transformers
```

### æ‘˜è¦

æ‘˜è¦ä»è¾ƒé•¿çš„æ–‡æœ¬ä¸­åˆ›å»ºä¸€ä¸ªè¾ƒçŸ­çš„ç‰ˆæœ¬ï¼ŒåŒæ—¶å°½å¯èƒ½ä¿ç•™åŸå§‹æ–‡æ¡£çš„å¤§éƒ¨åˆ†å«ä¹‰ã€‚æ‘˜è¦æ˜¯ä¸€ä¸ªåºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡ï¼›å®ƒè¾“å‡ºæ¯”è¾“å…¥æ›´çŸ­çš„æ–‡æœ¬åºåˆ—ã€‚æœ‰è®¸å¤šé•¿ç¯‡æ–‡æ¡£å¯ä»¥è¿›è¡Œæ‘˜è¦ï¼Œä»¥å¸®åŠ©è¯»è€…å¿«é€Ÿäº†è§£ä¸»è¦è¦ç‚¹ã€‚æ³•æ¡ˆã€æ³•å¾‹å’Œè´¢åŠ¡æ–‡ä»¶ã€ä¸“åˆ©å’Œç§‘å­¦è®ºæ–‡ç­‰æ–‡æ¡£å¯ä»¥æ‘˜è¦ï¼Œä»¥èŠ‚çœè¯»è€…çš„æ—¶é—´å¹¶ä½œä¸ºé˜…è¯»è¾…åŠ©å·¥å…·ã€‚

åƒé—®ç­”ä¸€æ ·ï¼Œæ‘˜è¦æœ‰ä¸¤ç§ç±»å‹ï¼š

* æå–å¼ï¼šä»åŸå§‹æ–‡æœ¬ä¸­è¯†åˆ«å’Œæå–æœ€é‡è¦çš„å¥å­
* æŠ½è±¡å¼ï¼šä»åŸå§‹æ–‡æœ¬ç”Ÿæˆç›®æ ‡æ‘˜è¦ï¼ˆå¯èƒ½åŒ…æ‹¬ä¸åœ¨è¾“å…¥æ–‡æ¡£ä¸­çš„æ–°å•è¯ï¼‰ï¼›[`SummarizationPipeline`]ä½¿ç”¨æŠ½è±¡æ–¹æ³•ã€‚


```py
>>> from transformers import pipeline

>>> summarizer = pipeline(task="summarization")
>>> summarizer(
...     "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles."
... )
[{'summary_text': ' The Transformer is the first sequence transduction model based entirely on attention . It replaces the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention . For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .'}]
```

### ç¿»è¯‘

ç¿»è¯‘å°†ä¸€ç§è¯­è¨€çš„æ–‡æœ¬åºåˆ—è½¬æ¢ä¸ºå¦ä¸€ç§è¯­è¨€ã€‚å®ƒå¯¹äºå¸®åŠ©æ¥è‡ªä¸åŒèƒŒæ™¯çš„äººä»¬ç›¸äº’äº¤æµã€å¸®åŠ©ç¿»è¯‘å†…å®¹ä»¥å¸å¼•æ›´å¹¿æ³›çš„å—ä¼—ï¼Œç”šè‡³æˆä¸ºå­¦ä¹ å·¥å…·ä»¥å¸®åŠ©äººä»¬å­¦ä¹ ä¸€é—¨æ–°è¯­è¨€éƒ½éå¸¸é‡è¦ã€‚é™¤äº†æ‘˜è¦ä¹‹å¤–ï¼Œç¿»è¯‘ä¹Ÿæ˜¯ä¸€ä¸ªåºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡ï¼Œæ„å‘³ç€æ¨¡å‹æ¥æ”¶è¾“å…¥åºåˆ—å¹¶è¿”å›ç›®æ ‡è¾“å‡ºåºåˆ—ã€‚

åœ¨æ—©æœŸï¼Œç¿»è¯‘æ¨¡å‹å¤§å¤šæ˜¯å•è¯­çš„ï¼Œä½†æœ€è¿‘ï¼Œè¶Šæ¥è¶Šå¤šçš„äººå¯¹å¯ä»¥åœ¨å¤šç§è¯­è¨€ä¹‹é—´è¿›è¡Œç¿»è¯‘çš„å¤šè¯­è¨€æ¨¡å‹æ„Ÿå…´è¶£ã€‚

```py
>>> from transformers import pipeline

>>> text = "translate English to French: Hugging Face is a community-based open-source platform for machine learning."
>>> translator = pipeline(task="translation", model="google-t5/t5-small")
>>> translator(text)
[{'translation_text': "Hugging Face est une tribune communautaire de l'apprentissage des machines."}]
```

### è¯­è¨€æ¨¡å‹

è¯­è¨€æ¨¡å‹æ˜¯ä¸€ç§é¢„æµ‹æ–‡æœ¬åºåˆ—ä¸­å•è¯çš„ä»»åŠ¡ã€‚å®ƒå·²æˆä¸ºä¸€ç§éå¸¸æµè¡Œçš„NLPä»»åŠ¡ï¼Œå› ä¸ºé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹å¯ä»¥å¾®è°ƒç”¨äºè®¸å¤šå…¶ä»–ä¸‹æ¸¸ä»»åŠ¡ã€‚æœ€è¿‘ï¼Œäººä»¬å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¡¨ç°å‡ºäº†æå¤§çš„å…´è¶£ï¼Œè¿™äº›æ¨¡å‹å±•ç¤ºäº†`zero learning`æˆ–`few-shot learning`çš„èƒ½åŠ›ã€‚è¿™æ„å‘³ç€æ¨¡å‹å¯ä»¥è§£å†³å®ƒæœªè¢«æ˜ç¡®è®­ç»ƒè¿‡çš„ä»»åŠ¡ï¼è¯­è¨€æ¨¡å‹å¯ç”¨äºç”Ÿæˆæµç•…å’Œä»¤äººä¿¡æœçš„æ–‡æœ¬ï¼Œä½†éœ€è¦å°å¿ƒï¼Œå› ä¸ºæ–‡æœ¬å¯èƒ½å¹¶ä¸æ€»æ˜¯å‡†ç¡®çš„ã€‚

æœ‰ä¸¤ç§ç±»å‹çš„è¯è¯­æ¨¡å‹ï¼š

* causalï¼šæ¨¡å‹çš„ç›®æ ‡æ˜¯é¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ª`token`ï¼Œè€Œæœªæ¥çš„`tokens`è¢«é®ç›–ã€‚
  

    ```py
    >>> from transformers import pipeline

    >>> prompt = "Hugging Face is a community-based open-source platform for machine learning."
    >>> generator = pipeline(task="text-generation")
    >>> generator(prompt)  # doctest: +SKIP
    ```

*  maskedï¼šæ¨¡å‹çš„ç›®æ ‡æ˜¯é¢„æµ‹åºåˆ—ä¸­è¢«é®è”½çš„`token`ï¼ŒåŒæ—¶å…·æœ‰å¯¹åºåˆ—ä¸­æ‰€æœ‰`tokens`çš„å®Œå…¨è®¿é—®æƒé™ã€‚

    
    ```py
    >>> text = "Hugging Face is a community-based open-source <mask> for machine learning."
    >>> fill_mask = pipeline(task="fill-mask")
    >>> preds = fill_mask(text, top_k=1)
    >>> preds = [
    ...     {
    ...         "score": round(pred["score"], 4),
    ...         "token": pred["token"],
    ...         "token_str": pred["token_str"],
    ...         "sequence": pred["sequence"],
    ...     }
    ...     for pred in preds
    ... ]
    >>> preds
    [{'score': 0.2236,
      'token': 1761,
      'token_str': ' platform',
      'sequence': 'Hugging Face is a community-based open-source platform for machine learning.'}]
    ```

## å¤šæ¨¡æ€

å¤šæ¨¡æ€ä»»åŠ¡è¦æ±‚æ¨¡å‹å¤„ç†å¤šç§æ•°æ®æ¨¡æ€ï¼ˆæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ï¼‰ä»¥è§£å†³ç‰¹å®šé—®é¢˜ã€‚å›¾åƒæè¿°æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€ä»»åŠ¡çš„ä¾‹å­ï¼Œå…¶ä¸­æ¨¡å‹å°†å›¾åƒä½œä¸ºè¾“å…¥å¹¶è¾“å‡ºæè¿°å›¾åƒæˆ–å›¾åƒæŸäº›å±æ€§çš„æ–‡æœ¬åºåˆ—ã€‚

è™½ç„¶å¤šæ¨¡æ€æ¨¡å‹å¤„ç†ä¸åŒçš„æ•°æ®ç±»å‹æˆ–æ¨¡æ€ï¼Œä½†å†…éƒ¨é¢„å¤„ç†æ­¥éª¤å¸®åŠ©æ¨¡å‹å°†æ‰€æœ‰æ•°æ®ç±»å‹è½¬æ¢ä¸º`embeddings`ï¼ˆå‘é‡æˆ–æ•°å­—åˆ—è¡¨ï¼ŒåŒ…å«æœ‰å…³æ•°æ®çš„æœ‰æ„ä¹‰ä¿¡æ¯ï¼‰ã€‚å¯¹äºåƒå›¾åƒæè¿°è¿™æ ·çš„ä»»åŠ¡ï¼Œæ¨¡å‹å­¦ä¹ å›¾åƒåµŒå…¥å’Œæ–‡æœ¬åµŒå…¥ä¹‹é—´çš„å…³ç³»ã€‚

### æ–‡æ¡£é—®ç­”

æ–‡æ¡£é—®ç­”æ˜¯ä»æ–‡æ¡£ä¸­å›ç­”è‡ªç„¶è¯­è¨€é—®é¢˜çš„ä»»åŠ¡ã€‚ä¸`token-level`é—®ç­”ä»»åŠ¡ä¸åŒï¼Œæ–‡æ¡£é—®ç­”å°†åŒ…å«é—®é¢˜çš„æ–‡æ¡£çš„å›¾åƒä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ç­”æ¡ˆã€‚æ–‡æ¡£é—®ç­”å¯ç”¨äºè§£æç»“æ„åŒ–æ–‡æ¡£å¹¶ä»ä¸­æå–å…³é”®ä¿¡æ¯ã€‚åœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œå¯ä»¥ä»æ”¶æ®ä¸­æå–æ€»é‡‘é¢å’Œæ‰¾é›¶é‡‘é¢ã€‚

```py
>>> from transformers import pipeline
>>> from PIL import Image
>>> import requests

>>> url = "https://huggingface.co/datasets/hf-internal-testing/example-documents/resolve/main/jpeg_images/2.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> doc_question_answerer = pipeline("document-question-answering", model="magorshunov/layoutlm-invoices")
>>> preds = doc_question_answerer(
...     question="What is the total amount?",
...     image=image,
... )
>>> preds
[{'score': 0.8531, 'answer': '17,000', 'start': 4, 'end': 4}]
```

å¸Œæœ›è¿™ä¸ªé¡µé¢ä¸ºæ‚¨æä¾›äº†ä¸€äº›æœ‰å…³æ¯ç§æ¨¡æ€ä¸­æ‰€æœ‰ç±»å‹ä»»åŠ¡çš„èƒŒæ™¯ä¿¡æ¯ä»¥åŠæ¯ä¸ªä»»åŠ¡çš„å®é™…é‡è¦æ€§ã€‚åœ¨[ä¸‹ä¸€èŠ‚](tasks_explained)ä¸­ï¼Œæ‚¨å°†äº†è§£Transformerså¦‚ä½•è§£å†³è¿™äº›ä»»åŠ¡ã€‚
