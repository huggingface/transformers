<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# å¿«é€Ÿä¸Šæ‰‹

[[open-in-colab]]

å¿«æ¥ä½¿ç”¨ ğŸ¤— Transformers å§! æ— è®ºä½ æ˜¯å¼€å‘äººå‘˜è¿˜æ˜¯æ—¥å¸¸ç”¨æˆ·, è¿™ç¯‡å¿«é€Ÿä¸Šæ‰‹æ•™ç¨‹éƒ½å°†å¸®åŠ©ä½ å…¥é—¨å¹¶ä¸”å‘ä½ å±•ç¤ºå¦‚ä½•ä½¿ç”¨[`pipeline`]è¿›è¡Œæ¨ç†, ä½¿ç”¨[AutoClass](./model_doc/auto)åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹å’Œé¢„å¤„ç†å™¨, ä»¥åŠä½¿ç”¨PyTorchæˆ–TensorFlowå¿«é€Ÿè®­ç»ƒä¸€ä¸ªæ¨¡å‹. å¦‚æœä½ æ˜¯ä¸€ä¸ªåˆå­¦è€…, æˆ‘ä»¬å»ºè®®ä½ æ¥ä¸‹æ¥æŸ¥çœ‹æˆ‘ä»¬çš„æ•™ç¨‹æˆ–è€…[è¯¾ç¨‹](https://huggingface.co/course/chapter1/1), æ¥æ›´æ·±å…¥åœ°äº†è§£åœ¨è¿™é‡Œä»‹ç»åˆ°çš„æ¦‚å¿µ.

åœ¨å¼€å§‹ä¹‹å‰, ç¡®ä¿ä½ å·²ç»å®‰è£…äº†æ‰€æœ‰å¿…è¦çš„åº“:

```bash
!pip install transformers datasets
```

ä½ è¿˜éœ€è¦å®‰è£…å–œæ¬¢çš„æœºå™¨å­¦ä¹ æ¡†æ¶:

<frameworkcontent>
<pt>
```bash
pip install torch
```
</pt>
<tf>
```bash
pip install tensorflow
```
</tf>
</frameworkcontent>

## Pipeline

<Youtube id="tiZFewofSLM"/>

ä½¿ç”¨[`pipeline`]æ˜¯åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¨ç†çš„æœ€ç®€å•çš„æ–¹å¼. ä½ èƒ½å¤Ÿå°†[`pipeline`]å¼€ç®±å³ç”¨åœ°ç”¨äºè·¨ä¸åŒæ¨¡æ€çš„å¤šç§ä»»åŠ¡. æ¥çœ‹çœ‹å®ƒæ”¯æŒçš„ä»»åŠ¡åˆ—è¡¨:

| **ä»»åŠ¡**                     | **æè¿°**                                                                                                      | **æ¨¡æ€**        | **Pipeline**                       |
|------------------------------|--------------------------------------------------------------------------------------------------------------|-----------------|-----------------------------------------------|
| æ–‡æœ¬åˆ†ç±»                      | ä¸ºç»™å®šçš„æ–‡æœ¬åºåˆ—åˆ†é…ä¸€ä¸ªæ ‡ç­¾                                                                                    | NLP             | pipeline(task="sentiment-analysis")           |
| æ–‡æœ¬ç”Ÿæˆ                      | æ ¹æ®ç»™å®šçš„æç¤ºç”Ÿæˆæ–‡æœ¬                                                                                         | NLP             | pipeline(task="text-generation")              |
| å‘½åå®ä½“è¯†åˆ«                  | ä¸ºåºåˆ—é‡Œçš„æ¯ä¸ªtokenåˆ†é…ä¸€ä¸ªæ ‡ç­¾(äºº, ç»„ç»‡, åœ°å€ç­‰ç­‰)                                                              | NLP             | pipeline(task="ner")                          |
| é—®ç­”ç³»ç»Ÿ                      | é€šè¿‡ç»™å®šçš„ä¸Šä¸‹æ–‡å’Œé—®é¢˜, åœ¨æ–‡æœ¬ä¸­æå–ç­”æ¡ˆ                                                                         | NLP             | pipeline(task="question-answering")           |
| æ©ç›–å¡«å……                      | é¢„æµ‹å‡ºæ­£ç¡®çš„åœ¨åºåˆ—ä¸­è¢«æ©ç›–çš„token                                                                               | NLP             | pipeline(task="fill-mask")                    |
| æ–‡æœ¬æ‘˜è¦                      | ä¸ºæ–‡æœ¬åºåˆ—æˆ–æ–‡æ¡£ç”Ÿæˆæ€»ç»“                                                                                        | NLP             | pipeline(task="summarization")                |
| æ–‡æœ¬ç¿»è¯‘                      | å°†æ–‡æœ¬ä»ä¸€ç§è¯­è¨€ç¿»è¯‘ä¸ºå¦ä¸€ç§è¯­è¨€                                                                                | NLP             | pipeline(task="translation")                  |
| å›¾åƒåˆ†ç±»                      | ä¸ºå›¾åƒåˆ†é…ä¸€ä¸ªæ ‡ç­¾                                                                                             | Computer vision | pipeline(task="image-classification")         |
| å›¾åƒåˆ†å‰²                      | ä¸ºå›¾åƒä¸­æ¯ä¸ªç‹¬ç«‹çš„åƒç´ åˆ†é…æ ‡ç­¾(æ”¯æŒè¯­ä¹‰ã€å…¨æ™¯å’Œå®ä¾‹åˆ†å‰²)                                                          | Computer vision | pipeline(task="image-segmentation")           |
| ç›®æ ‡æ£€æµ‹                      | é¢„æµ‹å›¾åƒä¸­ç›®æ ‡å¯¹è±¡çš„è¾¹ç•Œæ¡†å’Œç±»åˆ«                                                                                | Computer vision | pipeline(task="object-detection")             |
| éŸ³é¢‘åˆ†ç±»                      | ç»™éŸ³é¢‘æ–‡ä»¶åˆ†é…ä¸€ä¸ªæ ‡ç­¾                                                                                         | Audio           | pipeline(task="audio-classification")         |
| è‡ªåŠ¨è¯­éŸ³è¯†åˆ«                   | å°†éŸ³é¢‘æ–‡ä»¶ä¸­çš„è¯­éŸ³æå–ä¸ºæ–‡æœ¬                                                                                   | Audio           | pipeline(task="automatic-speech-recognition") |
| è§†è§‰é—®ç­”                      | ç»™å®šä¸€ä¸ªå›¾åƒå’Œä¸€ä¸ªé—®é¢˜ï¼Œæ­£ç¡®åœ°å›ç­”æœ‰å…³å›¾åƒçš„é—®é¢˜                                                                  | Multimodal      | pipeline(task="vqa")                          |

åˆ›å»ºä¸€ä¸ª[`pipeline`]å®ä¾‹å¹¶ä¸”æŒ‡å®šä½ æƒ³è¦å°†å®ƒç”¨äºçš„ä»»åŠ¡, å°±å¯ä»¥å¼€å§‹äº†. ä½ å¯ä»¥å°†[`pipeline`]ç”¨äºä»»ä½•ä¸€ä¸ªä¸Šé¢æåˆ°çš„ä»»åŠ¡, å¦‚æœæƒ³çŸ¥é“æ”¯æŒçš„ä»»åŠ¡çš„å®Œæ•´åˆ—è¡¨, å¯ä»¥æŸ¥é˜…[pipeline API å‚è€ƒ](./main_classes/pipelines). ä¸è¿‡, åœ¨è¿™ç¯‡æ•™ç¨‹ä¸­, ä½ å°†æŠŠ [`pipeline`]ç”¨åœ¨ä¸€ä¸ªæƒ…æ„Ÿåˆ†æç¤ºä¾‹ä¸Š:

```py
>>> from transformers import pipeline

>>> classifier = pipeline("sentiment-analysis")
```

[`pipeline`] ä¼šä¸‹è½½å¹¶ç¼“å­˜ä¸€ä¸ªç”¨äºæƒ…æ„Ÿåˆ†æçš„é»˜è®¤çš„[é¢„è®­ç»ƒæ¨¡å‹](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)å’Œåˆ†è¯å™¨. ç°åœ¨ä½ å¯ä»¥åœ¨ç›®æ ‡æ–‡æœ¬ä¸Šä½¿ç”¨ `classifier`äº†:

```py
>>> classifier("We are very happy to show you the ğŸ¤— Transformers library.")
[{'label': 'POSITIVE', 'score': 0.9998}]
```

å¦‚æœä½ æœ‰ä¸æ­¢ä¸€ä¸ªè¾“å…¥, å¯ä»¥æŠŠæ‰€æœ‰è¾“å…¥æ”¾å…¥ä¸€ä¸ªåˆ—è¡¨ç„¶åä¼ ç»™[`pipeline`], å®ƒå°†ä¼šè¿”å›ä¸€ä¸ªå­—å…¸åˆ—è¡¨:

```py
>>> results = classifier(["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."])
>>> for result in results:
...     print(f"label: {result['label']}, with score: {round(result['score'], 4)}")
label: POSITIVE, with score: 0.9998
label: NEGATIVE, with score: 0.5309
```

[`pipeline`] ä¹Ÿå¯ä»¥ä¸ºä»»ä½•ä½ å–œæ¬¢çš„ä»»åŠ¡éå†æ•´ä¸ªæ•°æ®é›†. åœ¨ä¸‹é¢è¿™ä¸ªç¤ºä¾‹ä¸­, è®©æˆ‘ä»¬é€‰æ‹©è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä½œä¸ºæˆ‘ä»¬çš„ä»»åŠ¡:

```py
>>> import torch
>>> from transformers import pipeline

>>> speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h")
```

åŠ è½½ä¸€ä¸ªä½ æƒ³éå†çš„éŸ³é¢‘æ•°æ®é›† (æŸ¥é˜… ğŸ¤— Datasets [å¿«é€Ÿå¼€å§‹](https://huggingface.co/docs/datasets/quickstart#audio) è·å¾—æ›´å¤šä¿¡æ¯). æ¯”å¦‚, åŠ è½½ [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) æ•°æ®é›†:

```py
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")  # doctest: +IGNORE_RESULT
```

ä½ éœ€è¦ç¡®ä¿æ•°æ®é›†ä¸­çš„éŸ³é¢‘çš„é‡‡æ ·ç‡ä¸ [`facebook/wav2vec2-base-960h`](https://huggingface.co/facebook/wav2vec2-base-960h) è®­ç»ƒç”¨åˆ°çš„éŸ³é¢‘çš„é‡‡æ ·ç‡ä¸€è‡´:

```py
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))
```

å½“è°ƒç”¨`"audio"` columnæ—¶, éŸ³é¢‘æ–‡ä»¶å°†ä¼šè‡ªåŠ¨åŠ è½½å¹¶é‡é‡‡æ ·.
ä»å‰å››ä¸ªæ ·æœ¬ä¸­æå–åŸå§‹æ³¢å½¢æ•°ç»„, å°†å®ƒä½œä¸ºåˆ—è¡¨ä¼ ç»™pipeline:

```py
>>> result = speech_recognizer(dataset[:4]["audio"])
>>> print([d["text"] for d in result])
['I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT', "FODING HOW I'D SET UP A JOIN TO HET WITH MY WIFE AND WHERE THE AP MIGHT BE", "I I'D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I'M NOT SEEING THE OPTION TO DO IT ON THE AP SO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AND I'M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS", 'HOW DO I THURN A JOIN A COUNT']
```

å¯¹äºè¾“å…¥éå¸¸åºå¤§çš„å¤§å‹æ•°æ®é›† (æ¯”å¦‚è¯­éŸ³æˆ–è§†è§‰), ä½ ä¼šæƒ³åˆ°ä½¿ç”¨ä¸€ä¸ªç”Ÿæˆå™¨, è€Œä¸æ˜¯ä¸€ä¸ªå°†æ‰€æœ‰è¾“å…¥éƒ½åŠ è½½è¿›å†…å­˜çš„åˆ—è¡¨. æŸ¥é˜… [pipeline API å‚è€ƒ](./main_classes/pipelines) æ¥è·å–æ›´å¤šä¿¡æ¯.

### åœ¨pipelineä¸­ä½¿ç”¨å¦ä¸€ä¸ªæ¨¡å‹å’Œåˆ†è¯å™¨

[`pipeline`]å¯ä»¥å®¹çº³[Hub](https://huggingface.co/models)ä¸­çš„ä»»ä½•æ¨¡å‹, è¿™è®©[`pipeline`]æ›´å®¹æ˜“é€‚ç”¨äºå…¶ä»–ç”¨ä¾‹. æ¯”å¦‚, ä½ æƒ³è¦ä¸€ä¸ªèƒ½å¤Ÿå¤„ç†æ³•è¯­æ–‡æœ¬çš„æ¨¡å‹, å°±å¯ä»¥ä½¿ç”¨Hubä¸Šçš„æ ‡è®°æ¥ç­›é€‰å‡ºåˆé€‚çš„æ¨¡å‹. é å‰çš„ç­›é€‰ç»“æœä¼šè¿”å›ä¸€ä¸ªä¸ºæƒ…æ„Ÿåˆ†æå¾®è°ƒçš„å¤šè¯­è¨€çš„ [BERT æ¨¡å‹](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment), ä½ å¯ä»¥å°†å®ƒç”¨äºæ³•è¯­æ–‡æœ¬:

```py
>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
```

<frameworkcontent>
<pt>
ä½¿ç”¨ [`AutoModelForSequenceClassification`]å’Œ[`AutoTokenizer`]æ¥åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œå®ƒå…³è”çš„åˆ†è¯å™¨ (æ›´å¤šä¿¡æ¯å¯ä»¥å‚è€ƒä¸‹ä¸€èŠ‚çš„ `AutoClass`):

```py
>>> from transformers import AutoTokenizer, AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained(model_name)
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```
</pt>
<tf>
ä½¿ç”¨ [`TFAutoModelForSequenceClassification`]å’Œ[`AutoTokenizer`] æ¥åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œå®ƒå…³è”çš„åˆ†è¯å™¨ (æ›´å¤šä¿¡æ¯å¯ä»¥å‚è€ƒä¸‹ä¸€èŠ‚çš„ `TFAutoClass`):

```py
>>> from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

>>> model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```
</tf>
</frameworkcontent>

åœ¨[`pipeline`]ä¸­æŒ‡å®šæ¨¡å‹å’Œåˆ†è¯å™¨, ç°åœ¨ä½ å°±å¯ä»¥åœ¨æ³•è¯­æ–‡æœ¬ä¸Šä½¿ç”¨ `classifier`äº†:

```py
>>> classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
>>> classifier("Nous sommes trÃ¨s heureux de vous prÃ©senter la bibliothÃ¨que ğŸ¤— Transformers.")
[{'label': '5 stars', 'score': 0.7273}]
```

å¦‚æœä½ æ²¡æœ‰æ‰¾åˆ°é€‚åˆä½ çš„æ¨¡å‹, å°±éœ€è¦åœ¨ä½ çš„æ•°æ®ä¸Šå¾®è°ƒä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹äº†. æŸ¥çœ‹[å¾®è°ƒæ•™ç¨‹](./training) æ¥å­¦ä¹ æ€æ ·è¿›è¡Œå¾®è°ƒ. æœ€å, å¾®è°ƒå®Œæ¨¡å‹å, è€ƒè™‘ä¸€ä¸‹åœ¨Hubä¸Šä¸ç¤¾åŒº [åˆ†äº«](./model_sharing) è¿™ä¸ªæ¨¡å‹, æŠŠæœºå™¨å­¦ä¹ æ™®åŠåˆ°æ¯ä¸€ä¸ªäºº! ğŸ¤—

## AutoClass

<Youtube id="AhChOFRegn4"/>

åœ¨å¹•å, æ˜¯ç”±[`AutoModelForSequenceClassification`]å’Œ[`AutoTokenizer`]ä¸€èµ·æ”¯æŒä½ åœ¨ä¸Šé¢ç”¨åˆ°çš„[`pipeline`].  [AutoClass](./model_doc/auto) æ˜¯ä¸€ä¸ªèƒ½å¤Ÿé€šè¿‡é¢„è®­ç»ƒæ¨¡å‹çš„åç§°æˆ–è·¯å¾„è‡ªåŠ¨æŸ¥æ‰¾å…¶æ¶æ„çš„å¿«æ·æ–¹å¼. ä½ åªéœ€è¦ä¸ºä½ çš„ä»»åŠ¡é€‰æ‹©åˆé€‚çš„ `AutoClass` å’Œå®ƒå…³è”çš„é¢„å¤„ç†ç±». 

è®©æˆ‘ä»¬å›è¿‡å¤´æ¥çœ‹ä¸Šä¸€èŠ‚çš„ç¤ºä¾‹, çœ‹çœ‹æ€æ ·ä½¿ç”¨ `AutoClass` æ¥é‡ç°ä½¿ç”¨[`pipeline`]çš„ç»“æœ.

### AutoTokenizer

åˆ†è¯å™¨è´Ÿè´£é¢„å¤„ç†æ–‡æœ¬, å°†æ–‡æœ¬è½¬æ¢ä¸ºç”¨äºè¾“å…¥æ¨¡å‹çš„æ•°å­—æ•°ç»„. æœ‰å¤šä¸ªç”¨æ¥ç®¡ç†åˆ†è¯è¿‡ç¨‹çš„è§„åˆ™, åŒ…æ‹¬å¦‚ä½•æ‹†åˆ†å•è¯å’Œåœ¨ä»€ä¹ˆæ ·çš„çº§åˆ«ä¸Šæ‹†åˆ†å•è¯ (åœ¨ [åˆ†è¯å™¨æ€»ç»“](./tokenizer_summary)å­¦ä¹ æ›´å¤šå…³äºåˆ†è¯çš„ä¿¡æ¯). è¦è®°ä½æœ€é‡è¦çš„æ˜¯ä½ éœ€è¦å®ä¾‹åŒ–çš„åˆ†è¯å™¨è¦ä¸æ¨¡å‹çš„åç§°ç›¸åŒ, æ¥ç¡®ä¿å’Œæ¨¡å‹è®­ç»ƒæ—¶ä½¿ç”¨ç›¸åŒçš„åˆ†è¯è§„åˆ™.

ä½¿ç”¨[`AutoTokenizer`]åŠ è½½ä¸€ä¸ªåˆ†è¯å™¨:

```py
>>> from transformers import AutoTokenizer

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```

å°†æ–‡æœ¬ä¼ å…¥åˆ†è¯å™¨:

```py
>>> encoding = tokenizer("We are very happy to show you the ğŸ¤— Transformers library.")
>>> print(encoding)
{'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

åˆ†è¯å™¨è¿”å›äº†å«æœ‰å¦‚ä¸‹å†…å®¹çš„å­—å…¸:

* [input_ids](./glossary#input-ids): ç”¨æ•°å­—è¡¨ç¤ºçš„token.
* [attention_mask](.glossary#attention-mask): åº”è¯¥å…³æ³¨å“ªäº›tokençš„æŒ‡ç¤º.

åˆ†è¯å™¨ä¹Ÿå¯ä»¥æ¥å—åˆ—è¡¨ä½œä¸ºè¾“å…¥, å¹¶å¡«å……å’Œæˆªæ–­æ–‡æœ¬, è¿”å›å…·æœ‰ç»Ÿä¸€é•¿åº¦çš„æ‰¹æ¬¡:

<frameworkcontent>
<pt>
```py
>>> pt_batch = tokenizer(
...     ["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."],
...     padding=True,
...     truncation=True,
...     max_length=512,
...     return_tensors="pt",
... )
```
</pt>
<tf>
```py
>>> tf_batch = tokenizer(
...     ["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."],
...     padding=True,
...     truncation=True,
...     max_length=512,
...     return_tensors="tf",
... )
```
</tf>
</frameworkcontent>

<Tip>

æŸ¥é˜…[é¢„å¤„ç†](./preprocessing)æ•™ç¨‹æ¥è·å¾—æœ‰å…³åˆ†è¯çš„æ›´è¯¦ç»†çš„ä¿¡æ¯, ä»¥åŠå¦‚ä½•ä½¿ç”¨[`AutoFeatureExtractor`]å’Œ[`AutoProcessor`]æ¥å¤„ç†å›¾åƒ, éŸ³é¢‘, è¿˜æœ‰å¤šæ¨¡å¼è¾“å…¥.

</Tip>

### AutoModel

<frameworkcontent>
<pt>
ğŸ¤— Transformers æä¾›äº†ä¸€ç§ç®€å•ç»Ÿä¸€çš„æ–¹å¼æ¥åŠ è½½é¢„è®­ç»ƒçš„å®ä¾‹. è¿™è¡¨ç¤ºä½ å¯ä»¥åƒåŠ è½½[`AutoTokenizer`]ä¸€æ ·åŠ è½½[`AutoModel`]. å”¯ä¸€ä¸åŒçš„åœ°æ–¹æ˜¯ä¸ºä½ çš„ä»»åŠ¡é€‰æ‹©æ­£ç¡®çš„[`AutoModel`]. å¯¹äºæ–‡æœ¬ (æˆ–åºåˆ—) åˆ†ç±», ä½ åº”è¯¥åŠ è½½[`AutoModelForSequenceClassification`]:

```py
>>> from transformers import AutoModelForSequenceClassification

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
```

<Tip>

é€šè¿‡[ä»»åŠ¡æ‘˜è¦](./task_summary)æŸ¥æ‰¾[`AutoModel`]æ”¯æŒçš„ä»»åŠ¡.

</Tip>

ç°åœ¨å¯ä»¥æŠŠé¢„å¤„ç†å¥½çš„è¾“å…¥æ‰¹æ¬¡ç›´æ¥é€è¿›æ¨¡å‹. ä½ åªéœ€è¦æ·»åŠ `**`æ¥è§£åŒ…å­—å…¸:

```py
>>> pt_outputs = pt_model(**pt_batch)
```

æ¨¡å‹åœ¨`logits`å±æ€§è¾“å‡ºæœ€ç»ˆçš„æ¿€æ´»ç»“æœ. åœ¨ `logits`ä¸Šåº”ç”¨softmaxå‡½æ•°æ¥æŸ¥è¯¢æ¦‚ç‡:

```py
>>> from torch import nn

>>> pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
>>> print(pt_predictions)
tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],
        [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)
```
</pt>
<tf>
ğŸ¤— Transformers æä¾›äº†ä¸€ç§ç®€å•ç»Ÿä¸€çš„æ–¹å¼æ¥åŠ è½½é¢„è®­ç»ƒçš„å®ä¾‹. è¿™è¡¨ç¤ºä½ å¯ä»¥åƒåŠ è½½[`AutoTokenizer`]ä¸€æ ·åŠ è½½[`TFAutoModel`]. å”¯ä¸€ä¸åŒçš„åœ°æ–¹æ˜¯ä¸ºä½ çš„ä»»åŠ¡é€‰æ‹©æ­£ç¡®çš„[`TFAutoModel`], å¯¹äºæ–‡æœ¬ (æˆ–åºåˆ—) åˆ†ç±», ä½ åº”è¯¥åŠ è½½[`TFAutoModelForSequenceClassification`]:

```py
>>> from transformers import TFAutoModelForSequenceClassification

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
```

<Tip>

é€šè¿‡[ä»»åŠ¡æ‘˜è¦](./task_summary)æŸ¥æ‰¾[`AutoModel`]æ”¯æŒçš„ä»»åŠ¡.

</Tip>

ç°åœ¨é€šè¿‡ç›´æ¥å°†å­—å…¸çš„é”®ä¼ ç»™å¼ é‡ï¼Œå°†é¢„å¤„ç†çš„è¾“å…¥æ‰¹æ¬¡ä¼ ç»™æ¨¡å‹.

```py
>>> tf_outputs = tf_model(tf_batch)
```

æ¨¡å‹åœ¨`logits`å±æ€§è¾“å‡ºæœ€ç»ˆçš„æ¿€æ´»ç»“æœ. åœ¨ `logits`ä¸Šåº”ç”¨softmaxå‡½æ•°æ¥æŸ¥è¯¢æ¦‚ç‡:

```py
>>> import tensorflow as tf

>>> tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)
>>> tf_predictions  # doctest: +IGNORE_RESULT
```
</tf>
</frameworkcontent>

<Tip>

æ‰€æœ‰ ğŸ¤— Transformers æ¨¡å‹ (PyTorch æˆ– TensorFlow) åœ¨æœ€ç»ˆçš„æ¿€æ´»å‡½æ•°(æ¯”å¦‚softmax)*ä¹‹å‰* è¾“å‡ºå¼ é‡,
å› ä¸ºæœ€ç»ˆçš„æ¿€æ´»å‡½æ•°å¸¸å¸¸ä¸lossèåˆ. æ¨¡å‹çš„è¾“å‡ºæ˜¯ç‰¹æ®Šçš„æ•°æ®ç±», æ‰€ä»¥å®ƒä»¬çš„å±æ€§å¯ä»¥åœ¨IDEä¸­è¢«è‡ªåŠ¨è¡¥å…¨. æ¨¡å‹çš„è¾“å‡ºå°±åƒä¸€ä¸ªå…ƒç»„æˆ–å­—å…¸ (ä½ å¯ä»¥é€šè¿‡æ•´æ•°ã€åˆ‡ç‰‡æˆ–å­—ç¬¦ä¸²æ¥ç´¢å¼•å®ƒ), åœ¨è¿™ç§æƒ…å†µä¸‹, ä¸ºNoneçš„å±æ€§ä¼šè¢«å¿½ç•¥.

</Tip>

### ä¿å­˜æ¨¡å‹

<frameworkcontent>
<pt>
å½“ä½ çš„æ¨¡å‹å¾®è°ƒå®Œæˆ, ä½ å°±å¯ä»¥ä½¿ç”¨[`PreTrainedModel.save_pretrained`]æŠŠå®ƒå’Œå®ƒçš„åˆ†è¯å™¨ä¿å­˜ä¸‹æ¥:

```py
>>> pt_save_directory = "./pt_save_pretrained"
>>> tokenizer.save_pretrained(pt_save_directory)  # doctest: +IGNORE_RESULT
>>> pt_model.save_pretrained(pt_save_directory)
```

å½“ä½ å‡†å¤‡å†æ¬¡ä½¿ç”¨è¿™ä¸ªæ¨¡å‹æ—¶, å°±å¯ä»¥ä½¿ç”¨[`PreTrainedModel.from_pretrained`]åŠ è½½å®ƒäº†:

```py
>>> pt_model = AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained")
```
</pt>
<tf>
å½“ä½ çš„æ¨¡å‹å¾®è°ƒå®Œæˆ, ä½ å°±å¯ä»¥ä½¿ç”¨[`TFPreTrainedModel.save_pretrained`]æŠŠå®ƒå’Œå®ƒçš„åˆ†è¯å™¨ä¿å­˜ä¸‹æ¥:

```py
>>> tf_save_directory = "./tf_save_pretrained"
>>> tokenizer.save_pretrained(tf_save_directory)  # doctest: +IGNORE_RESULT
>>> tf_model.save_pretrained(tf_save_directory)
```

å½“ä½ å‡†å¤‡å†æ¬¡ä½¿ç”¨è¿™ä¸ªæ¨¡å‹æ—¶, å°±å¯ä»¥ä½¿ç”¨[`TFPreTrainedModel.from_pretrained`]åŠ è½½å®ƒäº†:

```py
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained("./tf_save_pretrained")
```
</tf>
</frameworkcontent>

ğŸ¤— Transformersæœ‰ä¸€ä¸ªç‰¹åˆ«é…·çš„åŠŸèƒ½, å®ƒèƒ½å¤Ÿä¿å­˜ä¸€ä¸ªæ¨¡å‹, å¹¶ä¸”å°†å®ƒåŠ è½½ä¸ºPyTorchæˆ–TensorFlowæ¨¡å‹. `from_pt`æˆ–`from_tf`å‚æ•°å¯ä»¥å°†æ¨¡å‹ä»ä¸€ä¸ªæ¡†æ¶è½¬æ¢ä¸ºå¦ä¸€ä¸ªæ¡†æ¶:

<frameworkcontent>
<pt>
```py
>>> from transformers import AutoModel

>>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
>>> pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)
```
</pt>
<tf>
```py
>>> from transformers import TFAutoModel

>>> tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)
```
</tf>
</frameworkcontent>

## è‡ªå®šä¹‰æ¨¡å‹æ„å»º

ä½ å¯ä»¥ä¿®æ”¹æ¨¡å‹çš„é…ç½®ç±»æ¥æ”¹å˜æ¨¡å‹çš„æ„å»ºæ–¹å¼. é…ç½®æŒ‡æ˜äº†æ¨¡å‹çš„å±æ€§, æ¯”å¦‚éšè—å±‚æˆ–è€…æ³¨æ„åŠ›å¤´çš„æ•°é‡. å½“ä½ ä»è‡ªå®šä¹‰çš„é…ç½®ç±»åˆå§‹åŒ–æ¨¡å‹æ—¶, ä½ å°±å¼€å§‹è‡ªå®šä¹‰æ¨¡å‹æ„å»ºäº†. æ¨¡å‹å±æ€§æ˜¯éšæœºåˆå§‹åŒ–çš„, ä½ éœ€è¦å…ˆè®­ç»ƒæ¨¡å‹, ç„¶åæ‰èƒ½å¾—åˆ°æœ‰æ„ä¹‰çš„ç»“æœ.

é€šè¿‡å¯¼å…¥[`AutoConfig`]æ¥å¼€å§‹, ä¹‹ååŠ è½½ä½ æƒ³ä¿®æ”¹çš„é¢„è®­ç»ƒæ¨¡å‹. åœ¨[`AutoConfig.from_pretrained`]ä¸­, ä½ èƒ½å¤ŸæŒ‡å®šæƒ³è¦ä¿®æ”¹çš„å±æ€§, æ¯”å¦‚æ³¨æ„åŠ›å¤´çš„æ•°é‡:

```py
>>> from transformers import AutoConfig

>>> my_config = AutoConfig.from_pretrained("distilbert-base-uncased", n_heads=12)
```

<frameworkcontent>
<pt>
ä½¿ç”¨[`AutoModel.from_config`]æ ¹æ®ä½ çš„è‡ªå®šä¹‰é…ç½®åˆ›å»ºä¸€ä¸ªæ¨¡å‹:

```py
>>> from transformers import AutoModel

>>> my_model = AutoModel.from_config(my_config)
```
</pt>
<tf>
ä½¿ç”¨[`TFAutoModel.from_config`]æ ¹æ®ä½ çš„è‡ªå®šä¹‰é…ç½®åˆ›å»ºä¸€ä¸ªæ¨¡å‹:

```py
>>> from transformers import TFAutoModel

>>> my_model = TFAutoModel.from_config(my_config)
```
</tf>
</frameworkcontent>

æŸ¥é˜…[åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰ç»“æ„](./create_a_model)æŒ‡å—è·å–æ›´å¤šå…³äºæ„å»ºè‡ªå®šä¹‰é…ç½®çš„ä¿¡æ¯.

## Trainer - PyTorchä¼˜åŒ–è®­ç»ƒå¾ªç¯

æ‰€æœ‰çš„æ¨¡å‹éƒ½æ˜¯æ ‡å‡†çš„[`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module), æ‰€ä»¥ä½ å¯ä»¥åœ¨ä»»ä½•å…¸å‹çš„è®­ç»ƒæ¨¡å‹ä¸­ä½¿ç”¨å®ƒä»¬. å½“ä½ ç¼–å†™è‡ªå·±çš„è®­ç»ƒå¾ªç¯æ—¶W, ğŸ¤— Transformersä¸ºPyTorchæä¾›äº†ä¸€ä¸ª[`Trainer`]ç±», å®ƒåŒ…å«äº†åŸºç¡€çš„è®­ç»ƒå¾ªç¯å¹¶ä¸”ä¸ºè¯¸å¦‚åˆ†å¸ƒå¼è®­ç»ƒ, æ··åˆç²¾åº¦ç­‰ç‰¹æ€§å¢åŠ äº†é¢å¤–çš„åŠŸèƒ½.

å–å†³äºä½ çš„ä»»åŠ¡, ä½ é€šå¸¸å¯ä»¥ä¼ é€’ä»¥ä¸‹çš„å‚æ•°ç»™[`Trainer`]:

1. [`PreTrainedModel`]æˆ–è€…[`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module):

   ```py
   >>> from transformers import AutoModelForSequenceClassification

   >>> model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")
   ```

2. [`TrainingArguments`]å«æœ‰ä½ å¯ä»¥ä¿®æ”¹çš„æ¨¡å‹è¶…å‚æ•°, æ¯”å¦‚å­¦ä¹ ç‡, æ‰¹æ¬¡å¤§å°å’Œè®­ç»ƒæ—¶çš„è¿­ä»£æ¬¡æ•°. å¦‚æœä½ æ²¡æœ‰æŒ‡å®šè®­ç»ƒå‚æ•°, é‚£ä¹ˆå®ƒä¼šä½¿ç”¨é»˜è®¤å€¼:

   ```py
   >>> from transformers import TrainingArguments

   >>> training_args = TrainingArguments(
   ...     output_dir="path/to/save/folder/",
   ...     learning_rate=2e-5,
   ...     per_device_train_batch_size=8,
   ...     per_device_eval_batch_size=8,
   ...     num_train_epochs=2,
   ... )
   ```

3. ä¸€ä¸ªé¢„å¤„ç†ç±», æ¯”å¦‚åˆ†è¯å™¨, ç‰¹å¾æå–å™¨æˆ–è€…å¤„ç†å™¨:

   ```py
   >>> from transformers import AutoTokenizer

   >>> tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
   ```

4. åŠ è½½ä¸€ä¸ªæ•°æ®é›†:

   ```py
   >>> from datasets import load_dataset

   >>> dataset = load_dataset("rotten_tomatoes")  # doctest: +IGNORE_RESULT
   ```

5. åˆ›å»ºä¸€ä¸ªç»™æ•°æ®é›†åˆ†è¯çš„å‡½æ•°, å¹¶ä¸”ä½¿ç”¨[`~datasets.Dataset.map`]åº”ç”¨åˆ°æ•´ä¸ªæ•°æ®é›†:

   ```py
   >>> def tokenize_dataset(dataset):
   ...     return tokenizer(dataset["text"])


   >>> dataset = dataset.map(tokenize_dataset, batched=True)
   ```

6. ç”¨æ¥ä»æ•°æ®é›†ä¸­åˆ›å»ºæ‰¹æ¬¡çš„[`DataCollatorWithPadding`]:

   ```py
   >>> from transformers import DataCollatorWithPadding

   >>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
   ```

ç°åœ¨æŠŠæ‰€æœ‰çš„ç±»ä¼ ç»™[`Trainer`]:

```py
>>> from transformers import Trainer

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=dataset["train"],
...     eval_dataset=dataset["test"],
...     tokenizer=tokenizer,
...     data_collator=data_collator,
... )  # doctest: +SKIP
```

ä¸€åˆ‡å‡†å¤‡å°±ç»ªå, è°ƒç”¨[`~Trainer.train`]è¿›è¡Œè®­ç»ƒ:

```py
>>> trainer.train()  # doctest: +SKIP
```

<Tip>

å¯¹äºåƒç¿»è¯‘æˆ–æ‘˜è¦è¿™äº›ä½¿ç”¨åºåˆ—åˆ°åºåˆ—æ¨¡å‹çš„ä»»åŠ¡, ç”¨[`Seq2SeqTrainer`]å’Œ[`Seq2SeqTrainingArguments`]æ¥æ›¿ä»£.

</Tip>

ä½ å¯ä»¥é€šè¿‡å­ç±»åŒ–[`Trainer`]ä¸­çš„æ–¹æ³•æ¥è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯. è¿™æ ·ä½ å°±å¯ä»¥è‡ªå®šä¹‰åƒæŸå¤±å‡½æ•°, ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨è¿™æ ·çš„ç‰¹æ€§. æŸ¥é˜…[`Trainer`]å‚è€ƒæ‰‹å†Œäº†è§£å“ªäº›æ–¹æ³•èƒ½å¤Ÿè¢«å­ç±»åŒ–. 

å¦ä¸€ä¸ªè‡ªå®šä¹‰è®­ç»ƒå¾ªç¯çš„æ–¹å¼æ˜¯é€šè¿‡[å›è°ƒ](./main_classes/callbacks). ä½ å¯ä»¥ä½¿ç”¨å›è°ƒæ¥ä¸å…¶ä»–åº“é›†æˆ, æŸ¥çœ‹è®­ç»ƒå¾ªç¯æ¥æŠ¥å‘Šè¿›åº¦æˆ–æå‰ç»“æŸè®­ç»ƒ. å›è°ƒä¸ä¼šä¿®æ”¹è®­ç»ƒå¾ªç¯. å¦‚æœæƒ³è‡ªå®šä¹‰æŸå¤±å‡½æ•°ç­‰, å°±éœ€è¦å­ç±»åŒ–[`Trainer`]äº†.

## ä½¿ç”¨Tensorflowè®­ç»ƒ

æ‰€æœ‰æ¨¡å‹éƒ½æ˜¯æ ‡å‡†çš„[`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model), æ‰€ä»¥ä½ å¯ä»¥é€šè¿‡[Keras](https://keras.io/) APIå®ç°åœ¨Tensorflowä¸­è®­ç»ƒ. ğŸ¤— Transformersæä¾›äº†[`~TFPreTrainedModel.prepare_tf_dataset`]æ–¹æ³•æ¥è½»æ¾åœ°å°†æ•°æ®é›†åŠ è½½ä¸º`tf.data.Dataset`, è¿™æ ·ä½ å°±å¯ä»¥ä½¿ç”¨Kerasçš„[`compile`](https://keras.io/api/models/model_training_apis/#compile-method)å’Œ[`fit`](https://keras.io/api/models/model_training_apis/#fit-method)æ–¹æ³•é©¬ä¸Šå¼€å§‹è®­ç»ƒ.

1. ä½¿ç”¨[`TFPreTrainedModel`]æˆ–è€…[`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)æ¥å¼€å§‹:

   ```py
   >>> from transformers import TFAutoModelForSequenceClassification

   >>> model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")
   ```

2. ä¸€ä¸ªé¢„å¤„ç†ç±», æ¯”å¦‚åˆ†è¯å™¨, ç‰¹å¾æå–å™¨æˆ–è€…å¤„ç†å™¨:

   ```py
   >>> from transformers import AutoTokenizer

   >>> tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
   ```

3. åˆ›å»ºä¸€ä¸ªç»™æ•°æ®é›†åˆ†è¯çš„å‡½æ•°

   ```py
   >>> def tokenize_dataset(dataset):
   ...     return tokenizer(dataset["text"])  # doctest: +SKIP
   ```

4. ä½¿ç”¨[`~datasets.Dataset.map`]å°†åˆ†è¯å™¨åº”ç”¨åˆ°æ•´ä¸ªæ•°æ®é›†, ä¹‹åå°†æ•°æ®é›†å’Œåˆ†è¯å™¨ä¼ ç»™[`~TFPreTrainedModel.prepare_tf_dataset`]. å¦‚æœä½ éœ€è¦çš„è¯, ä¹Ÿå¯ä»¥åœ¨è¿™é‡Œæ”¹å˜æ‰¹æ¬¡å¤§å°å’Œæ˜¯å¦æ‰“ä¹±æ•°æ®é›†:

   ```py
   >>> dataset = dataset.map(tokenize_dataset)  # doctest: +SKIP
   >>> tf_dataset = model.prepare_tf_dataset(
   ...     dataset, batch_size=16, shuffle=True, tokenizer=tokenizer
   ... )  # doctest: +SKIP
   ```

5. ä¸€åˆ‡å‡†å¤‡å°±ç»ªå, è°ƒç”¨`compile`å’Œ`fit`å¼€å§‹è®­ç»ƒ:

   ```py
   >>> from tensorflow.keras.optimizers import Adam

   >>> model.compile(optimizer=Adam(3e-5))
   >>> model.fit(dataset)  # doctest: +SKIP
   ```

## æ¥ä¸‹æ¥åšä»€ä¹ˆ?

ç°åœ¨ä½ å·²ç»å®Œæˆäº† ğŸ¤— Transformers çš„å¿«é€Ÿä¸Šæ‰‹æ•™ç¨‹, æ¥çœ‹çœ‹æˆ‘ä»¬çš„æŒ‡å—å¹¶ä¸”å­¦ä¹ å¦‚ä½•åšä¸€äº›æ›´å…·ä½“çš„äº‹æƒ…, æ¯”å¦‚å†™ä¸€ä¸ªè‡ªå®šä¹‰æ¨¡å‹, ä¸ºæŸä¸ªä»»åŠ¡å¾®è°ƒä¸€ä¸ªæ¨¡å‹ä»¥åŠå¦‚ä½•ä½¿ç”¨è„šæœ¬æ¥è®­ç»ƒæ¨¡å‹. å¦‚æœä½ æœ‰å…´è¶£äº†è§£æ›´å¤š ğŸ¤— Transformers çš„æ ¸å¿ƒç« èŠ‚, é‚£å°±å–æ¯å’–å•¡ç„¶åæ¥çœ‹çœ‹æˆ‘ä»¬çš„æ¦‚å¿µæŒ‡å—å§!
