<!--Copyright 2024 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Best Practices for Generation with Cache
# ä½¿ç”¨ç¼“å­˜è¿›è¡Œç”Ÿæˆçš„æœ€ä½³å®è·µ

Efficient caching is crucial for optimizing the performance of models in various generative tasks,
including text generation, translation, summarization and other transformer-based applications.
Effective caching helps reduce computation time and improve response rates, especially in real-time or resource-intensive applications.

åœ¨å„ç§ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œé«˜æ•ˆç¼“å­˜å¯¹äºä¼˜åŒ–çš„æ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ï¼Œè¿™äº›ä»»åŠ¡åŒ…æ‹¬æ–‡æœ¬ç”Ÿæˆã€ç¿»è¯‘ã€æ‘˜è¦ä»¥åŠå…¶ä»–åŸºäºTransformerçš„åº”ç”¨ã€‚æœ‰æ•ˆçš„ç¼“å­˜æœ‰åŠ©äºå‡å°‘è®¡ç®—æ—¶é—´ï¼Œå¹¶æé«˜å“åº”é€Ÿåº¦ï¼Œå°¤å…¶æ˜¯åœ¨å®æ—¶æˆ–èµ„æºå¯†é›†å‹çš„åº”ç”¨ã€‚

Transformers support various caching methods, leveraging "Cache" classes to abstract and manage the caching logic.
This document outlines best practices for using these classes to maximize performance and efficiency.
Check out all the available `Cache` classes in the [API documentation](./internal/generation_utils).

Transformersæ”¯æŒå¤šç§ç¼“å­˜æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨â€œCacheâ€ç±»æ¥æŠ½è±¡å’Œç®¡ç†ç¼“å­˜é€»è¾‘ã€‚
æœ¬æ–‡æ¡£æ¦‚è¿°äº†ä½¿ç”¨è¿™äº›ç±»ä»¥å®ç°æœ€ä½³æ€§èƒ½å’Œæ•ˆç‡çš„æœ€ä½³å®è·µã€‚
æœ‰å…³æ‰€æœ‰å¯ç”¨çš„`Cache`ç±»ï¼Œè¯·æŸ¥çœ‹[APIæ–‡æ¡£](./internal/generation_utils)ã€‚

## What is Cache and why we should care?
## ä»€ä¹ˆæ˜¯ç¼“å­˜ï¼Œä»¥åŠæˆ‘ä»¬ä¸ºä½•éœ€è¦å…³æ³¨å®ƒï¼Ÿ

Imagine youâ€™re having a conversation with someone, and instead of remembering what was said previously, you have to start from scratch every time you respond. This would be slow and inefficient, right? In the world of Transformer models, a similar concept applies, and that's where Caching keys and values come into play. From now on, I'll refer to the concept as KV Cache.

æƒ³è±¡ä¸€ä¸‹ï¼Œä½ æ­£åœ¨ä¸æŸäººè¿›è¡Œå¯¹è¯ï¼Œè€Œæ¯æ¬¡å›åº”æ—¶ï¼Œä½ ä¸å¾—ä¸ä»å¤´å¼€å§‹å›å¿†ä¹‹å‰è¯´è¿‡çš„è¯ã€‚è¿™ç§æ–¹å¼æ—¢ç¼“æ…¢åˆä½æ•ˆï¼Œå¯¹å§ï¼Ÿåœ¨Transformeræ¨¡å‹çš„ä¸–ç•Œä¸­ï¼Œä¹Ÿå­˜åœ¨ç±»ä¼¼çš„æ¦‚å¿µï¼Œè¿™å°±å¼•å…¥äº†ç¼“å­˜ keys å’Œ values çš„é‡è¦æ€§ã€‚ä»ç°åœ¨èµ·ï¼Œæˆ‘å°†è¿™ä¸€æ¦‚å¿µç§°ä¸ºKVç¼“å­˜ï¼ˆKV Cacheã€key-value cacheï¼‰ã€‚

KV cache is needed to optimize the generation in autoregressive models, where the model predicts text token by token. This process can be slow since the model can generate only one token at a time, and each new prediction is dependent on the previous context. That means, to predict token number 1000 in the generation, you need information from the previous 999 tokens, which comes in the form of some matrix multiplications across the representations of those tokens. But to predict token number 1001, you also need the same information from the first 999 tokens, plus additional information from token number 1000. That is where key-value cache is used to optimize the sequential generation process by storing previous calculations to reuse in subsequent tokens, so they don't need to be computed again.

KV ç¼“å­˜çš„å­˜åœ¨æ˜¯ä¸ºäº†ä¼˜åŒ–è‡ªå›å½’æ¨¡å‹ä¸­çš„æ–‡æœ¬ç”Ÿæˆçš„è¿‡ç¨‹ï¼Œè¿™äº›æ¨¡å‹é€ä¸ªé¢„æµ‹æ–‡æœ¬çš„tokenã€‚ç”±äºæ¨¡å‹ä¸€æ¬¡åªèƒ½ç”Ÿæˆä¸€ä¸ªtokenï¼Œä¸”æ¯ä¸ªæ–°çš„é¢„æµ‹éƒ½ä¾èµ–äºä¹‹å‰çš„ä¸Šä¸‹æ–‡ï¼Œè¿™ä¸€è¿‡ç¨‹å¯èƒ½ç›¸å½“ç¼“æ…¢ã€‚è¿™æ„å‘³ç€ï¼Œè¦é¢„æµ‹ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ç¬¬1000ä¸ªtokenï¼Œä½ éœ€è¦å‰999ä¸ªtokençš„ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯ä»¥çŸ©é˜µä¹˜æ³•çš„æ–¹å¼é€šè¿‡é‚£äº›tokençš„è¡¨ç¤ºä¼ é€’è¿‡æ¥ã€‚ç„¶è€Œï¼Œè¦é¢„æµ‹ç¬¬1001ä¸ªtokenæ—¶ï¼Œä½ åŒæ ·éœ€è¦å‰999ä¸ªtokençš„ä¿¡æ¯ï¼Œå†åŠ ä¸Šç¬¬1000ä¸ªtokençš„é¢å¤–ä¿¡æ¯ã€‚æ­£æ˜¯åœ¨è¿™ä¸€ç¯èŠ‚ï¼Œkey-value ç¼“å­˜ æ´¾ä¸Šäº†ç”¨åœºï¼Œå®ƒé€šè¿‡å­˜å‚¨ä¹‹å‰çš„è®¡ç®—ç»“æœï¼Œä»¥ä¾¿åœ¨åç»§tokenä¸­å¤ç”¨ï¼Œä»è€Œé¿å…äº†é‡å¤è®¡ç®—ï¼Œä¼˜åŒ–äº†åºåˆ—ç”Ÿæˆè¿‡ç¨‹ã€‚

More concretely, key-value cache acts as a memory bank for these generative models, where the model stores key-value pairs derived from self-attention layers for previously processed tokens. By storing this information, the model can avoid redundant computations and instead retrieve keys and values of previous tokens from the cache. Note that caching can be used only in inference and should be disabled when training, otherwise it might cause unexpected errors.

æ›´ä¸ºå…·ä½“åœ°è¯´ï¼Œkey-value ç¼“å­˜ æ‰®æ¼”äº†ç”Ÿæˆæ¨¡å‹è®°å¿†åº“çš„è§’è‰²ï¼Œæ¨¡å‹åœ¨æ­¤å­˜å‚¨äº†ä»å‰å¤„ç†è¿‡çš„tokené€šè¿‡è‡ªæ³¨æ„åŠ›å±‚æå–çš„ key-value å¯¹ã€‚é€šè¿‡ä¿å­˜è¿™äº›ä¿¡æ¯ï¼Œæ¨¡å‹èƒ½å¤Ÿè§„é¿å†—ä½™è®¡ç®—ï¼Œç›´æ¥ä»ç¼“å­˜ä¸­æ£€ç´¢å…ˆå‰tokençš„keys å’Œ values å³å¯ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç¼“å­˜æœºåˆ¶ä»…é€‚ç”¨äºæ¨ç†é˜¶æ®µï¼Œåœ¨è®­ç»ƒæ—¶åº”å°†å…¶å…³é—­ï¼Œå¦åˆ™å¯èƒ½å¯¼è‡´æ„æƒ³ä¸åˆ°çš„é”™è¯¯ã€‚

<details>
  <summary><em>å¯¹äºé‚£äº›çƒ­è¡·äºæ·±å…¥æ¢ç©¶çš„å¥½å¥‡å¿ƒæœ‹å‹ä»¬ For the Curious Minds Who Like to Dive Deep</em></summary>

  ### æ­ç§˜ï¼šç¼“å­˜å¯¹è±¡åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„è¿ä½œåŸç† Under the Hood: How Cache Object Works in Attention Mechanism

  When utilizing a cache object in the input, the Attention module performs several critical steps to integrate past and present information seamlessly.
  åœ¨ä½¿ç”¨è¾“å…¥å‚æ•°ä¸­çš„ç¼“å­˜å¯¹è±¡æ—¶ï¼Œæ³¨æ„åŠ›æ¨¡å—ä¼šæ‰§è¡Œå‡ ä¸ªå…³é”®æ­¥éª¤ï¼Œä»¥å®ç°è¿‡å»ä¸å½“å‰ä¿¡æ¯çš„æ— ç¼æ•´åˆã€‚

  The Attention module concatenates the current key-values with the past key-values stored in the cache. This results in attention weights of shape `(new_tokens_length, past_kv_length + new_tokens_length)`. Essentially, the past and current key-values are combined to compute attention scores, ensuring that the model considers both previous context and new input. The concatenated key-values are used to compute the attention scores resulting in attention weights of shape `(new_tokens_length, past_kv_length + new_tokens_length)`.
  æ³¨æ„åŠ›æ¨¡å—å°†å½“å‰çš„ key-values ä¸ç¼“å­˜ä¸­å­˜å‚¨çš„è¿‡å»çš„ key-values è¿›è¡Œè¿æ¥ã€‚è¿™ä¼šäº§ç”Ÿå½¢çŠ¶ä¸º `(new_tokens_length, past_kv_length + new_tokens_length)` çš„æ³¨æ„åŠ›æƒé‡ã€‚æœ¬è´¨ä¸Šï¼Œè¿‡å»å’Œå½“å‰çš„ key-values ä¼šè¢«ç»“åˆç”¨æ¥è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼Œç¡®ä¿æ¨¡å‹åŒæ—¶è€ƒè™‘å…ˆå‰çš„ä¸Šä¸‹æ–‡å’Œæ–°è¾“å…¥ã€‚è¿æ¥åçš„ key-values ç”¨äºè®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼Œä»è€Œå¾—åˆ°å½¢çŠ¶ä¸º `(new_tokens_length, past_kv_length + new_tokens_length)` çš„æ³¨æ„åŠ›æƒé‡ã€‚

  Therefore, when iteratively calling `forward()` instead of the `generate()` method, itâ€™s crucial to ensure that the attention mask shape matches the combined length of past and current key-values. The attention mask should have the shape `(batch_size, past_kv_length + new_tokens_length)`. This is usually handled internally when you call `generate()` method. If you want to implement your own generation loop with Cache classes, take this into consideration and prepare the attention mask to hold values to current and past tokens.
  å› æ­¤ï¼Œé€æ¬¡è°ƒç”¨ `forward()` æ–¹æ³•è€Œä¸æ˜¯ `generate()` æ–¹æ³•æ—¶ï¼Œç¡®ä¿æ³¨æ„åŠ› mask çš„å½¢çŠ¶ä¸è¿‡å»å’Œå½“å‰ key-values çš„æ€»é•¿åº¦åŒ¹é…è‡³å…³é‡è¦ã€‚æ³¨æ„åŠ› mask åº”å…·æœ‰å½¢çŠ¶ `(batch_size, past_kv_length + new_tokens_length)`ã€‚é€šå¸¸åœ¨è°ƒç”¨ `generate()` æ–¹æ³•æ—¶ä¼šç”±å†…éƒ¨å¤„ç†ã€‚å¦‚æœä½ æƒ³è‡ªå·±å®ç°ç”Ÿæˆå¾ªç¯å¹¶ä½¿ç”¨ Cache ç±»ï¼Œéœ€è¦è€ƒè™‘è¿™ä¸€ç‚¹ï¼Œå¹¶å‡†å¤‡å¥½æ³¨æ„åŠ› mask ä»¥å®¹çº³å½“å‰å’Œè¿‡å» token çš„å€¼ã€‚

  <Tip warning={true}>

  One important concept you need to know when writing your own generation loop, is `cache_position`. In case you want to reuse an already filled Cache object by calling `forward()`, you have to pass in a valid `cache_position` which will indicate the positions of inputs in the sequence. Note that `cache_position` is not affected by padding, and always adds one more position for each token. For example, if key/value cache contains 10 tokens (no matter how many of it is a pad token), the cache position for the next token should be `torch.tensor([10])`.
  å½“ä½ è‡ªå·±å®ç°ç”Ÿæˆå¾ªç¯æ—¶ï¼Œéœ€è¦äº†è§£ä¸€ä¸ªé‡è¦çš„æ¦‚å¿µâ€”â€”`cache_position`ã€‚å¦‚æœä½ å¸Œæœ›é€šè¿‡è°ƒç”¨ `forward()` æ–¹æ³•é‡ç”¨å·²ç»å¡«å……çš„ Cache å¯¹è±¡ï¼Œéœ€è¦ä¼ é€’ä¸€ä¸ªæœ‰æ•ˆçš„ `cache_position`ï¼Œè¿™å°†æŒ‡ç¤ºåºåˆ—ä¸­è¾“å…¥çš„ä½ç½®ã€‚è¯·æ³¨æ„ï¼Œ`cache_position` ä¸å—å¡«å……å½±å“ï¼Œå¹¶ä¸”æ¯æ–°å¢ä¸€ä¸ª token æ€»ä¼šå¢åŠ ä¸€ä¸ªä½ç½®ã€‚ä¾‹å¦‚ï¼Œå¦‚æœkey-values ç¼“å­˜åŒ…å« 10 ä¸ª tokenï¼ˆæ— è®ºå…¶ä¸­æœ‰å¤šå°‘ä¸ªæ˜¯å¡«å…… tokenï¼‰ï¼Œä¸‹ä¸€ä¸ª token çš„ç¼“å­˜ä½ç½®åº”ä¸º `torch.tensor([10])`ã€‚

  </Tip>


  See an example below for how to implement your own generation loop.
  ä¸‹é¢æ˜¯ä¸€ä¸ªå¦‚ä½•å®ç°è‡ªå·±ç”Ÿæˆå¾ªç¯çš„ç¤ºä¾‹ã€‚

  ```python
  >>> import torch
  >>> from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache

  >>> model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  >>> model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map="auto")
  >>> tokenizer = AutoTokenizer.from_pretrained(model_id)

  >>> past_key_values = DynamicCache()
  >>> messages = [{"role": "user", "content": "Hello, what's your name."}]
  >>> inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt", return_dict=True).to(model.device)

  >>> generated_ids = inputs.input_ids
  >>> cache_position = torch.arange(inputs.input_ids.shape[1], dtype=torch.int64, device=model.device)
  >>> max_new_tokens = 10

  >>> for _ in range(max_new_tokens):
  ...     outputs = model(**inputs, cache_position=cache_position, past_key_values=past_key_values, use_cache=True)
  ...     # Greedily sample one next token
  ...     next_token_ids = outputs.logits[:, -1:].argmax(-1)
  ...     generated_ids = torch.cat([generated_ids, next_token_ids], dim=-1)
  ...
  ...     # Prepare inputs for the next generation step by leaaving unprocessed tokens, in our case we have only one new token
  ...     # and expanding attn mask for the new token, as explained above
  ...     attention_mask = inputs["attention_mask"]
  ...     attention_mask = torch.cat([attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1)
  ...     inputs = {"input_ids": next_token_ids, "attention_mask": attention_mask}
  ...     cache_position = cache_position[-1:] + 1 # add one more position for the next token

  >>> print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])
  ```
  ```txt
  <|user|>
  Hello, what's your name. 
  <|assistant|>
  My name is Sarah. 
  <|
  ```

</details>



## Generate with Cache
## ä½¿ç”¨ Cache ç”Ÿæˆ

In ğŸ¤— Transformers, we support various Cache types to optimize the performance across different models and tasks. By default, all models generate with caching,
åœ¨ ğŸ¤— Transformers ä¸­ï¼Œæˆ‘ä»¬æ”¯æŒå„ç§ Cache ç±»å‹ä»¥åœ¨ä¸åŒæ¨¡å‹å’Œä»»åŠ¡ä¸­ä¼˜åŒ–æ€§èƒ½ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæ‰€æœ‰æ¨¡å‹éƒ½ä¼šä½¿ç”¨ç¼“å­˜è¿›è¡Œç”Ÿæˆï¼Œ
with the [`~DynamicCache`] class being the default cache for most models. It allows us to dynamically grow cache size, by saving more and more keys and values as we generate. If for some reason you don't want to use caches, you can pass `use_cache=False` into the `generate()` method.
å…¶ä¸­ [`~DynamicCache`] ç±»æ˜¯å¤§å¤šæ•°æ¨¡å‹çš„é»˜è®¤ç¼“å­˜ã€‚å®ƒå…è®¸æˆ‘ä»¬åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­åŠ¨æ€å¢åŠ ç¼“å­˜å¤§å°ï¼Œé€šè¿‡ä¿å­˜æ›´å¤šçš„é”®å’Œå€¼ã€‚å¦‚æœä½ å‡ºäºæŸäº›åŸå› ä¸æƒ³ä½¿ç”¨ç¼“å­˜ï¼Œå¯ä»¥å°† `use_cache=False` ä¼ é€’ç»™ `generate()` æ–¹æ³•ã€‚

Refer to the table below to see the difference between cache types and choose the one that suits best for your use-case. Models for which initialization is recommended should be initialized before calling the model and passed to model as a kwarg. In all other cases you can simply define desired `cache_implementation` and we take care of the rest for you.
å‚è§ä¸‹è¡¨ï¼Œäº†è§£ä¸åŒç¼“å­˜ç±»å‹ä¹‹é—´çš„å·®å¼‚ï¼Œå¹¶æ ¹æ®ä½ çš„éœ€æ±‚é€‰æ‹©æœ€é€‚åˆçš„ç±»å‹ã€‚å»ºè®®åœ¨åˆå§‹åŒ–çš„æ¨¡å‹åº”åœ¨è°ƒç”¨æ¨¡å‹ä¹‹å‰è¿›è¡Œåˆå§‹åŒ–ï¼Œå¹¶ä½œä¸ºå…³é”®å­—å‚æ•°ä¼ é€’ç»™æ¨¡å‹ã€‚åœ¨å…¶ä»–æ‰€æœ‰æƒ…å†µä¸‹ï¼Œä½ å¯ä»¥ç›´æ¥å®šä¹‰æ‰€éœ€çš„ `cache_implementation`ï¼Œæˆ‘ä»¬ä¼šä¸ºä½ å¤„ç†å…¶ä½™éƒ¨åˆ†ã€‚

| Cache ç±»å‹             | æ˜¯å¦å†…å­˜é«˜æ•ˆ | æ˜¯å¦æ”¯æŒ torch.compile() | æ˜¯å¦éœ€è¦åˆå§‹åŒ– | å»¶è¿Ÿ | æ˜¯å¦æ”¯æŒé•¿å†…å®¹ç”Ÿæˆ |
|------------------------|------------------|--------------------------|----------------------------|---------|-------------------------|
| åŠ¨æ€ç¼“å­˜ Dynamic Cache     | No               | No                       | No                         | Mid     | No                      |
| é™æ€ç¼“å­˜ Static Cache      | No               | Yes                      | Yes                        | High    | No                      |
| å¯å¸è½½çš„ç¼“å­˜ Offloaded Cache | Yes              | No                       | No                         | Low     | Yes                     |
| å¯å¸è½½çš„é™æ€ç¼“å­˜ Offloaded Static Cache | No               | Yes                      | Yes                        | High    | Yes                     |
| å¯é‡åŒ–çš„ç¼“å­˜ Quantized Cache | Yes              | No                       | No                         | Low     | Yes                     |
| æ»‘åŠ¨çª—å£ç¼“å­˜ Sliding Window Cache | No               | Yes                      | Yes                        | High    | No                      |
| ä¸‹æ²‰ç¼“å­˜ Sink Cache        | Yes              | No                       | Yes                        | Mid     | Yes                     |

These cache classes can be set with a `cache_implementation` argument when generating. To learn about the available options for the cache_implementation flag, please refer to the [API Documentation](./main_classes/text_generation#transformers.GenerationConfig). Now, let's explore each cache type in detail and see how to use them. Note that the below examples are for decoder-only Tranformer-based models. We also support ["Model-Specific Cache"] classes for models such as Mamba or Jamba, keep reading for more details.
è¿™äº›ç¼“å­˜ç±»å¯ä»¥åœ¨ç”Ÿæˆæ—¶é€šè¿‡ `cache_implementation` å‚æ•°è®¾ç½®ã€‚è¦äº†è§£ `cache_implementation` æ ‡è®°çš„å¯ç”¨é€‰é¡¹ï¼Œè¯·å‚é˜… [API æ–‡æ¡£](./main_classes/text_generation#transformers.GenerationConfig)ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬è¯¦ç»†æ¢è®¨æ¯ç§ç¼“å­˜ç±»å‹ï¼Œå¹¶çœ‹çœ‹å¦‚ä½•ä½¿ç”¨å®ƒä»¬ã€‚è¯·æ³¨æ„ï¼Œä»¥ä¸‹ç¤ºä¾‹é€‚ç”¨äºåŸºäºdecoder-onlyçš„ Transformer æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜æ”¯æŒä¸º Mamba æˆ– Jamba ç­‰æ¨¡å‹çš„ç‰¹å®šç¼“å­˜ç±»ï¼Œç»§ç»­é˜…è¯»ä»¥è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚

### Quantized Cache
### å¯é‡åŒ–çš„ç¼“å­˜

The key and value cache can occupy a large portion of memory, becoming a [bottleneck for long-context generation](https://huggingface.co/blog/llama31#inference-memory-requirements), especially for Large Language Models.
Quantizing the cache when using `generate()` can significantly reduce memory requirements at the cost of speed.
Key å’Œ value çš„ç¼“å­˜å¯èƒ½ä¼šå æ®å¤§é‡å†…å­˜ï¼Œæˆä¸ºé•¿ä¸Šä¸‹æ–‡ç”Ÿæˆçš„ç“¶é¢ˆï¼Œå°¤å…¶æ˜¯å¯¹äºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚åœ¨ä½¿ç”¨ `generate()` æ—¶é‡åŒ–ç¼“å­˜å¯ä»¥æ˜¾è‘—å‡å°‘å†…å­˜éœ€æ±‚ï¼Œä½†ä»£ä»·æ˜¯é€Ÿåº¦å˜æ…¢ã€‚
åœ¨ä½¿ç”¨ `generate()` æ—¶é‡åŒ–ç¼“å­˜å¯ä»¥æ˜¾è‘—å‡å°‘å†…å­˜éœ€æ±‚ï¼Œä½†ä»£ä»·æ˜¯é€Ÿåº¦å˜æ…¢ã€‚

KV Cache quantization in `transformers` is largely inspired by the paper ["KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache"](https://arxiv.org/abs/2402.02750) and currently supports [`~QuantoQuantizedCache`] and [`~HQQQuantizedCache`] classes. For more information on the inner workings see the paper.
`transformers` ä¸­çš„ KV ç¼“å­˜é‡åŒ–ä¸»è¦å—åˆ°äº†è®ºæ–‡ ["KIVI: ä¸€ç§æ— éœ€è°ƒä¼˜çš„å¼‚æ„ 2 ä½é‡åŒ–æ–¹æ³•ç”¨äº KV ç¼“å­˜"](https://arxiv.org/abs/2402.02750) çš„å¯å‘ï¼Œå¹¶å½“å‰æ”¯æŒ `~QuantoQuantizedCache` å’Œ `~HQQQuantizedCache` ç±»ã€‚æœ‰å…³å†…éƒ¨æœºåˆ¶çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…è¯¥è®ºæ–‡ã€‚

To enable quantization of the key-value cache, one needs to indicate `cache_implementation="quantized"` in the `generation_config`.
Quantization related arguments should be passed to the `generation_config` either as a `dict` or an instance of a [`~QuantizedCacheConfig`] class.
One has to indicate which quantization backend to use in the [`~QuantizedCacheConfig`], the default is `quanto`.
è¦å¯ç”¨ KV ç¼“å­˜çš„é‡åŒ–ï¼Œéœ€è¦åœ¨ `generation_config` ä¸­æŒ‡å®š `cache_implementation="quantized"`ã€‚
ä¸é‡åŒ–ç›¸å…³çš„å‚æ•°åº”è¯¥é€šè¿‡ `dict` æˆ–è€… `~QuantizedCacheConfig` ç±»çš„å®ä¾‹ä¼ é€’ç»™ `generation_config`ã€‚
éœ€è¦åœ¨ `~QuantizedCacheConfig` ä¸­æŒ‡å®šä½¿ç”¨çš„é‡åŒ–åç«¯ï¼Œé»˜è®¤æ˜¯ `quanto`ã€‚

It is recommended to set `axis-key/axis-value` parameters in the cache config to `0` if you're using the `quanto` backend and to `1` if you're using the `HQQ` backend. For other config values, please use the defaults unless you're running out of memory. In that case, you may consider decreasing the residual length.
å¦‚æœä½¿ç”¨ `quanto` åç«¯ï¼Œå»ºè®®åœ¨ç¼“å­˜é…ç½®ä¸­å°† `axis-key` å’Œ `axis-value` å‚æ•°è®¾ç½®ä¸º `0`ï¼›å¦‚æœä½¿ç”¨ `HQQ` åç«¯ï¼Œåˆ™åº”å°†è¿™äº›å‚æ•°è®¾ç½®ä¸º `1`ã€‚å¯¹äºå…¶ä»–é…ç½®å‚æ•°ï¼Œè¯·ä½¿ç”¨é»˜è®¤å€¼ï¼Œé™¤éè¿è¡Œæ—¶å‡ºç°å†…å­˜ä¸è¶³çš„æƒ…å†µã€‚é‡åˆ°è¿™ç§æƒ…å†µï¼Œæ‚¨å¯ä»¥è€ƒè™‘å‡å°‘å‰©ä½™é•¿åº¦ã€‚

<Tip warning={true}>

Cache quantization can be detrimental in terms of latency if the context length is short and there is enough GPU VRAM available to run without cache quantization. It is recommended to seek balance between memory efficiency and latency.
å¦‚æœä¸Šä¸‹æ–‡é•¿åº¦è¾ƒçŸ­ä¸”æœ‰è¶³å¤Ÿçš„ GPU VRAM å¯ä»¥åœ¨æ— éœ€é‡åŒ–ç¼“å­˜çš„æƒ…å†µä¸‹è¿è¡Œï¼Œé‚£ä¹ˆç¼“å­˜é‡åŒ–å¯èƒ½ä¼šå› å»¶è¿Ÿå¢åŠ è€Œå˜å¾—ä¸åˆ©ã€‚å»ºè®®åœ¨å†…å­˜æ•ˆç‡å’Œå»¶è¿Ÿä¹‹é—´å¯»æ±‚å¹³è¡¡ã€‚
</Tip>


```python
>>> import torch
>>> from transformers import AutoTokenizer, AutoModelForCausalLM

>>> tokenizer = AutoTokenizer.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0")
>>> model = AutoModelForCausalLM.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0", torch_dtype=torch.float16, device_map="auto")
>>> inputs = tokenizer("I like rock music because", return_tensors="pt").to(model.device)

>>> out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation="quantized", cache_config={"nbits": 4, "backend": "quanto"})
>>> print(tokenizer.batch_decode(out, skip_special_tokens=True)[0])
I like rock music because it's a great way to express myself. I like the way it makes me feel, the
```

### Offloaded Cache
## å¯å¸è½½çš„ç¼“å­˜

Similarly to KV cache quantization, [`~OffloadedCache`] strategy aims to reduce GPU VRAM usage.
It does so by moving the KV cache for most layers to the CPU.
As the model's `forward()` method iterates over the layers, this strategy maintains the current layer cache on the GPU.
At the same time it asynchronously prefetches the next layer cache as well as sending the previous layer cache back to the CPU.
Unlike KV cache quantization, this strategy always produces the same result as the default KV cache implementation.
Thus, it can serve as a drop-in replacement or a fallback for it.
ç±»ä¼¼åœ°ï¼Œ`~OffloadedCache` ç­–ç•¥æ—¨åœ¨å‡å°‘ GPU VRAM çš„ä½¿ç”¨é‡ã€‚å®ƒé€šè¿‡å°†å¤§éƒ¨åˆ†å±‚çš„ KV ç¼“å­˜ç§»åˆ° CPU æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚éšç€æ¨¡å‹çš„ `forward()` æ–¹æ³•éå†å„å±‚ï¼Œè¯¥ç­–ç•¥å°†åœ¨ GPU ä¸Šç»´æŠ¤å½“å‰å±‚çš„ç¼“å­˜ï¼Œå¹¶å¼‚æ­¥é¢„å–ä¸‹ä¸€å±‚çš„ç¼“å­˜ï¼ŒåŒæ—¶å°†ä¸Šä¸€å±‚çš„ç¼“å­˜å‘é€å› CPUã€‚ä¸ KV ç¼“å­˜é‡åŒ–ä¸åŒï¼Œè¯¥ç­–ç•¥å§‹ç»ˆä¼šäº§ç”Ÿä¸é»˜è®¤ KV ç¼“å­˜å®ç°ç›¸åŒçš„ç»“æœï¼Œå› æ­¤å¯ä»¥ä½œä¸ºé»˜è®¤å®ç°çš„ç›´æ¥æ›¿æ¢æˆ–åå¤‡é€‰é¡¹ã€‚

Depending on your model and the characteristics of your generation task (size of context, number of generated tokens, number of beams, etc.)
you may notice a small degradation in generation throughput compared to the default KV cache implementation.
æ ¹æ®æ‚¨çš„æ¨¡å‹å’Œç”Ÿæˆä»»åŠ¡çš„ç‰¹ç‚¹ï¼ˆä¸Šä¸‹æ–‡é•¿åº¦ã€ç”Ÿæˆçš„tokensæ•°é‡ã€beam æ•°é‡ç­‰ï¼‰ï¼Œä¸é»˜è®¤ KV ç¼“å­˜å®ç°ç›¸æ¯”ï¼Œæ‚¨å¯èƒ½ä¼šæ³¨æ„åˆ°ç”Ÿæˆååé‡ç•¥æœ‰ä¸‹é™ã€‚

To enable KV cache offloading, pass `cache_implementation="offloaded"` in the `generation_config` or directly to the `generate()` call.
Use `cache_implementation="offloaded_static"` for an offloaded static cache (see also [Offloaded Static Cache](#offloaded-static-cache) below).
è¦å¯ç”¨ KV ç¼“å­˜å¸è½½ï¼Œè¯·åœ¨ `generation_config` ä¸­æˆ–ç›´æ¥åœ¨ `generate()` è°ƒç”¨ä¸­ä¼ é€’ `cache_implementation="offloaded"`ã€‚è¦ä½¿ç”¨å¯å¸è½½çš„é™æ€ KV ç¼“å­˜ï¼Œè¯·ä¼ é€’ `cache_implementation="offloaded_static"`ï¼ˆå‚è§ä¸‹æ–¹çš„ [å¯å¸è½½çš„é™æ€ç¼“å­˜](#offloaded-static-cache)ï¼‰ã€‚

```python
>>> import torch
>>> from transformers import AutoTokenizer, AutoModelForCausalLM
>>> ckpt = "microsoft/Phi-3-mini-4k-instruct"

>>> tokenizer = AutoTokenizer.from_pretrained(ckpt)
>>> model = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16, device_map="auto")
>>> inputs = tokenizer("Fun fact: The shortest", return_tensors="pt").to(model.device)

>>> out = model.generate(**inputs, do_sample=False, max_new_tokens=23, cache_implementation="offloaded")
>>> print(tokenizer.batch_decode(out, skip_special_tokens=True)[0])
Fun fact: The shortest war in history was between Britain and Zanzibar on August 27, 1896.

>>> out = model.generate(**inputs, do_sample=False, max_new_tokens=23)
>>> print(tokenizer.batch_decode(out, skip_special_tokens=True)[0])
Fun fact: The shortest war in history was between Britain and Zanzibar on August 27, 1896.
```

<Tip warning={true}>

Cache offloading requires a CUDA GPU and can be slower than dynamic KV cache. Use it if you are getting CUDA out of memory errors.
ç¼“å­˜å¸è½½éœ€è¦ CUDA GPUï¼Œå¹¶ä¸”å¯èƒ½æ¯”åŠ¨æ€ KV ç¼“å­˜æ…¢ã€‚å¦‚æœæ‚¨é‡åˆ° CUDA å†…å­˜ä¸è¶³é”™è¯¯ï¼Œè¯·ä½¿ç”¨å®ƒã€‚

</Tip>

The example below shows how KV cache offloading can be used as a fallback strategy.
ä»¥ä¸‹ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨KVç¼“å­˜å¸è½½ä½œä¸ºå¤‡ç”¨ç­–ç•¥ã€‚

```python
>>> import torch
>>> from transformers import AutoTokenizer, AutoModelForCausalLM
>>> def resilient_generate(model, *args, **kwargs):
...     oom = False
...     try:
...         return model.generate(*args, **kwargs)
...     except torch.cuda.OutOfMemoryError as e:
...         print(e)
...         print("retrying with cache_implementation='offloaded'")
...         oom = True
...     if oom:
...         torch.cuda.empty_cache()
...         kwargs["cache_implementation"] = "offloaded"
...         return model.generate(*args, **kwargs)
...
...
>>> ckpt = "microsoft/Phi-3-mini-4k-instruct"
>>> tokenizer = AutoTokenizer.from_pretrained(ckpt)
>>> model = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16).to("cuda:0")
>>> prompt = ["okay "*1000 + "Fun fact: The most"]
>>> inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
>>> beams = { "num_beams": 40, "num_beam_groups": 40, "num_return_sequences": 40, "diversity_penalty": 1.0, "max_new_tokens": 23, "early_stopping": True, }
>>> out = resilient_generate(model, **inputs, **beams)
>>> responses = tokenizer.batch_decode(out[:,-28:], skip_special_tokens=True)
```

On a GPU with 50 GB of RAM, running this code will print
åœ¨æ‹¥æœ‰50 GBå†…å­˜çš„GPUä¸Šï¼Œè¿è¡Œæ­¤ä»£ç ï¼Œåœ¨æˆåŠŸç”Ÿæˆ40ä¸ªbeamsä¹‹å‰å°†è¾“å‡º

```
CUDA out of memory. Tried to allocate 4.83 GiB. GPU
retrying with cache_implementation='offloaded'
```
### Static Cache
## é™æ€ç¼“å­˜

Since the "DynamicCache" dynamically grows with each generation step, it prevents you from taking advantage of JIT optimizations. The [`~StaticCache`] pre-allocates
a specific maximum size for the keys and values, allowing you to generate up to the maximum length without having to modify cache size. Check the below usage example.

ç”±äºâ€œåŠ¨æ€ç¼“å­˜â€åœ¨æ¯ä¸€æ­¥ç”Ÿæˆè¿‡ç¨‹ä¸­åŠ¨æ€å¢é•¿ï¼Œè¿™ä¼šå¦¨ç¢ä½ å……åˆ†åˆ©ç”¨å³æ—¶ç¼–è¯‘ï¼ˆJITï¼‰ä¼˜åŒ–çš„ä¼˜åŠ¿ã€‚è€Œ[`~StaticCache`]åˆ™é¢„å…ˆåˆ†é…äº†keyså’Œvaluesçš„æœ€å¤§ç©ºé—´ï¼Œä½¿å¾—åœ¨ç”Ÿæˆè‡³æœ€å¤§é•¿åº¦å‰æ— éœ€è°ƒæ•´ç¼“å­˜å¤§å°ã€‚è¯·å‚é˜…ä»¥ä¸‹ä½¿ç”¨ç¤ºä¾‹ã€‚

For more examples with Static Cache and JIT compilation, take a look at [StaticCache & torchcompile](./llm_optims#static-kv-cache-and-torchcompile)
æ¬²äº†è§£æ›´å¤šå…³äºé™æ€ç¼“å­˜ä¸å³æ—¶ç¼–è¯‘ï¼ˆJITï¼‰çš„ç¤ºä¾‹ï¼Œè¯·å‚é˜…[é™æ€ç¼“å­˜ä¸torch.compile](./llm_optims#static-kv-cache-and-torchcompile)ã€‚

```python
>>> import torch
>>> from transformers import AutoTokenizer, AutoModelForCausalLM

>>> tokenizer = AutoTokenizer.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0")
>>> model = AutoModelForCausalLM.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0", torch_dtype=torch.float16, device_map="auto")
>>> inputs = tokenizer("Hello, my name is", return_tensors="pt").to(model.device)

>>> # simply pass the cache implementation="static"
>>> out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation="static")
>>> tokenizer.batch_decode(out, skip_special_tokens=True)[0]
"Hello, my name is [Your Name] and I am a [Your Position] at [Your Company]. I am writing"
```


## Offloaded Static Cache
## å¯å¸è½½çš„é™æ€ç¼“å­˜

Like [`~OffloadedCache`] exists for offloading a "DynamicCache", there is also an offloaded static cache. It fully supports
JIT optimizations. Just pass `cache_implementation="offloaded_static"` in the `generation_config` or directly to the `generate()` call.
This will use the [`~OffloadedStaticCache`] implementation instead.
ç±»ä¼¼äº[`~OffloadedCache`]ç”¨äºå¸è½½â€œåŠ¨æ€ç¼“å­˜â€ï¼Œä¹Ÿå­˜åœ¨å¯å¸è½½çš„é™æ€ç¼“å­˜ã€‚å®ƒå®Œå…¨æ”¯æŒJITä¼˜åŒ–ã€‚åªéœ€åœ¨`generation_config`æˆ–ç›´æ¥ä¼ é€’ç»™`generate()`è°ƒç”¨æ—¶æ·»åŠ `cache_implementation="offloaded_static"`å‚æ•°å³å¯ã€‚è¿™å°†ä½¿ç”¨[`~OffloadedStaticCache`]å®ç°æ¥ä»£æ›¿é»˜è®¤æ–¹æ¡ˆã€‚

```python
>>> import torch
>>> from transformers import AutoTokenizer, AutoModelForCausalLM

>>> tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf")
>>> model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-chat-hf", torch_dtype=torch.float16, device_map="auto")
>>> inputs = tokenizer("Hello, my name is", return_tensors="pt").to(model.device)

>>> # simply pass the cache implementation="offloaded_static"
>>> out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation="offloaded_static")
>>> tokenizer.batch_decode(out, skip_special_tokens=True)[0]
"Hello, my name is [Your Name], and I am a [Your Profession] with [Number of Years] of"
```
Cache offloading requires a CUDA GPU.
ç¼“å­˜å¸è½½åŠŸèƒ½éœ€è¦é…å¤‡ä¸€å—CUDA GPUæ‰èƒ½ä½¿ç”¨ã€‚

### Sliding Window Cache
### æ»‘åŠ¨çª—å£ç¼“å­˜

As the name suggests, this cache type implements a sliding window over previous keys and values, retaining only the last `sliding_window` tokens. It should be used with models like Mistral that support sliding window attention. Additionally, similar to Static Cache, this one is JIT-friendly and can be used with the same compile tecniques as Static Cache.

Note that you can use this cache only for models that support sliding window, e.g. Mistral models.

æ­£å¦‚å…¶åæ‰€ç¤ºï¼Œè¿™ç§ç¼“å­˜ç±»å‹å®ç°äº†å¯¹å·²æœ‰çš„keyså’Œvaluesçš„æ»‘åŠ¨çª—å£æœºåˆ¶ï¼Œä»…ä¿ç•™æœ€è¿‘çš„`sliding_window`ä¸ªæ ‡è®°ã€‚å®ƒé€‚ç”¨äºæ”¯æŒæ»‘åŠ¨çª—å£æ³¨æ„åŠ›æœºåˆ¶çš„æ¨¡å‹ï¼Œå¦‚Mistralã€‚æ­¤å¤–ï¼Œä¸é™æ€ç¼“å­˜ç›¸ä¼¼ï¼Œè¿™ç§ç¼“å­˜åŒæ ·å¯¹JITå‹å¥½ï¼Œå¹¶èƒ½åº”ç”¨ä¸é™æ€ç¼“å­˜ç›¸åŒçš„ç¼–è¯‘æŠ€æœ¯ã€‚

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™ç§ç¼“å­˜ä»…é€‚ç”¨äºæ”¯æŒæ»‘åŠ¨çª—å£çš„æ¨¡å‹ï¼Œä¾‹å¦‚Mistralç³»åˆ—æ¨¡å‹ã€‚


```python
>>> import torch
>>> from transformers import AutoTokenizer, AutoModelForCausalLM, SinkCache

>>> tokenizer = AutoTokenizer.from_pretrained("teknium/OpenHermes-2.5-Mistral-7B")
>>> model = AutoModelForCausalLM.from_pretrained("teknium/OpenHermes-2.5-Mistral-7B", torch_dtype=torch.float16, device_map="auto")
>>> inputs = tokenizer("Yesterday I was on a rock concert and.", return_tensors="pt").to(model.device)

>>> # can be used by passing in cache implementation
>>> out = model.generate(**inputs, do_sample=False, max_new_tokens=30, cache_implementation="sliding_window")
>>> tokenizer.batch_decode(out, skip_special_tokens=True)[0]
"Yesterday I was on a rock concert and. I was so excited to see my favorite band perform live. I was so happy that I could hardly contain myself. I was jumping up and down and"
```

### Sink Cache
### ä¸‹æ²‰ç¼“å­˜

Sink Cache was introduced in ["Efficient Streaming Language Models with Attention Sinks"](https://arxiv.org/abs/2309.17453). It allows you to generate long sequences of text ("infinite length" according to the paper) without any fine-tuning. That is achieved by smart handling of previous keys and values, specifically it retains a few initial tokens from the sequence, called "sink tokens". This is based on the observation that these initial tokens attract a significant portion of attention scores during the generation process. Tokens that come after "sink tokens" are discarded on a sliding windowed basis, keeping only the latest `window_size` tokens. By keeping these initial tokens as "attention sinks," the model maintains stable performance even when dealing with very long texts, thus discarding most of the previous knowledge.

Unlike other cache classes, this one can't be used directly by indicating a `cache_implementation`. You have to initialize the Cache before calling on `generate()` as follows.
ä¸‹æ²‰ç¼“å­˜æ˜¯åœ¨ ["Efficient Streaming Language Models with Attention Sinks"](https://arxiv.org/abs/2309.17453)ä¸€æ–‡ä¸­æå‡ºçš„ã€‚å®ƒèƒ½å¤Ÿæ— éœ€å¾®è°ƒå³å¯ç”Ÿæˆé•¿æ–‡æœ¬åºåˆ—ï¼ˆæ ¹æ®è®ºæ–‡æè¿°ï¼Œè¾¾åˆ°â€œæ— é™é•¿åº¦â€ï¼‰ã€‚å¯¹å·²æœ‰çš„keyså’Œvaluesçš„æ™ºèƒ½å¤„ç†ç­–ç•¥ä½¿å¾—ä¸‹æ²‰ç¼“å­˜å¾—ä»¥å®ç°ï¼Œç‰¹åˆ«åœ°ï¼Œè¯¥æœºåˆ¶ä¿ç•™äº†åºåˆ—å¼€å¤´çš„å‡ ä¸ªæ ‡è®°ï¼Œç§°ä¸ºâ€œä¸‹æ²‰æ ‡è®°â€ã€‚è¿™ä¸€è®¾è®¡åŸºäºä¸€ä¸ªè§‚å¯Ÿï¼šåœ¨æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œè¿™äº›åˆå§‹æ ‡è®°ä¼šå¸å¼•å¤§é‡çš„æ³¨æ„åŠ›å¾—åˆ†ã€‚ç´§æ¥â€œä¸‹æ²‰æ ‡è®°â€ä¹‹åçš„æ ‡è®°åˆ™åŸºäºæ»‘åŠ¨çª—å£åŸåˆ™è¿›è¡Œèˆå¼ƒï¼Œä»…ä¿ç•™æœ€æ–°çš„`window_size`ä¸ªæ ‡è®°ã€‚é€šè¿‡ä¿ç•™è¿™äº›åˆå§‹æ ‡è®°ä½œä¸ºâ€œæ³¨æ„åŠ›ä¸‹æ²‰ç‚¹â€ï¼Œæ¨¡å‹å³ä½¿å¤„ç†è¶…é•¿æ–‡æœ¬ä¹Ÿèƒ½ä¿æŒç¨³å®šçš„æ€§èƒ½ï¼Œå°½ç®¡è¿™æ„å‘³ç€å¤§éƒ¨åˆ†è¿‡å¾€çš„çŸ¥è¯†è¢«èˆå¼ƒã€‚

ä¸å…¶ä»–ç¼“å­˜ç±»ä¸åŒï¼Œè¿™ç§ç¼“å­˜ä¸èƒ½ç›´æ¥é€šè¿‡æŒ‡å®š`cache_implementation`æ¥ä½¿ç”¨ã€‚ä½ éœ€è¦åœ¨è°ƒç”¨`generate()`ä¹‹å‰ï¼ŒæŒ‰ç…§ä»¥ä¸‹æ–¹å¼åˆå§‹åŒ–ç¼“å­˜ã€‚

```python
>>> import torch
>>> from transformers import AutoTokenizer, AutoModelForCausalLM, SinkCache

>>> tokenizer = AutoTokenizer.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0")
>>> model = AutoModelForCausalLM.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0", torch_dtype=torch.float16, device_map="auto")
>>> inputs = tokenizer("This is a long story about unicorns, fairies and magic.", return_tensors="pt").to(model.device)

>>> # get our cache, specify number of sink tokens and window size
>>> # Note that window size already includes sink tokens, so has to be larger
>>> past_key_values = SinkCache(window_length=256, num_sink_tokens=4)
>>> out = model.generate(**inputs, do_sample=False, max_new_tokens=30, past_key_values=past_key_values)
>>> tokenizer.batch_decode(out, skip_special_tokens=True)[0]
"This is a long story about unicorns, fairies and magic. It is a story about a young girl named Lily who discovers that she has the power to control the elements. She learns that she can"
```

### Encoder-Decoder Cache
### Encoder-Decoder ç¼“å­˜

The [`~EncoderDecoderCache`] is a wrapper designed to handle the caching needs of encoder-decoder models. This cache type is specifically built to manage both self-attention and cross-attention caches, ensuring storage and retrieval of past key/values required for these complex models. Cool thing about Encoder-Decoder Cache is that you can set different cache types for the encoder and for the decoder, depending on your use case. Currently this cache is only supported in [Whisper](./model_doc/whisper) models but we will be adding more models soon.

In terms of usage, there is nothing special to be done and calling `generate()` or `forward()` will handle everything for you.

[`~EncoderDecoderCache`]æ˜¯ä¸€ç§ä¸“é—¨è®¾è®¡ç”¨äºæ»¡è¶³Encoder-Decoder æ¨¡å‹ç¼“å­˜éœ€æ±‚çš„å°è£…å·¥å…·ã€‚æ­¤ç¼“å­˜ç±»å‹ç‰¹åˆ«æ„å»ºç”¨äºç®¡ç†è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›ç¼“å­˜ï¼Œç¡®ä¿å¤æ‚æ¨¡å‹æ‰€éœ€çš„å†å² key/values çš„å­˜å‚¨ä¸æ£€ç´¢ã€‚Encoder-Decoder ç¼“å­˜çš„ä¸€ä¸ªäº®ç‚¹åœ¨äºï¼Œä½ å¯ä»¥æ ¹æ®å…·ä½“åº”ç”¨åœºæ™¯ï¼Œåˆ†åˆ«ä¸ºEncoderå’ŒDecoderè®¾ç½®ä¸åŒçš„ç¼“å­˜ç±»å‹ã€‚ç›®å‰ï¼Œè¿™ä¸€ç¼“å­˜ä»…æ”¯æŒ[Whisper](./model_doc/whisper)æ¨¡å‹ï¼Œä½†æˆ‘ä»¬è®¡åˆ’å¾ˆå¿«å¢åŠ æ›´å¤šæ¨¡å‹çš„æ”¯æŒã€‚

åœ¨ç”¨æ³•ä¸Šï¼Œæ— éœ€ç‰¹åˆ«æ“ä½œï¼Œç›´æ¥è°ƒç”¨`generate()`æˆ–`forward()`å³å¯è‡ªåŠ¨å¤„ç†æ‰€æœ‰ç›¸å…³äº‹å®œã€‚

### Model-specific Cache Classes
### æ¨¡å‹ç‰¹å®šçš„ç¼“å­˜ç±»

Some models require storing previous keys, values, or states in a specific way, and the above cache classes cannot be used. For such cases, we have several specialized cache classes that are designed for specific models. These models only accept their own dedicated cache classes and do not support using any other cache types. Some examples include [`~HybridCache`] for [Gemma2](./model_doc/gemma2) series models or [`~MambaCache`] for [Mamba](./model_doc/mamba) architecture models.

æŸäº›æ¨¡å‹éœ€è¦ä»¥ç‰¹å®šæ–¹å¼å­˜å‚¨å…ˆå‰çš„keysã€valuesæˆ–çŠ¶æ€ï¼Œä¸Šè¿°é€šç”¨ç¼“å­˜ç±»æ— æ³•æ»¡è¶³éœ€æ±‚ã€‚é’ˆå¯¹è¿™äº›æƒ…å½¢ï¼Œæˆ‘ä»¬æä¾›äº†å¤šç§ä¸“é—¨è®¾è®¡çš„ç¼“å­˜ç±»ï¼Œé€‚é…ç‰¹å®šæ¨¡å‹ã€‚æ­¤ç±»æ¨¡å‹ä»…æ¥å—å®ƒä»¬ä¸“å±çš„ç¼“å­˜ç±»ï¼Œä¸æ”¯æŒä½¿ç”¨å…¶ä»–ç¼“å­˜ç±»å‹ã€‚ä¾‹å¦‚ï¼Œ[Gemma2](./model_doc/gemma2)ç³»åˆ—æ¨¡å‹ä½¿ç”¨[`~HybridCache`]ï¼Œè€Œ[Mamba](./model_doc/mamba)æ¶æ„æ¨¡å‹åˆ™é‡‡ç”¨[`~MambaCache`]ã€‚

## Iterative Generation with Cache
## ä½¿ç”¨ç¼“å­˜è¿›è¡Œäº¤äº’å¼ç”Ÿæˆ

We have seen how to use each of the cache types when generating. What if you want to use cache in iterative generation setting, for example in applications like chatbots, where interactions involve multiple turns and continuous back-and-forth exchanges. Iterative generation with cache allows these systems to handle ongoing conversations effectively without reprocessing the entire context at each step. But there are some tips that you should know before you start implementing:

The general format when doing iterative generation is as below. First you have to initialize an empty cache of the type you want, and you can start feeding in new prompts iteratively. Keeping track of dialogues history and formatting can be done with chat templates, read more on that in [chat_templating](./chat_templating)

In case you are using Sink Cache, you have to crop your inputs to that maximum length because Sink Cache can generate text longer than its maximum window size, but it expects the first input to not exceed the maximum cache length.

æˆ‘ä»¬åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å·²ç»äº†è§£äº†å„ç§ç¼“å­˜ç±»å‹çš„åº”ç”¨ã€‚å¦‚æœä½ å¸Œæœ›åœ¨è¿­ä»£ç”Ÿæˆè®¾ç½®ä¸­ä½¿ç”¨ç¼“å­˜ï¼Œä¾‹å¦‚åœ¨èŠå¤©æœºå™¨äººçš„åº”ç”¨ä¸­ï¼Œå…¶ä¸­äº¤äº’æ¶‰åŠå¤šè½®è¿ç»­çš„æ¥å›å¯¹è¯ã€‚ä½¿ç”¨ç¼“å­˜è¿›è¡Œè¿­ä»£ç”Ÿæˆå¯ä»¥è®©è¿™äº›ç³»ç»Ÿæœ‰æ•ˆåœ°å¤„ç†æŒç»­å¯¹è¯ï¼Œè€Œæ— éœ€åœ¨æ¯ä¸€æ­¥éƒ½é‡æ–°å¤„ç†æ•´ä¸ªä¸Šä¸‹æ–‡ã€‚ä½†åœ¨å¼€å§‹å®ç°ä¹‹å‰ï¼Œæœ‰ä¸€äº›æŠ€å·§ä½ éœ€è¦çŸ¥é“ï¼š

è¿›è¡Œè¿­ä»£ç”Ÿæˆçš„ä¸€èˆ¬æ ¼å¼å¦‚ä¸‹ã€‚é¦–å…ˆï¼Œä½ éœ€è¦åˆå§‹åŒ–ä¸€ä¸ªä½ æƒ³è¦ç±»å‹çš„ç©ºç¼“å­˜ï¼Œå¹¶å¯ä»¥å¼€å§‹é€è½®è¾“å…¥æ–°çš„æç¤ºã€‚é€šè¿‡èŠå¤©æ¨¡æ¿å¯ä»¥è·Ÿè¸ªå¯¹è¯å†å²å’Œæ ¼å¼åŒ–ï¼Œæ›´å¤šè¯¦ç»†ä¿¡æ¯è¯·å‚é˜…[èŠå¤©æ¨¡æ¿](./chat_templating)ã€‚

å¦‚æœä½ ä½¿ç”¨ä¸‹æ²‰ç¼“å­˜ Sink Cacheï¼Œå¿…é¡»å°†è¾“å…¥è£å‰ªä¸ºæœ€å¤§é•¿åº¦ï¼Œå› ä¸ºä¸‹æ²‰ç¼“å­˜å¯ä»¥ç”Ÿæˆè¶…è¿‡å…¶æœ€å¤§çª—å£é•¿åº¦çš„æ–‡æœ¬ï¼Œä½†å®ƒæœŸæœ›ç¬¬ä¸€ä¸ªè¾“å…¥ä¸è¦è¶…è¿‡æœ€å¤§ç¼“å­˜é•¿åº¦ã€‚


```python
>>> import torch
>>> from transformers import AutoTokenizer,AutoModelForCausalLM
>>> from transformers.cache_utils import (
...    DynamicCache,
...    SinkCache,
...    StaticCache,
...    SlidingWindowCache,
...    QuantoQuantizedCache,
...    QuantizedCacheConfig,
... )

>>> model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
>>> model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map='auto')
>>> tokenizer = AutoTokenizer.from_pretrained(model_id)

>>> user_prompts = ["Hello, what's your name?", "Btw, yesterday I was on a rock concert."]

>>> past_key_values = DynamicCache()
>>> max_cache_length = past_key_values.get_max_cache_shape()

>>> messages = []
>>> for prompt in user_prompts:
...     messages.append({"role": "user", "content": prompt})
...     inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt", return_dict=True).to(model.device)
...     if isinstance(past_key_values, SinkCache):
...         inputs = {k: v[:, -max_cache_length:] for k, v in inputs.items()}
...
...     input_length = inputs["input_ids"].shape[1]
...
...     outputs = model.generate(**inputs, do_sample=False, max_new_tokens=256, past_key_values=past_key_values)
...     completion = tokenizer.decode(outputs[0, input_length: ], skip_special_tokens=True)
...     messages.append({"role": "assistant", "content": completion})

print(messages)
[{'role': 'user', 'content': "Hello, what's your name?"}, {'role': 'assistant', 'content': "Hello, I'm AI."}, {'role': 'user', 'content': 'Btw, yesterday I was on a rock concert.'}, {'role': 'assistant', 'content': "I'm sorry to hear that you were on a rock concert yesterday. It sounds like a fun experience, but I'm not capable of experiencing music or concerts. However, I can provide you with some information about rock music and its history. Rock music emerged in the 1950s and 1960s in the United States and Britain, and it quickly gained popularity around the world. Some of the most famous rock bands of all time include The Beatles, The Rolling Stones, Led Zeppelin, and Pink Floyd. Rock music has a distinct sound and style, with elements of blues, country, and folk music. It often features guitar solos, heavy bass lines, and drums. Rock music has had a significant impact on popular culture, influencing genres such as punk rock, heavy metal, and alternative rock."}]
```

## Re-use Cache to continue generation
## åœ¨ç»§ç»­ç”Ÿæˆæ—¶å¤ç”¨ç¼“å­˜

Sometimes you would want to first fill-in cache object with key/values for certain prefix prompt and re-use it several times to generate different sequences from it. In that case you can construct a `Cache` object that will hold the instruction prompt, and re-use it several times with different text sequences.

æœ‰æ—¶ä½ å¯èƒ½å¸Œæœ›é¦–å…ˆç”¨ç‰¹å®šå‰ç¼€æç¤ºçš„å…³é”®å€¼å¡«å……ç¼“å­˜å¯¹è±¡ï¼Œå¹¶å¤šæ¬¡å¤ç”¨å®ƒæ¥ç”Ÿæˆä¸åŒçš„åºåˆ—ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ å¯ä»¥æ„é€ ä¸€ä¸ª `Cache` å¯¹è±¡æ¥ä¿ç•™æŒ‡ä»¤æç¤ºï¼Œå¹¶å¤šæ¬¡ä½¿ç”¨å®ƒæ¥è¾“å‡ºä¸åŒçš„æ–‡æœ¬åºåˆ—ã€‚

```python
>>> import copy
>>> import torch
>>> from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache, StaticCache
>>> from accelerate.test_utils.testing import get_backend

>>> DEVICE, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)
>>> model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
>>> model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=DEVICE)
>>> tokenizer = AutoTokenizer.from_pretrained(model_id)

>>> # Init StaticCache with big enough max-length (1024 tokens for the below example)
>>> # You can also init a DynamicCache, if that suits you better
>>> prompt_cache = StaticCache(config=model.config, max_batch_size=1, max_cache_len=1024, device=DEVICE, dtype=torch.bfloat16)

>>> INITIAL_PROMPT = "You are a helpful assistant. "
>>> inputs_initial_prompt = tokenizer(INITIAL_PROMPT, return_tensors="pt").to(DEVICE)
>>> # This is the common prompt cached, we need to run forward without grad to be abel to copy
>>> with torch.no_grad():
...      prompt_cache = model(**inputs_initial_prompt, past_key_values = prompt_cache).past_key_values

>>> prompts = ["Help me to write a blogpost about travelling.", "What is the capital of France?"]
>>> responses = []
>>> for prompt in prompts:
...     new_inputs = tokenizer(INITIAL_PROMPT + prompt, return_tensors="pt").to(DEVICE)
...     past_key_values = copy.deepcopy(prompt_cache)
...     outputs = model.generate(**new_inputs, past_key_values=past_key_values,max_new_tokens=20)
...     response = tokenizer.batch_decode(outputs)[0]
...     responses.append(response)

>>> print(responses)
['<s> You are a helpful assistant. Help me to write a blogpost about travelling.  I am excited to share my experiences with you.  I have been traveling for the past', '<s> You are a helpful assistant. What is the capital of France? \n\nAnswer: Paris is the capital of France.</s>']
```


## Legacy cache format
## ä¼ ç»Ÿçš„ç¼“å­˜æ ¼å¼

Prior to the introduction of the `Cache` object, the cache of LLMs used to be a tuple of tuples of tensors. The legacy
format has a dynamic size, growing as we generate text -- very similar to `DynamicCache`. If your project depend on
this legacy format, you can seamlessly convert it to a `DynamicCache` and back.

åœ¨ `Cache` å¯¹è±¡å¼•å…¥ä¹‹å‰ï¼ŒLLM çš„ç¼“å­˜æ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œé‡Œé¢çš„å…ƒç´ æ˜¯ç”±å¤šä¸ªtensorç»„æˆçš„å…ƒç»„ã€‚ä¼ ç»Ÿçš„ç¼“å­˜æ ¼å¼æ˜¯åŠ¨æ€å¤§å°çš„ï¼Œéšç€æˆ‘ä»¬ç”Ÿæˆæ–‡æœ¬è€Œå¢é•¿â€”â€”éå¸¸ç±»ä¼¼äº `DynamicCache`ã€‚å¦‚æœä½ çš„é¡¹ç›®ä¾èµ–äºè¿™ç§ä¼ ç»Ÿçš„ç¼“å­˜æ ¼å¼ï¼Œä½ å¯ä»¥æ— ç¼åœ°å°†å…¶è½¬æ¢ä¸º `DynamicCache`ï¼Œç„¶åå†è½¬æ¢å›æ¥ã€‚

```python
>>> import torch
>>> from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache

>>> tokenizer = AutoTokenizer.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0")
>>> model = AutoModelForCausalLM.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0", torch_dtype=torch.float16, device_map="auto")
>>> inputs = tokenizer("Hello, my name is", return_tensors="pt").to(model.device)

>>> # `return_dict_in_generate=True` is required to return the cache. `return_legacy_cache` forces the returned cache
>>> # to be of the legacy type
>>> generation_outputs = model.generate(**inputs, return_dict_in_generate=True, return_legacy_cache=True, max_new_tokens=5)

>>> # We can convert a legacy cache to a DynamicCache -- and the other way around. This is helpful if you have custom
>>> # logic to manipulate a cache in a specific format.
>>> cache = DynamicCache.from_legacy_cache(generation_outputs.past_key_values)
>>> legacy_format_cache = cache.to_legacy_cache()
```
