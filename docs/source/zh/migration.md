<!--Copyright 2025 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# ä»æ—§ç‰ˆæœ¬è¿ç§»

## ä» Transformers `v3.x` è¿ç§»åˆ° `v4.x`

ä»ç‰ˆæœ¬ 3 å‡çº§åˆ°ç‰ˆæœ¬ 4 æ—¶å¼•å…¥äº†ä¸€äº›æ›´æ”¹ã€‚ä»¥ä¸‹æ˜¯é¢„æœŸæ›´æ”¹çš„æ‘˜è¦ï¼š

#### 1. AutoTokenizer å’Œ pipeline ç°åœ¨é»˜è®¤ä½¿ç”¨å¿«é€Ÿåˆ†è¯å™¨ï¼ˆRustï¼‰ã€‚

Python å’Œ Rust åˆ†è¯å™¨çš„ API å¤§è‡´ç›¸åŒï¼Œä½† Rust åˆ†è¯å™¨å…·æœ‰æ›´å…¨é¢çš„åŠŸèƒ½é›†ã€‚

è¿™å¼•å…¥äº†ä¸¤ä¸ªä¸»è¦æ›´æ”¹ï¼š

- Python å’Œ Rust åˆ†è¯å™¨å¤„ç†è¯å…ƒæº¢å‡ºçš„æ–¹å¼ä¸åŒã€‚

- Rust åˆ†è¯å™¨åœ¨ç¼–ç æ–¹æ³•ä¸­ä¸æ¥å—æ•´æ•°ã€‚

##### å¦‚ä½•åœ¨ v4.x ä¸­å®ç°ä¸ v3.x ç›¸åŒçš„è¡Œä¸º

- Pipeline ç°åœ¨åŒ…å«ä¸€äº›å¼€ç®±å³ç”¨çš„é™„åŠ åŠŸèƒ½ã€‚è¯·å‚é˜…å¸¦æœ‰ `grouped_entities` æ ‡å¿—çš„è¯å…ƒåˆ†ç±» pipelineã€‚

- Auto-tokenizer ç°åœ¨è¿”å› Rust åˆ†è¯å™¨ã€‚è¦ä½¿ç”¨ Python åˆ†è¯å™¨ï¼Œç”¨æˆ·å¿…é¡»å°† `use_fast` æ ‡å¿—è®¾ç½®ä¸º `False`ï¼š

åœ¨ `v3.x` ç‰ˆæœ¬ä¸­ï¼š

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")
```

åœ¨ `v4.x` ç‰ˆæœ¬ä¸­å®ç°ç›¸åŒåŠŸèƒ½ï¼š

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased", use_fast=False)
```

#### 2. SentencePiece å·²ä»å¿…éœ€ä¾èµ–é¡¹ä¸­ç§»é™¤

`setup.py` æ–‡ä»¶ä¸­å·²ç§»é™¤å¯¹ SentencePiece çš„ä¾èµ–ã€‚è¿™æ ·åšæ˜¯ä¸ºäº†åœ¨ä¸ä¾èµ– `conda-forge` çš„æƒ…å†µä¸‹ï¼Œæä¾›ä¸€ä¸ªåŸºäº Anaconda äº‘çš„é€šé“ã€‚è¿™æ„å‘³ç€ï¼Œä¾èµ–äº SentencePiece åº“çš„åˆ†è¯å™¨å°†æ— æ³•åœ¨æ ‡å‡†çš„ `transformers` å®‰è£…ä¸­ä½¿ç”¨ã€‚

è¿™åŒ…æ‹¬ä»¥ä¸‹é€Ÿåº¦è¾ƒæ…¢çš„ç‰ˆæœ¬ï¼š

- `XLNetTokenizer`

- `AlbertTokenizer`

- `CamembertTokenizer`

- `MBartTokenizer`

- `PegasusTokenizer`

- `T5Tokenizer`

- `ReformerTokenizer`

- `XLMRobertaTokenizer`

##### å¦‚ä½•åœ¨ v4.x ç‰ˆæœ¬ä¸­è·å¾—ä¸ v3.x ç›¸åŒçš„è¡Œä¸º

è¦è·å¾—ä¸ `v3.x` ç‰ˆæœ¬ç›¸åŒçš„è¡Œä¸ºï¼Œæ‚¨è¿˜å¿…é¡»å®‰è£… `sentencepiece`ï¼š

åœ¨ `v3.x` ç‰ˆæœ¬ä¸­ï¼š

```bash
pip install trâ€‹â€‹ansformers
```

è¦åœ¨ `v4.x` ç‰ˆæœ¬ä¸­è·å¾—ç›¸åŒçš„è¡Œä¸ºï¼š

```bash
pip install trâ€‹â€‹ansformers[sentencepiece]
```

æˆ–
```bash
pip install trâ€‹â€‹ansformers sentencepiece
```

#### 3. ä»“åº“æ¶æ„å·²æ›´æ–°ï¼Œæ¯ä¸ªæ¨¡å‹éƒ½æœ‰è‡ªå·±çš„æ–‡ä»¶å¤¹

éšç€æ–°æ¨¡å‹çš„æ·»åŠ ï¼Œä»“åº“ä¸­çš„æ–‡ä»¶æ•°é‡ä¹Ÿä¼šå¢åŠ ã€‚ `src/transformers` æ–‡ä»¶å¤¹æŒç»­å¢é•¿ï¼Œå¯¼è‡´å…¶éš¾ä»¥æµè§ˆå’Œç†è§£ã€‚æˆ‘ä»¬å†³å®šå°†æ¯ä¸ªæ¨¡å‹åŠå…¶ç›¸å…³æ–‡ä»¶åˆ†åˆ«æ”¾ç½®åœ¨å„è‡ªçš„å­æ–‡ä»¶å¤¹ä¸­ã€‚

è¿™æ˜¯ä¸€é¡¹é‡å¤§å˜æ›´ï¼Œå› ä¸ºä½¿ç”¨æ¨¡å‹æ¨¡å—ç›´æ¥å¯¼å…¥ä¸­é—´å±‚éœ€è¦é€šè¿‡ä¸åŒçš„è·¯å¾„ã€‚

##### å¦‚ä½•åœ¨ v4.x ä¸­å®ç°ä¸ v3.x ç›¸åŒçš„è¡Œä¸º

è¦å®ç°ä¸ `v3.x` ç‰ˆæœ¬ç›¸åŒçš„è¡Œä¸ºï¼Œæ‚¨å¿…é¡»æ›´æ–°ç”¨äºè®¿é—®å›¾å±‚çš„è·¯å¾„ã€‚

åœ¨ `v3.x` ç‰ˆæœ¬ä¸­ï¼š

```bash
from transformers.modeling_bert import BertLayer
```

è¦åœ¨ `v4.x` ç‰ˆæœ¬ä¸­å®ç°ç›¸åŒçš„åŠŸèƒ½ï¼š

```bash
from transformers.models.bert.modeling_bert import BertLayer
```

#### 4. å°† `return_dict` å‚æ•°é»˜è®¤è®¾ç½®ä¸º `True`

`return_dict` å‚æ•°ï¼ˆ`main_classes/output`ï¼‰å…è®¸è¿”å›åŒ…å«æ¨¡å‹è¾“å‡ºçš„ç±»ä¼¼å­—å…¸çš„ Python å¯¹è±¡ï¼Œè€Œä¸æ˜¯æ ‡å‡†å…ƒç»„ã€‚è¯¥å¯¹è±¡å…·æœ‰è‡ªæ–‡æ¡£æ€§ï¼Œå› ä¸ºé”®å¯ç”¨äºæ£€ç´¢å€¼ï¼ˆå…¶è¡Œä¸ºä¹Ÿç±»ä¼¼äºå…ƒç»„ï¼‰ï¼Œå¹¶ä¸”ç”¨æˆ·å¯ä»¥æ£€ç´¢å¯¹è±¡ä»¥è¿›è¡Œç´¢å¼•æˆ–åˆ‡ç‰‡ã€‚

è¿™æ˜¯ä¸€ä¸ªé‡å¤§æ›´æ”¹ï¼Œå› ä¸ºå…ƒç»„æ— æ³•è§£åŒ…ï¼š`value0, value1 = outputs` å°†æ— æ³•å·¥ä½œã€‚

##### å¦‚ä½•åœ¨ v4.x ä¸­å®ç°ä¸ v3.x ç›¸åŒçš„è¡Œä¸º

è¦å®ç°ä¸ `v3.x` ç‰ˆæœ¬ç›¸åŒçš„è¡Œä¸ºï¼Œè¯·åœ¨æ¨¡å‹é…ç½®å’Œä¸‹ä¸€æ­¥ä¸­éƒ½å°† `return_dict` å‚æ•°æŒ‡å®šä¸º `False`ã€‚

åœ¨ `v3.x` ç‰ˆæœ¬ä¸­ï¼š

```bash
model = BertModel.from_pretrained("google-bert/bert-base-cased")
outputs = model(**inputs)
```

è¦åœ¨ `v4.x` ç‰ˆæœ¬ä¸­å®ç°ç›¸åŒçš„åŠŸèƒ½ï¼š

```bash
model = BertModel.from_pretrained("google-bert/bert-base-cased")
outputs = model(**inputs, return_dict=False)
```

æˆ–

```bash
model = BertModel.from_pretrained("google-bert/bert-base-cased", return_dict=False)
outputs = model(**inputs)
```

#### 5. ç§»é™¤æŸäº›å·²å¼ƒç”¨çš„å±æ€§

å¦‚æœå±æ€§å·²å¼ƒç”¨è‡³å°‘ä¸€ä¸ªæœˆï¼Œåˆ™å°†å…¶ç§»é™¤ã€‚å·²å¼ƒç”¨å±æ€§çš„å®Œæ•´åˆ—è¡¨å¯åœ¨ [#8604](https://github.com/huggingface/transformers/pull/8604) ä¸­æ‰¾åˆ°ã€‚

ä»¥ä¸‹æ˜¯è¿™äº›å±æ€§/æ–¹æ³•/å‚æ•°çš„åˆ—è¡¨åŠå…¶æ›¿ä»£æ–¹æ¡ˆï¼š

åœ¨å¤šä¸ªæ¨¡å‹ä¸­,æ ‡ç­¾å˜å¾—ä¸å…¶ä»–æ¨¡å‹ä¸€è‡´ï¼š

- `masked_lm_labels` åœ¨ `AlbertForMaskedLM` å’Œ `AlbertForPreTraining` ä¸­å˜ä¸º `labels`ã€‚
- `masked_lm_labels` åœ¨ `BertForMaskedLM` å’Œ `BertForPreTraining` ä¸­å˜ä¸º `labels`ã€‚
- `masked_lm_labels` åœ¨ `DistilBertForMaskedLM` ä¸­å˜ä¸º `labels`ã€‚
- `masked_lm_labels` åœ¨ `ElectraForMaskedLM` ä¸­å˜ä¸º `labels`ã€‚
- `masked_lm_labels` åœ¨ `LongformerForMaskedLM` ä¸­å˜ä¸º `labels`ã€‚
- `masked_lm_labels` åœ¨ `MobileBertForMaskedLM` ä¸­å˜ä¸º `labels`ã€‚
- `masked_lm_labels` åœ¨ `RobertaForMaskedLM` ä¸­å˜ä¸º `labels`ã€‚
- `lm_labels` åœ¨ `BartForConditionalGeneration` ä¸­å˜ä¸º `labels`ã€‚
- `lm_labels` åœ¨ `GPT2DoubleHeadsModel` ä¸­å˜ä¸º `labels`ã€‚
- `lm_labels` åœ¨ `OpenAIGPTDoubleHeadsModel` ä¸­å˜ä¸º `labels`ã€‚
- `lm_labels` åœ¨ `T5ForConditionalGeneration` ä¸­å˜ä¸º `labels`ã€‚

åœ¨å¤šä¸ªæ¨¡å‹ä¸­ï¼Œç¼“å­˜æœºåˆ¶å˜å¾—ä¸å…¶ä»–æ¨¡å‹ä¸€è‡´ï¼š

- `decoder_cached_states` åœ¨æ‰€æœ‰ç±» BARTã€FSMT å’Œ T5 æ¨¡å‹ä¸­å˜ä¸º `past_key_values`ã€‚
- `decoder_past_key_values` åœ¨æ‰€æœ‰ç±» BARTã€FSMT å’Œ T5 æ¨¡å‹ä¸­å˜ä¸º `past_key_values`ã€‚
- `past` åœ¨æ‰€æœ‰ CTRL æ¨¡å‹ä¸­å˜ä¸º `past_key_values`ã€‚
- `past` åœ¨æ‰€æœ‰ GPT-2 æ¨¡å‹ä¸­å˜ä¸º `past_key_values`ã€‚

å…³äºåˆ†è¯å™¨ç±»ï¼š

- åˆ†è¯å™¨å±æ€§ `max_len` å˜ä¸º `model_max_length`ã€‚
- åˆ†è¯å™¨å±æ€§ `return_lengths` å˜ä¸º `return_length`ã€‚
- åˆ†è¯å™¨ç¼–ç å‚æ•° `is_pretokenized` å˜ä¸º `is_split_into_words`ã€‚

å…³äº `Trainer` ç±»ï¼š

- `Trainer` çš„ `tb_writer` å‚æ•°å·²ç§»é™¤,æ”¹ç”¨å›è°ƒå‡½æ•° `TensorBoardCallback(tb_writer=...)`ã€‚
- `Trainer` çš„ `prediction_loss_only` å‚æ•°å·²ç§»é™¤,æ”¹ç”¨ç±»å‚æ•° `args.prediction_loss_only`ã€‚
- `Trainer` çš„ `data_collator` å±æ€§å°†æ˜¯å¯è°ƒç”¨çš„ã€‚
- `Trainer` çš„ `_log` æ–¹æ³•å·²å¼ƒç”¨,æ”¹ç”¨ `log`ã€‚
- `Trainer` çš„ `_training_step` æ–¹æ³•å·²å¼ƒç”¨,æ”¹ç”¨ `training_step`ã€‚
- `Trainer` çš„ `_prediction_loop` æ–¹æ³•å·²å¼ƒç”¨,æ”¹ç”¨ `prediction_loop`ã€‚
- `Trainer` çš„ `is_local_master` æ–¹æ³•å·²å¼ƒç”¨,æ”¹ç”¨ `is_local_process_zero`ã€‚
- `Trainer` çš„ `is_world_master` æ–¹æ³•å·²å¼ƒç”¨,æ”¹ç”¨ `is_world_process_zero`ã€‚

å…³äº `TrainingArguments` ç±»ï¼š

- `TrainingArguments` çš„ `evaluate_during_training` å‚æ•°å·²å¼ƒç”¨,æ”¹ç”¨ `eval_strategy`ã€‚

å…³äº Transfo-XL æ¨¡å‹ï¼š

- Transfo-XL çš„é…ç½®å±æ€§ `tie_weight` å˜ä¸º `tie_words_embeddings`ã€‚
- Transfo-XL çš„å»ºæ¨¡æ–¹æ³• `reset_length` å˜ä¸º `reset_memory_length`ã€‚

å…³äº pipelineï¼š

- `FillMaskPipeline` çš„ `topk` å‚æ•°å˜ä¸º `top_k`ã€‚

## ä» pytorch-transformers è¿ç§»åˆ° ğŸ¤— Transformers

ä»¥ä¸‹æ˜¯ä» `pytorch-transformers` è¿ç§»åˆ° ğŸ¤— Transformers æ—¶éœ€è¦æ³¨æ„çš„äº‹é¡¹çš„ç®€è¦æ‘˜è¦ã€‚

### æŸäº›æ¨¡å‹å…³é”®å­—è¾“å…¥çš„ä½ç½®é¡ºåºï¼ˆ`attention_mask`ã€`token_type_ids`...ï¼‰å·²æ›´æ”¹

ä¸ºäº†ä½¿ç”¨ Torchscriptï¼ˆå‚è§ #1010ã€#1204 å’Œ #1195ï¼‰ï¼ŒæŸäº›æ¨¡å‹çš„**å…³é”®å­—è¾“å…¥**ï¼ˆ`attention_mask`ã€`token_type_ids`...ï¼‰çš„ç‰¹å®šé¡ºåºå·²è¢«ä¿®æ”¹ã€‚

å¦‚æœæ‚¨ä½¿ç”¨å…³é”®å­—å‚æ•°åˆå§‹åŒ–æ¨¡å‹ï¼Œä¾‹å¦‚ `model(inputs_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)`ï¼Œè¿™ä¸åº”è¯¥å¼•èµ·ä»»ä½•å˜åŒ–ã€‚

å¦‚æœæ‚¨ä½¿ç”¨ä½ç½®å‚æ•°åˆå§‹åŒ–æ¨¡å‹ï¼Œä¾‹å¦‚ `model(inputs_ids, attention_mask, token_type_ids)`ï¼Œæ‚¨å¯èƒ½éœ€è¦ä»”ç»†æ£€æŸ¥è¾“å…¥å‚æ•°çš„ç¡®åˆ‡é¡ºåºã€‚

## ä» pytorch-pretrained-bert è¿ç§»

ä»¥ä¸‹æ˜¯ä» `pytorch-pretrained-bert` è¿ç§»åˆ° ğŸ¤— Transformers æ—¶éœ€è¦æ³¨æ„çš„äº‹é¡¹çš„ç®€è¦æ‘˜è¦ã€‚

### æ¨¡å‹å§‹ç»ˆè¿”å› `tuple`

ä» `pytorch-pretrained-bert` è¿ç§»åˆ° ğŸ¤— Transformers çš„ä¸»è¦é‡å¤§å˜åŒ–æ˜¯ï¼Œæ¨¡å‹çš„é¢„æµ‹æ–¹æ³•å§‹ç»ˆè¿”å›ä¸€ä¸ª `tuple`ï¼Œå…¶ä¸­åŒ…å«å„ç§å…ƒç´ ï¼Œå…·ä½“å–å†³äºæ¨¡å‹å’Œé…ç½®å‚æ•°ã€‚

æ¯ä¸ªæ¨¡å‹çš„å…ƒç»„çš„ç¡®åˆ‡å†…å®¹è¯¦ç»†æ˜¾ç¤ºåœ¨æ¨¡å‹çš„æ–‡æ¡£å­—ç¬¦ä¸²å’Œ[æ–‡æ¡£](https://huggingface.co/transformers/)ä¸­ã€‚

åœ¨å‡ ä¹æ‰€æœ‰æƒ…å†µä¸‹ï¼Œåªéœ€è·å–è¾“å‡ºçš„ç¬¬ä¸€ä¸ªå…ƒç´ å³å¯è·å¾—æ‚¨ä»¥å‰åœ¨ `pytorch-pretrained-bert` ä¸­ä½¿ç”¨çš„å†…å®¹ã€‚

ä»¥ä¸‹æ˜¯ä» `pytorch-pretrained-bert` è½¬æ¢ä¸º ğŸ¤— Transformers çš„ `BertForSequenceClassification` åˆ†ç±»æ¨¡å‹çš„ç¤ºä¾‹ï¼š

```python
# åŠ è½½æˆ‘ä»¬çš„æ¨¡å‹
model = BertForSequenceClassification.from_pretrained("google-bert/bert-base-uncased")

# å¦‚æœæ‚¨åœ¨ pytorch-pretrained-bert ä¸­ä½¿ç”¨æ­¤è¡Œï¼š
loss = model(input_ids, labels=labels)

# ç°åœ¨åœ¨ ğŸ¤— Transformers ä¸­ä½¿ç”¨æ­¤è¡Œä»è¾“å‡ºå…ƒç»„ä¸­æå–æŸå¤±ï¼š
outputs = model(input_ids, labels=labels)
loss = outputs[0]

# åœ¨ ğŸ¤— Transformers ä¸­ï¼Œæ‚¨è¿˜å¯ä»¥è®¿é—® logitsï¼š
loss, logits = outputs[:2]

# å¦‚æœæ‚¨å°†æ¨¡å‹é…ç½®ä¸ºè¿”å›æ³¨æ„åŠ›æƒé‡ï¼Œæ‚¨è¿˜å¯ä»¥è®¿é—®å®ƒä»¬ï¼ˆä»¥åŠå…¶ä»–è¾“å‡ºï¼Œè¯·å‚é˜…æ–‡æ¡£å­—ç¬¦ä¸²å’Œæ–‡æ¡£ï¼‰
model = BertForSequenceClassification.from_pretrained("google-bert/bert-base-uncased", output_attentions=True)
outputs = model(input_ids, labels=labels)
loss, logits, attentions = outputs
```

### åºåˆ—åŒ–

`from_pretrained()` æ–¹æ³•çš„é‡å¤§å˜åŒ–ï¼š

1. å½“ä½¿ç”¨ `from_pretrained()` æ–¹æ³•æ—¶ï¼Œæ¨¡å‹ç°åœ¨é»˜è®¤è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ã€‚è¦è®­ç»ƒå®ƒä»¬ï¼Œä¸è¦å¿˜è®°å°†å®ƒä»¬æ¢å¤åˆ°è®­ç»ƒæ¨¡å¼ï¼ˆ`model.train()`ï¼‰ä»¥æ¿€æ´» dropout æ¨¡å—ã€‚

2. æä¾›ç»™ `from_pretrained()` æ–¹æ³•çš„é¢å¤–å‚æ•° `*inputs` å’Œ `**kwargs` ä»¥å‰ç›´æ¥ä¼ é€’ç»™åº•å±‚æ¨¡å‹ç±»çš„ `__init__()` æ–¹æ³•ã€‚ç°åœ¨å®ƒä»¬é¦–å…ˆç”¨äºæ›´æ–°æ¨¡å‹çš„é…ç½®å±æ€§ï¼Œè¿™å¯èƒ½ä¸é€‚ç”¨äºåŸºäºå…ˆå‰ `BertForSequenceClassification` ç¤ºä¾‹æ„å»ºçš„æ´¾ç”Ÿæ¨¡å‹ç±»ã€‚æ›´å‡†ç¡®åœ°è¯´ï¼Œæä¾›ç»™ `from_pretrained()` çš„ä½ç½®å‚æ•° `*inputs` ä¼šç›´æ¥è½¬å‘åˆ°æ¨¡å‹çš„ `__init__()` æ–¹æ³•ï¼Œè€Œå…³é”®å­—å‚æ•° `**kwargs` (i) ä¸é…ç½®ç±»çš„å±æ€§åŒ¹é…çš„ï¼Œç”¨äºæ›´æ–°è¿™äº›å±æ€§ (ii) ä¸é…ç½®ç±»çš„ä»»ä½•å±æ€§éƒ½ä¸åŒ¹é…çš„ï¼Œåˆ™è½¬å‘åˆ° `__init__()` æ–¹æ³•ã€‚

æ­¤å¤–ï¼Œè™½ç„¶è¿™ä¸æ˜¯é‡å¤§å˜åŒ–ï¼Œä½†åºåˆ—åŒ–æ–¹æ³•å·²ç»æ ‡å‡†åŒ–ï¼Œå¦‚æœæ‚¨ä»¥å‰ä½¿ç”¨ä»»ä½•å…¶ä»–åºåˆ—åŒ–æ–¹æ³•ï¼Œæ‚¨å¯èƒ½åº”è¯¥åˆ‡æ¢åˆ°æ–°çš„ `save_pretrained(save_directory)` æ–¹æ³•ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼š

```python
### åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model = BertForSequenceClassification.from_pretrained("google-bert/bert-base-uncased")
tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-uncased")

### è®©æˆ‘ä»¬å¯¹æ¨¡å‹å’Œåˆ†è¯å™¨åšä¸€äº›äº‹æƒ…
# ä¾‹å¦‚ï¼šå‘æˆ‘ä»¬æ¨¡å‹çš„è¯æ±‡è¡¨å’ŒåµŒå…¥ä¸­æ·»åŠ æ–°æ ‡è®°
tokenizer.add_tokens(["[SPECIAL_TOKEN_1]", "[SPECIAL_TOKEN_2]"])
model.resize_token_embeddings(len(tokenizer))

# è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹
train(model)

### ç°åœ¨å°†æˆ‘ä»¬çš„æ¨¡å‹å’Œåˆ†è¯å™¨ä¿å­˜åˆ°æ–‡ä»¶å¤¹
model.save_pretrained("./my_saved_model_directory/")
tokenizer.save_pretrained("./my_saved_model_directory/")

### é‡æ–°åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model = BertForSequenceClassification.from_pretrained("./my_saved_model_directory/")

tokenizer = BertTokenizer.from_pretrained("./my_saved_model_directory/")
```

### ä¼˜åŒ–å™¨ï¼šBertAdam å’Œ OpenAIAdam ç°åœ¨æ˜¯ AdamWï¼Œè°ƒåº¦å™¨æ˜¯æ ‡å‡† PyTorch

ä»¥å‰åŒ…å«çš„ä¸¤ä¸ªä¼˜åŒ–å™¨ `BertAdam` å’Œ `OpenAIAdam` å·²è¢«å•ä¸ª `AdamW` æ›¿æ¢ï¼Œå®ƒå…·æœ‰ä¸€äº›å·®å¼‚ï¼š

- å®ƒä»…å®ç°æƒé‡è¡°å‡æ ¡æ­£ï¼Œ
- è°ƒåº¦å™¨ç°åœ¨æ˜¯å¤–éƒ¨çš„ï¼ˆè§ä¸‹æ–‡ï¼‰ï¼Œ
- æ¢¯åº¦è£å‰ªç°åœ¨ä¹Ÿæ˜¯å¤–éƒ¨çš„ï¼ˆè§ä¸‹æ–‡ï¼‰ã€‚

æ–°çš„ä¼˜åŒ–å™¨ `AdamW` ä¸ PyTorch çš„ `Adam` API ç›¸åŒ¹é…ï¼Œå¹¶å…è®¸æ‚¨ä½¿ç”¨ PyTorch æˆ– apex æ–¹æ³•è¿›è¡Œè°ƒåº¦å’Œè£å‰ªã€‚

è°ƒåº¦å™¨ç°åœ¨æ˜¯æ ‡å‡†çš„ [PyTorch å­¦ä¹ ç‡è°ƒåº¦å™¨](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)ï¼Œä¸å†æ˜¯ä¼˜åŒ–å™¨çš„ä¸€éƒ¨åˆ†ã€‚

ä»¥ä¸‹æ˜¯ä½¿ç”¨ `BertAdam` å’Œ `AdamW` è¿›è¡Œçº¿æ€§é¢„çƒ­å’Œè¡°å‡çš„ç¤ºä¾‹ï¼š

```python
# å‚æ•°ï¼š
lr = 1e-3
max_grad_norm = 1.0
num_training_steps = 1000
num_warmup_steps = 100
warmup_proportion = float(num_warmup_steps) / float(num_training_steps)  # 0.1

### ä»¥å‰ BertAdam ä¼˜åŒ–å™¨æ˜¯è¿™æ ·å®ä¾‹åŒ–çš„ï¼š
optimizer = BertAdam(
   model.parameters(),
   lr=lr,
   schedule="warmup_linear",
   warmup=warmup_proportion,
   num_training_steps=num_training_steps,
)

### å¹¶ä¸”è¿™æ ·ä½¿ç”¨ï¼š
for batch in train_data:
   loss = model(batch)
   loss.backward()
   optimizer.step()

### åœ¨ ğŸ¤— Transformers ä¸­ï¼Œä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨æ˜¯åˆ†å¼€çš„ï¼Œå¹¶ä¸”è¿™æ ·ä½¿ç”¨ï¼š
optimizer = AdamW(
   model.parameters(), lr=lr, correct_bias=False
)  # è¦å¤åˆ¶ BertAdam çš„ç‰¹å®šè¡Œä¸ºï¼Œè¯·è®¾ç½® correct_bias=False

scheduler = get_linear_schedule_with_warmup(
   optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps
)  # PyTorch è°ƒåº¦å™¨

### åº”è¯¥è¿™æ ·ä½¿ç”¨ï¼š
for batch in train_data:
   loss = model(batch)
   loss.backward()
   torch.nn.utils.clip_grad_norm_(
      model.parameters(), max_grad_norm
   )  # æ¢¯åº¦è£å‰ªä¸å†åœ¨ AdamW ä¸­ï¼ˆå› æ­¤æ‚¨å¯ä»¥æ¯«æ— é—®é¢˜åœ°ä½¿ç”¨ ampï¼‰
   optimizer.step()
   scheduler.step()
```