<!--
Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.
-->

# å¯¼å‡ºä¸º TorchScript

<Tip>

è¿™æ˜¯å¼€å§‹ä½¿ç”¨ TorchScript è¿›è¡Œå®éªŒçš„èµ·ç‚¹ï¼Œæˆ‘ä»¬ä»åœ¨æ¢ç´¢å…¶åœ¨å˜é‡è¾“å…¥å¤§å°æ¨¡å‹ä¸­çš„èƒ½åŠ›ã€‚
è¿™æ˜¯æˆ‘ä»¬å…³æ³¨çš„ç„¦ç‚¹ï¼Œæˆ‘ä»¬å°†åœ¨å³å°†å‘å¸ƒçš„ç‰ˆæœ¬ä¸­æ·±å…¥åˆ†æï¼Œæä¾›æ›´å¤šçš„ä»£ç ç¤ºä¾‹ã€æ›´çµæ´»çš„å®ç°ä»¥åŠæ¯”è¾ƒ
Python ä»£ç ä¸ç¼–è¯‘ TorchScript çš„æ€§èƒ½åŸºå‡†ã€‚

</Tip>

æ ¹æ® [TorchScript æ–‡æ¡£](https://pytorch.org/docs/stable/jit.html)ï¼š

> TorchScript æ˜¯ä» PyTorch ä»£ç åˆ›å»ºå¯åºåˆ—åŒ–å’Œå¯ä¼˜åŒ–çš„æ¨¡å‹çš„ä¸€ç§æ–¹å¼ã€‚

æœ‰ä¸¤ä¸ª PyTorch æ¨¡å—ï¼š[JIT å’Œ TRACE](https://pytorch.org/docs/stable/jit.html)ã€‚
è¿™ä¸¤ä¸ªæ¨¡å—å…è®¸å¼€å‘äººå‘˜å°†å…¶æ¨¡å‹å¯¼å‡ºåˆ°å…¶ä»–ç¨‹åºä¸­é‡ç”¨ï¼Œæ¯”å¦‚é¢å‘æ•ˆç‡çš„ C++ ç¨‹åºã€‚

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæ¥å£ï¼Œå…è®¸æ‚¨å°† ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºä¸º TorchScriptï¼Œ
ä»¥ä¾¿åœ¨ä¸åŸºäº PyTorch çš„ Python ç¨‹åºä¸åŒçš„ç¯å¢ƒä¸­é‡ç”¨ã€‚
æœ¬æ–‡è§£é‡Šå¦‚ä½•ä½¿ç”¨ TorchScript å¯¼å‡ºå¹¶ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹ã€‚

å¯¼å‡ºæ¨¡å‹éœ€è¦ä¸¤ä¸ªæ­¥éª¤ï¼š

- ä½¿ç”¨ `torchscript` å‚æ•°å®ä¾‹åŒ–æ¨¡å‹
- ä½¿ç”¨è™šæ‹Ÿè¾“å…¥è¿›è¡Œå‰å‘ä¼ é€’

è¿™äº›å¿…è¦æ¡ä»¶æ„å‘³ç€å¼€å‘äººå‘˜åº”è¯¥æ³¨æ„ä»¥ä¸‹è¯¦ç»†ä¿¡æ¯ã€‚

## TorchScript å‚æ•°å’Œç»‘å®šæƒé‡

`torchscript` å‚æ•°æ˜¯å¿…éœ€çš„ï¼Œå› ä¸ºå¤§å¤šæ•° ğŸ¤— Transformers è¯­è¨€æ¨¡å‹çš„ `Embedding` å±‚å’Œ
`Decoding` å±‚ä¹‹é—´æœ‰ç»‘å®šæƒé‡ã€‚TorchScript ä¸å…è®¸å¯¼å‡ºå…·æœ‰ç»‘å®šæƒé‡çš„æ¨¡å‹ï¼Œå› æ­¤å¿…é¡»äº‹å…ˆè§£ç»‘å’Œå…‹éš†æƒé‡ã€‚

ä½¿ç”¨ `torchscript` å‚æ•°å®ä¾‹åŒ–çš„æ¨¡å‹å°†å…¶ `Embedding` å±‚å’Œ `Decoding` å±‚åˆ†å¼€ï¼Œ
è¿™æ„å‘³ç€å®ƒä»¬ä¸åº”è¯¥åœ¨åç»­è¿›è¡Œè®­ç»ƒã€‚è®­ç»ƒå°†å¯¼è‡´è¿™ä¸¤å±‚ä¸åŒæ­¥ï¼Œäº§ç”Ÿæ„å¤–ç»“æœã€‚

å¯¹äºæ²¡æœ‰è¯­è¨€æ¨¡å‹å¤´éƒ¨çš„æ¨¡å‹ï¼Œæƒ…å†µä¸åŒï¼Œå› ä¸ºè¿™äº›æ¨¡å‹æ²¡æœ‰ç»‘å®šæƒé‡ã€‚
è¿™äº›æ¨¡å‹å¯ä»¥å®‰å…¨åœ°å¯¼å‡ºè€Œæ— éœ€ `torchscript` å‚æ•°ã€‚

## è™šæ‹Ÿè¾“å…¥å’Œæ ‡å‡†é•¿åº¦

è™šæ‹Ÿè¾“å…¥ç”¨äºæ¨¡å‹çš„å‰å‘ä¼ é€’ã€‚å½“è¾“å…¥çš„å€¼ä¼ æ’­åˆ°å„å±‚æ—¶ï¼ŒPyTorch ä¼šè·Ÿè¸ªåœ¨æ¯ä¸ªå¼ é‡ä¸Šæ‰§è¡Œçš„ä¸åŒæ“ä½œã€‚
ç„¶åä½¿ç”¨è®°å½•çš„æ“ä½œæ¥åˆ›å»ºæ¨¡å‹çš„ *trace* ã€‚

è·Ÿè¸ªæ˜¯ç›¸å¯¹äºè¾“å…¥çš„ç»´åº¦åˆ›å»ºçš„ã€‚å› æ­¤ï¼Œå®ƒå—åˆ°è™šæ‹Ÿè¾“å…¥çš„ç»´åº¦é™åˆ¶ï¼Œå¯¹äºä»»ä½•å…¶ä»–åºåˆ—é•¿åº¦æˆ–æ‰¹é‡å¤§å°éƒ½ä¸èµ·ä½œç”¨ã€‚
å½“å°è¯•ä½¿ç”¨ä¸åŒå¤§å°æ—¶ï¼Œä¼šå¼•å‘ä»¥ä¸‹é”™è¯¯ï¼š

```text
`The expanded size of the tensor (3) must match the existing size (7) at non-singleton dimension 2`
```

æˆ‘ä»¬å»ºè®®ä½¿ç”¨è‡³å°‘ä¸æ¨æ–­æœŸé—´å°†é¦ˆé€åˆ°æ¨¡å‹çš„æœ€å¤§è¾“å…¥ä¸€æ ·å¤§çš„è™šæ‹Ÿè¾“å…¥å¤§å°è¿›è¡Œè·Ÿè¸ªã€‚
å¡«å……å¯ä»¥å¸®åŠ©å¡«è¡¥ç¼ºå¤±çš„å€¼ã€‚ç„¶è€Œï¼Œç”±äºæ¨¡å‹æ˜¯ä½¿ç”¨æ›´å¤§çš„è¾“å…¥å¤§å°è¿›è¡Œè·Ÿè¸ªçš„ï¼ŒçŸ©é˜µçš„ç»´åº¦ä¹Ÿä¼šå¾ˆå¤§ï¼Œå¯¼è‡´æ›´å¤šçš„è®¡ç®—ã€‚

åœ¨æ¯ä¸ªè¾“å…¥ä¸Šæ‰§è¡Œçš„æ“ä½œæ€»æ•°è¦ä»”ç»†è€ƒè™‘ï¼Œå¹¶åœ¨å¯¼å‡ºä¸åŒåºåˆ—é•¿åº¦æ¨¡å‹æ—¶å¯†åˆ‡å…³æ³¨æ€§èƒ½ã€‚

## åœ¨ Python ä¸­ä½¿ç”¨ TorchScript

æœ¬èŠ‚æ¼”ç¤ºäº†å¦‚ä½•ä¿å­˜å’ŒåŠ è½½æ¨¡å‹ä»¥åŠå¦‚ä½•ä½¿ç”¨ trace è¿›è¡Œæ¨æ–­ã€‚

### ä¿å­˜æ¨¡å‹

è¦ä½¿ç”¨ TorchScript å¯¼å‡º `BertModel`ï¼Œè¯·ä» `BertConfig` ç±»å®ä¾‹åŒ– `BertModel`ï¼Œ
ç„¶åå°†å…¶ä¿å­˜åˆ°åä¸º `traced_bert.pt` çš„ç£ç›˜æ–‡ä»¶ä¸­ï¼š

```python
from transformers import BertModel, BertTokenizer, BertConfig
import torch

enc = BertTokenizer.from_pretrained("google-bert/bert-base-uncased")

# å¯¹è¾“å…¥æ–‡æœ¬åˆ†è¯
text = "[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]"
tokenized_text = enc.tokenize(text)

# å±è”½ä¸€ä¸ªè¾“å…¥ token
masked_index = 8
tokenized_text[masked_index] = "[MASK]"
indexed_tokens = enc.convert_tokens_to_ids(tokenized_text)
segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]

# åˆ›å»ºè™šæ‹Ÿè¾“å…¥
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])
dummy_input = [tokens_tensor, segments_tensors]

# ä½¿ç”¨ torchscript å‚æ•°åˆå§‹åŒ–æ¨¡å‹
# å³ä½¿æ­¤æ¨¡å‹æ²¡æœ‰ LM Headï¼Œä¹Ÿå°†å‚æ•°è®¾ç½®ä¸º Trueã€‚
config = BertConfig(
    vocab_size_or_config_json_file=32000,
    hidden_size=768,
    num_hidden_layers=12,
    num_attention_heads=12,
    intermediate_size=3072,
    torchscript=True,
)

# å®ä¾‹åŒ–æ¨¡å‹
model = BertModel(config)

# æ¨¡å‹éœ€è¦å¤„äºè¯„ä¼°æ¨¡å¼
model.eval()

# å¦‚æœæ‚¨ä½¿ç”¨ *from_pretrained* å®ä¾‹åŒ–æ¨¡å‹ï¼Œè¿˜å¯ä»¥è½»æ¾è®¾ç½® TorchScript å‚æ•°
model = BertModel.from_pretrained("google-bert/bert-base-uncased", torchscript=True)

# åˆ›å»º trace
traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])
torch.jit.save(traced_model, "traced_bert.pt")
```

### åŠ è½½æ¨¡å‹

ç°åœ¨ï¼Œæ‚¨å¯ä»¥ä»ç£ç›˜åŠ è½½å…ˆå‰ä¿å­˜çš„ `BertModel`ã€`traced_bert.pt`ï¼Œå¹¶åœ¨å…ˆå‰åˆå§‹åŒ–çš„ `dummy_input` ä¸Šä½¿ç”¨ï¼š

```python
loaded_model = torch.jit.load("traced_bert.pt")
loaded_model.eval()

all_encoder_layers, pooled_output = loaded_model(*dummy_input)
```

### ä½¿ç”¨ trace æ¨¡å‹è¿›è¡Œæ¨æ–­

é€šè¿‡ä½¿ç”¨å…¶ `__call__` dunder æ–¹æ³•ä½¿ç”¨ trace æ¨¡å‹è¿›è¡Œæ¨æ–­ï¼š

```python
traced_model(tokens_tensor, segments_tensors)
```

## ä½¿ç”¨ Neuron SDK å°† Hugging Face TorchScript æ¨¡å‹éƒ¨ç½²åˆ° AWS

AWS å¼•å…¥äº†ç”¨äºäº‘ç«¯ä½æˆæœ¬ã€é«˜æ€§èƒ½æœºå™¨å­¦ä¹ æ¨ç†çš„
[Amazon EC2 Inf1](https://aws.amazon.com/ec2/instance-types/inf1/) å®ä¾‹ç³»åˆ—ã€‚
Inf1 å®ä¾‹ç”± AWS Inferentia èŠ¯ç‰‡æä¾›æ”¯æŒï¼Œè¿™æ˜¯ä¸€æ¬¾ä¸“ä¸ºæ·±åº¦å­¦ä¹ æ¨ç†å·¥ä½œè´Ÿè½½è€Œæ„å»ºçš„å®šåˆ¶ç¡¬ä»¶åŠ é€Ÿå™¨ã€‚
[AWS Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/#) æ˜¯
Inferentia çš„ SDKï¼Œæ”¯æŒå¯¹ transformers æ¨¡å‹è¿›è¡Œè·Ÿè¸ªå’Œä¼˜åŒ–ï¼Œä»¥ä¾¿åœ¨ Inf1 ä¸Šéƒ¨ç½²ã€‚Neuron SDK æä¾›ï¼š

1. ç®€å•æ˜“ç”¨çš„ APIï¼Œåªéœ€æ›´æ”¹ä¸€è¡Œä»£ç å³å¯ä¸ºäº‘ç«¯æ¨ç†è·Ÿè¸ªå’Œä¼˜åŒ– TorchScript æ¨¡å‹ã€‚
2. é’ˆå¯¹[æ”¹è¿›çš„æ€§èƒ½æˆæœ¬](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/benchmark/)çš„å³æ’å³ç”¨æ€§èƒ½ä¼˜åŒ–ã€‚
3. æ”¯æŒä½¿ç”¨ [PyTorch](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/bert_tutorial/tutorial_pretrained_bert.html)
   æˆ– [TensorFlow](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/tensorflow/huggingface_bert/huggingface_bert.html)
   æ„å»ºçš„ Hugging Face transformers æ¨¡å‹ã€‚

### å½±å“

åŸºäº [BERTï¼ˆæ¥è‡ª Transformers çš„åŒå‘ç¼–ç å™¨è¡¨ç¤ºï¼‰](https://huggingface.co/docs/transformers/main/model_doc/bert)æ¶æ„çš„
transformers æ¨¡å‹ï¼Œæˆ–å…¶å˜ä½“ï¼Œå¦‚ [distilBERT](https://huggingface.co/docs/transformers/main/model_doc/distilbert)
å’Œ [roBERTa](https://huggingface.co/docs/transformers/main/model_doc/roberta) åœ¨ Inf1 ä¸Šè¿è¡Œæœ€ä½³ï¼Œ
å¯ç”¨äºç”ŸæˆæŠ½å–å¼é—®ç­”ã€åºåˆ—åˆ†ç±»å’Œæ ‡è®°åˆ†ç±»ç­‰ä»»åŠ¡ã€‚ç„¶è€Œï¼Œæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä»å¯ä»¥é€‚åº”åœ¨ Inf1 ä¸Šè¿è¡Œï¼Œ
å¦‚è¿™ç¯‡ [AWS Neuron MarianMT æ•™ç¨‹](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/transformers-marianmt.html)æ‰€è¿°ã€‚
æœ‰å…³å¯ä»¥ç›´æ¥åœ¨ Inferentia ä¸Šè½¬æ¢çš„æ¨¡å‹çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜… Neuron æ–‡æ¡£çš„[æ¨¡å‹æ¶æ„é€‚é…](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/models/models-inferentia.html#models-inferentia)ç« èŠ‚ã€‚

### ä¾èµ–å…³ç³»

ä½¿ç”¨ AWS Neuron å°†æ¨¡å‹è½¬æ¢ä¸ºæ¨¡å‹éœ€è¦ä¸€ä¸ª
[Neuron SDK ç¯å¢ƒ](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-frameworks/pytorch-neuron/index.html#installation-guide)ï¼Œ
å®ƒå·²ç»é¢„å…ˆé…ç½®åœ¨ [AWS æ·±åº¦å­¦ä¹  AMI](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-launching.html)ä¸Šã€‚

### å°†æ¨¡å‹è½¬æ¢ä¸º AWS Neuron

ä½¿ç”¨ä¸ [Python ä¸­ä½¿ç”¨ TorchScript](torchscript#using-torchscript-in-python) ç›¸åŒçš„ä»£ç æ¥è·Ÿè¸ª
`BertModel` ä»¥å°†æ¨¡å‹è½¬æ¢ä¸º AWS NEURONã€‚å¯¼å…¥ `torch.neuron` æ¡†æ¶æ‰©å±•ä»¥é€šè¿‡ Python API è®¿é—® Neuron SDK çš„ç»„ä»¶ï¼š

```python
from transformers import BertModel, BertTokenizer, BertConfig
import torch
import torch.neuron
```

æ‚¨åªéœ€è¦ä¿®æ”¹ä¸‹é¢è¿™ä¸€è¡Œï¼š

```diff
- torch.jit.trace(model, [tokens_tensor, segments_tensors])
+ torch.neuron.trace(model, [token_tensor, segments_tensors])
```

è¿™æ ·å°±èƒ½ä½¿ Neuron SDK è·Ÿè¸ªæ¨¡å‹å¹¶å¯¹å…¶è¿›è¡Œä¼˜åŒ–ï¼Œä»¥åœ¨ Inf1 å®ä¾‹ä¸Šè¿è¡Œã€‚

è¦äº†è§£æœ‰å…³ AWS Neuron SDK åŠŸèƒ½ã€å·¥å…·ã€ç¤ºä¾‹æ•™ç¨‹å’Œæœ€æ–°æ›´æ–°çš„æ›´å¤šä¿¡æ¯ï¼Œ
è¯·å‚é˜… [AWS NeuronSDK æ–‡æ¡£](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html)ã€‚
