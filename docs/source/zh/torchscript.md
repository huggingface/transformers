<!--ç‰ˆæƒæ‰€æœ‰2022å¹´HuggingFaceå›¢é˜Ÿã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚
æ ¹æ®Apacheè®¸å¯è¯ç¬¬2.0ç‰ˆï¼ˆâ€œè®¸å¯è¯â€ï¼‰è·å¾—è®¸å¯ï¼›é™¤éç¬¦åˆè®¸å¯è¯çš„è§„å®šï¼Œå¦åˆ™æ‚¨ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ä½ç½®è·å–è®¸å¯è¯çš„å‰¯æœ¬
http://www.apache.org/licenses/LICENSE-2.0
é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼ŒæŒ‰â€œåŸæ ·â€åˆ†å‘çš„è½¯ä»¶æ ¹æ®è®¸å¯è¯åˆ†å‘ï¼Œå¹¶ä¸”æ²¡æœ‰ä»»ä½•å½¢å¼çš„æ‹…ä¿æˆ–æ¡ä»¶ã€‚è¯·å‚é˜…è®¸å¯è¯ä»¥äº†è§£ç‰¹å®šè¯­è¨€ä¸‹çš„æƒé™å’Œé™åˆ¶ã€‚å…·ä½“è¯­è¨€ä¸‹çš„æƒé™å’Œé™åˆ¶ã€‚
âš ï¸è¯·æ³¨æ„ï¼Œæ­¤æ–‡ä»¶æ˜¯ä½¿ç”¨Markdownç¼–å†™çš„ï¼Œä½†åŒ…å«æˆ‘ä»¬çš„æ–‡æ¡£ç”Ÿæˆå™¨ï¼ˆç±»ä¼¼äºMDXï¼‰çš„ç‰¹å®šè¯­æ³•ï¼Œå¯èƒ½æ— æ³•åœ¨æ‚¨çš„MarkdownæŸ¥çœ‹å™¨ä¸­æ­£ç¡®å‘ˆç°ã€‚
-->

# å¯¼å‡ºåˆ°TorchScript
<Tip>
è¿™æ˜¯æˆ‘ä»¬ä¸TorchScriptçš„å®éªŒçš„æœ€å¼€å§‹é˜¶æ®µï¼Œæˆ‘ä»¬ä»åœ¨æ¢ç´¢å…¶åœ¨å¯å˜è¾“å…¥å¤§å°æ¨¡å‹ä¸Šçš„èƒ½åŠ›ã€‚è¿™æ˜¯æˆ‘ä»¬çš„å…´è¶£é‡ç‚¹ï¼Œå¹¶å°†åœ¨å³å°†å‘å¸ƒçš„ç‰ˆæœ¬ä¸­è¿›è¡Œæ·±å…¥åˆ†æï¼Œæä¾›æ›´å¤šä»£ç ç¤ºä¾‹ï¼Œæ›´çµæ´»çš„å®ç°ä»¥åŠä¸ç¼–è¯‘åçš„TorchScriptçš„Pythonä»£ç è¿›è¡Œæ¯”è¾ƒçš„åŸºå‡†æµ‹è¯•ã€‚
</Tip>
</Tip>

æ ¹æ®[TorchScriptæ–‡æ¡£](https://pytorch.org/docs/stable/jit.html)ï¼š

>TorchScriptæ˜¯ä¸€ç§ä»PyTorchä»£ç åˆ›å»ºå¯åºåˆ—åŒ–å’Œå¯ä¼˜åŒ–æ¨¡å‹çš„æ–¹æ³•ã€‚

æœ‰ä¸¤ä¸ªPyTorchæ¨¡å—ï¼Œ[JITå’ŒTRACE](https://pytorch.org/docs/stable/jit.html)ï¼Œä½¿å¼€å‘äººå‘˜èƒ½å¤Ÿå¯¼å‡ºå…¶æ¨¡å‹ä»¥åœ¨å…¶ä»–ç¨‹åºä¸­é‡å¤ä½¿ç”¨ï¼Œæ¯”å¦‚é¢å‘æ•ˆç‡çš„C++ç¨‹åºã€‚

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæ¥å£ï¼Œå¯ä»¥è®©æ‚¨å°†ğŸ¤— Transformersæ¨¡å‹å¯¼å‡ºåˆ°TorchScriptä»¥ä¾¿å®ƒä»¬å¯ä»¥åœ¨ä¸åŸºäºPyTorchçš„Pythonç¨‹åºä¸åŒçš„ç¯å¢ƒä¸­é‡å¤ä½¿ç”¨ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†è§£é‡Šå¦‚ä½•ä½¿ç”¨TorchScriptå¯¼å‡ºå’Œä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹ã€‚

å¯¼å‡ºæ¨¡å‹éœ€è¦ä¸¤ä¸ªæ­¥éª¤ï¼š

-ä½¿ç”¨`torchscript`æ ‡å¿—è¿›è¡Œæ¨¡å‹å®ä¾‹åŒ–
-ä½¿ç”¨è™šæ‹Ÿè¾“å…¥è¿›è¡Œå‰å‘ä¼ é€’

è¿™äº›è¦æ±‚æ„å‘³ç€å¼€å‘äººå‘˜éœ€è¦æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼Œå¦‚ä¸‹æ‰€è¿°ã€‚

## TorchScriptæ ‡å¿—å’Œç»‘å®šæƒé‡

`torchscript`æ ‡å¿—æ˜¯å¿…éœ€çš„ï¼Œå› ä¸ºå¤§å¤šæ•°ğŸ¤— Transformersè¯­è¨€æ¨¡å‹å…¶`Embedding`å±‚å’Œ`Decoding` å±‚ä¹‹é—´æœ‰ç»‘å®šæƒé‡ã€‚

TorchScriptä¸å…è®¸æ‚¨å¯¼å‡ºå…·æœ‰ç»‘å®šæƒé‡çš„æ¨¡å‹ï¼Œå› æ­¤éœ€è¦åœ¨å¯¼å‡ºä¹‹å‰è§£é™¤ç»‘å®šå¹¶å…‹éš†æƒé‡ã€‚necessary to untie and clone the weights beforehand.

ä½¿ç”¨`torchscript`æ ‡å¿—å®ä¾‹åŒ–çš„æ¨¡å‹å°†å…¶â€œåµŒå…¥â€å±‚å’Œâ€œè§£ç â€å±‚åˆ†å¼€ï¼Œè¿™æ„å‘³ç€å®ƒä»¬ä¸åº”è¯¥è¢«è®­ç»ƒã€‚è®­ç»ƒå°†å¯¼è‡´ä¸¤ä¸ªå±‚ä¸åŒæ­¥ï¼Œå¯¼è‡´æ„å¤–ç»“æœã€‚

å¯¹äºæ²¡æœ‰è¯­è¨€æ¨¡å‹å¤´çš„æ¨¡å‹ä¸æ˜¯è¿™ç§æƒ…å†µï¼Œå› ä¸ºè¿™äº›æ¨¡å‹æ²¡æœ‰ç»‘å®šæƒé‡ã€‚è¿™äº›æ¨¡å‹å¯ä»¥å®‰å…¨åœ°å¯¼å‡ºï¼Œè€Œæ— éœ€ä½¿ç”¨`torchscript`æ ‡å¿—ã€‚

## è™šæ‹Ÿè¾“å…¥å’Œæ ‡å‡†é•¿åº¦ Dummy inputs and standard lengths

è™šæ‹Ÿè¾“å…¥ç”¨äºæ¨¡å‹çš„å‰å‘ä¼ é€’ã€‚åœ¨è¾“å…¥çš„å€¼é€šè¿‡å±‚æ—¶ï¼ŒPyTorchä¼šè·Ÿè¸ªæ¯ä¸ªå¼ é‡ä¸Šæ‰§è¡Œçš„ä¸åŒæ“ä½œã€‚ç„¶åä½¿ç”¨è¿™äº›è®°å½•çš„æ“ä½œæ¥åˆ›å»ºæ¨¡å‹çš„*trace*ã€‚

è¯¥traceæ˜¯ç›¸å¯¹äºè¾“å…¥çš„ç»´åº¦åˆ›å»ºçš„ã€‚å› æ­¤å®ƒå—åˆ°è™šæ‹Ÿè¾“å…¥çš„ç»´åº¦é™åˆ¶ï¼Œä¸é€‚ç”¨äºä»»ä½•å…¶ä»–åºåˆ—é•¿åº¦æˆ–æ‰¹æ¬¡å¤§å°ã€‚å°è¯•ä½¿ç”¨ä¸åŒå¤§å°æ—¶ï¼Œå°†å¼•å‘ä»¥ä¸‹é”™è¯¯ï¼š

```
`The expanded size of the tensor (3) must match the existing size (7) at non-singleton dimension 2`
```

æˆ‘ä»¬å»ºè®®æ‚¨ä½¿ç”¨è™šæ‹Ÿè¾“å…¥å¤§å°è‡³å°‘ä¸å°†åœ¨æ¨ç†æœŸé—´é¦ˆé€ç»™æ¨¡å‹çš„æœ€å¤§è¾“å…¥å¤§å°ä¸€æ ·å¤§è¿›è¡Œè·Ÿè¸ªã€‚å¡«å……å¯ä»¥å¸®åŠ©å¡«å……ç¼ºå¤±çš„å€¼ã€‚

ä½†æ˜¯ï¼Œç”±äºä½¿ç”¨è¾ƒå¤§çš„è¾“å…¥å¤§å°æ¥è·Ÿè¸ªæ¨¡å‹ï¼ŒçŸ©é˜µçš„ç»´åº¦ä¹Ÿä¼šå˜å¤§ï¼Œå¯¼è‡´æ›´å¤šçš„è®¡ç®—ã€‚
åœ¨å¯¼å‡ºå…·æœ‰ä¸åŒåºåˆ—é•¿åº¦çš„æ¨¡å‹æ—¶ï¼Œè¯·æ³¨æ„æ¯ä¸ªè¾“å…¥ä¸Šæ‰§è¡Œçš„æ“ä½œæ€»æ•°ï¼Œå¹¶å¯†åˆ‡å…³æ³¨æ€§èƒ½ã€‚

##  åœ¨Pythonä¸­ä½¿ç”¨TorchScript

æœ¬èŠ‚æ¼”ç¤ºäº†å¦‚ä½•ä¿å­˜å’ŒåŠ è½½æ¨¡å‹ä»¥åŠå¦‚ä½•ä½¿ç”¨traceè¿›è¡Œæ¨ç†ã€‚for inference.

### ä¿å­˜æ¨¡å‹

è¦ä½¿ç”¨TorchScriptå¯¼å‡º`BertModel`ï¼Œè¯·ä»`BertConfig`ç±»å®ä¾‹åŒ–`BertModel`ï¼Œç„¶åå°†å…¶ä¿å­˜åˆ°åä¸º`traced_bert.pt`çš„ç£ç›˜ä¸­ï¼š

```python
from transformers import BertModel, BertTokenizer, BertConfig
import torch

enc = BertTokenizer.from_pretrained("bert-base-uncased")

# Tokenizing input text
text = "[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]"
tokenized_text = enc.tokenize(text)

# Masking one of the input tokens
masked_index = 8
tokenized_text[masked_index] = "[MASK]"
indexed_tokens = enc.convert_tokens_to_ids(tokenized_text)
segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]

# Creating a dummy input
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])
dummy_input = [tokens_tensor, segments_tensors]

# Initializing the model with the torchscript flag
# Flag set to True even though it is not necessary as this model does not have an LM Head.
config = BertConfig(
    vocab_size_or_config_json_file=32000,
    hidden_size=768,
    num_hidden_layers=12,
    num_attention_heads=12,
    intermediate_size=3072,
    torchscript=True,
)

# Instantiating the model
model = BertModel(config)

# The model needs to be in evaluation mode
model.eval()

# If you are instantiating the model with *from_pretrained* you can also easily set the TorchScript flag
model = BertModel.from_pretrained("bert-base-uncased", torchscript=True)

# Creating the trace
traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])
torch.jit.save(traced_model, "traced_bert.pt")
```

### åŠ è½½æ¨¡å‹

ç°åœ¨å¯ä»¥ä»ç£ç›˜åŠ è½½å…ˆå‰ä¿å­˜çš„`BertModel`ï¼Œ`traced_bert.pt`ï¼Œå¹¶ä½¿ç”¨åœ¨å…ˆå‰åˆå§‹åŒ–çš„`dummy_input`ä¸Šè¿›è¡Œä½¿ç”¨ï¼š

```python
loaded_model = torch.jit.load("traced_bert.pt")
loaded_model.eval()

all_encoder_layers, pooled_output = loaded_model(*dummy_input)
```

### ä½¿ç”¨traceæ¨¡å‹è¿›è¡Œæ¨ç†

é€šè¿‡ä½¿ç”¨å…¶`__call__`æ–¹æ³•ä½¿ç”¨traceæ¨¡å‹è¿›è¡Œæ¨ç†ï¼š

```python
traced_model(tokens_tensor, segments_tensors)
```

## ä½¿ç”¨Neuron SDKå°†Hugging Face TorchScriptæ¨¡å‹éƒ¨ç½²åˆ°AWS
AWSæ¨å‡ºäº†[Amazon EC2 Inf1](https://aws.amazon.com/ec2/instance-types/inf1/)å®ä¾‹ç³»åˆ—ï¼Œç”¨äºäº‘ä¸­ä½æˆæœ¬ã€é«˜æ€§èƒ½çš„æœºå™¨å­¦ä¹ æ¨ç†ã€‚Inf1å®ä¾‹ç”±AWS InferentiaèŠ¯ç‰‡æä¾›æ”¯æŒï¼Œè¿™æ˜¯ä¸€æ¬¾å®šåˆ¶çš„ç¡¬ä»¶åŠ é€Ÿå™¨ï¼Œä¸“é—¨ç”¨äºæ·±åº¦å­¦ä¹ æ¨ç†å·¥ä½œè´Ÿè½½ã€‚[AWSNeuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/#)æ˜¯Inferentiaçš„SDKï¼Œæ”¯æŒå¯¹transformersæ¨¡å‹è¿›è¡Œè·Ÿè¸ªå’Œä¼˜åŒ–éƒ¨ç½²åˆ°Inf1ä¸Šã€‚Neuron SDKæä¾›ä»¥ä¸‹åŠŸèƒ½ï¼š
1. æ˜“äºä½¿ç”¨çš„APIï¼Œåªéœ€æ›´æ”¹ä¸€è¡Œä»£ç å³å¯å¯¹TorchScriptè¿›è¡Œè·Ÿè¸ªå’Œä¼˜åŒ–  ç”¨äºäº‘ä¸­æ¨ç†çš„æ¨¡å‹ã€‚
2. é’ˆå¯¹[æ”¹è¿›çš„  æˆæœ¬æ•ˆç›Šæ€§çš„æ€§èƒ½ä¼˜åŒ–ï¼ˆhttps://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/benchmark/ï¼‰ã€‚
3. æ”¯æŒä½¿ç”¨  
    [PyTorch](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/bert_tutorial/tutorial_pretrained_bert.html)  æˆ–  [TensorFlow](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/tensorflow/huggingface_bert/huggingface_bert.html)æ„å»ºçš„Hugging Face transformersæ¨¡å‹ã€‚

### å½±å“

åŸºäº[BERTï¼ˆåŒå‘ç¼–ç å™¨è¡¨ç¤ºæ¥è‡ªTransformersï¼‰çš„transformeræ¨¡å‹](https://huggingface.co/docs/transformers/main/model_doc/bert)æ¶æ„ï¼Œæˆ–å…¶å˜ä½“ï¼Œå¦‚[distilBERT](https://huggingface.co/docs/transformers/main/model_doc/distilbert)å’Œ[roBERTa](https://huggingface.co/docs/transformers/main/model_doc/roberta)åœ¨Inf1ä¸Šè¿è¡Œæœ€ä½³ï¼Œç”¨äºéç”Ÿæˆä»»åŠ¡ï¼Œå¦‚æŠ½å–æ€§é—®é¢˜å›ç­”ã€åºåˆ—åˆ†ç±»å’Œæ ‡è®°åˆ†ç±»ã€‚ç„¶è€Œï¼Œæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä»ç„¶å¯ä»¥é€‚åº”Inf1ä¸Šè¿è¡Œï¼Œæ ¹æ®è¿™ä¸ª[AWS Neuron MarianMT
æ•™ç¨‹](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/transformers-marianmt.html)ã€‚
æœ‰å…³å¯ä»¥åœ¨Inferentiaä¸Šç›´æ¥è½¬æ¢çš„æ¨¡å‹çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[æ¨¡å‹æ¶æ„é€‚é…](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/models/models-inferentia.html#models-inferentia)éƒ¨åˆ†çš„Neuronæ–‡æ¡£ã€‚

### ä¾èµ–é¡¹
ä½¿ç”¨AWS Neuronè½¬æ¢æ¨¡å‹éœ€è¦[Neuron SDKç¯å¢ƒ](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-frameworks/pytorch-neuron/index.html#installation-guide)å·²ç»é¢„é…ç½®åœ¨[AW+AWSæ·±åº¦å­¦ä¹ AMI](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-launching.html)ä¸Šã€‚

### å°†æ¨¡å‹è½¬æ¢ä¸ºAWS Neuron

ä½¿ç”¨ä¸[åœ¨Pythonä¸­ä½¿ç”¨TorchScript](torchscript#using-torchscript-in-python)ç›¸åŒçš„ä»£ç å°†æ¨¡å‹è½¬æ¢ä¸ºAWS NEURONã€‚

å¯¼å…¥`torch.neuron`æ¡†æ¶æ‰©å±•ï¼Œä»¥é€šè¿‡Python APIè®¿é—®Neuron SDKçš„ç»„ä»¶ï¼š

åªéœ€ä¿®æ”¹ä»¥ä¸‹è¡Œï¼šè¿™æ ·å¯ä»¥ä½¿Neuron SDKè·Ÿè¸ªæ¨¡å‹å¹¶å¯¹å…¶è¿›è¡Œä¼˜åŒ–ï¼Œä»¥ç”¨äºInf1å®ä¾‹ã€‚è¦äº†è§£æœ‰å…³AWS Neuron SDKåŠŸèƒ½ã€å·¥å…·ã€ç¤ºä¾‹æ•™ç¨‹å’Œæœ€æ–°æ›´æ–°çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[AWS Neuron SDKæ–‡æ¡£](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html)ã€‚
Python API:

```python
from transformers import BertModel, BertTokenizer, BertConfig
import torch
import torch.neuron
```

You only need to modify the following line:

```diff
- torch.jit.trace(model, [tokens_tensor, segments_tensors])
+ torch.neuron.trace(model, [token_tensor, segments_tensors])
```

This enables the Neuron SDK to trace the model and optimize it for Inf1 instances.

To learn more about AWS Neuron SDK features, tools, example tutorials and latest
updates, please see the [AWS NeuronSDK
documentation](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html).
