<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# å¯¼å‡ºä¸º ONNX

åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½² ğŸ¤— Transformers æ¨¡å‹é€šå¸¸éœ€è¦æˆ–è€…èƒ½å¤Ÿå—ç›Šäºï¼Œå°†æ¨¡å‹å¯¼å‡ºä¸ºå¯åœ¨ä¸“é—¨çš„è¿è¡Œæ—¶å’Œç¡¬ä»¶ä¸ŠåŠ è½½å’Œæ‰§è¡Œçš„åºåˆ—åŒ–æ ¼å¼ã€‚

ğŸ¤— Optimum æ˜¯ Transformers çš„æ‰©å±•ï¼Œå¯ä»¥é€šè¿‡å…¶ `exporters` æ¨¡å—å°†æ¨¡å‹ä» PyTorch æˆ– TensorFlow å¯¼å‡ºä¸º ONNX åŠ TFLite ç­‰åºåˆ—åŒ–æ ¼å¼ã€‚ğŸ¤— Optimum è¿˜æä¾›äº†ä¸€å¥—æ€§èƒ½ä¼˜åŒ–å·¥å…·ï¼Œå¯ä»¥åœ¨ç›®æ ‡ç¡¬ä»¶ä¸Šä»¥æœ€é«˜æ•ˆç‡è®­ç»ƒå’Œè¿è¡Œæ¨¡å‹ã€‚

æœ¬æŒ‡å—æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨ ğŸ¤— Optimum å°† ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºä¸º ONNXã€‚æœ‰å…³å°†æ¨¡å‹å¯¼å‡ºä¸º TFLite çš„æŒ‡å—ï¼Œè¯·å‚è€ƒ [å¯¼å‡ºä¸º TFLite é¡µé¢](tflite)ã€‚

## å¯¼å‡ºä¸º ONNX

[ONNX (Open Neural Network eXchange å¼€æ”¾ç¥ç»ç½‘ç»œäº¤æ¢)](http://onnx.ai) æ˜¯ä¸€ä¸ªå¼€æ”¾çš„æ ‡å‡†ï¼Œå®ƒå®šä¹‰äº†ä¸€ç»„é€šç”¨çš„è¿ç®—ç¬¦å’Œä¸€ç§é€šç”¨çš„æ–‡ä»¶æ ¼å¼ï¼Œç”¨äºè¡¨ç¤ºåŒ…æ‹¬ PyTorch å’Œ TensorFlow åœ¨å†…çš„å„ç§æ¡†æ¶ä¸­çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚å½“ä¸€ä¸ªæ¨¡å‹è¢«å¯¼å‡ºä¸º ONNXæ—¶ï¼Œè¿™äº›è¿ç®—ç¬¦è¢«ç”¨äºæ„å»ºè®¡ç®—å›¾ï¼ˆé€šå¸¸è¢«ç§°ä¸º*ä¸­é—´è¡¨ç¤º*ï¼‰ï¼Œè¯¥å›¾è¡¨ç¤ºæ•°æ®åœ¨ç¥ç»ç½‘ç»œä¸­çš„æµåŠ¨ã€‚

é€šè¿‡å…¬å¼€å…·æœ‰æ ‡å‡†åŒ–è¿ç®—ç¬¦å’Œæ•°æ®ç±»å‹çš„å›¾ï¼ŒONNXä½¿å¾—æ¨¡å‹èƒ½å¤Ÿè½»æ¾åœ¨ä¸åŒæ·±åº¦å­¦ä¹ æ¡†æ¶é—´åˆ‡æ¢ã€‚ä¾‹å¦‚ï¼Œåœ¨ PyTorch ä¸­è®­ç»ƒçš„æ¨¡å‹å¯ä»¥è¢«å¯¼å‡ºä¸º ONNXï¼Œç„¶åå†å¯¼å…¥åˆ° TensorFlowï¼ˆåä¹‹äº¦ç„¶ï¼‰ã€‚

å¯¼å‡ºä¸º ONNX åï¼Œæ¨¡å‹å¯ä»¥ï¼š
- é€šè¿‡ [å›¾ä¼˜åŒ–ï¼ˆgraph optimizationï¼‰](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/optimization) å’Œ [é‡åŒ–ï¼ˆquantizationï¼‰](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/quantization) ç­‰æŠ€æœ¯è¿›è¡Œæ¨ç†ä¼˜åŒ–ã€‚ 
- é€šè¿‡ [`ORTModelForXXX` ç±»](https://huggingface.co/docs/optimum/onnxruntime/package_reference/modeling_ort) ä½¿ç”¨ ONNX Runtime è¿è¡Œï¼Œå®ƒåŒæ ·éµå¾ªä½ ç†Ÿæ‚‰çš„ Transformers ä¸­çš„ `AutoModel` APIã€‚
- ä½¿ç”¨ [ä¼˜åŒ–æ¨ç†æµæ°´çº¿ï¼ˆpipelineï¼‰](https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/pipelines) è¿è¡Œï¼Œå…¶ API ä¸ ğŸ¤— Transformers ä¸­çš„ [`pipeline`] å‡½æ•°ç›¸åŒã€‚

ğŸ¤— Optimum é€šè¿‡åˆ©ç”¨é…ç½®å¯¹è±¡æä¾›å¯¹ ONNX å¯¼å‡ºçš„æ”¯æŒã€‚å¤šç§æ¨¡å‹æ¶æ„å·²ç»æœ‰ç°æˆçš„é…ç½®å¯¹è±¡ï¼Œå¹¶ä¸”é…ç½®å¯¹è±¡ä¹Ÿè¢«è®¾è®¡å¾—æ˜“äºæ‰©å±•ä»¥é€‚ç”¨äºå…¶ä»–æ¶æ„ã€‚

ç°æœ‰çš„é…ç½®åˆ—è¡¨è¯·å‚è€ƒ [ğŸ¤— Optimum æ–‡æ¡£](https://huggingface.co/docs/optimum/exporters/onnx/overview)ã€‚

æœ‰ä¸¤ç§æ–¹å¼å¯ä»¥å°† ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºä¸º ONNXï¼Œè¿™é‡Œæˆ‘ä»¬å±•ç¤ºè¿™ä¸¤ç§æ–¹æ³•ï¼š

- ä½¿ç”¨ ğŸ¤— Optimum çš„ CLIï¼ˆå‘½ä»¤è¡Œï¼‰å¯¼å‡ºã€‚
- ä½¿ç”¨ ğŸ¤— Optimum çš„ `optimum.onnxruntime` æ¨¡å—å¯¼å‡ºã€‚

### ä½¿ç”¨ CLI å°† ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºä¸º ONNX

è¦å°† ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºä¸º ONNXï¼Œé¦–å…ˆéœ€è¦å®‰è£…é¢å¤–çš„ä¾èµ–é¡¹ï¼š

```bash
pip install optimum[exporters]
```

è¯·å‚é˜… [ğŸ¤— Optimum æ–‡æ¡£](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model#exporting-a-model-to-onnx-using-the-cli) ä»¥æŸ¥çœ‹æ‰€æœ‰å¯ç”¨å‚æ•°ï¼Œæˆ–è€…åœ¨å‘½ä»¤è¡Œä¸­æŸ¥çœ‹å¸®åŠ©ï¼š

```bash
optimum-cli export onnx --help
```

è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œä»¥ä» ğŸ¤— Hub å¯¼å‡ºæ¨¡å‹çš„æ£€æŸ¥ç‚¹ï¼ˆcheckpointï¼‰ï¼Œä»¥ `distilbert/distilbert-base-uncased-distilled-squad` ä¸ºä¾‹ï¼š

```bash
optimum-cli export onnx --model distilbert/distilbert-base-uncased-distilled-squad distilbert_base_uncased_squad_onnx/
```

ä½ åº”è¯¥èƒ½åœ¨æ—¥å¿—ä¸­çœ‹åˆ°å¯¼å‡ºè¿›åº¦ä»¥åŠç”Ÿæˆçš„ `model.onnx` æ–‡ä»¶çš„ä¿å­˜ä½ç½®ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```bash
Validating ONNX model distilbert_base_uncased_squad_onnx/model.onnx...
	-[âœ“] ONNX model output names match reference model (start_logits, end_logits)
	- Validating ONNX Model output "start_logits":
		-[âœ“] (2, 16) matches (2, 16)
		-[âœ“] all values close (atol: 0.0001)
	- Validating ONNX Model output "end_logits":
		-[âœ“] (2, 16) matches (2, 16)
		-[âœ“] all values close (atol: 0.0001)
The ONNX export succeeded and the exported model was saved at: distilbert_base_uncased_squad_onnx
```

ä¸Šé¢çš„ç¤ºä¾‹è¯´æ˜äº†ä» ğŸ¤— Hub å¯¼å‡ºæ£€æŸ¥ç‚¹çš„è¿‡ç¨‹ã€‚å¯¼å‡ºæœ¬åœ°æ¨¡å‹æ—¶ï¼Œé¦–å…ˆéœ€è¦ç¡®ä¿å°†æ¨¡å‹çš„æƒé‡å’Œåˆ†è¯å™¨æ–‡ä»¶ä¿å­˜åœ¨åŒä¸€ç›®å½•ï¼ˆ`local_path`ï¼‰ä¸­ã€‚åœ¨ä½¿ç”¨ CLI æ—¶ï¼Œå°† `local_path` ä¼ é€’ç»™ `model` å‚æ•°ï¼Œè€Œä¸æ˜¯ ğŸ¤— Hub ä¸Šçš„æ£€æŸ¥ç‚¹åç§°ï¼Œå¹¶æä¾› `--task` å‚æ•°ã€‚ä½ å¯ä»¥åœ¨ [ğŸ¤— Optimum æ–‡æ¡£](https://huggingface.co/docs/optimum/exporters/task_manager)ä¸­æŸ¥çœ‹æ”¯æŒçš„ä»»åŠ¡åˆ—è¡¨ã€‚å¦‚æœæœªæä¾› `task` å‚æ•°ï¼Œå°†é»˜è®¤å¯¼å‡ºä¸å¸¦ç‰¹å®šä»»åŠ¡å¤´çš„æ¨¡å‹æ¶æ„ã€‚

```bash
optimum-cli export onnx --model local_path --task question-answering distilbert_base_uncased_squad_onnx/
```

ç”Ÿæˆçš„ `model.onnx` æ–‡ä»¶å¯ä»¥åœ¨æ”¯æŒ ONNX æ ‡å‡†çš„ [è®¸å¤šåŠ é€Ÿå¼•æ“ï¼ˆacceleratorsï¼‰](https://onnx.ai/supported-tools.html#deployModel) ä¹‹ä¸€ä¸Šè¿è¡Œã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ [ONNX Runtime](https://onnxruntime.ai/) åŠ è½½å’Œè¿è¡Œæ¨¡å‹ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
>>> from transformers import AutoTokenizer
>>> from optimum.onnxruntime import ORTModelForQuestionAnswering

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert_base_uncased_squad_onnx")
>>> model = ORTModelForQuestionAnswering.from_pretrained("distilbert_base_uncased_squad_onnx")
>>> inputs = tokenizer("What am I using?", "Using DistilBERT with ONNX Runtime!", return_tensors="pt")
>>> outputs = model(**inputs)
```

ä» Hub å¯¼å‡º TensorFlow æ£€æŸ¥ç‚¹çš„è¿‡ç¨‹ä¹Ÿä¸€æ ·ã€‚ä¾‹å¦‚ï¼Œä»¥ä¸‹æ˜¯ä» [Keras ç»„ç»‡](https://huggingface.co/keras-io) å¯¼å‡ºçº¯ TensorFlow æ£€æŸ¥ç‚¹çš„å‘½ä»¤ï¼š

```bash
optimum-cli export onnx --model keras-io/transformers-qa distilbert_base_cased_squad_onnx/
```

### ä½¿ç”¨ `optimum.onnxruntime` å°† ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºä¸º ONNX

é™¤äº† CLI ä¹‹å¤–ï¼Œä½ è¿˜å¯ä»¥ä½¿ç”¨ä»£ç å°† ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºä¸º ONNXï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
>>> from optimum.onnxruntime import ORTModelForSequenceClassification
>>> from transformers import AutoTokenizer

>>> model_checkpoint = "distilbert_base_uncased_squad"
>>> save_directory = "onnx/"

>>> # ä» transformers åŠ è½½æ¨¡å‹å¹¶å°†å…¶å¯¼å‡ºä¸º ONNX
>>> ort_model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, export=True)
>>> tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

>>> # ä¿å­˜ onnx æ¨¡å‹ä»¥åŠåˆ†è¯å™¨
>>> ort_model.save_pretrained(save_directory)
>>> tokenizer.save_pretrained(save_directory)
```

### å¯¼å‡ºå°šæœªæ”¯æŒçš„æ¶æ„çš„æ¨¡å‹

å¦‚æœä½ æƒ³è¦ä¸ºå½“å‰æ— æ³•å¯¼å‡ºçš„æ¨¡å‹æ·»åŠ æ”¯æŒï¼Œè¯·å…ˆæ£€æŸ¥ [`optimum.exporters.onnx`](https://huggingface.co/docs/optimum/exporters/onnx/overview) æ˜¯å¦æ”¯æŒè¯¥æ¨¡å‹ï¼Œå¦‚æœä¸æ”¯æŒï¼Œä½ å¯ä»¥ [ç›´æ¥ä¸º ğŸ¤— Optimum è´¡çŒ®ä»£ç ](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/contribute)ã€‚

### ä½¿ç”¨ `transformers.onnx` å¯¼å‡ºæ¨¡å‹

<Tip warning={true}>

`tranformers.onnx` ä¸å†è¿›è¡Œç»´æŠ¤ï¼Œè¯·å¦‚ä¸Šæ‰€è¿°ï¼Œä½¿ç”¨ ğŸ¤— Optimum å¯¼å‡ºæ¨¡å‹ã€‚è¿™éƒ¨åˆ†å†…å®¹å°†åœ¨æœªæ¥ç‰ˆæœ¬ä¸­åˆ é™¤ã€‚

</Tip>

è¦ä½¿ç”¨ `tranformers.onnx` å°† ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºä¸º ONNXï¼Œè¯·å®‰è£…é¢å¤–çš„ä¾èµ–é¡¹ï¼š

```bash
pip install transformers[onnx]
```

å°† `transformers.onnx` åŒ…ä½œä¸º Python æ¨¡å—ä½¿ç”¨ï¼Œä»¥ä½¿ç”¨ç°æˆçš„é…ç½®å¯¼å‡ºæ£€æŸ¥ç‚¹ï¼š

```bash
python -m transformers.onnx --model=distilbert/distilbert-base-uncased onnx/
```

ä»¥ä¸Šä»£ç å°†å¯¼å‡ºç”± `--model` å‚æ•°å®šä¹‰çš„æ£€æŸ¥ç‚¹çš„ ONNX å›¾ã€‚ä¼ å…¥ä»»ä½• ğŸ¤— Hub ä¸Šæˆ–è€…å­˜å‚¨ä¸æœ¬åœ°çš„æ£€æŸ¥ç‚¹ã€‚ç”Ÿæˆçš„ `model.onnx` æ–‡ä»¶å¯ä»¥åœ¨æ”¯æŒ ONNX æ ‡å‡†çš„ä¼—å¤šåŠ é€Ÿå¼•æ“ä¸Šè¿è¡Œã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨ ONNX Runtime åŠ è½½å¹¶è¿è¡Œæ¨¡å‹ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
>>> from transformers import AutoTokenizer
>>> from onnxruntime import InferenceSession

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
>>> session = InferenceSession("onnx/model.onnx")
>>> # ONNX Runtime expects NumPy arrays as input
>>> inputs = tokenizer("Using DistilBERT with ONNX Runtime!", return_tensors="np")
>>> outputs = session.run(output_names=["last_hidden_state"], input_feed=dict(inputs))
```

å¯ä»¥é€šè¿‡æŸ¥çœ‹æ¯ä¸ªæ¨¡å‹çš„ ONNX é…ç½®æ¥è·å–æ‰€éœ€çš„è¾“å‡ºåï¼ˆä¾‹å¦‚ `["last_hidden_state"]`ï¼‰ã€‚ä¾‹å¦‚ï¼Œå¯¹äº DistilBERTï¼Œå¯ä»¥ç”¨ä»¥ä¸‹ä»£ç è·å–è¾“å‡ºåç§°ï¼š

```python
>>> from transformers.models.distilbert import DistilBertConfig, DistilBertOnnxConfig

>>> config = DistilBertConfig()
>>> onnx_config = DistilBertOnnxConfig(config)
>>> print(list(onnx_config.outputs.keys()))
["last_hidden_state"]
```

ä» Hub å¯¼å‡º TensorFlow æ£€æŸ¥ç‚¹çš„è¿‡ç¨‹ä¹Ÿä¸€æ ·ã€‚å¯¼å‡ºçº¯ TensorFlow æ£€æŸ¥ç‚¹çš„ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š

```bash
python -m transformers.onnx --model=keras-io/transformers-qa onnx/
```

è¦å¯¼å‡ºæœ¬åœ°å­˜å‚¨çš„æ¨¡å‹ï¼Œè¯·å°†æ¨¡å‹çš„æƒé‡å’Œåˆ†è¯å™¨æ–‡ä»¶ä¿å­˜åœ¨åŒä¸€ç›®å½•ä¸­ï¼ˆä¾‹å¦‚ `local-pt-checkpoint`ï¼‰ï¼Œç„¶åé€šè¿‡å°† `transformers.onnx` åŒ…çš„ `--model` å‚æ•°æŒ‡å‘è¯¥ç›®å½•ï¼Œå°†å…¶å¯¼å‡ºä¸º ONNXï¼š

```bash
python -m transformers.onnx --model=local-pt-checkpoint onnx/
```