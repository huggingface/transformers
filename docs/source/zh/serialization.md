<!--ç‰ˆæƒæ‰€æœ‰ 2020 å¹´ The HuggingFace å›¢é˜Ÿã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚
æ ¹æ® Apache è®¸å¯è¯ç¬¬ 2.0 ç‰ˆï¼ˆâ€œè®¸å¯è¯â€ï¼‰æˆæƒï¼›é™¤éç¬¦åˆè®¸å¯è¯ï¼Œå¦åˆ™æ‚¨ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚æ‚¨å¯ä»¥åœ¨
http://www.apache.org/licenses/LICENSE-2.0
é€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„å‰æä¸‹ï¼Œæ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯æŒ‰ "æŒ‰åŸæ ·" çš„åŸºç¡€åˆ†å‘çš„ï¼Œä¸é™„å¸¦ä»»ä½•å½¢å¼çš„æ‹…ä¿æˆ–æ¡ä»¶ã€‚è¯·å‚é˜…è®¸å¯è¯ä»¥äº†è§£ç‰¹å®šè¯­è¨€ä¸‹çš„æƒé™å’Œé™åˆ¶ã€‚
âš ï¸è¯·æ³¨æ„ï¼Œæ­¤æ–‡ä»¶æ˜¯ Markdown æ ¼å¼çš„ï¼Œä½†åŒ…å«äº†æˆ‘ä»¬ doc-builder çš„ç‰¹å®šè¯­æ³•ï¼ˆç±»ä¼¼äº MDXï¼‰ï¼Œå¯èƒ½æ— æ³•åœ¨æ‚¨çš„ Markdown æŸ¥çœ‹å™¨ä¸­æ­£ç¡®æ˜¾ç¤ºã€‚
-->

# å¯¼å‡ºä¸º ONNX

åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²ğŸ¤— Transformers æ¨¡å‹ç»å¸¸éœ€è¦å°†æ¨¡å‹å¯¼å‡ºä¸ºåºåˆ—åŒ–æ ¼å¼ï¼Œä»¥ä¾¿åœ¨ä¸“ç”¨è¿è¡Œæ—¶å’Œç¡¬ä»¶ä¸ŠåŠ è½½å’Œæ‰§è¡Œã€‚ğŸ¤— Optimum æ˜¯ Transformers çš„æ‰©å±•ï¼Œé€šè¿‡å…¶ `exporters` æ¨¡å—ï¼Œå¯ä»¥å°†æ¨¡å‹ä» PyTorch æˆ– TensorFlow å¯¼å‡ºä¸º ONNX å’Œ TFLite ç­‰åºåˆ—åŒ–æ ¼å¼ã€‚æ­¤å¤–ï¼ŒğŸ¤— Optimum è¿˜æä¾›äº†ä¸€å¥—æ€§èƒ½ä¼˜åŒ–å·¥å…·ï¼Œä»¥å®ç°åœ¨ç›®æ ‡ç¡¬ä»¶ä¸Šä»¥æœ€å¤§æ•ˆç‡è®­ç»ƒå’Œè¿è¡Œæ¨¡å‹ã€‚

æœ¬æŒ‡å—æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨ğŸ¤— Optimum å°†ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºä¸º ONNXã€‚å¦‚æœè¦äº†è§£å°†æ¨¡å‹å¯¼å‡ºä¸º TFLite çš„æŒ‡å—ï¼Œè¯·å‚é˜… [å¯¼å‡ºä¸º TFLite é¡µé¢](tflite)ã€‚


## å¯¼å‡ºä¸º ONNX

[ONNX (Open Neural Network eXchange)](http://onnx.ai) æ˜¯ä¸€ç§å¼€æ”¾æ ‡å‡†ï¼Œå®šä¹‰äº†ä¸€ç»„é€šç”¨çš„æ“ä½œç¬¦å’Œä¸€ç§é€šç”¨çš„æ–‡ä»¶æ ¼å¼ï¼Œç”¨äºåœ¨å„ç§æ¡†æ¶ï¼ˆåŒ…æ‹¬ PyTorch å’Œ TensorFlowï¼‰ä¸­è¡¨ç¤ºæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚å½“æ¨¡å‹å¯¼å‡ºåˆ° ONNX æ ¼å¼æ—¶ï¼Œè¿™äº›æ“ä½œç¬¦è¢«ç”¨äºæ„å»ºä¸€ä¸ªè®¡ç®—å›¾ï¼ˆé€šå¸¸ç§°ä¸º_ä¸­é—´è¡¨ç¤º_ï¼‰ï¼Œè¡¨ç¤ºæ•°æ®åœ¨ç¥ç»ç½‘ç»œä¸­çš„æµåŠ¨ã€‚


é€šè¿‡å…¬å¼€å…·æœ‰æ ‡å‡†åŒ–æ“ä½œç¬¦å’Œæ•°æ®ç±»å‹çš„å›¾è¡¨ï¼ŒONNX ä½¿å¾—åœ¨ä¸åŒæ¡†æ¶ä¹‹é—´åˆ‡æ¢å˜å¾—å®¹æ˜“ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥å°†åœ¨ PyTorch ä¸­è®­ç»ƒçš„æ¨¡å‹å¯¼å‡ºä¸º ONNX æ ¼å¼ï¼Œç„¶ååœ¨ TensorFlow ä¸­å¯¼å…¥ï¼ˆåä¹‹äº¦ç„¶ï¼‰ã€‚

å°†æ¨¡å‹å¯¼å‡ºä¸º ONNX æ ¼å¼åï¼Œå¯ä»¥è¿›è¡Œä»¥ä¸‹æ“ä½œï¼š

- é€šè¿‡æŠ€æœ¯æ‰‹æ®µï¼ˆä¾‹å¦‚ [å›¾ä¼˜åŒ–](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/optimization) å’Œ [é‡åŒ–](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/quantization)ï¼‰å¯¹æ¨ç†è¿›è¡Œä¼˜åŒ–ã€‚
- é€šè¿‡ [`ORTModelForXXX` ç±»](https://huggingface.co/docs/optimum/onnxruntime/package_reference/modeling_ort) ä½¿ç”¨ ONNX Runtime è¿è¡Œæ¨¡å‹ã€‚è¿™äº›ç±»ä¸ğŸ¤— Transformers ä¸­çš„ `AutoModel` API ç›¸åŒã€‚
- ä½¿ç”¨ [ä¼˜åŒ–çš„æ¨ç†æµæ°´çº¿](https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/pipelines) è¿è¡Œæ¨¡å‹ï¼Œè¯¥æµæ°´çº¿ä¸ğŸ¤— Transformers ä¸­çš„ [`pipeline`] å‡½æ•°å…·æœ‰ç›¸åŒçš„ APIã€‚


ğŸ¤— Optimum é€šè¿‡åˆ©ç”¨é…ç½®å¯¹è±¡æ¥æ”¯æŒ ONNX å¯¼å‡ºã€‚è¿™äº›é…ç½®å¯¹è±¡ä¸ºè®¸å¤šæ¨¡å‹æ¶æ„æä¾›äº†ç°æˆçš„æ”¯æŒï¼Œå¹¶ä¸”è¢«è®¾è®¡æˆæ˜“äºæ‰©å±•åˆ°å…¶ä»–æ¶æ„ã€‚

æœ‰ä¸¤ç§å°†ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºä¸º ONNX çš„æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œéƒ½å±•ç¤ºä¸€ä¸‹ï¼š


- é€šè¿‡ CLI ä½¿ç”¨ğŸ¤— Optimum è¿›è¡Œå¯¼å‡ºã€‚- ä½¿ç”¨ `optimum.onnxruntime` ä½¿ç”¨ğŸ¤— Optimum è¿›è¡Œå¯¼å‡ºã€‚
### ä½¿ç”¨ CLI å°†ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºä¸º ONNX
è¦å°†ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºä¸º ONNXï¼Œé¦–å…ˆå®‰è£…ä¸€ä¸ªé¢å¤–çš„ä¾èµ–é¡¹ï¼š
```bash
pip install optimum[exporters]
```

è¦æŸ¥çœ‹æ‰€æœ‰å¯ç”¨å‚æ•°ï¼Œè¯·å‚é˜… [ğŸ¤— Optimum æ–‡æ¡£](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model#exporting-a-model-to-onnx-using-the-cli)ï¼Œæˆ–åœ¨å‘½ä»¤è¡Œä¸­æŸ¥çœ‹å¸®åŠ©ï¼š
```bash
optimum-cli export onnx --help
```

è¦ä»ğŸ¤— Hub å¯¼å‡ºæ¨¡å‹çš„æ£€æŸ¥ç‚¹ï¼Œä¾‹å¦‚ `distilbert-base-uncased-distilled-squad`ï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
```bash
optimum-cli export onnx --model distilbert-base-uncased-distilled-squad distilbert_base_uncased_squad_onnx/
```

æ‚¨åº”è¯¥çœ‹åˆ°æ—¥å¿—æ˜¾ç¤ºè¿›åº¦ï¼Œå¹¶æ˜¾ç¤ºä¿å­˜ç»“æœçš„ `model.onnx` çš„ä½ç½®ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
```bash
Validating ONNX model distilbert_base_uncased_squad_onnx/model.onnx...
	-[âœ“] ONNX model output names match reference model (start_logits, end_logits)
	- Validating ONNX Model output "start_logits":
		-[âœ“] (2, 16) matches (2, 16)
		-[âœ“] all values close (atol: 0.0001)
	- Validating ONNX Model output "end_logits":
		-[âœ“] (2, 16) matches (2, 16)
		-[âœ“] all values close (atol: 0.0001)
The ONNX export succeeded and the exported model was saved at: distilbert_base_uncased_squad_onnx
```

ä¸Šé¢çš„ç¤ºä¾‹æ¼”ç¤ºäº†ä»ğŸ¤— Hub å¯¼å‡ºæ£€æŸ¥ç‚¹çš„è¿‡ç¨‹ã€‚å½“å¯¼å‡ºæœ¬åœ°æ¨¡å‹æ—¶ï¼Œè¯·ç¡®ä¿å°†æ¨¡å‹çš„æƒé‡å’Œæ ‡è®°å™¨æ–‡ä»¶ä¿å­˜åœ¨åŒä¸€ä¸ªç›®å½•ï¼ˆ`local_path`ï¼‰ä¸­ã€‚ä½¿ç”¨ CLI æ—¶ï¼Œå°† `local_path` ä½œä¸º `model` å‚æ•°ä¼ é€’ï¼Œè€Œä¸æ˜¯åœ¨ğŸ¤— Hub ä¸Šçš„æ£€æŸ¥ç‚¹åç§°ï¼Œå¹¶æä¾› `--task` å‚æ•°ã€‚

æ‚¨å¯ä»¥åœ¨ [ğŸ¤— Optimum æ–‡æ¡£](https://huggingface.co/docs/optimum/exporters/task_manager) ä¸­æŸ¥çœ‹æ”¯æŒçš„ä»»åŠ¡åˆ—è¡¨ã€‚

å¦‚æœæœªæä¾› `task` å‚æ•°ï¼Œå®ƒå°†é»˜è®¤ä¸ºä¸å¸¦ä»»ä½•ä»»åŠ¡ç‰¹å®šå¤´çš„æ¨¡å‹æ¶æ„ã€‚
```bash
optimum-cli export onnx --model local_path --task question-answering distilbert_base_uncased_squad_onnx/
```

ç„¶åï¼Œå¯ä»¥åœ¨æ”¯æŒ ONNX æ ‡å‡†çš„ [å¤šä¸ªåŠ é€Ÿå™¨](https://onnx.ai/supported-tools.html#deployModel) ä¸Šè¿è¡Œç”Ÿæˆçš„ `model.onnx` æ–‡ä»¶ã€‚
ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ [ONNXRuntime](https://onnxruntime.ai/) åŠ è½½å’Œè¿è¡Œæ¨¡å‹ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼šRuntime](https://onnxruntime.ai/) as follows:

```python
>>> from transformers import AutoTokenizer
>>> from optimum.onnxruntime import ORTModelForQuestionAnswering

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert_base_uncased_squad_onnx")
>>> model = ORTModelForQuestionAnswering.from_pretrained("distilbert_base_uncased_squad_onnx")
>>> inputs = tokenizer("What am I using?", "Using DistilBERT with ONNX Runtime!", return_tensors="pt")
>>> outputs = model(**inputs)
```

ä½¿ç”¨ TensorFlow Hub ä¸Šçš„ TensorFlow æ£€æŸ¥ç‚¹çš„è¿‡ç¨‹ç›¸åŒã€‚

ä¾‹å¦‚ï¼Œä»¥ä¸‹æ˜¯å¦‚ä½•ä» [Keras ç»„ç»‡](https://huggingface.co/keras-io) å¯¼å‡ºçº¯ TensorFlow æ£€æŸ¥ç‚¹ï¼š
```bash
optimum-cli export onnx --model keras-io/transformers-qa distilbert_base_cased_squad_onnx/
```

### ä½¿ç”¨ `optimum.onnxruntime` å°†ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºä¸º ONNX

é™¤äº† CLIï¼Œæ‚¨è¿˜å¯ä»¥é€šè¿‡ç¼–ç¨‹æ–¹å¼å°†ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºä¸º ONNXï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
```python
>>> from optimum.onnxruntime import ORTModelForSequenceClassification
>>> from transformers import AutoTokenizer

>>> model_checkpoint = "distilbert_base_uncased_squad"
>>> save_directory = "onnx/"

>>> # Load a model from transformers and export it to ONNX
>>> ort_model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, export=True)
>>> tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

>>> # Save the onnx model and tokenizer
>>> ort_model.save_pretrained(save_directory)
>>> tokenizer.save_pretrained(save_directory)
```

### å¯¼å‡ºä¸å—æ”¯æŒçš„æ¶æ„çš„æ¨¡å‹

å¦‚æœè¦é€šè¿‡æ·»åŠ å¯¹å½“å‰æ— æ³•å¯¼å‡ºçš„æ¨¡å‹çš„æ”¯æŒæ¥è¿›è¡Œè´¡çŒ®ï¼Œé¦–å…ˆåº”æ£€æŸ¥æ˜¯å¦åœ¨ [`optimum.exporters.onnx`](https://huggingface.co/docs/optimum/exporters/onnx/overview) ä¸­æ”¯æŒè¯¥æ¨¡å‹ï¼Œå¦‚æœä¸æ”¯æŒï¼Œè¯·ç›´æ¥ [å‘ğŸ¤— Optimum è´¡çŒ®](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/contribute)ã€‚directly.

### ä½¿ç”¨ `transformers.onnx` å¯¼å‡ºæ¨¡å‹

<Tip warning={true}>

`tranformers.onnx` ä¸å†ç»´æŠ¤ï¼Œè¯·æŒ‰ä¸Šè¿°æ–¹æ³•ä½¿ç”¨ğŸ¤— Optimum å¯¼å‡ºæ¨¡å‹ã€‚æ­¤éƒ¨åˆ†å°†åœ¨å°†æ¥çš„ç‰ˆæœ¬ä¸­åˆ é™¤ã€‚
</Tip>
è¦ä½¿ç”¨ `tranformers.onnx` å°†ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºä¸º ONNXï¼Œè¯·å®‰è£…é¢å¤–çš„ä¾èµ–é¡¹ï¼š
```bash
pip install transformers[onnx]
```

ä½¿ç”¨ `transformers.onnx` åŒ…ä½œä¸º Python æ¨¡å—ï¼Œä½¿ç”¨ç°æˆçš„é…ç½®å¯¼å‡ºæ£€æŸ¥ç‚¹ï¼š
```bash
python -m transformers.onnx --model=distilbert-base-uncased onnx/
```

è¿™å°†å¯¼å‡ºç”± `--model` å‚æ•°å®šä¹‰çš„æ£€æŸ¥ç‚¹çš„ ONNX å›¾ã€‚ä¼ é€’ğŸ¤— Hub ä¸Šçš„ä»»ä½•æ£€æŸ¥ç‚¹æˆ–æœ¬åœ°å­˜å‚¨çš„æ£€æŸ¥ç‚¹ã€‚ç„¶åï¼Œå¯ä»¥åœ¨æ”¯æŒ ONNX æ ‡å‡†çš„è®¸å¤šåŠ é€Ÿå™¨ä¸Šè¿è¡Œç”Ÿæˆçš„ `model.onnx` æ–‡ä»¶ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨ ONNX Runtime åŠ è½½å’Œè¿è¡Œæ¨¡å‹ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
```python
>>> from transformers import AutoTokenizer
>>> from onnxruntime import InferenceSession

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
>>> session = InferenceSession("onnx/model.onnx")
>>> # ONNX Runtime expects NumPy arrays as input
>>> inputs = tokenizer("Using DistilBERT with ONNX Runtime!", return_tensors="np")
>>> outputs = session.run(output_names=["last_hidden_state"], input_feed=dict(inputs))
```

æ‰€éœ€çš„è¾“å‡ºåç§°ï¼ˆå¦‚ `["last_hidden_state"]`ï¼‰å¯ä»¥é€šè¿‡æŸ¥çœ‹æ¯ä¸ªæ¨¡å‹çš„ ONNX é…ç½®æ¥è·å¾—ã€‚

ä¾‹å¦‚ï¼Œå¯¹äº DistilBERTï¼Œæˆ‘ä»¬æœ‰ï¼šeach model. For example, for DistilBERT we have:

```python
>>> from transformers.models.distilbert import DistilBertConfig, DistilBertOnnxConfig

>>> config = DistilBertConfig()
>>> onnx_config = DistilBertOnnxConfig(config)
>>> print(list(onnx_config.outputs.keys()))
["last_hidden_state"]
```

TensorFlow Hub ä¸Šçš„ TensorFlow æ£€æŸ¥ç‚¹çš„è¿‡ç¨‹ç›¸åŒã€‚ä¾‹å¦‚ï¼Œå¯¼å‡ºçº¯ TensorFlow æ£€æŸ¥ç‚¹çš„æ“ä½œå¦‚ä¸‹æ‰€ç¤ºï¼š
```bash
python -m transformers.onnx --model=keras-io/transformers-qa onnx/
```

è¦å¯¼å‡ºæœ¬åœ°å­˜å‚¨çš„æ¨¡å‹ï¼Œè¯·å°†æ¨¡å‹çš„æƒé‡å’Œæ ‡è®°å™¨æ–‡ä»¶ä¿å­˜åœ¨åŒä¸€ä¸ªç›®å½•ä¸­ï¼ˆä¾‹å¦‚ `local-pt-checkpoint`ï¼‰ï¼Œç„¶åé€šè¿‡å°† `transformers.onnx` åŒ…çš„ `--model` å‚æ•°æŒ‡å‘æ‰€éœ€ç›®å½•ï¼Œå°†å…¶å¯¼å‡ºä¸º ONNXï¼š
```bash
python -m transformers.onnx --model=local-pt-checkpoint onnx/
```