<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# é‡åŒ– ğŸ¤— Transformers æ¨¡å‹

## AWQé›†æˆ

AWQæ–¹æ³•å·²ç»åœ¨[*AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration*è®ºæ–‡](https://huggingface.co/papers/2306.00978)ä¸­å¼•å…¥ã€‚é€šè¿‡AWQï¼Œæ‚¨å¯ä»¥ä»¥4ä½ç²¾åº¦è¿è¡Œæ¨¡å‹ï¼ŒåŒæ—¶ä¿ç•™å…¶åŸå§‹æ€§èƒ½ï¼ˆå³æ²¡æœ‰æ€§èƒ½é™çº§ï¼‰ï¼Œå¹¶å…·æœ‰æ¯”ä¸‹é¢ä»‹ç»çš„å…¶ä»–é‡åŒ–æ–¹æ³•æ›´å‡ºè‰²çš„ååé‡ - è¾¾åˆ°ä¸çº¯`float16`æ¨ç†ç›¸ä¼¼çš„ååé‡ã€‚

æˆ‘ä»¬ç°åœ¨æ”¯æŒä½¿ç”¨ä»»ä½•AWQæ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œè¿™æ„å‘³ç€ä»»ä½•äººéƒ½å¯ä»¥åŠ è½½å’Œä½¿ç”¨åœ¨Hubä¸Šæ¨é€æˆ–æœ¬åœ°ä¿å­˜çš„AWQæƒé‡ã€‚è¯·æ³¨æ„ï¼Œä½¿ç”¨AWQéœ€è¦è®¿é—®NVIDIA GPUã€‚ç›®å‰ä¸æ”¯æŒCPUæ¨ç†ã€‚


### é‡åŒ–ä¸€ä¸ªæ¨¡å‹

æˆ‘ä»¬å»ºè®®ç”¨æˆ·æŸ¥çœ‹ç”Ÿæ€ç³»ç»Ÿä¸­ä¸åŒçš„ç°æœ‰å·¥å…·ï¼Œä»¥ä½¿ç”¨AWQç®—æ³•å¯¹å…¶æ¨¡å‹è¿›è¡Œé‡åŒ–ï¼Œä¾‹å¦‚ï¼š

- [`llm-awq`](https://github.com/mit-han-lab/llm-awq)ï¼Œæ¥è‡ªMIT Han Lab
- [`autoawq`](https://github.com/casper-hansen/AutoAWQ)ï¼Œæ¥è‡ª[`casper-hansen`](https://github.com/casper-hansen)
- Intel neural compressorï¼Œæ¥è‡ªIntel - é€šè¿‡[`optimum-intel`](https://huggingface.co/docs/optimum/main/en/intel/optimization_inc)ä½¿ç”¨

ç”Ÿæ€ç³»ç»Ÿä¸­å¯èƒ½å­˜åœ¨è®¸å¤šå…¶ä»–å·¥å…·ï¼Œè¯·éšæ—¶æå‡ºPRå°†å®ƒä»¬æ·»åŠ åˆ°åˆ—è¡¨ä¸­ã€‚
ç›®å‰ä¸ğŸ¤— Transformersçš„é›†æˆä»…é€‚ç”¨äºä½¿ç”¨`autoawq`å’Œ`llm-awq`é‡åŒ–åçš„æ¨¡å‹ã€‚å¤§å¤šæ•°ä½¿ç”¨`auto-awq`é‡åŒ–çš„æ¨¡å‹å¯ä»¥åœ¨ğŸ¤— Hubçš„[`TheBloke`](https://huggingface.co/TheBloke)å‘½åç©ºé—´ä¸‹æ‰¾åˆ°ï¼Œè¦ä½¿ç”¨`llm-awq`å¯¹æ¨¡å‹è¿›è¡Œé‡åŒ–ï¼Œè¯·å‚é˜…[`llm-awq`](https://github.com/mit-han-lab/llm-awq/)çš„ç¤ºä¾‹æ–‡ä»¶å¤¹ä¸­çš„[`convert_to_hf.py`](https://github.com/mit-han-lab/llm-awq/blob/main/examples/convert_to_hf.py)è„šæœ¬ã€‚


### åŠ è½½ä¸€ä¸ªé‡åŒ–çš„æ¨¡å‹

æ‚¨å¯ä»¥ä½¿ç”¨`from_pretrained`æ–¹æ³•ä»HubåŠ è½½ä¸€ä¸ªé‡åŒ–æ¨¡å‹ã€‚é€šè¿‡æ£€æŸ¥æ¨¡å‹é…ç½®æ–‡ä»¶ï¼ˆ`configuration.json`ï¼‰ä¸­æ˜¯å¦å­˜åœ¨`quantization_config`å±æ€§ï¼Œæ¥è¿›è¡Œç¡®è®¤æ¨é€çš„æƒé‡æ˜¯é‡åŒ–çš„ã€‚æ‚¨å¯ä»¥é€šè¿‡æ£€æŸ¥å­—æ®µ`quantization_config.quant_method`æ¥ç¡®è®¤æ¨¡å‹æ˜¯å¦ä»¥AWQæ ¼å¼è¿›è¡Œé‡åŒ–ï¼Œè¯¥å­—æ®µåº”è¯¥è®¾ç½®ä¸º`"awq"`ã€‚è¯·æ³¨æ„ï¼Œä¸ºäº†æ€§èƒ½åŸå› ï¼Œé»˜è®¤æƒ…å†µä¸‹åŠ è½½æ¨¡å‹å°†è®¾ç½®å…¶ä»–æƒé‡ä¸º`float16`ã€‚å¦‚æœæ‚¨æƒ³æ›´æ”¹è¿™ç§è®¾ç½®ï¼Œå¯ä»¥é€šè¿‡å°†`dtype`å‚æ•°è®¾ç½®ä¸º`torch.float32`æˆ–`torch.bfloat16`ã€‚åœ¨ä¸‹é¢çš„éƒ¨åˆ†ä¸­ï¼Œæ‚¨å¯ä»¥æ‰¾åˆ°ä¸€äº›ç¤ºä¾‹ç‰‡æ®µå’Œnotebookã€‚


## ç¤ºä¾‹ä½¿ç”¨

é¦–å…ˆï¼Œæ‚¨éœ€è¦å®‰è£…[`autoawq`](https://github.com/casper-hansen/AutoAWQ)åº“

```bash
pip install autoawq
```

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "TheBloke/zephyr-7B-alpha-AWQ"
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="cuda:0")
```

å¦‚æœæ‚¨é¦–å…ˆå°†æ¨¡å‹åŠ è½½åˆ°CPUä¸Šï¼Œè¯·ç¡®ä¿åœ¨ä½¿ç”¨ä¹‹å‰å°†å…¶ç§»åŠ¨åˆ°GPUè®¾å¤‡ä¸Šã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "TheBloke/zephyr-7B-alpha-AWQ"
model = AutoModelForCausalLM.from_pretrained(model_id).to("cuda:0")
```

### ç»“åˆ AWQ å’Œ Flash Attention

æ‚¨å¯ä»¥å°†AWQé‡åŒ–ä¸Flash Attentionç»“åˆèµ·æ¥ï¼Œå¾—åˆ°ä¸€ä¸ªæ—¢è¢«é‡åŒ–åˆæ›´å¿«é€Ÿçš„æ¨¡å‹ã€‚åªéœ€ä½¿ç”¨`from_pretrained`åŠ è½½æ¨¡å‹ï¼Œå¹¶ä¼ é€’`attn_implementation="flash_attention_2"`å‚æ•°ã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("TheBloke/zephyr-7B-alpha-AWQ", attn_implementation="flash_attention_2", device_map="cuda:0")
```

### åŸºå‡†æµ‹è¯•

æˆ‘ä»¬ä½¿ç”¨[`optimum-benchmark`](https://github.com/huggingface/optimum-benchmark)åº“è¿›è¡Œäº†ä¸€äº›é€Ÿåº¦ã€ååé‡å’Œå»¶è¿ŸåŸºå‡†æµ‹è¯•ã€‚

è¯·æ³¨æ„ï¼Œåœ¨ç¼–å†™æœ¬æ–‡æ¡£éƒ¨åˆ†æ—¶ï¼Œå¯ç”¨çš„é‡åŒ–æ–¹æ³•åŒ…æ‹¬ï¼š`awq`ã€`gptq`å’Œ`bitsandbytes`ã€‚

åŸºå‡†æµ‹è¯•åœ¨ä¸€å°NVIDIA-A100å®ä¾‹ä¸Šè¿è¡Œï¼Œä½¿ç”¨[`TheBloke/Mistral-7B-v0.1-AWQ`](https://huggingface.co/TheBloke/Mistral-7B-v0.1-AWQ)ä½œä¸ºAWQæ¨¡å‹ï¼Œ[`TheBloke/Mistral-7B-v0.1-GPTQ`](https://huggingface.co/TheBloke/Mistral-7B-v0.1-GPTQ)ä½œä¸ºGPTQæ¨¡å‹ã€‚æˆ‘ä»¬è¿˜å°†å…¶ä¸`bitsandbytes`é‡åŒ–æ¨¡å‹å’Œ`float16`æ¨¡å‹è¿›è¡Œäº†å¯¹æ¯”ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›ç»“æœç¤ºä¾‹ï¼š


<div style="text-align: center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/quantization/forward_memory_plot.png">
</div>

<div style="text-align: center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/quantization/generate_memory_plot.png">
</div>

<div style="text-align: center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/quantization/generate_throughput_plot.png">
</div>

<div style="text-align: center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/quantization/forward_latency_plot.png">
</div>

ä½ å¯ä»¥åœ¨[æ­¤é“¾æ¥](https://github.com/huggingface/optimum-benchmark/tree/main/examples/running-mistrals)ä¸­æ‰¾åˆ°å®Œæ•´çš„ç»“æœä»¥åŠåŒ…ç‰ˆæœ¬ã€‚

ä»ç»“æœæ¥çœ‹ï¼ŒAWQé‡åŒ–æ–¹æ³•æ˜¯æ¨ç†ã€æ–‡æœ¬ç”Ÿæˆä¸­æœ€å¿«çš„é‡åŒ–æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æ–‡æœ¬ç”Ÿæˆçš„å³°å€¼å†…å­˜æ–¹é¢å±äºæœ€ä½ã€‚ç„¶è€Œï¼Œå¯¹äºæ¯æ‰¹æ•°æ®ï¼ŒAWQä¼¼ä¹æœ‰æœ€å¤§çš„å‰å‘å»¶è¿Ÿã€‚


### Google colab æ¼”ç¤º

æŸ¥çœ‹å¦‚ä½•åœ¨[Google Colabæ¼”ç¤º](https://colab.research.google.com/drive/1HzZH89yAXJaZgwJDhQj9LqSBux932BvY)ä¸­ä½¿ç”¨æ­¤é›†æˆï¼


### AwqConfig

[[autodoc]] AwqConfig

## `AutoGPTQ` é›†æˆ

ğŸ¤— Transformerså·²ç»æ•´åˆäº†`optimum` APIï¼Œç”¨äºå¯¹è¯­è¨€æ¨¡å‹æ‰§è¡ŒGPTQé‡åŒ–ã€‚æ‚¨å¯ä»¥ä»¥8ã€4ã€3ç”šè‡³2ä½åŠ è½½å’Œé‡åŒ–æ‚¨çš„æ¨¡å‹ï¼Œè€Œæ€§èƒ½æ— æ˜æ˜¾ä¸‹é™ï¼Œå¹¶ä¸”æ¨ç†é€Ÿåº¦æ›´å¿«ï¼è¿™å—åˆ°å¤§å¤šæ•°GPUç¡¬ä»¶çš„æ”¯æŒã€‚

è¦äº†è§£æ›´å¤šå…³äºé‡åŒ–æ¨¡å‹çš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ï¼š
- [GPTQ](https://huggingface.co/papers/2210.17323)è®ºæ–‡
- `optimum`å…³äºGPTQé‡åŒ–çš„[æŒ‡å—](https://huggingface.co/docs/optimum/llm_quantization/usage_guides/quantization)
- ç”¨ä½œåç«¯çš„[`AutoGPTQ`](https://github.com/PanQiWei/AutoGPTQ)åº“


### è¦æ±‚

ä¸ºäº†è¿è¡Œä¸‹é¢çš„ä»£ç ï¼Œæ‚¨éœ€è¦å®‰è£…ï¼š

- å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„ `AutoGPTQ` åº“
`pip install auto-gptq`

- ä»æºä»£ç å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„`optimum`
`pip install git+https://github.com/huggingface/optimum.git`

- ä»æºä»£ç å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„`transformers`
`pip install git+https://github.com/huggingface/transformers.git`

- å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„`accelerate`åº“ï¼š 
`pip install --upgrade accelerate`

è¯·æ³¨æ„ï¼Œç›®å‰GPTQé›†æˆä»…æ”¯æŒæ–‡æœ¬æ¨¡å‹ï¼Œå¯¹äºè§†è§‰ã€è¯­éŸ³æˆ–å¤šæ¨¡æ€æ¨¡å‹å¯èƒ½ä¼šé‡åˆ°é¢„æœŸä»¥å¤–ç»“æœã€‚

### åŠ è½½å’Œé‡åŒ–æ¨¡å‹

GPTQæ˜¯ä¸€ç§åœ¨ä½¿ç”¨é‡åŒ–æ¨¡å‹ä¹‹å‰éœ€è¦è¿›è¡Œæƒé‡æ ¡å‡†çš„é‡åŒ–æ–¹æ³•ã€‚å¦‚æœæ‚¨æƒ³ä»å¤´å¼€å§‹å¯¹transformersæ¨¡å‹è¿›è¡Œé‡åŒ–ï¼Œç”Ÿæˆé‡åŒ–æ¨¡å‹å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼ˆåœ¨Google Colabä¸Šå¯¹`facebook/opt-350m`æ¨¡å‹é‡åŒ–çº¦ä¸º5åˆ†é’Ÿï¼‰ã€‚

å› æ­¤ï¼Œæœ‰ä¸¤ç§ä¸åŒçš„æƒ…å†µä¸‹æ‚¨å¯èƒ½æƒ³ä½¿ç”¨GPTQé‡åŒ–æ¨¡å‹ã€‚ç¬¬ä¸€ç§æƒ…å†µæ˜¯åŠ è½½å·²ç»ç”±å…¶ä»–ç”¨æˆ·åœ¨Hubä¸Šé‡åŒ–çš„æ¨¡å‹ï¼Œç¬¬äºŒç§æƒ…å†µæ˜¯ä»å¤´å¼€å§‹å¯¹æ‚¨çš„æ¨¡å‹è¿›è¡Œé‡åŒ–å¹¶ä¿å­˜æˆ–æ¨é€åˆ°Hubï¼Œä»¥ä¾¿å…¶ä»–ç”¨æˆ·ä¹Ÿå¯ä»¥ä½¿ç”¨å®ƒã€‚


#### GPTQ é…ç½®

ä¸ºäº†åŠ è½½å’Œé‡åŒ–ä¸€ä¸ªæ¨¡å‹ï¼Œæ‚¨éœ€è¦åˆ›å»ºä¸€ä¸ª[`GPTQConfig`]ã€‚æ‚¨éœ€è¦ä¼ é€’`bits`çš„æ•°é‡ï¼Œä¸€ä¸ªç”¨äºæ ¡å‡†é‡åŒ–çš„`dataset`ï¼Œä»¥åŠæ¨¡å‹çš„`tokenizer`ä»¥å‡†å¤‡æ•°æ®é›†ã€‚

```python 
model_id = "facebook/opt-125m"
tokenizer = AutoTokenizer.from_pretrained(model_id)
gptq_config = GPTQConfig(bits=4, dataset = "c4", tokenizer=tokenizer)
```

è¯·æ³¨æ„ï¼Œæ‚¨å¯ä»¥å°†è‡ªå·±çš„æ•°æ®é›†ä»¥å­—ç¬¦ä¸²åˆ—è¡¨å½¢å¼ä¼ é€’åˆ°æ¨¡å‹ã€‚ç„¶è€Œï¼Œå¼ºçƒˆå»ºè®®æ‚¨ä½¿ç”¨GPTQè®ºæ–‡ä¸­æä¾›çš„æ•°æ®é›†ã€‚


```python
dataset = ["auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm."]
quantization = GPTQConfig(bits=4, dataset = dataset, tokenizer=tokenizer)
```

#### é‡åŒ–

æ‚¨å¯ä»¥é€šè¿‡ä½¿ç”¨`from_pretrained`å¹¶è®¾ç½®`quantization_config`æ¥å¯¹æ¨¡å‹è¿›è¡Œé‡åŒ–ã€‚

```python
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=gptq_config)

```

è¯·æ³¨æ„ï¼Œæ‚¨éœ€è¦ä¸€ä¸ªGPUæ¥é‡åŒ–æ¨¡å‹ã€‚æˆ‘ä»¬å°†æ¨¡å‹æ”¾åœ¨cpuä¸­ï¼Œå¹¶å°†æ¨¡å—æ¥å›ç§»åŠ¨åˆ°gpuä¸­ï¼Œä»¥ä¾¿å¯¹å…¶è¿›è¡Œé‡åŒ–ã€‚

å¦‚æœæ‚¨æƒ³åœ¨ä½¿ç”¨ CPU å¸è½½çš„åŒæ—¶æœ€å¤§åŒ– GPU ä½¿ç”¨ç‡ï¼Œæ‚¨å¯ä»¥è®¾ç½® `device_map = "auto"`ã€‚


```python
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", quantization_config=gptq_config)
```

è¯·æ³¨æ„ï¼Œä¸æ”¯æŒç£ç›˜å¸è½½ã€‚æ­¤å¤–ï¼Œå¦‚æœç”±äºæ•°æ®é›†è€Œå†…å­˜ä¸è¶³ï¼Œæ‚¨å¯èƒ½éœ€è¦åœ¨`from_pretrained`ä¸­è®¾ç½®`max_memory`ã€‚æŸ¥çœ‹è¿™ä¸ª[æŒ‡å—](https://huggingface.co/docs/accelerate/usage_guides/big_modeling#designing-a-device-map)ä»¥äº†è§£æœ‰å…³`device_map`å’Œ`max_memory`çš„æ›´å¤šä¿¡æ¯ã€‚


<Tip warning={true}>
ç›®å‰ï¼ŒGPTQé‡åŒ–ä»…é€‚ç”¨äºæ–‡æœ¬æ¨¡å‹ã€‚æ­¤å¤–ï¼Œé‡åŒ–è¿‡ç¨‹å¯èƒ½ä¼šèŠ±è´¹å¾ˆå¤šæ—¶é—´ï¼Œå…·ä½“å–å†³äºç¡¬ä»¶æ€§èƒ½ï¼ˆ175Bæ¨¡å‹åœ¨NVIDIA A100ä¸Šéœ€è¦4å°æ—¶ï¼‰ã€‚è¯·åœ¨Hubä¸Šæ£€æŸ¥æ˜¯å¦æœ‰æ¨¡å‹çš„GPTQé‡åŒ–ç‰ˆæœ¬ã€‚å¦‚æœæ²¡æœ‰ï¼Œæ‚¨å¯ä»¥åœ¨GitHubä¸Šæäº¤éœ€æ±‚ã€‚ 
</Tip>

### æ¨é€é‡åŒ–æ¨¡å‹åˆ° ğŸ¤— Hub

æ‚¨å¯ä»¥ä½¿ç”¨`push_to_hub`å°†é‡åŒ–æ¨¡å‹åƒä»»ä½•æ¨¡å‹ä¸€æ ·æ¨é€åˆ°Hubã€‚é‡åŒ–é…ç½®å°†ä¸æ¨¡å‹ä¸€èµ·ä¿å­˜å’Œæ¨é€ã€‚

```python
quantized_model.push_to_hub("opt-125m-gptq")
tokenizer.push_to_hub("opt-125m-gptq")
```

å¦‚æœæ‚¨æƒ³åœ¨æœ¬åœ°è®¡ç®—æœºä¸Šä¿å­˜é‡åŒ–æ¨¡å‹ï¼Œæ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨`save_pretrained`æ¥å®Œæˆï¼š

```python
quantized_model.save_pretrained("opt-125m-gptq")
tokenizer.save_pretrained("opt-125m-gptq")
```

è¯·æ³¨æ„ï¼Œå¦‚æœæ‚¨é‡åŒ–æ¨¡å‹æ—¶æƒ³ä½¿ç”¨`device_map`ï¼Œè¯·ç¡®ä¿åœ¨ä¿å­˜ä¹‹å‰å°†æ•´ä¸ªæ¨¡å‹ç§»åŠ¨åˆ°æ‚¨çš„GPUæˆ–CPUä¹‹ä¸€ã€‚

```python
quantized_model.to("cpu")
quantized_model.save_pretrained("opt-125m-gptq")
```

### ä» ğŸ¤— Hub åŠ è½½ä¸€ä¸ªé‡åŒ–æ¨¡å‹

æ‚¨å¯ä»¥ä½¿ç”¨`from_pretrained`ä»HubåŠ è½½é‡åŒ–æ¨¡å‹ã€‚
è¯·ç¡®ä¿æ¨é€æƒé‡æ˜¯é‡åŒ–çš„ï¼Œæ£€æŸ¥æ¨¡å‹é…ç½®å¯¹è±¡ä¸­æ˜¯å¦å­˜åœ¨`quantization_config`å±æ€§ã€‚


```python
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained("{your_username}/opt-125m-gptq")
```

å¦‚æœæ‚¨æƒ³æ›´å¿«åœ°åŠ è½½æ¨¡å‹ï¼Œå¹¶ä¸”ä¸éœ€è¦åˆ†é…æ¯”å®é™…éœ€è¦å†…å­˜æ›´å¤šçš„å†…å­˜ï¼Œé‡åŒ–æ¨¡å‹ä¹Ÿä½¿ç”¨`device_map`å‚æ•°ã€‚ç¡®ä¿æ‚¨å·²å®‰è£…`accelerate`åº“ã€‚

```python
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained("{your_username}/opt-125m-gptq", device_map="auto")
```

### Exllamaå†…æ ¸åŠ å¿«æ¨ç†é€Ÿåº¦

ä¿ç•™æ ¼å¼ï¼šå¯¹äº 4 ä½æ¨¡å‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ exllama å†…æ ¸æ¥æé«˜æ¨ç†é€Ÿåº¦ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒå¤„äºå¯ç”¨çŠ¶æ€ã€‚æ‚¨å¯ä»¥é€šè¿‡åœ¨ [`GPTQConfig`] ä¸­ä¼ é€’ `use_exllama` æ¥æ›´æ”¹æ­¤é…ç½®ã€‚è¿™å°†è¦†ç›–å­˜å‚¨åœ¨é…ç½®ä¸­çš„é‡åŒ–é…ç½®ã€‚è¯·æ³¨æ„ï¼Œæ‚¨åªèƒ½è¦†ç›–ä¸å†…æ ¸ç›¸å…³çš„å±æ€§ã€‚æ­¤å¤–ï¼Œå¦‚æœæ‚¨æƒ³ä½¿ç”¨ exllama å†…æ ¸ï¼Œæ•´ä¸ªæ¨¡å‹éœ€è¦å…¨éƒ¨éƒ¨ç½²åœ¨ gpus ä¸Šã€‚æ­¤å¤–ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ ç‰ˆæœ¬ > 0.4.2 çš„ Auto-GPTQ å¹¶ä¼ é€’ `device_map` = "cpu" æ¥æ‰§è¡Œ CPU æ¨ç†ã€‚å¯¹äº CPU æ¨ç†ï¼Œæ‚¨å¿…é¡»åœ¨ `GPTQConfig` ä¸­ä¼ é€’ `use_exllama = False`ã€‚

```py
import torch
gptq_config = GPTQConfig(bits=4)
model = AutoModelForCausalLM.from_pretrained("{your_username}/opt-125m-gptq", device_map="auto", quantization_config=gptq_config)
```

éšç€ exllamav2 å†…æ ¸çš„å‘å¸ƒï¼Œä¸ exllama å†…æ ¸ç›¸æ¯”ï¼Œæ‚¨å¯ä»¥è·å¾—æ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚æ‚¨åªéœ€åœ¨ [`GPTQConfig`] ä¸­ä¼ é€’ `exllama_config={"version": 2}`ï¼š

```py
import torch
gptq_config = GPTQConfig(bits=4, exllama_config={"version":2})
model = AutoModelForCausalLM.from_pretrained("{your_username}/opt-125m-gptq", device_map="auto", quantization_config = gptq_config)
```

è¯·æ³¨æ„ï¼Œç›®å‰ä»…æ”¯æŒ 4 ä½æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨ peft å¯¹é‡åŒ–æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå»ºè®®ç¦ç”¨ exllama å†…æ ¸ã€‚ 

æ‚¨å¯ä»¥åœ¨æ­¤æ‰¾åˆ°è¿™äº›å†…æ ¸çš„åŸºå‡†æµ‹è¯• [è¿™é‡Œ](https://github.com/huggingface/optimum/tree/main/tests/benchmark#gptq-benchmark)


#### å¾®è°ƒä¸€ä¸ªé‡åŒ–æ¨¡å‹

åœ¨Hugging Faceç”Ÿæ€ç³»ç»Ÿçš„å®˜æ–¹æ”¯æŒä¸‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨GPTQè¿›è¡Œé‡åŒ–åçš„æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ 
è¯·æŸ¥çœ‹`peft`åº“äº†è§£æ›´å¤šè¯¦æƒ…ã€‚

### ç¤ºä¾‹æ¼”ç¤º

è¯·æŸ¥çœ‹ Google Colab [notebook](https://colab.research.google.com/drive/1_TIrmuKOFhuRRiTWN94ilkUFu6ZX4ceb?usp=sharing)ï¼Œäº†è§£å¦‚ä½•ä½¿ç”¨GPTQé‡åŒ–æ‚¨çš„æ¨¡å‹ä»¥åŠå¦‚ä½•ä½¿ç”¨peftå¾®è°ƒé‡åŒ–æ¨¡å‹ã€‚

### GPTQConfig

[[autodoc]] GPTQConfig


## `bitsandbytes` é›†æˆ

ğŸ¤— Transformers ä¸ `bitsandbytes` ä¸Šæœ€å¸¸ç”¨çš„æ¨¡å—ç´§å¯†é›†æˆã€‚æ‚¨å¯ä»¥ä½¿ç”¨å‡ è¡Œä»£ç ä»¥ 8 ä½ç²¾åº¦åŠ è½½æ‚¨çš„æ¨¡å‹ã€‚
è‡ªbitsandbytesçš„0.37.0ç‰ˆæœ¬å‘å¸ƒä»¥æ¥ï¼Œå¤§å¤šæ•°GPUç¡¬ä»¶éƒ½æ”¯æŒè¿™ä¸€ç‚¹ã€‚

åœ¨[LLM.int8()](https://huggingface.co/papers/2208.07339)è®ºæ–‡ä¸­äº†è§£æ›´å¤šå…³äºé‡åŒ–æ–¹æ³•çš„ä¿¡æ¯ï¼Œæˆ–è€…åœ¨[åšå®¢æ–‡ç« ](https://huggingface.co/blog/hf-bitsandbytes-integration)ä¸­äº†è§£å…³äºåˆä½œçš„æ›´å¤šä¿¡æ¯ã€‚

è‡ªå…¶â€œ0.39.0â€ç‰ˆæœ¬å‘å¸ƒä»¥æ¥ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨FP4æ•°æ®ç±»å‹ï¼Œé€šè¿‡4ä½é‡åŒ–åŠ è½½ä»»ä½•æ”¯æŒâ€œdevice_mapâ€çš„æ¨¡å‹ã€‚

å¦‚æœæ‚¨æƒ³é‡åŒ–è‡ªå·±çš„ pytorch æ¨¡å‹ï¼Œè¯·æŸ¥çœ‹ ğŸ¤— Accelerate çš„[æ–‡æ¡£](https://huggingface.co/docs/accelerate/main/en/usage_guides/quantization)ã€‚

ä»¥ä¸‹æ˜¯æ‚¨å¯ä»¥ä½¿ç”¨â€œbitsandbytesâ€é›†æˆå®Œæˆçš„äº‹æƒ…

### é€šç”¨ç”¨æ³•

åªè¦æ‚¨çš„æ¨¡å‹æ”¯æŒä½¿ç”¨ ğŸ¤— Accelerate è¿›è¡ŒåŠ è½½å¹¶åŒ…å« `torch.nn.Linear` å±‚ï¼Œæ‚¨å¯ä»¥åœ¨è°ƒç”¨ [`~PreTrainedModel.from_pretrained`] æ–¹æ³•æ—¶ä½¿ç”¨ `load_in_8bit` æˆ– `load_in_4bit` å‚æ•°æ¥é‡åŒ–æ¨¡å‹ã€‚è¿™ä¹Ÿåº”è¯¥é€‚ç”¨äºä»»ä½•æ¨¡æ€ã€‚

```python
from transformers import AutoModelForCausalLM

model_8bit = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", load_in_8bit=True)
model_4bit = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", load_in_4bit=True)
```

é»˜è®¤æƒ…å†µä¸‹ï¼Œæ‰€æœ‰å…¶ä»–æ¨¡å—ï¼ˆä¾‹å¦‚ `torch.nn.LayerNorm`ï¼‰å°†è¢«è½¬æ¢ä¸º `torch.float16` ç±»å‹ã€‚ä½†å¦‚æœæ‚¨æƒ³æ›´æ”¹å®ƒä»¬çš„ `dtype`ï¼Œå¯ä»¥é‡è½½ `dtype` å‚æ•°ï¼š

```python
>>> import torch
>>> from transformers import AutoModelForCausalLM

>>> model_8bit = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", load_in_8bit=True, dtype=torch.float32)
>>> model_8bit.model.decoder.layers[-1].final_layer_norm.weight.dtype
torch.float32
```


### FP4 é‡åŒ– 

#### è¦æ±‚

ç¡®ä¿åœ¨è¿è¡Œä»¥ä¸‹ä»£ç æ®µä¹‹å‰å·²å®Œæˆä»¥ä¸‹è¦æ±‚ï¼š

- æœ€æ–°ç‰ˆæœ¬ `bitsandbytes` åº“
`pip install bitsandbytes>=0.39.0`

- å®‰è£…æœ€æ–°ç‰ˆæœ¬ `accelerate`
`pip install --upgrade accelerate`

- å®‰è£…æœ€æ–°ç‰ˆæœ¬ `transformers`
`pip install --upgrade transformers`

#### æç¤ºå’Œæœ€ä½³å®è·µ


- **é«˜çº§ç”¨æ³•ï¼š** è¯·å‚è€ƒ [æ­¤ Google Colab notebook](https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf) ä»¥è·å– 4 ä½é‡åŒ–é«˜çº§ç”¨æ³•å’Œæ‰€æœ‰å¯é€‰é€‰é¡¹ã€‚

- **ä½¿ç”¨ `batch_size=1` å®ç°æ›´å¿«çš„æ¨ç†ï¼š** è‡ª `bitsandbytes` çš„ `0.40.0` ç‰ˆæœ¬ä»¥æ¥ï¼Œè®¾ç½® `batch_size=1`ï¼Œæ‚¨å¯ä»¥ä»å¿«é€Ÿæ¨ç†ä¸­å—ç›Šã€‚è¯·æŸ¥çœ‹ [è¿™äº›å‘å¸ƒè¯´æ˜](https://github.com/TimDettmers/bitsandbytes/releases/tag/0.40.0) ï¼Œå¹¶ç¡®ä¿ä½¿ç”¨å¤§äº `0.40.0` çš„ç‰ˆæœ¬ä»¥ç›´æ¥åˆ©ç”¨æ­¤åŠŸèƒ½ã€‚

- **è®­ç»ƒï¼š** æ ¹æ® [QLoRA è®ºæ–‡](https://huggingface.co/papers/2305.14314)ï¼Œå¯¹äº4ä½åŸºæ¨¡å‹è®­ç»ƒï¼ˆä½¿ç”¨ LoRA é€‚é…å™¨ï¼‰ï¼Œåº”ä½¿ç”¨ `bnb_4bit_quant_type='nf4'`ã€‚

- **æ¨ç†ï¼š** å¯¹äºæ¨ç†ï¼Œ`bnb_4bit_quant_type` å¯¹æ€§èƒ½å½±å“ä¸å¤§ã€‚ä½†æ˜¯ä¸ºäº†ä¸æ¨¡å‹çš„æƒé‡ä¿æŒä¸€è‡´ï¼Œè¯·ç¡®ä¿ä½¿ç”¨ç›¸åŒçš„ `bnb_4bit_compute_dtype` å’Œ `dtype` å‚æ•°ã€‚

#### åŠ è½½ 4 ä½é‡åŒ–çš„å¤§æ¨¡å‹

åœ¨è°ƒç”¨ `.from_pretrained` æ–¹æ³•æ—¶ä½¿ç”¨ `load_in_4bit=True`ï¼Œå¯ä»¥å°†æ‚¨çš„å†…å­˜ä½¿ç”¨é‡å‡å°‘åˆ°å¤§çº¦åŸæ¥çš„ 1/4ã€‚

```python
# pip install transformers accelerate bitsandbytes
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "bigscience/bloom-1b7"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", load_in_4bit=True)
```

<Tip warning={true}>

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¸€æ—¦æ¨¡å‹ä»¥ 4 ä½é‡åŒ–æ–¹å¼åŠ è½½ï¼Œå°±æ— æ³•å°†é‡åŒ–åçš„æƒé‡æ¨é€åˆ° Hub ä¸Šã€‚æ­¤å¤–ï¼Œæ‚¨ä¸èƒ½è®­ç»ƒ 4 ä½é‡åŒ–æƒé‡ï¼Œå› ä¸ºç›®å‰å°šä¸æ”¯æŒæ­¤åŠŸèƒ½ã€‚ä½†æ˜¯ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ 4 ä½é‡åŒ–æ¨¡å‹æ¥è®­ç»ƒé¢å¤–å‚æ•°ï¼Œè¿™å°†åœ¨ä¸‹ä¸€éƒ¨åˆ†ä¸­ä»‹ç»ã€‚

</Tip>

### åŠ è½½ 8 ä½é‡åŒ–çš„å¤§æ¨¡å‹

æ‚¨å¯ä»¥é€šè¿‡åœ¨è°ƒç”¨ `.from_pretrained` æ–¹æ³•æ—¶ä½¿ç”¨ `load_in_8bit=True` å‚æ•°ï¼Œå°†å†…å­˜éœ€æ±‚å¤§è‡´å‡åŠæ¥åŠ è½½æ¨¡å‹


```python
# pip install transformers accelerate bitsandbytes
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_id = "bigscience/bloom-1b7"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=BitsAndBytesConfig(load_in_8bit=True))
```

ç„¶åï¼Œåƒé€šå¸¸ä½¿ç”¨ `PreTrainedModel` ä¸€æ ·ä½¿ç”¨æ‚¨çš„æ¨¡å‹ã€‚

æ‚¨å¯ä»¥ä½¿ç”¨ `get_memory_footprint` æ–¹æ³•æ£€æŸ¥æ¨¡å‹çš„å†…å­˜å ç”¨ã€‚


```python
print(model.get_memory_footprint())
```

é€šè¿‡è¿™ç§é›†æˆï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨è¾ƒå°çš„è®¾å¤‡ä¸ŠåŠ è½½å¤§æ¨¡å‹å¹¶è¿è¡Œå®ƒä»¬è€Œæ²¡æœ‰ä»»ä½•é—®é¢˜ã€‚

<Tip warning={true}>

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¸€æ—¦æ¨¡å‹ä»¥ 8 ä½é‡åŒ–æ–¹å¼åŠ è½½ï¼Œé™¤äº†ä½¿ç”¨æœ€æ–°çš„ `transformers` å’Œ `bitsandbytes` ä¹‹å¤–ï¼Œç›®å‰å°šæ— æ³•å°†é‡åŒ–åçš„æƒé‡æ¨é€åˆ° Hub ä¸Šã€‚æ­¤å¤–ï¼Œæ‚¨ä¸èƒ½è®­ç»ƒ 8 ä½é‡åŒ–æƒé‡ï¼Œå› ä¸ºç›®å‰å°šä¸æ”¯æŒæ­¤åŠŸèƒ½ã€‚ä½†æ˜¯ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ 8 ä½é‡åŒ–æ¨¡å‹æ¥è®­ç»ƒé¢å¤–å‚æ•°ï¼Œè¿™å°†åœ¨ä¸‹ä¸€éƒ¨åˆ†ä¸­ä»‹ç»ã€‚

æ³¨æ„ï¼Œ`device_map` æ˜¯å¯é€‰çš„ï¼Œä½†è®¾ç½® `device_map = 'auto'` æ›´é€‚åˆç”¨äºæ¨ç†ï¼Œå› ä¸ºå®ƒå°†æ›´æœ‰æ•ˆåœ°è°ƒåº¦å¯ç”¨èµ„æºä¸Šçš„æ¨¡å‹ã€‚


</Tip>

#### é«˜çº§ç”¨ä¾‹

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä»‹ç»ä½¿ç”¨ FP4 é‡åŒ–çš„ä¸€äº›é«˜çº§ç”¨ä¾‹ã€‚

##### æ›´æ”¹è®¡ç®—æ•°æ®ç±»å‹

è®¡ç®—æ•°æ®ç±»å‹ç”¨äºæ”¹å˜åœ¨è¿›è¡Œè®¡ç®—æ—¶ä½¿ç”¨çš„æ•°æ®ç±»å‹ã€‚ä¾‹å¦‚ï¼Œhidden stateså¯ä»¥æ˜¯ `float32`ï¼Œä½†ä¸ºäº†åŠ é€Ÿï¼Œè®¡ç®—æ—¶å¯ä»¥è¢«è®¾ç½®ä¸º `bf16`ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè®¡ç®—æ•°æ®ç±»å‹è¢«è®¾ç½®ä¸º `float32`ã€‚


```python
import torch
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)
```

#### ä½¿ç”¨ NF4ï¼ˆæ™®é€šæµ®ç‚¹æ•° 4ï¼‰æ•°æ®ç±»å‹

æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ NF4 æ•°æ®ç±»å‹ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ä½¿ç”¨æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–çš„æƒé‡è€Œé€‚åº”çš„æ–°å‹ 4 ä½æ•°æ®ç±»å‹ã€‚è¦è¿è¡Œï¼š

```python
from transformers import BitsAndBytesConfig

nf4_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
)

model_nf4 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)
```

#### ä½¿ç”¨åµŒå¥—é‡åŒ–è¿›è¡Œæ›´é«˜æ•ˆçš„å†…å­˜æ¨ç†

æˆ‘ä»¬è¿˜å»ºè®®ç”¨æˆ·ä½¿ç”¨åµŒå¥—é‡åŒ–æŠ€æœ¯ã€‚ä»æˆ‘ä»¬çš„ç»éªŒè§‚å¯Ÿæ¥çœ‹ï¼Œè¿™ç§æ–¹æ³•åœ¨ä¸å¢åŠ é¢å¤–æ€§èƒ½çš„æƒ…å†µä¸‹èŠ‚çœæ›´å¤šå†…å­˜ã€‚è¿™ä½¿å¾— llama-13b æ¨¡å‹èƒ½å¤Ÿåœ¨å…·æœ‰ 1024 ä¸ªåºåˆ—é•¿åº¦ã€1 ä¸ªæ‰¹æ¬¡å¤§å°å’Œ 4 ä¸ªæ¢¯åº¦ç´¯ç§¯æ­¥éª¤çš„ NVIDIA-T4 16GB ä¸Šè¿›è¡Œ fine-tuningã€‚

```python
from transformers import BitsAndBytesConfig

double_quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
)

model_double_quant = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=double_quant_config)
```

### å°†é‡åŒ–æ¨¡å‹æ¨é€åˆ°ğŸ¤— Hub

æ‚¨å¯ä»¥ä½¿ç”¨ `push_to_hub` æ–¹æ³•å°†é‡åŒ–æ¨¡å‹æ¨é€åˆ° Hub ä¸Šã€‚è¿™å°†é¦–å…ˆæ¨é€é‡åŒ–é…ç½®æ–‡ä»¶ï¼Œç„¶åæ¨é€é‡åŒ–æ¨¡å‹æƒé‡ã€‚
è¯·ç¡®ä¿ä½¿ç”¨ `bitsandbytes>0.37.2`ï¼ˆåœ¨æ’°å†™æœ¬æ–‡æ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ `bitsandbytes==0.38.0.post1`ï¼‰æ‰èƒ½ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚


```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model = AutoModelForCausalLM.from_pretrained("bigscience/bloom-560m", quantization_config=BitsAndBytesConfig(load_in_8bit=True))
tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom-560m")

model.push_to_hub("bloom-560m-8bit")
```

<Tip warning={true}>

å¯¹å¤§æ¨¡å‹ï¼Œå¼ºçƒˆé¼“åŠ±å°† 8 ä½é‡åŒ–æ¨¡å‹æ¨é€åˆ° Hub ä¸Šï¼Œä»¥ä¾¿è®©ç¤¾åŒºèƒ½å¤Ÿä»å†…å­˜å ç”¨å‡å°‘å’ŒåŠ è½½ä¸­å—ç›Šï¼Œä¾‹å¦‚åœ¨ Google Colab ä¸ŠåŠ è½½å¤§æ¨¡å‹ã€‚

</Tip>

### ä»ğŸ¤— HubåŠ è½½é‡åŒ–æ¨¡å‹

æ‚¨å¯ä»¥ä½¿ç”¨ `from_pretrained` æ–¹æ³•ä» Hub åŠ è½½é‡åŒ–æ¨¡å‹ã€‚è¯·ç¡®ä¿æ¨é€çš„æƒé‡æ˜¯é‡åŒ–çš„ï¼Œæ£€æŸ¥æ¨¡å‹é…ç½®å¯¹è±¡ä¸­æ˜¯å¦å­˜åœ¨ `quantization_config` å±æ€§ã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("{your_username}/bloom-560m-8bit", device_map="auto")
```

è¯·æ³¨æ„ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨ä¸éœ€è¦æŒ‡å®š `load_in_8bit=True` å‚æ•°ï¼Œä½†éœ€è¦ç¡®ä¿ `bitsandbytes` å’Œ `accelerate` å·²å®‰è£…ã€‚
æƒ…æ³¨æ„ï¼Œ`device_map` æ˜¯å¯é€‰çš„ï¼Œä½†è®¾ç½® `device_map = 'auto'` æ›´é€‚åˆç”¨äºæ¨ç†ï¼Œå› ä¸ºå®ƒå°†æ›´æœ‰æ•ˆåœ°è°ƒåº¦å¯ç”¨èµ„æºä¸Šçš„æ¨¡å‹ã€‚

### é«˜çº§ç”¨ä¾‹

æœ¬èŠ‚é¢å‘å¸Œæœ›æ¢ç´¢é™¤äº†åŠ è½½å’Œè¿è¡Œ 8 ä½æ¨¡å‹ä¹‹å¤–è¿˜èƒ½åšä»€ä¹ˆçš„è¿›é˜¶ç”¨æˆ·ã€‚

#### åœ¨ `cpu` å’Œ `gpu` ä¹‹é—´å¸è½½

æ­¤é«˜çº§ç”¨ä¾‹ä¹‹ä¸€æ˜¯èƒ½å¤ŸåŠ è½½æ¨¡å‹å¹¶å°†æƒé‡åˆ†æ´¾åˆ° `CPU` å’Œ `GPU` ä¹‹é—´ã€‚è¯·æ³¨æ„ï¼Œå°†åœ¨ CPU ä¸Šåˆ†æ´¾çš„æƒé‡ **ä¸ä¼š** è½¬æ¢ä¸º 8 ä½ï¼Œå› æ­¤ä¼šä¿ç•™ä¸º `float32`ã€‚æ­¤åŠŸèƒ½é€‚ç”¨äºæƒ³è¦é€‚åº”éå¸¸å¤§çš„æ¨¡å‹å¹¶å°†æ¨¡å‹åˆ†æ´¾åˆ° GPU å’Œ CPU ä¹‹é—´çš„ç”¨æˆ·ã€‚

é¦–å…ˆï¼Œä» `transformers` ä¸­åŠ è½½ä¸€ä¸ª [`BitsAndBytesConfig`]ï¼Œå¹¶å°†å±æ€§ `llm_int8_enable_fp32_cpu_offload` è®¾ç½®ä¸º `True`ï¼š


```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)
```

å‡è®¾æ‚¨æƒ³åŠ è½½ `bigscience/bloom-1b7` æ¨¡å‹ï¼Œæ‚¨çš„ GPUæ˜¾å­˜ä»…è¶³å¤Ÿå®¹çº³é™¤äº†`lm_head`å¤–çš„æ•´ä¸ªæ¨¡å‹ã€‚å› æ­¤ï¼Œæ‚¨å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ–¹å¼ç¼–å†™è‡ªå®šä¹‰çš„ device_mapï¼š

```python
device_map = {
    "transformer.word_embeddings": 0,
    "transformer.word_embeddings_layernorm": 0,
    "lm_head": "cpu",
    "transformer.h": 0,
    "transformer.ln_f": 0,
}
```

ç„¶åå¦‚ä¸‹åŠ è½½æ¨¡å‹ï¼š

```python
model_8bit = AutoModelForCausalLM.from_pretrained(
    "bigscience/bloom-1b7",
    device_map=device_map,
    quantization_config=quantization_config,
)
```

è¿™å°±æ˜¯å…¨éƒ¨å†…å®¹ï¼äº«å—æ‚¨çš„æ¨¡å‹å§ï¼

#### ä½¿ç”¨`llm_int8_threshold`

æ‚¨å¯ä»¥ä½¿ç”¨ `llm_int8_threshold` å‚æ•°æ¥æ›´æ”¹å¼‚å¸¸å€¼çš„é˜ˆå€¼ã€‚â€œå¼‚å¸¸å€¼â€æ˜¯ä¸€ä¸ªå¤§äºç‰¹å®šé˜ˆå€¼çš„`hidden state`å€¼ã€‚
è¿™å¯¹åº”äº`LLM.int8()`è®ºæ–‡ä¸­æè¿°çš„å¼‚å¸¸æ£€æµ‹çš„å¼‚å¸¸é˜ˆå€¼ã€‚ä»»ä½•é«˜äºæ­¤é˜ˆå€¼çš„`hidden state`å€¼éƒ½å°†è¢«è§†ä¸ºå¼‚å¸¸å€¼ï¼Œå¯¹è¿™äº›å€¼çš„æ“ä½œå°†åœ¨ fp16 ä¸­å®Œæˆã€‚å€¼é€šå¸¸æ˜¯æ­£æ€åˆ†å¸ƒçš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå¤§å¤šæ•°å€¼åœ¨ [-3.5, 3.5] èŒƒå›´å†…ï¼Œä½†æœ‰ä¸€äº›é¢å¤–çš„ç³»ç»Ÿå¼‚å¸¸å€¼ï¼Œå¯¹äºå¤§æ¨¡å‹æ¥è¯´ï¼Œå®ƒä»¬çš„åˆ†å¸ƒéå¸¸ä¸åŒã€‚è¿™äº›å¼‚å¸¸å€¼é€šå¸¸åœ¨åŒºé—´ [-60, -6] æˆ– [6, 60] å†…ã€‚Int8 é‡åŒ–å¯¹äºå¹…åº¦ä¸º ~5 çš„å€¼æ•ˆæœå¾ˆå¥½ï¼Œä½†è¶…å‡ºè¿™ä¸ªèŒƒå›´ï¼Œæ€§èƒ½å°±ä¼šæ˜æ˜¾ä¸‹é™ã€‚ä¸€ä¸ªå¥½çš„é»˜è®¤é˜ˆå€¼æ˜¯ 6ï¼Œä½†å¯¹äºæ›´ä¸ç¨³å®šçš„æ¨¡å‹ï¼ˆå°æ¨¡å‹ã€å¾®è°ƒï¼‰å¯èƒ½éœ€è¦æ›´ä½çš„é˜ˆå€¼ã€‚
è¿™ä¸ªå‚æ•°ä¼šå½±å“æ¨¡å‹çš„æ¨ç†é€Ÿåº¦ã€‚æˆ‘ä»¬å»ºè®®å°è¯•è¿™ä¸ªå‚æ•°ï¼Œä»¥æ‰¾åˆ°æœ€é€‚åˆæ‚¨çš„ç”¨ä¾‹çš„å‚æ•°ã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_id = "bigscience/bloom-1b7"

quantization_config = BitsAndBytesConfig(
    llm_int8_threshold=10,
)

model_8bit = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map=device_map,
    quantization_config=quantization_config,
)
tokenizer = AutoTokenizer.from_pretrained(model_id)
```

#### è·³è¿‡æŸäº›æ¨¡å—çš„è½¬æ¢

ä¸€äº›æ¨¡å‹æœ‰å‡ ä¸ªéœ€è¦ä¿æŒæœªè½¬æ¢çŠ¶æ€ä»¥ç¡®ä¿ç¨³å®šæ€§çš„æ¨¡å—ã€‚ä¾‹å¦‚ï¼ŒJukebox æ¨¡å‹æœ‰å‡ ä¸ª `lm_head` æ¨¡å—éœ€è¦è·³è¿‡ã€‚ä½¿ç”¨ `llm_int8_skip_modules` å‚æ•°è¿›è¡Œç›¸åº”æ“ä½œã€‚


```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_id = "bigscience/bloom-1b7"

quantization_config = BitsAndBytesConfig(
    llm_int8_skip_modules=["lm_head"],
)

model_8bit = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map=device_map,
    quantization_config=quantization_config,
)
tokenizer = AutoTokenizer.from_pretrained(model_id)
```

#### å¾®è°ƒå·²åŠ è½½ä¸º8ä½ç²¾åº¦çš„æ¨¡å‹

å€ŸåŠ©Hugging Faceç”Ÿæ€ç³»ç»Ÿä¸­é€‚é…å™¨ï¼ˆadaptersï¼‰çš„å®˜æ–¹æ”¯æŒï¼Œæ‚¨å¯ä»¥åœ¨8ä½ç²¾åº¦ä¸‹å¾®è°ƒæ¨¡å‹ã€‚è¿™ä½¿å¾—å¯ä»¥åœ¨å•ä¸ªGoogle Colabä¸­å¾®è°ƒå¤§æ¨¡å‹ï¼Œä¾‹å¦‚`flan-t5-large`æˆ–`facebook/opt-6.7b`ã€‚è¯·æŸ¥çœ‹[`peft`](https://github.com/huggingface/peft)åº“äº†è§£æ›´å¤šè¯¦æƒ…ã€‚

æ³¨æ„ï¼ŒåŠ è½½æ¨¡å‹è¿›è¡Œè®­ç»ƒæ—¶æ— éœ€ä¼ é€’`device_map`ã€‚å®ƒå°†è‡ªåŠ¨å°†æ‚¨çš„æ¨¡å‹åŠ è½½åˆ°GPUä¸Šã€‚å¦‚æœéœ€è¦ï¼Œæ‚¨å¯ä»¥å°†è®¾å¤‡æ˜ å°„ä¸ºç‰¹å®šè®¾å¤‡ï¼ˆä¾‹å¦‚`cuda:0`ã€`0`ã€`torch.device('cuda:0')`ï¼‰ã€‚è¯·æ³¨æ„ï¼Œ`device_map=auto`ä»…åº”ç”¨äºæ¨ç†ã€‚


### BitsAndBytesConfig

[[autodoc]] BitsAndBytesConfig


## ä½¿ç”¨ ğŸ¤— `optimum` è¿›è¡Œé‡åŒ–

è¯·æŸ¥çœ‹[Optimum æ–‡æ¡£](https://huggingface.co/docs/optimum/index)ä»¥äº†è§£æ›´å¤šå…³äº`optimum`æ”¯æŒçš„é‡åŒ–æ–¹æ³•ï¼Œå¹¶æŸ¥çœ‹è¿™äº›æ–¹æ³•æ˜¯å¦é€‚ç”¨äºæ‚¨çš„ç”¨ä¾‹ã€‚

