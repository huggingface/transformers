<!--ç‰ˆæƒæ‰€æœ‰ 2023 å¹´ HuggingFace å›¢é˜Ÿã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚
æ ¹æ® Apache è®¸å¯è¯ç¬¬ 2.0 ç‰ˆï¼ˆâ€œè®¸å¯è¯â€ï¼‰è®¸å¯ï¼›é™¤éç¬¦åˆè®¸å¯è¯çš„è§„å®šï¼Œå¦åˆ™æ‚¨ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ä½ç½®è·å¾—è®¸å¯è¯çš„å‰¯æœ¬
http://www.apache.org/licenses/LICENSE-2.0
é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œæ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶ä»¥â€œæŒ‰åŸæ ·â€åŸºç¡€åˆ†å‘ï¼Œä¸é™„å¸¦ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„æ‹…ä¿æˆ–æ¡ä»¶ã€‚è¯·å‚é˜…è®¸å¯è¯ä»¥è·å¾—ç‰¹å®šè¯­è¨€ä¸‹çš„æƒé™å’Œé™åˆ¶ã€‚
âš ï¸è¯·æ³¨æ„ï¼Œæ­¤æ–‡ä»¶æ˜¯ Markdown æ ¼å¼çš„ï¼Œä½†åŒ…å«äº†æˆ‘ä»¬çš„æ–‡æ¡£æ„å»ºå™¨ï¼ˆç±»ä¼¼äº MDXï¼‰çš„ç‰¹å®šè¯­æ³•ï¼Œå¯èƒ½æ— æ³•åœ¨æ‚¨çš„ Markdown æŸ¥çœ‹å™¨ä¸­æ­£ç¡®å‘ˆç°ã€‚
-->

# é‡åŒ–ğŸ¤— Transformers æ¨¡å‹

## `bitsandbytes` é›†æˆ

ğŸ¤— Transformers ä¸ `bitsandbytes` ä¸Šä½¿ç”¨æœ€é¢‘ç¹çš„æ¨¡å—ç´§å¯†é›†æˆã€‚æ‚¨åªéœ€ä½¿ç”¨å‡ è¡Œä»£ç å³å¯å°†æ¨¡å‹åŠ è½½åˆ° 8 ä½ç²¾åº¦ä¸­ã€‚è‡ª `bitsandbytes` çš„ `0.37.0` ç‰ˆæœ¬å‘å¸ƒä»¥æ¥ï¼Œå¤§å¤šæ•° GPU ç¡¬ä»¶éƒ½æ”¯æŒæ­¤åŠŸèƒ½ã€‚

äº†è§£æœ‰å…³ [LLM.int8()](https://arxiv.org/abs/2208.07339) è®ºæ–‡ä¸­çš„é‡åŒ–æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ï¼Œæˆ–äº†è§£æœ‰å…³åˆä½œçš„ [åšå®¢æ–‡ç« ](https://huggingface.co/blog/hf-bitsandbytes-integration) çš„æ›´å¤šä¿¡æ¯ã€‚

è‡ª `0.39.0` ç‰ˆæœ¬å‘å¸ƒä»¥æ¥ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ 4 ä½é‡åŒ–åŠ è½½ä»»ä½•æ”¯æŒ `device_map` çš„æ¨¡å‹ï¼Œåˆ©ç”¨ FP4 æ•°æ®ç±»å‹ã€‚

ä»¥ä¸‹æ˜¯ä½¿ç”¨ `bitsandbytes` é›†æˆå¯ä»¥å®Œæˆçš„æ“ä½œ

### FP4 é‡åŒ–

#### è¦æ±‚

åœ¨è¿è¡Œä»¥ä¸‹ä»»ä½•ä»£ç ç‰‡æ®µä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…ä»¥ä¸‹è¦æ±‚ã€‚

- æœ€æ–°çš„ `bitsandbytes` åº“ `pip install bitsandbytes>=0.39.0`
- ä»æºä»£ç å®‰è£…æœ€æ–°çš„ `accelerate` åº“ `pip install git+https://github.com/huggingface/accelerate.git`
- ä»æºä»£ç å®‰è£…æœ€æ–°çš„ `transformers` åº“ `pip install git+https://github.com/huggingface/transformers.git`

#### ä½¿ç”¨ 4 ä½åŠ è½½å¤§å‹æ¨¡å‹

åœ¨è°ƒç”¨ `.from_pretrained` æ–¹æ³•æ—¶ä½¿ç”¨ `load_in_4bit=True`ï¼Œå¯ä»¥å°†å†…å­˜ä½¿ç”¨å‡å°‘ 4 å€ï¼ˆå¤§çº¦ï¼‰ã€‚
```python
# pip install transformers accelerate bitsandbytes
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "bigscience/bloom-1b7"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", load_in_4bit=True)
```

<Tip warning={true}>

è¯·æ³¨æ„ï¼Œä¸€æ—¦ä¸€ä¸ªæ¨¡å‹ä»¥ 4 ä½åŠ è½½ï¼Œç›®å‰æ— æ³•å°†é‡åŒ–çš„æƒé‡æ¨é€åˆ° Hubã€‚è¿˜è¯·æ³¨æ„ï¼Œç›®å‰ä¸æ”¯æŒè®­ç»ƒ 4 ä½æƒé‡ã€‚ä½†æ˜¯ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ 4 ä½æ¨¡å‹æ¥è®­ç»ƒé¢å¤–çš„å‚æ•°ï¼Œè¿™å°†åœ¨ä¸‹ä¸€èŠ‚ä¸­ä»‹ç»ã€‚
</Tip>

### ä½¿ç”¨ 8 ä½åŠ è½½å¤§å‹æ¨¡å‹

æ‚¨å¯ä»¥é€šè¿‡åœ¨è°ƒç”¨ `.from_pretrained` æ–¹æ³•æ—¶ä½¿ç”¨ `load_in_8bit=True` å‚æ•°ï¼Œå°†æ¨¡å‹çš„å†…å­˜éœ€æ±‚å¤§è‡´å‡åŠã€‚

```python
# pip install transformers accelerate bitsandbytes
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "bigscience/bloom-1b7"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", load_in_8bit=True)
```

ç„¶åï¼Œåƒé€šå¸¸ä½¿ç”¨ [`PreTrainedModel`] ä¸€æ ·ä½¿ç”¨æ‚¨çš„æ¨¡å‹ã€‚

æ‚¨å¯ä»¥ä½¿ç”¨ `get_memory_footprint` æ–¹æ³•æ£€æŸ¥æ¨¡å‹çš„å†…å­˜å ç”¨ã€‚
```python
print(model.get_memory_footprint())
```

é€šè¿‡æ­¤é›†æˆï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨è¾ƒå°çš„è®¾å¤‡ä¸ŠåŠ è½½å¤§å‹æ¨¡å‹å¹¶ä¸”è¿è¡Œæ­£å¸¸ã€‚
<Tip warning={true}>

è¯·æ³¨æ„ï¼Œä¸€æ—¦ä¸€ä¸ªæ¨¡å‹ä»¥ 8 ä½åŠ è½½ï¼Œç›®å‰æ— æ³•å°†é‡åŒ–çš„æƒé‡æ¨é€åˆ° Hubï¼Œé™¤éæ‚¨ä½¿ç”¨æœ€æ–°çš„ `transformers` å’Œ `bitsandbytes`ã€‚è¿˜è¯·æ³¨æ„ï¼Œç›®å‰ä¸æ”¯æŒè®­ç»ƒ 8 ä½æƒé‡ã€‚ä½†æ˜¯ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ 8 ä½æ¨¡å‹æ¥è®­ç»ƒé¢å¤–çš„å‚æ•°ï¼Œè¿™å°†åœ¨ä¸‹ä¸€èŠ‚ä¸­ä»‹ç»ã€‚è¿˜è¯·æ³¨æ„ï¼Œ`device_map` æ˜¯å¯é€‰çš„ï¼Œä½†å¯¹äºæ¨ç†ï¼Œå»ºè®®å°† `device_map = 'auto'` è®¾ç½®ä¸ºè‡ªåŠ¨å°†æ¨¡å‹é«˜æ•ˆåœ°åˆ†æ´¾åˆ°å¯ç”¨èµ„æºä¸Šã€‚
</Tip>

#### é«˜çº§ç”¨ä¾‹

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä»‹ç»ä¸€äº›ä½¿ç”¨ FP4 é‡åŒ–å¯ä»¥æ‰§è¡Œçš„é«˜çº§ç”¨ä¾‹

##### æ›´æ”¹è®¡ç®—æ•°æ®ç±»å‹

è®¡ç®—æ•°æ®ç±»å‹ç”¨äºæ›´æ”¹è®¡ç®—è¿‡ç¨‹ä¸­ä½¿ç”¨çš„æ•°æ®ç±»å‹ã€‚ä¾‹å¦‚ï¼Œéšè—çŠ¶æ€å¯ä»¥æ˜¯ `float32`ï¼Œä½†è®¡ç®—å¯ä»¥è®¾ç½®ä¸º bf16 ä»¥æé«˜é€Ÿåº¦ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè®¡ç®—æ•°æ®ç±»å‹è®¾ç½®ä¸º `float32`ã€‚
```python
import torch
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)
```

##### ä½¿ç”¨ NF4ï¼ˆæ­£å¸¸æµ®ç‚¹ 4 ä½ï¼‰æ•°æ®ç±»å‹

æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ NF4 æ•°æ®ç±»å‹ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ä½¿ç”¨æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–çš„æƒé‡çš„æ–°çš„ 4 ä½æ•°æ®ç±»å‹ã€‚è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
```python
from transformers import BitsAndBytesConfig

nf4_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
)

model_nf4 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)
```

##### ä½¿ç”¨åµŒå¥—é‡åŒ–è¿›è¡Œæ›´é«˜æ•ˆçš„å†…å­˜æ¨ç†

æˆ‘ä»¬è¿˜å»ºè®®ç”¨æˆ·ä½¿ç”¨åµŒå¥—é‡åŒ–æŠ€æœ¯ã€‚è¿™å¯ä»¥åœ¨ä¸å½±å“æ€§èƒ½çš„æƒ…å†µä¸‹èŠ‚çœæ›´å¤šçš„å†…å­˜-æ ¹æ®æˆ‘ä»¬çš„ç»éªŒè§‚å¯Ÿï¼Œåœ¨ NVIDIA-T4 16GB ä¸Šï¼Œè¿™å¯ä»¥ä½¿ llama-13b æ¨¡å‹åœ¨åºåˆ—é•¿åº¦ä¸º 1024ã€æ‰¹æ¬¡å¤§å°ä¸º 1 å’Œæ¢¯åº¦ç´¯ç§¯æ­¥éª¤ä¸º 4 çš„æƒ…å†µä¸‹è¿›è¡Œå¾®è°ƒã€‚

```python
from transformers import BitsAndBytesConfig

double_quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
)

model_double_quant = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=double_quant_config)
```


### å°†é‡åŒ–æ¨¡å‹æ¨é€åˆ°ğŸ¤— Hub

æ‚¨å¯ä»¥é€šè¿‡ç®€å•åœ°ä½¿ç”¨ `push_to_hub` æ–¹æ³•å°†é‡åŒ–æ¨¡å‹æ¨é€åˆ° Hubã€‚è¿™å°†é¦–å…ˆæ¨é€é‡åŒ–é…ç½®æ–‡ä»¶ï¼Œç„¶åæ¨é€é‡åŒ–æ¨¡å‹æƒé‡ã€‚è¯·ç¡®ä¿ä½¿ç”¨çš„æ˜¯ `bitsandbytes>0.37.2`ï¼ˆåœ¨æ’°å†™æœ¬æ–‡æ—¶ï¼Œæˆ‘ä»¬åœ¨ `bitsandbytes==0.38.0.post1` ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼‰ï¼Œä»¥ä¾¿ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("bigscience/bloom-560m", device_map="auto", load_in_8bit=True)
tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom-560m")

model.push_to_hub("bloom-560m-8bit")
```

<Tip warning={true}>

å¼ºçƒˆå»ºè®®å°† 8 ä½æ¨¡å‹æ¨é€åˆ° Hub ä»¥ä¾¿äºå¤§å‹æ¨¡å‹ã€‚è¿™å°†ä½¿ç¤¾åŒºå—ç›Šäºå†…å­˜å ç”¨çš„å‡å°‘ï¼Œä¾‹å¦‚åœ¨ Google Colab ä¸ŠåŠ è½½å¤§å‹æ¨¡å‹ã€‚
</Tip>

### ä»ğŸ¤— Hub åŠ è½½é‡åŒ–æ¨¡å‹

æ‚¨å¯ä»¥ä½¿ç”¨ `from_pretrained` æ–¹æ³•ä» Hub åŠ è½½é‡åŒ–æ¨¡å‹ã€‚é€šè¿‡æ£€æŸ¥æ¨¡å‹é…ç½®å¯¹è±¡ä¸­æ˜¯å¦å­˜åœ¨ `quantization_config` å±æ€§ï¼Œç¡®ä¿æ¨é€çš„æƒé‡å·²ç»é‡åŒ–ã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("{your_username}/bloom-560m-8bit", device_map="auto")
```

è¯·æ³¨æ„ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨æ— éœ€æŒ‡å®š `load_in_8bit=True` å‚æ•°ï¼Œä½†æ‚¨éœ€è¦ç¡®ä¿å·²å®‰è£… `bitsandbytes` å’Œ `accelerate`ã€‚è¿˜è¯·æ³¨æ„ï¼Œ`device_map` æ˜¯å¯é€‰çš„ï¼Œä½†å¯¹äºæ¨ç†ï¼Œå»ºè®®å°† `device_map = 'auto'` è®¾ç½®ä¸ºè‡ªåŠ¨å°†æ¨¡å‹é«˜æ•ˆåœ°åˆ†æ´¾åˆ°å¯ç”¨èµ„æºä¸Šã€‚

### é«˜çº§ç”¨ä¾‹

æœ¬èŠ‚é¢å‘é«˜çº§ç”¨æˆ·ï¼Œä»–ä»¬å¸Œæœ›æ¢ç´¢é™¤åŠ è½½å’Œè¿è¡Œ 8 ä½æ¨¡å‹ä¹‹å¤–çš„å…¶ä»–å¯èƒ½æ€§ã€‚

#### åœ¨ `cpu` å’Œ `gpu` ä¹‹é—´è¿›è¡Œå¸è½½

å…¶ä¸­ä¸€ä¸ªé«˜çº§ç”¨ä¾‹æ˜¯èƒ½å¤Ÿåœ¨ `CPU` å’Œ `GPU` ä¹‹é—´åŠ è½½æ¨¡å‹å¹¶åˆ†æ´¾æƒé‡ã€‚è¯·æ³¨æ„ï¼Œå°†åœ¨ CPU ä¸Šåˆ†æ´¾çš„æƒé‡ **ä¸ä¼š** è½¬æ¢ä¸º 8 ä½ï¼Œè€Œæ˜¯ä¿æŒä¸º `float32`ã€‚æ­¤åŠŸèƒ½é€‚ç”¨äºæƒ³è¦é€‚åº”éå¸¸å¤§çš„æ¨¡å‹å¹¶åœ¨ GPU å’Œ CPU ä¹‹é—´åˆ†æ´¾æ¨¡å‹çš„ç”¨æˆ·ã€‚

é¦–å…ˆï¼Œä» `transformers` ä¸­åŠ è½½ `BitsAndBytesConfig` å¹¶å°†å±æ€§ `llm_int8_enable_fp32_cpu_offload` è®¾ç½®ä¸º `True`ï¼š
```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)
```

å‡è®¾æ‚¨è¦åŠ è½½ `bigscience/bloom-1b7` æ¨¡å‹ï¼Œå¹¶ä¸”æ‚¨çš„ GPU RAM åˆšå¥½è¶³å¤Ÿå®¹çº³æ•´ä¸ªæ¨¡å‹ï¼Œä½†ä¸åŒ…æ‹¬ `lm_head`ã€‚å› æ­¤ï¼Œç¼–å†™è‡ªå®šä¹‰çš„ `device_map`ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
```python
device_map = {
    "transformer.word_embeddings": 0,
    "transformer.word_embeddings_layernorm": 0,
    "lm_head": "cpu",
    "transformer.h": 0,
    "transformer.ln_f": 0,
}
```

ç„¶åï¼ŒæŒ‰å¦‚ä¸‹æ–¹å¼åŠ è½½æ‚¨çš„æ¨¡å‹ï¼š

```python
model_8bit = AutoModelForCausalLM.from_pretrained(
    "bigscience/bloom-1b7",
    device_map=device_map,
    quantization_config=quantization_config,
)
```

å°±æ˜¯è¿™æ ·ï¼äº«å—æ‚¨çš„æ¨¡å‹å§ï¼

#### è°ƒæ•´ `llm_int8_threshold`

æ‚¨å¯ä»¥è°ƒæ•´ `llm_int8_threshold` å‚æ•°ä»¥æ›´æ”¹ç¦»ç¾¤å€¼çš„é˜ˆå€¼ã€‚"ç¦»ç¾¤å€¼" æ˜¯å¤§äºæŸä¸ªç‰¹å®šé˜ˆå€¼çš„éšè—çŠ¶æ€å€¼ã€‚

è¿™å¯¹åº”äºåœ¨ `LLM.int8()` è®ºæ–‡ä¸­æè¿°çš„å¼‚å¸¸å€¼æ£€æµ‹çš„å¼‚å¸¸å€¼é˜ˆå€¼ã€‚ä»»ä½•è¶…è¿‡æ­¤é˜ˆå€¼çš„éšè—çŠ¶æ€å€¼éƒ½å°†è¢«è§†ä¸ºå¼‚å¸¸å€¼ï¼Œå¹¶ä¸”å¯¹è¿™äº›å€¼çš„æ“ä½œå°†ä½¿ç”¨ fp16 è¿›è¡Œã€‚è¿™äº›å€¼é€šå¸¸æœä»æ­£æ€åˆ†å¸ƒï¼Œå³å¤§å¤šæ•°å€¼åœ¨ [-3.5, 3.5] èŒƒå›´å†…ï¼Œä½†å¯¹äºå¤§å‹æ¨¡å‹ï¼Œæœ‰ä¸€äº›å¼‚å¸¸ç³»ç»Ÿæ€§å¼‚å¸¸å€¼çš„åˆ†å¸ƒæ–¹å¼éå¸¸ä¸åŒã€‚è¿™äº›å¼‚å¸¸å€¼é€šå¸¸åœ¨åŒºé—´ [-60, -6] æˆ– [6, 60] å†…ã€‚å¯¹äºå¤§å°çº¦ä¸º 5 çš„å€¼ï¼ŒInt8 é‡åŒ–æ•ˆæœå¾ˆå¥½ï¼Œä½†è¶…è¿‡è¯¥èŒƒå›´ä¼šå¯¼è‡´æ˜¾è‘—çš„æ€§èƒ½æŸå¤±ã€‚ä¸€ä¸ªå¾ˆå¥½çš„é»˜è®¤é˜ˆå€¼æ˜¯ 6ï¼Œä½†å¯¹äºä¸ç¨³å®šçš„æ¨¡å‹ï¼ˆå°æ¨¡å‹ã€å¾®è°ƒï¼‰ï¼Œå¯èƒ½éœ€è¦æ›´ä½çš„é˜ˆå€¼ã€‚

æ­¤å‚æ•°å¯ä»¥å½±å“æ¨¡å‹çš„æ¨æ–­é€Ÿåº¦ã€‚å»ºè®®è°ƒæ•´æ­¤å‚æ•°ä»¥æ‰¾åˆ°æœ€é€‚åˆæ‚¨çš„ç”¨ä¾‹çš„å€¼ã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_id = "bigscience/bloom-1b7"

quantization_config = BitsAndBytesConfig(
    llm_int8_threshold=10,
)

model_8bit = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map=device_map,
    quantization_config=quantization_config,
)
tokenizer = AutoTokenizer.from_pretrained(model_id)
```

#### è·³è¿‡æŸäº›æ¨¡å—çš„è½¬æ¢

æŸäº›æ¨¡å‹å…·æœ‰å¤šä¸ªæ¨¡å—ï¼Œéœ€è¦åœ¨ 8 ä½ä¸­ä¸è¿›è¡Œè½¬æ¢ä»¥ç¡®ä¿ç¨³å®šæ€§ã€‚ä¾‹å¦‚ï¼ŒJukebox æ¨¡å‹å…·æœ‰å¤šä¸ªåº”è·³è¿‡çš„ `lm_head` æ¨¡å—ã€‚è¯·ä½¿ç”¨ `llm_int8_skip_modules` è¿›è¡Œè°ƒæ•´ã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_id = "bigscience/bloom-1b7"

quantization_config = BitsAndBytesConfig(
    llm_int8_skip_modules=["lm_head"],
)

model_8bit = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map=device_map,
    quantization_config=quantization_config,
)
tokenizer = AutoTokenizer.from_pretrained(model_id)
```

#### å¯¹å·²åŠ è½½çš„ 8 ä½æ¨¡å‹è¿›è¡Œå¾®è°ƒ

åœ¨ Hugging Face ç”Ÿæ€ç³»ç»Ÿä¸­æ­£å¼æ”¯æŒé€‚é…å™¨åï¼Œæ‚¨å¯ä»¥å¯¹å·²åŠ è½½çš„ 8 ä½æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚è¿™ä½¿å¾—å¯ä»¥åœ¨å•ä¸ª Google Colab ä¸­å¾®è°ƒå¤§å‹æ¨¡å‹ï¼Œå¦‚ `flan-t5-large` æˆ– `facebook/opt-6.7b`ã€‚è¯·å‚é˜… [`peft`](https://github.com/huggingface/peft) åº“äº†è§£æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚
è¯·æ³¨æ„ï¼Œåœ¨åŠ è½½æ¨¡å‹è¿›è¡Œè®­ç»ƒæ—¶ï¼Œæ— éœ€ä¼ é€’ `device_map`ã€‚å®ƒå°†è‡ªåŠ¨å°†æ‚¨çš„æ¨¡å‹åŠ è½½åˆ° GPU ä¸Šã€‚å¦‚æœéœ€è¦ï¼Œæ‚¨è¿˜å¯ä»¥å°†è®¾å¤‡æ˜ å°„è®¾ç½®ä¸ºç‰¹å®šè®¾å¤‡ï¼ˆä¾‹å¦‚ `cuda:0`ï¼Œ`0`ï¼Œ`torch.device('cuda:0')`ï¼‰ã€‚

è¯·æ³¨æ„ï¼Œ`device_map=auto` ä»…é€‚ç”¨äºæ¨æ–­ã€‚

### BitsAndBytesConfig
[[autodoc]] BitsAndBytesConfig

## ä½¿ç”¨ğŸ¤— `optimum` è¿›è¡Œé‡åŒ–

è¯·æŸ¥çœ‹ [Optimum æ–‡æ¡£](https://huggingface.co/docs/optimum/index) ä»¥äº†è§£æ›´å¤šå…³äº `optimum` æ”¯æŒçš„é‡åŒ–æ–¹æ³•ï¼Œå¹¶æŸ¥çœ‹è¿™äº›æ–¹æ³•æ˜¯å¦é€‚ç”¨äºæ‚¨çš„ç”¨ä¾‹ã€‚
