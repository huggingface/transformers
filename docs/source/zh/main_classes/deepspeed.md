<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# DeepSpeedé›†æˆ

[DeepSpeed](https://github.com/deepspeedai/DeepSpeed)å®ç°äº†[ZeROè®ºæ–‡](https://huggingface.co/papers/1910.02054)ä¸­æè¿°çš„æ‰€æœ‰å†…å®¹ã€‚ç›®å‰ï¼Œå®ƒæä¾›å¯¹ä»¥ä¸‹åŠŸèƒ½çš„å…¨é¢æ”¯æŒï¼š

1. ä¼˜åŒ–å™¨çŠ¶æ€åˆ†åŒºï¼ˆZeRO stage 1ï¼‰
2. æ¢¯åº¦åˆ†åŒºï¼ˆZeRO stage 2ï¼‰
3. å‚æ•°åˆ†åŒºï¼ˆZeRO stage 3ï¼‰
4. è‡ªå®šä¹‰æ··åˆç²¾åº¦è®­ç»ƒå¤„ç†
5. ä¸€ç³»åˆ—åŸºäºCUDAæ‰©å±•çš„å¿«é€Ÿä¼˜åŒ–å™¨
6. ZeRO-Offload åˆ° CPU å’Œ NVMe

ZeRO-Offloadæœ‰å…¶è‡ªå·±çš„ä¸“é—¨è®ºæ–‡ï¼š[ZeRO-Offload: Democratizing Billion-Scale Model Training](https://huggingface.co/papers/2101.06840)ã€‚è€ŒNVMeæ”¯æŒåœ¨è®ºæ–‡[ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning](https://huggingface.co/papers/2104.07857)ä¸­è¿›è¡Œäº†æè¿°ã€‚

DeepSpeed ZeRO-2ä¸»è¦ç”¨äºè®­ç»ƒï¼Œå› ä¸ºå®ƒçš„ç‰¹æ€§å¯¹æ¨ç†æ²¡æœ‰ç”¨å¤„ã€‚

DeepSpeed ZeRO-3ä¹Ÿå¯ä»¥ç”¨äºæ¨ç†ï¼Œå› ä¸ºå®ƒå…è®¸å°†å•ä¸ªGPUæ— æ³•åŠ è½½çš„å¤§æ¨¡å‹åŠ è½½åˆ°å¤šä¸ªGPUä¸Šã€‚

ğŸ¤— Transformersé€šè¿‡ä»¥ä¸‹ä¸¤ç§æ–¹å¼é›†æˆäº†[DeepSpeed](https://github.com/deepspeedai/DeepSpeed)ï¼š

1. é€šè¿‡[`Trainer`]é›†æˆæ ¸å¿ƒçš„DeepSpeedåŠŸèƒ½ã€‚è¿™æ˜¯ä¸€ç§â€œä¸ºæ‚¨å®Œæˆä¸€åˆ‡â€å¼çš„é›†æˆ - æ‚¨åªéœ€æä¾›è‡ªå®šä¹‰é…ç½®æ–‡ä»¶æˆ–ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡æ¿é…ç½®æ–‡ä»¶ã€‚æœ¬æ–‡æ¡£çš„å¤§éƒ¨åˆ†å†…å®¹éƒ½é›†ä¸­åœ¨è¿™ä¸ªåŠŸèƒ½ä¸Šã€‚
2. å¦‚æœæ‚¨ä¸ä½¿ç”¨[`Trainer`]å¹¶å¸Œæœ›åœ¨è‡ªå·±çš„Trainerä¸­é›†æˆDeepSpeedï¼Œé‚£ä¹ˆåƒ`from_pretrained`å’Œ`from_config`è¿™æ ·çš„æ ¸å¿ƒåŠŸèƒ½å‡½æ•°å°†åŒ…æ‹¬ZeRO stage 3åŠä»¥ä¸Šçš„DeepSpeedçš„åŸºç¡€éƒ¨åˆ†ï¼Œå¦‚`zero.Init`ã€‚è¦åˆ©ç”¨æ­¤åŠŸèƒ½ï¼Œè¯·é˜…è¯»æœ‰å…³[éTrainer DeepSpeedé›†æˆ](#nontrainer-deepspeed-integration)çš„æ–‡æ¡£ã€‚

é›†æˆçš„å†…å®¹ï¼š

è®­ç»ƒï¼š

1. DeepSpeed ZeROè®­ç»ƒæ”¯æŒå®Œæ•´çš„ZeRO stages 1ã€2å’Œ3ï¼Œä»¥åŠZeRO-Infinityï¼ˆCPUå’ŒNVMe offloadï¼‰ã€‚

æ¨ç†ï¼š

1. DeepSpeed ZeROæ¨ç†æ”¯æŒZeRO stage 3å’ŒZeRO-Infinityã€‚å®ƒä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„ZeROåè®®ï¼Œä½†ä¸ä½¿ç”¨ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œåªæœ‰stage 3ä¸æ¨ç†ç›¸å…³ã€‚æ›´å¤šè¯¦ç»†ä¿¡æ¯è¯·å‚é˜…ï¼š[zero-inference](#zero-inference)ã€‚

æ­¤å¤–è¿˜æœ‰DeepSpeedæ¨ç† - è¿™æ˜¯ä¸€ç§å®Œå…¨ä¸åŒçš„æŠ€æœ¯ï¼Œå®ƒä½¿ç”¨å¼ é‡å¹¶è¡Œè€Œä¸æ˜¯ZeROï¼ˆå³å°†æ¨å‡ºï¼‰ã€‚


<a id='deepspeed-trainer-integration'></a>


## Trainer DeepSpeed é›†æˆ


<a id='deepspeed-installation'></a>

### å®‰è£…

é€šè¿‡pypiå®‰è£…åº“ï¼š


```bash
pip install deepspeed
```

æˆ–é€šè¿‡ `transformers` çš„ `extras`å®‰è£…ï¼š

```bash
pip install transformers[deepspeed]
```

æˆ–åœ¨ [DeepSpeed çš„ GitHub é¡µé¢](https://github.com/deepspeedai/DeepSpeed#installation) å’Œ
[é«˜çº§å®‰è£…](https://www.deepspeed.ai/tutorials/advanced-install/) ä¸­æŸ¥æ‰¾æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚

å¦‚æœæ„å»ºè¿‡ç¨‹ä¸­ä»ç„¶é‡åˆ°é—®é¢˜ï¼Œè¯·é¦–å…ˆç¡®ä¿é˜…è¯» [CUDA æ‰©å±•å®‰è£…æ³¨æ„äº‹é¡¹](trainer#cuda-extension-installation-notes)ã€‚

å¦‚æœæ‚¨æ²¡æœ‰é¢„å…ˆæ„å»ºæ‰©å±•è€Œæ˜¯åœ¨è¿è¡Œæ—¶æ„å»ºå®ƒä»¬ï¼Œè€Œä¸”æ‚¨å°è¯•äº†ä»¥ä¸Šæ‰€æœ‰è§£å†³æ–¹æ¡ˆéƒ½æ— æ•ˆï¼Œä¸‹ä¸€æ­¥å¯ä»¥å°è¯•åœ¨å®‰è£…ä¹‹å‰é¢„å…ˆæ„å»ºæ‰©å±•ã€‚

è¿›è¡Œ DeepSpeed çš„æœ¬åœ°æ„å»ºï¼š


```bash
git clone https://github.com/deepspeedai/DeepSpeed/
cd DeepSpeed
rm -rf build
TORCH_CUDA_ARCH_LIST="8.6" DS_BUILD_CPU_ADAM=1 DS_BUILD_UTILS=1 pip install . \
--global-option="build_ext" --global-option="-j8" --no-cache -v \
--disable-pip-version-check 2>&1 | tee build.log
```

å¦‚æœæ‚¨æ‰“ç®—ä½¿ç”¨ NVMe offloadï¼Œæ‚¨è¿˜éœ€è¦åœ¨ä¸Šè¿°è¯´æ˜ä¸­æ·»åŠ  `DS_BUILD_AIO=1`ï¼ˆå¹¶ä¸”è¿˜éœ€è¦åœ¨ç³»ç»ŸèŒƒå›´å†…å®‰è£… *libaio-dev*ï¼‰ã€‚

ç¼–è¾‘ `TORCH_CUDA_ARCH_LIST` ä»¥æ’å…¥æ‚¨æ‰“ç®—ä½¿ç”¨çš„ GPU å¡çš„æ¶æ„ä»£ç ã€‚å‡è®¾æ‚¨çš„æ‰€æœ‰å¡éƒ½æ˜¯ç›¸åŒçš„ï¼Œæ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è·å–æ¶æ„ï¼š

```bash
CUDA_VISIBLE_DEVICES=0 python -c "import torch; print(torch.cuda.get_device_capability())"
```

å› æ­¤ï¼Œå¦‚æœæ‚¨å¾—åˆ° `8, 6`ï¼Œåˆ™ä½¿ç”¨ `TORCH_CUDA_ARCH_LIST="8.6"`ã€‚å¦‚æœæ‚¨æœ‰å¤šä¸ªä¸åŒçš„å¡ï¼Œæ‚¨å¯ä»¥åƒè¿™æ ·åˆ—å‡ºæ‰€æœ‰å¡ `TORCH_CUDA_ARCH_LIST="6.1;8.6"`ã€‚

å¦‚æœæ‚¨éœ€è¦åœ¨å¤šå°æœºå™¨ä¸Šä½¿ç”¨ç›¸åŒçš„è®¾ç½®ï¼Œè¯·åˆ›å»ºä¸€ä¸ªäºŒè¿›åˆ¶ wheelï¼š


```bash
git clone https://github.com/deepspeedai/DeepSpeed/
cd DeepSpeed
rm -rf build
TORCH_CUDA_ARCH_LIST="8.6" DS_BUILD_CPU_ADAM=1 DS_BUILD_UTILS=1 \
python setup.py build_ext -j8 bdist_wheel
```

å®ƒå°†ç”Ÿæˆç±»ä¼¼äº `dist/deepspeed-0.3.13+8cd046f-cp38-cp38-linux_x86_64.whl` çš„æ–‡ä»¶ï¼Œç°åœ¨æ‚¨å¯ä»¥åœ¨æœ¬åœ°æˆ–ä»»ä½•å…¶ä»–æœºå™¨ä¸Šå®‰è£…å®ƒï¼Œå¦‚ `pip install deepspeed-0.3.13+8cd046f-cp38-cp38-linux_x86_64.whl`ã€‚

å†æ¬¡æé†’ç¡®ä¿è°ƒæ•´ `TORCH_CUDA_ARCH_LIST` ä»¥åŒ¹é…ç›®æ ‡æ¶æ„ã€‚

æ‚¨å¯ä»¥åœ¨[è¿™é‡Œ](https://developer.nvidia.com/cuda-gpus)æ‰¾åˆ°å®Œæ•´çš„ NVIDIA GPU åˆ—è¡¨åŠå…¶å¯¹åº”çš„ **è®¡ç®—èƒ½åŠ›**ï¼ˆä¸æ­¤ä¸Šä¸‹æ–‡ä¸­çš„æ¶æ„ç›¸åŒï¼‰ã€‚

æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ£€æŸ¥ PyTorch æ„å»ºæ—¶ä½¿ç”¨çš„æ¶æ„ï¼š


```bash
python -c "import torch; print(torch.cuda.get_arch_list())"
```

ä»¥ä¸‹æ˜¯å¦‚ä½•æŸ¥æ‰¾å·²å®‰è£… GPU ä¸­çš„ä¸€å¼ å¡çš„æ¶æ„ã€‚ä¾‹å¦‚ï¼Œå¯¹äº GPU 0ï¼š

```bash
CUDA_VISIBLE_DEVICES=0 python -c "import torch; \
print(torch.cuda.get_device_properties(torch.device('cuda')))"
```

å¦‚æœè¾“å‡ºç»“æœå¦‚ä¸‹ï¼š

```bash
_CudaDeviceProperties(name='GeForce RTX 3090', major=8, minor=6, total_memory=24268MB, multi_processor_count=82)
```

ç„¶åæ‚¨å°±çŸ¥é“è¿™å¼ å¡çš„æ¶æ„æ˜¯ `8.6`ã€‚

æ‚¨ä¹Ÿå¯ä»¥å®Œå…¨çœç•¥ `TORCH_CUDA_ARCH_LIST`ï¼Œç„¶åæ„å»ºç¨‹åºå°†è‡ªåŠ¨æŸ¥è¯¢æ„å»ºæ‰€åœ¨çš„ GPU çš„æ¶æ„ã€‚è¿™å¯èƒ½ä¸ç›®æ ‡æœºå™¨ä¸Šçš„ GPU ä¸åŒ¹é…ï¼Œå› æ­¤æœ€å¥½æ˜ç¡®æŒ‡å®šæ‰€éœ€çš„æ¶æ„ã€‚

å¦‚æœå°è¯•äº†æ‰€æœ‰å»ºè®®çš„æ–¹æ³•ä»ç„¶é‡åˆ°æ„å»ºé—®é¢˜ï¼Œè¯·ç»§ç»­åœ¨ [Deepspeed](https://github.com/deepspeedai/DeepSpeed/issues)çš„ GitHub Issue ä¸Šæäº¤é—®é¢˜ã€‚


<a id='deepspeed-multi-gpu'></a>

### å¤šGPUå¯ç”¨

ä¸ºäº†å¯ç”¨DeepSpeed é›†æˆï¼Œè°ƒæ•´ [`Trainer`] çš„å‘½ä»¤è¡Œå‚æ•°ï¼Œæ·»åŠ ä¸€ä¸ªæ–°çš„å‚æ•° `--deepspeed ds_config.json`ï¼Œå…¶ä¸­ `ds_config.json` æ˜¯ DeepSpeed é…ç½®æ–‡ä»¶ï¼Œå¦‚æ–‡æ¡£ [è¿™é‡Œ](https://www.deepspeed.ai/docs/config-json/) æ‰€è¿°ã€‚æ–‡ä»¶å‘½åç”±æ‚¨å†³å®šã€‚
å»ºè®®ä½¿ç”¨ DeepSpeed çš„ `add_config_arguments` ç¨‹åºå°†å¿…è¦çš„å‘½ä»¤è¡Œå‚æ•°æ·»åŠ åˆ°æ‚¨çš„ä»£ç ä¸­ã€‚
æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜… [DeepSpeed çš„å‚æ•°è§£æ](https://deepspeed.readthedocs.io/en/latest/initialize.html#argument-parsing) æ–‡æ¡£ã€‚

åœ¨è¿™é‡Œï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ‚¨å–œæ¬¢çš„å¯åŠ¨å™¨ã€‚æ‚¨å¯ä»¥ç»§ç»­ä½¿ç”¨ PyTorch å¯åŠ¨å™¨ï¼š


```bash
torch.distributed.run --nproc_per_node=2 your_program.py <normal cl args> --deepspeed ds_config.json
```

æˆ–ä½¿ç”¨ç”± `deepspeed` æä¾›çš„å¯åŠ¨å™¨ï¼š


```bash
deepspeed --num_gpus=2 your_program.py <normal cl args> --deepspeed ds_config.json
```


æ­£å¦‚æ‚¨æ‰€è§ï¼Œè¿™ä¸¤ä¸ªå¯åŠ¨å™¨çš„å‚æ•°ä¸åŒï¼Œä½†å¯¹äºå¤§å¤šæ•°éœ€æ±‚ï¼Œä»»ä½•ä¸€ä¸ªéƒ½å¯ä»¥æ»¡è¶³å·¥ä½œéœ€æ±‚ã€‚æœ‰å…³å¦‚ä½•é…ç½®å„ä¸ªèŠ‚ç‚¹å’Œ GPU çš„å®Œæ•´è¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ [æ­¤å¤„](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node)ã€‚

å½“æ‚¨ä½¿ç”¨ `deepspeed` å¯åŠ¨å™¨å¹¶ä¸”å¸Œæœ›ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„ GPU æ—¶ï¼Œæ‚¨å¯ä»¥ç®€å•åœ°çœç•¥ `--num_gpus` æ ‡å¿—ã€‚

ä»¥ä¸‹æ˜¯åœ¨ DeepSpeed ä¸­å¯ç”¨ä½¿ç”¨æ‰€æœ‰å¯ç”¨ GPUæƒ…å†µä¸‹ï¼Œ è¿è¡Œ `run_translation.py` çš„ç¤ºä¾‹ï¼š


```bash
deepspeed examples/pytorch/translation/run_translation.py \
--deepspeed tests/deepspeed/ds_config_zero3.json \
--model_name_or_path google-t5/t5-small --per_device_train_batch_size 1 \
--output_dir output_dir --overwrite_output_dir --fp16 \
--do_train --max_train_samples 500 --num_train_epochs 1 \
--dataset_name wmt16 --dataset_config "ro-en" \
--source_lang en --target_lang ro
```

è¯·æ³¨æ„ï¼Œåœ¨ DeepSpeed æ–‡æ¡£ä¸­ï¼Œæ‚¨å¯èƒ½ä¼šçœ‹åˆ° `--deepspeed --deepspeed_config ds_config.json` - å³ä¸¤ä¸ªä¸ DeepSpeed ç›¸å…³çš„å‚æ•°ï¼Œä½†ä¸ºç®€å•èµ·è§ï¼Œå¹¶ä¸”å› ä¸ºå·²ç»æœ‰å¾ˆå¤šå‚æ•°è¦å¤„ç†ï¼Œæˆ‘ä»¬å°†ä¸¤è€…åˆå¹¶ä¸ºä¸€ä¸ªå•ä¸€å‚æ•°ã€‚

æœ‰å…³ä¸€äº›å®é™…ä½¿ç”¨ç¤ºä¾‹ï¼Œè¯·å‚é˜… [æ­¤å¸–](https://github.com/huggingface/transformers/issues/8771#issuecomment-759248400)ã€‚



<a id='deepspeed-one-gpu'></a>

### å•GPUå¯ç”¨

è¦ä½¿ç”¨ä¸€å¼  GPU å¯ç”¨ DeepSpeedï¼Œè°ƒæ•´ [`Trainer`] çš„å‘½ä»¤è¡Œå‚æ•°å¦‚ä¸‹ï¼š


```bash
deepspeed --num_gpus=1 examples/pytorch/translation/run_translation.py \
--deepspeed tests/deepspeed/ds_config_zero2.json \
--model_name_or_path google-t5/t5-small --per_device_train_batch_size 1 \
--output_dir output_dir --overwrite_output_dir --fp16 \
--do_train --max_train_samples 500 --num_train_epochs 1 \
--dataset_name wmt16 --dataset_config "ro-en" \
--source_lang en --target_lang ro
```

è¿™ä¸å¤š GPU çš„æƒ…å†µå‡ ä¹ç›¸åŒï¼Œä½†åœ¨è¿™é‡Œæˆ‘ä»¬é€šè¿‡ `--num_gpus=1` æ˜ç¡®å‘Šè¯‰ DeepSpeed ä»…ä½¿ç”¨ä¸€å¼  GPUã€‚é»˜è®¤æƒ…å†µä¸‹ï¼ŒDeepSpeed å¯ç”¨ç»™å®šèŠ‚ç‚¹ä¸Šå¯ä»¥çœ‹åˆ°çš„æ‰€æœ‰ GPUã€‚å¦‚æœæ‚¨ä¸€å¼€å§‹åªæœ‰ä¸€å¼  GPUï¼Œé‚£ä¹ˆæ‚¨ä¸éœ€è¦è¿™ä¸ªå‚æ•°ã€‚ä»¥ä¸‹ [æ–‡æ¡£](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node) è®¨è®ºäº†å¯åŠ¨å™¨çš„é€‰é¡¹ã€‚

ä¸ºä»€ä¹ˆè¦åœ¨ä»…ä½¿ç”¨ä¸€å¼  GPU çš„æƒ…å†µä¸‹ä½¿ç”¨ DeepSpeed å‘¢ï¼Ÿ

1. å®ƒå…·æœ‰ ZeRO-offload åŠŸèƒ½ï¼Œå¯ä»¥å°†ä¸€äº›è®¡ç®—å’Œå†…å­˜å§”æ‰˜ç»™ä¸»æœºçš„ CPU å’Œ å†…å­˜ï¼Œä»è€Œä¸ºæ¨¡å‹çš„éœ€æ±‚ä¿ç•™æ›´å¤š GPU èµ„æº - ä¾‹å¦‚æ›´å¤§çš„æ‰¹å¤„ç†å¤§å°ï¼Œæˆ–å¯ç”¨æ­£å¸¸æƒ…å†µä¸‹æ— æ³•å®¹çº³çš„éå¸¸å¤§æ¨¡å‹ã€‚
2. å®ƒæä¾›äº†æ™ºèƒ½çš„ GPU å†…å­˜ç®¡ç†ç³»ç»Ÿï¼Œæœ€å°åŒ–å†…å­˜ç¢ç‰‡ï¼Œè¿™å†æ¬¡å…è®¸æ‚¨å®¹çº³æ›´å¤§çš„æ¨¡å‹å’Œæ•°æ®æ‰¹æ¬¡ã€‚

è™½ç„¶æ¥ä¸‹æ¥æˆ‘ä»¬å°†è¯¦ç»†è®¨è®ºé…ç½®ï¼Œä½†åœ¨å•ä¸ª GPU ä¸Šé€šè¿‡ DeepSpeed å®ç°å·¨å¤§æ€§èƒ½æå‡çš„å…³é”®æ˜¯åœ¨é…ç½®æ–‡ä»¶ä¸­è‡³å°‘æœ‰ä»¥ä¸‹é…ç½®ï¼š


```json
{
  "zero_optimization": {
     "stage": 2,
     "offload_optimizer": {
         "device": "cpu",
         "pin_memory": true
     },
     "allgather_partitions": true,
     "allgather_bucket_size": 2e8,
     "reduce_scatter": true,
     "reduce_bucket_size": 2e8,
     "overlap_comm": true,
     "contiguous_gradients": true
  }
}
```

è¿™ä¼šå¯ç”¨`optimizer offload `å’Œä¸€äº›å…¶ä»–é‡è¦åŠŸèƒ½ã€‚æ‚¨å¯ä»¥å°è¯•ä¸åŒçš„bufferå¤§å°ï¼Œæœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ä¸‹é¢çš„è®¨è®ºã€‚

å…³äºè¿™ç§å¯ç”¨ç±»å‹çš„å®é™…ä½¿ç”¨ç¤ºä¾‹ï¼Œè¯·å‚é˜… [æ­¤å¸–](https://github.com/huggingface/transformers/issues/8771#issuecomment-759176685)ã€‚

æ‚¨è¿˜å¯ä»¥å°è¯•ä½¿ç”¨æœ¬æ–‡åé¢è¿›ä¸€æ­¥è§£é‡Šçš„æ”¯æŒ`CPU å’Œ NVMe offload`åŠŸèƒ½çš„ZeRO-3 ã€‚


<!--- TODO: Benchmark whether we can get better performance out of ZeRO-3 vs. ZeRO-2 on a single GPU, and then
recommend ZeRO-3 config as starting one. -->

æ³¨æ„ï¼š

- å¦‚æœæ‚¨éœ€è¦åœ¨ç‰¹å®šçš„ GPU ä¸Šè¿è¡Œï¼Œè€Œä¸æ˜¯ GPU 0ï¼Œåˆ™æ— æ³•ä½¿ç”¨ `CUDA_VISIBLE_DEVICES` æ¥é™åˆ¶å¯ç”¨ GPU çš„å¯è§èŒƒå›´ã€‚ç›¸åï¼Œæ‚¨å¿…é¡»ä½¿ç”¨ä»¥ä¸‹è¯­æ³•ï¼š

  ```bash
  deepspeed --include localhost:1 examples/pytorch/translation/run_translation.py ...
  ```

  åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å‘Šè¯‰ DeepSpeed ä½¿ç”¨ GPU 1ï¼ˆç¬¬äºŒä¸ª GPUï¼‰ã€‚



<a id='deepspeed-multi-node'></a>

### å¤šèŠ‚ç‚¹å¯ç”¨

è¿™ä¸€éƒ¨åˆ†çš„ä¿¡æ¯ä¸ä»…é€‚ç”¨äº DeepSpeed é›†æˆï¼Œä¹Ÿé€‚ç”¨äºä»»ä½•å¤šèŠ‚ç‚¹ç¨‹åºã€‚ä½† DeepSpeed æä¾›äº†ä¸€ä¸ªæ¯”å…¶ä»–å¯åŠ¨å™¨æ›´æ˜“äºä½¿ç”¨çš„ `deepspeed` å¯åŠ¨å™¨ï¼Œé™¤éæ‚¨åœ¨ SLURM ç¯å¢ƒä¸­ã€‚

åœ¨æœ¬èŠ‚ï¼Œè®©æˆ‘ä»¬å‡è®¾æ‚¨æœ‰ä¸¤ä¸ªèŠ‚ç‚¹ï¼Œæ¯ä¸ªèŠ‚ç‚¹æœ‰ 8 å¼  GPUã€‚æ‚¨å¯ä»¥é€šè¿‡ `ssh hostname1` è®¿é—®ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ï¼Œé€šè¿‡ `ssh hostname2` è®¿é—®ç¬¬äºŒä¸ªèŠ‚ç‚¹ï¼Œä¸¤è€…å¿…é¡»èƒ½å¤Ÿåœ¨æœ¬åœ°é€šè¿‡ ssh æ— å¯†ç æ–¹å¼ç›¸äº’è®¿é—®ã€‚å½“ç„¶ï¼Œæ‚¨éœ€è¦å°†è¿™äº›ä¸»æœºï¼ˆèŠ‚ç‚¹ï¼‰åç§°é‡å‘½åä¸ºæ‚¨å®é™…ä½¿ç”¨çš„ä¸»æœºåç§°ã€‚


#### torch.distributed.runå¯åŠ¨å™¨


ä¾‹å¦‚ï¼Œè¦ä½¿ç”¨ `torch.distributed.run`ï¼Œæ‚¨å¯ä»¥æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

```bash
python -m torch.distributed.run --nproc_per_node=8 --nnode=2 --node_rank=0 --master_addr=hostname1 \
--master_port=9901 your_program.py <normal cl args> --deepspeed ds_config.json
```

æ‚¨å¿…é¡» ssh åˆ°æ¯ä¸ªèŠ‚ç‚¹ï¼Œå¹¶åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šè¿è¡Œç›¸åŒçš„å‘½ä»¤ï¼ä¸ç”¨æ‹…å¿ƒï¼Œå¯åŠ¨å™¨ä¼šç­‰å¾…ä¸¤ä¸ªèŠ‚ç‚¹åŒæ­¥å®Œæˆã€‚

æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜… [torchrun](https://pytorch.org/docs/stable/elastic/run.html)ã€‚é¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œè¿™ä¹Ÿæ˜¯æ›¿ä»£äº†å‡ ä¸ª PyTorch ç‰ˆæœ¬å‰çš„ `torch.distributed.launch` çš„å¯åŠ¨å™¨ã€‚


#### deepspeedå¯åŠ¨å™¨

è¦æ”¹ç”¨ `deepspeed` å¯åŠ¨å™¨ï¼Œé¦–å…ˆéœ€è¦åˆ›å»ºä¸€ä¸ª `hostfile` æ–‡ä»¶ï¼š

```
hostname1 slots=8
hostname2 slots=8
```
ç„¶åï¼Œæ‚¨å¯ä»¥è¿™æ ·å¯åŠ¨ï¼š

```bash
deepspeed --num_gpus 8 --num_nodes 2 --hostfile hostfile --master_addr hostname1 --master_port=9901 \
your_program.py <normal cl args> --deepspeed ds_config.json
```

ä¸ `torch.distributed.run` å¯åŠ¨å™¨ä¸åŒï¼Œ`deepspeed` å°†è‡ªåŠ¨åœ¨ä¸¤ä¸ªèŠ‚ç‚¹ä¸Šå¯åŠ¨æ­¤å‘½ä»¤ï¼

æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[èµ„æºé…ç½®ï¼ˆå¤šèŠ‚ç‚¹ï¼‰](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node)ã€‚


#### åœ¨ SLURM ç¯å¢ƒä¸­å¯åŠ¨

åœ¨ SLURM ç¯å¢ƒä¸­ï¼Œå¯ä»¥é‡‡ç”¨ä»¥ä¸‹æ–¹æ³•ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ª SLURM è„šæœ¬ `launch.slurm`ï¼Œæ‚¨éœ€è¦æ ¹æ®æ‚¨çš„å…·ä½“ SLURM ç¯å¢ƒè¿›è¡Œè°ƒæ•´ã€‚

```bash
#SBATCH --job-name=test-nodes        # name
#SBATCH --nodes=2                    # nodes
#SBATCH --ntasks-per-node=1          # crucial - only 1 task per dist per node!
#SBATCH --cpus-per-task=10           # number of cores per tasks
#SBATCH --gres=gpu:8                 # number of gpus
#SBATCH --time 20:00:00              # maximum execution time (HH:MM:SS)
#SBATCH --output=%x-%j.out           # output file name

export GPUS_PER_NODE=8
export MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)
export MASTER_PORT=9901

srun --jobid $SLURM_JOBID bash -c 'python -m torch.distributed.run \
 --nproc_per_node $GPUS_PER_NODE --nnodes $SLURM_NNODES --node_rank $SLURM_PROCID \
 --master_addr $MASTER_ADDR --master_port $MASTER_PORT \
your_program.py <normal cl args> --deepspeed ds_config.json'
```

å‰©ä¸‹çš„å°±æ˜¯è¿è¡Œå®ƒï¼š

```bash
sbatch launch.slurm
```

`srun` å°†è´Ÿè´£åœ¨æ‰€æœ‰èŠ‚ç‚¹ä¸ŠåŒæ—¶å¯åŠ¨ç¨‹åºã€‚


#### ä½¿ç”¨éå…±äº«æ–‡ä»¶ç³»ç»Ÿ

é»˜è®¤æƒ…å†µä¸‹ï¼ŒDeepSpeed å‡å®šå¤šèŠ‚ç‚¹ç¯å¢ƒä½¿ç”¨å…±äº«å­˜å‚¨ã€‚å¦‚æœä¸æ˜¯è¿™ç§æƒ…å†µï¼Œæ¯ä¸ªèŠ‚ç‚¹åªèƒ½çœ‹åˆ°æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿï¼Œä½ éœ€è¦è°ƒæ•´é…ç½®æ–‡ä»¶ï¼ŒåŒ…å«ä¸€ä¸ª [`checkpoint` éƒ¨åˆ†](https://www.deepspeed.ai/docs/config-json/#checkpoint-options)å¹¶è®¾ç½®å¦‚ä¸‹é€‰é¡¹ï¼š

```json
{
  "checkpoint": {
    "use_node_local_storage": true
  }
}
```

æˆ–è€…ï¼Œä½ è¿˜å¯ä»¥ä½¿ç”¨ [`Trainer`] çš„ `--save_on_each_node` å‚æ•°ï¼Œä¸Šè¿°é…ç½®å°†è‡ªåŠ¨æ·»åŠ ã€‚


<a id='deepspeed-notebook'></a>

### åœ¨Notebookså¯ç”¨

åœ¨å°†`notebook cells`ä½œä¸ºè„šæœ¬è¿è¡Œçš„æƒ…å†µä¸‹ï¼Œé—®é¢˜åœ¨äºæ²¡æœ‰æ­£å¸¸çš„ `deepspeed` å¯åŠ¨å™¨å¯ä¾èµ–ï¼Œå› æ­¤åœ¨æŸäº›è®¾ç½®ä¸‹ï¼Œæˆ‘ä»¬å¿…é¡»ä»¿çœŸè¿è¡Œå®ƒã€‚

å¦‚æœæ‚¨åªä½¿ç”¨ä¸€ä¸ª GPUï¼Œä»¥ä¸‹æ˜¯å¦‚ä½•è°ƒæ•´notebookä¸­çš„è®­ç»ƒä»£ç ä»¥ä½¿ç”¨ DeepSpeedã€‚

```python
# DeepSpeed requires a distributed environment even when only one process is used.
# This emulates a launcher in the notebook
import os

os.environ["MASTER_ADDR"] = "localhost"
os.environ["MASTER_PORT"] = "9994"  # modify if RuntimeError: Address already in use
os.environ["RANK"] = "0"
os.environ["LOCAL_RANK"] = "0"
os.environ["WORLD_SIZE"] = "1"

# Now proceed as normal, plus pass the deepspeed config file
training_args = TrainingArguments(..., deepspeed="ds_config_zero3.json")
trainer = Trainer(...)
trainer.train()
```

æ³¨æ„ï¼š`...` ä»£è¡¨æ‚¨ä¼ é€’ç»™å‡½æ•°çš„æ­£å¸¸å‚æ•°ã€‚

å¦‚æœè¦ä½¿ç”¨å¤šäºä¸€ä¸ª GPUï¼Œæ‚¨å¿…é¡»åœ¨ DeepSpeed ä¸­ä½¿ç”¨å¤šè¿›ç¨‹ç¯å¢ƒã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæ‚¨å¿…é¡»ä½¿ç”¨ä¸“é—¨çš„å¯åŠ¨å™¨æ¥å®ç°è¿™ä¸€ç›®çš„ï¼Œè€Œä¸èƒ½é€šè¿‡ä»¿çœŸæœ¬èŠ‚å¼€å¤´å‘ˆç°çš„åˆ†å¸ƒå¼ç¯å¢ƒæ¥å®Œæˆã€‚

å¦‚æœæƒ³è¦åœ¨notebookä¸­åŠ¨æ€åˆ›å»ºé…ç½®æ–‡ä»¶å¹¶ä¿å­˜åœ¨å½“å‰ç›®å½•ï¼Œæ‚¨å¯ä»¥åœ¨ä¸€ä¸ªä¸“ç”¨çš„cellä¸­ä½¿ç”¨ï¼š

```python no-style
%%bash
cat <<'EOT' > ds_config_zero3.json
{
    "fp16": {
        "enabled": "auto",
        "loss_scale": 0,
        "loss_scale_window": 1000,
        "initial_scale_power": 16,
        "hysteresis": 2,
        "min_loss_scale": 1
    },

    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": "auto",
            "betas": "auto",
            "eps": "auto",
            "weight_decay": "auto"
        }
    },

    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": "auto",
            "warmup_max_lr": "auto",
            "warmup_num_steps": "auto"
        }
    },

    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "offload_param": {
            "device": "cpu",
            "pin_memory": true
        },
        "overlap_comm": true,
        "contiguous_gradients": true,
        "sub_group_size": 1e9,
        "reduce_bucket_size": "auto",
        "stage3_prefetch_bucket_size": "auto",
        "stage3_param_persistence_threshold": "auto",
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
        "stage3_gather_16bit_weights_on_model_save": true
    },

    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "steps_per_print": 2000,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "wall_clock_breakdown": false
}
EOT
```

å¦‚æœè®­ç»ƒè„šæœ¬åœ¨ä¸€ä¸ªæ™®é€šæ–‡ä»¶ä¸­è€Œä¸æ˜¯åœ¨notebook cellsä¸­ï¼Œæ‚¨å¯ä»¥é€šè¿‡ç¬”è®°æœ¬ä¸­çš„ shell æ­£å¸¸å¯åŠ¨ `deepspeed`ã€‚ä¾‹å¦‚ï¼Œè¦ä½¿ç”¨ `run_translation.py`ï¼Œæ‚¨å¯ä»¥è¿™æ ·å¯åŠ¨ï¼š

```python no-style
!git clone https://github.com/huggingface/transformers
!cd transformers; deepspeed examples/pytorch/translation/run_translation.py ...
```

æˆ–è€…ä½¿ç”¨ `%%bash` é­”æœ¯å‘½ä»¤ï¼Œæ‚¨å¯ä»¥ç¼–å†™å¤šè¡Œä»£ç ï¼Œç”¨äºè¿è¡Œ shell ç¨‹åºï¼š

```python no-style
%%bash

git clone https://github.com/huggingface/transformers
cd transformers
deepspeed examples/pytorch/translation/run_translation.py ...
```

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨ä¸éœ€è¦æœ¬èŠ‚å¼€å¤´å‘ˆç°çš„ä»»ä½•ä»£ç ã€‚

æ³¨æ„ï¼šè™½ç„¶ `%%bash` é­”æœ¯å‘½ä»¤å¾ˆæ–¹ä¾¿ï¼Œä½†ç›®å‰å®ƒä¼šç¼“å†²è¾“å‡ºï¼Œå› æ­¤åœ¨è¿›ç¨‹å®Œæˆä¹‹å‰æ‚¨çœ‹ä¸åˆ°æ—¥å¿—ã€‚


<a id='deepspeed-config'></a>

### é…ç½®

æœ‰å…³å¯ä»¥åœ¨ DeepSpeed é…ç½®æ–‡ä»¶ä¸­ä½¿ç”¨çš„å®Œæ•´é…ç½®é€‰é¡¹çš„è¯¦ç»†æŒ‡å—ï¼Œè¯·å‚é˜…[ä»¥ä¸‹æ–‡æ¡£](https://www.deepspeed.ai/docs/config-json/)ã€‚

æ‚¨å¯ä»¥åœ¨ [DeepSpeedExamples ä»“åº“](https://github.com/deepspeedai/DeepSpeedExamples)ä¸­æ‰¾åˆ°è§£å†³å„ç§å®é™…éœ€æ±‚çš„æ•°åä¸ª DeepSpeed é…ç½®ç¤ºä¾‹ã€‚

```bash
git clone https://github.com/deepspeedai/DeepSpeedExamples
cd DeepSpeedExamples
find . -name '*json'
```

å»¶ç»­ä¸Šé¢çš„ä»£ç ï¼Œå‡è®¾æ‚¨è¦é…ç½® Lamb ä¼˜åŒ–å™¨ã€‚é‚£ä¹ˆæ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼åœ¨ç¤ºä¾‹çš„ `.json` æ–‡ä»¶ä¸­è¿›è¡Œæœç´¢ï¼š

```bash
grep -i Lamb $(find . -name '*json')
```

è¿˜å¯ä»¥åœ¨[ä¸»ä»“](https://github.com/deepspeedai/DeepSpeed)ä¸­æ‰¾åˆ°æ›´å¤šç¤ºä¾‹ã€‚

åœ¨ä½¿ç”¨ DeepSpeed æ—¶ï¼Œæ‚¨æ€»æ˜¯éœ€è¦æä¾›ä¸€ä¸ª DeepSpeed é…ç½®æ–‡ä»¶ï¼Œä½†æ˜¯ä¸€äº›é…ç½®å‚æ•°å¿…é¡»é€šè¿‡å‘½ä»¤è¡Œè¿›è¡Œé…ç½®ã€‚æ‚¨å°†åœ¨æœ¬æŒ‡å—çš„å‰©ä½™ç« èŠ‚æ‰¾åˆ°è¿™äº›ç»†å¾®å·®åˆ«ã€‚

ä¸ºäº†äº†è§£ DeepSpeed é…ç½®æ–‡ä»¶ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªæ¿€æ´» ZeRO stage 2 åŠŸèƒ½çš„ç¤ºä¾‹ï¼ŒåŒ…æ‹¬ä¼˜åŒ–å™¨çŠ¶æ€çš„ CPU offloadï¼Œä½¿ç”¨ `AdamW` ä¼˜åŒ–å™¨å’Œ `WarmupLR`  è°ƒåº¦å™¨ï¼Œå¹¶ä¸”å¦‚æœä¼ é€’äº† `--fp16` å‚æ•°å°†å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼š

```json
{
    "fp16": {
        "enabled": "auto",
        "loss_scale": 0,
        "loss_scale_window": 1000,
        "initial_scale_power": 16,
        "hysteresis": 2,
        "min_loss_scale": 1
    },

    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": "auto",
            "betas": "auto",
            "eps": "auto",
            "weight_decay": "auto"
        }
    },

    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": "auto",
            "warmup_max_lr": "auto",
            "warmup_num_steps": "auto"
        }
    },

    "zero_optimization": {
        "stage": 2,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "allgather_partitions": true,
        "allgather_bucket_size": 2e8,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 2e8,
        "contiguous_gradients": true
    },

    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
}
```

å½“æ‚¨æ‰§è¡Œç¨‹åºæ—¶ï¼ŒDeepSpeed å°†æŠŠå®ƒä» [`Trainer`] æ”¶åˆ°çš„é…ç½®æ—¥å¿—è¾“å‡ºåˆ°consoleï¼Œå› æ­¤æ‚¨å¯ä»¥çœ‹åˆ°ä¼ é€’ç»™å®ƒçš„æœ€ç»ˆé…ç½®ã€‚



<a id='deepspeed-config-passing'></a>

### ä¼ é€’é…ç½®

æ­£å¦‚æœ¬æ–‡æ¡£è®¨è®ºçš„é‚£æ ·ï¼Œé€šå¸¸å°† DeepSpeed é…ç½®ä½œä¸ºæŒ‡å‘ JSON æ–‡ä»¶çš„è·¯å¾„ä¼ é€’ï¼Œä½†å¦‚æœæ‚¨æ²¡æœ‰ä½¿ç”¨å‘½ä»¤è¡Œç•Œé¢é…ç½®è®­ç»ƒï¼Œè€Œæ˜¯é€šè¿‡ [`TrainingArguments`] å®ä¾‹åŒ– [`Trainer`]ï¼Œé‚£ä¹ˆå¯¹äº `deepspeed` å‚æ•°ï¼Œä½ å¯ä»¥ä¼ é€’ä¸€ä¸ªåµŒå¥—çš„ `dict`ã€‚è¿™ä½¿æ‚¨èƒ½å¤Ÿå³æ—¶åˆ›å»ºé…ç½®ï¼Œè€Œæ— éœ€åœ¨å°†å…¶ä¼ é€’ç»™ [`TrainingArguments`] ä¹‹å‰å°†å…¶å†™å…¥æ–‡ä»¶ç³»ç»Ÿã€‚

æ€»ç»“èµ·æ¥ï¼Œæ‚¨å¯ä»¥è¿™æ ·åšï¼š

```python
TrainingArguments(..., deepspeed="/path/to/ds_config.json")
```

æˆ–è€…:

```python
ds_config_dict = dict(scheduler=scheduler_params, optimizer=optimizer_params)
TrainingArguments(..., deepspeed=ds_config_dict)
```

<a id='deepspeed-config-shared'></a>

### å…±äº«é…ç½®


<Tip warning={true}>

è¿™ä¸€éƒ¨åˆ†æ˜¯å¿…è¯»çš„ã€‚

</Tip>

ä¸€äº›é…ç½®å€¼å¯¹äº [`Trainer`] å’Œ DeepSpeed æ­£å¸¸è¿è¡Œéƒ½æ˜¯å¿…éœ€çš„ï¼Œå› æ­¤ï¼Œä¸ºäº†é˜²æ­¢å®šä¹‰å†²çªåŠå¯¼è‡´çš„éš¾ä»¥æ£€æµ‹çš„é”™è¯¯ï¼Œæˆ‘ä»¬é€‰æ‹©é€šè¿‡ [`Trainer`] å‘½ä»¤è¡Œå‚æ•°é…ç½®è¿™äº›å€¼ã€‚

æ­¤å¤–ï¼Œä¸€äº›é…ç½®å€¼æ˜¯åŸºäºæ¨¡å‹çš„é…ç½®è‡ªåŠ¨æ´¾ç”Ÿçš„ï¼Œå› æ­¤ï¼Œä¸å…¶è®°ä½æ‰‹åŠ¨è°ƒæ•´å¤šä¸ªå€¼ï¼Œæœ€å¥½è®© [`Trainer`] ä¸ºæ‚¨åšå¤§éƒ¨åˆ†é…ç½®ã€‚

å› æ­¤ï¼Œåœ¨æœ¬æŒ‡å—çš„å…¶ä½™éƒ¨åˆ†ï¼Œæ‚¨å°†æ‰¾åˆ°ä¸€ä¸ªç‰¹æ®Šçš„é…ç½®å€¼ï¼š`auto`ï¼Œå½“è®¾ç½®æ—¶å°†è‡ªåŠ¨å°†å‚æ•°æ›¿æ¢ä¸ºæ­£ç¡®æˆ–æœ€æœ‰æ•ˆçš„å€¼ã€‚è¯·éšæ„é€‰æ‹©å¿½ç•¥æ­¤å»ºè®®æˆ–æ˜¾å¼è®¾ç½®è¯¥å€¼ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¯·åŠ¡å¿…ç¡®ä¿ [`Trainer`] å‚æ•°å’Œ DeepSpeed é…ç½®ä¿æŒä¸€è‡´ã€‚ä¾‹å¦‚ï¼Œæ‚¨æ˜¯å¦ä½¿ç”¨ç›¸åŒçš„å­¦ä¹ ç‡ã€æ‰¹é‡å¤§å°æˆ–æ¢¯åº¦ç´¯ç§¯è®¾ç½®ï¼Ÿå¦‚æœè¿™äº›ä¸åŒ¹é…ï¼Œè®­ç»ƒå¯èƒ½ä»¥éå¸¸éš¾ä»¥æ£€æµ‹çš„æ–¹å¼å¤±è´¥ã€‚è¯·é‡è§†è¯¥è­¦å‘Šã€‚

è¿˜æœ‰ä¸€äº›å‚æ•°æ˜¯ä»…é€‚ç”¨äº DeepSpeed çš„ï¼Œå¹¶ä¸”è¿™äº›å‚æ•°å¿…é¡»æ‰‹åŠ¨è®¾ç½®ä»¥é€‚åº”æ‚¨çš„éœ€æ±‚ã€‚

åœ¨æ‚¨è‡ªå·±çš„ç¨‹åºä¸­ï¼Œå¦‚æœæ‚¨æƒ³è¦ä½œä¸ºä¸»åŠ¨ä¿®æ”¹ DeepSpeed é…ç½®å¹¶ä»¥æ­¤é…ç½® [`TrainingArguments`]ï¼Œæ‚¨è¿˜å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•ã€‚æ­¥éª¤å¦‚ä¸‹ï¼š

1. åˆ›å»ºæˆ–åŠ è½½è¦ç”¨ä½œä¸»é…ç½®çš„ DeepSpeed é…ç½®
2. æ ¹æ®è¿™äº›å‚æ•°å€¼åˆ›å»º [`TrainingArguments`] å¯¹è±¡

è¯·æ³¨æ„ï¼Œä¸€äº›å€¼ï¼Œæ¯”å¦‚ `scheduler.params.total_num_steps`ï¼Œæ˜¯åœ¨ [`Trainer`] çš„ `train` è¿‡ç¨‹ä¸­è®¡ç®—çš„ï¼Œä½†å½“ç„¶æ‚¨ä¹Ÿå¯ä»¥è‡ªå·±è®¡ç®—è¿™äº›å€¼ã€‚


<a id='deepspeed-zero'></a>

### ZeRO

[Zero Redundancy Optimizer (ZeRO)](https://www.deepspeed.ai/tutorials/zero/) æ˜¯ DeepSpeed çš„å·¥ä½œæ ¸å¿ƒã€‚å®ƒæ”¯æŒ3ä¸ªä¸åŒçº§åˆ«ï¼ˆstagesï¼‰çš„ä¼˜åŒ–ã€‚Stage 1 å¯¹äºæ‰©å±•æ€§æ¥è¯´ä¸æ˜¯å¾ˆæœ‰è¶£ï¼Œå› æ­¤æœ¬æ–‡æ¡£é‡ç‚¹å…³æ³¨Stage 2å’ŒStage 3ã€‚Stage 3é€šè¿‡æœ€æ–°çš„ ZeRO-Infinity è¿›ä¸€æ­¥æ”¹è¿›ã€‚ä½ å¯ä»¥åœ¨ DeepSpeed æ–‡æ¡£ä¸­æ‰¾åˆ°æ›´è¯¦ç»†çš„ä¿¡æ¯ã€‚

é…ç½®æ–‡ä»¶çš„ `zero_optimization` éƒ¨åˆ†æ˜¯æœ€é‡è¦çš„éƒ¨åˆ†ï¼ˆ[æ–‡æ¡£](https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training)ï¼‰ï¼Œå› ä¸ºåœ¨è¿™é‡Œæ‚¨å®šä¹‰äº†è¦å¯ç”¨å“ªäº› ZeRO stages ä»¥åŠå¦‚ä½•é…ç½®å®ƒä»¬ã€‚æ‚¨å¯ä»¥åœ¨ DeepSpeed æ–‡æ¡£ä¸­æ‰¾åˆ°æ¯ä¸ªå‚æ•°çš„è§£é‡Šã€‚

è¿™ä¸€éƒ¨åˆ†å¿…é¡»é€šè¿‡ DeepSpeed é…ç½®æ–‡ä»¶å•ç‹¬é…ç½® - [`Trainer`] ä¸æä¾›ç›¸åº”çš„å‘½ä»¤è¡Œå‚æ•°ã€‚

æ³¨æ„ï¼šç›®å‰ DeepSpeed ä¸éªŒè¯å‚æ•°åç§°ï¼Œå› æ­¤å¦‚æœæ‚¨æ‹¼é”™äº†ä»»ä½•å‚æ•°ï¼Œå®ƒå°†ä½¿ç”¨æ‹¼å†™é”™è¯¯çš„å‚æ•°çš„é»˜è®¤è®¾ç½®ã€‚æ‚¨å¯ä»¥è§‚å¯Ÿ DeepSpeed å¼•æ“å¯åŠ¨æ—¥å¿—æ¶ˆæ¯ï¼Œçœ‹çœ‹å®ƒå°†ä½¿ç”¨å“ªäº›å€¼ã€‚

<a id='deepspeed-zero2-config'></a>

#### ZeRO-2 é…ç½®

ä»¥ä¸‹æ˜¯ ZeRO stage 2 çš„é…ç½®ç¤ºä¾‹ï¼š

```json
{
    "zero_optimization": {
        "stage": 2,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "allgather_partitions": true,
        "allgather_bucket_size": 5e8,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 5e8,
        "contiguous_gradients": true
    }
}
```

**æ€§èƒ½è°ƒä¼˜ï¼š**

- å¯ç”¨ `offload_optimizer` åº”è¯¥å‡å°‘ GPU å†…å­˜ä½¿ç”¨ï¼ˆéœ€è¦ `"stage": 2`ï¼‰ã€‚
- `"overlap_comm": true` é€šè¿‡å¢åŠ  GPU å†…å­˜ä½¿ç”¨æ¥é™ä½all-reduce çš„å»¶è¿Ÿã€‚ `overlap_comm` ä½¿ç”¨äº† `allgather_bucket_size` å’Œ `reduce_bucket_size` å€¼çš„4.5å€ã€‚å› æ­¤ï¼Œå¦‚æœå®ƒä»¬è®¾ç½®ä¸º `5e8`ï¼Œè¿™å°†éœ€è¦ä¸€ä¸ª9GBçš„å†…å­˜å ç”¨ï¼ˆ`5e8 x 2Bytes x 2 x 4.5`ï¼‰ã€‚å› æ­¤ï¼Œå¦‚æœæ‚¨çš„ GPU å†…å­˜ä¸º8GBæˆ–æ›´å°ï¼Œä¸ºäº†é¿å…å‡ºç°OOMé”™è¯¯ï¼Œæ‚¨éœ€è¦å°†è¿™äº›å‚æ•°å‡å°åˆ°çº¦ `2e8`ï¼Œè¿™å°†éœ€è¦3.6GBã€‚å¦‚æœæ‚¨çš„ GPU å®¹é‡æ›´å¤§ï¼Œå½“æ‚¨å¼€å§‹é‡åˆ°OOMæ—¶ï¼Œä½ å¯èƒ½ä¹Ÿéœ€è¦è¿™æ ·åšã€‚
- å½“å‡å°è¿™äº›buffersæ—¶ï¼Œæ‚¨ä»¥æ›´æ…¢çš„é€šä¿¡é€Ÿåº¦æ¥æ¢å–æ›´å¤šçš„ GPU å†…å­˜ã€‚bufferså¤§å°è¶Šå°ï¼Œé€šä¿¡é€Ÿåº¦è¶Šæ…¢ï¼ŒGPU å¯ç”¨äºå…¶ä»–ä»»åŠ¡çš„å†…å­˜å°±è¶Šå¤šã€‚å› æ­¤ï¼Œå¦‚æœæ›´å¤§çš„æ‰¹å¤„ç†å¤§å°å¾ˆé‡è¦ï¼Œé‚£ä¹ˆç¨å¾®å‡æ…¢è®­ç»ƒæ—¶é—´å¯èƒ½æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æƒè¡¡ã€‚

æ­¤å¤–ï¼Œ`deepspeed==0.4.4` æ·»åŠ äº†ä¸€ä¸ªæ–°é€‰é¡¹ `round_robin_gradients`ï¼Œæ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å¯ç”¨ï¼š

```json
{
    "zero_optimization": {
        "round_robin_gradients": true
    }
}
```
è¿™æ˜¯ä¸€ä¸ªç”¨äº CPU offloading çš„stage 2ä¼˜åŒ–ï¼Œé€šè¿‡ç»†ç²’åº¦æ¢¯åº¦åˆ†åŒºåœ¨ ranks ä¹‹é—´å¹¶è¡Œå¤åˆ¶åˆ° CPU å†…å­˜ï¼Œä»è€Œå®ç°äº†æ€§èƒ½çš„æå‡ã€‚æ€§èƒ½ä¼˜åŠ¿éšç€æ¢¯åº¦ç´¯ç§¯æ­¥éª¤ï¼ˆåœ¨ä¼˜åŒ–å™¨æ­¥éª¤ä¹‹é—´è¿›è¡Œæ›´å¤šå¤åˆ¶ï¼‰æˆ– GPU æ•°é‡ï¼ˆå¢åŠ å¹¶è¡Œæ€§ï¼‰å¢åŠ è€Œå¢åŠ ã€‚

<a id='deepspeed-zero3-config'></a>

#### ZeRO-3 é…ç½®

ä»¥ä¸‹æ˜¯ ZeRO stage 3çš„é…ç½®ç¤ºä¾‹ï¼š

```json
{
    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "offload_param": {
            "device": "cpu",
            "pin_memory": true
        },
        "overlap_comm": true,
        "contiguous_gradients": true,
        "sub_group_size": 1e9,
        "reduce_bucket_size": "auto",
        "stage3_prefetch_bucket_size": "auto",
        "stage3_param_persistence_threshold": "auto",
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
        "stage3_gather_16bit_weights_on_model_save": true
    }
}
```

å¦‚æœæ‚¨å› ä¸ºä½ çš„æ¨¡å‹æˆ–æ¿€æ´»å€¼è¶…è¿‡ GPU å†…å­˜è€Œé‡åˆ°OOMé—®é¢˜ï¼Œå¹¶ä¸”æ‚¨æœ‰æœªä½¿ç”¨çš„ CPU å†…å­˜ï¼Œå¯ä»¥é€šè‚¡ç¥¨ä½¿ç”¨ `"device": "cpu"` å°†ä¼˜åŒ–å™¨çŠ¶æ€å’Œå‚æ•°å¸è½½åˆ° CPU å†…å­˜ä¸­ï¼Œæ¥è§£å†³è¿™ä¸ªé™åˆ¶ã€‚å¦‚æœæ‚¨ä¸æƒ³å¸è½½åˆ° CPU å†…å­˜ï¼Œå¯ä»¥åœ¨ `device` æ¡ç›®ä¸­ä½¿ç”¨ `none` ä»£æ›¿ `cpu`ã€‚å°†ä¼˜åŒ–å™¨çŠ¶æ€å¸è½½åˆ° NVMe ä¸Šä¼šåœ¨åé¢è¿›ä¸€æ­¥è®¨è®ºã€‚

é€šè¿‡å°† `pin_memory` è®¾ç½®ä¸º `true` å¯ç”¨å›ºå®šå†…å­˜ã€‚æ­¤åŠŸèƒ½ä¼šä»¥å‡å°‘å¯ç”¨äºå…¶ä»–è¿›ç¨‹çš„å†…å­˜ä¸ºä»£ä»·æ¥æé«˜ååé‡ã€‚å›ºå®šå†…å­˜è¢«åˆ†é…ç»™ç‰¹å®šè¯·æ±‚å®ƒçš„è¿›ç¨‹ï¼Œé€šå¸¸æ¯”æ™®é€š CPU å†…å­˜è®¿é—®é€Ÿåº¦æ›´å¿«ã€‚

**æ€§èƒ½è°ƒä¼˜ï¼š**

- `stage3_max_live_parameters`: `1e9`
- `stage3_max_reuse_distance`: `1e9`

å¦‚æœé‡åˆ°OOMé—®é¢˜ï¼Œè¯·å‡å° `stage3_max_live_parameters` å’Œ `stage3_max_reuse_distance`ã€‚å®ƒä»¬å¯¹æ€§èƒ½çš„å½±å“åº”è¯¥å¾ˆå°ï¼Œé™¤éæ‚¨æ­£åœ¨è¿›è¡Œæ¿€æ´»å€¼checkpointingã€‚`1e9` å¤§çº¦ä¼šæ¶ˆè€— ~2GBã€‚å†…å­˜ç”± `stage3_max_live_parameters` å’Œ `stage3_max_reuse_distance` å…±äº«ï¼Œæ‰€ä»¥å®ƒä¸æ˜¯å åŠ çš„ï¼Œè€Œæ˜¯æ€»å…±2GBã€‚

`stage3_max_live_parameters` æ˜¯åœ¨ä»»ä½•ç»™å®šæ—¶é—´è¦åœ¨ GPU ä¸Šä¿ç•™å¤šå°‘ä¸ªå®Œæ•´å‚æ•°çš„ä¸Šé™ã€‚"reuse distance" æ˜¯æˆ‘ä»¬ç”¨æ¥ç¡®å®šå‚æ•°åœ¨å°†æ¥ä½•æ—¶ä¼šå†æ¬¡ä½¿ç”¨çš„åº¦é‡æ ‡å‡†ï¼Œæˆ‘ä»¬ä½¿ç”¨ `stage3_max_reuse_distance` æ¥å†³å®šæ˜¯ä¸¢å¼ƒå‚æ•°è¿˜æ˜¯ä¿ç•™å‚æ•°ã€‚å¦‚æœä¸€ä¸ªå‚æ•°åœ¨ä¸ä¹…çš„å°†æ¥ï¼ˆå°äº `stage3_max_reuse_distance`ï¼‰å°†è¢«å†æ¬¡ä½¿ç”¨ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°†å…¶ä¿ç•™ä»¥å‡å°‘é€šä¿¡å¼€é”€ã€‚è¿™åœ¨å¯ç”¨æ¿€æ´»å€¼checkpoingæ—¶éå¸¸æœ‰ç”¨ï¼Œå…¶ä¸­æˆ‘ä»¬ä»¥å•å±‚ç²’åº¦è¿›è¡Œå‰å‘é‡è®¡ç®—å’Œåå‘ä¼ æ’­ï¼Œå¹¶å¸Œæœ›åœ¨åå‘ä¼ æ’­æœŸé—´ä¿ç•™å‰å‘é‡è®¡ç®—ä¸­çš„å‚æ•°ã€‚

ä»¥ä¸‹é…ç½®å€¼å–å†³äºæ¨¡å‹çš„éšè—å¤§å°ï¼š

- `reduce_bucket_size`: `hidden_size*hidden_size`
- `stage3_prefetch_bucket_size`: `0.9 * hidden_size * hidden_size`
- `stage3_param_persistence_threshold`: `10 * hidden_size`

å› æ­¤ï¼Œå°†è¿™äº›å€¼è®¾ç½®ä¸º `auto`ï¼Œ[`Trainer`] å°†è‡ªåŠ¨åˆ†é…æ¨èçš„å‚æ•°å€¼ã€‚å½“ç„¶ï¼Œå¦‚æœæ‚¨æ„¿æ„ï¼Œä¹Ÿå¯ä»¥æ˜¾å¼è®¾ç½®è¿™äº›å€¼ã€‚

`stage3_gather_16bit_weights_on_model_save` åœ¨æ¨¡å‹ä¿å­˜æ—¶å¯ç”¨æ¨¡å‹çš„ fp16 æƒé‡æ•´åˆã€‚å¯¹äºå¤§æ¨¡å‹å’Œå¤šä¸ª GPUï¼Œæ— è®ºæ˜¯åœ¨å†…å­˜è¿˜æ˜¯é€Ÿåº¦æ–¹é¢ï¼Œè¿™éƒ½æ˜¯ä¸€é¡¹æ˜‚è´µçš„æ“ä½œã€‚ç›®å‰å¦‚æœè®¡åˆ’æ¢å¤è®­ç»ƒï¼Œè¿™æ˜¯å¿…éœ€çš„ã€‚è¯·æ³¨æ„æœªæ¥çš„æ›´æ–°å¯èƒ½ä¼šåˆ é™¤æ­¤é™åˆ¶å¹¶è®©ä½¿ç”¨æ›´åŠ çµæ´»ã€‚

å¦‚æœæ‚¨ä» ZeRO-2 é…ç½®è¿ç§»ï¼Œè¯·æ³¨æ„ `allgather_partitions`ã€`allgather_bucket_size` å’Œ `reduce_scatter` é…ç½®å‚æ•°åœ¨ ZeRO-3 ä¸­ä¸è¢«ä½¿ç”¨ã€‚å¦‚æœä¿ç•™è¿™äº›é…ç½®æ–‡ä»¶ï¼Œå®ƒä»¬å°†è¢«å¿½ç•¥ã€‚

- `sub_group_size`: `1e9`

`sub_group_size` æ§åˆ¶åœ¨ä¼˜åŒ–å™¨æ­¥éª¤æœŸé—´æ›´æ–°å‚æ•°çš„ç²’åº¦ã€‚å‚æ•°è¢«åˆ†ç»„åˆ°å¤§å°ä¸º `sub_group_size` çš„æ¡¶ä¸­ï¼Œæ¯ä¸ªæ¡¶é€ä¸ªæ›´æ–°ã€‚åœ¨ ZeRO-Infinity ä¸­ä¸ NVMe offloadä¸€èµ·ä½¿ç”¨æ—¶ï¼Œ`sub_group_size` æ§åˆ¶äº†åœ¨ä¼˜åŒ–å™¨æ­¥éª¤æœŸé—´åœ¨ NVMe å’Œ CPU å†…å­˜ä¹‹é—´ç§»åŠ¨æ¨¡å‹çŠ¶æ€çš„ç²’åº¦ã€‚è¿™å¯ä»¥é˜²æ­¢éå¸¸å¤§çš„æ¨¡å‹è€—å°½ CPU å†…å­˜ã€‚

å½“ä¸ä½¿ç”¨ NVMe offloadæ—¶ï¼Œå¯ä»¥å°† `sub_group_size` ä¿ç•™ä¸ºå…¶é»˜è®¤å€¼ *1e9*ã€‚åœ¨ä»¥ä¸‹æƒ…å†µä¸‹ï¼Œæ‚¨å¯èƒ½éœ€è¦æ›´æ”¹å…¶é»˜è®¤å€¼ï¼š

1. åœ¨ä¼˜åŒ–å™¨æ­¥éª¤ä¸­é‡åˆ°OOMï¼šå‡å° `sub_group_size` ä»¥å‡å°‘ä¸´æ—¶buffersçš„å†…å­˜åˆ©ç”¨
2. ä¼˜åŒ–å™¨æ­¥éª¤èŠ±è´¹å¾ˆé•¿æ—¶é—´ï¼šå¢åŠ  `sub_group_size` ä»¥æé«˜ç”±äºå¢åŠ çš„æ•°æ®buffersè€Œå¯¼è‡´çš„å¸¦å®½åˆ©ç”¨ç‡ã€‚


#### ZeRO-0 é…ç½®

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å°† Stage 0 å’Œ 1 æ”¾åœ¨æœ€åï¼Œå› ä¸ºå®ƒä»¬å¾ˆå°‘ä½¿ç”¨ã€‚

Stage 0 ç¦ç”¨äº†æ‰€æœ‰ç±»å‹çš„åˆ†ç‰‡ï¼Œåªæ˜¯å°† DeepSpeed ä½œä¸º DDP ä½¿ç”¨ã€‚æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å¯ç”¨ï¼š

```json
{
    "zero_optimization": {
        "stage": 0
    }
}
```

è¿™å°†å®è´¨ä¸Šç¦ç”¨ ZeROï¼Œè€Œæ— éœ€æ›´æ”¹å…¶ä»–ä»»ä½•å†…å®¹ã€‚


#### ZeRO-1 é…ç½®


Stage 1 ç­‰åŒäº Stage 2 å‡å»æ¢¯åº¦åˆ†ç‰‡ã€‚æ‚¨å¯ä»¥å°è¯•ä½¿ç”¨ä»¥ä¸‹é…ç½®ï¼Œä»…å¯¹ä¼˜åŒ–å™¨çŠ¶æ€è¿›è¡Œåˆ†ç‰‡ï¼Œä»¥ç¨å¾®åŠ é€Ÿï¼š


```json
{
    "zero_optimization": {
        "stage": 1
    }
}
```



<a id='deepspeed-nvme'></a>

### NVMe æ”¯æŒ

ZeRO-Infinity é€šè¿‡ä½¿ç”¨ NVMe å†…å­˜æ‰©å±• GPU å’Œ CPU å†…å­˜ï¼Œä»è€Œå…è®¸è®­ç»ƒéå¸¸å¤§çš„æ¨¡å‹ã€‚ç”±äºæ™ºèƒ½åˆ†åŒºå’Œå¹³é“ºç®—æ³•ï¼Œåœ¨offloadæœŸé—´æ¯ä¸ª GPU éœ€è¦å‘é€å’Œæ¥æ”¶éå¸¸å°é‡çš„æ•°æ®ï¼Œå› æ­¤ NVMe è¢«è¯æ˜é€‚ç”¨äºè®­ç»ƒè¿‡ç¨‹ä¸­æä¾›æ›´å¤§çš„æ€»å†…å­˜æ± ã€‚ZeRO-Infinity éœ€è¦å¯ç”¨ ZeRO-3ã€‚

ä»¥ä¸‹é…ç½®ç¤ºä¾‹å¯ç”¨ NVMe æ¥offloadä¼˜åŒ–å™¨çŠ¶æ€å’Œå‚æ•°ï¼š

```json
{
    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {
            "device": "nvme",
            "nvme_path": "/local_nvme",
            "pin_memory": true,
            "buffer_count": 4,
            "fast_init": false
        },
        "offload_param": {
            "device": "nvme",
            "nvme_path": "/local_nvme",
            "pin_memory": true,
            "buffer_count": 5,
            "buffer_size": 1e8,
            "max_in_cpu": 1e9
        },
        "aio": {
            "block_size": 262144,
            "queue_depth": 32,
            "thread_count": 1,
            "single_submit": false,
            "overlap_events": true
        },
        "overlap_comm": true,
        "contiguous_gradients": true,
        "sub_group_size": 1e9,
        "reduce_bucket_size": "auto",
        "stage3_prefetch_bucket_size": "auto",
        "stage3_param_persistence_threshold": "auto",
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
        "stage3_gather_16bit_weights_on_model_save": true
    },
}
```

æ‚¨å¯ä»¥é€‰æ‹©å°†ä¼˜åŒ–å™¨çŠ¶æ€å’Œå‚æ•°éƒ½å¸è½½åˆ° NVMeï¼Œä¹Ÿå¯ä»¥åªé€‰æ‹©å…¶ä¸­ä¸€ä¸ªï¼Œæˆ–è€…éƒ½ä¸é€‰æ‹©ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æœ‰å¤§é‡çš„ CPU å†…å­˜å¯ç”¨ï¼Œåªå¸è½½åˆ° CPU å†…å­˜è®­ç»ƒé€Ÿåº¦ä¼šæ›´å¿«ï¼ˆæç¤ºï¼š"device": "cpu"ï¼‰ã€‚

è¿™æ˜¯æœ‰å…³å¸è½½ [ä¼˜åŒ–å™¨çŠ¶æ€](https://www.deepspeed.ai/docs/config-json/#optimizer-offloading) å’Œ [å‚æ•°](https://www.deepspeed.ai/docs/config-json/#parameter-offloading) çš„å®Œæ•´æ–‡æ¡£ã€‚

ç¡®ä¿æ‚¨çš„ `nvme_path` å®é™…ä¸Šæ˜¯ä¸€ä¸ª NVMeï¼Œå› ä¸ºå®ƒä¸æ™®é€šç¡¬ç›˜æˆ– SSD ä¸€èµ·å·¥ä½œï¼Œä½†é€Ÿåº¦ä¼šæ…¢å¾—å¤šã€‚å¿«é€Ÿå¯æ‰©å±•çš„è®­ç»ƒæ˜¯æ ¹æ®ç°ä»£ NVMe ä¼ è¾“é€Ÿåº¦è®¾è®¡çš„ï¼ˆæˆªè‡³æœ¬æ–‡æ’°å†™æ—¶ï¼Œå¯ä»¥è¾¾åˆ° ~3.5GB/s è¯»å–ï¼Œ~3GB/s å†™å…¥çš„å³°å€¼é€Ÿåº¦ï¼‰ã€‚

ä¸ºäº†æ‰¾å‡ºæœ€ä½³çš„ `aio` é…ç½®å—ï¼Œæ‚¨å¿…é¡»åœ¨ç›®æ ‡è®¾ç½®ä¸Šè¿è¡Œä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œå…·ä½“æ“ä½œè¯·å‚è§[è¯´æ˜](https://github.com/deepspeedai/DeepSpeed/issues/998)ã€‚



<a id='deepspeed-zero2-zero3-performance'></a>

#### ZeRO-2 å’Œ ZeRO-3 æ€§èƒ½å¯¹æ¯”

å¦‚æœå…¶ä»–ä¸€åˆ‡éƒ½é…ç½®ç›¸åŒï¼ŒZeRO-3 å¯èƒ½æ¯” ZeRO-2 æ…¢ï¼Œå› ä¸ºå‰è€…é™¤äº† ZeRO-2 çš„æ“ä½œå¤–ï¼Œè¿˜å¿…é¡»æ”¶é›†æ¨¡å‹æƒé‡ã€‚å¦‚æœ ZeRO-2 æ»¡è¶³æ‚¨çš„éœ€æ±‚ï¼Œè€Œä¸”æ‚¨ä¸éœ€è¦æ‰©å±•åˆ°å‡ ä¸ª GPU ä»¥ä¸Šï¼Œé‚£ä¹ˆæ‚¨å¯ä»¥é€‰æ‹©ç»§ç»­ä½¿ç”¨å®ƒã€‚é‡è¦çš„æ˜¯è¦ç†è§£ï¼ŒZeRO-3 ä»¥é€Ÿåº¦ä¸ºä»£ä»·å®ç°äº†æ›´é«˜çš„å¯æ‰©å±•æ€§ã€‚

å¯ä»¥è°ƒæ•´ ZeRO-3 é…ç½®ä½¿å…¶æ€§èƒ½æ¥è¿‘ ZeRO-2ï¼š

- å°† `stage3_param_persistence_threshold` è®¾ç½®ä¸ºä¸€ä¸ªéå¸¸å¤§çš„æ•°å­— - å¤§äºæœ€å¤§çš„å‚æ•°ï¼Œä¾‹å¦‚ `6 * hidden_size * hidden_size`ã€‚è¿™å°†ä¿ç•™å‚æ•°åœ¨ GPU ä¸Šã€‚
- å…³é—­ `offload_params`ï¼Œå› ä¸º ZeRO-2 æ²¡æœ‰è¿™ä¸ªé€‰é¡¹ã€‚

å³ä½¿ä¸æ›´æ”¹ `stage3_param_persistence_threshold`ï¼Œä»…å°† `offload_params` å…³é—­ï¼Œæ€§èƒ½å¯èƒ½ä¼šæ˜¾è‘—æé«˜ã€‚å½“ç„¶ï¼Œè¿™äº›æ›´æ”¹å°†å½±å“æ‚¨å¯ä»¥è®­ç»ƒçš„æ¨¡å‹çš„å¤§å°ã€‚å› æ­¤ï¼Œè¿™äº›æ›´æ”¹å¯æ ¹æ®éœ€æ±‚å¸®åŠ©æ‚¨åœ¨å¯æ‰©å±•æ€§å’Œé€Ÿåº¦ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚



<a id='deepspeed-zero2-example'></a>

#### ZeRO-2 ç¤ºä¾‹

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„ ZeRO-2 è‡ªåŠ¨é…ç½®æ–‡ä»¶ `ds_config_zero2.json`ï¼š

```json
{
    "fp16": {
        "enabled": "auto",
        "loss_scale": 0,
        "loss_scale_window": 1000,
        "initial_scale_power": 16,
        "hysteresis": 2,
        "min_loss_scale": 1
    },

    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": "auto",
            "betas": "auto",
            "eps": "auto",
            "weight_decay": "auto"
        }
    },

    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": "auto",
            "warmup_max_lr": "auto",
            "warmup_num_steps": "auto"
        }
    },

    "zero_optimization": {
        "stage": 2,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "allgather_partitions": true,
        "allgather_bucket_size": 2e8,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 2e8,
        "contiguous_gradients": true
    },

    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "steps_per_print": 2000,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "wall_clock_breakdown": false
}
```

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ‰‹åŠ¨è®¾ç½®çš„å¯ç”¨æ‰€æœ‰åŠŸèƒ½çš„ ZeRO-2 é…ç½®æ–‡ä»¶ã€‚ä¸»è¦æ˜¯ä¸ºäº†è®©æ‚¨çœ‹åˆ°å…¸å‹çš„å‚æ•°å€¼æ˜¯ä»€ä¹ˆæ ·çš„ï¼Œä½†æˆ‘ä»¬å¼ºçƒˆå»ºè®®ä½¿ç”¨å…¶ä¸­åŒ…å«å¤šä¸ª `auto` è®¾ç½®çš„é…ç½®æ–‡ä»¶ã€‚

```json
{
    "fp16": {
        "enabled": true,
        "loss_scale": 0,
        "loss_scale_window": 1000,
        "initial_scale_power": 16,
        "hysteresis": 2,
        "min_loss_scale": 1
    },

    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 3e-5,
            "betas": [0.8, 0.999],
            "eps": 1e-8,
            "weight_decay": 3e-7
        }
    },

    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": 0,
            "warmup_max_lr": 3e-5,
            "warmup_num_steps": 500
        }
    },

    "zero_optimization": {
        "stage": 2,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "allgather_partitions": true,
        "allgather_bucket_size": 2e8,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 2e8,
        "contiguous_gradients": true
    },

    "steps_per_print": 2000,
    "wall_clock_breakdown": false
}
```

<a id='deepspeed-zero3-example'></a>

#### ZeRO-3 ç¤ºä¾‹

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„ ZeRO-3 è‡ªåŠ¨é…ç½®æ–‡ä»¶ `ds_config_zero3.json`ï¼š

```json
{
    "fp16": {
        "enabled": "auto",
        "loss_scale": 0,
        "loss_scale_window": 1000,
        "initial_scale_power": 16,
        "hysteresis": 2,
        "min_loss_scale": 1
    },

    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": "auto",
            "betas": "auto",
            "eps": "auto",
            "weight_decay": "auto"
        }
    },

    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": "auto",
            "warmup_max_lr": "auto",
            "warmup_num_steps": "auto"
        }
    },

    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "offload_param": {
            "device": "cpu",
            "pin_memory": true
        },
        "overlap_comm": true,
        "contiguous_gradients": true,
        "sub_group_size": 1e9,
        "reduce_bucket_size": "auto",
        "stage3_prefetch_bucket_size": "auto",
        "stage3_param_persistence_threshold": "auto",
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
        "stage3_gather_16bit_weights_on_model_save": true
    },

    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "steps_per_print": 2000,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "wall_clock_breakdown": false
}
```

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„ æ‰‹åŠ¨è®¾ç½®çš„å¯ç”¨æ‰€æœ‰åŠŸèƒ½çš„ZeRO-3 é…ç½®æ–‡ä»¶ã€‚ä¸»è¦æ˜¯ä¸ºäº†è®©æ‚¨çœ‹åˆ°å…¸å‹çš„å‚æ•°å€¼æ˜¯ä»€ä¹ˆæ ·çš„ï¼Œä½†æˆ‘ä»¬å¼ºçƒˆå»ºè®®ä½¿ç”¨å…¶ä¸­åŒ…å«å¤šä¸ª `auto` è®¾ç½®çš„é…ç½®æ–‡ä»¶ã€‚

```json
{
    "fp16": {
        "enabled": true,
        "loss_scale": 0,
        "loss_scale_window": 1000,
        "initial_scale_power": 16,
        "hysteresis": 2,
        "min_loss_scale": 1
    },

    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 3e-5,
            "betas": [0.8, 0.999],
            "eps": 1e-8,
            "weight_decay": 3e-7
        }
    },

    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": 0,
            "warmup_max_lr": 3e-5,
            "warmup_num_steps": 500
        }
    },

    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "offload_param": {
            "device": "cpu",
            "pin_memory": true
        },
        "overlap_comm": true,
        "contiguous_gradients": true,
        "sub_group_size": 1e9,
        "reduce_bucket_size": 1e6,
        "stage3_prefetch_bucket_size": 0.94e6,
        "stage3_param_persistence_threshold": 1e4,
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
        "stage3_gather_16bit_weights_on_model_save": true
    },

    "steps_per_print": 2000,
    "wall_clock_breakdown": false
}
```

#### å¦‚ä½•é€‰æ‹©æœ€ä½³æ€§èƒ½çš„ZeRO Stageå’Œ offloads

äº†è§£äº†è¿™äº›ä¸åŒstagesåï¼Œç°åœ¨æ‚¨éœ€è¦å†³å®šä½¿ç”¨å“ªä¸ªstageã€‚æœ¬èŠ‚å°†å°è¯•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚

é€šå¸¸ï¼Œä»¥ä¸‹è§„åˆ™é€‚ç”¨ï¼š

- é€Ÿåº¦æ–¹é¢ï¼ˆå·¦è¾¹æ¯”å³è¾¹å¿«ï¼‰

  stage 0ï¼ˆDDPï¼‰ > stage 1 > stage 2 > stage 2 + offload  > stage 3 > stage3 + offload

- GPUå†…å­˜ä½¿ç”¨æ–¹é¢ï¼ˆå³è¾¹æ¯”å·¦è¾¹æ›´èŠ‚çœGPUå†…å­˜ï¼‰

  stage 0ï¼ˆDDPï¼‰ < stage 1 < stage 2 < stage 2 + offload < stage 3 < stage 3 + offload

æ‰€ä»¥ï¼Œå½“æ‚¨å¸Œæœ›åœ¨å°½é‡ä½¿ç”¨è¾ƒå°‘æ•°é‡çš„GPUçš„åŒæ—¶è·å¾—æœ€å¿«çš„æ‰§è¡Œé€Ÿåº¦æ—¶ï¼Œå¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œã€‚æˆ‘ä»¬ä»æœ€å¿«çš„æ–¹æ³•å¼€å§‹ï¼Œå¦‚æœé‡åˆ°GPUå†…å­˜æº¢å‡ºï¼Œç„¶ååˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªé€Ÿåº¦è¾ƒæ…¢ä½†ä½¿ç”¨çš„GPUå†…å­˜æ›´å°‘çš„æ–¹æ³•ã€‚ä»¥æ­¤ç±»æ¨ã€‚

é¦–å…ˆï¼Œå°†æ‰¹é‡å¤§å°è®¾ç½®ä¸º1ï¼ˆæ‚¨å§‹ç»ˆå¯ä»¥ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ¥è·å¾—ä»»ä½•æ‰€éœ€çš„æœ‰æ•ˆæ‰¹é‡å¤§å°ï¼‰ã€‚


1. å¯ç”¨ `--gradient_checkpointing 1`ï¼ˆHF Trainerï¼‰æˆ–ç›´æ¥ `model.gradient_checkpointing_enable()` - å¦‚æœå‘ç”ŸOOMï¼ˆOut of Memoryï¼‰ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ã€‚
2. é¦–å…ˆå°è¯• ZeRO stage 2ã€‚å¦‚æœå‘ç”ŸOOMï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ã€‚
3. å°è¯• ZeRO stage 2 + `offload_optimizer` - å¦‚æœå‘ç”ŸOOMï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ã€‚
4. åˆ‡æ¢åˆ° ZeRO stage 3 - å¦‚æœå‘ç”ŸOOMï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ã€‚
5. å¯ç”¨ `offload_param` åˆ° `cpu` - å¦‚æœå‘ç”ŸOOMï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ã€‚
6. å¯ç”¨ `offload_optimizer` åˆ° `cpu` - å¦‚æœå‘ç”ŸOOMï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ã€‚
7. å¦‚æœä»ç„¶æ— æ³•é€‚åº”æ‰¹é‡å¤§å°ä¸º1ï¼Œè¯·é¦–å…ˆæ£€æŸ¥å„ç§é»˜è®¤å€¼å¹¶å°½å¯èƒ½é™ä½å®ƒä»¬ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½¿ç”¨ `generate` å¹¶ä¸”ä¸ä½¿ç”¨å®½æœç´¢æŸï¼Œå°†å…¶ç¼©å°ï¼Œå› ä¸ºå®ƒä¼šå ç”¨å¤§é‡å†…å­˜ã€‚
8. ç»å¯¹è¦ä½¿ç”¨æ··åˆåŠç²¾åº¦è€Œéfp32 - åœ¨AmpereåŠæ›´é«˜çš„GPUä¸Šä½¿ç”¨bf16ï¼Œåœ¨æ—§çš„GPUä½“ç³»ç»“æ„ä¸Šä½¿ç”¨fp16ã€‚
9. å¦‚æœä»ç„¶å‘ç”ŸOOMï¼Œå¯ä»¥æ·»åŠ æ›´å¤šç¡¬ä»¶æˆ–å¯ç”¨ZeRO-Infinity - å³åˆ‡æ¢ `offload_param` å’Œ `offload_optimizer` åˆ° `nvme`ã€‚æ‚¨éœ€è¦ç¡®ä¿å®ƒæ˜¯éå¸¸å¿«çš„NVMeã€‚ä½œä¸ºè¶£é—»ï¼Œæˆ‘æ›¾ç»èƒ½å¤Ÿåœ¨ä¸€ä¸ªå°å‹GPUä¸Šä½¿ç”¨BLOOM-176Bè¿›è¡Œæ¨ç†ï¼Œä½¿ç”¨äº†ZeRO-Infinityï¼Œå°½ç®¡é€Ÿåº¦éå¸¸æ…¢ã€‚ä½†å®ƒå¥æ•ˆäº†ï¼

å½“ç„¶ï¼Œæ‚¨ä¹Ÿå¯ä»¥æŒ‰ç›¸åçš„é¡ºåºè¿›è¡Œè¿™äº›æ­¥éª¤ï¼Œä»æœ€èŠ‚çœGPUå†…å­˜çš„é…ç½®å¼€å§‹ï¼Œç„¶åé€æ­¥åå‘è¿›è¡Œï¼Œæˆ–è€…å°è¯•è¿›è¡ŒäºŒåˆ†æ³•ã€‚

ä¸€æ—¦æ‚¨çš„æ‰¹é‡å¤§å°ä¸º1ä¸ä¼šå¯¼è‡´OOMï¼Œå°±æµ‹é‡æ‚¨çš„æœ‰æ•ˆååé‡ã€‚

æ¥ä¸‹æ¥å°è¯•å°†æ‰¹é‡å¤§å°å¢åŠ åˆ°å°½å¯èƒ½å¤§ï¼Œå› ä¸ºæ‰¹é‡å¤§å°è¶Šå¤§ï¼ŒGPUçš„æ•ˆç‡è¶Šé«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å®ƒä»¬ä¹˜æ³•è¿ç®—çš„çŸ©é˜µå¾ˆå¤§æ—¶ã€‚

ç°åœ¨æ€§èƒ½ä¼˜åŒ–æ¸¸æˆå¼€å§‹äº†ã€‚æ‚¨å¯ä»¥å…³é—­ä¸€äº›offloadç‰¹æ€§ï¼Œæˆ–è€…é™ä½ZeRO stageï¼Œå¹¶å¢åŠ /å‡å°‘æ‰¹é‡å¤§å°ï¼Œå†æ¬¡æµ‹é‡æœ‰æ•ˆååé‡ã€‚åå¤å°è¯•ï¼Œç›´åˆ°æ»¡æ„ä¸ºæ­¢ã€‚

ä¸è¦èŠ±è´¹å¤ªå¤šæ—¶é—´ï¼Œä½†å¦‚æœæ‚¨å³å°†å¼€å§‹ä¸€ä¸ªä¸ºæœŸ3ä¸ªæœˆçš„è®­ç»ƒ - è¯·èŠ±å‡ å¤©æ—¶é—´æ‰¾åˆ°ååé‡æ–¹é¢æœ€æœ‰æ•ˆçš„è®¾ç½®ã€‚è¿™æ ·æ‚¨çš„è®­ç»ƒæˆæœ¬å°†æœ€ä½ï¼Œè€Œä¸”æ‚¨ä¼šæ›´å¿«åœ°å®Œæˆè®­ç»ƒã€‚åœ¨å½“å‰å¿«èŠ‚å¥çš„æœºå™¨å­¦ä¹ ä¸–ç•Œä¸­ï¼Œå¦‚æœæ‚¨èŠ±è´¹ä¸€ä¸ªé¢å¤–çš„æœˆä»½æ¥è®­ç»ƒæŸæ ·ä¸œè¥¿ï¼Œä½ å¾ˆå¯èƒ½ä¼šé”™è¿‡ä¸€ä¸ªé»„é‡‘æœºä¼šã€‚å½“ç„¶ï¼Œè¿™åªæ˜¯æˆ‘åˆ†äº«çš„ä¸€ç§è§‚å¯Ÿï¼Œæˆ‘å¹¶ä¸æ˜¯åœ¨å‚¬ä¿ƒä½ ã€‚åœ¨å¼€å§‹è®­ç»ƒBLOOM-176Bä¹‹å‰ï¼Œæˆ‘èŠ±äº†2å¤©æ—¶é—´è¿›è¡Œè¿™ä¸ªè¿‡ç¨‹ï¼ŒæˆåŠŸå°†ååé‡ä»90 TFLOPsæé«˜åˆ°150 TFLOPsï¼è¿™ä¸€åŠªåŠ›ä¸ºæˆ‘ä»¬èŠ‚çœäº†ä¸€ä¸ªå¤šæœˆçš„è®­ç»ƒæ—¶é—´ã€‚

è¿™äº›æ³¨é‡Šä¸»è¦æ˜¯ä¸ºè®­ç»ƒæ¨¡å¼ç¼–å†™çš„ï¼Œä½†å®ƒä»¬åœ¨æ¨ç†ä¸­ä¹Ÿåº”è¯¥å¤§éƒ¨åˆ†é€‚ç”¨ã€‚ä¾‹å¦‚ï¼Œåœ¨æ¨ç†ä¸­ï¼ŒGradient Checkpointing æ˜¯æ— ç”¨çš„ï¼Œå› ä¸ºå®ƒåªåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœ‰ç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°ï¼Œå¦‚æœä½ æ­£åœ¨è¿›è¡Œå¤šGPUæ¨ç†å¹¶ä¸”ä¸ä½¿ç”¨ [DeepSpeed-Inference](https://www.deepspeed.ai/tutorials/inference-tutorial/)ï¼Œ[Accelerate](https://huggingface.co/blog/bloom-inference-pytorch-scripts) åº”è¯¥æä¾›æ›´ä¼˜è¶Šçš„æ€§èƒ½ã€‚

å…¶ä»–ä¸æ€§èƒ½ç›¸å…³çš„å¿«é€Ÿæ³¨é‡Šï¼š
- å¦‚æœæ‚¨ä»å¤´å¼€å§‹è®­ç»ƒæŸä¸ªæ¨¡å‹ï¼Œè¯·å°½é‡ç¡®ä¿å¼ é‡çš„å½¢çŠ¶å¯ä»¥è¢«16æ•´é™¤ï¼ˆä¾‹å¦‚éšè—å±‚å¤§å°ï¼‰ã€‚å¯¹äºæ‰¹é‡å¤§å°ï¼Œè‡³å°‘å°è¯•å¯è¢«2æ•´é™¤ã€‚å¦‚æœæ‚¨æƒ³ä»GPUä¸­æŒ¤å–æ›´é«˜æ€§èƒ½ï¼Œè¿˜æœ‰ä¸€äº›ç¡¬ä»¶ç‰¹å®šçš„[waveå’Œtileé‡åŒ–](https://developer.nvidia.com/blog/optimizing-gpu-performance-tensor-cores/)çš„å¯æ•´é™¤æ€§ã€‚



### Activation Checkpointing æˆ– Gradient Checkpointing

Activation Checkpointingå’ŒGradient Checkpointingæ˜¯æŒ‡ç›¸åŒæ–¹æ³•çš„ä¸¤ä¸ªä¸åŒæœ¯è¯­ã€‚è¿™ç¡®å®è®©äººæ„Ÿåˆ°å›°æƒ‘ï¼Œä½†äº‹å®å°±æ˜¯è¿™æ ·ã€‚

Gradient Checkpointingå…è®¸é€šè¿‡ç‰ºç‰²é€Ÿåº¦æ¥æ¢å–GPUå†…å­˜ï¼Œè¿™è¦ä¹ˆä½¿æ‚¨èƒ½å¤Ÿå…‹æœGPUå†…å­˜æº¢å‡ºï¼Œè¦ä¹ˆå¢åŠ æ‰¹é‡å¤§å°æ¥è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚

HF Transformers æ¨¡å‹å¯¹DeepSpeedçš„Activation Checkpointingä¸€æ— æ‰€çŸ¥ï¼Œå› æ­¤å¦‚æœå°è¯•åœ¨DeepSpeedé…ç½®æ–‡ä»¶ä¸­å¯ç”¨è¯¥åŠŸèƒ½ï¼Œä»€ä¹ˆéƒ½ä¸ä¼šå‘ç”Ÿã€‚

å› æ­¤ï¼Œæ‚¨æœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥åˆ©ç”¨è¿™ä¸ªéå¸¸æœ‰ç›Šçš„åŠŸèƒ½ï¼š

1. å¦‚æœæ‚¨æƒ³ä½¿ç”¨ HF Transformers æ¨¡å‹ï¼Œä½ å¯ä»¥ä½¿ç”¨ `model.gradient_checkpointing_enable()` æˆ–åœ¨ HF Trainer ä¸­ä½¿ç”¨ `--gradient_checkpointing`ï¼Œå®ƒä¼šè‡ªåŠ¨ä¸ºæ‚¨å¯ç”¨è¿™ä¸ªåŠŸèƒ½ã€‚åœ¨è¿™é‡Œä½¿ç”¨äº† `torch.utils.checkpoint`ã€‚
2. å¦‚æœæ‚¨ç¼–å†™è‡ªå·±çš„æ¨¡å‹å¹¶å¸Œæœ›ä½¿ç”¨DeepSpeedçš„Activation Checkpointingï¼Œå¯ä»¥ä½¿ç”¨[è§„å®šçš„API](https://deepspeed.readthedocs.io/en/latest/activation-checkpointing.html)ã€‚æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ HF Transformers çš„æ¨¡å‹ä»£ç ï¼Œå°† `torch.utils.checkpoint` æ›¿æ¢ä¸º DeepSpeed çš„APIã€‚åè€…æ›´çµæ´»ï¼Œå› ä¸ºå®ƒå…è®¸æ‚¨å°†å‰å‘æ¿€æ´»å€¼å¸è½½åˆ°CPUå†…å­˜ï¼Œè€Œä¸æ˜¯é‡æ–°è®¡ç®—å®ƒä»¬ã€‚


### Optimizer å’Œ Scheduler

åªè¦ä½ ä¸å¯ç”¨ `offload_optimizer`ï¼Œæ‚¨å¯ä»¥æ··åˆä½¿ç”¨DeepSpeedå’ŒHuggingFaceçš„è°ƒåº¦å™¨å’Œä¼˜åŒ–å™¨ï¼Œä½†æœ‰ä¸€ä¸ªä¾‹å¤–ï¼Œå³ä¸è¦ä½¿ç”¨HuggingFaceè°ƒåº¦å™¨å’ŒDeepSpeedä¼˜åŒ–å™¨çš„ç»„åˆï¼š


| Combos       | HF Scheduler | DS Scheduler |
|:-------------|:-------------|:-------------|
| HF Optimizer | Yes          | Yes          |
| DS Optimizer | No           | Yes          |

åœ¨å¯ç”¨ `offload_optimizer` çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥ä½¿ç”¨éDeepSpeedä¼˜åŒ–å™¨ï¼Œåªè¦è¯¥ä¼˜åŒ–å™¨å…·æœ‰CPUå’ŒGPUçš„å®ç°ï¼ˆé™¤äº†LAMBï¼‰ã€‚

<a id='deepspeed-optimizer'></a>

#### Optimizer

DeepSpeedçš„ä¸»è¦ä¼˜åŒ–å™¨åŒ…æ‹¬Adamã€AdamWã€OneBitAdamå’ŒLambã€‚è¿™äº›ä¼˜åŒ–å™¨å·²ç»ä¸ZeROè¿›è¡Œäº†å½»åº•çš„æµ‹è¯•ï¼Œå› æ­¤å»ºè®®ä½¿ç”¨å®ƒä»¬ã€‚ç„¶è€Œï¼Œä¹Ÿå¯ä»¥å¯¼å…¥`torch`ä¸­çš„å…¶ä»–ä¼˜åŒ–å™¨ã€‚å®Œæ•´çš„æ–‡æ¡£åœ¨[è¿™é‡Œ](https://www.deepspeed.ai/docs/config-json/#optimizer-parameters)ã€‚

å¦‚æœåœ¨é…ç½®æ–‡ä»¶ä¸­ä¸é…ç½®`optimizer`æ¡ç›®ï¼Œ[`Trainer`] å°†è‡ªåŠ¨å°†å…¶è®¾ç½®ä¸º `AdamW`ï¼Œå¹¶ä½¿ç”¨æä¾›çš„å€¼æˆ–ä»¥ä¸‹å‘½ä»¤è¡Œå‚æ•°çš„é»˜è®¤å€¼ï¼š`--learning_rate`ã€`--adam_beta1`ã€`--adam_beta2`ã€`--adam_epsilon` å’Œ `--weight_decay`ã€‚

ä»¥ä¸‹æ˜¯`AdamW` çš„è‡ªåŠ¨é…ç½®ç¤ºä¾‹ï¼š

```json
{
   "optimizer": {
       "type": "AdamW",
       "params": {
         "lr": "auto",
         "betas": "auto",
         "eps": "auto",
         "weight_decay": "auto"
       }
   }
}
```

è¯·æ³¨æ„ï¼Œå‘½ä»¤è¡Œå‚æ•°å°†è®¾ç½®é…ç½®æ–‡ä»¶ä¸­çš„å€¼ã€‚è¿™æ˜¯ä¸ºäº†æœ‰ä¸€ä¸ªæ˜ç¡®çš„å€¼æ¥æºï¼Œå¹¶é¿å…åœ¨ä¸åŒåœ°æ–¹è®¾ç½®å­¦ä¹ ç‡ç­‰å€¼æ—¶éš¾ä»¥æ‰¾åˆ°çš„é”™è¯¯ã€‚å‘½ä»¤è¡Œå‚æ•°é…ç½®é«˜äºå…¶ä»–ã€‚è¢«è¦†ç›–çš„å€¼åŒ…æ‹¬ï¼š

- `lr` çš„å€¼ä¸º `--learning_rate`
- `betas` çš„å€¼ä¸º `--adam_beta1 --adam_beta2`
- `eps` çš„å€¼ä¸º `--adam_epsilon`
- `weight_decay` çš„å€¼ä¸º `--weight_decay`

å› æ­¤ï¼Œè¯·è®°ä½åœ¨å‘½ä»¤è¡Œä¸Šè°ƒæ•´å…±äº«çš„è¶…å‚æ•°ã€‚

æ‚¨ä¹Ÿå¯ä»¥æ˜¾å¼åœ°è®¾ç½®è¿™äº›å€¼ï¼š

```json
{
   "optimizer": {
       "type": "AdamW",
       "params": {
         "lr": 0.001,
         "betas": [0.8, 0.999],
         "eps": 1e-8,
         "weight_decay": 3e-7
       }
   }
}
```

ä½†åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨éœ€è¦è‡ªå·±åŒæ­¥[`Trainer`]å‘½ä»¤è¡Œå‚æ•°å’ŒDeepSpeedé…ç½®ã€‚

å¦‚æœæ‚¨æƒ³ä½¿ç”¨ä¸Šé¢æœªåˆ—å‡ºçš„å…¶ä»–ä¼˜åŒ–å™¨ï¼Œæ‚¨å°†ä¸å¾—ä¸å°†å…¶æ·»åŠ åˆ°é¡¶å±‚é…ç½®ä¸­ã€‚

```json
{
   "zero_allow_untested_optimizer": true
}
```

ç±»ä¼¼äº `AdamW`ï¼Œæ‚¨å¯ä»¥é…ç½®å…¶ä»–å®˜æ–¹æ”¯æŒçš„ä¼˜åŒ–å™¨ã€‚åªæ˜¯è®°ä½è¿™äº›å¯èƒ½æœ‰ä¸åŒçš„é…ç½®å€¼ã€‚ä¾‹å¦‚ï¼Œå¯¹äºAdamï¼Œæ‚¨å¯èƒ½éœ€è¦å°† `weight_decay` è®¾ç½®åœ¨ `0.01` å·¦å³ã€‚

æ­¤å¤–ï¼Œå½“ä¸DeepSpeedçš„CPU Adamä¼˜åŒ–å™¨ä¸€èµ·ä½¿ç”¨æ—¶ï¼Œoffloadçš„æ•ˆæœæœ€å¥½ã€‚å¦‚æœæ‚¨æƒ³åœ¨offloadæ—¶ä½¿ç”¨ä¸åŒçš„ä¼˜åŒ–å™¨ï¼Œè‡ª `deepspeed==0.8.3` èµ·ï¼Œæ‚¨è¿˜éœ€è¦æ·»åŠ ï¼š


```json
{
   "zero_force_ds_cpu_optimizer": false
}
```
åˆ°é¡¶å±‚é…ç½®ä¸­ã€‚



<a id='deepspeed-scheduler'></a>

#### Scheduler

DeepSpeedæ”¯æŒ`LRRangeTest`ã€`OneCycle`ã€`WarmupLR`å’Œ`WarmupDecayLR`å­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚å®Œæ•´æ–‡æ¡£åœ¨[è¿™é‡Œ](https://www.deepspeed.ai/docs/config-json/#scheduler-parameters)ã€‚

ä»¥ä¸‹æ˜¯ğŸ¤— Transformers å’Œ DeepSpeed ä¹‹é—´çš„è°ƒåº¦å™¨é‡å éƒ¨åˆ†ï¼š

- é€šè¿‡ `--lr_scheduler_type constant_with_warmup` å®ç° `WarmupLR`
- é€šè¿‡ `--lr_scheduler_type linear` å®ç° `WarmupDecayLR`ã€‚è¿™ä¹Ÿæ˜¯ `--lr_scheduler_type` çš„é»˜è®¤å€¼ï¼Œå› æ­¤ï¼Œå¦‚æœä¸é…ç½®è°ƒåº¦å™¨ï¼Œè¿™å°†æ˜¯é»˜è®¤é…ç½®çš„è°ƒåº¦å™¨ã€‚

å¦‚æœåœ¨é…ç½®æ–‡ä»¶ä¸­ä¸é…ç½® `scheduler` æ¡ç›®ï¼Œ[`Trainer`] å°†ä½¿ç”¨ `--lr_scheduler_type`ã€`--learning_rate` å’Œ `--warmup_steps` æˆ– `--warmup_ratio` çš„å€¼æ¥é…ç½®å…¶ğŸ¤— Transformers ç‰ˆæœ¬ã€‚

ä»¥ä¸‹æ˜¯ `WarmupLR` çš„è‡ªåŠ¨é…ç½®ç¤ºä¾‹ï¼š

```json
{
   "scheduler": {
         "type": "WarmupLR",
         "params": {
             "warmup_min_lr": "auto",
             "warmup_max_lr": "auto",
             "warmup_num_steps": "auto"
         }
     }
}
```

ç”±äºä½¿ç”¨äº† *"auto"*ï¼Œ[`Trainer`] çš„å‚æ•°å°†åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®æ­£ç¡®çš„å€¼ã€‚è¿™æ˜¯ä¸ºäº†æœ‰ä¸€ä¸ªæ˜ç¡®çš„å€¼æ¥æºï¼Œå¹¶é¿å…åœ¨ä¸åŒåœ°æ–¹è®¾ç½®å­¦ä¹ ç‡ç­‰å€¼æ—¶éš¾ä»¥æ‰¾åˆ°çš„é”™è¯¯ã€‚å‘½ä»¤è¡Œé…ç½®é«˜äºå…¶ä»–ã€‚è¢«è®¾ç½®çš„å€¼åŒ…æ‹¬ï¼š

- `warmup_min_lr` çš„å€¼ä¸º `0`ã€‚
- `warmup_max_lr` çš„å€¼ä¸º `--learning_rate`ã€‚
- `warmup_num_steps` çš„å€¼ä¸º `--warmup_steps`ï¼ˆå¦‚æœæä¾›ï¼‰ã€‚å¦åˆ™ï¼Œå°†ä½¿ç”¨ `--warmup_ratio` ä¹˜ä»¥è®­ç»ƒæ­¥éª¤çš„æ•°é‡ï¼Œå¹¶å››èˆäº”å…¥ã€‚
- `total_num_steps` çš„å€¼ä¸º `--max_steps` æˆ–è€…å¦‚æœæ²¡æœ‰æä¾›ï¼Œå°†åœ¨è¿è¡Œæ—¶æ ¹æ®ç¯å¢ƒã€æ•°æ®é›†çš„å¤§å°å’Œå…¶ä»–å‘½ä»¤è¡Œå‚æ•°ï¼ˆå¯¹äº `WarmupDecayLR` æ¥è¯´éœ€è¦ï¼‰è‡ªåŠ¨æ¨å¯¼ã€‚

å½“ç„¶ï¼Œæ‚¨å¯ä»¥æ¥ç®¡ä»»ä½•æˆ–æ‰€æœ‰çš„é…ç½®å€¼ï¼Œå¹¶è‡ªè¡Œè®¾ç½®è¿™äº›å€¼ï¼š

```json
{
   "scheduler": {
         "type": "WarmupLR",
         "params": {
             "warmup_min_lr": 0,
             "warmup_max_lr": 0.001,
             "warmup_num_steps": 1000
         }
     }
}
```

ä½†åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨éœ€è¦è‡ªå·±åŒæ­¥[`Trainer`]å‘½ä»¤è¡Œå‚æ•°å’ŒDeepSpeedé…ç½®ã€‚

ä¾‹å¦‚ï¼Œå¯¹äº `WarmupDecayLR`ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ¡ç›®ï¼š

```json
{
   "scheduler": {
         "type": "WarmupDecayLR",
         "params": {
             "last_batch_iteration": -1,
             "total_num_steps": "auto",
             "warmup_min_lr": "auto",
             "warmup_max_lr": "auto",
             "warmup_num_steps": "auto"
         }
     }
}
```

ç„¶åï¼Œ`total_num_steps`ã€`warmup_max_lr`ã€`warmup_num_steps` å’Œ `total_num_steps` å°†åœ¨åŠ è½½æ—¶è®¾ç½®ã€‚


<a id='deepspeed-fp32'></a>

### fp32ç²¾åº¦

DeepSpeedæ”¯æŒå®Œæ•´çš„fp32å’Œfp16æ··åˆç²¾åº¦ã€‚

ç”±äºfp16æ··åˆç²¾åº¦å…·æœ‰æ›´å°çš„å†…å­˜éœ€æ±‚å’Œæ›´å¿«çš„é€Ÿåº¦ï¼Œå”¯ä¸€ä¸ä½¿ç”¨å®ƒçš„æ—¶å€™æ˜¯å½“æ‚¨ä½¿ç”¨çš„æ¨¡å‹åœ¨è¿™ç§è®­ç»ƒæ¨¡å¼ä¸‹è¡¨ç°ä¸ä½³æ—¶ã€‚é€šå¸¸ï¼Œå½“æ¨¡å‹æ²¡æœ‰åœ¨fp16æ··åˆç²¾åº¦ä¸‹è¿›è¡Œé¢„è®­ç»ƒæ—¶ï¼ˆä¾‹å¦‚ï¼Œbf16é¢„è®­ç»ƒæ¨¡å‹ç»å¸¸å‡ºç°è¿™ç§æƒ…å†µï¼‰ï¼Œä¼šå‡ºç°è¿™ç§æƒ…å†µã€‚è¿™æ ·çš„æ¨¡å‹å¯èƒ½ä¼šå‘ç”Ÿæº¢å‡ºæˆ–ä¸‹æº¢ï¼Œå¯¼è‡´ `NaN` æŸå¤±ã€‚å¦‚æœæ˜¯è¿™ç§æƒ…å†µï¼Œé‚£ä¹ˆæ‚¨å°†å¸Œæœ›ä½¿ç”¨å®Œæ•´çš„fp32æ¨¡å¼ï¼Œé€šè¿‡æ˜¾å¼ç¦ç”¨é»˜è®¤å¯ç”¨çš„fp16æ··åˆç²¾åº¦æ¨¡å¼ï¼š

```json
{
    "fp16": {
        "enabled": false,
    }
}
```

å¦‚æœæ‚¨ä½¿ç”¨åŸºäºAmpereæ¶æ„çš„GPUï¼ŒPyTorchç‰ˆæœ¬1.7åŠæ›´é«˜ç‰ˆæœ¬å°†è‡ªåŠ¨åˆ‡æ¢åˆ°ä½¿ç”¨æ›´é«˜æ•ˆçš„tf32æ ¼å¼è¿›è¡Œä¸€äº›æ“ä½œï¼Œä½†ç»“æœä»å°†ä»¥fp32æ ¼å¼å‘ˆç°ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯å’ŒåŸºå‡†æµ‹è¯•ï¼Œè¯·å‚è§[TensorFloat-32(TF32) on Ampere devices](https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)ã€‚å¦‚æœå‡ºäºæŸç§åŸå› æ‚¨ä¸å¸Œæœ›ä½¿ç”¨å®ƒï¼Œè¯¥æ–‡æ¡£åŒ…æ‹¬æœ‰å…³å¦‚ä½•ç¦ç”¨æ­¤è‡ªåŠ¨è½¬æ¢çš„è¯´æ˜ã€‚

åœ¨ğŸ¤— Trainerä¸­ï¼Œä½ å¯ä»¥ä½¿ç”¨ `--tf32` æ¥å¯ç”¨å®ƒï¼Œæˆ–ä½¿ç”¨ `--tf32 0` æˆ– `--no_tf32` æ¥ç¦ç”¨å®ƒã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œä½¿ç”¨PyTorchçš„é»˜è®¤è®¾ç½®ã€‚



<a id='deepspeed-amp'></a>

### è‡ªåŠ¨æ··åˆç²¾åº¦

æ‚¨å¯ä»¥ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦ï¼Œå¯ä»¥é€‰æ‹©ä½¿ç”¨ç±»ä¼¼ PyTorch AMP çš„æ–¹å¼ï¼Œä¹Ÿå¯ä»¥é€‰æ‹©ä½¿ç”¨ç±»ä¼¼ Apex çš„æ–¹å¼ï¼š

### fp16

è¦é…ç½®PyTorch AMP-like çš„ fp16ï¼ˆfloat16ï¼‰ æ¨¡å¼ï¼Œè¯·è®¾ç½®ï¼š

```json
{
    "fp16": {
        "enabled": "auto",
        "loss_scale": 0,
        "loss_scale_window": 1000,
        "initial_scale_power": 16,
        "hysteresis": 2,
        "min_loss_scale": 1
    }
}
```

å¹¶ä¸”ï¼Œ[`Trainer`]å°†æ ¹æ®`args.fp16_backend`çš„å€¼è‡ªåŠ¨å¯ç”¨æˆ–ç¦ç”¨å®ƒã€‚å…¶ä½™çš„é…ç½®å€¼ç”±æ‚¨å†³å®šã€‚

å½“ä¼ é€’`--fp16 --fp16_backend amp`æˆ–`--fp16_full_eval`å‘½ä»¤è¡Œå‚æ•°æ—¶ï¼Œæ­¤æ¨¡å¼å°†è¢«å¯ç”¨ã€‚

æ‚¨ä¹Ÿå¯ä»¥æ˜¾å¼åœ°å¯ç”¨/ç¦ç”¨æ­¤æ¨¡å¼ï¼š

```json
{
    "fp16": {
        "enabled": true,
        "loss_scale": 0,
        "loss_scale_window": 1000,
        "initial_scale_power": 16,
        "hysteresis": 2,
        "min_loss_scale": 1
    }
}
```

ä½†æ˜¯ä¹‹åæ‚¨éœ€è¦è‡ªå·±åŒæ­¥[`Trainer`]å‘½ä»¤è¡Œå‚æ•°å’ŒDeepSpeedé…ç½®ã€‚

ä»¥ä¸‹æ˜¯[ç›¸å…³æ–‡æ¡£](https://www.deepspeed.ai/docs/config-json/#fp16-training-options)


### bf16

å¦‚æœéœ€è¦ä½¿ç”¨bfloat16è€Œä¸æ˜¯fp16ï¼Œé‚£ä¹ˆå¯ä»¥ä½¿ç”¨ä»¥ä¸‹é…ç½®éƒ¨åˆ†ï¼š

```json
{
    "bf16": {
        "enabled": "auto"
    }
}
```

bf16å…·æœ‰ä¸fp32ç›¸åŒçš„åŠ¨æ€èŒƒå›´ï¼Œå› æ­¤ä¸éœ€è¦æŸå¤±ç¼©æ”¾ã€‚

å½“ä¼ é€’`--bf16`æˆ–`--bf16_full_eval`å‘½ä»¤è¡Œå‚æ•°æ—¶ï¼Œå¯ç”¨æ­¤æ¨¡å¼ã€‚

æ‚¨è¿˜å¯ä»¥æ˜¾å¼åœ°å¯ç”¨/ç¦ç”¨æ­¤æ¨¡å¼ï¼š

```json
{
    "bf16": {
        "enabled": true
    }
}
```

<Tip>

åœ¨`deepspeed==0.6.0`ç‰ˆæœ¬ä¸­ï¼Œbf16æ”¯æŒæ˜¯æ–°çš„å®éªŒæ€§åŠŸèƒ½ã€‚

å¦‚æœæ‚¨å¯ç”¨äº†bf16æ¥è¿›è¡Œ[æ¢¯åº¦ç´¯ç§¯](#gradient-accumulation)ï¼Œæ‚¨éœ€è¦æ„è¯†åˆ°å®ƒä¼šä»¥bf16ç´¯ç§¯æ¢¯åº¦ï¼Œè¿™å¯èƒ½ä¸æ˜¯æ‚¨æƒ³è¦çš„ï¼Œå› ä¸ºè¿™ç§æ ¼å¼çš„ä½ç²¾åº¦å¯èƒ½ä¼šå¯¼è‡´lossy accumulationã€‚

ä¿®å¤è¿™ä¸ªé—®é¢˜çš„å·¥ä½œæ­£åœ¨åŠªåŠ›è¿›è¡Œï¼ŒåŒæ—¶æä¾›äº†ä½¿ç”¨æ›´é«˜ç²¾åº¦çš„`dtype`ï¼ˆfp16æˆ–fp32ï¼‰çš„é€‰é¡¹ã€‚

</Tip>


### NCCLé›†åˆ

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæœ‰ä¸¤ç§æ•°æ®ç±»å‹ï¼š`dtype`å’Œç”¨äºé€šä¿¡æ”¶é›†æ“ä½œçš„`dtype`ï¼Œå¦‚å„ç§å½’çº¦å’Œæ”¶é›†/åˆ†æ•£æ“ä½œã€‚

æ‰€æœ‰çš„gather/scatteræ“ä½œéƒ½æ˜¯åœ¨æ•°æ®ç›¸åŒçš„`dtype`ä¸­æ‰§è¡Œçš„ï¼Œæ‰€ä»¥å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨bf16çš„è®­ç»ƒæ¨¡å¼ï¼Œé‚£ä¹ˆå®ƒå°†åœ¨bf16ä¸­è¿›è¡Œgatheræ“ä½œ - gatheræ“ä½œæ˜¯éæŸå¤±æ€§çš„ã€‚

å„ç§reduceæ“ä½œå¯èƒ½ä¼šæ˜¯éå¸¸æŸå¤±æ€§çš„ï¼Œä¾‹å¦‚å½“æ¢¯åº¦åœ¨å¤šä¸ªgpuä¸Šå¹³å‡æ—¶ï¼Œå¦‚æœé€šä¿¡æ˜¯åœ¨fp16æˆ–bf16ä¸­è¿›è¡Œçš„ï¼Œé‚£ä¹ˆç»“æœå¯èƒ½æ˜¯æœ‰æŸå¤±æ€§çš„ - å› ä¸ºå½“åœ¨ä¸€ä¸ªä½ç²¾åº¦ä¸­æ·»åŠ å¤šä¸ªæ•°å­—æ—¶ï¼Œç»“æœå¯èƒ½ä¸æ˜¯ç²¾ç¡®çš„ã€‚æ›´ç³Ÿç³•çš„æ˜¯ï¼Œbf16æ¯”fp16å…·æœ‰æ›´ä½çš„ç²¾åº¦ã€‚é€šå¸¸ï¼Œå½“å¹³å‡æ¢¯åº¦æ—¶ï¼ŒæŸå¤±æœ€å°ï¼Œè¿™äº›æ¢¯åº¦é€šå¸¸éå¸¸å°ã€‚å› æ­¤ï¼Œå¯¹äºåŠç²¾åº¦è®­ç»ƒï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œfp16è¢«ç”¨ä½œreductionæ“ä½œçš„é»˜è®¤å€¼ã€‚ä½†æ˜¯ï¼Œæ‚¨å¯ä»¥å®Œå…¨æ§åˆ¶è¿™ä¸ªåŠŸèƒ½ï¼Œå¦‚æœä½ é€‰æ‹©çš„è¯ï¼Œæ‚¨å¯ä»¥æ·»åŠ ä¸€ä¸ªå°çš„å¼€é”€ï¼Œå¹¶ç¡®ä¿reductionså°†ä½¿ç”¨fp32ä½œä¸ºç´¯ç§¯æ•°æ®ç±»å‹ï¼Œåªæœ‰å½“ç»“æœå‡†å¤‡å¥½æ—¶ï¼Œå®ƒæ‰ä¼šé™çº§åˆ°æ‚¨åœ¨è®­ç»ƒä¸­ä½¿ç”¨çš„åŠç²¾åº¦`dtype`ã€‚

è¦è¦†ç›–é»˜è®¤è®¾ç½®ï¼Œæ‚¨åªéœ€æ·»åŠ ä¸€ä¸ªæ–°çš„é…ç½®æ¡ç›®ï¼š

```json
{
    "communication_data_type": "fp32"
}
```

æ ¹æ®è¿™ä¸ªä¿¡æ¯ï¼Œæœ‰æ•ˆçš„å€¼åŒ…æ‹¬"fp16"ã€"bfp16"å’Œ"fp32"ã€‚

æ³¨æ„ï¼šåœ¨stage zero 3ä¸­ï¼Œbf16é€šä¿¡æ•°æ®ç±»å‹å­˜åœ¨ä¸€ä¸ªbugï¼Œè¯¥é—®é¢˜å·²åœ¨`deepspeed==0.8.1`ç‰ˆæœ¬ä¸­å¾—åˆ°ä¿®å¤ã€‚


### apex

é…ç½®apex AMP-likeæ¨¡å¼ï¼š

```json
"amp": {
    "enabled": "auto",
    "opt_level": "auto"
}
```

å¹¶ä¸”ï¼Œ[`Trainer`]å°†æ ¹æ®`args.fp16_backend`å’Œ`args.fp16_opt_level`çš„å€¼è‡ªåŠ¨é…ç½®å®ƒã€‚

å½“ä¼ é€’`--fp16 --fp16_backend apex --fp16_opt_level 01`å‘½ä»¤è¡Œå‚æ•°æ—¶ï¼Œæ­¤æ¨¡å¼å°†è¢«å¯ç”¨ã€‚

æ‚¨è¿˜å¯ä»¥æ˜¾å¼é…ç½®æ­¤æ¨¡å¼ï¼š

```json
{
    "amp": {
        "enabled": true,
        "opt_level": "O1"
    }
}
```

ä½†æ˜¯ï¼Œæ‚¨éœ€è¦è‡ªå·±åŒæ­¥[`Trainer`]å‘½ä»¤è¡Œå‚æ•°å’ŒDeepSpeedé…ç½®ã€‚

è¿™é‡Œæ˜¯[æ–‡æ¡£](https://www.deepspeed.ai/docs/config-json/#automatic-mixed-precision-amp-training-options)


<a id='deepspeed-bs'></a>

### Batch Size

é…ç½®batch sizeå¯ä»¥ä½¿ç”¨å¦‚ä¸‹å‚æ•°:

```json
{
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto"
}
```

å¹¶ä¸”ï¼Œ[`Trainer`]å°†è‡ªåŠ¨å°†`train_micro_batch_size_per_gpu`è®¾ç½®ä¸º`args.per_device_train_batch_size`çš„å€¼ï¼Œå¹¶å°†`train_batch_size`è®¾ç½®ä¸º`args.world_size * args.per_device_train_batch_size * args.gradient_accumulation_steps`ã€‚

æ‚¨ä¹Ÿå¯ä»¥æ˜¾å¼è®¾ç½®è¿™äº›å€¼ï¼š

```json
{
    "train_batch_size": 12,
    "train_micro_batch_size_per_gpu": 4
}
```

ä½†æ˜¯ï¼Œæ‚¨éœ€è¦è‡ªå·±åŒæ­¥[`Trainer`]å‘½ä»¤è¡Œå‚æ•°å’ŒDeepSpeedé…ç½®ã€‚


<a id='deepspeed-grad-acc'></a>

### Gradient Accumulation

é…ç½®gradient accumulationè®¾ç½®å¦‚ä¸‹:

```json
{
    "gradient_accumulation_steps": "auto"
}
```

å¹¶ä¸”ï¼Œ[`Trainer`]å°†è‡ªåŠ¨å°†å…¶è®¾ç½®ä¸º`args.gradient_accumulation_steps`çš„å€¼ã€‚

æ‚¨ä¹Ÿå¯ä»¥æ˜¾å¼è®¾ç½®è¿™ä¸ªå€¼ï¼š

```json
{
    "gradient_accumulation_steps": 3
}
```

ä½†æ˜¯ï¼Œæ‚¨éœ€è¦è‡ªå·±åŒæ­¥[`Trainer`]å‘½ä»¤è¡Œå‚æ•°å’ŒDeepSpeedé…ç½®ã€‚


<a id='deepspeed-grad-clip'></a>

### Gradient Clipping

é…ç½®gradient clippingå¦‚ä¸‹:

```json
{
    "gradient_clipping": "auto"
}
```

å¹¶ä¸”ï¼Œ[`Trainer`]å°†è‡ªåŠ¨å°†å…¶è®¾ç½®ä¸º`args.max_grad_norm`çš„å€¼ã€‚

æ‚¨ä¹Ÿå¯ä»¥æ˜¾å¼è®¾ç½®è¿™ä¸ªå€¼ï¼š

```json
{
    "gradient_clipping": 1.0
}
```

ä½†æ˜¯ï¼Œæ‚¨éœ€è¦è‡ªå·±åŒæ­¥[`Trainer`]å‘½ä»¤è¡Œå‚æ•°å’ŒDeepSpeedé…ç½®ã€‚



<a id='deepspeed-weight-extraction'></a>

### è·å–æ¨¡å‹æƒé‡

åªè¦æ‚¨ç»§ç»­ä½¿ç”¨DeepSpeedè¿›è¡Œè®­ç»ƒå’Œæ¢å¤ï¼Œæ‚¨å°±ä¸éœ€è¦æ‹…å¿ƒä»»ä½•äº‹æƒ…ã€‚DeepSpeedåœ¨å…¶è‡ªå®šä¹‰æ£€æŸ¥ç‚¹ä¼˜åŒ–å™¨æ–‡ä»¶ä¸­å­˜å‚¨fp32ä¸»æƒé‡ï¼Œè¿™äº›æ–‡ä»¶æ˜¯`global_step*/*optim_states.pt`ï¼ˆè¿™æ˜¯globæ¨¡å¼ï¼‰ï¼Œå¹¶ä¿å­˜åœ¨æ­£å¸¸çš„checkpointä¸‹ã€‚

**FP16æƒé‡ï¼š**

å½“æ¨¡å‹ä¿å­˜åœ¨ZeRO-2ä¸‹æ—¶ï¼Œæ‚¨æœ€ç»ˆä¼šå¾—åˆ°ä¸€ä¸ªåŒ…å«æ¨¡å‹æƒé‡çš„æ™®é€š`pytorch_model.bin`æ–‡ä»¶ï¼Œä½†å®ƒä»¬åªæ˜¯æƒé‡çš„fp16ç‰ˆæœ¬ã€‚

åœ¨ZeRO-3ä¸‹ï¼Œäº‹æƒ…è¦å¤æ‚å¾—å¤šï¼Œå› ä¸ºæ¨¡å‹æƒé‡åˆ†å¸ƒåœ¨å¤šä¸ªGPUä¸Šï¼Œå› æ­¤éœ€è¦`"stage3_gather_16bit_weights_on_model_save": true`æ‰èƒ½è®©`Trainer`ä¿å­˜fp16ç‰ˆæœ¬çš„æƒé‡ã€‚å¦‚æœè¿™ä¸ªè®¾ç½®æ˜¯`False`ï¼Œ`pytorch_model.bin`å°†ä¸ä¼šè¢«åˆ›å»ºã€‚è¿™æ˜¯å› ä¸ºé»˜è®¤æƒ…å†µä¸‹ï¼ŒDeepSpeedçš„`state_dict`åŒ…å«ä¸€ä¸ªå ä½ç¬¦è€Œä¸æ˜¯å®é™…çš„æƒé‡ã€‚å¦‚æœæˆ‘ä»¬ä¿å­˜è¿™ä¸ª`state_dict`ï¼Œå°±æ— æ³•å†åŠ è½½å®ƒäº†ã€‚


```json
{
    "zero_optimization": {
        "stage3_gather_16bit_weights_on_model_save": true
    }
}
```

**FP32æƒé‡ï¼š**

è™½ç„¶fp16æƒé‡é€‚åˆæ¢å¤è®­ç»ƒï¼Œä½†å¦‚æœæ‚¨å®Œæˆäº†æ¨¡å‹çš„å¾®è°ƒå¹¶å¸Œæœ›å°†å…¶ä¸Šä¼ åˆ°[models hub](https://huggingface.co/models)æˆ–ä¼ é€’ç»™å…¶ä»–äººï¼Œæ‚¨å¾ˆå¯èƒ½æƒ³è¦è·å–fp32æƒé‡ã€‚è¿™æœ€å¥½ä¸è¦åœ¨è®­ç»ƒæœŸé—´å®Œæˆï¼Œå› ä¸ºè¿™éœ€è¦å¤§é‡å†…å­˜ï¼Œå› æ­¤æœ€å¥½åœ¨è®­ç»ƒå®Œæˆåç¦»çº¿è¿›è¡Œã€‚ä½†æ˜¯ï¼Œå¦‚æœéœ€è¦å¹¶ä¸”æœ‰å……è¶³çš„ç©ºé—²CPUå†…å­˜ï¼Œå¯ä»¥åœ¨ç›¸åŒçš„è®­ç»ƒè„šæœ¬ä¸­å®Œæˆã€‚ä»¥ä¸‹éƒ¨åˆ†å°†è®¨è®ºè¿™ä¸¤ç§æ–¹æ³•ã€‚

**å®æ—¶FP32æƒé‡æ¢å¤ï¼š**

å¦‚æœæ‚¨çš„æ¨¡å‹å¾ˆå¤§ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒç»“æŸæ—¶å‡ ä¹æ²¡æœ‰å‰©ä½™çš„ç©ºé—²CPUå†…å­˜ï¼Œè¿™ç§æ–¹æ³•å¯èƒ½ä¸èµ·ä½œç”¨ã€‚

å¦‚æœæ‚¨è‡³å°‘ä¿å­˜äº†ä¸€ä¸ªæ£€æŸ¥ç‚¹ï¼Œå¹¶ä¸”æƒ³è¦ä½¿ç”¨æœ€æ–°çš„ä¸€ä¸ªï¼Œå¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š

```python
from transformers.trainer_utils import get_last_checkpoint
from deepspeed.utils.zero_to_fp32 import load_state_dict_from_zero_checkpoint

checkpoint_dir = get_last_checkpoint(trainer.args.output_dir)
fp32_model = load_state_dict_from_zero_checkpoint(trainer.model, checkpoint_dir)
```

å¦‚æœæ‚¨åœ¨ä½¿ç”¨`--load_best_model_at_end`ç±»ï¼š*~transformers.TrainingArguments*å‚æ•°ï¼ˆç”¨äºè·Ÿè¸ªæœ€ä½³
æ£€æŸ¥ç‚¹ï¼‰ï¼Œé‚£ä¹ˆä½ å¯ä»¥é¦–å…ˆæ˜¾å¼åœ°ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼Œç„¶åå†æ‰§è¡Œç›¸åŒçš„æ“ä½œï¼š

```python
from deepspeed.utils.zero_to_fp32 import load_state_dict_from_zero_checkpoint

checkpoint_dir = os.path.join(trainer.args.output_dir, "checkpoint-final")
trainer.deepspeed.save_checkpoint(checkpoint_dir)
fp32_model = load_state_dict_from_zero_checkpoint(trainer.model, checkpoint_dir)
```

<Tip>

æ³¨æ„ï¼Œä¸€æ—¦è¿è¡Œäº†`load_state_dict_from_zero_checkpoint`ï¼Œè¯¥æ¨¡å‹å°†ä¸å†å¯ä»¥åœ¨ç›¸åŒçš„åº”ç”¨ç¨‹åºçš„DeepSpeedä¸Šä¸‹æ–‡ä¸­ä½¿ç”¨ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæ‚¨éœ€è¦é‡æ–°åˆå§‹åŒ–deepspeedå¼•æ“ï¼Œå› ä¸º`model.load_state_dict(state_dict)`ä¼šä»å…¶ä¸­ç§»é™¤æ‰€æœ‰çš„DeepSpeedç›¸å…³ç‚¹ã€‚æ‰€ä»¥æ‚¨åªèƒ½è®­ç»ƒç»“æŸæ—¶è¿™æ ·åšã€‚

</Tip>

å½“ç„¶ï¼Œæ‚¨ä¸å¿…ä½¿ç”¨ç±»ï¼š*~transformers.Trainer*ï¼Œæ‚¨å¯ä»¥æ ¹æ®ä½ çš„éœ€æ±‚è°ƒæ•´ä¸Šé¢çš„ç¤ºä¾‹ã€‚

å¦‚æœæ‚¨å‡ºäºæŸç§åŸå› æƒ³è¦æ›´å¤šçš„ä¼˜åŒ–ï¼Œæ‚¨ä¹Ÿå¯ä»¥æå–æƒé‡çš„fp32 `state_dict`å¹¶æŒ‰ç…§ä»¥ä¸‹ç¤ºä¾‹è¿›è¡Œæ“ä½œï¼š

```python
from deepspeed.utils.zero_to_fp32 import get_fp32_state_dict_from_zero_checkpoint

state_dict = get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir)  # already on cpu
model = model.cpu()
model.load_state_dict(state_dict)
```

**ç¦»çº¿FP32æƒé‡æ¢å¤ï¼š**

DeepSpeedä¼šåˆ›å»ºä¸€ä¸ªç‰¹æ®Šçš„è½¬æ¢è„šæœ¬`zero_to_fp32.py`ï¼Œå¹¶å°†å…¶æ”¾ç½®åœ¨checkpointæ–‡ä»¶å¤¹çš„é¡¶å±‚ã€‚ä½¿ç”¨æ­¤è„šæœ¬ï¼Œæ‚¨å¯ä»¥åœ¨ä»»ä½•æ—¶å€™æå–æƒé‡ã€‚è¯¥è„šæœ¬æ˜¯ç‹¬ç«‹çš„ï¼Œæ‚¨ä¸å†éœ€è¦é…ç½®æ–‡ä»¶æˆ–`Trainer`æ¥æ‰§è¡Œæå–æ“ä½œã€‚

å‡è®¾æ‚¨çš„checkpointæ–‡ä»¶å¤¹å¦‚ä¸‹æ‰€ç¤ºï¼š

```bash
$ ls -l output_dir/checkpoint-1/
-rw-rw-r-- 1 stas stas 1.4K Mar 27 20:42 config.json
drwxrwxr-x 2 stas stas 4.0K Mar 25 19:52 global_step1/
-rw-rw-r-- 1 stas stas   12 Mar 27 13:16 latest
-rw-rw-r-- 1 stas stas 827K Mar 27 20:42 optimizer.pt
-rw-rw-r-- 1 stas stas 231M Mar 27 20:42 pytorch_model.bin
-rw-rw-r-- 1 stas stas  623 Mar 27 20:42 scheduler.pt
-rw-rw-r-- 1 stas stas 1.8K Mar 27 20:42 special_tokens_map.json
-rw-rw-r-- 1 stas stas 774K Mar 27 20:42 spiece.model
-rw-rw-r-- 1 stas stas 1.9K Mar 27 20:42 tokenizer_config.json
-rw-rw-r-- 1 stas stas  339 Mar 27 20:42 trainer_state.json
-rw-rw-r-- 1 stas stas 2.3K Mar 27 20:42 training_args.bin
-rwxrw-r-- 1 stas stas 5.5K Mar 27 13:16 zero_to_fp32.py*
```

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œåªæœ‰ä¸€ä¸ªDeepSpeedæ£€æŸ¥ç‚¹å­æ–‡ä»¶å¤¹*global_step1*ã€‚å› æ­¤ï¼Œè¦é‡æ„fp32æƒé‡ï¼Œåªéœ€è¿è¡Œï¼š

```bash
python zero_to_fp32.py . pytorch_model.bin
```

è¿™å°±æ˜¯å®ƒã€‚`pytorch_model.bin`ç°åœ¨å°†åŒ…å«ä»å¤šä¸ªGPUsåˆå¹¶çš„å®Œæ•´çš„fp32æ¨¡å‹æƒé‡ã€‚

è¯¥è„šæœ¬å°†è‡ªåŠ¨èƒ½å¤Ÿå¤„ç†ZeRO-2æˆ–ZeRO-3 checkpointã€‚

`python zero_to_fp32.py -h`å°†ä¸ºæ‚¨æä¾›ä½¿ç”¨ç»†èŠ‚ã€‚

è¯¥è„šæœ¬å°†é€šè¿‡æ–‡ä»¶`latest`çš„å†…å®¹è‡ªåŠ¨å‘ç°deepspeedå­æ–‡ä»¶å¤¹ï¼Œåœ¨å½“å‰ç¤ºä¾‹ä¸­ï¼Œå®ƒå°†åŒ…å«`global_step1`ã€‚

æ³¨æ„ï¼šç›®å‰è¯¥è„šæœ¬éœ€è¦2å€äºæœ€ç»ˆfp32æ¨¡å‹æƒé‡çš„é€šç”¨å†…å­˜ã€‚


### ZeRO-3 å’Œ Infinity Nuances

ZeRO-3ä¸ZeRO-2æœ‰å¾ˆå¤§çš„ä¸åŒï¼Œä¸»è¦æ˜¯å› ä¸ºå®ƒçš„å‚æ•°åˆ†ç‰‡åŠŸèƒ½ã€‚

ZeRO-Infinityè¿›ä¸€æ­¥æ‰©å±•äº†ZeRO-3ï¼Œä»¥æ”¯æŒNVMeå†…å­˜å’Œå…¶ä»–é€Ÿåº¦å’Œå¯æ‰©å±•æ€§æ”¹è¿›ã€‚

å°½ç®¡æ‰€æœ‰åŠªåŠ›éƒ½æ˜¯ä¸ºäº†åœ¨ä¸éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œä»»ä½•ç‰¹æ®Šæ›´æ”¹çš„æƒ…å†µä¸‹å°±èƒ½æ­£å¸¸è¿è¡Œï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ‚¨å¯èƒ½éœ€è¦ä»¥ä¸‹ä¿¡æ¯ã€‚


#### æ„å»ºå¤§æ¨¡å‹

DeepSpeed/ZeRO-3å¯ä»¥å¤„ç†å‚æ•°é‡è¾¾åˆ°æ•°ä¸‡äº¿çš„æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹å¯èƒ½æ— æ³•é€‚åº”ç°æœ‰çš„å†…å­˜ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¦‚æœæ‚¨è¿˜æ˜¯å¸Œæœ›åˆå§‹åŒ–æ›´å¿«åœ°å‘ç”Ÿï¼Œå¯ä»¥ä½¿ç”¨*deepspeed.zero.Init()*ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆä¹Ÿæ˜¯ä¸€ä¸ªå‡½æ•°è£…é¥°å™¨ï¼‰æ¥åˆå§‹åŒ–æ¨¡å‹ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
from transformers import T5ForConditionalGeneration, T5Config
import deepspeed

with deepspeed.zero.Init():
    config = T5Config.from_pretrained("google-t5/t5-small")
    model = T5ForConditionalGeneration(config)
```

å¦‚æ‚¨æ‰€è§ï¼Œè¿™ä¼šä¸ºæ‚¨éšæœºåˆå§‹åŒ–ä¸€ä¸ªæ¨¡å‹ã€‚

å¦‚æœæ‚¨æƒ³ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œ`model_class.from_pretrained`å°†åœ¨`is_deepspeed_zero3_enabled()`è¿”å›`True`çš„æƒ…å†µä¸‹æ¿€æ´»æ­¤åŠŸèƒ½ï¼Œç›®å‰è¿™æ˜¯é€šè¿‡ä¼ é€’çš„DeepSpeedé…ç½®æ–‡ä»¶ä¸­çš„ZeRO-3é…ç½®éƒ¨åˆ†è®¾ç½®çš„ã€‚å› æ­¤ï¼Œåœ¨è°ƒç”¨`from_pretrained`ä¹‹å‰ï¼Œæ‚¨å¿…é¡»åˆ›å»º**TrainingArguments**å¯¹è±¡ã€‚ä»¥ä¸‹æ˜¯å¯èƒ½çš„é¡ºåºç¤ºä¾‹ï¼š

```python
from transformers import AutoModel, Trainer, TrainingArguments

training_args = TrainingArguments(..., deepspeed=ds_config)
model = AutoModel.from_pretrained("google-t5/t5-small")
trainer = Trainer(model=model, args=training_args, ...)
```

å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯å®˜æ–¹ç¤ºä¾‹è„šæœ¬ï¼Œå¹¶ä¸”å‘½ä»¤è¡Œå‚æ•°ä¸­åŒ…å«`--deepspeed ds_config.json`ä¸”å¯ç”¨äº†ZeRO-3é…ç½®ï¼Œé‚£ä¹ˆä¸€åˆ‡éƒ½å·²ç»ä¸ºæ‚¨å‡†å¤‡å¥½äº†ï¼Œå› ä¸ºè¿™æ˜¯ç¤ºä¾‹è„šæœ¬çš„ç¼–å†™æ–¹å¼ã€‚

æ³¨æ„ï¼šå¦‚æœæ¨¡å‹çš„fp16æƒé‡æ— æ³•é€‚åº”å•ä¸ªGPUçš„å†…å­˜ï¼Œåˆ™å¿…é¡»ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚

æœ‰å…³æ­¤æ–¹æ³•å’Œå…¶ä»–ç›¸å…³åŠŸèƒ½çš„å®Œæ•´è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[æ„å»ºå¤§æ¨¡å‹](https://deepspeed.readthedocs.io/en/latest/zero3.html#constructing-massive-models)ã€‚

æ­¤å¤–ï¼Œåœ¨åŠ è½½fp16é¢„è®­ç»ƒæ¨¡å‹æ—¶ï¼Œæ‚¨å¸Œæœ›`from_pretrained`ä½¿ç”¨`dtype=torch.float16`ã€‚è¯¦æƒ…è¯·å‚è§[from_pretrained-torch-dtype](#from_pretrained-torch-dtype)ã€‚


#### å‚æ•°æ”¶é›†

åœ¨å¤šä¸ªGPUä¸Šä½¿ç”¨ZeRO-3æ—¶ï¼Œæ²¡æœ‰ä¸€ä¸ªGPUæ‹¥æœ‰æ‰€æœ‰å‚æ•°ï¼Œé™¤éå®ƒæ˜¯å½“å‰æ‰§è¡Œå±‚çš„å‚æ•°ã€‚å› æ­¤ï¼Œå¦‚æœæ‚¨éœ€è¦ä¸€æ¬¡è®¿é—®æ‰€æœ‰å±‚çš„æ‰€æœ‰å‚æ•°ï¼Œæœ‰ä¸€ä¸ªç‰¹å®šçš„æ–¹æ³•å¯ä»¥å®ç°ã€‚
æ‚¨å¯èƒ½ä¸éœ€è¦å®ƒï¼Œä½†å¦‚æœæ‚¨éœ€è¦ï¼Œè¯·å‚è€ƒ[å‚æ•°æ”¶é›†](https://deepspeed.readthedocs.io/en/latest/zero3.html#manual-parameter-coordination)ã€‚

ç„¶è€Œï¼Œæˆ‘ä»¬åœ¨å¤šä¸ªåœ°æ–¹ç¡®å®ä½¿ç”¨äº†å®ƒï¼Œå…¶ä¸­ä¸€ä¸ªä¾‹å­æ˜¯åœ¨`from_pretrained`ä¸­åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡ã€‚æˆ‘ä»¬ä¸€æ¬¡åŠ è½½ä¸€å±‚ï¼Œç„¶åç«‹å³å°†å…¶åˆ†åŒºåˆ°æ‰€æœ‰å‚ä¸çš„GPUä¸Šï¼Œå› ä¸ºå¯¹äºéå¸¸å¤§çš„æ¨¡å‹ï¼Œæ— æ³•åœ¨ä¸€ä¸ªGPUä¸Šä¸€æ¬¡æ€§åŠ è½½å¹¶å°†å…¶åˆ†å¸ƒåˆ°å¤šä¸ªGPUä¸Šï¼Œå› ä¸ºå†…å­˜é™åˆ¶ã€‚

æ­¤å¤–ï¼Œåœ¨ZeRO-3ä¸‹ï¼Œå¦‚æœæ‚¨ç¼–å†™è‡ªå·±çš„ä»£ç å¹¶é‡åˆ°çœ‹èµ·æ¥åƒè¿™æ ·çš„æ¨¡å‹å‚æ•°æƒé‡ï¼š

```python
tensor([1.0], device="cuda:0", dtype=torch.float16, requires_grad=True)
```

å¼ºè°ƒ`tensor([1.])`ï¼Œæˆ–è€…å¦‚æœæ‚¨é‡åˆ°ä¸€ä¸ªé”™è¯¯ï¼Œå®ƒè¯´å‚æ•°çš„å¤§å°æ˜¯`1`ï¼Œè€Œä¸æ˜¯æŸä¸ªæ›´å¤§çš„å¤šç»´å½¢çŠ¶ï¼Œè¿™æ„å‘³ç€å‚æ•°è¢«åˆ’åˆ†äº†ï¼Œä½ çœ‹åˆ°çš„æ˜¯ä¸€ä¸ªZeRO-3å ä½ç¬¦ã€‚



<a id='deepspeed-zero-inference'></a>


### ZeRO æ¨ç†

"ZeRO æ¨æ–­" ä½¿ç”¨ä¸ "ZeRO-3 è®­ç»ƒ" ç›¸åŒçš„é…ç½®ã€‚æ‚¨åªéœ€è¦å»æ‰ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨éƒ¨åˆ†ã€‚å®é™…ä¸Šï¼Œå¦‚æœæ‚¨å¸Œæœ›ä¸è®­ç»ƒå…±äº«ç›¸åŒçš„é…ç½®æ–‡ä»¶ï¼Œæ‚¨å¯ä»¥å°†å®ƒä»¬ä¿ç•™åœ¨é…ç½®æ–‡ä»¶ä¸­ï¼Œå®ƒä»¬åªä¼šè¢«å¿½ç•¥ã€‚

æ‚¨åªéœ€è¦ä¼ é€’é€šå¸¸çš„[`TrainingArguments`]å‚æ•°ã€‚ä¾‹å¦‚ï¼š

```bash
deepspeed --num_gpus=2 your_program.py <normal cl args> --do_eval --deepspeed ds_config.json
```

å”¯ä¸€çš„é‡è¦äº‹æƒ…æ˜¯æ‚¨éœ€è¦ä½¿ç”¨ZeRO-3é…ç½®ï¼Œå› ä¸ºZeRO-2å¯¹äºæ¨ç†æ²¡æœ‰ä»»ä½•ä¼˜åŠ¿ï¼Œå› ä¸ºåªæœ‰ZeRO-3æ‰å¯¹å‚æ•°è¿›è¡Œåˆ†ç‰‡ï¼Œè€ŒZeRO-1åˆ™å¯¹æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€è¿›è¡Œåˆ†ç‰‡ã€‚

ä»¥ä¸‹æ˜¯åœ¨DeepSpeedä¸‹è¿è¡Œ`run_translation.py`å¯ç”¨æ‰€æœ‰å¯ç”¨GPUçš„ç¤ºä¾‹ï¼š

```bash
deepspeed examples/pytorch/translation/run_translation.py \
--deepspeed tests/deepspeed/ds_config_zero3.json \
--model_name_or_path google-t5/t5-small --output_dir output_dir \
--do_eval --max_eval_samples 50 --warmup_steps 50  \
--max_source_length 128 --val_max_target_length 128 \
--overwrite_output_dir --per_device_eval_batch_size 4 \
--predict_with_generate --dataset_config "ro-en" --fp16 \
--source_lang en --target_lang ro --dataset_name wmt16 \
--source_prefix "translate English to Romanian: "
```

ç”±äºåœ¨æ¨ç†é˜¶æ®µï¼Œä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦ä¸éœ€è¦é¢å¤–çš„å¤§é‡å†…å­˜ï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿå°†æ›´å¤§çš„æ‰¹æ¬¡å’Œ/æˆ–åºåˆ—é•¿åº¦æ”¾åˆ°ç›¸åŒçš„ç¡¬ä»¶ä¸Šã€‚

æ­¤å¤–ï¼ŒDeepSpeedç›®å‰æ­£åœ¨å¼€å‘ä¸€ä¸ªåä¸ºDeepspeed-Inferenceçš„ç›¸å…³äº§å“ï¼Œå®ƒä¸ZeROæŠ€æœ¯æ— å…³ï¼Œè€Œæ˜¯ä½¿ç”¨å¼ é‡å¹¶è¡Œæ¥æ‰©å±•æ— æ³•é€‚åº”å•ä¸ªGPUçš„æ¨¡å‹ã€‚è¿™æ˜¯ä¸€ä¸ªæ­£åœ¨è¿›è¡Œçš„å·¥ä½œï¼Œä¸€æ—¦è¯¥äº§å“å®Œæˆï¼Œæˆ‘ä»¬å°†æä¾›é›†æˆã€‚


### å†…å­˜è¦æ±‚

ç”±äº DeepSpeed ZeRO å¯ä»¥å°†å†…å­˜å¸è½½åˆ° CPUï¼ˆå’Œ NVMeï¼‰ï¼Œè¯¥æ¡†æ¶æä¾›äº†ä¸€äº›å·¥å…·ï¼Œå…è®¸æ ¹æ®ä½¿ç”¨çš„ GPU æ•°é‡å‘ŠçŸ¥å°†éœ€è¦å¤šå°‘ CPU å’Œ GPU å†…å­˜ã€‚

è®©æˆ‘ä»¬ä¼°è®¡åœ¨å•ä¸ªGPUä¸Šå¾®è°ƒ"bigscience/T0_3B"æ‰€éœ€çš„å†…å­˜ï¼š

```bash
$ python -c 'from transformers import AutoModel; \
from deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live; \
model = AutoModel.from_pretrained("bigscience/T0_3B"); \
estimate_zero3_model_states_mem_needs_all_live(model, num_gpus_per_node=1, num_nodes=1)'
[...]
Estimated memory needed for params, optim states and gradients for a:
HW: Setup with 1 node, 1 GPU per node.
SW: Model with 2783M total params, 65M largest layer params.
  per CPU  |  per GPU |   Options
   70.00GB |   0.25GB | offload_param=cpu , offload_optimizer=cpu , zero_init=1
   70.00GB |   0.25GB | offload_param=cpu , offload_optimizer=cpu , zero_init=0
   62.23GB |   5.43GB | offload_param=none, offload_optimizer=cpu , zero_init=1
   62.23GB |   5.43GB | offload_param=none, offload_optimizer=cpu , zero_init=0
    0.37GB |  46.91GB | offload_param=none, offload_optimizer=none, zero_init=1
   15.56GB |  46.91GB | offload_param=none, offload_optimizer=none, zero_init=0
```

å› æ­¤ï¼Œæ‚¨å¯ä»¥å°†æ¨¡å‹æ‹Ÿåˆåœ¨å•ä¸ª80GBçš„GPUä¸Šï¼Œä¸è¿›è¡ŒCPU offloadï¼Œæˆ–è€…ä½¿ç”¨å¾®å°çš„8GB GPUï¼Œä½†éœ€è¦çº¦60GBçš„CPUå†…å­˜ã€‚ï¼ˆè¯·æ³¨æ„ï¼Œè¿™ä»…æ˜¯å‚æ•°ã€ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦æ‰€éœ€çš„å†…å­˜ - æ‚¨è¿˜éœ€è¦ä¸ºCUDAå†…æ ¸ã€æ¿€æ´»å€¼å’Œä¸´æ—¶å˜é‡åˆ†é…æ›´å¤šçš„å†…å­˜ã€‚ï¼‰

ç„¶åï¼Œè¿™æ˜¯æˆæœ¬ä¸é€Ÿåº¦çš„æƒè¡¡ã€‚è´­ä¹°/ç§Ÿç”¨è¾ƒå°çš„ GPUï¼ˆæˆ–è¾ƒå°‘çš„ GPUï¼Œå› ä¸ºæ‚¨å¯ä»¥ä½¿ç”¨å¤šä¸ª GPU è¿›è¡Œ Deepspeed ZeROï¼‰ã€‚ä½†è¿™æ ·ä¼šæ›´æ…¢ï¼Œå› æ­¤å³ä½¿æ‚¨ä¸å…³å¿ƒå®ŒæˆæŸé¡¹ä»»åŠ¡çš„é€Ÿåº¦ï¼Œå‡é€Ÿä¹Ÿç›´æ¥å½±å“ GPU ä½¿ç”¨çš„æŒç»­æ—¶é—´ï¼Œä»è€Œå¯¼è‡´æ›´å¤§çš„æˆæœ¬ã€‚å› æ­¤ï¼Œè¯·è¿›è¡Œå®éªŒå¹¶æ¯”è¾ƒå“ªç§æ–¹æ³•æ•ˆæœæœ€å¥½ã€‚

å¦‚æœæ‚¨æœ‰è¶³å¤Ÿçš„GPUå†…å­˜ï¼Œè¯·ç¡®ä¿ç¦ç”¨CPU/NVMeå¸è½½ï¼Œå› ä¸ºè¿™ä¼šä½¿æ‰€æœ‰æ“ä½œæ›´å¿«ã€‚

ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬é‡å¤ç›¸åŒçš„æ“ä½œï¼Œä½¿ç”¨2ä¸ªGPUï¼š

```bash
$ python -c 'from transformers import AutoModel; \
from deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live; \
model = AutoModel.from_pretrained("bigscience/T0_3B"); \
estimate_zero3_model_states_mem_needs_all_live(model, num_gpus_per_node=2, num_nodes=1)'
[...]
Estimated memory needed for params, optim states and gradients for a:
HW: Setup with 1 node, 2 GPUs per node.
SW: Model with 2783M total params, 65M largest layer params.
  per CPU  |  per GPU |   Options
   70.00GB |   0.25GB | offload_param=cpu , offload_optimizer=cpu , zero_init=1
   70.00GB |   0.25GB | offload_param=cpu , offload_optimizer=cpu , zero_init=0
   62.23GB |   2.84GB | offload_param=none, offload_optimizer=cpu , zero_init=1
   62.23GB |   2.84GB | offload_param=none, offload_optimizer=cpu , zero_init=0
    0.74GB |  23.58GB | offload_param=none, offload_optimizer=none, zero_init=1
   31.11GB |  23.58GB | offload_param=none, offload_optimizer=none, zero_init=0

```

æ‰€ä»¥ï¼Œæ‚¨éœ€è¦2ä¸ª32GBæˆ–æ›´é«˜çš„GPUï¼Œä¸”ä¸è¿›è¡ŒCPUå¸è½½ã€‚

å¦‚éœ€äº†è§£æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[å†…å­˜ä¼°ç®—å™¨](https://deepspeed.readthedocs.io/en/latest/memory.html)ã€‚



### å½’æ¡£Issues

è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æäº¤é—®é¢˜ï¼Œä»¥ä¾¿æˆ‘ä»¬èƒ½å¤Ÿè¿…é€Ÿæ‰¾åˆ°é—®é¢˜å¹¶å¸®åŠ©æ‚¨è§£é™¤å·¥ä½œé˜»å¡ã€‚

åœ¨æ‚¨çš„æŠ¥å‘Šä¸­ï¼Œè¯·å§‹ç»ˆåŒ…æ‹¬ä»¥ä¸‹å†…å®¹ï¼š

1. å®Œæ•´çš„Deepspeedé…ç½®æ–‡ä»¶
2. å¦‚æœä½¿ç”¨äº†[`Trainer`]ï¼Œåˆ™åŒ…æ‹¬å‘½ä»¤è¡Œå‚æ•°ï¼›å¦‚æœè‡ªå·±ç¼–å†™äº†Trainerè®¾ç½®ï¼Œåˆ™åŒ…æ‹¬[`TrainingArguments`]å‚æ•°ã€‚è¯·ä¸è¦å¯¼å‡º[`TrainingArguments`]ï¼Œå› ä¸ºå®ƒæœ‰å‡ åä¸ªä¸é—®é¢˜æ— å…³çš„æ¡ç›®ã€‚
3. è¾“å‡ºï¼š

    ```bash
    python -c 'import torch; print(f"torch: {torch.__version__}")'
    python -c 'import transformers; print(f"transformers: {transformers.__version__}")'
    python -c 'import deepspeed; print(f"deepspeed: {deepspeed.__version__}")'
    ```

4. å¦‚æœå¯èƒ½ï¼Œè¯·åŒ…å«ä¸€ä¸ªGoogle Colab notebooké“¾æ¥ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒæ¥é‡ç°é—®é¢˜ã€‚æ‚¨å¯ä»¥ä½¿ç”¨è¿™ä¸ª[notebook](https://github.com/stas00/porting/blob/master/transformers/deepspeed/DeepSpeed_on_colab_CLI.ipynb)ä½œä¸ºèµ·ç‚¹ã€‚
5. é™¤éä¸å¯èƒ½ï¼Œå¦åˆ™è¯·å§‹ç»ˆä½¿ç”¨æ ‡å‡†æ•°æ®é›†ï¼Œè€Œä¸æ˜¯è‡ªå®šä¹‰æ•°æ®é›†ã€‚
6. å¦‚æœå¯èƒ½ï¼Œå°è¯•ä½¿ç”¨ç°æœ‰[ç¤ºä¾‹](https://github.com/huggingface/transformers/tree/main/examples/pytorch)ä¹‹ä¸€æ¥é‡ç°é—®é¢˜ã€‚

éœ€è¦è€ƒè™‘çš„å› ç´ ï¼š

- Deepspeedé€šå¸¸ä¸æ˜¯é—®é¢˜çš„åŸå› ã€‚

  ä¸€äº›å·²æäº¤çš„é—®é¢˜è¢«è¯æ˜ä¸Deepspeedæ— å…³ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œä¸€æ—¦å°†Deepspeedä»è®¾ç½®ä¸­ç§»é™¤ï¼Œé—®é¢˜ä»ç„¶å­˜åœ¨ã€‚

  å› æ­¤ï¼Œå¦‚æœé—®é¢˜æ˜æ˜¾ä¸DeepSpeedç›¸å…³ï¼Œä¾‹å¦‚æ‚¨å¯ä»¥çœ‹åˆ°æœ‰ä¸€ä¸ªå¼‚å¸¸å¹¶ä¸”å¯ä»¥çœ‹åˆ°DeepSpeedæ¨¡å—æ¶‰åŠå…¶ä¸­ï¼Œè¯·å…ˆé‡æ–°æµ‹è¯•æ²¡æœ‰DeepSpeedçš„è®¾ç½®ã€‚åªæœ‰å½“é—®é¢˜ä»ç„¶å­˜åœ¨æ—¶ï¼Œæ‰å‘Deepspeedæä¾›æ‰€æœ‰å¿…éœ€çš„ç»†èŠ‚ã€‚

- å¦‚æœæ‚¨æ˜ç¡®é—®é¢˜æ˜¯åœ¨Deepspeedæ ¸å¿ƒä¸­è€Œä¸æ˜¯é›†æˆéƒ¨åˆ†ï¼Œè¯·ç›´æ¥å‘[Deepspeed](https://github.com/deepspeedai/DeepSpeed/)æäº¤é—®é¢˜ã€‚å¦‚æœæ‚¨ä¸ç¡®å®šï¼Œè¯·ä¸è¦æ‹…å¿ƒï¼Œæ— è®ºä½¿ç”¨å“ªä¸ªissueè·Ÿè¸ªé—®é¢˜éƒ½å¯ä»¥ï¼Œä¸€æ—¦æ‚¨å‘å¸ƒé—®é¢˜ï¼Œæˆ‘ä»¬ä¼šå¼„æ¸…æ¥šå¹¶å°†å…¶é‡å®šå‘åˆ°å¦ä¸€ä¸ªissueè·Ÿè¸ªï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰ã€‚



### Troubleshooting

#### å¯åŠ¨æ—¶`deepspeed`è¿›ç¨‹è¢«ç»ˆæ­¢ï¼Œæ²¡æœ‰å›æº¯

å¦‚æœå¯åŠ¨æ—¶`deepspeed`è¿›ç¨‹è¢«ç»ˆæ­¢ï¼Œæ²¡æœ‰å›æº¯ï¼Œè¿™é€šå¸¸æ„å‘³ç€ç¨‹åºå°è¯•åˆ†é…çš„CPUå†…å­˜è¶…è¿‡äº†ç³»ç»Ÿçš„é™åˆ¶æˆ–è¿›ç¨‹è¢«å…è®¸åˆ†é…çš„å†…å­˜ï¼Œæ“ä½œç³»ç»Ÿå†…æ ¸æ€æ­»äº†è¯¥è¿›ç¨‹ã€‚è¿™æ˜¯å› ä¸ºæ‚¨çš„é…ç½®æ–‡ä»¶å¾ˆå¯èƒ½å°†`offload_optimizer`æˆ–`offload_param`æˆ–ä¸¤è€…éƒ½é…ç½®ä¸ºå¸è½½åˆ°`cpu`ã€‚å¦‚æœæ‚¨æœ‰NVMeï¼Œå¯ä»¥å°è¯•åœ¨ZeRO-3ä¸‹å¸è½½åˆ°NVMeã€‚è¿™é‡Œæ˜¯å¦‚ä½•[ä¼°è®¡ç‰¹å®šæ¨¡å‹æ‰€éœ€çš„å†…å­˜](https://deepspeed.readthedocs.io/en/latest/memory.html)ã€‚

#### è®­ç»ƒå’Œ/æˆ–è¯„ä¼°/é¢„æµ‹lossä¸º`NaN`

è¿™ç§æƒ…å†µé€šå¸¸å‘ç”Ÿåœ¨ä½¿ç”¨bf16æ··åˆç²¾åº¦æ¨¡å¼é¢„è®­ç»ƒçš„æ¨¡å‹è¯•å›¾åœ¨fp16ï¼ˆå¸¦æˆ–ä¸å¸¦æ··åˆç²¾åº¦ï¼‰ä¸‹ä½¿ç”¨æ—¶ã€‚å¤§å¤šæ•°åœ¨TPUä¸Šè®­ç»ƒçš„æ¨¡å‹ä»¥åŠç”±è°·æ­Œå‘å¸ƒçš„æ¨¡å‹éƒ½å±äºè¿™ä¸ªç±»åˆ«ï¼ˆä¾‹å¦‚ï¼Œå‡ ä¹æ‰€æœ‰åŸºäºt5çš„æ¨¡å‹ï¼‰ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè§£å†³æ–¹æ¡ˆæ˜¯è¦ä¹ˆä½¿ç”¨fp32ï¼Œè¦ä¹ˆåœ¨æ”¯æŒçš„æƒ…å†µä¸‹ä½¿ç”¨bf16ï¼ˆå¦‚TPUã€Ampere GPUæˆ–æ›´æ–°çš„ç‰ˆæœ¬ï¼‰ã€‚

å¦ä¸€ä¸ªé—®é¢˜å¯èƒ½ä¸ä½¿ç”¨fp16æœ‰å…³ã€‚å½“æ‚¨é…ç½®æ­¤éƒ¨åˆ†æ—¶ï¼š

```json
{
    "fp16": {
        "enabled": "auto",
        "loss_scale": 0,
        "loss_scale_window": 1000,
        "initial_scale_power": 16,
        "hysteresis": 2,
        "min_loss_scale": 1
    }
}
```

å¹¶ä¸”æ‚¨åœ¨æ—¥å¿—ä¸­çœ‹åˆ°DeepspeedæŠ¥å‘Š`OVERFLOW`å¦‚ä¸‹

```
0%|                                                                                                                             | 0/189 [00:00<?, ?it/s]
 [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 262144
  1%|â–Œ                                                                                                                    | 1/189 [00:00<01:26,  2.17it/s]
 [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072.0
  1%|â–ˆâ–
 [...]
 [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, reducing to 1
 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                   | 27/189 [00:14<01:13,  2.21it/s]
 [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, reducing to 1
 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                  | 28/189 [00:14<01:13,  2.18it/s]
 [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, reducing to 1
 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                  | 29/189 [00:15<01:13,  2.18it/s]
 [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, reducing to 1
[...]
```

è¿™æ„å‘³ç€DeepspeedæŸå¤±ç¼©æ”¾å™¨æ— æ³•æ‰¾åˆ°ä¸€ä¸ªå…‹æœæŸå¤±æº¢å‡ºçš„ç¼©æ”¾ç³»æ•°ã€‚

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé€šå¸¸éœ€è¦æé«˜`initial_scale_power`çš„å€¼ã€‚å°†å…¶è®¾ç½®ä¸º`"initial_scale_power": 32`é€šå¸¸ä¼šè§£å†³é—®é¢˜ã€‚



### æ³¨æ„äº‹é¡¹

- å°½ç®¡ DeepSpeed æœ‰ä¸€ä¸ªå¯å®‰è£…çš„ PyPI åŒ…ï¼Œä½†å¼ºçƒˆå»ºè®®ä»æºä»£ç å®‰è£…å®ƒï¼Œä»¥æœ€å¥½åœ°åŒ¹é…æ‚¨çš„ç¡¬ä»¶ï¼Œå¦‚æœæ‚¨éœ€è¦å¯ç”¨æŸäº›åŠŸèƒ½ï¼Œå¦‚ 1-bit Adamï¼Œè¿™äº›åŠŸèƒ½åœ¨ pypi å‘è¡Œç‰ˆä¸­ä¸å¯ç”¨ã€‚
- æ‚¨ä¸å¿…ä½¿ç”¨ğŸ¤—  Transformersçš„ [`Trainer`] æ¥ä½¿ç”¨ DeepSpeed   - æ‚¨å¯ä»¥ä½¿ç”¨ä»»ä½•æ¨¡å‹ä¸è‡ªå·±çš„è®­ç»ƒå™¨ï¼Œæ‚¨è¿˜éœ€è¦æ ¹æ® [DeepSpeed é›†æˆè¯´æ˜](https://www.deepspeed.ai/getting-started/#writing-deepspeed-models) è°ƒæ•´åè€…ã€‚



## Non-Trainer Deepspeedé›†æˆ

å½“`Trainer`æ²¡æœ‰è¢«ä½¿ç”¨æ—¶ï¼Œ`~integrations.HfDeepSpeedConfig`è¢«ç”¨æ¥å°†Deepspeedé›†æˆåˆ°huggingfaceçš„Transformersæ ¸å¿ƒåŠŸèƒ½ä¸­ã€‚å®ƒå”¯ä¸€åšçš„äº‹æƒ…å°±æ˜¯åœ¨`from_pretrained`è°ƒç”¨æœŸé—´å¤„ç†Deepspeed ZeRO-3å‚æ•°æ”¶é›†å’Œå°†æ¨¡å‹è‡ªåŠ¨åˆ†å‰²åˆ°å¤šä¸ªGPUä¸Šã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæ‚¨éœ€è¦è‡ªå·±å®Œæˆå…¶ä»–æ‰€æœ‰å·¥ä½œã€‚

å½“ä½¿ç”¨`Trainer`æ—¶ï¼Œæ‰€æœ‰äº‹æƒ…éƒ½è‡ªåŠ¨å¾—åˆ°äº†å¤„ç†ã€‚

å½“ä¸ä½¿ç”¨`Trainer`æ—¶ï¼Œä¸ºäº†é«˜æ•ˆåœ°éƒ¨ç½²Deepspeed ZeRO-3ï¼Œæ‚¨å¿…é¡»åœ¨å®ä¾‹åŒ–æ¨¡å‹ä¹‹å‰å®ä¾‹åŒ–`~integrations.HfDeepSpeedConfig`å¯¹è±¡å¹¶ä¿æŒè¯¥å¯¹è±¡æ´»è·ƒã€‚

å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨Deepspeed ZeRO-1æˆ–ZeRO-2ï¼Œæ‚¨æ ¹æœ¬ä¸éœ€è¦ä½¿ç”¨`HfDeepSpeedConfig`ã€‚

ä»¥é¢„è®­ç»ƒæ¨¡å‹ä¸ºä¾‹:

```python
from transformers.integrations import HfDeepSpeedConfig
from transformers import AutoModel
import deepspeed

ds_config = {...}  # deepspeed config object or path to the file
# must run before instantiating the model to detect zero 3
dschf = HfDeepSpeedConfig(ds_config)  # keep this object alive
model = AutoModel.from_pretrained("openai-community/gpt2")
engine = deepspeed.initialize(model=model, config_params=ds_config, ...)
```

æˆ–è€…ä»¥éé¢„è®­ç»ƒæ¨¡å‹ä¸ºä¾‹ï¼š

```python
from transformers.integrations import HfDeepSpeedConfig
from transformers import AutoModel, AutoConfig
import deepspeed

ds_config = {...}  # deepspeed config object or path to the file
# must run before instantiating the model to detect zero 3
dschf = HfDeepSpeedConfig(ds_config)  # keep this object alive
config = AutoConfig.from_pretrained("openai-community/gpt2")
model = AutoModel.from_config(config)
engine = deepspeed.initialize(model=model, config_params=ds_config, ...)
```

è¯·æ³¨æ„ï¼Œå¦‚æœæ‚¨æ²¡æœ‰ä½¿ç”¨[`Trainer`]é›†æˆï¼Œæ‚¨å®Œå…¨éœ€è¦è‡ªå·±åŠ¨æ‰‹ã€‚åŸºæœ¬ä¸Šéµå¾ª[Deepspeed](https://www.deepspeed.ai/)ç½‘ç«™ä¸Šçš„æ–‡æ¡£ã€‚åŒæ—¶ï¼Œæ‚¨å¿…é¡»æ˜¾å¼é…ç½®é…ç½®æ–‡ä»¶ - ä¸èƒ½ä½¿ç”¨`"auto"`å€¼ï¼Œè€Œå¿…é¡»æ”¾å…¥å®é™…å€¼ã€‚


## HfDeepSpeedConfig

[[autodoc]] integrations.HfDeepSpeedConfig
    - all

### è‡ªå®šä¹‰DeepSpeed ZeROæ¨ç†

ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œæ¼”ç¤ºäº†åœ¨æ— æ³•å°†æ¨¡å‹æ”¾å…¥å•ä¸ª GPU æ—¶å¦‚æœä¸ä½¿ç”¨[Trainer]è¿›è¡Œ DeepSpeed ZeRO æ¨ç† ã€‚è¯¥è§£å†³æ–¹æ¡ˆåŒ…æ‹¬ä½¿ç”¨é¢å¤–çš„ GPU æˆ–/å’Œå°† GPU å†…å­˜å¸è½½åˆ° CPU å†…å­˜ã€‚

è¿™é‡Œè¦ç†è§£çš„é‡è¦ç»†å¾®å·®åˆ«æ˜¯ï¼ŒZeROçš„è®¾è®¡æ–¹å¼å¯ä»¥è®©æ‚¨åœ¨ä¸åŒçš„GPUä¸Šå¹¶è¡Œå¤„ç†ä¸åŒçš„è¾“å…¥ã€‚

è¿™ä¸ªä¾‹å­æœ‰å¾ˆå¤šæ³¨é‡Šï¼Œå¹¶ä¸”æ˜¯è‡ªæ–‡æ¡£åŒ–çš„ã€‚

è¯·ç¡®ä¿ï¼š

1. å¦‚æœæ‚¨æœ‰è¶³å¤Ÿçš„GPUå†…å­˜ï¼ˆå› ä¸ºè¿™ä¼šå‡æ…¢é€Ÿåº¦ï¼‰ï¼Œç¦ç”¨CPU offloadã€‚
2. å¦‚æœæ‚¨æ‹¥æœ‰Ampereæ¶æ„æˆ–æ›´æ–°çš„GPUï¼Œå¯ç”¨bf16ä»¥åŠ å¿«é€Ÿåº¦ã€‚å¦‚æœæ‚¨æ²¡æœ‰è¿™ç§ç¡¬ä»¶ï¼Œåªè¦ä¸ä½¿ç”¨ä»»ä½•åœ¨bf16æ··åˆç²¾åº¦ä¸‹é¢„è®­ç»ƒçš„æ¨¡å‹ï¼ˆå¦‚å¤§å¤šæ•°t5æ¨¡å‹ï¼‰ï¼Œå°±å¯ä»¥å¯ç”¨fp16ã€‚å¦åˆ™è¿™äº›æ¨¡å‹é€šå¸¸åœ¨fp16ä¸­æº¢å‡ºï¼Œæ‚¨ä¼šçœ‹åˆ°è¾“å‡ºæ— æ•ˆç»“æœã€‚

```python
#!/usr/bin/env python

# This script demonstrates how to use Deepspeed ZeRO in an inference mode when one can't fit a model
# into a single GPU
#
# 1. Use 1 GPU with CPU offload
# 2. Or use multiple GPUs instead
#
# First you need to install deepspeed: pip install deepspeed
#
# Here we use a 3B "bigscience/T0_3B" model which needs about 15GB GPU RAM - so 1 largish or 2
# small GPUs can handle it. or 1 small GPU and a lot of CPU memory.
#
# To use a larger model like "bigscience/T0" which needs about 50GB, unless you have an 80GB GPU -
# you will need 2-4 gpus. And then you can adapt the script to handle more gpus if you want to
# process multiple inputs at once.
#
# The provided deepspeed config also activates CPU memory offloading, so chances are that if you
# have a lot of available CPU memory and you don't mind a slowdown you should be able to load a
# model that doesn't normally fit into a single GPU. If you have enough GPU memory the program will
# run faster if you don't want offload to CPU - so disable that section then.
#
# To deploy on 1 gpu:
#
# deepspeed --num_gpus 1 t0.py
# or:
# python -m torch.distributed.run --nproc_per_node=1 t0.py
#
# To deploy on 2 gpus:
#
# deepspeed --num_gpus 2 t0.py
# or:
# python -m torch.distributed.run --nproc_per_node=2 t0.py


from transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM
from transformers.integrations import HfDeepSpeedConfig
import deepspeed
import os
import torch

os.environ["TOKENIZERS_PARALLELISM"] = "false"  # To avoid warnings about parallelism in tokenizers

# distributed setup
local_rank = int(os.getenv("LOCAL_RANK", "0"))
world_size = int(os.getenv("WORLD_SIZE", "1"))
torch.cuda.set_device(local_rank)
deepspeed.init_distributed()

model_name = "bigscience/T0_3B"

config = AutoConfig.from_pretrained(model_name)
model_hidden_size = config.d_model

# batch size has to be divisible by world_size, but can be bigger than world_size
train_batch_size = 1 * world_size

# ds_config notes
#
# - enable bf16 if you use Ampere or higher GPU - this will run in mixed precision and will be
# faster.
#
# - for older GPUs you can enable fp16, but it'll only work for non-bf16 pretrained models - e.g.
# all official t5 models are bf16-pretrained
#
# - set offload_param.device to "none" or completely remove the `offload_param` section if you don't
# - want CPU offload
#
# - if using `offload_param` you can manually finetune stage3_param_persistence_threshold to control
# - which params should remain on gpus - the larger the value the smaller the offload size
#
# For in-depth info on Deepspeed config see
# https://huggingface.co/docs/transformers/main/main_classes/deepspeed

# keeping the same format as json for consistency, except it uses lower case for true/false
# fmt: off
ds_config = {
    "fp16": {
        "enabled": False
    },
    "bf16": {
        "enabled": False
    },
    "zero_optimization": {
        "stage": 3,
        "offload_param": {
            "device": "cpu",
            "pin_memory": True
        },
        "overlap_comm": True,
        "contiguous_gradients": True,
        "reduce_bucket_size": model_hidden_size * model_hidden_size,
        "stage3_prefetch_bucket_size": 0.9 * model_hidden_size * model_hidden_size,
        "stage3_param_persistence_threshold": 10 * model_hidden_size
    },
    "steps_per_print": 2000,
    "train_batch_size": train_batch_size,
    "train_micro_batch_size_per_gpu": 1,
    "wall_clock_breakdown": False
}
# fmt: on

# next line instructs transformers to partition the model directly over multiple gpus using
# deepspeed.zero.Init when model's `from_pretrained` method is called.
#
# **it has to be run before loading the model AutoModelForSeq2SeqLM.from_pretrained(model_name)**
#
# otherwise the model will first be loaded normally and only partitioned at forward time which is
# less efficient and when there is little CPU RAM may fail
dschf = HfDeepSpeedConfig(ds_config)  # keep this object alive

# now a model can be loaded.
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# initialise Deepspeed ZeRO and store only the engine object
ds_engine = deepspeed.initialize(model=model, config_params=ds_config)[0]
ds_engine.module.eval()  # inference

# Deepspeed ZeRO can process unrelated inputs on each GPU. So for 2 gpus you process 2 inputs at once.
# If you use more GPUs adjust for more.
# And of course if you have just one input to process you then need to pass the same string to both gpus
# If you use only one GPU, then you will have only rank 0.
rank = torch.distributed.get_rank()
if rank == 0:
    text_in = "Is this review positive or negative? Review: this is the best cast iron skillet you will ever buy"
elif rank == 1:
    text_in = "Is this review positive or negative? Review: this is the worst restaurant ever"

tokenizer = AutoTokenizer.from_pretrained(model_name)
inputs = tokenizer.encode(text_in, return_tensors="pt").to(device=local_rank)
with torch.no_grad():
    outputs = ds_engine.module.generate(inputs, synced_gpus=True)
text_out = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(f"rank{rank}:\n   in={text_in}\n  out={text_out}")
```

è®©æˆ‘ä»¬ä¿å­˜å®ƒä¸º `t0.py`å¹¶è¿è¡Œï¼š
```bash
$ deepspeed --num_gpus 2 t0.py
rank0:
   in=Is this review positive or negative? Review: this is the best cast iron skillet you will ever buy
  out=Positive
rank1:
   in=Is this review positive or negative? Review: this is the worst restaurant ever
  out=negative
```

è¿™æ˜¯ä¸€ä¸ªéå¸¸åŸºæœ¬çš„ä¾‹å­ï¼Œæ‚¨éœ€è¦æ ¹æ®è‡ªå·±çš„éœ€æ±‚è¿›è¡Œä¿®æ”¹ã€‚

### `generate` çš„å·®å¼‚

åœ¨ä½¿ç”¨ZeRO stage 3çš„å¤šGPUæ—¶ï¼Œéœ€è¦é€šè¿‡è°ƒç”¨`generate(..., synced_gpus=True)`æ¥åŒæ­¥GPUã€‚å¦‚æœä¸€ä¸ªGPUåœ¨å…¶å®ƒGPUä¹‹å‰å®Œæˆç”Ÿæˆï¼Œæ•´ä¸ªç³»ç»Ÿå°†æŒ‚èµ·ï¼Œå› ä¸ºå…¶ä»–GPUæ— æ³•ä»åœæ­¢ç”Ÿæˆçš„GPUæ¥æ”¶æƒé‡åˆ†ç‰‡ã€‚

ä»`transformers>=4.28`å¼€å§‹ï¼Œå¦‚æœæ²¡æœ‰æ˜ç¡®æŒ‡å®š`synced_gpus`ï¼Œæ£€æµ‹åˆ°è¿™äº›æ¡ä»¶åå®ƒå°†è‡ªåŠ¨è®¾ç½®ä¸º`True`ã€‚ä½†å¦‚æœæ‚¨éœ€è¦è¦†ç›–`synced_gpus`çš„å€¼ï¼Œä»ç„¶å¯ä»¥è¿™æ ·åšã€‚



## æµ‹è¯• DeepSpeed é›†æˆ

å¦‚æœæ‚¨æäº¤äº†ä¸€ä¸ªæ¶‰åŠDeepSpeedé›†æˆçš„PRï¼Œè¯·æ³¨æ„æˆ‘ä»¬çš„CircleCI PR CIè®¾ç½®æ²¡æœ‰GPUï¼Œå› æ­¤æˆ‘ä»¬åªåœ¨å¦ä¸€ä¸ªCIå¤œé—´è¿è¡Œéœ€è¦GPUçš„æµ‹è¯•ã€‚å› æ­¤ï¼Œå¦‚æœæ‚¨åœ¨PRä¸­è·å¾—ç»¿è‰²çš„CIæŠ¥å‘Šï¼Œå¹¶ä¸æ„å‘³ç€DeepSpeedæµ‹è¯•é€šè¿‡ã€‚

è¦è¿è¡ŒDeepSpeedæµ‹è¯•ï¼Œè¯·è‡³å°‘è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```bash
RUN_SLOW=1 pytest tests/deepspeed/test_deepspeed.py
```

å¦‚æœä½ æ›´æ”¹äº†ä»»ä½•æ¨¡å‹æˆ–PyTorchç¤ºä¾‹ä»£ç ï¼Œè¯·åŒæ—¶è¿è¡Œå¤šæ¨¡å‹æµ‹è¯•ã€‚ä»¥ä¸‹å°†è¿è¡Œæ‰€æœ‰DeepSpeedæµ‹è¯•ï¼š

```bash
RUN_SLOW=1 pytest tests/deepspeed
```

## ä¸»è¦çš„DeepSpeedèµ„æº

- [é¡¹ç›®GitHub](https://github.com/deepspeedai/DeepSpeed)
- [ä½¿ç”¨æ–‡æ¡£](https://www.deepspeed.ai/getting-started/)
- [APIæ–‡æ¡£](https://deepspeed.readthedocs.io/en/latest/index.html)
- [åšå®¢æ–‡ç« ](https://www.microsoft.com/en-us/research/search/?q=deepspeed)

è®ºæ–‡:

- [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://huggingface.co/papers/1910.02054)
- [ZeRO-Offload: Democratizing Billion-Scale Model Training](https://huggingface.co/papers/2101.06840)
- [ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning](https://huggingface.co/papers/2104.07857)

æœ€åï¼Œè¯·è®°ä½ï¼ŒHuggingFace [`Trainer`]ä»…é›†æˆäº†DeepSpeedï¼Œå› æ­¤å¦‚æœæ‚¨åœ¨ä½¿ç”¨DeepSpeedæ—¶é‡åˆ°ä»»ä½•é—®é¢˜æˆ–ç–‘é—®ï¼Œè¯·åœ¨[DeepSpeed GitHub](https://github.com/deepspeedai/DeepSpeed/issues)ä¸Šæäº¤ä¸€ä¸ªissueã€‚
