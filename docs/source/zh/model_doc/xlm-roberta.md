<!--ç‰ˆæƒæ‰€æœ‰ 2020 å¹´çš„ HuggingFace å›¢é˜Ÿã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚
æ ¹æ® Apache è®¸å¯è¯ï¼Œç¬¬ 2 ç‰ˆï¼ˆâ€œè®¸å¯è¯â€ï¼‰è¿›è¡Œè®¸å¯ï¼›æ‚¨é™¤éç¬¦åˆè®¸å¯è¯çš„è§„å®šï¼Œå¦åˆ™ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å–è®¸å¯è¯çš„å‰¯æœ¬
http://www.apache.org/licenses/LICENSE-2.0
é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œæ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯æŒ‰åŸæ ·åˆ†å‘çš„ï¼Œä¸é™„å¸¦ä»»ä½•å½¢å¼çš„æ‹…ä¿æˆ–æ¡ä»¶ã€‚è¯·å‚é˜…è®¸å¯è¯ä¸­çš„å…·ä½“è¯­è¨€ï¼Œä»¥äº†è§£æƒé™å’Œé™åˆ¶ã€‚âš ï¸è¯·æ³¨æ„ï¼Œæ­¤æ–‡ä»¶æ˜¯ Markdown æ ¼å¼çš„ï¼Œä½†åŒ…å«ç‰¹å®šäºæˆ‘ä»¬çš„æ–‡æ¡£æ„å»ºå™¨ï¼ˆç±»ä¼¼äº MDXï¼‰çš„è¯­æ³•ï¼Œå¯èƒ½æ— æ³•åœ¨æ‚¨çš„ Markdown æŸ¥çœ‹å™¨ä¸­æ­£ç¡®å‘ˆç°ã€‚-->



# XLM-RoBERTa

<div class="flex flex-wrap space-x-1">
<a href="https://huggingface.co/models?filter=xlm-roberta">
<img alt="Models" src="https://img.shields.io/badge/All_model_pages-xlm--roberta-blueviolet">
</a>
<a href="https://huggingface.co/spaces/docs-demos/xlm-roberta-base">
<img alt="Spaces" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue">
</a>
</div>

## æ¦‚è¿°

XLM-RoBERTa æ¨¡å‹æ˜¯ç”± Alexis Conneauã€Kartikay Khandelwalã€Naman Goyalã€Vishrav Chaudharyã€Guillaume Wenzekã€Francisco Guzm Ã¡ nã€Edouard Graveã€Myle Ottã€Luke Zettlemoyer å’Œ Veselin Stoyanov åœ¨ã€Šè§„æ¨¡ä¸Šçš„æ— ç›‘ç£è·¨è¯­è¨€è¡¨ç¤ºå­¦ä¹ ã€‹ä¸­æå‡ºçš„ã€‚å®ƒåŸºäº Facebook äº 2019 å¹´å‘å¸ƒçš„ RoBERTa æ¨¡å‹ã€‚å®ƒæ˜¯ä¸€ä¸ªå¤§å‹çš„å¤šè¯­è¨€è¯­è¨€æ¨¡å‹ï¼Œä½¿ç”¨äº† 2.5TB çš„ç»è¿‡ç­›é€‰çš„ CommonCrawl æ•°æ®è¿›è¡Œè®­ç»ƒã€‚

è®ºæ–‡çš„æ‘˜è¦å¦‚ä¸‹æ‰€ç¤ºï¼š

*æœ¬æ–‡è¡¨æ˜ï¼Œå¤§è§„æ¨¡é¢„è®­ç»ƒå¤šè¯­è¨€è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨å¹¿æ³›çš„è·¨è¯­è¨€è¿ç§»ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬åœ¨ä¸€ç™¾ç§è¯­è¨€ä¸Šè®­ç»ƒäº†åŸºäº Transformer çš„é®è”½è¯­è¨€æ¨¡å‹ï¼Œä½¿ç”¨äº†è¶…è¿‡ä¸¤åƒå…†å­—èŠ‚çš„ç»è¿‡ç­›é€‰çš„ CommonCrawl æ•°æ®ã€‚æˆ‘ä»¬çš„æ¨¡å‹è¢«ç§°ä¸º XLM-Rï¼Œåœ¨å„ç§è·¨è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­æ˜æ˜¾ä¼˜äºå¤šè¯­è¨€ BERTï¼ˆmBERTï¼‰ï¼ŒåŒ…æ‹¬ XNLI å¹³å‡å‡†ç¡®æ€§æé«˜äº† 13.8 ï¼…ï¼ŒMLQA å¹³å‡ F1 åˆ†æ•°æé«˜äº† 12.3 ï¼…ï¼ŒNER å¹³å‡ F1 åˆ†æ•°æé«˜äº† 2.1 ï¼…ã€‚XLM-R åœ¨ä½èµ„æºè¯­è¨€ä¸Šè¡¨ç°å°¤ä¸ºå‡ºè‰²ï¼Œç›¸æ¯”ä¹‹å‰çš„ XLM æ¨¡å‹ï¼Œå…¶åœ¨æ–¯ç“¦å¸Œé‡Œè¯­çš„ XNLI å‡†ç¡®æ€§æé«˜äº† 11.8 ï¼…ï¼Œä¹Œå°”éƒ½è¯­æé«˜äº† 9.2 ï¼…ã€‚æˆ‘ä»¬è¿˜å¯¹å®ç°è¿™äº›å¢ç›Šæ‰€éœ€çš„å…³é”®å› ç´ è¿›è¡Œäº†è¯¦ç»†çš„å®è¯è¯„ä¼°ï¼ŒåŒ…æ‹¬ï¼ˆ1ï¼‰æ­£å‘è¿ç§»å’Œå®¹é‡ç¨€é‡Šä¹‹é—´çš„æƒè¡¡ï¼Œä»¥åŠï¼ˆ2ï¼‰é«˜èµ„æºå’Œä½èµ„æºè¯­è¨€çš„æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬é¦–æ¬¡å±•ç¤ºäº†åœ¨ä¸ç‰ºç‰²æ¯ç§è¯­è¨€æ€§èƒ½çš„æƒ…å†µä¸‹è¿›è¡Œå¤šè¯­è¨€å»ºæ¨¡çš„å¯èƒ½æ€§ï¼›XLM-R åœ¨ GLUE å’Œ XNLI åŸºå‡†æµ‹è¯•ä¸­ä¸å¼ºå¤§çš„å•è¯­æ¨¡å‹ç«äº‰åŠ›åè¶³ã€‚æˆ‘ä»¬å°†å…¬å¼€æä¾› XLM-R çš„ä»£ç ã€æ•°æ®å’Œæ¨¡å‹ã€‚* 

æç¤ºï¼š

- XLM-RoBERTa æ˜¯ä¸€ä¸ªåœ¨ 100 ç§ä¸åŒè¯­è¨€ä¸Šè®­ç»ƒçš„å¤šè¯­è¨€æ¨¡å‹ã€‚ä¸æŸäº› XLM å¤šè¯­è¨€æ¨¡å‹ä¸åŒï¼Œå®ƒä¸éœ€è¦ `lang` å¼ é‡æ¥ç¡®å®šä½¿ç”¨çš„è¯­è¨€ï¼Œå¹¶ä¸”åº”è¯¥èƒ½å¤Ÿä»è¾“å…¥ id ä¸­ç¡®å®šæ­£ç¡®çš„è¯­è¨€ã€‚- ä½¿ç”¨ RoBERTa çš„æŠ€å·§è¿›è¡Œ XLM æ–¹æ³•ï¼Œä½†ä¸ä½¿ç”¨ç¿»è¯‘è¯­è¨€å»ºæ¨¡ç›®æ ‡ã€‚å®ƒåªä½¿ç”¨æ¥è‡ªä¸€ç§è¯­è¨€çš„å¥å­è¿›è¡Œé®è”½è¯­è¨€å»ºæ¨¡ã€‚  
- æ­¤å®ç°ä¸ RoBERTa ç›¸åŒã€‚æœ‰å…³ç”¨æ³•ç¤ºä¾‹ä»¥åŠè¾“å…¥å’Œè¾“å‡ºçš„ç›¸å…³ä¿¡æ¯ï¼Œè¯·å‚é˜… [RoBERTa çš„æ–‡æ¡£](roberta)ã€‚  

æ­¤æ¨¡å‹ç”± [stefan-it](https://huggingface.co/stefan-it) è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯åœ¨ [æ­¤å¤„](https://github.com/pytorch/fairseq/tree/master/examples/xlmr) æ‰¾åˆ°ã€‚

## èµ„æº

ä»¥ä¸‹æ˜¯å®˜æ–¹ Hugging Face å’Œç¤¾åŒºï¼ˆä»¥ğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨ XLM-RoBERTaã€‚å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æäº¤æ‹‰å–è¯·æ±‚ï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æŸ¥ï¼è¯¥èµ„æºåº”è¯¥å±•ç¤ºå‡ºæ–°çš„ä¸œè¥¿ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰çš„èµ„æºã€‚
<PipelineTag pipeline="text-classification"/>
- æœ‰å…³å¦‚ä½• [ä½¿ç”¨ Habana Gaudi åœ¨ AWS ä¸Šå¾®è°ƒ XLM RoBERTa è¿›è¡Œå¤šç±»åˆ†ç±»çš„åšå®¢æ–‡ç« ](https://www.philschmid.de/habana-distributed-training)
- [`XLMRobertaForSequenceClassification`] ç”±æ­¤ [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification) å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb) æ”¯æŒã€‚
- [`TFXLMRobertaForSequenceClassification`] ç”±æ­¤ [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification) å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb) æ”¯æŒã€‚
- [`FlaxXLMRobertaForSequenceClassification`] ç”±æ­¤ [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/flax/text-classification) å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_flax.ipynb) æ”¯æŒã€‚
- ğŸ¤— Hugging Face ä»»åŠ¡æŒ‡å—çš„ [æ–‡æœ¬åˆ†ç±»](https://huggingface.co/docs/transformers/tasks/sequence_classification) ç« èŠ‚ã€‚
- [æ–‡æœ¬åˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/sequence_classification)
<PipelineTag pipeline="token-classification"/>
- [`XLMRobertaForTokenClassification`] ç”±æ­¤ [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification) å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb) æ”¯æŒã€‚
- [`TFXLMRobertaForTokenClassification`] ç”±æ­¤ [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/token-classification) å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb) æ”¯æŒã€‚
- [`FlaxXLMRobertaForTokenClassification`] ç”±æ­¤ [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/flax/token-classification) æ”¯æŒã€‚
- ğŸ¤— Hugging Face è¯¾ç¨‹ä¸­çš„ [æ ‡è®°åˆ†ç±»](https://huggingface.co/course/chapter7/2?fw=pt) ç« èŠ‚ã€‚
- [æ ‡è®°åˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/token_classification)
<PipelineTag pipeline="text-generation"/>
- [`XLMRobertaForCausalLM`] ç”±æ­¤ [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling) å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb) æ”¯æŒã€‚
- ğŸ¤— Hugging Face ä»»åŠ¡æŒ‡å—çš„ [å› æœè¯­è¨€å»ºæ¨¡](https://huggingface.co/docs/transformers/tasks/language_modeling) ç« èŠ‚ã€‚
- [å› æœè¯­è¨€å»ºæ¨¡ä»»åŠ¡æŒ‡å—](../tasks/language_modeling)
<PipelineTag pipeline="fill-mask"/>
- [`XLMRobertaForMaskedLM`] åœ¨è¿™ä¸ª [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#robertabertdistilbert-and-masked-language-modeling) å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb) ä¸­å¾—åˆ°æ”¯æŒã€‚
- [`TFXLMRobertaForMaskedLM`] åœ¨è¿™ä¸ª [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_mlmpy) å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb) ä¸­å¾—åˆ°æ”¯æŒã€‚
- [`FlaxXLMRobertaForMaskedLM`] åœ¨è¿™ä¸ª [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#masked-language-modeling) å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked_language_modeling_flax.ipynb) ä¸­å¾—åˆ°æ”¯æŒã€‚- [é®è”½è¯­è¨€å»ºæ¨¡](https://huggingface.co/course/chapter7/3?fw=pt) ç« èŠ‚çš„ğŸ¤— Hugging Face è¯¾ç¨‹ã€‚
- [é®è”½è¯­è¨€å»ºæ¨¡](../tasks/masked_language_modeling)
<PipelineTag pipeline="question-answering"/>

- [`XLMRobertaForQuestionAnswering`] åœ¨è¿™ä¸ª [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb) ä¸­å¾—åˆ°æ”¯æŒã€‚
- [`TFXLMRobertaForQuestionAnswering`] åœ¨è¿™ä¸ª [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering) å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb) ä¸­å¾—åˆ°æ”¯æŒã€‚
- [`FlaxXLMRobertaForQuestionAnswering`] åœ¨è¿™ä¸ª [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/flax/question-answering) ä¸­å¾—åˆ°æ”¯æŒã€‚
- [é—®é¢˜å›ç­”](https://huggingface.co/course/chapter7/7?fw=pt) ç« èŠ‚çš„ğŸ¤— Hugging Face è¯¾ç¨‹ã€‚
- [é—®é¢˜å›ç­”ä»»åŠ¡æŒ‡å—](../tasks/question_answering)

**å¤šé¡¹é€‰æ‹©**

- [`XLMRobertaForMultipleChoice`] åœ¨è¿™ä¸ª [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb) ä¸­å¾—åˆ°æ”¯æŒã€‚
- [`TFXLMRobertaForMultipleChoice`] åœ¨è¿™ä¸ª [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice) å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb) ä¸­å¾—åˆ°æ”¯æŒã€‚
- [å¤šé¡¹é€‰æ‹©ä»»åŠ¡æŒ‡å—](../tasks/multiple_choice)

ğŸš€ éƒ¨ç½²

- å¦‚ä½•åœ¨ AWS Lambda ä¸Š [éƒ¨ç½²æ— æœåŠ¡å™¨ XLM RoBERTa](https://www.philschmid.de/multilingual-serverless-xlm-roberta-with-huggingface) çš„åšæ–‡ã€‚
## XLMRobertaConfig

[[autodoc]] XLMRobertaConfig

## XLMRobertaTokenizer

[[autodoc]] XLMRobertaTokenizer
    - build_inputs_with_special_tokens
    - get_special_tokens_mask
    - create_token_type_ids_from_sequences
    - save_vocabulary

## XLMRobertaTokenizerFast

[[autodoc]] XLMRobertaTokenizerFast

## XLMRobertaModel

[[autodoc]] XLMRobertaModel
    - forward

## XLMRobertaForCausalLM

[[autodoc]] XLMRobertaForCausalLM
    - forward

## XLMRobertaForMaskedLM

[[autodoc]] XLMRobertaForMaskedLM
    - forward

## XLMRobertaForSequenceClassification

[[autodoc]] XLMRobertaForSequenceClassification
    - forward

## XLMRobertaForMultipleChoice

[[autodoc]] XLMRobertaForMultipleChoice
    - forward

## XLMRobertaForTokenClassification

[[autodoc]] XLMRobertaForTokenClassification
    - forward

## XLMRobertaForQuestionAnswering

[[autodoc]] XLMRobertaForQuestionAnswering
    - forward

## TFXLMRobertaModel

[[autodoc]] TFXLMRobertaModel
    - call

## TFXLMRobertaForCausalLM

[[autodoc]] TFXLMRobertaForCausalLM
    - call

## TFXLMRobertaForMaskedLM

[[autodoc]] TFXLMRobertaForMaskedLM
    - call

## TFXLMRobertaForSequenceClassification

[[autodoc]] TFXLMRobertaForSequenceClassification
    - call

## TFXLMRobertaForMultipleChoice

[[autodoc]] TFXLMRobertaForMultipleChoice
    - call

## TFXLMRobertaForTokenClassification

[[autodoc]] TFXLMRobertaForTokenClassification
    - call

## TFXLMRobertaForQuestionAnswering

[[autodoc]] TFXLMRobertaForQuestionAnswering
    - call

## FlaxXLMRobertaModel

[[autodoc]] FlaxXLMRobertaModel
    - __call__

## FlaxXLMRobertaForCausalLM

[[autodoc]] FlaxXLMRobertaForCausalLM
    - __call__

## FlaxXLMRobertaForMaskedLM

[[autodoc]] FlaxXLMRobertaForMaskedLM
    - __call__

## FlaxXLMRobertaForSequenceClassification

[[autodoc]] FlaxXLMRobertaForSequenceClassification
    - __call__

## FlaxXLMRobertaForMultipleChoice

[[autodoc]] FlaxXLMRobertaForMultipleChoice
    - __call__

## FlaxXLMRobertaForTokenClassification

[[autodoc]] FlaxXLMRobertaForTokenClassification
    - __call__

## FlaxXLMRobertaForQuestionAnswering

[[autodoc]] FlaxXLMRobertaForQuestionAnswering
    - __call__
