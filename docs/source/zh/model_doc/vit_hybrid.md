<!--ç‰ˆæƒæ‰€æœ‰2022å¹´HuggingFaceå›¢é˜Ÿã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚-->
æ ¹æ® Apache è®¸å¯è¯ï¼Œç‰ˆæœ¬ 2.0ï¼ˆâ€œè®¸å¯è¯â€ï¼‰çš„è§„å®šï¼Œé™¤éç¬¦åˆè®¸å¯è¯çš„è¦æ±‚ï¼Œå¦åˆ™æ‚¨æ— æƒä½¿ç”¨æœ¬æ–‡ä»¶ã€‚æ‚¨å¯ä»¥åœ¨ä¸‹é¢çš„é“¾æ¥å¤„è·å¾—è®¸å¯è¯çš„å‰¯æœ¬ã€‚
http://www.apache.org/licenses/LICENSE-2.0
é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼ŒæŒ‰â€œåŸæ ·â€åˆ†å‘çš„è½¯ä»¶åœ¨è®¸å¯è¯ä¸‹åˆ†å‘ï¼Œä¸é™„å¸¦ä»»ä½•å½¢å¼çš„æ‹…ä¿æˆ–æ¡ä»¶ã€‚è¯·å‚é˜…è®¸å¯è¯ä»¥äº†è§£ç‰¹å®šè¯­è¨€ä¸‹çš„æƒé™å’Œé™åˆ¶ã€‚âš ï¸è¯·æ³¨æ„ï¼Œæ­¤æ–‡ä»¶æ˜¯ Markdown æ ¼å¼çš„ï¼Œä½†åŒ…å«äº†æˆ‘ä»¬çš„æ–‡æ¡£ç”Ÿæˆå™¨ï¼ˆç±»ä¼¼äº MDXï¼‰çš„ç‰¹å®šè¯­æ³•ï¼Œå¯èƒ½æ— æ³•åœ¨æ‚¨çš„ Markdown æŸ¥çœ‹å™¨ä¸­
æ­£ç¡®å‘ˆç°ã€‚rendered properly in your Markdown viewer.
-->
# æ··åˆè§†è§‰ Transformerï¼ˆViT Hybridï¼‰
## æ¦‚è¿°
æ··åˆè§†è§‰ Transformerï¼ˆViTï¼‰æ¨¡å‹æ˜¯ç”± Alexey Dosovitskiyã€Lucas Beyerã€Alexander Kolesnikovã€DirkWeissenbornã€Xiaohua Zhaiã€Thomas Unterthinerã€Mostafa Dehghaniã€Matthias Mindererã€Georg Heigoldã€Sylvain Gellyã€JakobUszkoreitã€Neil Houlsby åœ¨è®ºæ–‡ [An Image is Worth 16x16 Words: Transformers for Image Recognitionat Scale](https://arxiv.org/abs/2010.11929) ä¸­æå‡ºçš„ã€‚è¿™æ˜¯ç¬¬ä¸€ç¯‡æˆåŠŸåœ°åœ¨ ImageNet ä¸Šè®­ç»ƒ Transformer ç¼–ç å™¨å¹¶å–å¾—ä¸å¸¸è§çš„å·ç§¯æ¶æ„ç›¸æ¯”éå¸¸å¥½çš„ç»“æœçš„è®ºæ–‡ã€‚ViT Hybrid æ˜¯ [plain Vision Transformer](vit) çš„ä¸€ä¸ªå°å˜ä½“ï¼Œå®ƒåˆ©ç”¨äº†å·ç§¯ä¸»å¹²ï¼ˆå…·ä½“æ¥è¯´æ˜¯ [BiT](bit)ï¼‰çš„ç‰¹å¾ä½œä¸º Transformer çš„åˆå§‹â€œä»¤ç‰Œâ€ã€‚

è®ºæ–‡ä¸­çš„æ‘˜è¦å¦‚ä¸‹æ‰€ç¤ºï¼š
*å°½ç®¡ Transformer æ¶æ„å·²æˆä¸ºè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„äº‹å®æ ‡å‡†ï¼Œä½†å…¶åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚åœ¨è§†è§‰ä»»åŠ¡ä¸­ï¼Œæ³¨æ„åŠ›è¦ä¹ˆä¸å·ç§¯ç½‘ç»œä¸€èµ·ä½¿ç”¨ï¼Œè¦ä¹ˆç”¨æ¥æ›¿æ¢å·ç§¯ç½‘ç»œçš„æŸäº›ç»„ä»¶ï¼ŒåŒæ—¶ä¿æŒå…¶æ•´ä½“ç»“æ„ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œè¿™ç§å¯¹å·ç§¯ç½‘ç»œçš„ä¾èµ–å¹¶éå¿…éœ€ï¼Œç›´æ¥åº”ç”¨çº¯ Transformer åœ¨å›¾åƒå—åºåˆ—ä¸Šå¯ä»¥åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°éå¸¸å¥½ã€‚å½“åœ¨å¤§é‡æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶è¿ç§»åˆ°å¤šä¸ªä¸­å°å‹å›¾åƒè¯†åˆ«åŸºå‡†ï¼ˆImageNetã€CIFAR-100ã€VTAB ç­‰ï¼‰æ—¶ï¼ŒVision Transformerï¼ˆViTï¼‰ç›¸æ¯”æœ€å…ˆè¿›çš„å·ç§¯ç½‘ç»œå–å¾—äº†å‡ºè‰²çš„ç»“æœï¼ŒåŒæ—¶æ‰€éœ€çš„è®¡ç®—èµ„æºè¦å°‘å¾—å¤šã€‚* Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring
æ­¤æ¨¡å‹ç”± [nielsr](https://huggingface.co/nielsr) è´¡çŒ®ã€‚åŸå§‹ä»£ç ï¼ˆä½¿ç”¨ JAX ç¼–å†™ï¼‰å¯ä»¥åœ¨
This model was contributed by [nielsr](https://huggingface.co/nielsr). The original code (written in JAX) can be
æ­¤å¤„æ‰¾åˆ°ï¼ˆhttps://github.com/google-research/vision_transformerï¼‰ã€‚

## èµ„æº
ä»¥ä¸‹æ˜¯å®˜æ–¹ Hugging Face å’Œç¤¾åŒºï¼ˆç”± ğŸŒ æ ‡ç¤ºï¼‰èµ„æºçš„åˆ—è¡¨ï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨ ViT Hybridã€‚
<PipelineTag pipeline="image-classification"/>
- [`ViTHybridForImageClassification`] åœ¨æ­¤ [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) å’Œ [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb) ä¸­æä¾›æ”¯æŒã€‚- å¦è¯·å‚é˜…ï¼š[å›¾åƒåˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/image_classification)
å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æå‡ºæ‹‰å–è¯·æ±‚ï¼Œæˆ‘ä»¬å°†å¯¹å…¶è¿›è¡Œå®¡æŸ¥ï¼è¯¥èµ„æºåº”è¯¥å±•ç¤ºå‡ºæ–°çš„å†…å®¹ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰çš„èµ„æºã€‚

## ViTHybridConfig
[[autodoc]] ViTHybridConfig
## ViTHybridImageProcessor
[[autodoc]] ViTHybridImageProcessor    - preprocess
## ViTHybridModel
[[autodoc]] ViTHybridModel    - forward
## ViTHybridForImageClassification
[[autodoc]] ViTHybridForImageClassification    - forward