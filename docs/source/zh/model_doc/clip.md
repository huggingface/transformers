<!--ç‰ˆæƒæ‰€æœ‰2021å¹´HuggingFaceå›¢é˜Ÿã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚
æ ¹æ® Apache è®¸å¯è¯ç¬¬ 2.0 ç‰ˆï¼ˆâ€œè®¸å¯è¯â€ï¼‰è®¸å¯ï¼›é™¤éç¬¦åˆè®¸å¯è¯çš„è§„å®šï¼Œå¦åˆ™æ‚¨ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ä½ç½®è·å–è®¸å¯è¯çš„å‰¯æœ¬
http://www.apache.org/licenses/LICENSE-2.0
é™¤éé€‚ç”¨çš„æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œæ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯åŸºäºâ€œæŒ‰åŸæ ·â€åˆ†å‘çš„ï¼Œä¸é™„å¸¦ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„ä¿è¯æˆ–æ¡ä»¶ã€‚è¯·å‚é˜…è®¸å¯è¯ä»¥è·å–ç‰¹å®šè¯­è¨€ä¸‹çš„æƒé™å’Œé™åˆ¶ã€‚âš ï¸è¯·æ³¨æ„ï¼Œæ­¤æ–‡ä»¶æ˜¯ Markdown æ ¼å¼çš„ï¼Œä½†åŒ…å«æˆ‘ä»¬çš„æ–‡æ¡£æ„å»ºå™¨çš„ç‰¹å®šè¯­æ³•ï¼ˆç±»ä¼¼äº MDXï¼‰ï¼Œåœ¨æ‚¨çš„ Markdown æŸ¥çœ‹å™¨ä¸­å¯èƒ½æ— æ³•æ­£ç¡®æ¸²æŸ“ã€‚
-->

# CLIP

## æ¦‚è¿°

CLIP æ¨¡å‹æ˜¯ç”± Alec Radfordï¼ŒJong Wook Kimï¼ŒChris Hallacyï¼ŒAditya Rameshï¼ŒGabriel Gohï¼ŒSandhini Agarwalï¼ŒGirish Sastryï¼ŒAmanda Askellï¼ŒPamela Mishkinï¼ŒJack Clarkï¼ŒGretchen Kruegerï¼ŒIlya Sutskever åœ¨ [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) ä¸­æå‡ºçš„ã€‚CLIPï¼ˆå¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒï¼‰æ˜¯ä¸€ä¸ªåœ¨å„ç§ï¼ˆå›¾åƒï¼Œæ–‡æœ¬ï¼‰å¯¹ä¸Šè¿›è¡Œè®­ç»ƒçš„ç¥ç»ç½‘ç»œã€‚å®ƒå¯ä»¥æ ¹æ®è‡ªç„¶è¯­è¨€çš„æŒ‡ä»¤ï¼Œåœ¨ç»™å®šå›¾åƒçš„æƒ…å†µä¸‹é¢„æµ‹æœ€ç›¸å…³çš„æ–‡æœ¬ç‰‡æ®µï¼Œè€Œæ— éœ€ç›´æ¥é’ˆå¯¹ä»»åŠ¡è¿›è¡Œä¼˜åŒ–ï¼Œç±»ä¼¼äº GPT-2 å’Œ 3 çš„é›¶-shot èƒ½åŠ›ã€‚

æ¥è‡ªè®ºæ–‡çš„æ‘˜è¦å¦‚ä¸‹ï¼š

*æœ€å…ˆè¿›çš„è®¡ç®—æœºè§†è§‰ç³»ç»Ÿè¢«è®­ç»ƒä»¥é¢„æµ‹ä¸€ç»„å›ºå®šçš„é¢„å®šä¹‰å¯¹è±¡ç±»åˆ«ã€‚è¿™ç§å—é™å½¢å¼çš„ç›‘ç£é™åˆ¶äº†å®ƒä»¬çš„æ™®é€‚æ€§å’Œå¯ç”¨æ€§ï¼Œå› ä¸ºéœ€è¦é¢å¤–çš„å·²æ ‡è®°æ•°æ®æ¥æŒ‡å®šä»»ä½•å…¶ä»–è§†è§‰æ¦‚å¿µã€‚ç›´æ¥ä»åŸå§‹æ–‡æœ¬ä¸­å­¦ä¹ å…³äºå›¾åƒçš„å†…å®¹æ˜¯ä¸€ç§æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨äº†æ›´å¹¿æ³›çš„ç›‘ç£æ¥æºã€‚æˆ‘ä»¬è¯æ˜ï¼Œé¢„æµ‹å“ªä¸ªæ ‡é¢˜ä¸å“ªä¸ªå›¾åƒé…å¯¹çš„ç®€å•é¢„è®­ç»ƒä»»åŠ¡æ˜¯ä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„ä»å¤´å¼€å§‹å­¦ä¹  SOTA å›¾åƒè¡¨ç¤ºçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨äº†ä»äº’è”ç½‘æ”¶é›†çš„ 4 äº¿ä¸ªï¼ˆå›¾åƒï¼Œæ–‡æœ¬ï¼‰å¯¹çš„æ•°æ®é›†ã€‚åœ¨é¢„è®­ç»ƒä¹‹åï¼Œè‡ªç„¶è¯­è¨€ç”¨äºå¼•ç”¨å·²å­¦ä¹ çš„è§†è§‰æ¦‚å¿µï¼ˆæˆ–æè¿°æ–°çš„æ¦‚å¿µï¼‰ï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤Ÿé›¶-shot åœ°è¿ç§»åˆ°ä¸‹æ¸¸ä»»åŠ¡ã€‚æˆ‘ä»¬é€šè¿‡å¯¹è¶…è¿‡ 30 ä¸ªä¸åŒçš„ç°æœ‰è®¡ç®—æœºè§†è§‰æ•°æ®é›†è¿›è¡ŒåŸºå‡†æµ‹è¯•æ¥ç ”ç©¶æ­¤æ–¹æ³•çš„æ€§èƒ½ï¼Œè¿™äº›æ•°æ®é›†æ¶µç›–äº† OCRï¼Œè§†é¢‘ä¸­çš„åŠ¨ä½œè¯†åˆ«ï¼Œåœ°ç†å®šä½ä»¥åŠè®¸å¤šç±»å‹çš„ç»†ç²’åº¦å¯¹è±¡åˆ†ç±»ç­‰ä»»åŠ¡ã€‚è¯¥æ¨¡å‹åœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸­éƒ½èƒ½å¤Ÿè¿›è¡Œéå¹³å‡¡çš„è¿ç§»ï¼Œå¹¶ä¸”é€šå¸¸èƒ½å¤Ÿä¸å®Œå…¨ç›‘ç£çš„åŸºçº¿æ¨¡å‹ç«äº‰ï¼Œè€Œæ— éœ€è¿›è¡Œä»»ä½•ç‰¹å®šäºæ•°æ®é›†çš„è®­ç»ƒã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬åœ¨ ImageNet é›¶-shot ä»»åŠ¡ä¸ŠåŒ¹é…äº†åŸå§‹ ResNet-50 çš„å‡†ç¡®åº¦ï¼Œè€Œæ— éœ€ä½¿ç”¨å…¶è®­ç»ƒçš„ 128 ä¸‡ä¸ªæ ·æœ¬ä¹‹ä¸€ã€‚æˆ‘ä»¬åœ¨æ­¤ https URL ä¸Šå‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹æƒé‡ã€‚*

## ç”¨é€” 

CLIP æ˜¯ä¸€ç§å¤šæ¨¡æ€è§†è§‰å’Œè¯­è¨€æ¨¡å‹ã€‚å®ƒå¯ä»¥ç”¨äºå›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦å’Œé›¶-shot å›¾åƒåˆ†ç±»ã€‚CLIP ä½¿ç”¨ç±»ä¼¼ ViT çš„ Transformer è·å–è§†è§‰ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨å› æœè¯­è¨€æ¨¡å‹è·å–æ–‡æœ¬ç‰¹å¾ã€‚ç„¶åï¼Œæ–‡æœ¬å’Œè§†è§‰ç‰¹å¾éƒ½è¢«æŠ•å½±åˆ°å…·æœ‰ç›¸åŒç»´åº¦çš„æ½œç©ºé—´ä¸­ã€‚æŠ•å½±åçš„å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾ä¹‹é—´çš„ç‚¹ç§¯è¢«ç”¨ä½œç›¸ä¼¼åˆ†æ•°ã€‚

ä¸ºäº†å°†å›¾åƒé¦ˆé€åˆ° Transformer ç¼–ç å™¨ä¸­ï¼Œæ¯ä¸ªå›¾åƒè¢«åˆ†å‰²æˆä¸€ç³»åˆ—å›ºå®šå¤§å°ä¸”ä¸é‡å çš„è¡¥ä¸ï¼Œç„¶åè¿›è¡Œçº¿æ€§åµŒå…¥ã€‚æ·»åŠ ä¸€ä¸ª [CLS] ä»¤ç‰Œä½œä¸ºæ•´ä¸ªå›¾åƒçš„è¡¨ç¤ºã€‚ä½œè€…è¿˜æ·»åŠ äº†ç»å¯¹ä½ç½®åµŒå…¥ï¼Œå¹¶å°†æ‰€å¾—çš„å‘é‡åºåˆ—é¦ˆé€åˆ°æ ‡å‡† Transformer ç¼–ç å™¨ä¸­ã€‚

[`CLIPFeatureExtractor`] å¯ç”¨äºä¸ºæ¨¡å‹è°ƒæ•´å¤§å°ï¼ˆæˆ–é‡æ–°ç¼©æ”¾ï¼‰å’Œå½’ä¸€åŒ–å›¾åƒã€‚
[`CLIPTokenizer`] ç”¨äºå¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ã€‚[`CLIPProcessor`] å°† [`CLIPFeatureExtractor`] å’Œ [`CLIPTokenizer`] å°è£…ä¸ºå•ä¸ªå®ä¾‹ï¼Œæ—¢å¯ä»¥ç¼–ç æ–‡æœ¬ï¼Œåˆå¯ä»¥å‡†å¤‡å›¾åƒã€‚ä»¥ä¸‹ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ [`CLIPProcessor`] å’Œ [`CLIPModel`] è·å–å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦åˆ†æ•°ã€‚

```python
>>> from PIL import Image
>>> import requests

>>> from transformers import CLIPProcessor, CLIPModel

>>> model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
>>> processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(text=["a photo of a cat", "a photo of a dog"], images=image, return_tensors="pt", padding=True)

>>> outputs = model(**inputs)
>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
>>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities
```

æ­¤æ¨¡å‹ç”± [valhalla](https://huggingface.co/valhalla) è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯ä»¥åœ¨æ­¤å¤„æ‰¾åˆ°ï¼š[é“¾æ¥](https://github.com/openai/CLIP)ã€‚

## èµ„æº

ä»¥ä¸‹æ˜¯å®˜æ–¹ Hugging Face å’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨ CLIPã€‚

- ä¸€ç¯‡å…³äº [å¦‚ä½•åœ¨ 10,000 ä¸ªå›¾åƒ-æ–‡æœ¬å¯¹ä¸Šå¾®è°ƒ CLIP](https://huggingface.co/blog/fine-tune-clip-rsicd) çš„åšå®¢æ–‡ç« ã€‚
- CLIP ç”±æ­¤ [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/contrastive-image-text) æ”¯æŒã€‚

å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶å‘èµ· Pull Requestï¼Œæˆ‘ä»¬å°†å¯¹å…¶è¿›è¡Œå®¡æ ¸ã€‚èµ„æºåº”è¯¥å±•ç¤ºä¸€äº›æ–°ä¸œè¥¿ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰çš„å†…å®¹ã€‚
## CLIPConfig

[[autodoc]] CLIPConfig
    - from_text_vision_configs

## CLIPTextConfig

[[autodoc]] CLIPTextConfig

## CLIPVisionConfig

[[autodoc]] CLIPVisionConfig

## CLIPTokenizer

[[autodoc]] CLIPTokenizer
    - build_inputs_with_special_tokens
    - get_special_tokens_mask
    - create_token_type_ids_from_sequences
    - save_vocabulary

## CLIPTokenizerFast

[[autodoc]] CLIPTokenizerFast

## CLIPImageProcessor

[[autodoc]] CLIPImageProcessor
    - preprocess

## CLIPFeatureExtractor

[[autodoc]] CLIPFeatureExtractor

## CLIPProcessor

[[autodoc]] CLIPProcessor

## CLIPModel

[[autodoc]] CLIPModel
    - forward
    - get_text_features
    - get_image_features

## CLIPTextModel

[[autodoc]] CLIPTextModel
    - forward

## CLIPTextModelWithProjection

[[autodoc]] CLIPTextModelWithProjection
    - forward

## CLIPVisionModelWithProjection

[[autodoc]] CLIPVisionModelWithProjection
    - forward


## CLIPVisionModel

[[autodoc]] CLIPVisionModel
    - forward

## TFCLIPModel

[[autodoc]] TFCLIPModel
    - call
    - get_text_features
    - get_image_features

## TFCLIPTextModel

[[autodoc]] TFCLIPTextModel
    - call

## TFCLIPVisionModel

[[autodoc]] TFCLIPVisionModel
    - call

## FlaxCLIPModel

[[autodoc]] FlaxCLIPModel
    - __call__
    - get_text_features
    - get_image_features

## FlaxCLIPTextModel

[[autodoc]] FlaxCLIPTextModel
    - __call__

## FlaxCLIPVisionModel

[[autodoc]] FlaxCLIPVisionModel
    - __call__
