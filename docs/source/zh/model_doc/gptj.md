<!--ç‰ˆæƒæ‰€æœ‰ 2021 å¹´ HuggingFace å›¢é˜Ÿã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚
æ ¹æ® Apache Licenseï¼ŒVersion 2.0ï¼ˆâ€œè®¸å¯è¯â€ï¼‰æˆæƒï¼›é™¤éç¬¦åˆè®¸å¯è¯çš„è§„å®šï¼Œå¦åˆ™ç¦æ­¢ä½¿ç”¨æœ¬æ–‡ä»¶ã€‚æ‚¨å¯ä»¥åœ¨
http://www.apache.org/licenses/LICENSE-2.0
è·å–è®¸å¯è¯çš„å‰¯æœ¬ã€‚é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œæ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯åŸºäº "AS IS" çš„åŸºç¡€ä¸Šæä¾›çš„ï¼Œæ— è®ºæ˜¯æ˜ç¤ºæˆ–æš—ç¤ºçš„ä¿è¯æˆ–æ¡ä»¶ã€‚è¯·å‚é˜…è®¸å¯è¯ä»¥äº†è§£
ç‰¹å®šè¯­è¨€ä¸‹çš„æƒé™å’Œé™åˆ¶ã€‚æ­£ç¡®åœ°æ¸²æŸ“ã€‚
-->
# GPT-J

## æ¦‚è¿°

GPT-J æ¨¡å‹æ˜¯ç”± Ben Wang å’Œ Aran Komatsuzaki åœ¨ [kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax) ä»£ç åº“ä¸­å‘å¸ƒçš„ã€‚å®ƒæ˜¯ä¸€ä¸ªç±»ä¼¼äº GPT-2 çš„å› æœè¯­è¨€æ¨¡å‹ï¼Œä½¿ç”¨äº† [Pile](https://pile.eleuther.ai/) æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚

æ­¤æ¨¡å‹ç”± [Stella Biderman](https://huggingface.co/stellaathena) è´¡çŒ®ã€‚

æç¤ºï¼š

- è¦åŠ è½½ [float32 æ ¼å¼çš„ GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B)ï¼Œè‡³å°‘éœ€è¦ 2 å€çš„æ¨¡å‹å¤§å°çš„  RAMï¼š1 å€ç”¨äºåˆå§‹åŒ–æƒé‡ï¼Œå¦å¤– 1 å€ç”¨äºåŠ è½½æ£€æŸ¥ç‚¹ã€‚å› æ­¤ï¼Œå¯¹äº GPT-J æ¨¡å‹ï¼Œè‡³å°‘éœ€è¦ 48GB çš„  RAM æ‰èƒ½åŠ è½½æ¨¡å‹ã€‚ä¸ºäº†å‡å°‘ RAM çš„ä½¿ç”¨é‡ï¼Œæœ‰å‡ ä¸ªé€‰é¡¹ã€‚`torch_dtype` å‚æ•°å¯ä»¥  ç”¨äºä»…åœ¨ CUDA è®¾å¤‡ä¸Šä»¥åŠç²¾åº¦åˆå§‹åŒ–æ¨¡å‹ã€‚è¿˜æœ‰ä¸€ä¸ª fp16 åˆ†æ”¯ï¼Œç”¨äºå­˜å‚¨ fp16 æƒé‡ï¼Œ  å¯ä»¥è¿›ä¸€æ­¥å‡å°‘ RAM çš„ä½¿ç”¨é‡ï¼š
```python
>>> from transformers import GPTJForCausalLM
>>> import torch

>>> device = "cuda"
>>> model = GPTJForCausalLM.from_pretrained(
...     "EleutherAI/gpt-j-6B",
...     revision="float16",
...     torch_dtype=torch.float16,
... ).to(device)
```

- æ¨¡å‹åœ¨æ¨ç†æ—¶åº”é€‚åˆ 16GB çš„ GPUã€‚å¯¹äºè®­ç»ƒ/å¾®è°ƒï¼Œéœ€è¦æ›´å¤šçš„ GPU RAMã€‚ä¾‹å¦‚ï¼ŒAdam  ä¼˜åŒ–å™¨ä¼šåˆ›å»ºæ¨¡å‹çš„å››ä¸ªå‰¯æœ¬ï¼šæ¨¡å‹ã€æ¢¯åº¦ã€æ¢¯åº¦çš„å¹³å‡å€¼å’Œå¹³æ–¹å¹³å‡å€¼ã€‚  å› æ­¤ï¼Œå³ä½¿ä½¿ç”¨æ··åˆç²¾åº¦ï¼Œæ¢¯åº¦æ›´æ–°ä¹Ÿæ˜¯ fp32 æ ¼å¼ï¼Œæ‰€ä»¥è‡³å°‘éœ€è¦ 4 å€æ¨¡å‹å¤§å°çš„ GPU å†…å­˜ã€‚  è¿™è¿˜ä¸åŒ…æ‹¬æ¿€æ´»å’Œæ•°æ®æ‰¹æ¬¡ï¼Œè¿™äº›ä¹Ÿéœ€è¦é¢å¤–çš„ GPU RAMã€‚å› æ­¤ï¼Œåº”è¯¥æ¢ç´¢  ä¸€äº›è§£å†³æ–¹æ¡ˆï¼Œä¾‹å¦‚ä½¿ç”¨ DeepSpeed æ¥è®­ç»ƒ/å¾®è°ƒæ¨¡å‹ã€‚

å¦ä¸€ä¸ªé€‰æ‹©æ˜¯ä½¿ç”¨åŸå§‹ä»£ç åº“  åœ¨ TPU ä¸Šè®­ç»ƒ/å¾®è°ƒæ¨¡å‹ï¼Œç„¶åå°†æ¨¡å‹è½¬æ¢ä¸º Transformers æ ¼å¼è¿›è¡Œæ¨ç†ã€‚æœ‰å…³  æ­¤è¿‡ç¨‹çš„è¯´æ˜å¯ä»¥åœ¨ [è¿™é‡Œæ‰¾åˆ°](https://github.com/kingoflolz/mesh-transformer-jax/blob/master/howto_finetune.md)
- å°½ç®¡åµŒå…¥çŸ©é˜µçš„å¤§å°ä¸º 50400ï¼Œä½† GPT-2 åˆ†è¯å™¨ (Tokenizer)ä»…ä½¿ç”¨ 50257 ä¸ªæ¡ç›®ã€‚ä¸ºäº†åœ¨ TPU ä¸Šæé«˜æ•ˆç‡ï¼Œ  æ·»åŠ äº†è¿™äº›é¢å¤–çš„ä»¤ç‰Œã€‚ä¸ºäº†é¿å…åµŒå…¥çŸ©é˜µå¤§å°ä¸è¯æ±‡è¡¨å¤§å°ä¸åŒ¹é…ï¼Œ  [GPT-J åˆ†è¯å™¨ (Tokenizer)](https://huggingface.co/EleutherAI/gpt-j-6B) åŒ…å«äº†é¢å¤–çš„ 143 ä¸ªä»¤ç‰Œ  `<|extratoken_1|>...<|extratoken_143|>`ï¼Œå› æ­¤åˆ†è¯å™¨ (Tokenizer)çš„ `vocab_size` ä¹Ÿå˜ä¸ºäº† 50400ã€‚

### ç”Ÿæˆ

å¯ä»¥ä½¿ç”¨ [`~generation.GenerationMixin.generate`] æ–¹æ³•ä½¿ç”¨ GPT-J æ¨¡å‹ç”Ÿæˆæ–‡æœ¬ã€‚
```python
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> model = AutoModelForCausalLM.from_pretrained("EleutherAI/gpt-j-6B")
>>> tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-j-6B")

>>> prompt = (
...     "In a shocking finding, scientists discovered a herd of unicorns living in a remote, "
...     "previously unexplored valley, in the Andes Mountains. Even more surprising to the "
...     "researchers was the fact that the unicorns spoke perfect English."
... )

>>> input_ids = tokenizer(prompt, return_tensors="pt").input_ids

>>> gen_tokens = model.generate(
...     input_ids,
...     do_sample=True,
...     temperature=0.9,
...     max_length=100,
... )
>>> gen_text = tokenizer.batch_decode(gen_tokens)[0]
```

...æˆ–è€…ä½¿ç”¨ float16 ç²¾åº¦ï¼š
```python
>>> from transformers import GPTJForCausalLM, AutoTokenizer
>>> import torch

>>> device = "cuda"
>>> model = GPTJForCausalLM.from_pretrained("EleutherAI/gpt-j-6B", torch_dtype=torch.float16).to(device)
>>> tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-j-6B")

>>> prompt = (
...     "In a shocking finding, scientists discovered a herd of unicorns living in a remote, "
...     "previously unexplored valley, in the Andes Mountains. Even more surprising to the "
...     "researchers was the fact that the unicorns spoke perfect English."
... )

>>> input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(device)

>>> gen_tokens = model.generate(
...     input_ids,
...     do_sample=True,
...     temperature=0.9,
...     max_length=100,
... )
>>> gen_text = tokenizer.batch_decode(gen_tokens)[0]
```

## èµ„æº

ä»¥ä¸‹æ˜¯å®˜æ–¹ Hugging Face å’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºçš„åˆ—è¡¨ï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨ GPT-Jã€‚å¦‚æœæ‚¨æœ‰å…´è¶£
æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æå‡º Pull Requestï¼Œæˆ‘ä»¬å°†å¯¹å…¶è¿›è¡Œå®¡æ ¸ï¼èµ„æºåº”è¯¥
å±•ç¤ºä¸€äº›æ–°çš„ä¸œè¥¿ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚- [GPT-J çš„æè¿°](https://huggingface.co/EleutherAI/gpt-j-6B)- æœ‰å…³å¦‚ä½• [ä½¿ç”¨ Hugging Face Transformers å’Œ Amazon SageMaker éƒ¨ç½² GPT-J 6B è¿›è¡Œæ¨ç†](https://huggingface.co/blog/gptj-sagemaker) çš„åšå®¢æ–‡ç« 
- ä»‹ç» [GPT-J-6B: 6B JAX-Based Transformer](https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/) çš„åšå®¢æ–‡ç«  ğŸŒ
- [GPT-J-6B æ¨ç†æ¼”ç¤ºçš„ Notebook](https://colab.research.google.com/github/kingoflolz/mesh-transformer-jax/blob/master/colab_demo.ipynb) ğŸŒ
- å¦ä¸€ä¸ªæ¼”ç¤º [GPT-J-6B æ¨ç†çš„ Notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/GPT-J-6B/Inference_with_GPT_J_6B.ipynb)
- ğŸ¤— Hugging Face è¯¾ç¨‹ä¸­çš„ [å› æœè¯­è¨€å»ºæ¨¡](https://huggingface.co/course/en/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch) ç« èŠ‚
- [`GPTJForCausalLM`] æ”¯æŒä½¿ç”¨æ­¤ [å› æœè¯­è¨€å»ºæ¨¡ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#gpt-2gpt-and-causal-language-modeling)ã€[æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-generation) å’Œ [Notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)
- [`TFGPTJForCausalLM`] æ”¯æŒä½¿ç”¨æ­¤ [å› æœè¯­è¨€å»ºæ¨¡ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_clmpy) å’Œ [Notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)
- [`FlaxGPTJForCausalLM`] æ”¯æŒä½¿ç”¨æ­¤ [å› æœè¯­è¨€å»ºæ¨¡ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#causal-language-modeling) å’Œ [Notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/causal_language_modeling_flax.ipynb)

**æ–‡æ¡£èµ„æº**
- [æ–‡æœ¬åˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/sequence_classification)
- [é—®ç­”ä»»åŠ¡æŒ‡å—](../tasks/question_answering)
- [å› æœè¯­è¨€å»ºæ¨¡ä»»åŠ¡æŒ‡å—](../tasks/language_modeling)

## GPTJConfig

[[autodoc]] GPTJConfig
    - all

## GPTJModel

[[autodoc]] GPTJModel
    - forward

## GPTJForCausalLM

[[autodoc]] GPTJForCausalLM
    - forward

## GPTJForSequenceClassification

[[autodoc]] GPTJForSequenceClassification
    - forward

## GPTJForQuestionAnswering

[[autodoc]] GPTJForQuestionAnswering
    - forward

## TFGPTJModel

[[autodoc]] TFGPTJModel
    - call

## TFGPTJForCausalLM

[[autodoc]] TFGPTJForCausalLM
    - call

## TFGPTJForSequenceClassification

[[autodoc]] TFGPTJForSequenceClassification
    - call

## TFGPTJForQuestionAnswering

[[autodoc]] TFGPTJForQuestionAnswering
    - call

## FlaxGPTJModel

[[autodoc]] FlaxGPTJModel
    - __call__

## FlaxGPTJForCausalLM

[[autodoc]] FlaxGPTJForCausalLM
    - __call__
