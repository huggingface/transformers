<!--ç‰ˆæƒæ‰€æœ‰ 2020 å¹´ The HuggingFace å›¢é˜Ÿã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚
æ ¹æ® Apache è®¸å¯è¯ï¼Œç¬¬ 2.0 ç‰ˆï¼ˆâ€œè®¸å¯è¯â€ï¼‰è·å¾—è®¸å¯ï¼›é™¤éç¬¦åˆè®¸å¯è¯çš„è§„å®šï¼Œå¦åˆ™ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ä½ç½®è·å–è®¸å¯è¯çš„å‰¯æœ¬
http://www.apache.org/licenses/LICENSE-2.0
é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œæ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯æŒ‰ç…§â€œåŸæ ·â€åŸºç¡€åˆ†å‘çš„ï¼Œä¸æä¾›ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„æ‹…ä¿æˆ–æ¡ä»¶ã€‚è¯·å‚é˜…è®¸å¯è¯ä»¥äº†è§£ç‰¹å®šè¯­è¨€ä¸‹çš„æƒé™å’Œé™åˆ¶ã€‚
âš ï¸ è¯·æ³¨æ„ï¼Œæ­¤æ–‡ä»¶ä¸º Markdown æ ¼å¼ï¼Œä½†åŒ…å«ç‰¹å®šçš„è¯­æ³•ä»¥ä¾›æˆ‘ä»¬çš„æ–‡æ¡£æ„å»ºå™¨ï¼ˆç±»ä¼¼äº MDXï¼‰ä½¿ç”¨ï¼Œè¿™å¯èƒ½æ— æ³•åœ¨æ‚¨çš„ Markdown æŸ¥çœ‹å™¨ä¸­æ­£ç¡®æ¸²æŸ“ã€‚
-->

# ä½¿ç”¨ğŸ¤— Tokenizers

[`PreTrainedTokenizerFast`] ä¾èµ–äº [ğŸ¤— Tokenizers](https://huggingface.co/docs/tokenizers) åº“ã€‚ä» ğŸ¤— Tokenizers åº“ä¸­è·å–çš„ tokenizer å¯ä»¥éå¸¸ç®€å•åœ°åŠ è½½åˆ° ğŸ¤— Transformers ä¸­ã€‚

åœ¨è¿›å…¥è¯¦ç»†ä¿¡æ¯ä¹‹å‰ï¼Œè®©æˆ‘ä»¬é¦–å…ˆé€šè¿‡å‡ è¡Œä»£ç åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿ tokenizerï¼š
```python
>>> from tokenizers import Tokenizer
>>> from tokenizers.models import BPE
>>> from tokenizers.trainers import BpeTrainer
>>> from tokenizers.pre_tokenizers import Whitespace

>>> tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
>>> trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])

>>> tokenizer.pre_tokenizer = Whitespace()
>>> files = [...]
>>> tokenizer.train(files, trainer)
```

ç°åœ¨æˆ‘ä»¬æœ‰ä¸€ä¸ªåœ¨æˆ‘ä»¬å®šä¹‰çš„æ–‡ä»¶ä¸Šè®­ç»ƒçš„ tokenizerã€‚æˆ‘ä»¬å¯ä»¥åœ¨è¯¥è¿è¡Œæ—¶ç»§ç»­ä½¿ç”¨å®ƒï¼Œä¹Ÿå¯ä»¥å°†å…¶ä¿å­˜åˆ° JSON æ–‡ä»¶ä¸­ä»¥ä¾›å°†æ¥é‡ç”¨ã€‚

## ç›´æ¥ä» tokenizer å¯¹è±¡åŠ è½½

è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åœ¨ğŸ¤— Transformers åº“ä¸­åˆ©ç”¨è¿™ä¸ª tokenizer å¯¹è±¡ã€‚[`PreTrainedTokenizerFast`] ç±»å…è®¸é€šè¿‡æ¥å—å®ä¾‹åŒ–çš„ *tokenizer* å¯¹è±¡ä½œä¸ºå‚æ•°æ¥è½»æ¾å®ä¾‹åŒ–ï¼š

```python
>>> from transformers import PreTrainedTokenizerFast

>>> fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)
```

ç°åœ¨ï¼Œè¿™ä¸ªå¯¹è±¡å¯ä»¥ä¸ ğŸ¤— Transformers tokenizers å…±äº«çš„æ‰€æœ‰æ–¹æ³•ä¸€èµ·ä½¿ç”¨ï¼è¯·è®¿é—® [tokenizer é¡µé¢](main_classes/tokenizer) äº†è§£æ›´å¤šä¿¡æ¯ã€‚

## ä» JSON æ–‡ä»¶åŠ è½½

ä¸ºäº†ä» JSON æ–‡ä»¶åŠ è½½ tokenizerï¼Œè®©æˆ‘ä»¬é¦–å…ˆä¿å­˜æˆ‘ä»¬çš„ tokenizerï¼š

```python
>>> tokenizer.save("tokenizer.json")
```

æˆ‘ä»¬ä¿å­˜æ­¤æ–‡ä»¶çš„è·¯å¾„å¯ä»¥é€šè¿‡ [`PreTrainedTokenizerFast`] åˆå§‹åŒ–æ–¹æ³•çš„ `tokenizer_file` å‚æ•°ä¼ é€’ï¼š
```python
>>> from transformers import PreTrainedTokenizerFast

>>> fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file="tokenizer.json")
```

ç°åœ¨ï¼Œè¿™ä¸ªå¯¹è±¡å¯ä»¥ä¸ ğŸ¤— Transformers tokenizers å…±äº«çš„æ‰€æœ‰æ–¹æ³•ä¸€èµ·ä½¿ç”¨ï¼è¯·è®¿é—® [tokenizer é¡µé¢](main_classes/tokenizer) äº†è§£æ›´å¤šä¿¡æ¯ã€‚