<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# åŸºå‡†æµ‹è¯•

<Tip warning={true}>

å°æç¤ºï¼šHugging Faceçš„åŸºå‡†æµ‹è¯•å·¥å…·å·²ç»ä¸å†æ›´æ–°ï¼Œå»ºè®®ä½¿ç”¨å¤–éƒ¨åŸºå‡†æµ‹è¯•åº“æ¥è¡¡é‡Transformeræ¨¡
å‹çš„é€Ÿåº¦å’Œå†…å­˜å¤æ‚åº¦ã€‚

</Tip>

[[open-in-colab]]

è®©æˆ‘ä»¬æ¥çœ‹çœ‹å¦‚ä½•å¯¹ğŸ¤— Transformersæ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œä»¥åŠè¿›è¡Œæµ‹è¯•çš„æ¨èç­–ç•¥å’Œå·²æœ‰çš„åŸºå‡†æµ‹è¯•ç»“æœã€‚

å¦‚æœä½ éœ€è¦æ›´è¯¦ç»†çš„å›ç­”ï¼Œå¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°å…³äºå¦‚ä½•å¯¹ğŸ¤— Transformersæ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•çš„[è¯´æ˜](https://github.com/huggingface/notebooks/tree/main/examples/benchmark.ipynb)ã€‚

<!-- Let's take a look at how ğŸ¤— Transformers models can be benchmarked, best practices, and already available benchmarks.

A notebook explaining in more detail how to benchmark ğŸ¤— Transformers models can be found [here](https://github.com/huggingface/notebooks/tree/main/examples/benchmark.ipynb). -->

<!-- ## How to benchmark ğŸ¤— Transformers models -->
## å¦‚ä½•å¯¹ğŸ¤— Transformersæ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•

<!-- The classes [`PyTorchBenchmark`] and [`TensorFlowBenchmark`] allow to flexibly benchmark ğŸ¤— Transformers models. The benchmark classes allow us to measure the _peak memory usage_ and _required time_ for both _inference_ and _training_. -->
ä½¿ç”¨[`PyTorchBenchmark`]å’Œ[`TensorFlowBenchmark`]ç±»å¯ä»¥çµæ´»åœ°å¯¹ğŸ¤— Transformersæ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚è¿™äº›åŸºå‡†æµ‹è¯•ç±»å¯ä»¥è¡¡é‡æ¨¡å‹åœ¨**æ¨ç†**å’Œ**è®­ç»ƒ**è¿‡ç¨‹ä¸­æ‰€éœ€çš„**å³°å€¼å†…å­˜**å’Œ**æ—¶é—´**ã€‚

<Tip>

<!-- Here, _inference_ is defined by a single forward pass, and _training_ is defined by a single forward pass and
backward pass. -->
è¿™é‡Œçš„**æ¨ç†**æŒ‡çš„æ˜¯ä¸€æ¬¡å‰å‘ä¼ æ’­(forward pass)ï¼Œè®­ç»ƒåˆ™æŒ‡çš„æ˜¯ä¸€æ¬¡å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­(backward pass)ã€‚

</Tip>

<!-- The benchmark classes [`PyTorchBenchmark`] and [`TensorFlowBenchmark`] expect an object of type [`PyTorchBenchmarkArguments`] and
[`TensorFlowBenchmarkArguments`], respectively, for instantiation. [`PyTorchBenchmarkArguments`] and [`TensorFlowBenchmarkArguments`] are data classes and contain all relevant configurations for their corresponding benchmark class. In the following example, it is shown how a BERT model of type _bert-base-cased_ can be benchmarked. -->

åŸºå‡†æµ‹è¯•ç±» [`PyTorchBenchmark`] å’Œ [`TensorFlowBenchmark`] éœ€è¦åˆ†åˆ«ä¼ å…¥ [`PyTorchBenchmarkArguments`] å’Œ [`TensorFlowBenchmarkArguments`] ç±»å‹çš„å¯¹è±¡æ¥è¿›è¡Œå®ä¾‹åŒ–ã€‚è¿™äº›ç±»æ˜¯æ•°æ®ç±»å‹ï¼ŒåŒ…å«äº†æ‰€æœ‰ç›¸å…³çš„é…ç½®å‚æ•°ï¼Œç”¨äºå…¶å¯¹åº”çš„åŸºå‡†æµ‹è¯•ç±»ã€‚

åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•å¯¹ç±»å‹ä¸º **bert-base-cased** çš„BERTæ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼š

<frameworkcontent>
<pt>
```py
>>> from transformers import PyTorchBenchmark, PyTorchBenchmarkArguments

>>> args = PyTorchBenchmarkArguments(models=["google-bert/bert-base-uncased"], batch_sizes=[8], sequence_lengths=[8, 32, 128, 512])
>>> benchmark = PyTorchBenchmark(args)
```
</pt>
<tf>
```py
>>> from transformers import TensorFlowBenchmark, TensorFlowBenchmarkArguments

>>> args = TensorFlowBenchmarkArguments(
...     models=["google-bert/bert-base-uncased"], batch_sizes=[8], sequence_lengths=[8, 32, 128, 512]
... )
>>> benchmark = TensorFlowBenchmark(args)
```
</tf>
</frameworkcontent>

åœ¨è¿™é‡Œï¼ŒåŸºå‡†æµ‹è¯•çš„å‚æ•°æ•°æ®ç±»æ¥å—äº†ä¸‰ä¸ªä¸»è¦çš„å‚æ•°ï¼Œå³ `models`ã€`batch_sizes` å’Œ`sequence_lengths`ã€‚å…¶ä¸­ï¼Œ`models` æ˜¯å¿…éœ€çš„å‚æ•°ï¼Œå®ƒæœŸæœ›ä¸€ä¸ªæ¥è‡ª[æ¨¡å‹åº“](https://huggingface.co/models)çš„æ¨¡å‹æ ‡è¯†ç¬¦åˆ—è¡¨ã€‚`batch_sizes` å’Œ `sequence_lengths` æ˜¯åˆ—è¡¨ç±»å‹çš„å‚æ•°ï¼Œå®šä¹‰äº†è¿›è¡ŒåŸºå‡†æµ‹è¯•æ—¶ `input_ids` çš„æ‰¹é‡å¤§å°å’Œåºåˆ—é•¿åº¦ã€‚

è¿™äº›æ˜¯åŸºå‡†æµ‹è¯•æ•°æ®ç±»ä¸­å¯ä»¥é…ç½®çš„ä¸€äº›ä¸»è¦å‚æ•°ã€‚é™¤æ­¤ä¹‹å¤–ï¼ŒåŸºå‡†æµ‹è¯•æ•°æ®ç±»ä¸­è¿˜å¯ä»¥é…ç½®å¾ˆå¤šå…¶ä»–å‚æ•°ã€‚å¦‚éœ€è¦æŸ¥çœ‹æ›´è¯¦ç»†çš„é…ç½®å‚æ•°ï¼Œå¯ä»¥ç›´æ¥æŸ¥çœ‹ä»¥ä¸‹æ–‡ä»¶ï¼š

* `src/transformers/benchmark/benchmark_args_utils.py`
* `src/transformers/benchmark/benchmark_args.py`ï¼ˆé’ˆå¯¹ PyTorchï¼‰
* `src/transformers/benchmark/benchmark_args_tf.py`ï¼ˆé’ˆå¯¹ TensorFlowï¼‰
  
å¦å¤–ï¼Œæ‚¨è¿˜å¯ä»¥é€šè¿‡åœ¨æ ¹ç›®å½•ä¸‹è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼ŒæŸ¥çœ‹é’ˆå¯¹ PyTorch å’Œ TensorFlow çš„æ‰€æœ‰å¯é…ç½®å‚æ•°çš„æè¿°åˆ—è¡¨ï¼š
``` bash python examples/pytorch/benchmarking/run_benchmark.py --help ```
è¿™äº›å‘½ä»¤å°†åˆ—å‡ºæ‰€æœ‰å¯ä»¥é…ç½®çš„å‚æ•°ã€‚æœ‰äº†è¿™äº›å·¥å…·çš„å¸®åŠ©ï¼Œæ‚¨å¯ä»¥æ›´åŠ çµæ´»åœ°è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚


<!-- Here, three arguments are given to the benchmark argument data classes, namely `models`, `batch_sizes`, and
`sequence_lengths`. The argument `models` is required and expects a `list` of model identifiers from the
[model hub](https://huggingface.co/models) The `list` arguments `batch_sizes` and `sequence_lengths` define
the size of the `input_ids` on which the model is benchmarked. There are many more parameters that can be configured
via the benchmark argument data classes. For more detail on these one can either directly consult the files
`src/transformers/benchmark/benchmark_args_utils.py`, `src/transformers/benchmark/benchmark_args.py` (for PyTorch)
and `src/transformers/benchmark/benchmark_args_tf.py` (for Tensorflow). Alternatively, running the following shell
commands from root will print out a descriptive list of all configurable parameters for PyTorch and Tensorflow
respectively. -->

<frameworkcontent>
<pt>
<!-- ```bash
python examples/pytorch/benchmarking/run_benchmark.py --help
``` -->

<!-- An instantiated benchmark object can then simply be run by calling `benchmark.run()`. -->

ä»¥ä¸‹ä»£ç é€šè¿‡`PyTorchBenchmarkArguments`è®¾ç½®æ¨¡å‹æ‰¹å¤„ç†å¤§å°å’Œåºåˆ—é•¿åº¦ï¼Œç„¶åè°ƒç”¨`benchmark.run()`æ‰§è¡ŒåŸºå‡†æµ‹è¯•ã€‚

```py
>>> results = benchmark.run()
>>> print(results)
====================       INFERENCE - SPEED - RESULT       ====================
--------------------------------------------------------------------------------
Model Name             Batch Size     Seq Length     Time in s                  
--------------------------------------------------------------------------------
google-bert/bert-base-uncased          8               8             0.006     
google-bert/bert-base-uncased          8               32            0.006     
google-bert/bert-base-uncased          8              128            0.018     
google-bert/bert-base-uncased          8              512            0.088     
--------------------------------------------------------------------------------

====================      INFERENCE - MEMORY - RESULT       ====================
--------------------------------------------------------------------------------
Model Name             Batch Size     Seq Length    Memory in MB 
--------------------------------------------------------------------------------
google-bert/bert-base-uncased          8               8             1227
google-bert/bert-base-uncased          8               32            1281
google-bert/bert-base-uncased          8              128            1307
google-bert/bert-base-uncased          8              512            1539
--------------------------------------------------------------------------------

====================        ENVIRONMENT INFORMATION         ====================

- transformers_version: 2.11.0
- framework: PyTorch
- use_torchscript: False
- framework_version: 1.4.0
- python_version: 3.6.10
- system: Linux
- cpu: x86_64
- architecture: 64bit
- date: 2020-06-29
- time: 08:58:43.371351
- fp16: False
- use_multiprocessing: True
- only_pretrain_model: False
- cpu_ram_mb: 32088
- use_gpu: True
- num_gpus: 1
- gpu: TITAN RTX
- gpu_ram_mb: 24217
- gpu_power_watts: 280.0
- gpu_performance_state: 2
- use_tpu: False
```
</pt>
<tf>
```bash
python examples/tensorflow/benchmarking/run_benchmark_tf.py --help
```

<!-- An instantiated benchmark object can then simply be run by calling `benchmark.run()`. -->
æ¥ä¸‹æ¥ï¼Œåªéœ€è¦è°ƒç”¨ `benchmark.run()` å°±èƒ½è½»æ¾è¿è¡Œå·²ç»å®ä¾‹åŒ–çš„åŸºå‡†æµ‹è¯•å¯¹è±¡ã€‚

```py
>>> results = benchmark.run()
>>> print(results)
>>> results = benchmark.run()
>>> print(results)
====================       INFERENCE - SPEED - RESULT       ====================
--------------------------------------------------------------------------------
Model Name             Batch Size     Seq Length     Time in s                  
--------------------------------------------------------------------------------
google-bert/bert-base-uncased          8               8             0.005
google-bert/bert-base-uncased          8               32            0.008
google-bert/bert-base-uncased          8              128            0.022
google-bert/bert-base-uncased          8              512            0.105
--------------------------------------------------------------------------------

====================      INFERENCE - MEMORY - RESULT       ====================
--------------------------------------------------------------------------------
Model Name             Batch Size     Seq Length    Memory in MB 
--------------------------------------------------------------------------------
google-bert/bert-base-uncased          8               8             1330
google-bert/bert-base-uncased          8               32            1330
google-bert/bert-base-uncased          8              128            1330
google-bert/bert-base-uncased          8              512            1770
--------------------------------------------------------------------------------

====================        ENVIRONMENT INFORMATION         ====================

- transformers_version: 2.11.0
- framework: Tensorflow
- use_xla: False
- framework_version: 2.2.0
- python_version: 3.6.10
- system: Linux
- cpu: x86_64
- architecture: 64bit
- date: 2020-06-29
- time: 09:26:35.617317
- fp16: False
- use_multiprocessing: True
- only_pretrain_model: False
- cpu_ram_mb: 32088
- use_gpu: True
- num_gpus: 1
- gpu: TITAN RTX
- gpu_ram_mb: 24217
- gpu_power_watts: 280.0
- gpu_performance_state: 2
- use_tpu: False
```
</tf>
</frameworkcontent>

<!-- By default, the _time_ and the _required memory_ for _inference_ are benchmarked. In the example output above the first
two sections show the result corresponding to _inference time_ and _inference memory_. In addition, all relevant
information about the computing environment, _e.g._ the GPU type, the system, the library versions, etc... are printed
out in the third section under _ENVIRONMENT INFORMATION_. This information can optionally be saved in a _.csv_ file
when adding the argument `save_to_csv=True` to [`PyTorchBenchmarkArguments`] and
[`TensorFlowBenchmarkArguments`] respectively. In this case, every section is saved in a separate
_.csv_ file. The path to each _.csv_ file can optionally be defined via the argument data classes. -->


åœ¨ä¸€èˆ¬æƒ…å†µä¸‹ï¼ŒåŸºå‡†æµ‹è¯•ä¼šæµ‹é‡æ¨ç†ï¼ˆinferenceï¼‰çš„**æ—¶é—´**å’Œ**æ‰€éœ€å†…å­˜**ã€‚åœ¨ä¸Šé¢çš„ç¤ºä¾‹è¾“å‡ºä¸­ï¼Œå‰ä¸¤éƒ¨åˆ†æ˜¾ç¤ºäº†ä¸**æ¨ç†æ—¶é—´**å’Œ**æ¨ç†å†…å­˜**å¯¹åº”çš„ç»“æœã€‚ä¸æ­¤åŒæ—¶ï¼Œå…³äºè®¡ç®—ç¯å¢ƒçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ï¼ˆä¾‹å¦‚ GPU ç±»å‹ã€ç³»ç»Ÿã€åº“ç‰ˆæœ¬ç­‰ï¼‰ä¼šåœ¨ç¬¬ä¸‰éƒ¨åˆ†çš„**ç¯å¢ƒä¿¡æ¯**ä¸­æ‰“å°å‡ºæ¥ã€‚ä½ å¯ä»¥é€šè¿‡åœ¨ [`PyTorchBenchmarkArguments`] å’Œ [`TensorFlowBenchmarkArguments`] ä¸­æ·»åŠ  `save_to_csv=True`å‚æ•°ï¼Œå°†è¿™äº›ä¿¡æ¯ä¿å­˜åˆ°ä¸€ä¸ª .csv æ–‡ä»¶ä¸­ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¯ä¸€éƒ¨åˆ†çš„ä¿¡æ¯ä¼šåˆ†åˆ«ä¿å­˜åœ¨ä¸åŒçš„ .csv æ–‡ä»¶ä¸­ã€‚æ¯ä¸ª .csv æ–‡ä»¶çš„è·¯å¾„ä¹Ÿå¯ä»¥é€šè¿‡å‚æ•°æ•°æ®ç±»è¿›è¡Œå®šä¹‰ã€‚

<!-- Instead of benchmarking pre-trained models via their model identifier, _e.g._ `google-bert/bert-base-uncased`, the user can
alternatively benchmark an arbitrary configuration of any available model class. In this case, a `list` of
configurations must be inserted with the benchmark args as follows. -->

æ‚¨å¯ä»¥é€‰æ‹©ä¸é€šè¿‡é¢„è®­ç»ƒæ¨¡å‹çš„æ¨¡å‹æ ‡è¯†ç¬¦ï¼ˆå¦‚ `google-bert/bert-base-uncased`ï¼‰è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œè€Œæ˜¯å¯¹ä»»ä½•å¯ç”¨æ¨¡å‹ç±»çš„ä»»æ„é…ç½®è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¿…é¡»å°†ä¸€ç³»åˆ—é…ç½®ä¸åŸºå‡†æµ‹è¯•å‚æ•°ä¸€èµ·ä¼ å…¥ï¼Œæ–¹æ³•å¦‚ä¸‹ï¼š

<frameworkcontent>
<pt>
```py
>>> from transformers import PyTorchBenchmark, PyTorchBenchmarkArguments, BertConfig

>>> args = PyTorchBenchmarkArguments(
...     models=["bert-base", "bert-384-hid", "bert-6-lay"], batch_sizes=[8], sequence_lengths=[8, 32, 128, 512]
... )
>>> config_base = BertConfig()
>>> config_384_hid = BertConfig(hidden_size=384)
>>> config_6_lay = BertConfig(num_hidden_layers=6)

>>> benchmark = PyTorchBenchmark(args, configs=[config_base, config_384_hid, config_6_lay])
>>> benchmark.run()
====================       INFERENCE - SPEED - RESULT       ====================
--------------------------------------------------------------------------------
Model Name             Batch Size     Seq Length       Time in s                  
--------------------------------------------------------------------------------
bert-base                  8              128            0.006
bert-base                  8              512            0.006
bert-base                  8              128            0.018     
bert-base                  8              512            0.088     
bert-384-hid              8               8             0.006     
bert-384-hid              8               32            0.006     
bert-384-hid              8              128            0.011     
bert-384-hid              8              512            0.054     
bert-6-lay                 8               8             0.003     
bert-6-lay                 8               32            0.004     
bert-6-lay                 8              128            0.009     
bert-6-lay                 8              512            0.044
--------------------------------------------------------------------------------

====================      INFERENCE - MEMORY - RESULT       ====================
--------------------------------------------------------------------------------
Model Name             Batch Size     Seq Length      Memory in MB 
--------------------------------------------------------------------------------
bert-base                  8               8             1277
bert-base                  8               32            1281
bert-base                  8              128            1307     
bert-base                  8              512            1539     
bert-384-hid              8               8             1005     
bert-384-hid              8               32            1027     
bert-384-hid              8              128            1035     
bert-384-hid              8              512            1255     
bert-6-lay                 8               8             1097     
bert-6-lay                 8               32            1101     
bert-6-lay                 8              128            1127     
bert-6-lay                 8              512            1359
--------------------------------------------------------------------------------

====================        ENVIRONMENT INFORMATION         ====================

- transformers_version: 2.11.0
- framework: PyTorch
- use_torchscript: False
- framework_version: 1.4.0
- python_version: 3.6.10
- system: Linux
- cpu: x86_64
- architecture: 64bit
- date: 2020-06-29
- time: 09:35:25.143267
- fp16: False
- use_multiprocessing: True
- only_pretrain_model: False
- cpu_ram_mb: 32088
- use_gpu: True
- num_gpus: 1
- gpu: TITAN RTX
- gpu_ram_mb: 24217
- gpu_power_watts: 280.0
- gpu_performance_state: 2
- use_tpu: False
```
</pt>
<tf>
```py
>>> from transformers import TensorFlowBenchmark, TensorFlowBenchmarkArguments, BertConfig

>>> args = TensorFlowBenchmarkArguments(
...     models=["bert-base", "bert-384-hid", "bert-6-lay"], batch_sizes=[8], sequence_lengths=[8, 32, 128, 512]
... )
>>> config_base = BertConfig()
>>> config_384_hid = BertConfig(hidden_size=384)
>>> config_6_lay = BertConfig(num_hidden_layers=6)

>>> benchmark = TensorFlowBenchmark(args, configs=[config_base, config_384_hid, config_6_lay])
>>> benchmark.run()
====================       INFERENCE - SPEED - RESULT       ====================
--------------------------------------------------------------------------------
Model Name             Batch Size     Seq Length       Time in s                  
--------------------------------------------------------------------------------
bert-base                  8               8             0.005
bert-base                  8               32            0.008
bert-base                  8              128            0.022
bert-base                  8              512            0.106
bert-384-hid              8               8             0.005
bert-384-hid              8               32            0.007
bert-384-hid              8              128            0.018
bert-384-hid              8              512            0.064
bert-6-lay                 8               8             0.002
bert-6-lay                 8               32            0.003
bert-6-lay                 8              128            0.0011
bert-6-lay                 8              512            0.074
--------------------------------------------------------------------------------

====================      INFERENCE - MEMORY - RESULT       ====================
--------------------------------------------------------------------------------
Model Name             Batch Size     Seq Length      Memory in MB 
--------------------------------------------------------------------------------
bert-base                  8               8             1330
bert-base                  8               32            1330
bert-base                  8              128            1330
bert-base                  8              512            1770
bert-384-hid              8               8             1330
bert-384-hid              8               32            1330
bert-384-hid              8              128            1330
bert-384-hid              8              512            1540
bert-6-lay                 8               8             1330
bert-6-lay                 8               32            1330
bert-6-lay                 8              128            1330
bert-6-lay                 8              512            1540
--------------------------------------------------------------------------------

====================        ENVIRONMENT INFORMATION         ====================

- transformers_version: 2.11.0
- framework: Tensorflow
- use_xla: False
- framework_version: 2.2.0
- python_version: 3.6.10
- system: Linux
- cpu: x86_64
- architecture: 64bit
- date: 2020-06-29
- time: 09:38:15.487125
- fp16: False
- use_multiprocessing: True
- only_pretrain_model: False
- cpu_ram_mb: 32088
- use_gpu: True
- num_gpus: 1
- gpu: TITAN RTX
- gpu_ram_mb: 24217
- gpu_power_watts: 280.0
- gpu_performance_state: 2
- use_tpu: False
```
</tf>
</frameworkcontent>

<!-- Again, _inference time_ and _required memory_ for _inference_ are measured, but this time for customized configurations
of the `BertModel` class. This feature can especially be helpful when deciding for which configuration the model
should be trained. -->

é‡æ–°æµ‹é‡çš„æ˜¯ **æ¨ç†æ—¶é—´** å’Œ **æ¨ç†æ‰€éœ€å†…å­˜**ï¼Œä¸è¿‡è¿™æ¬¡æ˜¯é’ˆå¯¹ `BertModel` ç±»çš„è‡ªå®šä¹‰é…ç½®è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚è¿™ä¸ªåŠŸèƒ½åœ¨å†³å®šæ¨¡å‹åº”è¯¥ä½¿ç”¨å“ªç§é…ç½®è¿›è¡Œè®­ç»ƒæ—¶å°¤å…¶æœ‰ç”¨ã€‚


## åŸºå‡†æµ‹è¯•çš„æ¨èç­–ç•¥
æœ¬èŠ‚åˆ—å‡ºäº†ä¸€äº›åœ¨å¯¹æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•æ—¶æ¯”è¾ƒæ¨èçš„ç­–ç•¥ï¼š

* ç›®å‰ï¼Œè¯¥æ¨¡å—åªæ”¯æŒå•è®¾å¤‡åŸºå‡†æµ‹è¯•ã€‚åœ¨è¿›è¡Œ GPU åŸºå‡†æµ‹è¯•æ—¶ï¼Œå»ºè®®ç”¨æˆ·é€šè¿‡è®¾ç½® `CUDA_VISIBLE_DEVICES` ç¯å¢ƒå˜é‡æ¥æŒ‡å®šä»£ç åº”åœ¨å“ªä¸ªè®¾å¤‡ä¸Šè¿è¡Œï¼Œä¾‹å¦‚åœ¨è¿è¡Œä»£ç å‰æ‰§è¡Œ `export CUDA_VISIBLE_DEVICES=0`ã€‚
* `no_multi_processing` é€‰é¡¹ä»…åº”åœ¨æµ‹è¯•å’Œè°ƒè¯•æ—¶è®¾ç½®ä¸º `True`ã€‚ä¸ºäº†ç¡®ä¿å†…å­˜æµ‹é‡çš„å‡†ç¡®æ€§ï¼Œå»ºè®®å°†æ¯ä¸ªå†…å­˜åŸºå‡†æµ‹è¯•å•ç‹¬è¿è¡Œåœ¨ä¸€ä¸ªè¿›ç¨‹ä¸­ï¼Œå¹¶ç¡®ä¿ `no_multi_processing` è®¾ç½®ä¸º `True`ã€‚
* å½“æ‚¨åˆ†äº«æ¨¡å‹åŸºå‡†æµ‹è¯•ç»“æœæ—¶ï¼Œåº”å§‹ç»ˆæä¾›ç¯å¢ƒä¿¡æ¯ã€‚ç”±äº GPU è®¾å¤‡ã€åº“ç‰ˆæœ¬ç­‰ä¹‹é—´å¯èƒ½å­˜åœ¨è¾ƒå¤§å·®å¼‚ï¼Œå•ç‹¬çš„åŸºå‡†æµ‹è¯•ç»“æœå¯¹ç¤¾åŒºçš„å¸®åŠ©æœ‰é™ã€‚

<!-- ## Benchmark best practices

This section lists a couple of best practices one should be aware of when benchmarking a model.

- Currently, only single device benchmarking is supported. When benchmarking on GPU, it is recommended that the user
  specifies on which device the code should be run by setting the `CUDA_VISIBLE_DEVICES` environment variable in the
  shell, _e.g._ `export CUDA_VISIBLE_DEVICES=0` before running the code.
- The option `no_multi_processing` should only be set to `True` for testing and debugging. To ensure accurate
  memory measurement it is recommended to run each memory benchmark in a separate process by making sure
  `no_multi_processing` is set to `True`.
- One should always state the environment information when sharing the results of a model benchmark. Results can vary
  heavily between different GPU devices, library versions, etc., as a consequence, benchmark results on their own are not very
  useful for the community. -->

## åˆ†äº«æ‚¨çš„åŸºå‡†æµ‹è¯•ç»“æœ

ä»¥å‰ï¼Œæ‰€æœ‰å¯ç”¨çš„æ ¸å¿ƒæ¨¡å‹ï¼ˆå½“æ—¶æœ‰10ä¸ªï¼‰éƒ½å·²é’ˆå¯¹ **æ¨ç†æ—¶é—´** è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†å¤šç§ä¸åŒçš„è®¾ç½®ï¼šä½¿ç”¨ PyTorchï¼ˆåŒ…ä¸åŒ…å« TorchScriptï¼‰ï¼Œä½¿ç”¨ TensorFlowï¼ˆåŒ…ä¸åŒ…å« XLAï¼‰ã€‚æ‰€æœ‰è¿™äº›æµ‹è¯•éƒ½åœ¨ CPUï¼ˆé™¤äº† TensorFlow XLAï¼‰å’Œ GPU ä¸Šè¿›è¡Œã€‚

è¿™ç§æ–¹æ³•çš„è¯¦ç»†ä¿¡æ¯å¯ä»¥åœ¨ [è¿™ç¯‡åšå®¢](https://medium.com/huggingface/benchmarking-transformers-pytorch-and-tensorflow-e2917fb891c2) ä¸­æ‰¾åˆ°ï¼Œæµ‹è¯•ç»“æœå¯ä»¥åœ¨ [è¿™é‡Œ](https://docs.google.com/spreadsheets/d/1sryqufw2D0XlUH4sq3e9Wnxu5EAQkaohzrJbd5HdQ_w/edit?usp=sharing) æŸ¥çœ‹ã€‚


æ‚¨å¯ä»¥å€ŸåŠ©æ–°çš„ **åŸºå‡†æµ‹è¯•** å·¥å…·æ¯”ä»¥å¾€ä»»ä½•æ—¶å€™éƒ½æ›´å®¹æ˜“åœ°åˆ†äº«æ‚¨çš„åŸºå‡†æµ‹è¯•ç»“æœï¼

- [PyTorch åŸºå‡†æµ‹è¯•ç»“æœ](https://github.com/huggingface/transformers/tree/main/examples/pytorch/benchmarking/README.md)
- [TensorFlow åŸºå‡†æµ‹è¯•ç»“æœ](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/benchmarking/README.md)


<!-- ## Sharing your benchmark

Previously all available core models (10 at the time) have been benchmarked for _inference time_, across many different
settings: using PyTorch, with and without TorchScript, using TensorFlow, with and without XLA. All of those tests were
done across CPUs (except for TensorFlow XLA) and GPUs.

The approach is detailed in the [following blogpost](https://medium.com/huggingface/benchmarking-transformers-pytorch-and-tensorflow-e2917fb891c2) and the results are
available [here](https://docs.google.com/spreadsheets/d/1sryqufw2D0XlUH4sq3e9Wnxu5EAQkaohzrJbd5HdQ_w/edit?usp=sharing).

With the new _benchmark_ tools, it is easier than ever to share your benchmark results with the community

- [PyTorch Benchmarking Results](https://github.com/huggingface/transformers/tree/main/examples/pytorch/benchmarking/README.md).
- [TensorFlow Benchmarking Results](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/benchmarking/README.md). -->
