<!--Copyright 2024 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# ç”Ÿæˆç­–ç•¥

è§£ç ç­–ç•¥å†³å®šäº†æ¨¡å‹åº”è¯¥å¦‚ä½•é€‰æ‹©ä¸‹ä¸€ä¸ªç”Ÿæˆçš„tokenã€‚æœ‰è®¸å¤šç±»å‹çš„è§£ç ç­–ç•¥ï¼Œé€‰æ‹©åˆé€‚çš„ç­–ç•¥å¯¹ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡æœ‰æ˜¾è‘—å½±å“ã€‚

æœ¬æŒ‡å—å°†å¸®åŠ©ä½ ç†è§£ Transformers ä¸­å¯ç”¨çš„ä¸åŒè§£ç ç­–ç•¥ï¼Œä»¥åŠå¦‚ä½•å’Œä½•æ—¶ä½¿ç”¨å®ƒä»¬ã€‚

## åŸºç¡€è§£ç æ–¹æ³•

è¿™äº›æ˜¯æˆç†Ÿçš„è§£ç æ–¹æ³•ï¼Œåº”è¯¥ä½œä¸ºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡çš„èµ·ç‚¹ã€‚

### è´ªå©ªæœç´¢

è´ªå©ªæœç´¢æ˜¯é»˜è®¤çš„è§£ç ç­–ç•¥ã€‚å®ƒåœ¨æ¯ä¸€æ­¥é€‰æ‹©æœ€å¯èƒ½çš„ä¸‹ä¸€ä¸ªtokenã€‚é™¤éåœ¨ [`GenerationConfig`] ä¸­æŒ‡å®šï¼Œå¦åˆ™æ­¤ç­–ç•¥æœ€å¤šç”Ÿæˆ20ä¸ªæ–°tokenã€‚

è´ªå©ªæœç´¢é€‚ç”¨äºè¾“å‡ºç›¸å¯¹è¾ƒçŸ­ä¸”ä¸éœ€è¦åˆ›é€ æ€§çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå½“ç”Ÿæˆè¾ƒé•¿çš„åºåˆ—æ—¶ï¼Œå®ƒä¼šå¼€å§‹é‡å¤è‡ªå·±ï¼Œæ•ˆæœä¼šå˜å·®ã€‚

```py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from accelerate import Accelerator

device = Accelerator().device

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
inputs = tokenizer("Hugging Face is an open-source company", return_tensors="pt").to(device)

model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf", dtype=torch.float16).to(device)
# æ˜¾å¼è®¾ç½®ä¸ºé»˜è®¤é•¿åº¦ï¼Œå› ä¸ºLlama2çš„ç”Ÿæˆé•¿åº¦æ˜¯4096
outputs = model.generate(**inputs, max_new_tokens=20)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
'Hugging Face is an open-source company that provides a suite of tools and services for building, deploying, and maintaining natural language processing'
```

### é‡‡æ ·

é‡‡æ ·ï¼Œæˆ–å¤šé¡¹å¼é‡‡æ ·ï¼Œæ ¹æ®æ¨¡å‹æ•´ä¸ªè¯æ±‡è¡¨ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒéšæœºé€‰æ‹©ä¸€ä¸ªtokenï¼ˆè€Œä¸æ˜¯åƒè´ªå©ªæœç´¢é‚£æ ·é€‰æ‹©æœ€å¯èƒ½çš„tokenï¼‰ã€‚è¿™æ„å‘³ç€æ¯ä¸ªå…·æœ‰éé›¶æ¦‚ç‡çš„tokenéƒ½æœ‰æœºä¼šè¢«é€‰ä¸­ã€‚é‡‡æ ·ç­–ç•¥å¯ä»¥å‡å°‘é‡å¤ï¼Œå¹¶ç”Ÿæˆæ›´æœ‰åˆ›æ„å’Œå¤šæ ·æ€§çš„è¾“å‡ºã€‚

é€šè¿‡è®¾ç½® `do_sample=True` å’Œ `num_beams=1` æ¥å¯ç”¨å¤šé¡¹å¼é‡‡æ ·ã€‚

```py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from accelerate import Accelerator

device = Accelerator().device

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
inputs = tokenizer("Hugging Face is an open-source company", return_tensors="pt").to(device)

model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf", dtype=torch.float16).to(device)
# æ˜¾å¼è®¾ç½®ä¸º100ï¼Œå› ä¸ºLlama2çš„ç”Ÿæˆé•¿åº¦æ˜¯4096
outputs = model.generate(**inputs, max_new_tokens=50, do_sample=True, num_beams=1)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
'Hugging Face is an open-source company ğŸ¤—\nWe are open-source and believe that open-source is the best way to build technology. Our mission is to make AI accessible to everyone, and we believe that open-source is the best way to achieve that.'
```

### æŸæœç´¢

æŸæœç´¢åœ¨æ¯ä¸ªæ—¶é—´æ­¥ä¿æŒè·Ÿè¸ªå¤šä¸ªç”Ÿæˆçš„åºåˆ—ï¼ˆç§°ä¸º"æŸ"ï¼‰ã€‚åœ¨ä¸€å®šæ•°é‡çš„æ­¥éª¤åï¼Œå®ƒé€‰æ‹©*æ•´ä½“*æ¦‚ç‡æœ€é«˜çš„åºåˆ—ã€‚ä¸è´ªå©ªæœç´¢ä¸åŒï¼Œè¿™ç§ç­–ç•¥å¯ä»¥"å‘å‰çœ‹"ï¼Œå³ä½¿åˆå§‹tokençš„æ¦‚ç‡è¾ƒä½ï¼Œä¹Ÿèƒ½é€‰æ‹©æ•´ä½“æ¦‚ç‡æ›´é«˜çš„åºåˆ—ã€‚å®ƒæœ€é€‚åˆåŸºäºè¾“å…¥çš„ä»»åŠ¡ï¼Œå¦‚æè¿°å›¾åƒæˆ–è¯­éŸ³è¯†åˆ«ã€‚ä½ ä¹Ÿå¯ä»¥åœ¨æŸæœç´¢ä¸­ä½¿ç”¨ `do_sample=True` åœ¨æ¯ä¸€æ­¥è¿›è¡Œé‡‡æ ·ï¼Œä½†æŸæœç´¢ä»ä¼šåœ¨æ­¥éª¤ä¹‹é—´è´ªå©ªåœ°å‰ªé™¤ä½æ¦‚ç‡åºåˆ—ã€‚

> [!TIP]
> æŸ¥çœ‹ [æŸæœç´¢å¯è§†åŒ–å·¥å…·](https://huggingface.co/spaces/m-ric/beam_search_visualizer) æ¥äº†è§£æŸæœç´¢çš„å·¥ä½œåŸç†ã€‚

é€šè¿‡ `num_beams` å‚æ•°å¯ç”¨æŸæœç´¢ï¼ˆåº”å¤§äº1ï¼Œå¦åˆ™ç­‰åŒäºè´ªå©ªæœç´¢ï¼‰ã€‚

```py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from accelerate import Accelerator

device = Accelerator().device

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
inputs = tokenizer("Hugging Face is an open-source company", return_tensors="pt").to(device)

model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf", dtype=torch.float16).to(device)
# æ˜¾å¼è®¾ç½®ä¸º100ï¼Œå› ä¸ºLlama2çš„ç”Ÿæˆé•¿åº¦æ˜¯4096
outputs = model.generate(**inputs, max_new_tokens=50, num_beams=2)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
"['Hugging Face is an open-source company that develops and maintains the Hugging Face platform, which is a collection of tools and libraries for building and deploying natural language processing (NLP) models. Hugging Face was founded in 2018 by Thomas Wolf']"
```

## è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•

è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•å¯ä»¥å®ç°ç‰¹æ®Šçš„è¡Œä¸ºï¼Œä¾‹å¦‚ï¼š

- å¦‚æœæ¨¡å‹ä¸ç¡®å®šï¼Œè®©å®ƒç»§ç»­æ€è€ƒï¼›
- å¦‚æœæ¨¡å‹å¡ä½äº†ï¼Œå›æ»šç”Ÿæˆï¼›
- ä½¿ç”¨è‡ªå®šä¹‰é€»è¾‘å¤„ç†ç‰¹æ®Štokenï¼›
- ä½¿ç”¨ä¸“é—¨çš„ KV ç¼“å­˜ï¼›

æˆ‘ä»¬é€šè¿‡æ¨¡å‹ä»“åº“å¯ç”¨è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•ï¼Œå‡è®¾æœ‰ç‰¹å®šçš„æ¨¡å‹æ ‡ç­¾å’Œæ–‡ä»¶ç»“æ„ï¼ˆè§ä¸‹é¢çš„å°èŠ‚ï¼‰ã€‚æ­¤åŠŸèƒ½æ˜¯ [è‡ªå®šä¹‰å»ºæ¨¡ä»£ç ](./models.md#custom-models) çš„æ‰©å±•ï¼Œå› æ­¤åŒæ ·éœ€è¦è®¾ç½® `trust_remote_code=True`ã€‚

å¦‚æœæ¨¡å‹ä»“åº“åŒ…å«è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•ï¼Œæœ€ç®€å•çš„å°è¯•æ–¹æ³•æ˜¯åŠ è½½æ¨¡å‹å¹¶ç”¨å®ƒç”Ÿæˆï¼š

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

# `transformers-community/custom_generate_example` åŒ…å« `Qwen/Qwen2.5-0.5B-Instruct` çš„å‰¯æœ¬ï¼Œä½†
# å¸¦æœ‰è‡ªå®šä¹‰ç”Ÿæˆä»£ç  -> è°ƒç”¨ `generate` ä¼šä½¿ç”¨è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•ï¼
tokenizer = AutoTokenizer.from_pretrained("transformers-community/custom_generate_example")
model = AutoModelForCausalLM.from_pretrained(
    "transformers-community/custom_generate_example", device_map="auto", trust_remote_code=True
)

inputs = tokenizer(["The quick brown"], return_tensors="pt").to(model.device)
# è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•æ˜¯ä¸€ä¸ªæœ€å°çš„è´ªå©ªè§£ç å®ç°ã€‚å®ƒè¿˜ä¼šåœ¨è¿è¡Œæ—¶æ‰“å°è‡ªå®šä¹‰æ¶ˆæ¯ã€‚
gen_out = model.generate(**inputs)
# ä½ ç°åœ¨åº”è¯¥èƒ½çœ‹åˆ°å®ƒçš„è‡ªå®šä¹‰æ¶ˆæ¯ï¼Œ"âœ¨ using a custom generation method âœ¨"
print(tokenizer.batch_decode(gen_out, skip_special_tokens=True))
'The quick brown fox jumps over a lazy dog, and the dog is a type of animal. Is'
```

å…·æœ‰è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•çš„æ¨¡å‹ä»“åº“æœ‰ä¸€ä¸ªç‰¹æ®Šå±æ€§ï¼šå®ƒä»¬çš„ç”Ÿæˆæ–¹æ³•å¯ä»¥é€šè¿‡ [`~GenerationMixin.generate`] çš„ `custom_generate` å‚æ•°ä»**ä»»ä½•**æ¨¡å‹åŠ è½½ã€‚è¿™æ„å‘³ç€ä»»ä½•äººéƒ½å¯ä»¥åˆ›å»ºå’Œå…±äº«ä»–ä»¬çš„è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•ï¼Œä½¿å…¶èƒ½ä¸ä»»ä½• Transformers æ¨¡å‹ä¸€èµ·å·¥ä½œï¼Œè€Œæ— éœ€ç”¨æˆ·å®‰è£…é¢å¤–çš„ Python åŒ…ã€‚

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct")
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct", device_map="auto")

inputs = tokenizer(["The quick brown"], return_tensors="pt").to(model.device)
# `custom_generate` ç”¨ `transformers-community/custom_generate_example` ä¸­å®šä¹‰çš„
# è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•æ›¿æ¢åŸå§‹çš„ `generate`
gen_out = model.generate(**inputs, custom_generate="transformers-community/custom_generate_example", trust_remote_code=True)
print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])
'The quick brown fox jumps over a lazy dog, and the dog is a type of animal. Is'
```

ä½ åº”è¯¥é˜…è¯»åŒ…å«è‡ªå®šä¹‰ç”Ÿæˆç­–ç•¥çš„ä»“åº“çš„ `README.md` æ–‡ä»¶ï¼Œä»¥äº†è§£æ–°å‚æ•°å’Œè¾“å‡ºç±»å‹çš„å·®å¼‚ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰ã€‚å¦åˆ™ï¼Œä½ å¯ä»¥å‡è®¾å®ƒçš„å·¥ä½œæ–¹å¼ä¸åŸºç¡€ [`~GenerationMixin.generate`] æ–¹æ³•ç›¸åŒã€‚

> [!TIP]
> ä½ å¯ä»¥é€šè¿‡ [æœç´¢è‡ªå®šä¹‰æ ‡ç­¾](https://huggingface.co/models?other=custom_generate) `custom_generate` æ¥æ‰¾åˆ°æ‰€æœ‰è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•ã€‚

ä»¥ Hub ä»“åº“ [transformers-community/custom_generate_example](https://huggingface.co/transformers-community/custom_generate_example) ä¸ºä¾‹ã€‚`README.md` è¯´æ˜å®ƒæœ‰ä¸€ä¸ªé¢å¤–çš„è¾“å…¥å‚æ•° `left_padding`ï¼Œå¯ä»¥åœ¨æç¤ºè¯å‰æ·»åŠ è‹¥å¹²ä¸ªå¡«å……tokenã€‚

```py
gen_out = model.generate(
    **inputs, custom_generate="transformers-community/custom_generate_example", trust_remote_code=True, left_padding=5
)
print(tokenizer.batch_decode(gen_out)[0])
'<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>The quick brown fox jumps over the lazy dog.\n\nThe sentence "The quick'
```

å¦‚æœè‡ªå®šä¹‰æ–¹æ³•æœ‰ä½ çš„ç¯å¢ƒä¸æ»¡è¶³çš„å›ºå®š Python ä¾èµ–è¦æ±‚ï¼Œä½ ä¼šæ”¶åˆ°å…³äºç¼ºå°‘ä¾èµ–çš„å¼‚å¸¸ã€‚ä¾‹å¦‚ï¼Œ[transformers-community/custom_generate_bad_requirements](https://huggingface.co/transformers-community/custom_generate_bad_requirements) åœ¨å…¶ `custom_generate/requirements.txt` æ–‡ä»¶ä¸­å®šä¹‰äº†ä¸€ç»„ä¸å¯èƒ½çš„ä¾èµ–è¦æ±‚ï¼Œå¦‚æœä½ å°è¯•è¿è¡Œå®ƒï¼Œä¼šçœ‹åˆ°ä»¥ä¸‹é”™è¯¯æ¶ˆæ¯ã€‚

```text
ImportError: Missing requirements in your local environment for `transformers-community/custom_generate_bad_requirements`:
foo (installed: None)
bar==0.0.0 (installed: None)
torch>=99.0 (installed: 2.6.0)
```

ç›¸åº”åœ°æ›´æ–°ä½ çš„ Python ä¾èµ–å°†æ¶ˆé™¤æ­¤é”™è¯¯æ¶ˆæ¯ã€‚

### åˆ›å»ºè‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•

è¦åˆ›å»ºæ–°çš„ç”Ÿæˆæ–¹æ³•ï¼Œä½ éœ€è¦åˆ›å»ºä¸€ä¸ªæ–°çš„ [**æ¨¡å‹**](https://huggingface.co/new) ä»“åº“å¹¶å‘å…¶ä¸­æ¨é€ä¸€äº›æ–‡ä»¶ã€‚

1. ä½ ä¸ºå…¶è®¾è®¡ç”Ÿæˆæ–¹æ³•çš„æ¨¡å‹ã€‚
2. `custom_generate/generate.py`ï¼ŒåŒ…å«ä½ è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•çš„æ‰€æœ‰é€»è¾‘ã€‚
3. `custom_generate/requirements.txt`ï¼Œç”¨äºå¯é€‰åœ°æ·»åŠ æ–°çš„ Python ä¾èµ–å’Œ/æˆ–é”å®šç‰¹å®šç‰ˆæœ¬ä»¥æ­£ç¡®ä½¿ç”¨ä½ çš„æ–¹æ³•ã€‚
4. `README.md`ï¼Œä½ åº”è¯¥åœ¨è¿™é‡Œæ·»åŠ  `custom_generate` æ ‡ç­¾ï¼Œå¹¶è®°å½•ä½ è‡ªå®šä¹‰æ–¹æ³•çš„ä»»ä½•æ–°å‚æ•°æˆ–è¾“å‡ºç±»å‹å·®å¼‚ã€‚

æ·»åŠ æ‰€æœ‰å¿…éœ€æ–‡ä»¶åï¼Œä½ çš„ä»“åº“åº”è¯¥å¦‚ä¸‹æ‰€ç¤ºï¼š

```text
your_repo/
â”œâ”€â”€ README.md          # åŒ…å« 'custom_generate' æ ‡ç­¾
â”œâ”€â”€ config.json
â”œâ”€â”€ ...
â””â”€â”€ custom_generate/
    â”œâ”€â”€ generate.py
    â””â”€â”€ requirements.txt
```

#### æ·»åŠ åŸºç¡€æ¨¡å‹

ä½ è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•çš„èµ·ç‚¹æ˜¯ä¸€ä¸ªä¸å…¶ä»–ä»»ä½•æ¨¡å‹ä»“åº“ä¸€æ ·çš„æ¨¡å‹ä»“åº“ã€‚è¦æ·»åŠ åˆ°æ­¤ä»“åº“çš„æ¨¡å‹åº”è¯¥æ˜¯ä½ ä¸ºå…¶è®¾è®¡æ–¹æ³•çš„æ¨¡å‹ï¼Œå®ƒæ—¨åœ¨æˆä¸ºä¸€ä¸ªå¯å·¥ä½œçš„è‡ªåŒ…å«æ¨¡å‹-ç”Ÿæˆå¯¹çš„ä¸€éƒ¨åˆ†ã€‚å½“åŠ è½½æ­¤ä»“åº“ä¸­çš„æ¨¡å‹æ—¶ï¼Œä½ çš„è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•å°†è¦†ç›– `generate`ã€‚ä¸ç”¨æ‹…å¿ƒâ€”â€”å¦‚ä¸Šä¸€èŠ‚æ‰€è¿°ï¼Œä½ çš„ç”Ÿæˆæ–¹æ³•ä»ç„¶å¯ä»¥ä¸ä»»ä½•å…¶ä»– Transformers æ¨¡å‹ä¸€èµ·åŠ è½½ã€‚

å¦‚æœä½ åªæ˜¯æƒ³å¤åˆ¶ç°æœ‰æ¨¡å‹ï¼Œå¯ä»¥è¿™æ ·åšï¼š

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("source/model_repo")
model = AutoModelForCausalLM.from_pretrained("source/model_repo")
tokenizer.save_pretrained("your/generation_method", push_to_hub=True)
model.save_pretrained("your/generation_method", push_to_hub=True)
```

#### generate.py

è¿™æ˜¯ä½ ç”Ÿæˆæ–¹æ³•çš„æ ¸å¿ƒã€‚å®ƒ*å¿…é¡»*åŒ…å«ä¸€ä¸ªåä¸º `generate` çš„æ–¹æ³•ï¼Œä¸”è¯¥æ–¹æ³•*å¿…é¡»*ä»¥ `model` å‚æ•°ä½œä¸ºå…¶ç¬¬ä¸€ä¸ªå‚æ•°ã€‚`model` æ˜¯æ¨¡å‹å®ä¾‹ï¼Œè¿™æ„å‘³ç€ä½ å¯ä»¥è®¿é—®æ¨¡å‹ä¸­çš„æ‰€æœ‰å±æ€§å’Œæ–¹æ³•ï¼ŒåŒ…æ‹¬ [`GenerationMixin`] ä¸­å®šä¹‰çš„æ–¹æ³•ï¼ˆå¦‚åŸºç¡€ `generate` æ–¹æ³•ï¼‰ã€‚

> [!WARNING]
> `generate.py` å¿…é¡»æ”¾åœ¨åä¸º `custom_generate` çš„æ–‡ä»¶å¤¹ä¸­ï¼Œè€Œä¸æ˜¯ä»“åº“çš„æ ¹ç›®å½•ã€‚æ­¤åŠŸèƒ½çš„æ–‡ä»¶è·¯å¾„æ˜¯ç¡¬ç¼–ç çš„ã€‚

åœ¨åº•å±‚ï¼Œå½“åŸºç¡€ [`~GenerationMixin.generate`] æ–¹æ³•è¢«è°ƒç”¨å¹¶å¸¦æœ‰ `custom_generate` å‚æ•°æ—¶ï¼Œå®ƒé¦–å…ˆæ£€æŸ¥å…¶ Python ä¾èµ–è¦æ±‚ï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œç„¶ååœ¨ `generate.py` ä¸­å®šä½è‡ªå®šä¹‰ `generate` æ–¹æ³•ï¼Œæœ€åè°ƒç”¨è‡ªå®šä¹‰ `generate`ã€‚æ‰€æœ‰æ¥æ”¶åˆ°çš„å‚æ•°å’Œ `model` éƒ½ä¼šè½¬å‘åˆ°ä½ çš„è‡ªå®šä¹‰ `generate` æ–¹æ³•ï¼Œä½†ç”¨äºè§¦å‘è‡ªå®šä¹‰ç”Ÿæˆçš„å‚æ•°ï¼ˆ`trust_remote_code` å’Œ `custom_generate`ï¼‰é™¤å¤–ã€‚

è¿™æ„å‘³ç€ä½ çš„ `generate` å¯ä»¥æ··åˆä½¿ç”¨åŸå§‹å‚æ•°å’Œè‡ªå®šä¹‰å‚æ•°ï¼ˆä»¥åŠä¸åŒçš„è¾“å‡ºç±»å‹ï¼‰ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚

```py
import torch

def generate(model, input_ids, generation_config=None, left_padding=None, **kwargs):
    generation_config = generation_config or model.generation_config  # é»˜è®¤ä½¿ç”¨æ¨¡å‹ç”Ÿæˆé…ç½®
    cur_length = input_ids.shape[1]
    max_length = generation_config.max_length or cur_length + generation_config.max_new_tokens

    # è‡ªå®šä¹‰å‚æ•°ç¤ºä¾‹ï¼šåœ¨æç¤ºè¯å‰æ·»åŠ  `left_padding`ï¼ˆæ•´æ•°ï¼‰ä¸ªå¡«å……token
    if left_padding is not None:
        if not isinstance(left_padding, int) or left_padding < 0:
            raise ValueError(f"left_padding å¿…é¡»æ˜¯å¤§äº0çš„æ•´æ•°ï¼Œä½†å¾—åˆ°çš„æ˜¯ {left_padding}")

        pad_token = kwargs.pop("pad_token", None) or generation_config.pad_token_id or model.config.pad_token_id
        if pad_token is None:
            raise ValueError("pad_token æœªå®šä¹‰")
        batch_size = input_ids.shape[0]
        pad_tensor = torch.full(size=(batch_size, left_padding), fill_value=pad_token).to(input_ids.device)
        input_ids = torch.cat((pad_tensor, input_ids), dim=1)
        cur_length = input_ids.shape[1]

    # ç®€å•çš„è´ªå©ªè§£ç å¾ªç¯
    while cur_length < max_length:
        logits = model(input_ids).logits
        next_token_logits = logits[:, -1, :]
        next_tokens = torch.argmax(next_token_logits, dim=-1)
        input_ids = torch.cat((input_ids, next_tokens[:, None]), dim=-1)
        cur_length += 1

    return input_ids
```

éµå¾ªä»¥ä¸‹æ¨èåšæ³•ä»¥ç¡®ä¿ä½ çš„è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•æŒ‰é¢„æœŸå·¥ä½œã€‚

- éšæ„é‡ç”¨åŸå§‹ [`~GenerationMixin.generate`] ä¸­çš„éªŒè¯å’Œè¾“å…¥å‡†å¤‡é€»è¾‘ã€‚
- å¦‚æœåœ¨ `model` ä¸­ä½¿ç”¨ä»»ä½•ç§æœ‰æ–¹æ³•/å±æ€§ï¼Œè¯·åœ¨ requirements ä¸­å›ºå®š `transformers` ç‰ˆæœ¬ã€‚
- è€ƒè™‘æ·»åŠ æ¨¡å‹éªŒè¯ã€è¾“å…¥éªŒè¯ï¼Œç”šè‡³å•ç‹¬çš„æµ‹è¯•æ–‡ä»¶ï¼Œä»¥å¸®åŠ©ç”¨æˆ·åœ¨å…¶ç¯å¢ƒä¸­å¯¹ä½ çš„ä»£ç è¿›è¡Œå¥å…¨æ€§æ£€æŸ¥ã€‚

ä½ çš„è‡ªå®šä¹‰ `generate` æ–¹æ³•å¯ä»¥ä» `custom_generate` æ–‡ä»¶å¤¹ç›¸å¯¹å¯¼å…¥ä»£ç ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æœ‰ä¸€ä¸ª `utils.py` æ–‡ä»¶ï¼Œå¯ä»¥è¿™æ ·å¯¼å…¥ï¼š

```py
from .utils import some_function
```

ä»…æ”¯æŒä»åŒçº§ `custom_generate` æ–‡ä»¶å¤¹çš„ç›¸å¯¹å¯¼å…¥ã€‚çˆ¶çº§/å…„å¼Ÿæ–‡ä»¶å¤¹å¯¼å…¥æ— æ•ˆã€‚`custom_generate` å‚æ•°ä¹Ÿå¯ä»¥åœ¨æœ¬åœ°ä¸åŒ…å« `custom_generate` ç»“æ„çš„ä»»ä½•ç›®å½•ä¸€èµ·ä½¿ç”¨ã€‚è¿™æ˜¯å¼€å‘è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•çš„æ¨èå·¥ä½œæµç¨‹ã€‚

#### requirements.txt

ä½ å¯ä»¥åœ¨ `custom_generate` æ–‡ä»¶å¤¹å†…çš„ `requirements.txt` æ–‡ä»¶ä¸­å¯é€‰åœ°æŒ‡å®šé¢å¤–çš„ Python ä¾èµ–è¦æ±‚ã€‚è¿™äº›ä¼šåœ¨è¿è¡Œæ—¶æ£€æŸ¥ï¼Œå¦‚æœç¼ºå°‘ä¼šæŠ›å‡ºå¼‚å¸¸ï¼Œæç¤ºç”¨æˆ·ç›¸åº”åœ°æ›´æ–°å…¶ç¯å¢ƒã€‚

#### README.md

æ¨¡å‹ä»“åº“æ ¹ç›®å½•çš„ `README.md` é€šå¸¸æè¿°å…¶ä¸­çš„æ¨¡å‹ã€‚ç„¶è€Œï¼Œç”±äºä»“åº“çš„é‡ç‚¹æ˜¯è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•ï¼Œæˆ‘ä»¬å¼ºçƒˆå»ºè®®å°†å…¶é‡ç‚¹è½¬å‘æè¿°è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•ã€‚é™¤äº†æ–¹æ³•çš„æè¿°å¤–ï¼Œæˆ‘ä»¬å»ºè®®è®°å½•ä¸åŸå§‹ [`~GenerationMixin.generate`] çš„ä»»ä½•è¾“å…¥å’Œ/æˆ–è¾“å‡ºå·®å¼‚ã€‚è¿™æ ·ï¼Œç”¨æˆ·å¯ä»¥ä¸“æ³¨äºæ–°å†…å®¹ï¼Œå¹¶ä¾èµ– Transformers æ–‡æ¡£äº†è§£é€šç”¨å®ç°ç»†èŠ‚ã€‚

ä¸ºäº†ä¾¿äºå‘ç°ï¼Œæˆ‘ä»¬å¼ºçƒˆå»ºè®®ä½ ä¸ºä»“åº“æ·»åŠ  `custom_generate` æ ‡ç­¾ã€‚ä¸ºæ­¤ï¼Œä½ çš„ `README.md` æ–‡ä»¶é¡¶éƒ¨åº”å¦‚ä¸‹ä¾‹æ‰€ç¤ºã€‚æ¨é€æ–‡ä»¶åï¼Œä½ åº”è¯¥èƒ½åœ¨ä»“åº“ä¸­çœ‹åˆ°è¯¥æ ‡ç­¾ï¼

```text
---
library_name: transformers
tags:
  - custom_generate
---

(ä½ çš„ markdown å†…å®¹åœ¨è¿™é‡Œ)
```

æ¨èåšæ³•ï¼š

- è®°å½• [`~GenerationMixin.generate`] çš„è¾“å…¥å’Œè¾“å‡ºå·®å¼‚ã€‚
- æ·»åŠ è‡ªåŒ…å«çš„ç¤ºä¾‹ä»¥ä¾¿å¿«é€Ÿå®éªŒã€‚
- æè¿°è½¯æ€§è¦æ±‚ï¼Œä¾‹å¦‚è¯¥æ–¹æ³•æ˜¯å¦ä»…é€‚ç”¨äºç‰¹å®šæ¨¡å‹ç³»åˆ—ã€‚

### é‡ç”¨ `generate` çš„è¾“å…¥å‡†å¤‡

å¦‚æœä½ æ­£åœ¨æ·»åŠ æ–°çš„è§£ç å¾ªç¯ï¼Œä½ å¯èƒ½å¸Œæœ›ä¿ç•™ `generate` ä¸­çš„è¾“å…¥å‡†å¤‡ï¼ˆæ‰¹æ¬¡æ‰©å±•ã€æ³¨æ„åŠ›æ©ç ã€logits å¤„ç†å™¨ã€åœæ­¢æ¡ä»¶ç­‰ï¼‰ã€‚ä½ ä¹Ÿå¯ä»¥ä¼ é€’ä¸€ä¸ª**å¯è°ƒç”¨å¯¹è±¡**ç»™ `custom_generate`ï¼Œä»¥é‡ç”¨ [`~GenerationMixin.generate`] çš„å®Œæ•´å‡†å¤‡æµç¨‹ï¼ŒåŒæ—¶ä»…è¦†ç›–è§£ç å¾ªç¯ã€‚

```py
def custom_loop(model, input_ids, attention_mask, logits_processor, stopping_criteria, generation_config, **model_kwargs):
    next_tokens = input_ids
    while input_ids.shape[1] < stopping_criteria[0].max_length:
        logits = model(next_tokens, attention_mask=attention_mask, **model_kwargs).logits
        next_token_logits = logits_processor(input_ids, logits[:, -1, :])
        next_tokens = torch.argmax(next_token_logits, dim=-1)[:, None]
        input_ids = torch.cat((input_ids, next_tokens), dim=-1)
        attention_mask = torch.cat((attention_mask, torch.ones_like(next_tokens)), dim=-1)
    return input_ids

output = model.generate(
    **inputs,
    custom_generate=custom_loop,
    max_new_tokens=10,
)
```

> [!TIP]
> å¦‚æœä½ å‘å¸ƒ `custom_generate` ä»“åº“ï¼Œä½ çš„ `generate` å®ç°æœ¬èº«å¯ä»¥å®šä¹‰ä¸€ä¸ªå¯è°ƒç”¨å¯¹è±¡å¹¶å°†å…¶ä¼ é€’ç»™ `model.generate()`ã€‚è¿™è®©ä½ å¯ä»¥è‡ªå®šä¹‰è§£ç å¾ªç¯ï¼ŒåŒæ—¶ä»ç„¶å—ç›Šäº Transformers å†…ç½®çš„è¾“å…¥å‡†å¤‡é€»è¾‘ã€‚

### æŸ¥æ‰¾è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•

ä½ å¯ä»¥é€šè¿‡ [æœç´¢è‡ªå®šä¹‰æ ‡ç­¾](https://huggingface.co/models?other=custom_generate) `custom_generate` æ¥æ‰¾åˆ°æ‰€æœ‰è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•ã€‚é™¤äº†æ ‡ç­¾å¤–ï¼Œæˆ‘ä»¬è¿˜ç­–åˆ’äº†ä¸¤ä¸ª `custom_generate` æ–¹æ³•é›†åˆï¼š

- [è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³• - ç¤¾åŒº](https://huggingface.co/collections/transformers-community/custom-generation-methods-community-6888fb1da0efbc592d3a8ab6) -- ç¤¾åŒºè´¡çŒ®çš„å¼ºå¤§æ–¹æ³•é›†åˆï¼›
- [è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³• - æ•™ç¨‹](https://huggingface.co/collections/transformers-community/custom-generation-methods-tutorials-6823589657a94940ea02cfec) -- ä»¥å‰æ˜¯ `transformers` ä¸€éƒ¨åˆ†çš„æ–¹æ³•çš„å‚è€ƒå®ç°é›†åˆï¼Œä»¥åŠ `custom_generate` çš„æ•™ç¨‹ã€‚

## èµ„æº

é˜…è¯» [å¦‚ä½•ç”Ÿæˆæ–‡æœ¬ï¼šä½¿ç”¨ä¸åŒçš„è§£ç æ–¹æ³•è¿›è¡Œ Transformers è¯­è¨€ç”Ÿæˆ](https://huggingface.co/blog/how-to-generate) åšå®¢æ–‡ç« ï¼Œäº†è§£å¸¸è§è§£ç ç­–ç•¥çš„å·¥ä½œåŸç†è§£é‡Šã€‚
