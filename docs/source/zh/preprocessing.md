<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# é¢„å¤„ç†

[[open-in-colab]]

åœ¨æ‚¨å¯ä»¥åœ¨æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œæ•°æ®éœ€è¦è¢«é¢„å¤„ç†ä¸ºæœŸæœ›çš„æ¨¡å‹è¾“å…¥æ ¼å¼ã€‚æ— è®ºæ‚¨çš„æ•°æ®æ˜¯æ–‡æœ¬ã€å›¾åƒè¿˜æ˜¯éŸ³é¢‘ï¼Œå®ƒä»¬éƒ½éœ€è¦è¢«è½¬æ¢å¹¶ç»„åˆæˆæ‰¹é‡çš„å¼ é‡ã€‚ğŸ¤— Transformers æä¾›äº†ä¸€ç»„é¢„å¤„ç†ç±»æ¥å¸®åŠ©å‡†å¤‡æ•°æ®ä»¥ä¾›æ¨¡å‹ä½¿ç”¨ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæ‚¨å°†äº†è§£ä»¥ä¸‹å†…å®¹ï¼š

* å¯¹äºæ–‡æœ¬ï¼Œä½¿ç”¨[åˆ†è¯å™¨](./main_classes/tokenizer)(`Tokenizer`)å°†æ–‡æœ¬è½¬æ¢ä¸ºä¸€ç³»åˆ—æ ‡è®°(`tokens`)ï¼Œå¹¶åˆ›å»º`tokens`çš„æ•°å­—è¡¨ç¤ºï¼Œå°†å®ƒä»¬ç»„åˆæˆå¼ é‡ã€‚
* å¯¹äºè¯­éŸ³å’ŒéŸ³é¢‘ï¼Œä½¿ç”¨[ç‰¹å¾æå–å™¨](./main_classes/feature_extractor)(`Feature extractor`)ä»éŸ³é¢‘æ³¢å½¢ä¸­æå–é¡ºåºç‰¹å¾å¹¶å°†å…¶è½¬æ¢ä¸ºå¼ é‡ã€‚
* å›¾åƒè¾“å…¥ä½¿ç”¨[å›¾åƒå¤„ç†å™¨](./main_classes/image)(`ImageProcessor`)å°†å›¾åƒè½¬æ¢ä¸ºå¼ é‡ã€‚
* å¤šæ¨¡æ€è¾“å…¥ï¼Œä½¿ç”¨[å¤„ç†å™¨](./main_classes/processors)(`Processor`)ç»“åˆäº†`Tokenizer`å’Œ`ImageProcessor`æˆ–`Processor`ã€‚

> [!TIP]
> `AutoProcessor` **å§‹ç»ˆ**æœ‰æ•ˆçš„è‡ªåŠ¨é€‰æ‹©é€‚ç”¨äºæ‚¨ä½¿ç”¨çš„æ¨¡å‹çš„æ­£ç¡®`class`ï¼Œæ— è®ºæ‚¨ä½¿ç”¨çš„æ˜¯`Tokenizer`ã€`ImageProcessor`ã€`Feature extractor`è¿˜æ˜¯`Processor`ã€‚

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·å®‰è£…ğŸ¤— Datasetsï¼Œä»¥ä¾¿æ‚¨å¯ä»¥åŠ è½½ä¸€äº›æ•°æ®é›†æ¥è¿›è¡Œå®éªŒï¼š


```bash
pip install datasets
```

## è‡ªç„¶è¯­è¨€å¤„ç†

<Youtube id="Yffk5aydLzg"/>

å¤„ç†æ–‡æœ¬æ•°æ®çš„ä¸»è¦å·¥å…·æ˜¯[Tokenizer](main_classes/tokenizer)ã€‚`Tokenizer`æ ¹æ®ä¸€ç»„è§„åˆ™å°†æ–‡æœ¬æ‹†åˆ†ä¸º`tokens`ã€‚ç„¶åå°†è¿™äº›`tokens`è½¬æ¢ä¸ºæ•°å­—ï¼Œç„¶åè½¬æ¢ä¸ºå¼ é‡ï¼Œæˆä¸ºæ¨¡å‹çš„è¾“å…¥ã€‚æ¨¡å‹æ‰€éœ€çš„ä»»ä½•é™„åŠ è¾“å…¥éƒ½ç”±`Tokenizer`æ·»åŠ ã€‚

> [!TIP]
> å¦‚æœæ‚¨è®¡åˆ’ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œé‡è¦çš„æ˜¯ä½¿ç”¨ä¸ä¹‹å…³è”çš„é¢„è®­ç»ƒ`Tokenizer`ã€‚è¿™ç¡®ä¿æ–‡æœ¬çš„æ‹†åˆ†æ–¹å¼ä¸é¢„è®­ç»ƒè¯­æ–™åº“ç›¸åŒï¼Œå¹¶åœ¨é¢„è®­ç»ƒæœŸé—´ä½¿ç”¨ç›¸åŒçš„æ ‡è®°-ç´¢å¼•çš„å¯¹åº”å…³ç³»ï¼ˆé€šå¸¸ç§°ä¸º*è¯æ±‡è¡¨*-`vocab`ï¼‰ã€‚

å¼€å§‹ä½¿ç”¨[`AutoTokenizer.from_pretrained`]æ–¹æ³•åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒ`tokenizer`ã€‚è¿™å°†ä¸‹è½½æ¨¡å‹é¢„è®­ç»ƒçš„`vocab`ï¼š


```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")
```

ç„¶åå°†æ‚¨çš„æ–‡æœ¬ä¼ é€’ç»™`tokenizer`ï¼š


```py
>>> encoded_input = tokenizer("Do not meddle in the affairs of wizards, for they are subtle and quick to anger.")
>>> print(encoded_input)
{'input_ids': [101, 2079, 2025, 19960, 10362, 1999, 1996, 3821, 1997, 16657, 1010, 2005, 2027, 2024, 11259, 1998, 4248, 2000, 4963, 1012, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

`tokenizer`è¿”å›ä¸€ä¸ªåŒ…å«ä¸‰ä¸ªé‡è¦å¯¹è±¡çš„å­—å…¸ï¼š

* [input_ids](glossary#input-ids) æ˜¯ä¸å¥å­ä¸­æ¯ä¸ª`token`å¯¹åº”çš„ç´¢å¼•ã€‚
* [attention_mask](glossary#attention-mask) æŒ‡ç¤ºæ˜¯å¦åº”è¯¥å…³æ³¨ä¸€ä¸ª`token`ã€‚
* [token_type_ids](glossary#token-type-ids) åœ¨å­˜åœ¨å¤šä¸ªåºåˆ—æ—¶æ ‡è¯†ä¸€ä¸ª`token`å±äºå“ªä¸ªåºåˆ—ã€‚

é€šè¿‡è§£ç  `input_ids` æ¥è¿”å›æ‚¨çš„è¾“å…¥ï¼š


```py
>>> tokenizer.decode(encoded_input["input_ids"])
'[CLS] Do not meddle in the affairs of wizards, for they are subtle and quick to anger. [SEP]'
```

å¦‚æ‚¨æ‰€è§ï¼Œ`tokenizer`å‘å¥å­ä¸­æ·»åŠ äº†ä¸¤ä¸ªç‰¹æ®Š`token` - `CLS` å’Œ `SEP`ï¼ˆåˆ†ç±»å™¨å’Œåˆ†éš”ç¬¦ï¼‰ã€‚å¹¶éæ‰€æœ‰æ¨¡å‹éƒ½éœ€è¦ç‰¹æ®Š`token`ï¼Œä½†å¦‚æœéœ€è¦ï¼Œ`tokenizer`ä¼šè‡ªåŠ¨ä¸ºæ‚¨æ·»åŠ ã€‚

å¦‚æœæœ‰å¤šä¸ªå¥å­éœ€è¦é¢„å¤„ç†ï¼Œå°†å®ƒä»¬ä½œä¸ºåˆ—è¡¨ä¼ é€’ç»™`tokenizer`ï¼š


```py
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_inputs = tokenizer(batch_sentences)
>>> print(encoded_inputs)
{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102],
               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],
               [101, 1327, 1164, 5450, 23434, 136, 102]],
 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0]],
 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1],
                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                    [1, 1, 1, 1, 1, 1, 1]]}
```

### å¡«å……

å¥å­çš„é•¿åº¦å¹¶ä¸æ€»æ˜¯ç›¸åŒï¼Œè¿™å¯èƒ½ä¼šæˆä¸ºä¸€ä¸ªé—®é¢˜ï¼Œå› ä¸ºæ¨¡å‹è¾“å…¥çš„å¼ é‡éœ€è¦å…·æœ‰ç»Ÿä¸€çš„å½¢çŠ¶ã€‚å¡«å……æ˜¯ä¸€ç§ç­–ç•¥ï¼Œé€šè¿‡åœ¨è¾ƒçŸ­çš„å¥å­ä¸­æ·»åŠ ä¸€ä¸ªç‰¹æ®Šçš„`padding token`ï¼Œä»¥ç¡®ä¿å¼ é‡æ˜¯çŸ©å½¢çš„ã€‚

å°† `padding` å‚æ•°è®¾ç½®ä¸º `True`ï¼Œä»¥ä½¿æ‰¹æ¬¡ä¸­è¾ƒçŸ­çš„åºåˆ—å¡«å……åˆ°ä¸æœ€é•¿åºåˆ—ç›¸åŒ¹é…çš„é•¿åº¦ï¼š

```py
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_input = tokenizer(batch_sentences, padding=True)
>>> print(encoded_input)
{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],
               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],
               [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],
 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                    [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}
```

ç¬¬ä¸€å¥å’Œç¬¬ä¸‰å¥å› ä¸ºè¾ƒçŸ­ï¼Œé€šè¿‡`0`è¿›è¡Œå¡«å……ï¼Œã€‚

### æˆªæ–­

å¦ä¸€æ–¹é¢ï¼Œæœ‰æ—¶å€™ä¸€ä¸ªåºåˆ—å¯èƒ½å¯¹æ¨¡å‹æ¥è¯´å¤ªé•¿äº†ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨éœ€è¦å°†åºåˆ—æˆªæ–­ä¸ºæ›´çŸ­çš„é•¿åº¦ã€‚

å°† `truncation` å‚æ•°è®¾ç½®ä¸º `True`ï¼Œä»¥å°†åºåˆ—æˆªæ–­ä¸ºæ¨¡å‹æ¥å—çš„æœ€å¤§é•¿åº¦ï¼š


```py
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True)
>>> print(encoded_input)
{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],
               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],
               [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],
 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                    [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}
```

> [!TIP]
> æŸ¥çœ‹[å¡«å……å’Œæˆªæ–­](./pad_truncation)æ¦‚å¿µæŒ‡å—ï¼Œäº†è§£æ›´å¤šæœ‰å…³å¡«å……å’Œæˆªæ–­å‚æ•°çš„ä¿¡æ¯ã€‚

### æ„å»ºå¼ é‡

æœ€åï¼Œ`tokenizer`å¯ä»¥è¿”å›å®é™…è¾“å…¥åˆ°æ¨¡å‹çš„å¼ é‡ã€‚

å°† `return_tensors` å‚æ•°è®¾ç½®ä¸º `pt`ï¼ˆå¯¹äºPyTorchï¼‰æˆ– `tf`ï¼ˆå¯¹äºTensorFlowï¼‰ï¼š



```py
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors="pt")
>>> print(encoded_input)
{'input_ids': tensor([[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],
                      [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],
                      [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]]),
 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),
 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
                           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                           [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}
```

## éŸ³é¢‘

å¯¹äºéŸ³é¢‘ä»»åŠ¡ï¼Œæ‚¨éœ€è¦[feature extractor](main_classes/feature_extractor)æ¥å‡†å¤‡æ‚¨çš„æ•°æ®é›†ä»¥ä¾›æ¨¡å‹ä½¿ç”¨ã€‚`feature extractor`æ—¨åœ¨ä»åŸå§‹éŸ³é¢‘æ•°æ®ä¸­æå–ç‰¹å¾ï¼Œå¹¶å°†å®ƒä»¬è½¬æ¢ä¸ºå¼ é‡ã€‚

åŠ è½½[MInDS-14](https://huggingface.co/datasets/PolyAI/minds14)æ•°æ®é›†ï¼ˆæœ‰å…³å¦‚ä½•åŠ è½½æ•°æ®é›†çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…ğŸ¤— [Datasetsæ•™ç¨‹](https://huggingface.co/docs/datasets/load_hub)ï¼‰ä»¥äº†è§£å¦‚ä½•åœ¨éŸ³é¢‘æ•°æ®é›†ä¸­ä½¿ç”¨`feature extractor`ï¼š


```py
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")
```

è®¿é—® `audio` åˆ—çš„ç¬¬ä¸€ä¸ªå…ƒç´ ä»¥æŸ¥çœ‹è¾“å…¥ã€‚è°ƒç”¨ `audio` åˆ—ä¼šè‡ªåŠ¨åŠ è½½å’Œé‡æ–°é‡‡æ ·éŸ³é¢‘æ–‡ä»¶ï¼š

```py
>>> dataset[0]["audio"]
{'array': array([ 0.        ,  0.00024414, -0.00024414, ..., -0.00024414,
         0.        ,  0.        ], dtype=float32),
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',
 'sampling_rate': 8000}
```

è¿™ä¼šè¿”å›ä¸‰ä¸ªå¯¹è±¡ï¼š

* `array` æ˜¯åŠ è½½çš„è¯­éŸ³ä¿¡å· - å¹¶åœ¨å¿…è¦æ—¶é‡æ–°é‡‡ä¸º`1D array`ã€‚
* `path` æŒ‡å‘éŸ³é¢‘æ–‡ä»¶çš„ä½ç½®ã€‚
* `sampling_rate` æ˜¯æ¯ç§’æµ‹é‡çš„è¯­éŸ³ä¿¡å·æ•°æ®ç‚¹æ•°é‡ã€‚

å¯¹äºæœ¬æ•™ç¨‹ï¼Œæ‚¨å°†ä½¿ç”¨[Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base)æ¨¡å‹ã€‚æŸ¥çœ‹æ¨¡å‹å¡ç‰‡ï¼Œæ‚¨å°†äº†è§£åˆ°Wav2Vec2æ˜¯åœ¨16kHzé‡‡æ ·çš„è¯­éŸ³éŸ³é¢‘æ•°æ®ä¸Šé¢„è®­ç»ƒçš„ã€‚é‡è¦çš„æ˜¯ï¼Œæ‚¨çš„éŸ³é¢‘æ•°æ®çš„é‡‡æ ·ç‡è¦ä¸ç”¨äºé¢„è®­ç»ƒæ¨¡å‹çš„æ•°æ®é›†çš„é‡‡æ ·ç‡åŒ¹é…ã€‚å¦‚æœæ‚¨çš„æ•°æ®çš„é‡‡æ ·ç‡ä¸åŒï¼Œé‚£ä¹ˆæ‚¨éœ€è¦å¯¹æ•°æ®è¿›è¡Œé‡æ–°é‡‡æ ·ã€‚

1. ä½¿ç”¨ğŸ¤— Datasetsçš„[`~datasets.Dataset.cast_column`]æ–¹æ³•å°†é‡‡æ ·ç‡æå‡åˆ°16kHzï¼š

```py
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=16_000))
```

2. å†æ¬¡è°ƒç”¨ `audio` åˆ—ä»¥é‡æ–°é‡‡æ ·éŸ³é¢‘æ–‡ä»¶ï¼š


```py
>>> dataset[0]["audio"]
{'array': array([ 2.3443763e-05,  2.1729663e-04,  2.2145823e-04, ...,
         3.8356509e-05, -7.3497440e-06, -2.1754686e-05], dtype=float32),
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',
 'sampling_rate': 16000}
```

æ¥ä¸‹æ¥ï¼ŒåŠ è½½ä¸€ä¸ª`feature extractor`ä»¥å¯¹è¾“å…¥è¿›è¡Œæ ‡å‡†åŒ–å’Œå¡«å……ã€‚å½“å¡«å……æ–‡æœ¬æ•°æ®æ—¶ï¼Œä¼šä¸ºè¾ƒçŸ­çš„åºåˆ—æ·»åŠ  `0`ã€‚ç›¸åŒçš„ç†å¿µé€‚ç”¨äºéŸ³é¢‘æ•°æ®ã€‚`feature extractor`æ·»åŠ  `0` - è¢«è§£é‡Šä¸ºé™éŸ³ - åˆ°`array` ã€‚

ä½¿ç”¨ [`AutoFeatureExtractor.from_pretrained`] åŠ è½½`feature extractor`ï¼š


```py
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base")
```

å°†éŸ³é¢‘ `array` ä¼ é€’ç»™`feature extractor`ã€‚æˆ‘ä»¬è¿˜å»ºè®®åœ¨`feature extractor`ä¸­æ·»åŠ  `sampling_rate` å‚æ•°ï¼Œä»¥æ›´å¥½åœ°è°ƒè¯•å¯èƒ½å‘ç”Ÿçš„é™éŸ³é”™è¯¯ï¼š


```py
>>> audio_input = [dataset[0]["audio"]["array"]]
>>> feature_extractor(audio_input, sampling_rate=16000)
{'input_values': [array([ 3.8106556e-04,  2.7506407e-03,  2.8015103e-03, ...,
        5.6335266e-04,  4.6588284e-06, -1.7142107e-04], dtype=float32)]}
```

å°±åƒ`tokenizer`ä¸€æ ·ï¼Œæ‚¨å¯ä»¥åº”ç”¨å¡«å……æˆ–æˆªæ–­æ¥å¤„ç†æ‰¹æ¬¡ä¸­çš„å¯å˜åºåˆ—ã€‚è¯·æŸ¥çœ‹è¿™ä¸¤ä¸ªéŸ³é¢‘æ ·æœ¬çš„åºåˆ—é•¿åº¦ï¼š


```py
>>> dataset[0]["audio"]["array"].shape
(173398,)

>>> dataset[1]["audio"]["array"].shape
(106496,)
```

åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥é¢„å¤„ç†æ•°æ®é›†ï¼Œä»¥ä½¿éŸ³é¢‘æ ·æœ¬å…·æœ‰ç›¸åŒçš„é•¿åº¦ã€‚é€šè¿‡æŒ‡å®šæœ€å¤§æ ·æœ¬é•¿åº¦ï¼Œ`feature extractor`å°†å¡«å……æˆ–æˆªæ–­åºåˆ—ä»¥ä½¿å…¶åŒ¹é…ï¼š


```py
>>> def preprocess_function(examples):
...     audio_arrays = [x["array"] for x in examples["audio"]]
...     inputs = feature_extractor(
...         audio_arrays,
...         sampling_rate=16000,
...         padding=True,
...         max_length=100000,
...         truncation=True,
...     )
...     return inputs
```

å°†`preprocess_function`åº”ç”¨äºæ•°æ®é›†ä¸­çš„å‰å‡ ä¸ªç¤ºä¾‹ï¼š


```py
>>> processed_dataset = preprocess_function(dataset[:5])
```

ç°åœ¨æ ·æœ¬é•¿åº¦æ˜¯ç›¸åŒçš„ï¼Œå¹¶ä¸”ä¸æŒ‡å®šçš„æœ€å¤§é•¿åº¦åŒ¹é…ã€‚æ‚¨ç°åœ¨å¯ä»¥å°†ç»è¿‡å¤„ç†çš„æ•°æ®é›†ä¼ é€’ç»™æ¨¡å‹äº†ï¼


```py
>>> processed_dataset["input_values"][0].shape
(100000,)

>>> processed_dataset["input_values"][1].shape
(100000,)
```

## è®¡ç®—æœºè§†è§‰

å¯¹äºè®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œæ‚¨éœ€è¦ä¸€ä¸ª[ image processor](main_classes/image_processor)æ¥å‡†å¤‡æ•°æ®é›†ä»¥ä¾›æ¨¡å‹ä½¿ç”¨ã€‚å›¾åƒé¢„å¤„ç†åŒ…æ‹¬å¤šä¸ªæ­¥éª¤å°†å›¾åƒè½¬æ¢ä¸ºæ¨¡å‹æœŸæœ›è¾“å…¥çš„æ ¼å¼ã€‚è¿™äº›æ­¥éª¤åŒ…æ‹¬ä½†ä¸é™äºè°ƒæ•´å¤§å°ã€æ ‡å‡†åŒ–ã€é¢œè‰²é€šé“æ ¡æ­£ä»¥åŠå°†å›¾åƒè½¬æ¢ä¸ºå¼ é‡ã€‚

> [!TIP]
> å›¾åƒé¢„å¤„ç†é€šå¸¸éµå¾ªæŸç§å½¢å¼çš„å›¾åƒå¢å¼ºã€‚å›¾åƒé¢„å¤„ç†å’Œå›¾åƒå¢å¼ºéƒ½ä¼šæ”¹å˜å›¾åƒæ•°æ®ï¼Œä½†å®ƒä»¬æœ‰ä¸åŒçš„ç›®çš„ï¼š
>
> * å›¾åƒå¢å¼ºå¯ä»¥å¸®åŠ©é˜²æ­¢è¿‡æ‹Ÿåˆå¹¶å¢åŠ æ¨¡å‹çš„é²æ£’æ€§ã€‚æ‚¨å¯ä»¥åœ¨æ•°æ®å¢å¼ºæ–¹é¢å……åˆ†å‘æŒ¥åˆ›é€ æ€§ - è°ƒæ•´äº®åº¦å’Œé¢œè‰²ã€è£å‰ªã€æ—‹è½¬ã€è°ƒæ•´å¤§å°ã€ç¼©æ”¾ç­‰ã€‚ä½†è¦æ³¨æ„ä¸è¦æ”¹å˜å›¾åƒçš„å«ä¹‰ã€‚
> * å›¾åƒé¢„å¤„ç†ç¡®ä¿å›¾åƒä¸æ¨¡å‹é¢„æœŸçš„è¾“å…¥æ ¼å¼åŒ¹é…ã€‚åœ¨å¾®è°ƒè®¡ç®—æœºè§†è§‰æ¨¡å‹æ—¶ï¼Œå¿…é¡»å¯¹å›¾åƒè¿›è¡Œä¸æ¨¡å‹è®­ç»ƒæ—¶ç›¸åŒçš„é¢„å¤„ç†ã€‚
>
> æ‚¨å¯ä»¥ä½¿ç”¨ä»»ä½•æ‚¨å–œæ¬¢çš„å›¾åƒå¢å¼ºåº“ã€‚å¯¹äºå›¾åƒé¢„å¤„ç†ï¼Œè¯·ä½¿ç”¨ä¸æ¨¡å‹ç›¸å…³è”çš„`ImageProcessor`ã€‚

åŠ è½½[food101](https://huggingface.co/datasets/food101)æ•°æ®é›†ï¼ˆæœ‰å…³å¦‚ä½•åŠ è½½æ•°æ®é›†çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…ğŸ¤— [Datasetsæ•™ç¨‹](https://huggingface.co/docs/datasets/load_hub)ï¼‰ä»¥äº†è§£å¦‚ä½•åœ¨è®¡ç®—æœºè§†è§‰æ•°æ®é›†ä¸­ä½¿ç”¨å›¾åƒå¤„ç†å™¨ï¼š

> [!TIP]
> å› ä¸ºæ•°æ®é›†ç›¸å½“å¤§ï¼Œè¯·ä½¿ç”¨ğŸ¤— Datasetsçš„`split`å‚æ•°åŠ è½½è®­ç»ƒé›†ä¸­çš„å°‘é‡æ ·æœ¬ï¼


```py
>>> from datasets import load_dataset

>>> dataset = load_dataset("food101", split="train[:100]")
```

æ¥ä¸‹æ¥ï¼Œä½¿ç”¨ğŸ¤— Datasetsçš„[`Image`](https://huggingface.co/docs/datasets/package_reference/main_classes?highlight=image#datasets.Image)åŠŸèƒ½æŸ¥çœ‹å›¾åƒï¼š


```py
>>> dataset[0]["image"]
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/vision-preprocess-tutorial.png"/>
</div>

ä½¿ç”¨ [`AutoImageProcessor.from_pretrained`] åŠ è½½`image processor`ï¼š

```py
>>> from transformers import AutoImageProcessor

>>> image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
```

é¦–å…ˆï¼Œè®©æˆ‘ä»¬è¿›è¡Œå›¾åƒå¢å¼ºã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä»»ä½•æ‚¨å–œæ¬¢çš„åº“ï¼Œä½†åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨torchvisionçš„[`transforms`](https://pytorch.org/vision/stable/transforms.html)æ¨¡å—ã€‚å¦‚æœæ‚¨æœ‰å…´è¶£ä½¿ç”¨å…¶ä»–æ•°æ®å¢å¼ºåº“ï¼Œè¯·å‚é˜…[Albumentations](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb)æˆ–[Kornia notebooks](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_kornia.ipynb)ä¸­çš„ç¤ºä¾‹ã€‚

1. åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨[`Compose`](https://pytorch.org/vision/master/generated/torchvision.transforms.Compose.html)å°†[`RandomResizedCrop`](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html)å’Œ [`ColorJitter`](https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html)å˜æ¢è¿æ¥åœ¨ä¸€èµ·ã€‚è¯·æ³¨æ„ï¼Œå¯¹äºè°ƒæ•´å¤§å°ï¼Œæˆ‘ä»¬å¯ä»¥ä»`image_processor`ä¸­è·å–å›¾åƒå°ºå¯¸è¦æ±‚ã€‚å¯¹äºä¸€äº›æ¨¡å‹ï¼Œç²¾ç¡®çš„é«˜åº¦å’Œå®½åº¦éœ€è¦è¢«å®šä¹‰ï¼Œå¯¹äºå…¶ä»–æ¨¡å‹åªéœ€å®šä¹‰`shortest_edge`ã€‚


```py
>>> from torchvision.transforms import RandomResizedCrop, ColorJitter, Compose

>>> size = (
...     image_processor.size["shortest_edge"]
...     if "shortest_edge" in image_processor.size
...     else (image_processor.size["height"], image_processor.size["width"])
... )

>>> _transforms = Compose([RandomResizedCrop(size), ColorJitter(brightness=0.5, hue=0.5)])
```

2. æ¨¡å‹æ¥å— [`pixel_values`](model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderModel.forward.pixel_values) ä½œä¸ºè¾“å…¥ã€‚`ImageProcessor` å¯ä»¥è¿›è¡Œå›¾åƒçš„æ ‡å‡†åŒ–ï¼Œå¹¶ç”Ÿæˆé€‚å½“çš„å¼ é‡ã€‚åˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œå°†å›¾åƒå¢å¼ºå’Œå›¾åƒé¢„å¤„ç†æ­¥éª¤ç»„åˆèµ·æ¥å¤„ç†æ‰¹é‡å›¾åƒï¼Œå¹¶ç”Ÿæˆ `pixel_values`ï¼š


```py
>>> def transforms(examples):
...     images = [_transforms(img.convert("RGB")) for img in examples["image"]]
...     examples["pixel_values"] = image_processor(images, do_resize=False, return_tensors="pt")["pixel_values"]
...     return examples
```

> [!TIP]
> åœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬è®¾ç½®`do_resize=False`ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»åœ¨å›¾åƒå¢å¼ºè½¬æ¢ä¸­è°ƒæ•´äº†å›¾åƒçš„å¤§å°ï¼Œå¹¶åˆ©ç”¨äº†é€‚å½“çš„`image_processor`çš„`size`å±æ€§ã€‚å¦‚æœæ‚¨åœ¨å›¾åƒå¢å¼ºæœŸé—´ä¸è°ƒæ•´å›¾åƒçš„å¤§å°ï¼Œè¯·å°†æ­¤å‚æ•°æ’é™¤åœ¨å¤–ã€‚é»˜è®¤æƒ…å†µä¸‹`ImageProcessor`å°†å¤„ç†è°ƒæ•´å¤§å°ã€‚
>
> å¦‚æœå¸Œæœ›å°†å›¾åƒæ ‡å‡†åŒ–æ­¥éª¤ä¸ºå›¾åƒå¢å¼ºçš„ä¸€éƒ¨åˆ†ï¼Œè¯·ä½¿ç”¨`image_processor.image_mean`å’Œ`image_processor.image_std`ã€‚

3. ç„¶åä½¿ç”¨ğŸ¤— Datasetsçš„[`set_transform`](https://huggingface.co/docs/datasets/process#format-transform)åœ¨è¿è¡Œæ—¶åº”ç”¨è¿™äº›å˜æ¢ï¼š


```py
>>> dataset.set_transform(transforms)
```

4. ç°åœ¨ï¼Œå½“æ‚¨è®¿é—®å›¾åƒæ—¶ï¼Œæ‚¨å°†æ³¨æ„åˆ°`image processor`å·²æ·»åŠ äº† `pixel_values`ã€‚æ‚¨ç°åœ¨å¯ä»¥å°†ç»è¿‡å¤„ç†çš„æ•°æ®é›†ä¼ é€’ç»™æ¨¡å‹äº†ï¼


```py
>>> dataset[0].keys()
```

è¿™æ˜¯åœ¨åº”ç”¨å˜æ¢åçš„å›¾åƒæ ·å­ã€‚å›¾åƒå·²è¢«éšæœºè£å‰ªï¼Œå¹¶å…¶é¢œè‰²å±æ€§å‘ç”Ÿäº†å˜åŒ–ã€‚


```py
>>> import numpy as np
>>> import matplotlib.pyplot as plt

>>> img = dataset[0]["pixel_values"]
>>> plt.imshow(img.permute(1, 2, 0))
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/preprocessed_image.png"/>
</div>

> [!TIP]
> å¯¹äºè¯¸å¦‚ç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ã€å®ä¾‹åˆ†å‰²å’Œå…¨æ™¯åˆ†å‰²ç­‰ä»»åŠ¡ï¼Œ`ImageProcessor`æä¾›äº†è®­ç»ƒåå¤„ç†æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•å°†æ¨¡å‹çš„åŸå§‹è¾“å‡ºè½¬æ¢ä¸ºæœ‰æ„ä¹‰çš„é¢„æµ‹ï¼Œå¦‚è¾¹ç•Œæ¡†æˆ–åˆ†å‰²åœ°å›¾ã€‚

### å¡«å……

åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä¾‹å¦‚ï¼Œåœ¨å¾®è°ƒ[DETR](./model_doc/detr)æ—¶ï¼Œæ¨¡å‹åœ¨è®­ç»ƒæ—¶åº”ç”¨äº†å°ºåº¦å¢å¼ºã€‚è¿™å¯èƒ½å¯¼è‡´æ‰¹å¤„ç†ä¸­çš„å›¾åƒå¤§å°ä¸åŒã€‚æ‚¨å¯ä»¥ä½¿ç”¨[`DetrImageProcessor.pad`]æ¥æŒ‡å®šè‡ªå®šä¹‰çš„`collate_fn`å°†å›¾åƒæ‰¹å¤„ç†åœ¨ä¸€èµ·ã€‚

```py
>>> def collate_fn(batch):
...     pixel_values = [item["pixel_values"] for item in batch]
...     encoding = image_processor.pad(pixel_values, return_tensors="pt")
...     labels = [item["labels"] for item in batch]
...     batch = {}
...     batch["pixel_values"] = encoding["pixel_values"]
...     batch["pixel_mask"] = encoding["pixel_mask"]
...     batch["labels"] = labels
...     return batch
```

## å¤šæ¨¡æ€

å¯¹äºæ¶‰åŠå¤šæ¨¡æ€è¾“å…¥çš„ä»»åŠ¡ï¼Œæ‚¨éœ€è¦[processor](main_classes/processors)æ¥ä¸ºæ¨¡å‹å‡†å¤‡æ•°æ®é›†ã€‚`processor`å°†ä¸¤ä¸ªå¤„ç†å¯¹è±¡-ä¾‹å¦‚`tokenizer`å’Œ`feature extractor`-ç»„åˆåœ¨ä¸€èµ·ã€‚

åŠ è½½[LJ Speech](https://huggingface.co/datasets/lj_speech)æ•°æ®é›†ï¼ˆæœ‰å…³å¦‚ä½•åŠ è½½æ•°æ®é›†çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…ğŸ¤— [Datasets æ•™ç¨‹](https://huggingface.co/docs/datasets/load_hub)ï¼‰ä»¥äº†è§£å¦‚ä½•ä½¿ç”¨`processor`è¿›è¡Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ï¼š


```py
>>> from datasets import load_dataset

>>> lj_speech = load_dataset("lj_speech", split="train")
```

å¯¹äºASRï¼ˆè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼‰ï¼Œä¸»è¦å…³æ³¨`audio`å’Œ`text`ï¼Œå› æ­¤å¯ä»¥åˆ é™¤å…¶ä»–åˆ—ï¼š


```py
>>> lj_speech = lj_speech.map(remove_columns=["file", "id", "normalized_text"])
```

ç°åœ¨æŸ¥çœ‹`audio`å’Œ`text`åˆ—ï¼š

```py
>>> lj_speech[0]["audio"]
{'array': array([-7.3242188e-04, -7.6293945e-04, -6.4086914e-04, ...,
         7.3242188e-04,  2.1362305e-04,  6.1035156e-05], dtype=float32),
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/917ece08c95cf0c4115e45294e3cd0dee724a1165b7fc11798369308a465bd26/LJSpeech-1.1/wavs/LJ001-0001.wav',
 'sampling_rate': 22050}

>>> lj_speech[0]["text"]
'Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition'
```

è¯·è®°ä½ï¼Œæ‚¨åº”å§‹ç»ˆ[é‡æ–°é‡‡æ ·](preprocessing#audio)éŸ³é¢‘æ•°æ®é›†çš„é‡‡æ ·ç‡ï¼Œä»¥åŒ¹é…ç”¨äºé¢„è®­ç»ƒæ¨¡å‹æ•°æ®é›†çš„é‡‡æ ·ç‡ï¼


```py
>>> lj_speech = lj_speech.cast_column("audio", Audio(sampling_rate=16_000))
```

ä½¿ç”¨[`AutoProcessor.from_pretrained`]åŠ è½½ä¸€ä¸ª`processor`ï¼š


```py
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")
```

1. åˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºå°†åŒ…å«åœ¨ `array` ä¸­çš„éŸ³é¢‘æ•°æ®å¤„ç†ä¸º `input_values`ï¼Œå¹¶å°† `text` æ ‡è®°ä¸º `labels`ã€‚è¿™äº›å°†æ˜¯è¾“å…¥æ¨¡å‹çš„æ•°æ®ï¼š

```py
>>> def prepare_dataset(example):
...     audio = example["audio"]

...     example.update(processor(audio=audio["array"], text=example["text"], sampling_rate=16000))

...     return example
```

2. å°† `prepare_dataset` å‡½æ•°åº”ç”¨äºä¸€ä¸ªç¤ºä¾‹ï¼š

```py
>>> prepare_dataset(lj_speech[0])
```

`processor`ç°åœ¨å·²ç»æ·»åŠ äº† `input_values` å’Œ `labels`ï¼Œå¹¶ä¸”é‡‡æ ·ç‡ä¹Ÿæ­£ç¡®é™ä½ä¸ºä¸º16kHzã€‚ç°åœ¨å¯ä»¥å°†å¤„ç†åçš„æ•°æ®é›†ä¼ é€’ç»™æ¨¡å‹ï¼
