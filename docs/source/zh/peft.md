<!--Copyright 2023 The HuggingFace Team. All rights reserved.
Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.
-->

# ä½¿ç”¨ ğŸ¤— PEFT åŠ è½½adapters

[[open-in-colab]]

[å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•](https://huggingface.co/blog/peft)åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å†»ç»“é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°ï¼Œå¹¶åœ¨å…¶é¡¶éƒ¨æ·»åŠ å°‘é‡å¯è®­ç»ƒå‚æ•°ï¼ˆadaptersï¼‰ã€‚adaptersè¢«è®­ç»ƒä»¥å­¦ä¹ ç‰¹å®šä»»åŠ¡çš„ä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•å·²è¢«è¯æ˜éå¸¸èŠ‚çœå†…å­˜ï¼ŒåŒæ—¶å…·æœ‰è¾ƒä½çš„è®¡ç®—ä½¿ç”¨é‡ï¼ŒåŒæ—¶äº§ç”Ÿä¸å®Œå…¨å¾®è°ƒæ¨¡å‹ç›¸å½“çš„ç»“æœã€‚

ä½¿ç”¨PEFTè®­ç»ƒçš„adaptersé€šå¸¸æ¯”å®Œæ•´æ¨¡å‹å°ä¸€ä¸ªæ•°é‡çº§ï¼Œä½¿å…¶æ–¹ä¾¿å…±äº«ã€å­˜å‚¨å’ŒåŠ è½½ã€‚

<div class="flex flex-col justify-center">
  <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/PEFT-hub-screenshot.png"/>
  <figcaption class="text-center">ä¸å®Œæ•´å°ºå¯¸çš„æ¨¡å‹æƒé‡ï¼ˆçº¦ä¸º700MBï¼‰ç›¸æ¯”ï¼Œå­˜å‚¨åœ¨Hubä¸Šçš„OPTForCausalLMæ¨¡å‹çš„adapteræƒé‡ä»…ä¸º~6MBã€‚</figcaption>
</div>

å¦‚æœæ‚¨å¯¹å­¦ä¹ æ›´å¤šå…³äºğŸ¤— PEFTåº“æ„Ÿå…´è¶£ï¼Œè¯·æŸ¥çœ‹[æ–‡æ¡£](https://huggingface.co/docs/peft/index)ã€‚


## è®¾ç½®

é¦–å…ˆå®‰è£… ğŸ¤— PEFTï¼š

```bash
pip install peft
```

å¦‚æœä½ æƒ³å°è¯•å…¨æ–°çš„ç‰¹æ€§ï¼Œä½ å¯èƒ½ä¼šæœ‰å…´è¶£ä»æºä»£ç å®‰è£…è¿™ä¸ªåº“ï¼š

```bash
pip install git+https://github.com/huggingface/peft.git
```
## æ”¯æŒçš„ PEFT æ¨¡å‹

TransformersåŸç”Ÿæ”¯æŒä¸€äº›PEFTæ–¹æ³•ï¼Œè¿™æ„å‘³ç€ä½ å¯ä»¥åŠ è½½æœ¬åœ°å­˜å‚¨æˆ–åœ¨Hubä¸Šçš„adapteræƒé‡ï¼Œå¹¶ä½¿ç”¨å‡ è¡Œä»£ç è½»æ¾è¿è¡Œæˆ–è®­ç»ƒå®ƒä»¬ã€‚ä»¥ä¸‹æ˜¯å—æ”¯æŒçš„æ–¹æ³•ï¼š

- [Low Rank Adapters](https://huggingface.co/docs/peft/conceptual_guides/lora)
- [IA3](https://huggingface.co/docs/peft/conceptual_guides/ia3)
- [AdaLoRA](https://arxiv.org/abs/2303.10512)

å¦‚æœä½ æƒ³ä½¿ç”¨å…¶ä»–PEFTæ–¹æ³•ï¼Œä¾‹å¦‚æç¤ºå­¦ä¹ æˆ–æç¤ºå¾®è°ƒï¼Œæˆ–è€…å…³äºé€šç”¨çš„ ğŸ¤— PEFTåº“ï¼Œè¯·å‚é˜…[æ–‡æ¡£](https://huggingface.co/docs/peft/index)ã€‚

## åŠ è½½ PEFT adapter

è¦ä»huggingfaceçš„Transformersåº“ä¸­åŠ è½½å¹¶ä½¿ç”¨PEFTadapteræ¨¡å‹ï¼Œè¯·ç¡®ä¿Hubä»“åº“æˆ–æœ¬åœ°ç›®å½•åŒ…å«ä¸€ä¸ª`adapter_config.json`æ–‡ä»¶å’Œadapteræƒé‡ï¼Œå¦‚ä¸Šä¾‹æ‰€ç¤ºã€‚ç„¶åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`AutoModelFor`ç±»åŠ è½½PEFT adapteræ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œè¦ä¸ºå› æœè¯­è¨€å»ºæ¨¡åŠ è½½ä¸€ä¸ªPEFT adapteræ¨¡å‹ï¼š

1. æŒ‡å®šPEFTæ¨¡å‹id
2. å°†å…¶ä¼ é€’ç»™[`AutoModelForCausalLM`]ç±»

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

peft_model_id = "ybelkada/opt-350m-lora"
model = AutoModelForCausalLM.from_pretrained(peft_model_id)
```

<Tip>

ä½ å¯ä»¥ä½¿ç”¨`AutoModelFor`ç±»æˆ–åŸºç¡€æ¨¡å‹ç±»ï¼ˆå¦‚`OPTForCausalLM`æˆ–`LlamaForCausalLM`ï¼‰æ¥åŠ è½½ä¸€ä¸ªPEFT adapterã€‚


</Tip>

æ‚¨ä¹Ÿå¯ä»¥é€šè¿‡`load_adapter`æ–¹æ³•æ¥åŠ è½½ PEFT adapterã€‚

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "facebook/opt-350m"
peft_model_id = "ybelkada/opt-350m-lora"

model = AutoModelForCausalLM.from_pretrained(model_id)
model.load_adapter(peft_model_id)
```

## åŸºäº8bitæˆ–4bitè¿›è¡ŒåŠ è½½

`bitsandbytes`é›†æˆæ”¯æŒ8bitå’Œ4bitç²¾åº¦æ•°æ®ç±»å‹ï¼Œè¿™å¯¹äºåŠ è½½å¤§æ¨¡å‹éå¸¸æœ‰ç”¨ï¼Œå› ä¸ºå®ƒå¯ä»¥èŠ‚çœå†…å­˜ï¼ˆè¯·å‚é˜…`bitsandbytes`[æŒ‡å—](./quantization#bitsandbytes-integration)ä»¥äº†è§£æ›´å¤šä¿¡æ¯ï¼‰ã€‚è¦æœ‰æ•ˆåœ°å°†æ¨¡å‹åˆ†é…åˆ°æ‚¨çš„ç¡¬ä»¶ï¼Œè¯·åœ¨[`~PreTrainedModel.from_pretrained`]ä¸­æ·»åŠ `load_in_8bit`æˆ–`load_in_4bit`å‚æ•°ï¼Œå¹¶å°†`device_map="auto"`è®¾ç½®ä¸ºï¼š

```py
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

peft_model_id = "ybelkada/opt-350m-lora"
model = AutoModelForCausalLM.from_pretrained(peft_model_id, quantization_config=BitsAndBytesConfig(load_in_8bit=True))
```

## æ·»åŠ æ–°çš„adapter

ä½ å¯ä»¥ä½¿ç”¨[`~peft.PeftModel.add_adapter`]æ–¹æ³•ä¸ºä¸€ä¸ªå·²æœ‰adapterçš„æ¨¡å‹æ·»åŠ ä¸€ä¸ªæ–°çš„adapterï¼Œåªè¦æ–°adapterçš„ç±»å‹ä¸å½“å‰adapterç›¸åŒå³å¯ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æœ‰ä¸€ä¸ªé™„åŠ åˆ°æ¨¡å‹ä¸Šçš„LoRA adapterï¼š

```py
from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
from peft import PeftConfig

model_id = "facebook/opt-350m"
model = AutoModelForCausalLM.from_pretrained(model_id)

lora_config = LoraConfig(
    target_modules=["q_proj", "k_proj"],
    init_lora_weights=False
)

model.add_adapter(lora_config, adapter_name="adapter_1")
```


æ·»åŠ ä¸€ä¸ªæ–°çš„adapterï¼š

```py
# attach new adapter with same config
model.add_adapter(lora_config, adapter_name="adapter_2")
```
ç°åœ¨æ‚¨å¯ä»¥ä½¿ç”¨[`~peft.PeftModel.set_adapter`]æ¥è®¾ç½®è¦ä½¿ç”¨çš„adapterã€‚

```py
# use adapter_1
model.set_adapter("adapter_1")
output = model.generate(**inputs)
print(tokenizer.decode(output_disabled[0], skip_special_tokens=True))

# use adapter_2
model.set_adapter("adapter_2")
output_enabled = model.generate(**inputs)
print(tokenizer.decode(output_enabled[0], skip_special_tokens=True))
```

## å¯ç”¨å’Œç¦ç”¨adapters
ä¸€æ—¦æ‚¨å°†adapteræ·»åŠ åˆ°æ¨¡å‹ä¸­ï¼Œæ‚¨å¯ä»¥å¯ç”¨æˆ–ç¦ç”¨adapteræ¨¡å—ã€‚è¦å¯ç”¨adapteræ¨¡å—ï¼š


```py
from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
from peft import PeftConfig

model_id = "facebook/opt-350m"
adapter_model_id = "ybelkada/opt-350m-lora"
tokenizer = AutoTokenizer.from_pretrained(model_id)
text = "Hello"
inputs = tokenizer(text, return_tensors="pt")

model = AutoModelForCausalLM.from_pretrained(model_id)
peft_config = PeftConfig.from_pretrained(adapter_model_id)

# to initiate with random weights
peft_config.init_lora_weights = False

model.add_adapter(peft_config)
model.enable_adapters()
output = model.generate(**inputs)
```
è¦ç¦ç”¨adapteræ¨¡å—ï¼š

```py
model.disable_adapters()
output = model.generate(**inputs)
```
## è®­ç»ƒä¸€ä¸ª PEFT adapter

PEFTé€‚é…å™¨å—[`Trainer`]ç±»æ”¯æŒï¼Œå› æ­¤æ‚¨å¯ä»¥ä¸ºæ‚¨çš„ç‰¹å®šç”¨ä¾‹è®­ç»ƒé€‚é…å™¨ã€‚å®ƒåªéœ€è¦æ·»åŠ å‡ è¡Œä»£ç å³å¯ã€‚ä¾‹å¦‚ï¼Œè¦è®­ç»ƒä¸€ä¸ªLoRA adapterï¼š


<Tip>

å¦‚æœä½ ä¸ç†Ÿæ‚‰å¦‚ä½•ä½¿ç”¨[`Trainer`]å¾®è°ƒæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹[å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹](training)æ•™ç¨‹ã€‚

</Tip>

1. ä½¿ç”¨ä»»åŠ¡ç±»å‹å’Œè¶…å‚æ•°å®šä¹‰adapteré…ç½®ï¼ˆå‚è§[`~peft.LoraConfig`]ä»¥äº†è§£è¶…å‚æ•°çš„è¯¦ç»†ä¿¡æ¯ï¼‰ã€‚

```py
from peft import LoraConfig

peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="CAUSAL_LM",
)
```

2. å°†adapteræ·»åŠ åˆ°æ¨¡å‹ä¸­ã€‚

```py
model.add_adapter(peft_config)
```

3. ç°åœ¨å¯ä»¥å°†æ¨¡å‹ä¼ é€’ç»™[`Trainer`]äº†ï¼

```py
trainer = Trainer(model=model, ...)
trainer.train()
```

è¦ä¿å­˜è®­ç»ƒå¥½çš„adapterå¹¶é‡æ–°åŠ è½½å®ƒï¼š

```py
model.save_pretrained(save_dir)
model = AutoModelForCausalLM.from_pretrained(save_dir)
```

<!--
TODO: (@younesbelkada @stevhliu)
-   Link to PEFT docs for further details
-   Trainer  
-   8-bit / 4-bit examples ?
-->
