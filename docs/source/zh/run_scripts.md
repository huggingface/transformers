<!--ç‰ˆæƒæ‰€æœ‰ 2022 å¹´ HuggingFace å›¢é˜Ÿã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚
æ ¹æ® Apache è®¸å¯è¯ç¬¬ 2.0 ç‰ˆï¼ˆâ€œè®¸å¯è¯â€ï¼‰è·å¾—è®¸å¯ï¼›é™¤éç¬¦åˆè®¸å¯è¯çš„è¦æ±‚ï¼Œå¦åˆ™æ‚¨ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚æ‚¨å¯ä»¥åœ¨è®¸å¯è¯çš„å‰¯æœ¬ã€‚
http://www.apache.org/licenses/LICENSE-2.0
é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œæ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯åŸºäºâ€œæŒ‰åŸæ ·â€åˆ†å‘çš„ï¼Œä¸é™„å¸¦ä»»ä½•å½¢å¼çš„æ‹…ä¿æˆ–æ¡ä»¶ï¼Œæ— è®ºæ˜¯æ˜ç¤ºè¿˜æ˜¯æš—ç¤ºã€‚æœ‰å…³è®¸å¯è¯çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…ç‰¹å®šè¯­è¨€ä¸‹çš„æƒé™å’Œé™åˆ¶ã€‚è®¸å¯è¯ã€‚
âš ï¸ è¯·æ³¨æ„ï¼Œæ­¤æ–‡ä»¶æ˜¯ Markdown æ ¼å¼ï¼Œä½†åŒ…å«æˆ‘ä»¬çš„æ–‡æ¡£æ„å»ºå™¨çš„ç‰¹å®šè¯­æ³•ï¼ˆç±»ä¼¼ MDXï¼‰ï¼Œå¯èƒ½æ— æ³•åœ¨æ‚¨çš„ Markdown æŸ¥çœ‹å™¨ä¸­æ­£ç¡®å‘ˆç°ã€‚
-->

# ä½¿ç”¨è„šæœ¬è¿›è¡Œè®­ç»ƒ

é™¤äº†ğŸ¤— Transformers [notebooks](./noteboks/README) ä¹‹å¤–ï¼Œè¿˜æœ‰æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ [PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch)ã€[TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow) æˆ– [JAX/Flax](https://github.com/huggingface/transformers/tree/main/examples/flax) è®­ç»ƒæ¨¡å‹çš„ç¤ºä¾‹è„šæœ¬ã€‚

æ‚¨è¿˜å°†åœ¨æˆ‘ä»¬çš„ [ç ”ç©¶é¡¹ç›®](https://github.com/huggingface/transformers/tree/main/examples/research_projects) å’Œ [æ—§ç¤ºä¾‹](https://github.com/huggingface/transformers/tree/main/examples/legacy) ä¸­æ‰¾åˆ°æˆ‘ä»¬ä½¿ç”¨çš„è„šæœ¬ï¼Œè¿™äº›è„šæœ¬ä¸»è¦ç”±ç¤¾åŒºè´¡çŒ®ã€‚è¿™äº›è„šæœ¬æ²¡æœ‰è¢«ç§¯æç»´æŠ¤ï¼Œå¹¶ä¸”éœ€è¦ä¸æœ€æ–°ç‰ˆæœ¬çš„åº“ä¸å…¼å®¹çš„ç‰¹å®šç‰ˆæœ¬çš„ğŸ¤— Transformersã€‚

ä¸å¸Œæœ›ç¤ºä¾‹è„šæœ¬èƒ½å¤Ÿç«‹å³åœ¨æ¯ä¸ªé—®é¢˜ä¸Šæ­£å¸¸å·¥ä½œï¼Œå¹¶ä¸”æ‚¨å¯èƒ½éœ€è¦æ ¹æ®æ‚¨è¦è§£å†³çš„é—®é¢˜è°ƒæ•´è„šæœ¬ã€‚ä¸ºäº†å¸®åŠ©æ‚¨ï¼Œå¤§å¤šæ•°è„šæœ¬å®Œå…¨æš´éœ²äº†æ•°æ®çš„é¢„å¤„ç†æ–¹å¼ï¼Œå…è®¸æ‚¨æ ¹æ®éœ€è¦è¿›è¡Œç¼–è¾‘ä»¥é€‚åº”æ‚¨çš„ç”¨ä¾‹ã€‚

å¦‚æœæ‚¨æƒ³åœ¨ç¤ºä¾‹è„šæœ¬ä¸­å®ç°ä»»ä½•åŠŸèƒ½ï¼Œè¯·åœ¨æäº¤ Pull Request ä¹‹å‰åœ¨ [è®ºå›](https://discuss.huggingface.co/) æˆ– [é—®é¢˜](https://github.com/huggingface/transformers/issues) ä¸­è¿›è¡Œè®¨è®ºã€‚è™½ç„¶æˆ‘ä»¬æ¬¢è¿ä¿®å¤é”™è¯¯ï¼Œä½†æˆ‘ä»¬ä¸å¤ªå¯èƒ½åˆå¹¶æ·»åŠ æ›´å¤šåŠŸèƒ½ä½†å¯è¯»æ€§é™ä½çš„ Pull Requestã€‚

æœ¬æŒ‡å—å°†æ¼”ç¤ºå¦‚ä½•åœ¨ [PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization) å’Œ [TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/summarization) ä¸­è¿è¡Œä¸€ä¸ªç¤ºä¾‹æ‘˜è¦è®­ç»ƒè„šæœ¬ã€‚é™¤éå¦æœ‰è¯´æ˜ï¼Œæ‰€æœ‰ç¤ºä¾‹éƒ½é¢„è®¡å¯ä¸è¿™ä¸¤ä¸ªæ¡†æ¶ä¸€èµ·å·¥ä½œã€‚

## è®¾ç½®

è¦æˆåŠŸè¿è¡Œæœ€æ–°ç‰ˆæœ¬çš„ç¤ºä¾‹è„šæœ¬ï¼Œæ‚¨éœ€è¦åœ¨æ–°çš„è™šæ‹Ÿç¯å¢ƒä¸­ **ä»æºä»£ç å®‰è£…ğŸ¤— Transformers**ï¼š
```bash
git clone https://github.com/huggingface/transformers
cd transformers
pip install .
```

å¯¹äºæ—§ç‰ˆæœ¬çš„ç¤ºä¾‹è„šæœ¬ï¼Œè¯·å•å‡»ä¸‹é¢çš„åˆ‡æ¢ï¼š
<details>  <summary> ğŸ¤— Transformers æ—§ç‰ˆæœ¬ç¤ºä¾‹ </summary>	<ul>		<li> <a href="https://github.com/huggingface/transformers/tree/v4.5.1/examples"> v4.5.1 </a> </li>		<li> <a href="https://github.com/huggingface/transformers/tree/v4.4.2/examples"> v4.4.2 </a> </li>		<li> <a href="https://github.com/huggingface/transformers/tree/v4.3.3/examples"> v4.3.3 </a> </li>		<li> <a href="https://github.com/huggingface/transformers/tree/v4.2.2/examples"> v4.2.2 </a> </li>		<li> <a href="https://github.com/huggingface/transformers/tree/v4.1.1/examples"> v4.1.1 </a> </li>		<li> <a href="https://github.com/huggingface/transformers/tree/v4.0.1/examples"> v4.0.1 </a> </li>		<li> <a href="https://github.com/huggingface/transformers/tree/v3.5.1/examples"> v3.5.1 </a> </li>		<li> <a href="https://github.com/huggingface/transformers/tree/v3.4.0/examples"> v3.4.0 </a> </li>		<li> <a href="https://github.com/huggingface/transformers/tree/v3.3.1/examples"> v3.3.1 </a> </li>		<li> <a href="https://github.com/huggingface/transformers/tree/v3.2.0/examples"> v3.2.0 </a> </li>		<li> <a href="https://github.com/huggingface/transformers/tree/v3.1.0/examples"> v3.1.0 </a> </li>		<li> <a href="https://github.com/huggingface/transformers/tree/v3.0.2/examples"> v3.0.2 </a> </li>		<li> <a href="https://github.com/huggingface/transformers/tree/v2.11.0/examples"> v2.11.0 </a> </li>		<li> <a href="https://github.com/huggingface/transformers/tree/v2.10.0/examples"> v2.10.0 </a> </li>		<li> <a href="https://github.com/huggingface/transformers/tree/v2.9.1/examples"> v2.9.1 </a> </li>		<li> <a href="https://github.com/huggingface/transformers/tree/v2.8.0/examples"> v2.8.0 </a> </li>		<li> <a href="https://github.com/huggingface/transformers/tree/v2.7.0/examples"> v2.7.0 </a> </li>		<li> <a href="https://github.com/huggingface/transformers/tree/v2.6.0/examples"> v2.6.0 </a> </li>		<li> <a href="https://github.com/huggingface/transformers/tree/v2.5.1/examples"> v2.5.1 </a> </li>		<li> <a href="https://github.com/huggingface/transformers/tree/v2.4.0/examples"> v2.4.0 </a> </li>		<li> <a href="https://github.com/huggingface/transformers/tree/v2.3.0/examples"> v2.3.0 </a> </li>		<li> <a href="https://github.com/huggingface/transformers/tree/v2.2.0/examples"> v2.2.0 </a> </li>		<li> <a href="https://github.com/huggingface/transformers/tree/v2.1.0/examples"> v2.1.1 </a> </li>		<li> <a href="https://github.com/huggingface/transformers/tree/v2.0.0/examples"> v2.0.0 </a> </li>		<li> <a href="https://github.com/huggingface/transformers/tree/v1.2.0/examples"> v1.2.0 </a> </li>		<li> <a href="https://github.com/huggingface/transformers/tree/v1.1.0/examples"> v1.1.0 </a> </li>		<li> <a href="https://github.com/huggingface/transformers/tree/v1.0.0/examples"> v1.0.0 </a> </li>	</ul> </details>
ç„¶åå°†æ‚¨å½“å‰çš„ğŸ¤— Transformers å…‹éš†åˆ‡æ¢åˆ°ç‰¹å®šç‰ˆæœ¬ï¼Œä¾‹å¦‚ v3.5.1ï¼š
```bash
git checkout tags/v3.5.1
```

è®¾ç½®æ­£ç¡®çš„åº“ç‰ˆæœ¬åï¼Œè½¬åˆ°æ‚¨é€‰æ‹©çš„ç¤ºä¾‹æ–‡ä»¶å¤¹å¹¶å®‰è£…ç¤ºä¾‹ç‰¹å®šè¦æ±‚ï¼š
```bash
pip install -r requirements.txt
```

## è¿è¡Œè„šæœ¬

<frameworkcontent> 
<pt> 
 ç¤ºä¾‹è„šæœ¬ä»ğŸ¤— [æ•°æ®é›†](https://huggingface.co/docs/datasets/) åº“ä¸‹è½½å’Œé¢„å¤„ç†æ•°æ®é›†ã€‚ç„¶åï¼Œè„šæœ¬ä½¿ç”¨ [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) åœ¨æ”¯æŒæ‘˜è¦çš„æ¶æ„ä¸Šå¯¹æ•°æ®é›†è¿›è¡Œå¾®è°ƒã€‚ä»¥ä¸‹ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•åœ¨ [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail) æ•°æ®é›†ä¸Šå¯¹ [T5-small](https://huggingface.co/t5-small) è¿›è¡Œå¾®è°ƒã€‚ç”±äº T5 æ˜¯å¦‚ä½•è®­ç»ƒçš„ï¼ŒT5 æ¨¡å‹éœ€è¦é¢å¤–çš„ `source_prefix` å‚æ•°ã€‚æ­¤æç¤ºè®© T5 çŸ¥é“è¿™æ˜¯ä¸€ä¸ªæ‘˜è¦ä»»åŠ¡ã€‚
```bash
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```
</pt> 
<tf> 

ç¤ºä¾‹è„šæœ¬ä»ğŸ¤— [æ•°æ®é›†](https://huggingface.co/docs/datasets/) åº“ä¸‹è½½å’Œé¢„å¤„ç†æ•°æ®é›†ã€‚ç„¶åï¼Œè„šæœ¬åœ¨æ”¯æŒæ‘˜è¦çš„æ¶æ„ä¸Šä½¿ç”¨ Keras å¯¹æ•°æ®é›†è¿›è¡Œå¾®è°ƒã€‚ä»¥ä¸‹ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•åœ¨ [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail) æ•°æ®é›†ä¸Šå¯¹ [T5-small](https://huggingface.co/t5-small) è¿›è¡Œå¾®è°ƒã€‚ç”±äº T5 æ˜¯å¦‚ä½•è®­ç»ƒçš„ï¼ŒT5 æ¨¡å‹éœ€è¦é¢å¤–çš„ `source_prefix` å‚æ•°ã€‚æ­¤æç¤ºè®© T5 çŸ¥é“è¿™æ˜¯ä¸€ä¸ªæ‘˜è¦ä»»åŠ¡ã€‚

```bash
python examples/tensorflow/summarization/run_summarization.py  \
    --model_name_or_path t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --output_dir /tmp/tst-summarization  \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 16 \
    --num_train_epochs 3 \
    --do_train \
    --do_eval
```
</tf>
</frameworkcontent>


## åˆ†å¸ƒå¼è®­ç»ƒå’Œæ··åˆç²¾åº¦

[Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒå’Œæ··åˆç²¾åº¦ï¼Œè¿™æ„å‘³ç€æ‚¨ä¹Ÿå¯ä»¥åœ¨è„šæœ¬ä¸­ä½¿ç”¨å®ƒã€‚

è¦å¯ç”¨è¿™ä¸¤ä¸ªåŠŸèƒ½ï¼š
- æ·»åŠ  `fp16` å‚æ•°ä»¥å¯ç”¨æ··åˆç²¾åº¦ã€‚- ä½¿ç”¨ `nproc_per_node` å‚æ•°è®¾ç½®è¦ä½¿ç”¨çš„ GPU æ•°é‡ã€‚

```bash
python -m torch.distributed.launch \
    --nproc_per_node 8 pytorch/summarization/run_summarization.py \
    --fp16 \
    --model_name_or_path t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

TensorFlow è„šæœ¬ä½¿ç”¨ [`MirroredStrategy`](https://www.tensorflow.org/guide/distributed_training#mirroredstrategy) è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒï¼Œæ‚¨ä¸éœ€è¦å‘è®­ç»ƒè„šæœ¬æ·»åŠ ä»»ä½•é¢å¤–çš„å‚æ•°ã€‚å¦‚æœå¯ç”¨ï¼ŒTensorFlow è„šæœ¬å°†é»˜è®¤ä½¿ç”¨å¤šä¸ª GPUã€‚

## åœ¨ TPU ä¸Šè¿è¡Œè„šæœ¬

<frameworkcontent> 
<pt> 
 Tensor Processing Units (TPUs)ä¸“ä¸ºæé«˜æ€§èƒ½è€Œè®¾è®¡ã€‚PyTorch ä½¿ç”¨ [XLA](https://www.tensorflow.org/xla) æ·±åº¦å­¦ä¹ ç¼–è¯‘å™¨æ¥æ”¯æŒ TPUï¼ˆæ›´å¤šç»†èŠ‚è¯·å‚è§ [æ­¤å¤„](https://github.com/pytorch/xla/blob/master/README.md)ï¼‰ã€‚è¦ä½¿ç”¨ TPUï¼Œè¯·å¯åŠ¨ `xla_spawn.py` è„šæœ¬ï¼Œå¹¶ä½¿ç”¨ `num_cores` å‚æ•°è®¾ç½®è¦ä½¿ç”¨çš„ TPU æ ¸å¿ƒæ•°é‡ã€‚
```bash
python xla_spawn.py --num_cores 8 \
    summarization/run_summarization.py \
    --model_name_or_path t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```
</pt> 
<tf> 

Tensor Processing Units (TPUs)ä¸“ä¸ºæé«˜æ€§èƒ½è€Œè®¾è®¡ã€‚TensorFlow è„šæœ¬ä½¿ç”¨ [`TPUStrategy`](https://www.tensorflow.org/guide/distributed_training#tpustrategy) åœ¨ TPU ä¸Šè¿›è¡Œè®­ç»ƒã€‚

è¦ä½¿ç”¨ TPUï¼Œè¯·å°† TPU èµ„æºçš„åç§°ä¼ é€’ç»™ `tpu` å‚æ•°ã€‚

```bash
python run_summarization.py  \
    --tpu name_of_tpu_resource \
    --model_name_or_path t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --output_dir /tmp/tst-summarization  \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 16 \
    --num_train_epochs 3 \
    --do_train \
    --do_eval
```
</tf>
</frameworkcontent>


## ä½¿ç”¨ğŸ¤— Accelerate è¿è¡Œè„šæœ¬

ğŸ¤— [Accelerate](https://huggingface.co/docs/accelerate) æ˜¯ä¸€ä¸ªä»…é€‚ç”¨äº PyTorch çš„åº“ï¼Œå®ƒæä¾›äº†ä¸€ç§åœ¨å¤šç§ç±»å‹çš„è®¾ç½®ï¼ˆä»… CPUã€å¤šä¸ª GPUã€TPUï¼‰ä¸Šè®­ç»ƒæ¨¡å‹çš„ç»Ÿä¸€æ–¹æ³•ï¼ŒåŒæ—¶å®Œå…¨å¯è§ PyTorch è®­ç»ƒå¾ªç¯ã€‚

å¦‚æœå°šæœªå®‰è£…ğŸ¤— Accelerateï¼Œè¯·ç¡®ä¿å®‰è£…äº†å®ƒï¼š
> æ³¨æ„ï¼šç”±äº Accelerate æ­£åœ¨å¿«é€Ÿå¼€å‘ä¸­ï¼Œå¿…é¡»å®‰è£…åŠ é€Ÿç‰ˆæœ¬çš„ git æ‰èƒ½è¿è¡Œè„šæœ¬ã€‚

```bash
pip install git+https://github.com/huggingface/accelerate
```

æ‚¨éœ€è¦ä½¿ç”¨ `run_summarization_no_trainer.py` è„šæœ¬è€Œä¸æ˜¯ `run_summarization.py` è„šæœ¬ã€‚æ”¯æŒğŸ¤— Accelerate çš„è„šæœ¬å°†åœ¨æ–‡ä»¶å¤¹ä¸­æœ‰ä¸€ä¸ª `task_no_trainer.py` æ–‡ä»¶ã€‚é¦–å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤ä»¥åˆ›å»ºå¹¶ä¿å­˜é…ç½®æ–‡ä»¶ï¼š
```bash
accelerate config
```

ä½¿ç”¨ä»¥ä¸‹å‚æ•°æµ‹è¯•æ‚¨çš„è®¾ç½®ä»¥ç¡®ä¿é…ç½®æ­£ç¡®ï¼š
```bash
accelerate test
```

ç°åœ¨ï¼Œæ‚¨å¯ä»¥å¯åŠ¨è®­ç»ƒäº†ï¼š
```bash
accelerate launch run_summarization_no_trainer.py \
    --model_name_or_path t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir ~/tmp/tst-summarization
```

## ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†
æ‘˜è¦è„šæœ¬æ”¯æŒè‡ªå®šä¹‰æ•°æ®é›†ï¼Œåªè¦å®ƒä»¬æ˜¯ CSV æˆ– JSON Line æ–‡ä»¶å³å¯ã€‚å½“ä½¿ç”¨è‡ªå·±çš„æ•°æ®é›†æ—¶ï¼Œæ‚¨éœ€è¦æŒ‡å®šå‡ ä¸ªé¢å¤–çš„å‚æ•°ï¼š
- `train_file` å’Œ `validation_file` æŒ‡å®šè®­ç»ƒå’ŒéªŒè¯æ–‡ä»¶çš„è·¯å¾„ã€‚- `text_column` æ˜¯è¦æ‘˜è¦çš„è¾“å…¥æ–‡æœ¬ã€‚- `summary_column` æ˜¯è¦è¾“å‡ºçš„ç›®æ ‡æ–‡æœ¬ã€‚
ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†çš„æ‘˜è¦è„šæœ¬å¦‚ä¸‹æ‰€ç¤ºï¼š
```bash
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path t5-small \
    --do_train \
    --do_eval \
    --train_file path_to_csv_or_jsonlines_file \
    --validation_file path_to_csv_or_jsonlines_file \
    --text_column text_column_name \
    --summary_column summary_column_name \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --overwrite_output_dir \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --predict_with_generate
```


## æµ‹è¯•è„šæœ¬

åœ¨æäº¤æ•´ä¸ªæ•°æ®é›†ä¹‹å‰ï¼Œå°†è„šæœ¬åº”ç”¨äºè¾ƒå°‘æ•°é‡çš„æ•°æ®é›†ç¤ºä¾‹é€šå¸¸æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ï¼Œä»¥ç¡®ä¿ä¸€åˆ‡æ­£å¸¸è¿è¡Œã€‚ä½¿ç”¨ä»¥ä¸‹å‚æ•°å°†æ•°æ®é›†æˆªæ–­ä¸ºæœ€å¤§æ ·æœ¬æ•°ï¼š

- `max_train_samples`- `max_eval_samples`- `max_predict_samples`
```bash
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path t5-small \
    --max_train_samples 50 \
    --max_eval_samples 50 \
    --max_predict_samples 50 \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

å¹¶éæ‰€æœ‰ç¤ºä¾‹è„šæœ¬éƒ½æ”¯æŒ `max_predict_samples` å‚æ•°ã€‚å¦‚æœä¸ç¡®å®šè„šæœ¬æ˜¯å¦æ”¯æŒæ­¤å‚æ•°ï¼Œè¯·æ·»åŠ  `-h` å‚æ•°è¿›è¡Œæ£€æŸ¥ï¼š
```bash
examples/pytorch/summarization/run_summarization.py -h
```

## ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ

è¿˜æœ‰ä¸€ä¸ªæœ‰ç”¨çš„é€‰é¡¹æ˜¯å¯ç”¨ä»å…ˆå‰æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒçš„åŠŸèƒ½ã€‚è¿™å°†ç¡®ä¿æ‚¨å¯ä»¥åœ¨ä¸­æ–­è®­ç»ƒåç»§ç»­ä¹‹å‰çš„è¿›åº¦ï¼Œè€Œæ— éœ€é‡æ–°å¼€å§‹ã€‚æœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒã€‚

ç¬¬ä¸€ç§æ–¹æ³•ä½¿ç”¨ `output_dir previous_output_dir` å‚æ•°ä» `output_dir` ä¸­å­˜å‚¨çš„æœ€æ–°æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ

ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨åº”è¯¥åˆ é™¤ `overwrite_output_dir`ï¼š
```bash
python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --output_dir previous_output_dir \
    --predict_with_generate
```

ç¬¬äºŒç§æ–¹æ³•ä½¿ç”¨ `resume_from_checkpoint path_to_specific_checkpoint` å‚æ•°ä»ç‰¹å®šæ£€æŸ¥ç‚¹æ–‡ä»¶å¤¹æ¢å¤è®­ç»ƒã€‚
```bash
python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --resume_from_checkpoint path_to_specific_checkpoint \
    --predict_with_generate
```

## å…±äº«æ‚¨çš„æ¨¡å‹

æ‰€æœ‰è„šæœ¬éƒ½å¯ä»¥å°†æœ€ç»ˆæ¨¡å‹ä¸Šä¼ åˆ° [Model Hub](https://huggingface.co/models)ã€‚ç¡®ä¿æ‚¨å·²ç™»å½• Hugging Faceï¼š
```bash
huggingface-cli login
```

ç„¶åå°† `push_to_hub` å‚æ•°æ·»åŠ åˆ°è„šæœ¬ä¸­ã€‚æ­¤å‚æ•°å°†ä½¿ç”¨æ‚¨çš„ Hugging Face ç”¨æˆ·åå’Œ `output_dir` ä¸­æŒ‡å®šçš„æ–‡ä»¶å¤¹åç§°åˆ›å»ºä¸€ä¸ªå­˜å‚¨åº“ã€‚

è¦ä¸ºå­˜å‚¨åº“æŒ‡å®šç‰¹å®šåç§°ï¼Œè¯·ä½¿ç”¨ `push_to_hub_model_id` å‚æ•°è¿›è¡Œæ·»åŠ ã€‚å­˜å‚¨åº“å°†è‡ªåŠ¨åˆ—åœ¨æ‚¨çš„å‘½åç©ºé—´ä¸‹ã€‚

ä»¥ä¸‹ç¤ºä¾‹æ˜¾ç¤ºäº†å¦‚ä½•ä¸Šä¼ å…·æœ‰ç‰¹å®šå­˜å‚¨åº“åç§°çš„æ¨¡å‹ï¼š

```bash
python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --push_to_hub \
    --push_to_hub_model_id finetuned-t5-cnn_dailymail \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```