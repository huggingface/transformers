<!--ç‰ˆæƒæ‰€æœ‰ 2022 å¹´ HuggingFace å›¢é˜Ÿã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚
æ ¹æ® Apache è®¸å¯è¯ï¼Œç¬¬ 2 ç‰ˆï¼ˆâ€œè®¸å¯è¯â€ï¼‰æˆæƒï¼›é™¤éç¬¦åˆè®¸å¯è¯ï¼Œå¦åˆ™ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶è®¸å¯è¯ã€‚æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ä½ç½®è·å–è®¸å¯è¯çš„å‰¯æœ¬
http://www.apache.org/licenses/LICENSE-2.0
é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œæ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯æŒ‰â€œæŒ‰åŸæ ·â€åŸºç¡€åˆ†å‘çš„ï¼Œä¸é™„å¸¦ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„ä¿è¯æˆ–æ¡ä»¶ã€‚æœ‰å…³è®¸å¯è¯çš„ç‰¹å®šè¯­è¨€çš„æƒé™å’Œé™åˆ¶ï¼Œè¯·å‚é˜…è®¸å¯è¯ã€‚
âš ï¸ è¯·æ³¨æ„ï¼Œæ­¤æ–‡ä»¶æ˜¯ Markdown æ ¼å¼ï¼Œä½†åŒ…å«äº†æˆ‘ä»¬çš„æ–‡æ¡£ç”Ÿæˆå™¨ï¼ˆç±»ä¼¼äº MDXï¼‰çš„ç‰¹å®šè¯­æ³•ï¼Œå¯èƒ½æ— æ³•åœ¨æ‚¨çš„ Markdown æŸ¥çœ‹å™¨ä¸­æ­£ç¡®æ˜¾ç¤ºã€‚
-->

# åˆ›å»ºè‡ªå®šä¹‰æ¶æ„

[`AutoClass`](model_doc/auto) ä¼šè‡ªåŠ¨æ¨æ–­æ¨¡å‹æ¶æ„å¹¶ä¸‹è½½é¢„è®­ç»ƒçš„é…ç½®å’Œæƒé‡ã€‚é€šå¸¸ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ `AutoClass` æ¥ç”Ÿæˆä¸æ£€æŸ¥ç‚¹æ— å…³çš„ä»£ç ã€‚ä½†æ˜¯ï¼Œå¸Œæœ›å¯¹ç‰¹å®šæ¨¡å‹å‚æ•°æœ‰æ›´å¤šæ§åˆ¶æƒçš„ç”¨æˆ·å¯ä»¥ä»å‡ ä¸ªåŸºç±»åˆ›å»ºè‡ªå®šä¹‰ğŸ¤— Transformers æ¨¡å‹ã€‚è¿™å¯¹äºå¯¹ğŸ¤— Transformers æ¨¡å‹è¿›è¡Œç ”ç©¶ã€è®­ç»ƒæˆ–å®éªŒæ„Ÿå…´è¶£çš„ä»»ä½•äººéƒ½éå¸¸æœ‰ç”¨ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæ·±å…¥äº†è§£å¦‚ä½•åˆ›å»ºæ²¡æœ‰ `AutoClass` çš„è‡ªå®šä¹‰æ¨¡å‹ã€‚å­¦ä¹ å¦‚ä½•ï¼š
- åŠ è½½å’Œè‡ªå®šä¹‰æ¨¡å‹é…ç½®ã€‚- åˆ›å»ºæ¨¡å‹æ¶æ„ã€‚- ä¸ºæ–‡æœ¬åˆ›å»ºæ…¢é€Ÿå’Œå¿«é€Ÿåˆ†è¯å™¨ (Tokenizer)ã€‚- ä¸ºè§†è§‰ä»»åŠ¡åˆ›å»ºå›¾åƒå¤„ç†å™¨ (Image Processor)ã€‚- ä¸ºéŸ³é¢‘ä»»åŠ¡åˆ›å»ºç‰¹å¾æå–å™¨ã€‚- ä¸ºå¤šæ¨¡æ€ä»»åŠ¡åˆ›å»ºå¤„ç†å™¨ã€‚

## é…ç½® Configuration

[é…ç½®](main_classes/configuration) æ˜¯æŒ‡æ¨¡å‹çš„ç‰¹å®šå±æ€§ã€‚æ¯ä¸ªæ¨¡å‹é…ç½®éƒ½å…·æœ‰ä¸åŒçš„å±æ€§ï¼›ä¾‹å¦‚ï¼Œæ‰€æœ‰ NLP æ¨¡å‹éƒ½å…·æœ‰ `hidden_size`ã€`num_attention_heads`ã€`num_hidden_layers` å’Œ `vocab_size` å±æ€§ã€‚è¿™äº›å±æ€§ç”¨äºæŒ‡å®šæ„å»ºæ¨¡å‹æ‰€éœ€çš„æ³¨æ„åŠ›å¤´æˆ–éšè—å±‚çš„æ•°é‡ã€‚

é€šè¿‡è®¿é—® [`DistilBertConfig`] æ¥è¯¦ç»†äº†è§£ [DistilBERT](model_doc/distilbert) çš„å±æ€§ï¼š
```py
>>> from transformers import DistilBertConfig

>>> config = DistilBertConfig()
>>> print(config)
DistilBertConfig {
  "activation": "gelu",
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "transformers_version": "4.16.2",
  "vocab_size": 30522
}
```

[`DistilBertConfig`] æ˜¾ç¤ºäº†æ„å»ºåŸºæœ¬ [`DistilBertModel`] æ‰€ä½¿ç”¨çš„æ‰€æœ‰é»˜è®¤å±æ€§ã€‚æ‰€æœ‰å±æ€§éƒ½æ˜¯å¯è‡ªå®šä¹‰çš„ï¼Œä¸ºå®éªŒæä¾›äº†ç©ºé—´ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ `activation` å‚æ•°è‡ªå®šä¹‰é»˜è®¤æ¨¡å‹çš„æ¿€æ´»å‡½æ•°ã€‚

- ä½¿ç”¨ `attention_dropout` å‚æ•°ï¼Œåœ¨æ³¨æ„åŠ›æ¦‚ç‡ä¸­ä½¿ç”¨æ›´é«˜çš„ dropout æ¯”ç‡ã€‚

- ä½¿ç”¨ [`~PretrainedConfig.from_pretrained`] å‡½æ•°å¯ä¿®æ”¹é¢„è®­ç»ƒæ¨¡å‹å±æ€§ï¼š

```py
>>> my_config = DistilBertConfig(activation="relu", attention_dropout=0.4)
>>> print(my_config)
DistilBertConfig {
  "activation": "relu",
  "attention_dropout": 0.4,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "transformers_version": "4.16.2",
  "vocab_size": 30522
}
```

é¢„è®­ç»ƒæ¨¡å‹å±æ€§å¯åœ¨ [`~PretrainedConfig.from_pretrained`] å‡½æ•°ä¸­è¿›è¡Œä¿®æ”¹ï¼š
```py
>>> my_config = DistilBertConfig.from_pretrained("distilbert-base-uncased", activation="relu", attention_dropout=0.4)
```

å®Œæˆæ¨¡å‹é…ç½®åï¼Œå¯ä»¥ä½¿ç”¨ [`~PretrainedConfig.save_pretrained`] ä¿å­˜é…ç½®ã€‚é…ç½®æ–‡ä»¶å°†ä»¥ JSON æ–‡ä»¶çš„å½¢å¼å­˜å‚¨åœ¨æŒ‡å®šçš„ä¿å­˜ç›®å½•ä¸­ï¼š
```py
>>> my_config.save_pretrained(save_directory="./your_model_save_path")
```

è¦é‡ç”¨é…ç½®æ–‡ä»¶ï¼Œè¯·ä½¿ç”¨ [`~PretrainedConfig.from_pretrained`] åŠ è½½å®ƒï¼š
```py
>>> my_config = DistilBertConfig.from_pretrained("./your_model_save_path/config.json")
```

<Tip>

æ‚¨è¿˜å¯ä»¥å°†é…ç½®æ–‡ä»¶ä¿å­˜ä¸ºå­—å…¸ï¼Œç”šè‡³åªä¿å­˜è‡ªå®šä¹‰é…ç½®å±æ€§ä¸é»˜è®¤é…ç½®å±æ€§ä¹‹é—´çš„å·®å¼‚ï¼æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [é…ç½®](main_classes/configuration) æ–‡æ¡£ã€‚
</Tip>

## æ¨¡å‹

ä¸‹ä¸€æ­¥æ˜¯åˆ›å»ºä¸€ä¸ª [æ¨¡å‹](main_classes/models)ã€‚æ¨¡å‹ï¼ˆä¹Ÿå¯ä»¥ç§°ä¸ºæ¶æ„ï¼‰å®šä¹‰æ¯ä¸ªå±‚æ‰€åšçš„å·¥ä½œå’Œè¿›è¡Œçš„æ“ä½œã€‚é…ç½®ä¸­çš„å±æ€§ï¼ˆä¾‹å¦‚ `num_hidden_layers`ï¼‰ç”¨äºå®šä¹‰æ¶æ„ã€‚æ¯ä¸ªæ¨¡å‹éƒ½å…±äº« [`PreTrainedModel`] åŸºç±»å’Œä¸€äº›å¸¸è§æ–¹æ³•ï¼Œä¾‹å¦‚è°ƒæ•´è¾“å…¥åµŒå…¥å’Œä¿®å‰ªè‡ªæ³¨æ„åŠ›å¤´ã€‚æ­¤å¤–ï¼Œæ‰€æœ‰æ¨¡å‹è¿˜éƒ½æ˜¯ [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)ã€[`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) æˆ– [`flax.linen.Module`](https://flax.readthedocs.io/en/latest/flax.linen.html#module) çš„å­ç±»ã€‚
è¿™æ„å‘³ç€æ¨¡å‹ä¸å„è‡ªæ¡†æ¶çš„ä½¿ç”¨æ–¹å¼å…¼å®¹ã€‚

<frameworkcontent> 
<pt>

 å°†è‡ªå®šä¹‰é…ç½®å±æ€§åŠ è½½åˆ°æ¨¡å‹ä¸­ï¼š
```py
>>> from transformers import DistilBertModel

>>> my_config = DistilBertConfig.from_pretrained("./your_model_save_path/config.json")
>>> model = DistilBertModel(my_config)
```

è¿™å°†åˆ›å»ºä¸€ä¸ªä½¿ç”¨éšæœºå€¼è€Œä¸æ˜¯é¢„è®­ç»ƒæƒé‡çš„æ¨¡å‹ã€‚åœ¨è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œæ‚¨æ— æ³•å°†å…¶ç”¨äºä»»ä½•æœ‰ç”¨çš„ç”¨é€”ã€‚

è®­ç»ƒæ˜¯ä¸€ç§æ˜‚è´µä¸”è€—æ—¶çš„è¿‡ç¨‹ã€‚

é€šå¸¸æƒ…å†µä¸‹ï¼Œæœ€å¥½ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ä»¥æ›´å°‘çš„èµ„æºè·å–æ›´å¥½çš„ç»“æœã€‚
ä½¿ç”¨ [`~PreTrainedModel.from_pretrained`] åˆ›å»ºé¢„è®­ç»ƒæ¨¡å‹ï¼š

```py
>>> model = DistilBertModel.from_pretrained("distilbert-base-uncased")
```

å½“åŠ è½½é¢„è®­ç»ƒæƒé‡æ—¶ï¼Œå¦‚æœæ¨¡å‹ç”±ğŸ¤— Transformers æä¾›ï¼Œå°†è‡ªåŠ¨åŠ è½½é»˜è®¤æ¨¡å‹é…ç½®ã€‚ä½†æ˜¯ï¼Œå¦‚æœéœ€è¦ï¼Œä»ç„¶å¯ä»¥æ›¿æ¢é»˜è®¤æ¨¡å‹é…ç½®å±æ€§çš„ä¸€éƒ¨åˆ†æˆ–å…¨éƒ¨ï¼š
```py
>>> model = DistilBertModel.from_pretrained("distilbert-base-uncased", config=my_config)
```
</pt> 
<tf> 

å°†è‡ªå®šä¹‰é…ç½®å±æ€§åŠ è½½åˆ°æ¨¡å‹ä¸­ï¼š

```py
>>> from transformers import TFDistilBertModel

>>> my_config = DistilBertConfig.from_pretrained("./your_model_save_path/my_config.json")
>>> tf_model = TFDistilBertModel(my_config)
```

è¿™å°†åˆ›å»ºä¸€ä¸ªä½¿ç”¨éšæœºå€¼è€Œä¸æ˜¯é¢„è®­ç»ƒæƒé‡çš„æ¨¡å‹ã€‚åœ¨è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œæ‚¨æ— æ³•å°†å…¶ç”¨äºä»»ä½•æœ‰ç”¨çš„ç”¨é€”ã€‚è®­ç»ƒæ˜¯ä¸€ç§æ˜‚è´µä¸”è€—æ—¶çš„è¿‡ç¨‹ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œæœ€å¥½ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ä»¥æ›´å°‘çš„èµ„æºè·å–æ›´å¥½çš„ç»“æœã€‚

ä½¿ç”¨ [`~TFPreTrainedModel.from_pretrained`] åˆ›å»ºé¢„è®­ç»ƒæ¨¡å‹ï¼š

```py
>>> tf_model = TFDistilBertModel.from_pretrained("distilbert-base-uncased")
```

å½“åŠ è½½é¢„è®­ç»ƒæƒé‡æ—¶ï¼Œå¦‚æœæ¨¡å‹ç”±ğŸ¤— Transformers æä¾›ï¼Œå°†è‡ªåŠ¨åŠ è½½é»˜è®¤æ¨¡å‹é…ç½®ã€‚ä½†æ˜¯ï¼Œå¦‚æœéœ€è¦ï¼Œä»ç„¶å¯ä»¥æ›¿æ¢é»˜è®¤æ¨¡å‹é…ç½®å±æ€§çš„ä¸€éƒ¨åˆ†æˆ–å…¨éƒ¨ï¼š
```py
>>> tf_model = TFDistilBertModel.from_pretrained("distilbert-base-uncased", config=my_config)
```
</tf> 
</frameworkcontent>

### Model heads æ¨¡å‹å¤´

æ­¤æ—¶ï¼Œæ‚¨æ‹¥æœ‰ä¸€ä¸ªåŸºæœ¬çš„ DistilBERT æ¨¡å‹ï¼Œå®ƒè¾“å‡º *éšè—çŠ¶æ€*ã€‚éšè—çŠ¶æ€å°†ä½œä¸ºè¾“å…¥ä¼ é€’ç»™æ¨¡å‹å¤´ä»¥ç”Ÿæˆæœ€ç»ˆè¾“å‡ºã€‚åªè¦æ¨¡å‹æ”¯æŒä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼Œæ‚¨ä¸èƒ½å°† DistilBERT ç”¨äºç¿»è¯‘ç­‰åºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡ï¼‰ï¼ŒğŸ¤— Transformers ä¸ºæ¯ä¸ªä»»åŠ¡æä¾›ä¸åŒçš„æ¨¡å‹å¤´ã€‚

<frameworkcontent> 
<pt> 

ä¾‹å¦‚ï¼Œ[`DistilBertForSequenceClassification`] æ˜¯ä¸€ä¸ªå¸¦æœ‰åºåˆ—åˆ†ç±»å¤´çš„åŸºæœ¬ DistilBERT æ¨¡å‹ã€‚åºåˆ—åˆ†ç±»å¤´æ˜¯åœ¨æ±‡èšçš„è¾“å‡ºä¹‹ä¸Šçš„çº¿æ€§å±‚ã€‚

```py
>>> from transformers import DistilBertForSequenceClassification

>>> model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")
```

é€šè¿‡åˆ‡æ¢åˆ°ä¸åŒçš„æ¨¡å‹å¤´ï¼Œå¯ä»¥è½»æ¾å°†æ­¤æ£€æŸ¥ç‚¹é‡ç”¨äºå…¶ä»–ä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œå¯¹äºé—®ç­”ä»»åŠ¡ï¼Œæ‚¨å°†ä½¿ç”¨ [`DistilBertForQuestionAnswering`] æ¨¡å‹å¤´ã€‚é—®ç­”å¤´ä¸åºåˆ—åˆ†ç±»å¤´ç±»ä¼¼ï¼Œåªæ˜¯åœ¨éšè—çŠ¶æ€è¾“å‡ºä¹‹ä¸Šçš„çº¿æ€§å±‚ã€‚
```py
>>> from transformers import DistilBertForQuestionAnswering

>>> model = DistilBertForQuestionAnswering.from_pretrained("distilbert-base-uncased")
```
</pt> 
<tf> 

ä¾‹å¦‚ï¼Œ[`TFDistilBertForSequenceClassification`] æ˜¯ä¸€ä¸ªå¸¦æœ‰åºåˆ—åˆ†ç±»å¤´çš„åŸºæœ¬ DistilBERT æ¨¡å‹ã€‚åºåˆ—åˆ†ç±»å¤´æ˜¯åœ¨æ±‡èšçš„è¾“å‡ºä¹‹ä¸Šçš„çº¿æ€§å±‚ã€‚

```py
>>> from transformers import TFDistilBertForSequenceClassification

>>> tf_model = TFDistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")
```

é€šè¿‡åˆ‡æ¢åˆ°ä¸åŒçš„æ¨¡å‹å¤´ï¼Œå¯ä»¥è½»æ¾å°†æ­¤æ£€æŸ¥ç‚¹é‡ç”¨äºå…¶ä»–ä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œå¯¹äºé—®ç­”ä»»åŠ¡ï¼Œæ‚¨å°†ä½¿ç”¨ [`TFDistilBertForQuestionAnswering`] æ¨¡å‹å¤´ã€‚

é—®ç­”å¤´ä¸åºåˆ—åˆ†ç±»å¤´ç±»ä¼¼ï¼Œåªæ˜¯åœ¨éšè—çŠ¶æ€è¾“å‡ºä¹‹ä¸Šçš„çº¿æ€§å±‚ã€‚

```py
>>> from transformers import TFDistilBertForQuestionAnswering

>>> tf_model = TFDistilBertForQuestionAnswering.from_pretrained("distilbert-base-uncased")
```
</tf>
</frameworkcontent>

## åˆ†è¯å™¨ (Tokenizer)

åœ¨å°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸ºå¼ é‡ä¹‹å‰ï¼Œæ‚¨éœ€è¦ä½¿ç”¨ [åˆ†è¯å™¨ (Tokenizer)](main_classes/tokenizer) ä½œä¸ºæ–‡æœ¬æ•°æ®çš„æœ€åä¸€ä¸ªåŸºç±»ã€‚ğŸ¤— Transformers æä¾›äº†ä¸¤ç§ç±»å‹çš„åˆ†è¯å™¨ (Tokenizer)ï¼š

- [`PreTrainedTokenizer`]: ä¸€ä¸ªåŸºäºPythonçš„åˆ†è¯å™¨å®ç°ã€‚
- [`PreTrainedTokenizerFast`]: ä¸€ä¸ªæ¥è‡ªæˆ‘ä»¬åŸºäºRustçš„[ğŸ¤— Tokenizer](https://huggingface.co/docs/tokenizers/python/latest/)åº“çš„åˆ†è¯å™¨ã€‚ç”±äºå…¶é‡‡ç”¨äº†Rustå®ç°ï¼Œè¿™ç§åˆ†è¯å™¨ç±»å‹åœ¨æ‰¹é‡åˆ†è¯è¿‡ç¨‹ä¸­å…·æœ‰æ˜¾è‘—çš„é€Ÿåº¦ä¼˜åŠ¿ã€‚å¿«é€Ÿåˆ†è¯å™¨è¿˜æä¾›äº†å…¶ä»–æ–¹æ³•ï¼Œå¦‚*åç§»æ˜ å°„*ï¼Œå¯å°†æ ‡è®°æ˜ å°„åˆ°å…¶åŸå§‹å•è¯æˆ–å­—ç¬¦ã€‚

è¿™ä¸¤ç§åˆ†è¯å™¨éƒ½æ”¯æŒå¸¸è§çš„æ–¹æ³•ï¼Œå¦‚ç¼–ç å’Œè§£ç ã€æ·»åŠ æ–°æ ‡è®°ä»¥åŠç®¡ç†ç‰¹æ®Šæ ‡è®°ã€‚

<Tip warning={true}>

å¹¶éæ‰€æœ‰æ¨¡å‹éƒ½æ”¯æŒå¿«é€Ÿåˆ†è¯å™¨ã€‚æ‚¨å¯ä»¥æŸ¥çœ‹è¿™ä¸ª [è¡¨æ ¼](index#supported-frameworks) æ¥æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒå¿«é€Ÿåˆ†è¯å™¨ã€‚

</Tip>

å¦‚æœæ‚¨è®­ç»ƒäº†è‡ªå·±çš„åˆ†è¯å™¨ï¼Œå¯ä»¥ä½¿ç”¨æ‚¨çš„ *è¯æ±‡è¡¨* æ–‡ä»¶åˆ›å»ºä¸€ä¸ªåˆ†è¯å™¨ï¼š

```py
>>> from transformers import DistilBertTokenizer

>>> my_tokenizer = DistilBertTokenizer(vocab_file="my_vocab_file.txt", do_lower_case=False, padding_side="left")
```

It is important to remember the vocabulary from a custom tokenizer will be different from the vocabulary generated by a pretrained model's tokenizer. You need to use a pretrained model's vocabulary if you are using a pretrained model, otherwise the inputs won't make sense. Create a tokenizer with a pretrained model's vocabulary with the [`DistilBertTokenizer`] class:

```py
>>> from transformers import DistilBertTokenizer

>>> slow_tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
```

Create a fast tokenizer with the [`DistilBertTokenizerFast`] class:

```py
>>> from transformers import DistilBertTokenizerFast

>>> fast_tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert-base-uncased")
```

<Tip>

By default, [`AutoTokenizer`] will try to load a fast tokenizer. You can disable this behavior by setting `use_fast=False` in `from_pretrained`.

</Tip>

## å›¾åƒå¤„ç†å™¨ (Image Processor) 

å›¾åƒå¤„ç†å™¨ (Image Processor)ç”¨äºå¤„ç†è§†è§‰è¾“å…¥ã€‚å®ƒç»§æ‰¿è‡ªåŸºç±» [`~image_processing_utils.ImageProcessingMixin`]ã€‚

è¦ä½¿ç”¨å›¾åƒå¤„ç†å™¨ (Image Processor)ï¼Œå¯ä»¥åˆ›å»ºä¸€ä¸ªä¸æ‚¨æ­£åœ¨ä½¿ç”¨çš„æ¨¡å‹å…³è”çš„å›¾åƒå¤„ç†å™¨ (Image Processor)ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨ [ViT](model_doc/vit) è¿›è¡Œå›¾åƒåˆ†ç±»ï¼Œå¯ä»¥åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„ [`ViTImageProcessor`]ï¼š

```py
>>> from transformers import ViTImageProcessor

>>> vit_extractor = ViTImageProcessor()
>>> print(vit_extractor)
ViTImageProcessor {
  "do_normalize": true,
  "do_resize": true,
  "feature_extractor_type": "ViTImageProcessor",
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "resample": 2,
  "size": 224
}
```

<Tip>

å¦‚æœæ‚¨ä¸éœ€è¦è¿›è¡Œä»»ä½•è‡ªå®šä¹‰ï¼Œåªéœ€ä½¿ç”¨ `from_pretrained` æ–¹æ³•åŠ è½½æ¨¡å‹çš„é»˜è®¤å›¾åƒå¤„ç†å™¨å‚æ•°å³å¯ã€‚

</Tip>

ä¿®æ”¹ä»»ä½• [`ViTImageProcessor`] å‚æ•°ä»¥åˆ›å»ºæ‚¨çš„è‡ªå®šä¹‰å›¾åƒå¤„ç†å™¨ï¼š

```py
>>> from transformers import ViTImageProcessor

>>> my_vit_extractor = ViTImageProcessor(resample="PIL.Image.BOX", do_normalize=False, image_mean=[0.3, 0.3, 0.3])
>>> print(my_vit_extractor)
ViTImageProcessor {
  "do_normalize": false,
  "do_resize": true,
  "feature_extractor_type": "ViTImageProcessor",
  "image_mean": [
    0.3,
    0.3,
    0.3
  ],
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "resample": "PIL.Image.BOX",
  "size": 224
}
```

## ç‰¹å¾æå–å™¨ ï¼ˆFeature Extractorï¼‰

ç‰¹å¾æå–å™¨ç”¨äºå¤„ç†éŸ³é¢‘è¾“å…¥ã€‚å®ƒç»§æ‰¿è‡ªåŸºç±» [`~feature_extraction_utils.FeatureExtractionMixin`]ï¼Œå¹¶ä¸”å¯èƒ½è¿˜ç»§æ‰¿è‡ª [`SequenceFeatureExtractor`] ç±»æ¥å¤„ç†éŸ³é¢‘è¾“å…¥ã€‚

è¦ä½¿ç”¨ç‰¹å¾æå–å™¨ï¼Œå¯ä»¥åˆ›å»ºä¸€ä¸ªä¸æ‚¨æ­£åœ¨ä½¿ç”¨çš„æ¨¡å‹å…³è”çš„ç‰¹å¾æå–å™¨ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨ [Wav2Vec2](model_doc/wav2vec2) è¿›è¡ŒéŸ³é¢‘åˆ†ç±»ï¼Œå¯ä»¥åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„ [`Wav2Vec2FeatureExtractor`]ï¼š

```py
>>> from transformers import Wav2Vec2FeatureExtractor

>>> w2v2_extractor = Wav2Vec2FeatureExtractor()
>>> print(w2v2_extractor)
Wav2Vec2FeatureExtractor {
  "do_normalize": true,
  "feature_extractor_type": "Wav2Vec2FeatureExtractor",
  "feature_size": 1,
  "padding_side": "right",
  "padding_value": 0.0,
  "return_attention_mask": false,
  "sampling_rate": 16000
}
```
<Tip>

å¦‚æœæ‚¨ä¸éœ€è¦è¿›è¡Œä»»ä½•è‡ªå®šä¹‰ï¼Œåªéœ€ä½¿ç”¨ `from_pretrained` æ–¹æ³•åŠ è½½æ¨¡å‹çš„é»˜è®¤ç‰¹å¾æå–å™¨å‚æ•°å³å¯ã€‚

</Tip>

ä¿®æ”¹ä»»ä½• [`Wav2Vec2FeatureExtractor`] çš„å‚æ•°ä»¥è¿›è¡Œè‡ªå®šä¹‰è®¾ç½®ï¼š

```py
>>> from transformers import Wav2Vec2FeatureExtractor

>>> w2v2_extractor = Wav2Vec2FeatureExtractor(sampling_rate=8000, do_normalize=False)
>>> print(w2v2_extractor)
Wav2Vec2FeatureExtractor {
  "do_normalize": false,
  "feature_extractor_type": "Wav2Vec2FeatureExtractor",
  "feature_size": 1,
  "padding_side": "right",
  "padding_value": 0.0,
  "return_attention_mask": false,
  "sampling_rate": 8000
}
```


##  å¤„ç†å™¨ï¼ˆProcessorï¼‰

å¯¹äºæ”¯æŒå¤šæ¨¡æ€ä»»åŠ¡çš„æ¨¡å‹ï¼ŒğŸ¤— Transformers æä¾›äº†ä¸€ä¸ªå¤„ç†å™¨ç±»ï¼Œå®ƒå¯ä»¥æ–¹ä¾¿åœ°å°†ç‰¹å¾æå–å™¨å’Œåˆ†è¯å™¨ç­‰å¤„ç†ç±»å°è£…æˆä¸€ä¸ªå•ä¸€å¯¹è±¡ã€‚ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨ [`Wav2Vec2Processor`] æ¥å¤„ç†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä»»åŠ¡ï¼ˆASRï¼‰ã€‚

ASR å°†éŸ³é¢‘è½¬å½•ä¸ºæ–‡æœ¬ï¼Œå› æ­¤æ‚¨å°†éœ€è¦ä¸€ä¸ªç‰¹å¾æå–å™¨å’Œä¸€ä¸ªåˆ†è¯å™¨ã€‚

åˆ›å»ºä¸€ä¸ªç‰¹å¾æå–å™¨æ¥å¤„ç†éŸ³é¢‘è¾“å…¥ï¼š

```py
>>> from transformers import Wav2Vec2FeatureExtractor

>>> feature_extractor = Wav2Vec2FeatureExtractor(padding_value=1.0, do_normalize=True)
```

Create a tokenizer to handle the text inputs:

```py
>>> from transformers import Wav2Vec2CTCTokenizer

>>> tokenizer = Wav2Vec2CTCTokenizer(vocab_file="my_vocab_file.txt")
```

Combine the feature extractor and tokenizer in [`Wav2Vec2Processor`]:

```py
>>> from transformers import Wav2Vec2Processor

>>> processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)
```

é€šè¿‡é…ç½®ç±»ã€æ¨¡å‹ç±»å’Œé¢å¤–çš„é¢„å¤„ç†ç±»ï¼ˆåˆ†è¯å™¨ã€å›¾åƒå¤„ç†å™¨ã€ç‰¹å¾æå–å™¨æˆ–å¤„ç†å™¨ï¼‰ï¼Œæ‚¨å¯ä»¥åˆ›å»ºğŸ¤— Transformers æ”¯æŒçš„ä»»ä½•æ¨¡å‹ã€‚æ¯ä¸ªåŸºç±»éƒ½æ˜¯å¯é…ç½®çš„ï¼Œå…è®¸æ‚¨ä½¿ç”¨æ‰€éœ€çš„ç‰¹å®šå±æ€§ã€‚æ‚¨å¯ä»¥è½»æ¾åœ°è®¾ç½®ä¸€ä¸ªç”¨äºè®­ç»ƒçš„æ¨¡å‹ï¼Œæˆ–ä¿®æ”¹ä¸€ä¸ªç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚
