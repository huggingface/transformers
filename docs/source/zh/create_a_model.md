<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# åˆ›å»ºè‡ªå®šä¹‰æ¶æ„

[`AutoClass`](model_doc/auto) è‡ªåŠ¨æ¨æ–­æ¨¡å‹æ¶æ„å¹¶ä¸‹è½½é¢„è®­ç»ƒçš„é…ç½®å’Œæƒé‡ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ `AutoClass` ç”Ÿæˆä¸æ£€æŸ¥ç‚¹ï¼ˆcheckpointï¼‰æ— å…³çš„ä»£ç ã€‚å¸Œæœ›å¯¹ç‰¹å®šæ¨¡å‹å‚æ•°æœ‰æ›´å¤šæ§åˆ¶çš„ç”¨æˆ·ï¼Œå¯ä»¥ä»…ä»å‡ ä¸ªåŸºç±»åˆ›å»ºè‡ªå®šä¹‰çš„ ğŸ¤— Transformers æ¨¡å‹ã€‚è¿™å¯¹äºä»»ä½•æœ‰å…´è¶£å­¦ä¹ ã€è®­ç»ƒæˆ–è¯•éªŒ ğŸ¤— Transformers æ¨¡å‹çš„äººå¯èƒ½ç‰¹åˆ«æœ‰ç”¨ã€‚é€šè¿‡æœ¬æŒ‡å—ï¼Œæ·±å…¥äº†è§£å¦‚ä½•ä¸é€šè¿‡ `AutoClass` åˆ›å»ºè‡ªå®šä¹‰æ¨¡å‹ã€‚äº†è§£å¦‚ä½•ï¼š

- åŠ è½½å¹¶è‡ªå®šä¹‰æ¨¡å‹é…ç½®ã€‚
- åˆ›å»ºæ¨¡å‹æ¶æ„ã€‚
- ä¸ºæ–‡æœ¬åˆ›å»ºæ…¢é€Ÿå’Œå¿«é€Ÿåˆ†è¯å™¨ã€‚
- ä¸ºè§†è§‰ä»»åŠ¡åˆ›å»ºå›¾åƒå¤„ç†å™¨ã€‚
- ä¸ºéŸ³é¢‘ä»»åŠ¡åˆ›å»ºç‰¹å¾æå–å™¨ã€‚
- ä¸ºå¤šæ¨¡æ€ä»»åŠ¡åˆ›å»ºå¤„ç†å™¨ã€‚

## é…ç½®

[é…ç½®](main_classes/configuration) æ¶‰åŠåˆ°æ¨¡å‹çš„å…·ä½“å±æ€§ã€‚æ¯ä¸ªæ¨¡å‹é…ç½®éƒ½æœ‰ä¸åŒçš„å±æ€§ï¼›ä¾‹å¦‚ï¼Œæ‰€æœ‰ NLP æ¨¡å‹éƒ½å…±äº« `hidden_size`ã€`num_attention_heads`ã€ `num_hidden_layers` å’Œ `vocab_size` å±æ€§ã€‚è¿™äº›å±æ€§ç”¨äºæŒ‡å®šæ„å»ºæ¨¡å‹æ—¶çš„æ³¨æ„åŠ›å¤´æ•°é‡æˆ–éšè—å±‚å±‚æ•°ã€‚

è®¿é—® [`DistilBertConfig`] ä»¥æ›´è¿‘ä¸€æ­¥äº†è§£ [DistilBERT](model_doc/distilbert)ï¼Œæ£€æŸ¥å®ƒçš„å±æ€§ï¼š

```py
>>> from transformers import DistilBertConfig

>>> config = DistilBertConfig()
>>> print(config)
DistilBertConfig {
  "activation": "gelu",
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "transformers_version": "4.16.2",
  "vocab_size": 30522
}
```

[`DistilBertConfig`] æ˜¾ç¤ºäº†æ„å»ºåŸºç¡€ [`DistilBertModel`] æ‰€ä½¿ç”¨çš„æ‰€æœ‰é»˜è®¤å±æ€§ã€‚æ‰€æœ‰å±æ€§éƒ½å¯ä»¥è¿›è¡Œè‡ªå®šä¹‰ï¼Œä¸ºå®éªŒåˆ›é€ äº†ç©ºé—´ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥å°†é»˜è®¤æ¨¡å‹è‡ªå®šä¹‰ä¸ºï¼š

- ä½¿ç”¨ `activation` å‚æ•°å°è¯•ä¸åŒçš„æ¿€æ´»å‡½æ•°ã€‚
- ä½¿ç”¨ `attention_dropout` å‚æ•°ä¸º attention probabilities ä½¿ç”¨æ›´é«˜çš„ dropout ratioã€‚

```py
>>> my_config = DistilBertConfig(activation="relu", attention_dropout=0.4)
>>> print(my_config)
DistilBertConfig {
  "activation": "relu",
  "attention_dropout": 0.4,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "transformers_version": "4.16.2",
  "vocab_size": 30522
}
```

é¢„è®­ç»ƒæ¨¡å‹çš„å±æ€§å¯ä»¥åœ¨ [`~PretrainedConfig.from_pretrained`] å‡½æ•°ä¸­è¿›è¡Œä¿®æ”¹ï¼š

```py
>>> my_config = DistilBertConfig.from_pretrained("distilbert-base-uncased", activation="relu", attention_dropout=0.4)
```

å½“ä½ å¯¹æ¨¡å‹é…ç½®æ»¡æ„æ—¶ï¼Œå¯ä»¥ä½¿ç”¨ [`~PretrainedConfig.save_pretrained`] æ¥ä¿å­˜é…ç½®ã€‚ä½ çš„é…ç½®æ–‡ä»¶å°†ä»¥ JSON æ–‡ä»¶çš„å½¢å¼å­˜å‚¨åœ¨æŒ‡å®šçš„ä¿å­˜ç›®å½•ä¸­ï¼š

```py
>>> my_config.save_pretrained(save_directory="./your_model_save_path")
```

è¦é‡ç”¨é…ç½®æ–‡ä»¶ï¼Œè¯·ä½¿ç”¨ [`~PretrainedConfig.from_pretrained`] è¿›è¡ŒåŠ è½½ï¼š

```py
>>> my_config = DistilBertConfig.from_pretrained("./your_model_save_path/config.json")
```

<Tip>

ä½ è¿˜å¯ä»¥å°†é…ç½®æ–‡ä»¶ä¿å­˜ä¸ºå­—å…¸ï¼Œç”šè‡³åªä¿å­˜è‡ªå®šä¹‰é…ç½®å±æ€§ä¸é»˜è®¤é…ç½®å±æ€§ä¹‹é—´çš„å·®å¼‚ï¼æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [é…ç½®](main_classes/configuration) æ–‡æ¡£ã€‚

</Tip>

## æ¨¡å‹

æ¥ä¸‹æ¥ï¼Œåˆ›å»ºä¸€ä¸ª[æ¨¡å‹](main_classes/models)ã€‚æ¨¡å‹ï¼Œä¹Ÿå¯æ³›æŒ‡æ¶æ„ï¼Œå®šä¹‰äº†æ¯ä¸€å±‚ç½‘ç»œçš„è¡Œä¸ºä»¥åŠè¿›è¡Œçš„æ“ä½œã€‚é…ç½®ä¸­çš„ `num_hidden_layers` ç­‰å±æ€§ç”¨äºå®šä¹‰æ¶æ„ã€‚æ¯ä¸ªæ¨¡å‹éƒ½å…±äº«åŸºç±» [`PreTrainedModel`] å’Œä¸€äº›å¸¸ç”¨æ–¹æ³•ï¼Œä¾‹å¦‚è°ƒæ•´è¾“å…¥åµŒå…¥çš„å¤§å°å’Œä¿®å‰ªè‡ªæ³¨æ„åŠ›å¤´ã€‚æ­¤å¤–ï¼Œæ‰€æœ‰æ¨¡å‹éƒ½æ˜¯ [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)ã€[`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) æˆ– [`flax.linen.Module`](https://flax.readthedocs.io/en/latest/flax.linen.html#module) çš„å­ç±»ã€‚è¿™æ„å‘³ç€æ¨¡å‹ä¸å„è‡ªæ¡†æ¶çš„ç”¨æ³•å…¼å®¹ã€‚

<frameworkcontent>
<pt>
å°†è‡ªå®šä¹‰é…ç½®å±æ€§åŠ è½½åˆ°æ¨¡å‹ä¸­ï¼š

```py
>>> from transformers import DistilBertModel

>>> my_config = DistilBertConfig.from_pretrained("./your_model_save_path/config.json")
>>> model = DistilBertModel(my_config)
```

è¿™æ®µä»£ç åˆ›å»ºäº†ä¸€ä¸ªå…·æœ‰éšæœºå‚æ•°è€Œä¸æ˜¯é¢„è®­ç»ƒæƒé‡çš„æ¨¡å‹ã€‚åœ¨è®­ç»ƒè¯¥æ¨¡å‹ä¹‹å‰ï¼Œæ‚¨è¿˜æ— æ³•å°†è¯¥æ¨¡å‹ç”¨äºä»»ä½•ç”¨é€”ã€‚è®­ç»ƒæ˜¯ä¸€é¡¹æ˜‚è´µä¸”è€—æ—¶çš„è¿‡ç¨‹ã€‚é€šå¸¸æ¥è¯´ï¼Œæœ€å¥½ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æ¥æ›´å¿«åœ°è·å¾—æ›´å¥½çš„ç»“æœï¼ŒåŒæ—¶ä»…ä½¿ç”¨è®­ç»ƒæ‰€éœ€èµ„æºçš„ä¸€å°éƒ¨åˆ†ã€‚

ä½¿ç”¨ [`~PreTrainedModel.from_pretrained`] åˆ›å»ºé¢„è®­ç»ƒæ¨¡å‹ï¼š

```py
>>> model = DistilBertModel.from_pretrained("distilbert-base-uncased")
```

å½“åŠ è½½é¢„è®­ç»ƒæƒé‡æ—¶ï¼Œå¦‚æœæ¨¡å‹æ˜¯ç”± ğŸ¤— Transformers æä¾›çš„ï¼Œå°†è‡ªåŠ¨åŠ è½½é»˜è®¤æ¨¡å‹é…ç½®ã€‚ç„¶è€Œï¼Œå¦‚æœä½ æ„¿æ„ï¼Œä»ç„¶å¯ä»¥å°†é»˜è®¤æ¨¡å‹é…ç½®çš„æŸäº›æˆ–è€…æ‰€æœ‰å±æ€§æ›¿æ¢æˆä½ è‡ªå·±çš„é…ç½®ï¼š

```py
>>> model = DistilBertModel.from_pretrained("distilbert-base-uncased", config=my_config)
```
</pt>
<tf>
å°†è‡ªå®šä¹‰é…ç½®å±æ€§åŠ è½½åˆ°æ¨¡å‹ä¸­ï¼š

```py
>>> from transformers import TFDistilBertModel

>>> my_config = DistilBertConfig.from_pretrained("./your_model_save_path/my_config.json")
>>> tf_model = TFDistilBertModel(my_config)
```

è¿™æ®µä»£ç åˆ›å»ºäº†ä¸€ä¸ªå…·æœ‰éšæœºå‚æ•°è€Œä¸æ˜¯é¢„è®­ç»ƒæƒé‡çš„æ¨¡å‹ã€‚åœ¨è®­ç»ƒè¯¥æ¨¡å‹ä¹‹å‰ï¼Œæ‚¨è¿˜æ— æ³•å°†è¯¥æ¨¡å‹ç”¨äºä»»ä½•ç”¨é€”ã€‚è®­ç»ƒæ˜¯ä¸€é¡¹æ˜‚è´µä¸”è€—æ—¶çš„è¿‡ç¨‹ã€‚é€šå¸¸æ¥è¯´ï¼Œæœ€å¥½ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æ¥æ›´å¿«åœ°è·å¾—æ›´å¥½çš„ç»“æœï¼ŒåŒæ—¶ä»…ä½¿ç”¨è®­ç»ƒæ‰€éœ€èµ„æºçš„ä¸€å°éƒ¨åˆ†ã€‚

ä½¿ç”¨ [`~TFPreTrainedModel.from_pretrained`] åˆ›å»ºé¢„è®­ç»ƒæ¨¡å‹ï¼š

```py
>>> tf_model = TFDistilBertModel.from_pretrained("distilbert-base-uncased")
```

å½“åŠ è½½é¢„è®­ç»ƒæƒé‡æ—¶ï¼Œå¦‚æœæ¨¡å‹æ˜¯ç”± ğŸ¤— Transformers æä¾›çš„ï¼Œå°†è‡ªåŠ¨åŠ è½½é»˜è®¤æ¨¡å‹é…ç½®ã€‚ç„¶è€Œï¼Œå¦‚æœä½ æ„¿æ„ï¼Œä»ç„¶å¯ä»¥å°†é»˜è®¤æ¨¡å‹é…ç½®çš„æŸäº›æˆ–è€…æ‰€æœ‰å±æ€§æ›¿æ¢æˆè‡ªå·±çš„é…ç½®ï¼š

```py
>>> tf_model = TFDistilBertModel.from_pretrained("distilbert-base-uncased", config=my_config)
```
</tf>
</frameworkcontent>

### æ¨¡å‹å¤´ï¼ˆModel headsï¼‰

æ­¤æ—¶ï¼Œä½ å·²ç»æœ‰äº†ä¸€ä¸ªè¾“å‡º*éšè—çŠ¶æ€*çš„åŸºç¡€ DistilBERT æ¨¡å‹ã€‚éšè—çŠ¶æ€ä½œä¸ºè¾“å…¥ä¼ é€’åˆ°æ¨¡å‹å¤´ä»¥ç”Ÿæˆæœ€ç»ˆè¾“å‡ºã€‚ğŸ¤— Transformers ä¸ºæ¯ä¸ªä»»åŠ¡æä¾›ä¸åŒçš„æ¨¡å‹å¤´ï¼Œåªè¦æ¨¡å‹æ”¯æŒè¯¥ä»»åŠ¡ï¼ˆå³ï¼Œæ‚¨ä¸èƒ½ä½¿ç”¨ DistilBERT æ¥æ‰§è¡Œåƒç¿»è¯‘è¿™æ ·çš„åºåˆ—åˆ°åºåˆ—ä»»åŠ¡ï¼‰ã€‚

<frameworkcontent>
<pt>
ä¾‹å¦‚ï¼Œ[`DistilBertForSequenceClassification`] æ˜¯ä¸€ä¸ªå¸¦æœ‰åºåˆ—åˆ†ç±»å¤´ï¼ˆsequence classification headï¼‰çš„åŸºç¡€ DistilBERT æ¨¡å‹ã€‚åºåˆ—åˆ†ç±»å¤´æ˜¯æ± åŒ–è¾“å‡ºä¹‹ä¸Šçš„çº¿æ€§å±‚ã€‚

```py
>>> from transformers import DistilBertForSequenceClassification

>>> model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")
```

é€šè¿‡åˆ‡æ¢åˆ°ä¸åŒçš„æ¨¡å‹å¤´ï¼Œå¯ä»¥è½»æ¾åœ°å°†æ­¤æ£€æŸ¥ç‚¹é‡å¤ç”¨äºå…¶ä»–ä»»åŠ¡ã€‚å¯¹äºé—®ç­”ä»»åŠ¡ï¼Œä½ å¯ä»¥ä½¿ç”¨ [`DistilBertForQuestionAnswering`] æ¨¡å‹å¤´ã€‚é—®ç­”å¤´ï¼ˆquestion answering headï¼‰ä¸åºåˆ—åˆ†ç±»å¤´ç±»ä¼¼ï¼Œä¸åŒç‚¹åœ¨äºå®ƒæ˜¯éšè—çŠ¶æ€è¾“å‡ºä¹‹ä¸Šçš„çº¿æ€§å±‚ã€‚

```py
>>> from transformers import DistilBertForQuestionAnswering

>>> model = DistilBertForQuestionAnswering.from_pretrained("distilbert-base-uncased")
```
</pt>
<tf>
ä¾‹å¦‚ï¼Œ[`TFDistilBertForSequenceClassification`] æ˜¯ä¸€ä¸ªå¸¦æœ‰åºåˆ—åˆ†ç±»å¤´ï¼ˆsequence classification headï¼‰çš„åŸºç¡€ DistilBERT æ¨¡å‹ã€‚åºåˆ—åˆ†ç±»å¤´æ˜¯æ± åŒ–è¾“å‡ºä¹‹ä¸Šçš„çº¿æ€§å±‚ã€‚

```py
>>> from transformers import TFDistilBertForSequenceClassification

>>> tf_model = TFDistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")
```

é€šè¿‡åˆ‡æ¢åˆ°ä¸åŒçš„æ¨¡å‹å¤´,å¯ä»¥è½»æ¾åœ°å°†æ­¤æ£€æŸ¥ç‚¹é‡å¤ç”¨äºå…¶ä»–ä»»åŠ¡ã€‚å¯¹äºé—®ç­”ä»»åŠ¡ï¼Œä½ å¯ä»¥ä½¿ç”¨ [`TFDistilBertForQuestionAnswering`] æ¨¡å‹å¤´ã€‚é—®ç­”å¤´ï¼ˆquestion answering headï¼‰ä¸åºåˆ—åˆ†ç±»å¤´ç±»ä¼¼ï¼Œä¸åŒç‚¹åœ¨äºå®ƒæ˜¯éšè—çŠ¶æ€è¾“å‡ºä¹‹ä¸Šçš„çº¿æ€§å±‚ã€‚

```py
>>> from transformers import TFDistilBertForQuestionAnswering

>>> tf_model = TFDistilBertForQuestionAnswering.from_pretrained("distilbert-base-uncased")
```
</tf>
</frameworkcontent>

## åˆ†è¯å™¨

åœ¨å°†æ¨¡å‹ç”¨äºæ–‡æœ¬æ•°æ®ä¹‹å‰ï¼Œä½ éœ€è¦çš„æœ€åä¸€ä¸ªåŸºç±»æ˜¯ [tokenizer](main_classes/tokenizer)ï¼Œå®ƒç”¨äºå°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸ºå¼ é‡ã€‚ğŸ¤— Transformers æ”¯æŒä¸¤ç§ç±»å‹çš„åˆ†è¯å™¨ï¼š

- [`PreTrainedTokenizer`]ï¼šåˆ†è¯å™¨çš„Pythonå®ç°
- [`PreTrainedTokenizerFast`]ï¼šæ¥è‡ªæˆ‘ä»¬åŸºäº Rust çš„ [ğŸ¤— Tokenizer](https://huggingface.co/docs/tokenizers/python/latest/) åº“çš„åˆ†è¯å™¨ã€‚å› ä¸ºå…¶ä½¿ç”¨äº† Rust å®ç°ï¼Œè¿™ç§åˆ†è¯å™¨ç±»å‹çš„é€Ÿåº¦è¦å¿«å¾—å¤šï¼Œå°¤å…¶æ˜¯åœ¨æ‰¹é‡åˆ†è¯ï¼ˆbatch tokenizationï¼‰çš„æ—¶å€™ã€‚å¿«é€Ÿåˆ†è¯å™¨è¿˜æä¾›å…¶ä»–çš„æ–¹æ³•ï¼Œä¾‹å¦‚*åç§»æ˜ å°„ï¼ˆoffset mappingï¼‰*ï¼Œå®ƒå°†æ ‡è®°ï¼ˆtokenï¼‰æ˜ å°„åˆ°å…¶åŸå§‹å•è¯æˆ–å­—ç¬¦ã€‚

è¿™ä¸¤ç§åˆ†è¯å™¨éƒ½æ”¯æŒå¸¸ç”¨çš„æ–¹æ³•ï¼Œå¦‚ç¼–ç å’Œè§£ç ã€æ·»åŠ æ–°æ ‡è®°ä»¥åŠç®¡ç†ç‰¹æ®Šæ ‡è®°ã€‚

<Tip warning={true}>

å¹¶éæ¯ä¸ªæ¨¡å‹éƒ½æ”¯æŒå¿«é€Ÿåˆ†è¯å™¨ã€‚å‚ç…§è¿™å¼  [è¡¨æ ¼](index#supported-frameworks) æŸ¥çœ‹æ¨¡å‹æ˜¯å¦æ”¯æŒå¿«é€Ÿåˆ†è¯å™¨ã€‚

</Tip>

å¦‚æœæ‚¨è®­ç»ƒäº†è‡ªå·±çš„åˆ†è¯å™¨ï¼Œåˆ™å¯ä»¥ä»*è¯è¡¨*æ–‡ä»¶åˆ›å»ºä¸€ä¸ªåˆ†è¯å™¨ï¼š

```py
>>> from transformers import DistilBertTokenizer

>>> my_tokenizer = DistilBertTokenizer(vocab_file="my_vocab_file.txt", do_lower_case=False, padding_side="left")
```

è¯·åŠ¡å¿…è®°ä½ï¼Œè‡ªå®šä¹‰åˆ†è¯å™¨ç”Ÿæˆçš„è¯è¡¨ä¸é¢„è®­ç»ƒæ¨¡å‹åˆ†è¯å™¨ç”Ÿæˆçš„è¯è¡¨æ˜¯ä¸åŒçš„ã€‚å¦‚æœä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œåˆ™éœ€è¦ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„è¯è¡¨ï¼Œå¦åˆ™è¾“å…¥å°†æ²¡æœ‰æ„ä¹‰ã€‚ ä½¿ç”¨ [`DistilBertTokenizer`] ç±»åˆ›å»ºå…·æœ‰é¢„è®­ç»ƒæ¨¡å‹è¯è¡¨çš„åˆ†è¯å™¨ï¼š

```py
>>> from transformers import DistilBertTokenizer

>>> slow_tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
```

ä½¿ç”¨ [`DistilBertTokenizerFast`] ç±»åˆ›å»ºå¿«é€Ÿåˆ†è¯å™¨ï¼š

```py
>>> from transformers import DistilBertTokenizerFast

>>> fast_tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert-base-uncased")
```

<Tip>

é»˜è®¤æƒ…å†µä¸‹ï¼Œ[`AutoTokenizer`] å°†å°è¯•åŠ è½½å¿«é€Ÿæ ‡è®°ç”Ÿæˆå™¨ã€‚ä½ å¯ä»¥é€šè¿‡åœ¨ `from_pretrained` ä¸­è®¾ç½® `use_fast=False` ä»¥ç¦ç”¨æ­¤è¡Œä¸ºã€‚

</Tip>

## å›¾åƒå¤„ç†å™¨

å›¾åƒå¤„ç†å™¨ç”¨äºå¤„ç†è§†è§‰è¾“å…¥ã€‚å®ƒç»§æ‰¿è‡ª [`~image_processing_utils.ImageProcessingMixin`] åŸºç±»ã€‚

è¦ä½¿ç”¨å®ƒï¼Œéœ€è¦åˆ›å»ºä¸€ä¸ªä¸ä½ ä½¿ç”¨çš„æ¨¡å‹å…³è”çš„å›¾åƒå¤„ç†å™¨ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ ä½¿ç”¨ [ViT](model_doc/vit) è¿›è¡Œå›¾åƒåˆ†ç±»ï¼Œå¯ä»¥åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„ [`ViTImageProcessor`]ï¼š

```py
>>> from transformers import ViTImageProcessor

>>> vit_extractor = ViTImageProcessor()
>>> print(vit_extractor)
ViTImageProcessor {
  "do_normalize": true,
  "do_resize": true,
  "image_processor_type": "ViTImageProcessor",
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "resample": 2,
  "size": 224
}
```

<Tip>

å¦‚æœæ‚¨ä¸éœ€è¦è¿›è¡Œä»»ä½•è‡ªå®šä¹‰ï¼Œåªéœ€ä½¿ç”¨ `from_pretrained` æ–¹æ³•åŠ è½½æ¨¡å‹çš„é»˜è®¤å›¾åƒå¤„ç†å™¨å‚æ•°ã€‚

</Tip>

ä¿®æ”¹ä»»ä½• [`ViTImageProcessor`] å‚æ•°ä»¥åˆ›å»ºè‡ªå®šä¹‰å›¾åƒå¤„ç†å™¨ï¼š

```py
>>> from transformers import ViTImageProcessor

>>> my_vit_extractor = ViTImageProcessor(resample="PIL.Image.BOX", do_normalize=False, image_mean=[0.3, 0.3, 0.3])
>>> print(my_vit_extractor)
ViTImageProcessor {
  "do_normalize": false,
  "do_resize": true,
  "image_processor_type": "ViTImageProcessor",
  "image_mean": [
    0.3,
    0.3,
    0.3
  ],
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "resample": "PIL.Image.BOX",
  "size": 224
}
```

## ç‰¹å¾æå–å™¨

ç‰¹å¾æå–å™¨ç”¨äºå¤„ç†éŸ³é¢‘è¾“å…¥ã€‚å®ƒç»§æ‰¿è‡ª [`~feature_extraction_utils.FeatureExtractionMixin`] åŸºç±»ï¼Œäº¦å¯ç»§æ‰¿ [`SequenceFeatureExtractor`] ç±»æ¥å¤„ç†éŸ³é¢‘è¾“å…¥ã€‚

è¦ä½¿ç”¨å®ƒï¼Œåˆ›å»ºä¸€ä¸ªä¸ä½ ä½¿ç”¨çš„æ¨¡å‹å…³è”çš„ç‰¹å¾æå–å™¨ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ ä½¿ç”¨ [Wav2Vec2](model_doc/wav2vec2) è¿›è¡ŒéŸ³é¢‘åˆ†ç±»ï¼Œå¯ä»¥åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„ [`Wav2Vec2FeatureExtractor`]ï¼š

```py
>>> from transformers import Wav2Vec2FeatureExtractor

>>> w2v2_extractor = Wav2Vec2FeatureExtractor()
>>> print(w2v2_extractor)
Wav2Vec2FeatureExtractor {
  "do_normalize": true,
  "feature_extractor_type": "Wav2Vec2FeatureExtractor",
  "feature_size": 1,
  "padding_side": "right",
  "padding_value": 0.0,
  "return_attention_mask": false,
  "sampling_rate": 16000
}
```

<Tip>

å¦‚æœæ‚¨ä¸éœ€è¦è¿›è¡Œä»»ä½•è‡ªå®šä¹‰ï¼Œåªéœ€ä½¿ç”¨ `from_pretrained` æ–¹æ³•åŠ è½½æ¨¡å‹çš„é»˜è®¤ç‰¹å¾æå–å™¨å‚æ•°ã€‚

</Tip>

ä¿®æ”¹ä»»ä½• [`Wav2Vec2FeatureExtractor`] å‚æ•°ä»¥åˆ›å»ºè‡ªå®šä¹‰ç‰¹å¾æå–å™¨ï¼š

```py
>>> from transformers import Wav2Vec2FeatureExtractor

>>> w2v2_extractor = Wav2Vec2FeatureExtractor(sampling_rate=8000, do_normalize=False)
>>> print(w2v2_extractor)
Wav2Vec2FeatureExtractor {
  "do_normalize": false,
  "feature_extractor_type": "Wav2Vec2FeatureExtractor",
  "feature_size": 1,
  "padding_side": "right",
  "padding_value": 0.0,
  "return_attention_mask": false,
  "sampling_rate": 8000
}
```


## å¤„ç†å™¨

å¯¹äºæ”¯æŒå¤šæ¨¡å¼ä»»åŠ¡çš„æ¨¡å‹ï¼ŒğŸ¤— Transformers æä¾›äº†ä¸€ä¸ªå¤„ç†å™¨ç±»ï¼Œå¯ä»¥æ–¹ä¾¿åœ°å°†ç‰¹å¾æå–å™¨å’Œåˆ†è¯å™¨ç­‰å¤„ç†ç±»åŒ…è£…åˆ°å•ä¸ªå¯¹è±¡ä¸­ã€‚ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨ [`Wav2Vec2Processor`] æ¥æ‰§è¡Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä»»åŠ¡ (ASR)ã€‚ ASR å°†éŸ³é¢‘è½¬å½•ä¸ºæ–‡æœ¬ï¼Œå› æ­¤æ‚¨å°†éœ€è¦ä¸€ä¸ªç‰¹å¾æå–å™¨å’Œä¸€ä¸ªåˆ†è¯å™¨ã€‚

åˆ›å»ºä¸€ä¸ªç‰¹å¾æå–å™¨æ¥å¤„ç†éŸ³é¢‘è¾“å…¥ï¼š

```py
>>> from transformers import Wav2Vec2FeatureExtractor

>>> feature_extractor = Wav2Vec2FeatureExtractor(padding_value=1.0, do_normalize=True)
```

åˆ›å»ºä¸€ä¸ªåˆ†è¯å™¨æ¥å¤„ç†æ–‡æœ¬è¾“å…¥ï¼š

```py
>>> from transformers import Wav2Vec2CTCTokenizer

>>> tokenizer = Wav2Vec2CTCTokenizer(vocab_file="my_vocab_file.txt")
```

å°†ç‰¹å¾æå–å™¨å’Œåˆ†è¯å™¨åˆå¹¶åˆ° [`Wav2Vec2Processor`] ä¸­ï¼š

```py
>>> from transformers import Wav2Vec2Processor

>>> processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)
```

é€šè¿‡ä¸¤ä¸ªåŸºç±» - é…ç½®ç±»å’Œæ¨¡å‹ç±» - ä»¥åŠä¸€ä¸ªé™„åŠ çš„é¢„å¤„ç†ç±»ï¼ˆåˆ†è¯å™¨ã€å›¾åƒå¤„ç†å™¨ã€ç‰¹å¾æå–å™¨æˆ–å¤„ç†å™¨ï¼‰ï¼Œä½ å¯ä»¥åˆ›å»º ğŸ¤— Transformers æ”¯æŒçš„ä»»ä½•æ¨¡å‹ã€‚ æ¯ä¸ªåŸºç±»éƒ½æ˜¯å¯é…ç½®çš„ï¼Œå…è®¸ä½ ä½¿ç”¨æ‰€éœ€çš„ç‰¹å®šå±æ€§ã€‚ ä½ å¯ä»¥è½»æ¾è®¾ç½®æ¨¡å‹è¿›è¡Œè®­ç»ƒæˆ–ä¿®æ”¹ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚
