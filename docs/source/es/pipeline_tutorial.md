<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Pipelines para inferencia

Un [`pipeline`] simplifica el uso de cualquier modelo del [Hub](https://huggingface.co/models) para la inferencia en una variedad de tareas como la generaci√≥n de texto, la segmentaci√≥n de im√°genes y la clasificaci√≥n de audio. Incluso si no tienes experiencia con una modalidad espec√≠fica o no comprendes el c√≥digo que alimenta los modelos, ¬°a√∫n puedes usarlos con el [`pipeline`]! Este tutorial te ense√±ar√° a:

* Utilizar un [`pipeline`] para inferencia.
* Utilizar un tokenizador o modelo espec√≠fico.
* Utilizar un [`pipeline`] para tareas de audio y visi√≥n.

<Tip>

Echa un vistazo a la documentaci√≥n de [`pipeline`] para obtener una lista completa de tareas admitidas.

</Tip>

## Uso del pipeline

Si bien cada tarea tiene un [`pipeline`] asociado, es m√°s sencillo usar la abstracci√≥n general [`pipeline`] que contiene todos los pipelines de tareas espec√≠ficas. El [`pipeline`] carga autom√°ticamente un modelo predeterminado y un tokenizador con capacidad de inferencia para tu tarea. Veamos el ejemplo de usar un [`pipeline`] para reconocimiento autom√°tico del habla (ASR), o texto a voz.

1. Comienza creando un [`pipeline`] y espec√≠fica una tarea de inferencia:

```py
>>> from transformers import pipeline

>>> transcriber = pipeline(task="automatic-speech-recognition")
```

2. Pasa tu entrada a la [`pipeline`]. En el caso del reconocimiento del habla, esto es un archivo de entrada de audio:

```py
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': 'I HAVE A DREAM BUT ONE DAY THIS NATION WILL RISE UP LIVE UP THE TRUE MEANING OF ITS TREES'}
```

¬øNo es el resultado que ten√≠as en mente? Echa un vistazo a algunos de los [modelos de reconocimiento autom√°tico del habla m√°s descargados](https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=trending) 
en el Hub para ver si puedes obtener una mejor transcripci√≥n.

Intentemos con el modelo [Whisper large-v2](https://huggingface.co/openai/whisper-large) de OpenAI. Whisper se lanz√≥ 
2 a√±os despu√©s que Wav2Vec2, y se entren√≥ con cerca de 10 veces m√°s datos. Como tal, supera a Wav2Vec2 en la mayor√≠a de las pruebas 
downstream. Tambi√©n tiene el beneficio adicional de predecir puntuaci√≥n y may√∫sculas, ninguno de los cuales es posible con  
Wav2Vec2.

Vamos a probarlo aqu√≠ para ver c√≥mo se desempe√±a:

```py
>>> transcriber = pipeline(model="openai/whisper-large-v2")
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
```

¬°Ahora este resultado parece m√°s preciso! Para una comparaci√≥n detallada de Wav2Vec2 vs Whisper, consulta el [Curso de Transformers de Audio](https://huggingface.co/learn/audio-course/chapter5/asr_models).
Realmente te animamos a que eches un vistazo al Hub para modelos en diferentes idiomas, modelos especializados en tu campo, y m√°s.
Puedes comparar directamente los resultados de los modelos desde tu navegador en el Hub para ver si se adapta o 
maneja casos de borde mejor que otros.
Y si no encuentras un modelo para tu caso de uso, siempre puedes empezar a [entrenar](training) el tuyo propio.

Si tienes varias entradas, puedes pasar tu entrada como una lista:

```py
transcriber(
    [
        "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac",
        "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac",
    ]
)
```

Los pipelines son ideales para la experimentaci√≥n, ya que cambiar de un modelo a otro es trivial; sin embargo, hay algunas formas de optimizarlas para cargas de trabajo m√°s grandes que la experimentaci√≥n. Consulta las siguientes gu√≠as que profundizan en iterar sobre conjuntos de datos completos o utilizar pipelines en un servidor web:
de la documentaci√≥n:

* [Uso de pipelines en un conjunto de datos](#uso-de-pipelines-en-un-conjunto-de-datos)
* [Uso de pipelines para un servidor web](./pipeline_webserver)

## Par√°metros

[`pipeline`] admite muchos par√°metros; algunos son espec√≠ficos de la tarea y algunos son generales para todas las pipelines. En general, puedes especificar par√°metros en cualquier lugar que desees:

```py
transcriber = pipeline(model="openai/whisper-large-v2", my_parameter=1)

out = transcriber(...)  # This will use `my_parameter=1`.
out = transcriber(..., my_parameter=2)  # This will override and use `my_parameter=2`.
out = transcriber(...)  # This will go back to using `my_parameter=1`.
```

Vamos a echar un vistazo a tres importantes:

### Device

Si usas `device=n`, el pipeline autom√°ticamente coloca el modelo en el dispositivo especificado. Esto funcionar√° independientemente de si est√°s utilizando PyTorch o Tensorflow.

```py
transcriber = pipeline(model="openai/whisper-large-v2", device=0)
```

Si el modelo es demasiado grande para una sola GPU y est√°s utilizando PyTorch, puedes establecer `device_map="auto"` para determinar autom√°ticamente c√≥mo cargar y almacenar los pesos del modelo. Utilizar el argumento `device_map` requiere el paquete ü§ó [Accelerate](https://huggingface.co/docs/accelerate):

```bash
pip install --upgrade accelerate
```

El siguiente c√≥digo carga y almacena autom√°ticamente los pesos del modelo en varios dispositivos:

```py
transcriber = pipeline(model="openai/whisper-large-v2", device_map="auto")
```

Tenga en cuenta que si se pasa `device_map="auto"`, no es necesario agregar el argumento `device=device` al instanciar tu `pipeline`, ¬°ya que podr√≠as encontrar alg√∫n comportamiento inesperado!

### Batch size

Por defecto, los pipelines no realizar√°n inferencia por lotes por razones explicadas en detalle [aqu√≠](https://huggingface.co/docs/transformers/main_classes/pipelines#pipeline-batching). La raz√≥n es que la agrupaci√≥n en lotes no es necesariamente m√°s r√°pida y, de hecho, puede ser bastante m√°s lenta en algunos casos.

Pero si funciona en tu caso de uso, puedes utilizar:

```py
transcriber = pipeline(model="openai/whisper-large-v2", device=0, batch_size=2)
audio_filenames = [f"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/{i}.flac" for i in range(1, 5)]
texts = transcriber(audio_filenames)
```

Esto ejecuta el pipeline en los 4 archivos de audio proporcionados, pero los pasar√° en lotes de a 2 al modelo (que est√° en una GPU, donde la agrupaci√≥n en lotes es m√°s probable que ayude) sin requerir ning√∫n c√≥digo adicional de tu parte. La salida siempre deber√≠a coincidir con lo que habr√≠as recibido sin agrupaci√≥n en lotes. Solo se pretende como una forma de ayudarte a obtener m√°s velocidad de una pipeline.

Los pipelines tambi√©n pueden aliviar algunas de las complejidades de la agrupaci√≥n en lotes porque, para algunos pipelines, un solo elemento (como un archivo de audio largo) necesita ser dividido en varias partes para ser procesado por un modelo. El pipeline realiza esta [*agrupaci√≥n en lotes de fragmentos*](https://huggingface.co/docs/transformers/main_classes/pipelines#pipeline-chunk-batching) por ti.

### Task specific parameters

Todas las tareas proporcionan par√°metros espec√≠ficos de la tarea que permiten flexibilidad adicional y opciones para ayudarte a completar tu trabajo. Por ejemplo, el m√©todo [`transformers.AutomaticSpeechRecognitionPipeline.__call__`] tiene un par√°metro `return_timestamps` que suena prometedor para subt√≠tulos de videos:

```py
>>> transcriber = pipeline(model="openai/whisper-large-v2", return_timestamps=True)
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.', 'chunks': [{'timestamp': (0.0, 11.88), 'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its'}, {'timestamp': (11.88, 12.38), 'text': ' creed.'}]}
```

Como puedes ver, el modelo infiri√≥ el texto y tambi√©n sali√≥ **cu√°ndo** se pronunciaron las distintas oraciones.

Hay muchos par√°metros disponibles para cada tarea, as√≠ que echa un vistazo a la referencia de la API de cada tarea para ver qu√© puedes ajustar. Por ejemplo, el [`~transformers.AutomaticSpeechRecognitionPipeline`] tiene un par√°metro `chunk_length_s` que es √∫til para trabajar con archivos de audio realmente largos (por ejemplo, subt√≠tulos de pel√≠culas completas o videos de una hora de duraci√≥n) que un modelo t√≠picamente no puede manejar solo:

```python
>>> transcriber = pipeline(model="openai/whisper-large-v2", chunk_length_s=30)
>>> transcriber("https://huggingface.co/datasets/reach-vb/random-audios/resolve/main/ted_60.wav")
{'text': " So in college, I was a government major, which means I had to write a lot of papers. Now, when a normal student writes a paper, they might spread the work out a little like this. So, you know. You get started maybe a little slowly, but you get enough done in the first week that with some heavier days later on, everything gets done and things stay civil. And I would want to do that like that. That would be the plan. I would have it all ready to go, but then actually the paper would come along, and then I would kind of do this. And that would happen every single paper. But then came my 90-page senior thesis, a paper you're supposed to spend a year on. I knew for a paper like that, my normal workflow was not an option, it was way too big a project. So I planned things out and I decided I kind of had to go something like this. This is how the year would go. So I'd start off light and I'd bump it up"}
```

¬°Si no puedes encontrar un par√°metro que te ayude, no dudes en [solicitarlo](https://github.com/huggingface/transformers/issues/new?assignees=&labels=feature&template=feature-request.yml)!

## Uso de pipelines en un conjunto de datos

Los pipeline tambi√©n puede ejecutar inferencia en un conjunto de datos grande. La forma m√°s f√°cil que recomendamos para hacer esto es utilizando un iterador:

```py
def data():
    for i in range(1000):
        yield f"My example {i}"


pipe = pipeline(model="openai-community/gpt2", device=0)
generated_characters = 0
for out in pipe(data()):
    generated_characters += len(out[0]["generated_text"])
```

El iterador `data()` produce cada resultado, y el pipeline autom√°ticamente
reconoce que la entrada es iterable y comenzar√° a buscar los datos mientras
contin√∫a proces√°ndolos en la GPU (dicho proceso utiliza [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)). Esto es importante porque no tienes que asignar memoria para todo el conjunto de datos y puedes alimentar la GPU lo m√°s r√°pido posible.

Dado que la agrupaci√≥n en lotes podr√≠a acelerar las cosas, puede ser √∫til intentar ajustar el par√°metro `batch_size` aqu√≠.

La forma m√°s sencilla de iterar sobre un conjunto de datos es cargandolo desde ü§ó [Datasets](https://github.com/huggingface/datasets/):

```py
# KeyDataset is a util that will just output the item we're interested in.
from transformers.pipelines.pt_utils import KeyDataset
from datasets import load_dataset

pipe = pipeline(model="hf-internal-testing/tiny-random-wav2vec2", device=0)
dataset = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation[:10]")

for out in pipe(KeyDataset(dataset, "audio")):
    print(out)
```

## Uso de pipelines para un servidor web

<Tip>
Crear un motor de inferencia es un tema complejo que merece su propia p√°gina.
</Tip>

[Link](./pipeline_webserver)

## Pipeline de visi√≥n

Usar un [`pipeline`] para tareas de visi√≥n es pr√°cticamente id√©ntico.

Especifica tu tarea y pasa tu imagen al clasificador. La imagen puede ser un enlace, una ruta local o una imagen codificada en base64. Por ejemplo, ¬øqu√© especie de gato se muestra a continuaci√≥n?

![pipeline-cat-chonk](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg)

```py
>>> from transformers import pipeline

>>> vision_classifier = pipeline(model="google/vit-base-patch16-224")
>>> preds = vision_classifier(
...     images="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> preds
[{'score': 0.4335, 'label': 'lynx, catamount'}, {'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.0239, 'label': 'Egyptian cat'}, {'score': 0.0229, 'label': 'tiger cat'}]
```

## Pipeline de texto

Usar un [`pipeline`] para tareas de PLN es pr√°cticamente id√©ntico.

```py
>>> from transformers import pipeline

>>> # This model is a `zero-shot-classification` model.
>>> # It will classify text, except you are free to choose any label you might imagine
>>> classifier = pipeline(model="facebook/bart-large-mnli")
>>> classifier(
...     "I have a problem with my iphone that needs to be resolved asap!!",
...     candidate_labels=["urgent", "not urgent", "phone", "tablet", "computer"],
... )
{'sequence': 'I have a problem with my iphone that needs to be resolved asap!!', 'labels': ['urgent', 'phone', 'computer', 'not urgent', 'tablet'], 'scores': [0.504, 0.479, 0.013, 0.003, 0.002]}
```

## Pipeline multimodal

[`pipeline`] admite m√°s de una modalidad. Por ejemplo, una tarea de respuesta a preguntas visuales (VQA) combina texto e imagen. No dudes en usar cualquier enlace de imagen que desees y una pregunta que quieras hacer sobre la imagen. La imagen puede ser una URL o una ruta local a la imagen.

Por ejemplo, si usas esta [imagen de factura](https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png):

```py
>>> from transformers import pipeline

>>> vqa = pipeline(model="impira/layoutlm-document-qa")
>>> output = vqa(
...     image="https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png",
...     question="What is the invoice number?",
... )
>>> output[0]["score"] = round(output[0]["score"], 3)
>>> output
[{'score': 0.425, 'answer': 'us-001', 'start': 16, 'end': 16}]
```

<Tip>

Para ejecutar el ejemplo anterior, debe tener instalado [`pytesseract`](https://pypi.org/project/pytesseract/) adem√°s de ü§ó Transformers:

```bash
sudo apt install -y tesseract-ocr
pip install pytesseract
```

</Tip>

## Uso de `pipeline` en modelos grandes con ü§ó `accelerate`:

¬°Puedes ejecutar f√°cilmente `pipeline` en modelos grandes utilizando ü§ó `accelerate`! Primero aseg√∫rate de haber instalado `accelerate` con `pip install accelerate`. 

¬°Luego carga tu modelo utilizando `device_map="auto"`! Utilizaremos `facebook/opt-1.3b` para nuestro ejemplo.

```py
# pip install accelerate
import torch
from transformers import pipeline

pipe = pipeline(model="facebook/opt-1.3b", torch_dtype=torch.bfloat16, device_map="auto")
output = pipe("This is a cool example!", do_sample=True, top_p=0.95)
```

Tambi√©n puedes pasar modelos cargados de 8 bits s√≠ instalas `bitsandbytes` y agregas el argumento `load_in_8bit=True`

```py
# pip install accelerate bitsandbytes
import torch
from transformers import pipeline

pipe = pipeline(model="facebook/opt-1.3b", device_map="auto", model_kwargs={"load_in_8bit": True})
output = pipe("This is a cool example!", do_sample=True, top_p=0.95)
```

Nota que puedes reemplazar el punto de control con cualquier modelo de Hugging Face que admita la carga de modelos grandes, como BLOOM.

## Crear demos web desde pipelines con `gradio`

Los pipelines est√°n autom√°ticamente soportadas en [Gradio](https://github.com/gradio-app/gradio/), una biblioteca que hace que crear aplicaciones de aprendizaje autom√°tico hermosas y f√°ciles de usar en la web sea un proceso sencillo. Primero, aseg√∫rate de tener Gradio instalado:

```
pip install gradio
```

Luego, puedes crear una demo web alrededor de una pipeline de clasificaci√≥n de im√°genes (o cualquier otra pipeline) en una sola l√≠nea de c√≥digo llamando a la funci√≥n `Interface.from_pipeline` de Gradio para lanzar la pipeline. Esto crea una interfaz intuitiva *drag-and-drop* en tu navegador:

```py
from transformers import pipeline
import gradio as gr

pipe = pipeline("image-classification", model="google/vit-base-patch16-224")

gr.Interface.from_pipeline(pipe).launch()
```


![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/panda-classification.png)

De forma predeterminada, la demo web se ejecuta en un servidor local. Si deseas compartirlo con otros, puedes generar un enlace p√∫blico temporal estableciendo `share=True` en `launch()`. Tambi√©n puedes hospedar tu demo en [Hugging Face Spaces](https://huggingface.co/spaces) para un enlace permanente.
