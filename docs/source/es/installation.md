<!---
Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Instalaci√≥n

En esta gu√≠a puedes encontrar informaci√≥n para instalar ü§ó Transformers para cualquier biblioteca de Machine Learning con la que est√©s trabajando. Adem√°s, encontrar√°s informaci√≥n sobre c√≥mo establecer el cach√© y c√≥mo configurar ü§ó Transformers para correrlo de manera offline (opcional).

ü§ó Transformers ha sido probada en Python 3.6+, PyTorch 1.1.0+, TensorFlow 2.0+, y Flax. Para instalar la biblioteca de deep learning con la que desees trabajar, sigue las instrucciones correspondientes listadas a continuaci√≥n:

* [PyTorch](https://pytorch.org/get-started/locally/)
* [TensorFlow 2.0](https://www.tensorflow.org/install/pip)
* [Flax](https://flax.readthedocs.io/en/latest/)

## Instalaci√≥n con pip

Es necesario instalar ü§ó Transformers en un [entorno virtual](https://docs.python.org/3/library/venv.html). Si necesitas m√°s informaci√≥n sobre entornos virtuales de Python, consulta esta [gu√≠a](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/
). Un entorno virtual facilita el manejo de proyectos y evita problemas de compatibilidad entre dependencias.

Comienza por crear un entorno virtual en el directorio de tu proyecto:

```bash
python -m venv .env
```

Activa el entorno virtual:

```bash
source .env/bin/activate
```

Ahora puedes instalar ü§ó Transformers con el siguiente comando:

```bash
pip install transformers
```

Solo para CPU, puedes instalar ü§ó Transformers y una biblioteca de deep learning con un comando de una sola l√≠nea.

Por ejemplo, instala ü§ó Transformers y Pytorch:

```bash
pip install transformers[torch]
```

ü§ó Transformers y TensorFlow 2.0:

```bash
pip install transformers[tf-cpu]
```

ü§ó Transformers y Flax:

```bash
pip install transformers[flax]
```

Por √∫ltimo, revisa si ü§ó Transformers ha sido instalada exitosamente con el siguiente comando que descarga un modelo pre-entrenado:

```bash
python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"
```
Despu√©s imprime la etiqueta y el puntaje:

```bash
[{'label': 'POSITIVE', 'score': 0.9998704791069031}]
```

## Instalaci√≥n desde la fuente

Instala ü§ó Transformers desde la fuente con el siguiente comando:

```bash
pip install git+https://github.com/huggingface/transformers
```

El comando de arriba instala la versi√≥n `master` m√°s actual en vez de la √∫ltima versi√≥n estable. La versi√≥n `master` es √∫til para obtener los √∫ltimos avances de  ü§ó Transformers. Por ejemplo, se puede dar el caso de que un error fue corregido despu√©s de la √∫ltima versi√≥n estable pero a√∫n no se ha liberado un nuevo lanzamiento. Sin embargo, existe la posibilidad de que la versi√≥n `master` no sea estable. El equipo trata de mantener la versi√≥n `master` operacional y la mayor√≠a de los errores son resueltos en unas cuantas horas o un d√≠a. Si encuentras alg√∫n problema, por favor abre un [Issue](https://github.com/huggingface/transformers/issues) para que pueda ser corregido m√°s r√°pido.

Verifica si ü§ó Transformers est√° instalada apropiadamente con el siguiente comando:

```bash
python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))"
```

## Instalaci√≥n editable

Necesitar√°s una instalaci√≥n editable si deseas:
* Usar la versi√≥n `master` del c√≥digo fuente.
* Contribuir a ü§ó Transformers y necesitas probar cambios en el c√≥digo.

Clona el repositorio e instala ü§ó Transformers con los siguientes comandos:

```bash
git clone https://github.com/huggingface/transformers.git
cd transformers
pip install -e .
```

√âstos comandos van a ligar el directorio desde donde clonamos el repositorio al path de las bibliotecas de Python. Python ahora buscar√° dentro de la carpeta que clonaste adem√°s de los paths normales de la biblioteca. Por ejemplo, si los paquetes de Python se encuentran instalados en `~/anaconda3/envs/main/lib/python3.7/site-packages/`, Python tambi√©n buscar√° en el directorio desde donde clonamos el repositorio `~/transformers/`.

<Tip warning={true}>

Debes mantener el directorio `transformers` si deseas seguir usando la biblioteca.

</Tip>

Puedes actualizar tu copia local a la √∫ltima versi√≥n de ü§ó Transformers con el siguiente comando:

```bash
cd ~/transformers/
git pull
```

El entorno de Python que creaste para la instalaci√≥n de ü§ó Transformers encontrar√° la versi√≥n `master` en la siguiente ejecuci√≥n.

## Instalaci√≥n con conda

Puedes instalar ü§ó Transformers desde el canal de conda `conda-forge` con el siguiente comando:

```bash
conda install conda-forge::transformers
```

## Configuraci√≥n de Cach√©

Los modelos preentrenados se descargan y almacenan en cach√© localmente en: `~/.cache/huggingface/transformers/`. Este es el directorio predeterminado proporcionado por la variable de entorno de shell `TRANSFORMERS_CACHE`. En Windows, el directorio predeterminado es dado por `C:\Users\username\.cache\huggingface\transformers`. Puedes cambiar las variables de entorno de shell que se muestran a continuaci√≥n, en orden de prioridad, para especificar un directorio de cach√© diferente:

1. Variable de entorno del shell (por defecto): `TRANSFORMERS_CACHE`.
2. Variable de entorno del shell:`HF_HOME` + `transformers/`.
3. Variable de entorno del shell: `XDG_CACHE_HOME` + `/huggingface/transformers`.

<Tip>

ü§ó Transformers usar√° las variables de entorno de shell `PYTORCH_TRANSFORMERS_CACHE` o `PYTORCH_PRETRAINED_BERT_CACHE` si viene de una iteraci√≥n anterior de la biblioteca y ha configurado esas variables de entorno, a menos que especifiques la variable de entorno de shell `TRANSFORMERS_CACHE`.

</Tip>


## Modo Offline

ü§ó Transformers puede ejecutarse en un entorno con firewall o fuera de l√≠nea (offline) usando solo archivos locales. Configura la variable de entorno `HF_HUB_OFFLINE=1` para habilitar este comportamiento.

<Tip>

Puedes a√±adir [ü§ó Datasets](https://huggingface.co/docs/datasets/) al flujo de entrenamiento offline declarando la variable de entorno  `HF_DATASETS_OFFLINE=1`.

</Tip>

Por ejemplo, normalmente ejecutar√≠as un programa en una red normal con firewall para instancias externas con el siguiente comando:

```bash
python examples/pytorch/translation/run_translation.py --model_name_or_path google-t5/t5-small --dataset_name wmt16 --dataset_config ro-en ...
```

Ejecuta este mismo programa en una instancia offline con el siguiente comando:

```bash
HF_DATASETS_OFFLINE=1 HF_HUB_OFFLINE=1 \
python examples/pytorch/translation/run_translation.py --model_name_or_path google-t5/t5-small --dataset_name wmt16 --dataset_config ro-en ...
```

El script ahora deber√≠a ejecutarse sin bloquearse ni esperar a que se agote el tiempo de espera porque sabe que solo debe buscar archivos locales.

### Obtener modelos y tokenizers para uso offline

Otra opci√≥n para usar ü§ó Transformers offline es descargando previamente los archivos y despu√©s apuntar al path local donde se encuentren. Hay tres maneras de hacer esto:

* Descarga un archivo mediante la interfaz de usuario del [Model Hub](https://huggingface.co/models) haciendo click en el √≠cono ‚Üì.

    ![download-icon](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/download-icon.png)


* Utiliza el flujo de [`PreTrainedModel.from_pretrained`] y [`PreTrainedModel.save_pretrained`]:
    1. Descarga previamente los archivos con [`PreTrainedModel.from_pretrained`]:

    ```py
    >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

    >>> tokenizer = AutoTokenizer.from_pretrained("bigscience/T0_3B")
    >>> model = AutoModelForSeq2SeqLM.from_pretrained("bigscience/T0_3B")
    ```


    2. Guarda los archivos en un directorio espec√≠fico con [`PreTrainedModel.save_pretrained`]:

    ```py
    >>> tokenizer.save_pretrained("./your/path/bigscience_t0")
    >>> model.save_pretrained("./your/path/bigscience_t0")
    ```

    3. Cuando te encuentres offline, recarga los archivos con [`PreTrainedModel.from_pretrained`] desde el directorio especificado:

    ```py
    >>> tokenizer = AutoTokenizer.from_pretrained("./your/path/bigscience_t0")
    >>> model = AutoModel.from_pretrained("./your/path/bigscience_t0")
    ```

* Descarga de manera program√°tica los archivos con la biblioteca [huggingface_hub](https://github.com/huggingface/huggingface_hub/tree/main/src/huggingface_hub):

    1. Instala la biblioteca [huggingface_hub](https://github.com/huggingface/huggingface_hub/tree/main/src/huggingface_hub) en tu entorno virtual:

    ```bash
    python -m pip install huggingface_hub
    ```

    2. Utiliza la funci√≥n [`hf_hub_download`](https://huggingface.co/docs/hub/adding-a-library#download-files-from-the-hub) para descargar un archivo a un path espec√≠fico. Por ejemplo, el siguiente comando descarga el archivo `config.json` del modelo [T0](https://huggingface.co/bigscience/T0_3B) al path deseado:

    ```py
    >>> from huggingface_hub import hf_hub_download

    >>> hf_hub_download(repo_id="bigscience/T0_3B", filename="config.json", cache_dir="./your/path/bigscience_t0")
    ```

Una vez que el archivo se descargue y se almacene en cach√© localmente, especifica tu ruta local para cargarlo y usarlo:

```py
>>> from transformers import AutoConfig

>>> config = AutoConfig.from_pretrained("./your/path/bigscience_t0/config.json")
```

<Tip>

Para m√°s detalles sobre c√≥mo descargar archivos almacenados en el Hub consulta la secci√≥n [How to download files from the Hub](https://huggingface.co/docs/hub/how-to-downstream).

</Tip>
