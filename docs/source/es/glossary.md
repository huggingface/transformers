<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

丘멆잺 Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Glosario

Este glosario define t칠rminos generales de aprendizaje autom치tico y t칠rminos relacionados con 游뱅 Transformers para ayudarte a comprender mejor la documentaci칩n.

## A

### attention mask

La m치scara de atenci칩n es un argumento opcional utilizado al agrupar secuencias.

<Youtube id="M6adb1j2jPI"/>

Este argumento indica al modelo qu칠 tokens deben recibir atenci칩n y cu치les no.

Por ejemplo, considera estas dos secuencias:

```python
>>> from transformers import BertTokenizer

>>> tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-cased")

>>> sequence_a = "This is a short sequence."
>>> sequence_b = "This is a rather long sequence. It is at least longer than the sequence A."

>>> encoded_sequence_a = tokenizer(sequence_a)["input_ids"]
>>> encoded_sequence_b = tokenizer(sequence_b)["input_ids"]
```

Las versiones codificadas tienen longitudes diferentes:

```python
>>> len(encoded_sequence_a), len(encoded_sequence_b)
(8, 19)
```

Por lo tanto, no podemos colocarlas juntas en el mismo tensor tal cual. La primera secuencia necesita ser rellenada hasta la longitud de la segunda, o la segunda necesita ser truncada hasta la longitud de la primera.

En el primer caso, la lista de IDs se extender치 con los 칤ndices de relleno. Podemos pasar una lista al tokenizador y pedirle que realice el relleno de esta manera:

```python
>>> padded_sequences = tokenizer([sequence_a, sequence_b], padding=True)
```

Podemos ver que se han agregado ceros a la derecha de la primera oraci칩n para que tenga la misma longitud que la segunda:

```python
>>> padded_sequences["input_ids"]
[[101, 1188, 1110, 170, 1603, 4954, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1188, 1110, 170, 1897, 1263, 4954, 119, 1135, 1110, 1120, 1655, 2039, 1190, 1103, 4954, 138, 119, 102]]
```

Esto luego se puede convertir en un tensor en PyTorch o TensorFlow. La m치scara de atenci칩n es un tensor binario que indica la posici칩n de los 칤ndices de relleno para que el modelo no los tenga en cuenta. Para el [`BertTokenizer`], `1` indica un valor al que se debe prestar atenci칩n, mientras que `0` indica un valor de relleno. Esta m치scara de atenci칩n est치 en el diccionario devuelto por el tokenizador bajo la clave "attention_mask":

```python
>>> padded_sequences["attention_mask"]
[[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]
```

### autoencoding models

Consulta [modelos de codificaci칩n](#encoder-models) y [modelado de lenguaje enmascarado](#masked-language-modeling-mlm)

### autoregressive models

Consulta [modelado de lenguaje causal](#causal-language-modeling) y [modelos de decodificaci칩n](#decoder-models)

## B

### backbone

La columna vertebral, backbone en ingl칠s, es la red (embeddings y layers) que produce los estados ocultos o caracter칤sticas crudas. Normalmente, est치 conectado a una [cabecera](#head), que acepta las caracter칤sticas como entrada para hacer una predicci칩n. Por ejemplo, [`ViTModel`] es una columna vertebral sin una cabecera espec칤fica encima. Otros modelos tambi칠n pueden usar [`VitModel`] como columna vertebral, como por ejemplo [DPT](model_doc/dpt).

## C

### causal language modeling

Una tarea de preentrenamiento donde el modelo lee los textos en orden y tiene que predecir la siguiente palabra. Generalmente, se realiza leyendo toda la oraci칩n, pero utilizando una m치scara dentro del modelo para ocultar los tokens futuros en un cierto paso de tiempo.

### channel

Las im치genes a color est치n compuestas por alguna combinaci칩n de valores en tres canales: rojo, verde y azul (RGB), y las im치genes en escala de grises solo tienen un canal. En 游뱅 Transformers, el canal puede ser la primera o 칰ltima dimensi칩n del tensor de una imagen: [`n_channels`, `height`, `width`] o [`height`, `width`, `n_channels`].

### connectionist temporal classification (CTC)

Un algoritmo que permite que un modelo aprenda sin saber exactamente c칩mo est치n alineadas la entrada y la salida; CTC calcula la distribuci칩n de todas las salidas posibles para una entrada dada y elige la salida m치s probable de ella. CTC se utiliza com칰nmente en tareas de reconocimiento de voz porque el habla no siempre se alinea perfectamente con la transcripci칩n debido a diversas razones, como las diferentes velocidades de habla de los oradores.

### convolution

Un tipo de capa en una red neuronal donde la matriz de entrada se multiplica elemento por elemento por una matriz m치s peque침a (n칰cleo o filtro) y los valores se suman en una nueva matriz. Esto se conoce como una operaci칩n de convoluci칩n que se repite sobre toda la matriz de entrada. Cada operaci칩n se aplica a un segmento diferente de la matriz de entrada. Las redes neuronales convolucionales (CNN) se utilizan com칰nmente en visi칩n por computadora.

## D

### DataParallel (DP)

T칠cnica de paralelismo para entrenamiento en m칰ltiples GPUs donde se replica la misma configuraci칩n varias veces, con cada instancia recibiendo una porci칩n de datos 칰nica. El procesamiento se realiza en paralelo y todas las configuraciones se sincronizan al final de cada paso de entrenamiento.

Obt칠n m치s informaci칩n sobre c칩mo funciona el DataParallel [aqu칤](perf_train_gpu_many#dataparallel-vs-distributeddataparallel).

### decoder input IDs

Esta entrada es espec칤fica para modelos codificador-decodificador y contiene los IDs de entrada que se enviar치n al decodificador. Estas entradas deben usarse para tareas de secuencia a secuencia, como traducci칩n o resumen, y generalmente se construyen de una manera espec칤fica para cada modelo.

La mayor칤a de los modelos codificador-decodificador (BART, T5) crean sus `decoder_input_ids` por s칤 mismos a partir de las `labels`. En tales modelos, pasar las `labels` es la forma preferida de manejar el entrenamiento.

Consulta la documentaci칩n de cada modelo para ver c칩mo manejan estos IDs de entrada para el entrenamiento de secuencia a secuencia.

### decoder models

Tambi칠n conocidos como modelos autorregresivos, los modelos decodificadores involucran una tarea de preentrenamiento (llamada modelado de lenguaje causal) donde el modelo lee los textos en orden y tiene que predecir la siguiente palabra. Generalmente, se realiza leyendo la oraci칩n completa con una m치scara para ocultar los tokens futuros en un cierto paso de tiempo.

<Youtube id="d_ixlCubqQw"/>

### deep learning (DL)

Algoritmos de aprendizaje autom치tico que utilizan redes neuronales con varias capas.

## E

### encoder models

Tambi칠n conocidos como modelos de codificaci칩n autom치tica (autoencoding models), los modelos codificadores toman una entrada (como texto o im치genes) y las transforman en una representaci칩n num칠rica condensada llamada embedding. A menudo, los modelos codificadores se entrenan previamente utilizando t칠cnicas como el [modelado de lenguaje enmascarado](#masked-language-modeling-mlm), que enmascara partes de la secuencia de entrada y obliga al modelo a crear representaciones m치s significativas.

<Youtube id="H39Z_720T5s"/>

## F

### feature extraction

El proceso de seleccionar y transformar datos crudos en un conjunto de caracter칤sticas m치s informativas y 칰tiles para algoritmos de aprendizaje autom치tico. Algunos ejemplos de extracci칩n de caracter칤sticas incluyen transformar texto crudo en embeddings de palabras y extraer caracter칤sticas importantes como bordes o formas de datos de im치genes/videos.

### feed forward chunking

En cada bloque de atenci칩n residual en los transformadores, la capa de autoatenci칩n suele ir seguida de 2 capas de avance. El tama침o de embedding intermedio de las capas de avance suele ser mayor que el tama침o oculto del modelo (por ejemplo, para `google-bert/bert-base-uncased`).

Para una entrada de tama침o `[batch_size, sequence_length]`, la memoria requerida para almacenar los embeddings intermedios de avance `[batch_size, sequence_length, config.intermediate_size]` puede representar una gran fracci칩n del uso de memoria. Los autores de [Reformer: The Efficient Transformer](https://huggingface.co/papers/2001.04451) observaron que, dado que el c치lculo es independiente de la dimensi칩n `sequence_length`, es matem치ticamente equivalente calcular los embeddings de salida de ambas capas de avance  `[batch_size, config.hidden_size]_0, ..., [batch_size, config.hidden_size]_n` individualmente y concatenarlos despu칠s a `[batch_size, sequence_length, config.hidden_size]` con `n = sequence_length`, lo que intercambia el aumento del tiempo de c치lculo por una reducci칩n en el uso de memoria, pero produce un resultado matem치ticamente **equivalente**.

Para modelos que utilizan la funci칩n [`apply_chunking_to_forward`], el `chunk_size` define el n칰mero de embeddings de salida que se calculan en paralelo y, por lo tanto, define el equilibrio entre la complejidad de memoria y tiempo. Si `chunk_size` se establece en 0, no se realiza ninguna fragmentaci칩n de avance.

### finetuned models

El ajuste fino es una forma de transferencia de aprendizaje que implica tomar un modelo entrenado previamente, congelar sus pesos y reemplazar la capa de salida con una nueva [cabecera de modelo](#head) reci칠n a침adida. La cabecera del modelo se entrena en tu conjunto de datos objetivo.

Consulta el tutorial [Ajustar finamente un modelo pre-entrenado](https://huggingface.co/docs/transformers/training) para obtener m치s detalles y aprende c칩mo ajustar finamente modelos con 游뱅 Transformers.

## H

### head

La cabecera del modelo se refiere a la 칰ltima capa de una red neuronal que acepta los estados ocultos crudos y los proyecta en una dimensi칩n diferente. Hay una cabecera de modelo diferente para cada tarea. Por ejemplo:

  * [`GPT2ForSequenceClassification`] es una cabecera de clasificaci칩n de secuencias, es decir, una capa lineal, encima del modelo base [`GPT2Model`].
  * [`ViTForImageClassification`] es una cabecera de clasificaci칩n de im치genes, es decir, una capa lineal encima del estado oculto final del token `CLS`, encima del modelo base [`ViTModel`].
  * [`Wav2Vec2ForCTC`] es una cabecera de modelado de lenguaje con [CTC](#connectionist-temporal-classification-ctc) encima del modelo base [`Wav2Vec2Model`].

## I

### image patch

Los modelos de Transformers basados en visi칩n dividen una imagen en parches m치s peque침os que se incorporan linealmente y luego se pasan como una secuencia al modelo. Puedes encontrar el `patch_size` (o resoluci칩n del modelo) en su configuraci칩n.

### inference

La inferencia es el proceso de evaluar un modelo en nuevos datos despu칠s de completar el entrenamiento. Consulta el tutorial [Pipeline for inference](https://huggingface.co/docs/transformers/pipeline_tutorial) para aprender c칩mo realizar inferencias con 游뱅 Transformers.

### input IDs

Los IDs de entrada a menudo son los 칰nicos par치metros necesarios que se deben pasar al modelo como entrada. Son 칤ndices de tokens, representaciones num칠ricas de tokens que construyen las secuencias que se utilizar치n como entrada por el modelo.

<Youtube id="VFp38yj8h3A"/>

Cada tokenizador funciona de manera diferente, pero el mecanismo subyacente sigue siendo el mismo. Aqu칤 tienes un ejemplo utilizando el tokenizador BERT, que es un tokenizador [WordPiece](https://huggingface.co/papers/1609.08144):

```python
>>> from transformers import BertTokenizer

>>> tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-cased")

>>> sequence = "A Titan RTX has 24GB of VRAM"
```

El tokenizador se encarga de dividir la secuencia en tokens disponibles en el vocabulario del tokenizador.

```python
>>> tokenized_sequence = tokenizer.tokenize(sequence)
```

Los tokens son palabras o sub palabras. Por ejemplo, "VRAM" no estaba en el vocabulario del modelo, as칤 que se dividi칩
en "V", "RA" y "M". Para indicar que estos tokens no son palabras separadas sino partes de la misma palabra, se a침ade un prefijo de doble almohadilla para "RA" y "M":

```python
>>> print(tokenized_sequence)
['A', 'Titan', 'R', '##T', '##X', 'has', '24', '##GB', 'of', 'V', '##RA', '##M']
```

Estos tokens luego se pueden convertir en IDs que son comprensibles por el modelo. Esto se puede hacer alimentando directamente la oraci칩n al tokenizador, que aprovecha la implementaci칩n en Rust de [游뱅 Tokenizers](https://github.com/huggingface/tokenizers) para obtener un rendimiento 칩ptimo.

```python
>>> inputs = tokenizer(sequence)
```

El tokenizador devuelve un diccionario con todos los argumentos necesarios para que su modelo correspondiente funcione correctamente. Los 칤ndices de los tokens est치n bajo la clave `input_ids`:

```python
>>> encoded_sequence = inputs["input_ids"]
>>> print(encoded_sequence)
[101, 138, 18696, 155, 1942, 3190, 1144, 1572, 13745, 1104, 159, 9664, 2107, 102]
```

Ten en cuenta que el tokenizador a침ade autom치ticamente "tokens especiales" (si el modelo asociado depende de ellos), que son IDs especiales que el modelo utiliza en ocasiones.

Si descodificamos la secuencia anterior de IDs,

```python
>>> decoded_sequence = tokenizer.decode(encoded_sequence)
```

Veremos

```python
>>> print(decoded_sequence)
[CLS] A Titan RTX has 24GB of VRAM [SEP]
```

Porque esta es la forma en que un [`BertModel`] espera sus entradas.

## L

### labels

Las etiquetas son un argumento opcional que se puede pasar para que el modelo calcule la p칠rdida por s칤 mismo. Estas etiquetas deber칤an ser la predicci칩n esperada del modelo: usar치 la p칠rdida est치ndar para calcular la p칠rdida entre sus
predicciones y el valor esperado (la etiqueta).

Estas etiquetas son diferentes seg칰n la cabecera del modelo, por ejemplo:

- Para modelos de clasificaci칩n de secuencias ([`BertForSequenceClassification`]), el modelo espera un tensor de dimensi칩n
  `(batch_size)` con cada valor del lote correspondiente a la etiqueta esperada de toda la secuencia.
- Para modelos de clasificaci칩n de tokens ([`BertForTokenClassification`]), el modelo espera un tensor de dimensi칩n
  `(batch_size, seq_length)` con cada valor correspondiente a la etiqueta esperada de cada token individual.
- Para el modelado de lenguaje enmascarado ([`BertForMaskedLM`]), el modelo espera un tensor de dimensi칩n `(batch_size, seq_length)` con cada valor correspondiente a la etiqueta esperada de cada token individual: las etiquetas son el ID del token enmascarado y los valores deben ignorarse para el resto (generalmente -100).
- Para tareas de secuencia a secuencia ([`BartForConditionalGeneration`], [`MBartForConditionalGeneration`]), el modelo
  espera un tensor de dimensi칩n `(batch_size, tgt_seq_length)` con cada valor correspondiente a las secuencias objetivo asociadas con cada secuencia de entrada. Durante el entrenamiento, tanto BART como T5 generar치n internamente los `decoder_input_ids` y las m치scaras de atenci칩n del decodificador. Por lo general, no es necesario suministrarlos. Esto no se aplica a los modelos que aprovechan el marco codificador-decodificador.
- Para modelos de clasificaci칩n de im치genes ([`ViTForImageClassification`]), el modelo espera un tensor de dimensi칩n
  `(batch_size)` con cada valor del lote correspondiente a la etiqueta esperada de cada imagen individual.
- Para modelos de segmentaci칩n sem치ntica ([`SegformerForSemanticSegmentation`]), el modelo espera un tensor de dimensi칩n
  `(batch_size, height, width)` con cada valor del lote correspondiente a la etiqueta esperada de cada p칤xel individual.
- Para modelos de detecci칩n de objetos ([`DetrForObjectDetection`]), el modelo espera una lista de diccionarios con claves `class_labels` y `boxes` donde cada valor del lote corresponde a la etiqueta esperada y el n칰mero de cajas delimitadoras de cada imagen individual.
- Para modelos de reconocimiento autom치tico de voz ([`Wav2Vec2ForCTC`]), el modelo espera un tensor de dimensi칩n `(batch_size, target_length)` con cada valor correspondiente a la etiqueta esperada de cada token individual.
  
<Tip>

Las etiquetas de cada modelo pueden ser diferentes, as칤 que aseg칰rate siempre de revisar la documentaci칩n de cada modelo para obtener m치s informaci칩n sobre sus etiquetas espec칤ficas.

</Tip>

Los modelos base ([`BertModel`]) no aceptan etiquetas, ya que estos son los modelos base de transformadores, que simplemente generan caracter칤sticas.

### large language models (LLM)

Un t칠rmino gen칠rico que se refiere a modelos de lenguaje de transformadores (GPT-3, BLOOM, OPT) que fueron entrenados con una gran cantidad de datos. Estos modelos tambi칠n tienden a tener un gran n칰mero de par치metros que se pueden aprender (por ejemplo, 175 mil millones para GPT-3).

## M

### masked language modeling (MLM)

Una tarea de preentrenamiento en la que el modelo ve una versi칩n corrupta de los textos, generalmente hecha
al enmascarar algunos tokens al azar, y tiene que predecir el texto original.

### multimodal

Una tarea que combina textos con otro tipo de entradas (por ejemplo: im치genes).

## N

### Natural language generation (NLG)

Todas las tareas relacionadas con la generaci칩n de texto (por ejemplo: [Escribe con Transformers](https://transformer.huggingface.co/) o traducci칩n).

### Natural language processing (NLP)

Una forma gen칠rica de decir "trabajar con textos".

### Natural language understanding (NLU)

Todas las tareas relacionadas con entender lo que hay en un texto (por ejemplo: clasificar el
texto completo o palabras individuales).

## P

### Pipeline

Un pipeline en 游뱅 Transformers es una abstracci칩n que se refiere a una serie de pasos que se ejecutan en un orden espec칤fico para preprocesar y transformar datos y devolver una predicci칩n de un modelo. Algunas etapas de ejemplo que se encuentran en un pipeline pueden ser el preprocesamiento de datos, la extracci칩n de caracter칤sticas y la normalizaci칩n.

Para obtener m치s detalles, consulta [Pipelines para inferencia](https://huggingface.co/docs/transformers/pipeline_tutorial).

### PipelineParallel (PP)

T칠cnica de paralelismo en la que el modelo se divide verticalmente (a nivel de capa) en varios GPU, de modo que solo una o varias capas del modelo se colocan en un solo GPU. Cada GPU procesa en paralelo diferentes etapas del pipeline y trabaja en un peque침o fragmento del lote. Obt칠n m치s informaci칩n sobre c칩mo funciona PipelineParallel [aqu칤](perf_train_gpu_many#from-naive-model-parallelism-to-pipeline-parallelism).

### pixel values

Un tensor de las representaciones num칠ricas de una imagen que se pasa a un modelo. Los valores de p칤xeles tienen una forma de [`batch_size`, `num_channels`, `height`, `width`], y se generan a partir de un procesador de im치genes.

### pooling

Una operaci칩n que reduce una matriz a una matriz m치s peque침a, ya sea tomando el m치ximo o el promedio de la dimensi칩n (o dimensiones) agrupada(s). Las capas de agrupaci칩n se encuentran com칰nmente entre capas convolucionales para reducir la representaci칩n de caracter칤sticas.

### position IDs

A diferencia de las RNN que tienen la posici칩n de cada token incrustada en ellas, los transformers no son conscientes de la posici칩n de cada token. Por lo tanto, se utilizan los IDs de posici칩n (`position_ids`) para que el modelo identifique la posici칩n de cada token en la lista de tokens.

Son un par치metro opcional. Si no se pasan `position_ids` al modelo, los IDs se crean autom치ticamente como embeddings de posici칩n absolutas.

Los embeddings de posici칩n absolutas se seleccionan en el rango `[0, config.max_position_embeddings - 1]`. Algunos modelos utilizan otros tipos de embeddings de posici칩n, como embeddings de posici칩n sinusoidales o embeddings de posici칩n relativas.

### preprocessing

La tarea de preparar datos crudos en un formato que pueda ser f치cilmente consumido por modelos de aprendizaje autom치tico. Por ejemplo, el texto se preprocesa t칤picamente mediante la tokenizaci칩n. Para tener una mejor idea de c칩mo es el preprocesamiento para otros tipos de entrada, consulta el tutorial [Pre-procesar](https://huggingface.co/docs/transformers/preprocessing).

### pretrained model

Un modelo que ha sido pre-entrenado en algunos datos (por ejemplo, toda Wikipedia). Los m칠todos de preentrenamiento involucran un objetivo auto-supervisado, que puede ser leer el texto e intentar predecir la siguiente palabra (ver [modelado de lenguaje causal](#causal-language-modeling)) o enmascarar algunas palabras e intentar predecirlas (ver [modelado de lenguaje enmascarado](#masked-language-modeling-mlm)).

Los modelos de habla y visi칩n tienen sus propios objetivos de pre-entrenamiento. Por ejemplo, Wav2Vec2 es un modelo de habla pre-entrenado en una tarea contrastiva que requiere que el modelo identifique la representaci칩n de habla "verdadera" de un conjunto de representaciones de habla "falsas". Por otro lado, BEiT es un modelo de visi칩n pre-entrenado en una tarea de modelado de im치genes enmascaradas que enmascara algunos de los parches de la imagen y requiere que el modelo prediga los parches enmascarados (similar al objetivo de modelado de lenguaje enmascarado).

## R

### recurrent neural network (RNN)

Un tipo de modelo que utiliza un bucle sobre una capa para procesar textos.

### representation learning

Un subcampo del aprendizaje autom치tico que se centra en aprender representaciones significativas de datos en bruto. Algunos ejemplos de t칠cnicas de aprendizaje de representaciones incluyen embeddings de palabras, auto-encoders y Redes Generativas Adversarias (Generative Adversarial Networks, GANs).

## S

### sampling rate

Una medida en hercios del n칰mero de muestras (la se침al de audio) tomadas por segundo. La tasa de muestreo es el resultado de aproximar una se침al continua como el habla.

### self-attention

Cada elemento de la entrada averigua a cu치les otros elementos de la entrada debe prestar atenci칩n.

### self-supervised learning

Una categor칤a de t칠cnicas de aprendizaje autom치tico en la que un modelo crea su propio objetivo de aprendizaje a partir de datos no etiquetados. Difiere del [aprendizaje no supervisado](#unsupervised-learning) y del [aprendizaje supervisado](#supervised-learning) en que el proceso de aprendizaje est치 supervisado, pero no expl칤citamente por el usuario.

Un ejemplo de aprendizaje auto-supervisado es el [modelado de lenguaje enmascarado](#masked-language-modeling-mlm), donde un modelo recibe oraciones con una proporci칩n de sus tokens eliminados y aprende a predecir los tokens faltantes.

### semi-supervised learning

Una amplia categor칤a de t칠cnicas de entrenamiento de aprendizaje autom치tico que aprovecha una peque침a cantidad de datos etiquetados con una mayor cantidad de datos no etiquetados para mejorar la precisi칩n de un modelo, a diferencia del [aprendizaje supervisado](#supervised-learning) y del [aprendizaje no supervisado](#unsupervised-learning).

Un ejemplo de un enfoque de aprendizaje semi-supervisado es "auto-entrenamiento", en el que un modelo se entrena con datos etiquetados y luego se utiliza para hacer predicciones sobre los datos no etiquetados. La porci칩n de datos no etiquetados que el modelo predice con mayor confianza se agrega al conjunto de datos etiquetados y se utiliza para volver a entrenar el modelo.

### sequence-to-sequence (seq2seq)

Modelos que generan una nueva secuencia a partir de una entrada, como modelos de traducci칩n o modelos de resumen (como
[Bart](model_doc/bart) o [T5](model_doc/t5)).

### Sharded DDP

Otro nombre para el concepto fundamental de [ZeRO](#zero-redundancy-optimizer-zero) utilizado por varias otras implementaciones de ZeRO.

### stride

En [convoluci칩n](#convolution) o [agrupaci칩n](#pooling), el paso (stride) se refiere a la distancia que recorre el n칰cleo sobre una matriz. Un paso de 1 significa que el n칰cleo se mueve un p칤xel a la vez, y un paso de 2 significa que el n칰cleo se mueve dos p칤xeles a la vez.

### supervised learning

Una forma de entrenamiento de modelos que utiliza directamente datos etiquetados para corregir y dirigir el rendimiento del modelo. Los datos se introducen en el modelo en entrenamiento, y sus predicciones se comparan con las etiquetas conocidas. El modelo actualiza sus pesos en funci칩n de cu치n incorrectas fueron sus predicciones, y el proceso se repite para optimizar el rendimiento del modelo.

## T

### Tensor Parallelism (TP)

T칠cnica de paralelismo para entrenamiento en m칰ltiples GPU en la que cada tensor se divide en m칰ltiples fragmentos, de modo que en lugar de tener todo el tensor en una sola GPU, cada fragmento del tensor reside en su GPU designada. Los fragmentos se procesan por separado y en paralelo en diferentes GPU y los resultados se sincronizan al final del paso de procesamiento.Esto es lo que a veces se llama paralelismo horizontal, ya que la divisi칩n ocurre a nivel horizontal.
Obt칠n m치s informaci칩n sobre el Paralelismo de Tensores [aqu칤](perf_train_gpu_many#tensor-parallelism).

### token

Parte de una oraci칩n, generalmente una palabra, pero tambi칠n puede ser una sub-palabra (las palabras no comunes a menudo se dividen en sub-palabras) o un s칤mbolo de puntuaci칩n.

### token Type IDs

Algunos modelos tienen como objetivo realizar clasificaci칩n en pares de oraciones o responder preguntas.

<Youtube id="0u3ioSwev3s"/>

Estos requieren que dos secuencias diferentes se unan en una 칰nica entrada "input_ids", lo cual generalmente se realiza con
la ayuda de tokens especiales, como el token de clasificaci칩n (`[CLS]`) y el token separador (`[SEP]`). Por ejemplo, el modelo BERT construye sus dos secuencias de entrada de la siguiente manera:

```python
>>> # [CLS] SEQUENCE_A [SEP] SEQUENCE_B [SEP]
```

Podemos utilizar nuestro tokenizador para generar autom치ticamente una oraci칩n de este tipo al pasar las dos secuencias a `tokenizer` como dos argumentos (y no como una lista, como antes) de la siguiente manera:

```python
>>> from transformers import BertTokenizer

>>> tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-cased")
>>> sequence_a = "HuggingFace is based in NYC"
>>> sequence_b = "Where is HuggingFace based?"

>>> encoded_dict = tokenizer(sequence_a, sequence_b)
>>> decoded = tokenizer.decode(encoded_dict["input_ids"])
```

Que devolver치:

```python
>>> print(decoded)
[CLS] HuggingFace is based in NYC [SEP] Where is HuggingFace based? [SEP]
```

Esto es suficiente para que algunos modelos comprendan d칩nde termina una secuencia y comienza otra. Sin embargo, otros modelos, como BERT, tambi칠n utilizan identificadores de tipo de token (tambi칠n llamados identificadores de segmento). Se representan como una m치scara binaria que identifica los dos tipos de secuencia en el modelo.

El tokenizador devuelve esta m치scara como la entrada "token_type_ids":

```python
>>> encoded_dict["token_type_ids"]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

La primera secuencia, el "contexto" utilizado para la pregunta, tiene todos sus tokens representados por un `0`, mientras que la segunda secuencia, correspondiente a la "pregunta", tiene todos sus tokens representados por un `1`.

Algunos modelos, como [`XLNetModel`], utilizan un token adicional representado por un `2`.

### transfer learning

Una t칠cnica que implica tomar un modelo pre-entrenado y adaptarlo a un conjunto de datos espec칤fico para tu tarea. En lugar de entrenar un modelo desde cero, puedes aprovechar el conocimiento obtenido de un modelo existente como punto de partida. Esto acelera el proceso de aprendizaje y reduce la cantidad de datos de entrenamiento necesarios.

### transformer

Arquitectura de modelo de aprendizaje profundo basada en auto-atenci칩n (Self-attention).

## U

### unsupervised learning

Una forma de entrenamiento de modelos en la que los datos proporcionados al modelo no est치n etiquetados. Las t칠cnicas de aprendizaje no supervisado aprovechan la informaci칩n estad칤stica de la distribuci칩n de datos para encontrar patrones 칰tiles para la tarea en cuesti칩n.

## Z

### Zero Redundancy Optimizer (ZeRO)

T칠cnica de paralelismo que realiza la fragmentaci칩n de los tensores de manera algo similar a [TensorParallel](#tensor-parallelism-tp), excepto que todo el tensor se reconstruye a tiempo para una computaci칩n hacia adelante o hacia atr치s, por lo tanto, el modelo no necesita ser modificado. Este m칠todo tambi칠n admite diversas t칠cnicas de descarga para compensar la memoria limitada de la GPU. Obt칠n m치s informaci칩n sobre ZeRO [aqu칤](perf_train_gpu_many#zero-data-parallelism).