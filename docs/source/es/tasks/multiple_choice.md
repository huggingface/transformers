<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# SelecciÃ³n mÃºltiple

La tarea de selecciÃ³n mÃºltiple es parecida a la de responder preguntas, con la excepciÃ³n de que se dan varias opciones de respuesta junto con el contexto. El modelo se entrena para escoger la respuesta correcta
entre varias opciones a partir del contexto dado.

Esta guÃ­a te mostrarÃ¡ como hacerle fine-tuning a [BERT](https://huggingface.co/google-bert/bert-base-uncased) en la configuraciÃ³n `regular` del dataset [SWAG](https://huggingface.co/datasets/swag), de forma
que seleccione la mejor respuesta a partir de varias opciones y algÃºn contexto.

## Cargar el dataset SWAG

Carga el dataset SWAG con la biblioteca ğŸ¤— Datasets:

```py
>>> from datasets import load_dataset

>>> swag = load_dataset("swag", "regular")
```

Ahora, Ã©chale un vistazo a un ejemplo del dataset:

```py
>>> swag["train"][0]
{'ending0': 'passes by walking down the street playing their instruments.',
 'ending1': 'has heard approaching them.',
 'ending2': "arrives and they're outside dancing and asleep.",
 'ending3': 'turns the lead singer watches the performance.',
 'fold-ind': '3416',
 'gold-source': 'gold',
 'label': 0,
 'sent1': 'Members of the procession walk down the street holding small horn brass instruments.',
 'sent2': 'A drum line',
 'startphrase': 'Members of the procession walk down the street holding small horn brass instruments. A drum line',
 'video-id': 'anetv_jkn6uvmqwh4'}
```

Los campos `sent1` y `sent2` muestran cÃ³mo comienza una oraciÃ³n, y cada campo `ending` indica cÃ³mo podrÃ­a terminar. Dado el comienzo de la oraciÃ³n, el modelo debe escoger el final de oraciÃ³n correcto indicado por el campo `label`.

## Preprocesmaiento

Carga el tokenizer de BERT para procesar el comienzo de cada oraciÃ³n y los cuatro finales posibles:

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
```

La funciÃ³n de preprocesmaiento debe hacer lo siguiente:

1. Hacer cuatro copias del campo `sent1` de forma que se pueda combinar cada una con el campo `sent2` para recrear la forma en que empieza la oraciÃ³n.
2. Combinar `sent2` con cada uno de los cuatro finales de oraciÃ³n posibles.
3. Aplanar las dos listas para que puedas tokenizarlas, y luego des-aplanarlas para que cada ejemplo tenga los campos `input_ids`, `attention_mask` y `labels` correspondientes.

```py
>>> ending_names = ["ending0", "ending1", "ending2", "ending3"]


>>> def preprocess_function(examples):
...     first_sentences = [[context] * 4 for context in examples["sent1"]]
...     question_headers = examples["sent2"]
...     second_sentences = [
...         [f"{header} {examples[end][i]}" for end in ending_names] for i, header in enumerate(question_headers)
...     ]

...     first_sentences = sum(first_sentences, [])
...     second_sentences = sum(second_sentences, [])

...     tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)
...     return {k: [v[i : i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}
```

Usa la funciÃ³n [`~datasets.Dataset.map`] de ğŸ¤— Datasets para aplicarle la funciÃ³n de preprocesamiento al dataset entero. Puedes acelerar la funciÃ³n `map` haciendo `batched=True` para procesar varios elementos del dataset a la vez.

```py
tokenized_swag = swag.map(preprocess_function, batched=True)
```

Para crear un lote de ejemplos para selecciÃ³n mÃºltiple, este tambiÃ©n le *aÃ±adirÃ¡ relleno de manera dinÃ¡mica* a tu texto y a las etiquetas para que tengan la longitud del elemento mÃ¡s largo en su lote, de forma que tengan una longitud uniforme. Aunque es posible rellenar el texto en la funciÃ³n `tokenizer` haciendo
`padding=True`, el rellenado dinÃ¡mico es mÃ¡s eficiente.

El [`DataCollatorForMultipleChoice`] aplanarÃ¡ todas las entradas del modelo, les aplicarÃ¡ relleno y luego des-aplanarÃ¡ los resultados.
```py
>>> from transformers import DataCollatorForMultipleChoice
>>> collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)
```

## Entrenamiento

Carga el modelo BERT con [`AutoModelForMultipleChoice`]:

```py
>>> from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer

>>> model = AutoModelForMultipleChoice.from_pretrained("google-bert/bert-base-uncased")
```

> [!TIP]
> Para familiarizarte con el fine-tuning con [`Trainer`], Â¡mira el tutorial bÃ¡sico [aquÃ­](../training#finetune-with-trainer)!

En este punto, solo quedan tres pasos:

1. Definir tus hiperparÃ¡metros de entrenamiento en [`TrainingArguments`].
2. Pasarle los argumentos del entrenamiento al [`Trainer`] jnto con el modelo, el dataset, el tokenizer y el collator de datos.
3. Invocar el mÃ©todo [`~Trainer.train`] para realizar el fine-tuning del modelo.

```py
>>> training_args = TrainingArguments(
...     output_dir="./results",
...     eval_strategy="epoch",
...     learning_rate=5e-5,
...     per_device_train_batch_size=16,
...     per_device_eval_batch_size=16,
...     num_train_epochs=3,
...     weight_decay=0.01,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_swag["train"],
...     eval_dataset=tokenized_swag["validation"],
...     processing_class=tokenizer,
...     data_collator=collator,
... )

>>> trainer.train()
```
