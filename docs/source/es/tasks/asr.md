<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Reconocimiento autom√°tico del habla

<Youtube id="TksaY_FDgnk"/>

El reconocimiento autom√°tico del habla (ASR, por sus siglas en ingl√©s) convierte una se√±al de habla en texto y mapea una secuencia de entradas de audio en salidas en forma de texto. Los asistentes virtuales como Siri y Alexa usan modelos de ASR para ayudar a sus usuarios todos los d√≠as. De igual forma, hay muchas otras aplicaciones, como la transcripci√≥n de contenidos en vivo y la toma autom√°tica de notas durante reuniones.

En esta gu√≠a te mostraremos como:

1. Hacer fine-tuning al modelo [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base) con el dataset [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) para transcribir audio a texto.
2. Usar tu modelo ajustado para tareas de inferencia.

<Tip>

Revisa la [p√°gina de la tarea](https://huggingface.co/tasks/automatic-speech-recognition) de reconocimiento autom√°tico del habla para acceder a m√°s informaci√≥n sobre los modelos, datasets y m√©tricas asociados.

</Tip>

Antes de comenzar, aseg√∫rate de haber instalado todas las librer√≠as necesarias:

```bash
pip install transformers datasets evaluate jiwer
```

Te aconsejamos iniciar sesi√≥n con tu cuenta de Hugging Face para que puedas subir tu modelo y comartirlo con la comunidad. Cuando te sea solicitado, ingresa tu token para iniciar sesi√≥n:

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## Cargar el dataset MInDS-14

Comencemos cargando un subconjunto m√°s peque√±o del dataset [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) desde la biblioteca ü§ó Datasets. De esta forma, tendr√°s la oportunidad de experimentar y asegurarte de que todo funcione antes de invertir m√°s tiempo entrenando con el dataset entero.

```py
>>> from datasets import load_dataset, Audio

>>> minds = load_dataset("PolyAI/minds14", name="en-US", split="train[:100]")
```
Divide la partici√≥n `train` (entrenamiento) en una partici√≥n de entrenamiento y una de prueba usando el m√©todo [`~Dataset.train_test_split`]:

```py
>>> minds = minds.train_test_split(test_size=0.2)
```

Ahora √©chale un vistazo al dataset:

```py
>>> minds
DatasetDict({
    train: Dataset({
        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],
        num_rows: 16
    })
    test: Dataset({
        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],
        num_rows: 4
    })
})
```

Aunque el dataset contiene mucha informaci√≥n √∫til, como los campos `lang_id` (identificador del lenguaje) y `english_transcription` (transcripci√≥n al ingl√©s), en esta gu√≠a nos enfocaremos en los campos `audio` y `transcription`. Puedes quitar las otras columnas con el m√©todo [`~datasets.Dataset.remove_columns`]:

```py
>>> minds = minds.remove_columns(["english_transcription", "intent_class", "lang_id"])
```

Vuelve a echarle un vistazo al ejemplo:

```py
>>> minds["train"][0]
{'audio': {'array': array([-0.00024414,  0.        ,  0.        , ...,  0.00024414,
          0.00024414,  0.00024414], dtype=float32),
  'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav',
  'sampling_rate': 8000},
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav',
 'transcription': "hi I'm trying to use the banking app on my phone and currently my checking and savings account balance is not refreshing"}
```

Hay dos campos:

- `audio`: un `array` (arreglo) unidimensional de la se√±al de habla que debe ser invocado para cargar y re-muestrear el archivo de audio.
- `transcription`: el texto objetivo.

## Preprocesamiento

El siguiente paso es cargar un procesador Wav2Vec2 para procesar la se√±al de audio:

```py
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base")
```
El dataset MInDS-14 tiene una tasa de muestreo de 8000kHz (puedes encontrar esta informaci√≥n en su [tarjeta de dataset](https://huggingface.co/datasets/PolyAI/minds14)), lo que significa que tendr√°s que re-muestrear el dataset a 16000kHz para poder usar el modelo Wav2Vec2 pre-entrenado:

```py
>>> minds = minds.cast_column("audio", Audio(sampling_rate=16_000))
>>> minds["train"][0]
{'audio': {'array': array([-2.38064706e-04, -1.58618059e-04, -5.43987835e-06, ...,
          2.78103951e-04,  2.38446111e-04,  1.18740834e-04], dtype=float32),
  'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav',
  'sampling_rate': 16000},
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav',
 'transcription': "hi I'm trying to use the banking app on my phone and currently my checking and savings account balance is not refreshing"}
```

Como puedes ver en el campo `transcription`, el texto contiene una mezcla de car√°cteres en may√∫sculas y en min√∫sculas. El tokenizer Wav2Vec2 fue entrenado √∫nicamente con car√°cteres en may√∫sculas, as√≠ que tendr√°s que asegurarte de que el texto se ajuste al vocabulario del tokenizer:

```py
>>> def uppercase(example):
...     return {"transcription": example["transcription"].upper()}


>>> minds = minds.map(uppercase)
```

Ahora vamos a crear una funci√≥n de preprocesamiento que:

1. Invoque la columna `audio` para cargar y re-muestrear el archivo de audio.
2. Extraiga el campo `input_values` (valores de entrada) del archivo de audio y haga la tokenizaci√≥n de la columna `transcription` con el procesador.

```py
>>> def prepare_dataset(batch):
...     audio = batch["audio"]
...     batch = processor(audio["array"], sampling_rate=audio["sampling_rate"], text=batch["transcription"])
...     batch["input_length"] = len(batch["input_values"][0])
...     return batch
```

Para aplicar la funci√≥n de preprocesamiento a todo el dataset, puedes usar la funci√≥n [`~datasets.Dataset.map`] de ü§ó Datasets. Para acelerar la funci√≥n `map` puedes incrementar el n√∫mero de procesos con el par√°metro `num_proc`. Quita las columnas que no necesites con el m√©todo [`~datasets.Dataset.remove_columns`]:

```py
>>> encoded_minds = minds.map(prepare_dataset, remove_columns=minds.column_names["train"], num_proc=4)
```

ü§ó Transformers no tiene un collator de datos para la tarea de ASR, as√≠ que tendr√°s que adaptar el [`DataCollatorWithPadding`] para crear un lote de ejemplos. El collator tambi√©n le aplicar√° padding din√°mico a tu texto y etiquetas para que tengan la longitud del elemento m√°s largo en su lote (en vez de la mayor longitud en el dataset entero), de forma que todas las muestras tengan una longitud uniforme. Aunque es posible hacerle padding a tu texto con el `tokenizer` haciendo `padding=True`, el padding din√°mico es m√°s eficiente.

A diferencia de otros collators de datos, este tiene que aplicarle un m√©todo de padding distinto a los campos `input_values` (valores de entrada) y `labels` (etiquetas):

```py
>>> import torch

>>> from dataclasses import dataclass, field
>>> from typing import Any, Dict, List, Optional, Union


>>> @dataclass
... class DataCollatorCTCWithPadding:
...     processor: AutoProcessor
...     padding: Union[bool, str] = "longest"

...     def __call__(self, features: list[dict[str, Union[list[int], torch.Tensor]]]) -> dict[str, torch.Tensor]:
...         # particiona las entradas y las etiquetas ya que tienen que tener longitudes distintas y
...         # requieren m√©todos de padding diferentes
...         input_features = [{"input_values": feature["input_values"][0]} for feature in features]
...         label_features = [{"input_ids": feature["labels"]} for feature in features]

...         batch = self.processor.pad(input_features, padding=self.padding, return_tensors="pt")

...         labels_batch = self.processor.pad(labels=label_features, padding=self.padding, return_tensors="pt")

...         # remplaza el padding con -100 para ignorar la p√©rdida de forma correcta
...         labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

...         batch["labels"] = labels

...         return batch
```

Ahora puedes instanciar tu `DataCollatorForCTCWithPadding`:

```py
>>> data_collator = DataCollatorCTCWithPadding(processor=processor, padding="longest")
```

## Evaluaci√≥n

A menudo es √∫til incluir una m√©trica durante el entrenamiento para evaluar el rendimiento de tu modelo. Puedes cargar un m√©todo de evaluaci√≥n r√°pidamente con la biblioteca ü§ó [Evaluate](https://huggingface.co/docs/evaluate/index). Para esta tarea, puedes usar la m√©trica de [tasa de error por palabra](https://huggingface.co/spaces/evaluate-metric/wer) (WER, por sus siglas en ingl√©s). Puedes ver la [gu√≠a r√°pida](https://huggingface.co/docs/evaluate/a_quick_tour) de ü§ó Evaluate para aprender m√°s acerca de c√≥mo cargar y computar una m√©trica.

```py
>>> import evaluate

>>> wer = evaluate.load("wer")
```

Ahora crea una funci√≥n que le pase tus predicciones y etiquetas a [`~evaluate.EvaluationModule.compute`] para calcular la WER:

```py
>>> import numpy as np


>>> def compute_metrics(pred):
...     pred_logits = pred.predictions
...     pred_ids = np.argmax(pred_logits, axis=-1)

...     pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id

...     pred_str = processor.batch_decode(pred_ids)
...     label_str = processor.batch_decode(pred.label_ids, group_tokens=False)

...     wer = wer.compute(predictions=pred_str, references=label_str)

...     return {"wer": wer}
```

Ahora tu funci√≥n `compute_metrics` (computar m√©tricas) est√° lista y podr√°s usarla cuando est√©s preparando tu entrenamiento.

## Entrenamiento

<frameworkcontent>
<pt>
<Tip>

Si no tienes experiencia haci√©ndole fine-tuning a un modelo con el [`Trainer`], ¬°√©chale un vistazo al tutorial b√°sico [aqu√≠](../training#train-with-pytorch-trainer)!

</Tip>

¬°Ya puedes empezar a entrenar tu modelo! Para ello, carga Wav2Vec2 con [`AutoModelForCTC`]. Especifica la reducci√≥n que quieres aplicar con el par√°metro `ctc_loss_reduction`. A menudo, es mejor usar el promedio en lugar de la sumatoria que se hace por defecto.

```py
>>> from transformers import AutoModelForCTC, TrainingArguments, Trainer

>>> model = AutoModelForCTC.from_pretrained(
...     "facebook/wav2vec2-base",
...     ctc_loss_reduction="mean",
...     pad_token_id=processor.tokenizer.pad_token_id,
... )
```
En este punto, solo quedan tres pasos:

1. Define tus hiperpar√°metros de entrenamiento en [`TrainingArguments`]. El √∫nico par√°metro obligatorio es `output_dir` (carpeta de salida), el cual especifica d√≥nde guardar tu modelo. Puedes subir este modelo al Hub haciendo `push_to_hub=True` (debes haber iniciado sesi√≥n en Hugging Face para subir tu modelo). Al final de cada √©poca, el [`Trainer`] evaluar√° la WER y guardar√° el punto de control del entrenamiento.
2. P√°sale los argumentos del entrenamiento al [`Trainer`] junto con el modelo, el dataset, el tokenizer, el collator de datos y la funci√≥n `compute_metrics`.
3. Llama el m√©todo [`~Trainer.train`] para hacerle fine-tuning a tu modelo.

```py
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_asr_mind_model",
...     per_device_train_batch_size=8,
...     gradient_accumulation_steps=2,
...     learning_rate=1e-5,
...     warmup_steps=500,
...     max_steps=2000,
...     gradient_checkpointing=True,
...     fp16=True,
...     group_by_length=True,
...     eval_strategy="steps",
...     per_device_eval_batch_size=8,
...     save_steps=1000,
...     eval_steps=1000,
...     logging_steps=25,
...     load_best_model_at_end=True,
...     metric_for_best_model="wer",
...     greater_is_better=False,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=encoded_minds["train"],
...     eval_dataset=encoded_minds["test"],
...     processing_class=processor.feature_extractor,
...     data_collator=data_collator,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

Una vez que el entrenamiento haya sido completado, comparte tu modelo en el Hub con el m√©todo [`~transformers.Trainer.push_to_hub`] para que todo el mundo pueda usar tu modelo:

```py
>>> trainer.push_to_hub()
```
</pt>
</frameworkcontent>

<Tip>

Para ver un ejemplo m√°s detallado de c√≥mo hacerle fine-tuning a un modelo para reconocimiento autom√°tico del habla, √©chale un vistazo a esta [entrada de blog](https://huggingface.co/blog/fine-tune-wav2vec2-english) para ASR en ingl√©s y a esta [entrada](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2) para ASR multiling√ºe.

</Tip>

## Inferencia

¬°Genial, ahora que le has hecho fine-tuning a un modelo, puedes usarlo para inferencia!

Carga el archivo de audio sobre el cual quieras correr la inferencia. ¬°Recuerda re-muestrar la tasa de muestreo del archivo de audio para que sea la misma del modelo si es necesario!

```py
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", "en-US", split="train")
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
>>> sampling_rate = dataset.features["audio"].sampling_rate
>>> audio_file = dataset[0]["audio"]["path"]
```

La manera m√°s simple de probar tu modelo para hacer inferencia es usarlo en un [`pipeline`]. Puedes instanciar un `pipeline` para reconocimiento autom√°tico del habla con tu modelo y pasarle tu archivo de audio:

```py
>>> from transformers import pipeline

>>> transcriber = pipeline("automatic-speech-recognition", model="stevhliu/my_awesome_asr_minds_model")
>>> transcriber(audio_file)
{'text': 'I WOUD LIKE O SET UP JOINT ACOUNT WTH Y PARTNER'}
```

<Tip>

La transcripci√≥n es decente, pero podr√≠a ser mejor. ¬°Intenta hacerle fine-tuning a tu modelo con m√°s ejemplos para obtener resultados a√∫n mejores!

</Tip>

Tambi√©n puedes replicar de forma manual los resultados del `pipeline` si lo deseas:

<frameworkcontent>
<pt>
Carga un procesador para preprocesar el archivo de audio y la transcripci√≥n y devuelve el `input` como un tensor de PyTorch:

```py
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("stevhliu/my_awesome_asr_mind_model")
>>> inputs = processor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")
```

P√°sale tus entradas al modelo y devuelve los logits:

```py
>>> from transformers import AutoModelForCTC

>>> model = AutoModelForCTC.from_pretrained("stevhliu/my_awesome_asr_mind_model")
>>> with torch.no_grad():
...     logits = model(**inputs).logits
```

Obt√©n los identificadores de los tokens con mayor probabilidad en las predicciones y usa el procesador para decodificarlos y transformarlos en texto:

```py
>>> import torch

>>> predicted_ids = torch.argmax(logits, dim=-1)
>>> transcription = processor.batch_decode(predicted_ids)
>>> transcription
['I WOUL LIKE O SET UP JOINT ACOUNT WTH Y PARTNER']
```
</pt>
</frameworkcontent>
