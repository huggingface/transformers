<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Exportar modelos  Transformers

Si necesitas implementar modelos  Transformers en entornos de producci贸n, te 
recomendamos exportarlos a un formato serializado que se pueda cargar y ejecutar 
en tiempos de ejecuci贸n y hardware especializados. En esta gu铆a, te mostraremos c贸mo 
exportar modelos  Transformers en dos formatos ampliamente utilizados: ONNX y TorchScript.

Una vez exportado, un modelo puede optimizarse para la inferencia a trav茅s de t茅cnicas 
como la cuantizaci贸n y _pruning_. Si est谩s interesado en optimizar tus modelos para
que funcionen con la m谩xima eficiencia, consulta la 
[biblioteca de  Optimum](https://github.com/huggingface/optimum).

## ONNX

El proyecto [ONNX (Open Neural Network eXchange)](http://onnx.ai) es un 
est谩ndar abierto que define un conjunto com煤n de operadores y un formato 
de archivo com煤n para representar modelos de aprendizaje profundo en una 
amplia variedad de _frameworks_, incluidos PyTorch y TensorFlow. Cuando un modelo 
se exporta al formato ONNX, estos operadores se usan para construir un 
grafo computacional (a menudo llamado _representaci贸n intermedia_) que 
representa el flujo de datos a trav茅s de la red neuronal.

Al exponer un grafo con operadores y tipos de datos estandarizados, ONNX facilita 
el cambio entre frameworks. Por ejemplo, un modelo entrenado en PyTorch se puede 
exportar a formato ONNX y luego importar en TensorFlow (y viceversa).

 Transformers proporciona un paquete llamado `transformers.onnx`, el cual permite convertir 
los checkpoints de un modelo en un grafo ONNX aprovechando los objetos de configuraci贸n. 
Estos objetos de configuraci贸n est谩n hechos a la medida de diferentes arquitecturas de modelos
y est谩n dise帽ados para ser f谩cilmente extensibles a otras arquitecturas.

Las configuraciones a la medida incluyen las siguientes arquitecturas:

<!--This table is automatically generated by `make fix-copies`, do not fill manually!-->

- ALBERT
- BART
- BEiT
- BERT
- BigBird
- BigBird-Pegasus
- Blenderbot
- BlenderbotSmall
- BLOOM
- CamemBERT
- CLIP
- CodeGen
- ConvBERT
- ConvNeXT
- Data2VecText
- Data2VecVision
- DeBERTa
- DeBERTa-v2
- DeiT
- DETR
- DistilBERT
- ELECTRA
- FlauBERT
- GPT Neo
- GPT-J
- I-BERT
- LayoutLM
- LayoutLMv3
- LeViT
- LongT5
- M2M100
- Marian
- mBART
- MobileBERT
- MobileViT
- MT5
- OpenAI GPT-2
- Perceiver
- PLBart
- ResNet
- RoBERTa
- RoFormer
- SqueezeBERT
- T5
- ViT
- XLM
- XLM-RoBERTa
- XLM-RoBERTa-XL
- YOLOS

En las pr贸ximas dos secciones, te mostraremos c贸mo:

* Exportar un modelo compatible utilizando el paquete `transformers.onnx`.
* Exportar un modelo personalizado para una arquitectura no compatible.

### Exportar un model a ONNX

Para exportar un modelo  Transformers a ONNX, tienes que instalar primero algunas
dependencias extra:

```bash
pip install transformers[onnx]
```

El paquete `transformers.onnx` puede ser usado luego como un m贸dulo de Python:

```bash
python -m transformers.onnx --help

usage: Hugging Face Transformers ONNX exporter [-h] -m MODEL [--feature {causal-lm, ...}] [--opset OPSET] [--atol ATOL] output

positional arguments:
  output                Path indicating where to store generated ONNX model.

optional arguments:
  -h, --help            show this help message and exit
  -m MODEL, --model MODEL
                        Model ID on huggingface.co or path on disk to load model from.
  --feature {causal-lm, ...}
                        The type of features to export the model with.
  --opset OPSET         ONNX opset version to export the model with.
  --atol ATOL           Absolute difference tolerence when validating the model.
```

Exportar un checkpoint usando una configuraci贸n a la medida se puede hacer de la siguiente manera:

```bash
python -m transformers.onnx --model=distilbert-base-uncased onnx/
```

que deber铆a mostrar los siguientes registros:

```bash
Validating ONNX model...
        -[] ONNX model output names match reference model ({'last_hidden_state'})
        - Validating ONNX Model output "last_hidden_state":
                -[] (2, 8, 768) matches (2, 8, 768)
                -[] all values close (atol: 1e-05)
All good, model saved at: onnx/model.onnx
```

Esto exporta un grafo ONNX del checkpoint definido por el argumento `--model`. 
En este ejemplo, es un modelo `distilbert-base-uncased`, pero puede ser cualquier
checkpoint en Hugging Face Hub o que est茅 almacenado localmente.

El archivo `model.onnx` resultante se puede ejecutar en uno de los 
[muchos aceleradores](https://onnx.ai/supported-tools.html#deployModel) 
que admiten el est谩ndar ONNX. Por ejemplo, podemos cargar y ejecutar el 
modelo con [ONNX Runtime](https://onnxruntime.ai/) de la siguiente manera:

```python
>>> from transformers import AutoTokenizer
>>> from onnxruntime import InferenceSession

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
>>> session = InferenceSession("onnx/model.onnx")
>>> # ONNX Runtime expects NumPy arrays as input
>>> inputs = tokenizer("Using DistilBERT with ONNX Runtime!", return_tensors="np")
>>> outputs = session.run(output_names=["last_hidden_state"], input_feed=dict(inputs))
```

Los nombres necesarios de salida (es decir, `["last_hidden_state"]`) se pueden obtener 
echando un vistazo a la configuraci贸n ONNX de cada modelo. Por ejemplo, para DistilBERT tenemos:

```python
>>> from transformers.models.distilbert import DistilBertConfig, DistilBertOnnxConfig

>>> config = DistilBertConfig()
>>> onnx_config = DistilBertOnnxConfig(config)
>>> print(list(onnx_config.outputs.keys()))
["last_hidden_state"]s
```

El proceso es id茅ntico para los checkpoints de TensorFlow en Hub. 
Por ejemplo, podemos exportar un checkpoint puro de TensorFlow desde 
[Keras](https://huggingface.co/keras-io) de la siguiente manera:

```bash
python -m transformers.onnx --model=keras-io/transformers-qa onnx/
```

Para exportar un modelo que est谩 almacenado localmente, deber谩s tener los pesos 
y tokenizadores del modelo almacenados en un directorio. Por ejemplo, podemos cargar 
y guardar un checkpoint de la siguiente manera:

<frameworkcontent>
<pt>
```python
>>> from transformers import AutoTokenizer, AutoModelForSequenceClassification

>>> # Load tokenizer and PyTorch weights form the Hub
>>> tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
>>> pt_model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")
>>> # Save to disk
>>> tokenizer.save_pretrained("local-pt-checkpoint")
>>> pt_model.save_pretrained("local-pt-checkpoint")
```

Una vez que se guarda el checkpoint, podemos exportarlo a ONNX usando el argumento `--model` 
del paquete `transformers.onnx` al directorio deseado:

```bash
python -m transformers.onnx --model=local-pt-checkpoint onnx/
```
</pt>
<tf>
```python
>>> from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

>>> # Load tokenizer and TensorFlow weights from the Hub
>>> tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")
>>> # Save to disk
>>> tokenizer.save_pretrained("local-tf-checkpoint")
>>> tf_model.save_pretrained("local-tf-checkpoint")
```

Una vez que se guarda el checkpoint, podemos exportarlo a ONNX usando el argumento `--model` 
del paquete `transformers.onnx` al directorio deseado:

```bash
python -m transformers.onnx --model=local-tf-checkpoint onnx/
```
</tf>
</frameworkcontent>

### Seleccionar caracter铆sticas para diferentes topolog铆as de un modelo

Cada configuraci贸n a la medida viene con un conjunto de _caracter铆sticas_ que te permiten exportar 
modelos para diferentes tipos de topolog铆as o tareas. Como se muestra en la siguiente tabla, cada 
funci贸n est谩 asociada con una auto-clase de autom贸vil diferente:

| Feature                              | Auto Class                           |
| ------------------------------------ | ------------------------------------ |
| `causal-lm`, `causal-lm-with-past`   | `AutoModelForCausalLM`               |
| `default`, `default-with-past`       | `AutoModel`                          |
| `masked-lm`                          | `AutoModelForMaskedLM`               |
| `question-answering`                 | `AutoModelForQuestionAnswering`      |
| `seq2seq-lm`, `seq2seq-lm-with-past` | `AutoModelForSeq2SeqLM`              |
| `sequence-classification`            | `AutoModelForSequenceClassification` |
| `token-classification`               | `AutoModelForTokenClassification`    |

Para cada configuraci贸n, puedes encontrar la lista de funciones admitidas a trav茅s de `FeaturesManager`. 
Por ejemplo, para DistilBERT tenemos:

```python
>>> from transformers.onnx.features import FeaturesManager

>>> distilbert_features = list(FeaturesManager.get_supported_features_for_model_type("distilbert").keys())
>>> print(distilbert_features)
["default", "masked-lm", "causal-lm", "sequence-classification", "token-classification", "question-answering"]
```

Le puedes pasar una de estas caracter铆sticas al argumento `--feature` en el paquete `transformers.onnx`. 
Por ejemplo, para exportar un modelo de clasificaci贸n de texto, podemos elegir un modelo ya ajustado del Hub y ejecutar:

```bash
python -m transformers.onnx --model=distilbert-base-uncased-finetuned-sst-2-english \
                            --feature=sequence-classification onnx/
```

que mostrar谩 los siguientes registros:

```bash
Validating ONNX model...
        -[] ONNX model output names match reference model ({'logits'})
        - Validating ONNX Model output "logits":
                -[] (2, 2) matches (2, 2)
                -[] all values close (atol: 1e-05)
All good, model saved at: onnx/model.onnx
```

Ten en cuenta que, en este caso, los nombres de salida del modelo ajustado son `logits` en lugar de `last_hidden_state` 
que vimos anteriormente con el checkpoint `distilbert-base-uncased`. Esto es de esperarse ya que el modelo ajustado 
tiene un cabezal de clasificaci贸n secuencial.

<Tip>

Las caracter铆sticas que tienen un sufijo 'with-past' (por ejemplo, 'causal-lm-with-past') corresponden a topolog铆as 
de modelo con estados ocultos precalculados (clave y valores en los bloques de atenci贸n) que se pueden usar para una 
decodificaci贸n autorregresiva m谩s r谩pida.

</Tip>


### Exportar un modelo para una arquitectura no compatible

Si deseas exportar un modelo cuya arquitectura no es compatible de forma nativa 
con la biblioteca, debes seguir tres pasos principales:

1. Implementa una configuraci贸n personalizada en ONNX.
2. Exporta el modelo a ONNX.
3. Valide los resultados de PyTorch y los modelos exportados.

En esta secci贸n, veremos c贸mo se implement贸 la serializaci贸n de DistilBERT 
para mostrar lo que implica cada paso.

#### Implementar una configuraci贸n personalizada en ONNX

Comencemos con el objeto de configuraci贸n de ONNX. Proporcionamos tres clases abstractas 
de las que debe heredar, seg煤n el tipo de arquitectura del modelo que quieras exportar:

* Modelos basados en el _Encoder_ inherente de [`~onnx.config.OnnxConfig`]
* Modelos basados en el _Decoder_ inherente de [`~onnx.config.OnnxConfigWithPast`]
* Modelos _Encoder-decoder_ inherente de [`~onnx.config.OnnxSeq2SeqConfigWithPast`]

<Tip>

Una buena manera de implementar una configuraci贸n personalizada en ONNX es observar la implementaci贸n 
existente en el archivo `configuration_<model_name>.py` de una arquitectura similar.

</Tip>

Dado que DistilBERT es un modelo de tipo _encoder_, su configuraci贸n se hereda de `OnnxConfig`:

```python
>>> from typing import Mapping, OrderedDict
>>> from transformers.onnx import OnnxConfig


>>> class DistilBertOnnxConfig(OnnxConfig):
...     @property
...     def inputs(self) -> Mapping[str, Mapping[int, str]]:
...         return OrderedDict(
...             [
...                 ("input_ids", {0: "batch", 1: "sequence"}),
...                 ("attention_mask", {0: "batch", 1: "sequence"}),
...             ]
...         )
```

Cada objeto de configuraci贸n debe implementar la propiedad `inputs` y devolver un mapeo, 
donde cada llave corresponde a una entrada esperada y cada valor indica el eje de esa entrada. 
Para DistilBERT, podemos ver que se requieren dos entradas: `input_ids` y `attention_mask`. 
Estas entradas tienen la misma forma de `(batch_size, sequence_length)`, es por lo que vemos 
los mismos ejes utilizados en la configuraci贸n.

<Tip>

Observa que la propiedad `inputs` para `DistilBertOnnxConfig` devuelve un `OrderedDict`.
Esto nos asegura que las entradas coincidan con su posici贸n relativa dentro del m茅todo 
`PreTrainedModel.forward()` al rastrear el grafo. Recomendamos usar un `OrderedDict` 
para las propiedades `inputs` y `outputs` al implementar configuraciones ONNX personalizadas.

</Tip>

Una vez que hayas implementado una configuraci贸n ONNX, puedes crear una 
instancia proporcionando la configuraci贸n del modelo base de la siguiente manera:

```python
>>> from transformers import AutoConfig

>>> config = AutoConfig.from_pretrained("distilbert-base-uncased")
>>> onnx_config = DistilBertOnnxConfig(config)
```

El objeto resultante tiene varias propiedades 煤tiles. Por ejemplo, puedes ver el conjunto de operadores ONNX que se 
utilizar谩 durante la exportaci贸n:

```python
>>> print(onnx_config.default_onnx_opset)
11
```

Tambi茅n puedes ver los resultados asociados con el modelo de la siguiente manera:

```python
>>> print(onnx_config.outputs)
OrderedDict([("last_hidden_state", {0: "batch", 1: "sequence"})])
```

Observa que la propiedad de salidas sigue la misma estructura que las entradas; 
devuelve un objecto `OrderedDict` de salidas nombradas y sus formas. La estructura 
de salida est谩 vinculada a la elecci贸n de la funci贸n con la que se inicializa la configuraci贸n.
Por defecto, la configuraci贸n de ONNX se inicializa con la funci贸n `default` que 
corresponde a exportar un modelo cargado con la clase `AutoModel`. Si quieres exportar 
una topolog铆a de modelo diferente, simplemente proporciona una caracter铆stica diferente 
al argumento `task` cuando inicialices la configuraci贸n de ONNX. Por ejemplo, si quisi茅ramos 
exportar DistilBERT con un cabezal de clasificaci贸n de secuencias, podr铆amos usar:

```python
>>> from transformers import AutoConfig

>>> config = AutoConfig.from_pretrained("distilbert-base-uncased")
>>> onnx_config_for_seq_clf = DistilBertOnnxConfig(config, task="sequence-classification")
>>> print(onnx_config_for_seq_clf.outputs)
OrderedDict([('logits', {0: 'batch'})])
```

<Tip>

Todas las propiedades base y m茅todos asociados con [`~onnx.config.OnnxConfig`] y las 
otras clases de configuraci贸n se pueden sobreescribir si es necesario.
Consulte [`BartOnnxConfig`] para ver un ejemplo avanzado.

</Tip>

#### Exportar el modelo

Una vez que hayas implementado la configuraci贸n de ONNX, el siguiente paso es exportar el modelo.
Aqu铆 podemos usar la funci贸n `export()` proporcionada por el paquete `transformers.onnx`.
Esta funci贸n espera la configuraci贸n de ONNX, junto con el modelo base y el tokenizador, 
y la ruta para guardar el archivo exportado:

```python
>>> from pathlib import Path
>>> from transformers.onnx import export
>>> from transformers import AutoTokenizer, AutoModel

>>> onnx_path = Path("model.onnx")
>>> model_ckpt = "distilbert-base-uncased"
>>> base_model = AutoModel.from_pretrained(model_ckpt)
>>> tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

>>> onnx_inputs, onnx_outputs = export(tokenizer, base_model, onnx_config, onnx_config.default_onnx_opset, onnx_path)
```

Los objetos `onnx_inputs` y `onnx_outputs` devueltos por la funci贸n `export()` 
son listas de llaves definidas en las propiedades `inputs` y `outputs` de la configuraci贸n.
Una vez exportado el modelo, puedes probar que el modelo est谩 bien formado de la siguiente manera:

```python
>>> import onnx

>>> onnx_model = onnx.load("model.onnx")
>>> onnx.checker.check_model(onnx_model)
```

<Tip>

Si tu modelo tiene m谩s de 2GB, ver谩s que se crean muchos archivos adicionales durante la exportaci贸n.
Esto es _esperado_ porque ONNX usa [B煤feres de protocolo](https://developers.google.com/protocol-buffers/) 
para almacenar el modelo y 茅stos tienen un l铆mite de tama帽o de 2 GB. Consulta la 
[documentaci贸n de ONNX](https://github.com/onnx/onnx/blob/master/docs/ExternalData.md) para obtener 
instrucciones sobre c贸mo cargar modelos con datos externos.

</Tip>

#### Validar los resultados del modelo

El paso final es validar que los resultados del modelo base y exportado coincidan dentro 
de cierta tolerancia absoluta. Aqu铆 podemos usar la funci贸n `validate_model_outputs()` 
proporcionada por el paquete `transformers.onnx` de la siguiente manera:

```python
>>> from transformers.onnx import validate_model_outputs

>>> validate_model_outputs(
...     onnx_config, tokenizer, base_model, onnx_path, onnx_outputs, onnx_config.atol_for_validation
... )
```

Esta funci贸n usa el m茅todo `OnnxConfig.generate_dummy_inputs()` para generar entradas para el modelo base 
y exportado, y la tolerancia absoluta se puede definir en la configuraci贸n. En general, encontramos una 
concordancia num茅rica en el rango de 1e-6 a 1e-4, aunque es probable que cualquier valor menor que 1e-3 est茅 bien.

### Contribuir con una nueva configuraci贸n a  Transformers

隆Estamos buscando expandir el conjunto de configuraciones a la medida para usar y agradecemos las contribuciones de la comunidad! 
Si deseas contribuir con su colaboraci贸n a la biblioteca, deber谩s:

* Implementa la configuraci贸n de ONNX en el archivo `configuration_<model_name>.py` correspondiente
* Incluye la arquitectura del modelo y las caracter铆sticas correspondientes en [`~onnx.features.FeatureManager`]
* Agrega tu arquitectura de modelo a las pruebas en `test_onnx_v2.py`

Revisa c贸mo fue la contribuci贸n para la [configuraci贸n de IBERT](https://github.com/huggingface/transformers/pull/14868/files) 
y as铆 tener una idea de lo que necesito.

## TorchScript

<Tip>

Este es el comienzo de nuestros experimentos con TorchScript y todav铆a estamos explorando sus capacidades con modelos de 
tama帽o de entrada variable. Es un tema de inter茅s y profundizaremos nuestro an谩lisis en las pr贸ximas 
versiones,  con m谩s ejemplos de c贸digo, una implementaci贸n m谩s flexible y puntos de referencia que comparen c贸digos 
basados en Python con TorchScript compilado.

</Tip>

Seg煤n la documentaci贸n de PyTorch: "TorchScript es una forma de crear modelos serializables y optimizables a partir del 
c贸digo de PyTorch". Los dos m贸dulos de Pytorch [JIT y TRACE](https://pytorch.org/docs/stable/jit.html) permiten al 
desarrollador exportar su modelo para reutilizarlo  en otros programas, como los programas C++ orientados a la eficiencia.

Hemos proporcionado una interfaz que permite exportar modelos de  Transformers a TorchScript para que puedan reutilizarse 
en un entorno diferente  al de un programa Python basado en PyTorch. Aqu铆 explicamos c贸mo exportar y usar nuestros modelos 
usando TorchScript.

Exportar un modelo requiere de dos cosas:

- un pase hacia adelante con entradas ficticias.
- instanciaci贸n del modelo con la indicador `torchscript`.

Estas necesidades implican varias cosas con las que los desarrolladores deben tener cuidado. stas se detallan a continuaci贸n.

### Indicador de TorchScript y pesos atados

Este indicador es necesario porque la mayor铆a de los modelos de lenguaje en este repositorio tienen pesos vinculados entre su capa 
de `Embedding` y su capa de `Decoding`. TorchScript no permite la exportaci贸n de modelos que tengan pesos atados, por lo que es 
necesario desvincular y clonar los pesos previamente.

Esto implica que los modelos instanciados con el indicador `torchscript` tienen su capa `Embedding` y `Decoding` separadas, 
lo que significa que no deben entrenarse m谩s adelante. El entrenamiento desincronizar铆a las dos capas, lo que generar铆a 
resultados inesperados.

Este no es el caso de los modelos que no tienen un cabezal de modelo de lenguaje, ya que no tienen pesos atados.
Estos modelos se pueden exportar de forma segura sin el indicador `torchscript`.

### Entradas ficticias y longitudes est谩ndar

Las entradas ficticias se utilizan para crear un modelo de pase hacia adelante. Mientras los valores de las entradas se 
propagan a trav茅s de las capas, PyTorch realiza un seguimiento de las diferentes operaciones ejecutadas en cada tensor.
Estas operaciones registradas se utilizan luego para crear el "rastro" del modelo.

El rastro se crea en relaci贸n con las dimensiones de las entradas. Por lo tanto, est谩 limitado por las dimensiones de la 
entrada ficticia y no funcionar谩 para ninguna otra longitud de secuencia o tama帽o de lote. Al intentar con un tama帽o diferente, 
un error como:

`The expanded size of the tensor (3) must match the existing size (7) at non-singleton dimension 2`

aparecer谩. Por lo tanto, se recomienda rastrear el modelo con un tama帽o de entrada ficticia al menos tan grande como la 
entrada m谩s  grande que se alimentar谩 al modelo durante la inferencia. El _padding_ se puede realizar para completar los 
valores que faltan.  Sin embargo, como el modelo se habr谩 rastreado con un tama帽o de entrada grande, las dimensiones de 
las diferentes matrices tambi茅n ser谩n grandes, lo que dar谩 como resultado m谩s c谩lculos.

Se recomienda tener cuidado con el n煤mero total de operaciones realizadas en cada entrada y seguir de cerca el rendimiento 
al exportar modelos de longitud de secuencia variable.

### Usar TorchScript en Python

A continuaci贸n se muestra un ejemplo que muestra c贸mo guardar, cargar modelos y c贸mo usar el rastreo para la inferencia.

#### Guardando un modelo

Este fragmento muestra c贸mo usar TorchScript para exportar un `BertModel`. Aqu铆, el `BertModel` se instancia de acuerdo 
con la clase `BertConfig` y luego se guarda en el disco con el nombre de archivo `traced_bert.pt`

```python
from transformers import BertModel, BertTokenizer, BertConfig
import torch

enc = BertTokenizer.from_pretrained("bert-base-uncased")

# Tokenizing input text
text = "[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]"
tokenized_text = enc.tokenize(text)

# Masking one of the input tokens
masked_index = 8
tokenized_text[masked_index] = "[MASK]"
indexed_tokens = enc.convert_tokens_to_ids(tokenized_text)
segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]

# Creating a dummy input
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])
dummy_input = [tokens_tensor, segments_tensors]

# Initializing the model with the torchscript flag
# Flag set to True even though it is not necessary as this model does not have an LM Head.
config = BertConfig(
    vocab_size_or_config_json_file=32000,
    hidden_size=768,
    num_hidden_layers=12,
    num_attention_heads=12,
    intermediate_size=3072,
    torchscript=True,
)

# Instantiating the model
model = BertModel(config)

# The model needs to be in evaluation mode
model.eval()

# If you are instantiating the model with *from_pretrained* you can also easily set the TorchScript flag
model = BertModel.from_pretrained("bert-base-uncased", torchscript=True)

# Creating the trace
traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])
torch.jit.save(traced_model, "traced_bert.pt")
```

#### Cargar un modelo

Este fragmento muestra c贸mo cargar el `BertModel` que se guard贸 previamente en el disco con el nombre `traced_bert.pt`.
Estamos reutilizando el `dummy_input` previamente inicializado.

```python
loaded_model = torch.jit.load("traced_bert.pt")
loaded_model.eval()

all_encoder_layers, pooled_output = loaded_model(*dummy_input)
```

#### Usar un modelo rastreado para la inferencia

Usar el modelo rastreado para la inferencia es tan simple como usar su m茅todo `__call__`:

```python
traced_model(tokens_tensor, segments_tensors)
```

### Implementar los modelos HuggingFace TorchScript en AWS mediante Neuron SDK

AWS present贸 la familia de instancias [Amazon EC2 Inf1](https://aws.amazon.com/ec2/instance-types/inf1/) para la inferencia 
de aprendizaje autom谩tico de bajo costo y  alto rendimiento en la nube. Las instancias Inf1 funcionan con el chip AWS 
Inferentia, un acelerador de hardware personalizado,  que se especializa en cargas de trabajo de inferencia de aprendizaje 
profundo. [AWS Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/#) es el kit de desarrollo para  Inferentia 
que admite el rastreo y la optimizaci贸n de modelos de  transformers para su implementaci贸n en Inf1. El SDK de Neuron proporciona:


1. API f谩cil de usar con una l铆nea de cambio de c贸digo para rastrear y optimizar un modelo de TorchScript para la inferencia en la nube.
2. Optimizaciones de rendimiento listas para usar con un [costo-rendimiento mejorado](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/benchmark/>)
3. Soporte para modelos HuggingFace Transformers construidos con [PyTorch](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/bert_tutorial/tutorial_pretrained_bert.html) 
o [TensorFlow](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/tensorflow/huggingface_bert/huggingface_bert.html).

#### Implicaciones

Los modelos Transformers basados en la arquitectura 
[BERT (Representaciones de _Enconder_ bidireccional de Transformers)](https://huggingface.co/docs/transformers/main/model_doc/bert), 
o sus variantes, como [distilBERT](https://huggingface.co/docs/transformers/main/model_doc/distilbert) y 
[roBERTa](https://huggingface.co/docs/transformers/main/model_doc/roberta), se ejecutar谩n mejor en Inf1 para tareas no 
generativas, como la respuesta extractiva de preguntas, la clasificaci贸n de secuencias y la clasificaci贸n de tokens.
Como alternativa, las tareas de generaci贸n de texto se pueden adaptar para ejecutarse en Inf1, seg煤n este 
[tutorial de AWS Neuron MarianMT](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/transformers-marianmt.html).
Puedes encontrar m谩s informaci贸n sobre los modelos que est谩n listos para usarse en Inferentia en la 
[secci贸n _Model Architecture Fit_ de la documentaci贸n de Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/models/models-inferentia.html#models-inferentia).

#### Dependencias

Usar AWS Neuron para convertir modelos requiere las siguientes dependencias y entornos:

* Un [entorno Neuron SDK](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-frameworks/pytorch-neuron/index.html#installation-guide), 
que viene preconfigurado en [AWS Deep Learning AMI](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-launching.html).

#### Convertir un modelo a AWS Neuron

Con el mismo script usado en [Uso de TorchScript en Python](https://huggingface.co/docs/transformers/main/es/serialization#using-torchscript-in-python)
para rastrear un "BertModel", puedes importar la extensi贸n del _framework_ `torch.neuron` para acceder a los componentes 
del SDK de Neuron a trav茅s de una API de Python.

```python
from transformers import BertModel, BertTokenizer, BertConfig
import torch
import torch.neuron
```
Y modificando la l铆nea de c贸digo de rastreo de:

```python
torch.jit.trace(model, [tokens_tensor, segments_tensors])
```

con lo siguiente:

```python
torch.neuron.trace(model, [token_tensor, segments_tensors])
```

Este cambio permite a Neuron SDK rastrear el modelo y optimizarlo para ejecutarse en instancias Inf1.

Para obtener m谩s informaci贸n sobre las funciones, las herramientas, los tutoriales de ejemplo y las 煤ltimas actualizaciones 
de AWS Neuron SDK, consulte la [documentaci贸n de AWS NeuronSDK](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html).
