<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Entrenamiento distribuido con  Accelerate

El paralelismo ha emergido como una estrategia para entrenar modelos grandes en hardware limitado e incrementar la velocidad de entrenamiento en varios 贸rdenes de magnitud. En Hugging Face creamos la biblioteca [ Accelerate](https://huggingface.co/docs/accelerate) para ayudar a los usuarios a entrenar modelos  Transformers en cualquier tipo de configuraci贸n distribuida, ya sea en una m谩quina con m煤ltiples GPUs o en m煤ltiples GPUs distribuidas entre muchas m谩quinas. En este tutorial aprender谩s c贸mo personalizar tu bucle de entrenamiento de PyTorch nativo para poder entrenar en entornos distribuidos.

## Configuraci贸n

Empecemos por instalar  Accelerate:

```bash
pip install accelerate
```

Luego, importamos y creamos un objeto [`Accelerator`](https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator). `Accelerator` detectar谩 autom谩ticamente el tipo de configuraci贸n distribuida que tengas disponible e inicializar谩 todos los componentes necesarios para el entrenamiento. No necesitas especificar el dispositivo en donde se debe colocar tu modelo.

```py
>>> from accelerate import Accelerator

>>> accelerator = Accelerator()
```

## Prep谩rate para acelerar

Pasa todos los objetos relevantes para el entrenamiento al m茅todo [`prepare`](https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator.prepare). Esto incluye los DataLoaders de entrenamiento y evaluaci贸n, un modelo y un optimizador:

```py
>>> train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
...     train_dataloader, eval_dataloader, model, optimizer
... )
```

## Backward

Por 煤ltimo, reemplaza el t铆pico `loss.backward()` en tu bucle de entrenamiento con el m茅todo [`backward`](https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator.backward) de  Accelerate:

```py
>>> for epoch in range(num_epochs):
...     for batch in train_dataloader:
...         outputs = model(**batch)
...         loss = outputs.loss
...         accelerator.backward(loss)

...         optimizer.step()
...         lr_scheduler.step()
...         optimizer.zero_grad()
...         progress_bar.update(1)
```

Como se puede ver en el siguiente c贸digo, 隆solo necesitas adicionar cuatro l铆neas de c贸digo a tu bucle de entrenamiento para habilitar el entrenamiento distribuido!

```diff
+ from accelerate import Accelerator
  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

+ accelerator = Accelerator()

  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
  optimizer = AdamW(model.parameters(), lr=3e-5)

- device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
- model.to(device)

+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
+     train_dataloader, eval_dataloader, model, optimizer
+ )

  num_epochs = 3
  num_training_steps = num_epochs * len(train_dataloader)
  lr_scheduler = get_scheduler(
      "linear",
      optimizer=optimizer,
      num_warmup_steps=0,
      num_training_steps=num_training_steps
  )

  progress_bar = tqdm(range(num_training_steps))

  model.train()
  for epoch in range(num_epochs):
      for batch in train_dataloader:
-         batch = {k: v.to(device) for k, v in batch.items()}
          outputs = model(**batch)
          loss = outputs.loss
-         loss.backward()
+         accelerator.backward(loss)

          optimizer.step()
          lr_scheduler.step()
          optimizer.zero_grad()
          progress_bar.update(1)
```

## Entrenamiento

Una vez que hayas a帽adido las l铆neas de c贸digo relevantes, inicia el entrenamiento desde un script o notebook como Colaboratory.

### Entrenar con un script

Si est谩s corriendo tu entrenamiento desde un script ejecuta el siguiente comando para crear y guardar un archivo de configuraci贸n:

```bash
accelerate config
```

Comienza el entrenamiento con:

```bash
accelerate launch train.py
```

### Entrenar con un notebook

 Accelerate puede correr en un notebook si, por ejemplo, est谩s planeando utilizar las TPUs de Colaboratory. Encierra el c贸digo responsable del entrenamiento en una funci贸n y p谩salo a `notebook_launcher`:

```py
>>> from accelerate import notebook_launcher

>>> notebook_launcher(training_function)
```

Para obtener m谩s informaci贸n sobre  Accelerate y sus numerosas funciones, consulta la [documentaci贸n](https://huggingface.co/docs/accelerate).
