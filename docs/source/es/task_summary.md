<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

丘멆잺 Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Lo que 游뱅 Transformers puede hacer

游뱅 Transformers es una biblioteca de modelos preentrenados de 칰ltima generaci칩n para procesamiento del lenguaje natural (PLN), visi칩n por computadora y tareas de procesamiento de audio y voz. No solo contiene modelos Transformer, sino tambi칠n modelos no Transformer como redes convolucionales modernas para tareas de visi칩n por computadora. Si observas algunos de los productos de consumo m치s populares hoy en d칤a, como tel칠fonos inteligentes, aplicaciones y televisores, es probable que haya alguna tecnolog칤a de aprendizaje profundo detr치s. 쯈uieres quitar un objeto de fondo de una foto tomada por tu tel칠fono inteligente? Este es un ejemplo de una tarea de segmentaci칩n pan칩ptica (no te preocupes si a칰n no sabes qu칠 significa, 춰lo describiremos en las siguientes secciones!).

Esta p치gina proporciona una descripci칩n general de las diferentes tareas de procesamiento de audio y voz, visi칩n por computadora y PLN que se pueden resolver con la biblioteca 游뱅 Transformers en solo tres l칤neas de c칩digo.

## Audio

Las tareas de procesamiento de audio y voz son un poco diferentes de las otras modalidades principalmente porque el audio como entrada es una se침al continua. A diferencia del texto, una forma de onda de audio cruda no se puede dividir ordenadamente en fragmentos discretos de la misma manera en que una oraci칩n puede dividirse en palabras. Para superar esto, la se침al de audio cruda generalmente se muestrea a intervalos regulares. Si tomas m치s muestras dentro de un intervalo, la tasa de muestreo es mayor y el audio se asemeja m치s a la fuente de audio original.

Enfoques anteriores preprocesaban el audio para extraer caracter칤sticas 칰tiles. Ahora es m치s com칰n comenzar las tareas de procesamiento de audio y voz alimentando directamente la forma de onda de audio cruda a un codificador de caracter칤sticas para extraer una representaci칩n de audio. Esto simplifica el paso de preprocesamiento y permite que el modelo aprenda las caracter칤sticas m치s esenciales.

### Clasificaci칩n de audio

La clasificaci칩n de audio es una tarea que etiqueta datos de audio con un conjunto predefinido de clases. Es una categor칤a amplia con muchas aplicaciones espec칤ficas, algunas de las cuales incluyen:

* clasificaci칩n de escena ac칰stica: etiquetar audio con una etiqueta de escena ("oficina", "playa", "estadio")
* detecci칩n de eventos ac칰sticos: etiquetar audio con una etiqueta de evento de sonido ("bocina de autom칩vil", "llamada de ballena", "cristal rompi칠ndose")
* etiquetado: etiquetar audio que contiene varios sonidos (canto de p치jaros, identificaci칩n de altavoces en una reuni칩n)
* clasificaci칩n de m칰sica: etiquetar m칰sica con una etiqueta de g칠nero ("metal", "hip-hop", "country")

```py
>>> from transformers import pipeline

>>> classifier = pipeline(task="audio-classification", model="superb/hubert-base-superb-er")
>>> preds = classifier("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> preds
[{'score': 0.4532, 'label': 'hap'},
 {'score': 0.3622, 'label': 'sad'},
 {'score': 0.0943, 'label': 'neu'},
 {'score': 0.0903, 'label': 'ang'}]
```

### Reconocimiento autom치tico del habla

El reconocimiento autom치tico del habla (ASR, por sus siglas en ingl칠s) transcribe el habla a texto. Es una de las tareas de audio m치s comunes, en parte debido a que el habla es una forma natural de comunicaci칩n humana. Hoy en d칤a, los sistemas ASR est치n integrados en productos de tecnolog칤a "inteligente" como altavoces, tel칠fonos y autom칩viles. Podemos pedirle a nuestros asistentes virtuales que reproduzcan m칰sica, establezcan recordatorios y nos informen sobre el clima.

Pero uno de los desaf칤os clave que las arquitecturas Transformer han ayudado a superar es en los idiomas con recursos limitados. Al preentrenar con grandes cantidades de datos de habla, afinar el modelo solo con una hora de datos de habla etiquetados en un idioma con recursos limitados a칰n puede producir resultados de alta calidad en comparaci칩n con los sistemas ASR anteriores entrenados con 100 veces m치s datos etiquetados.

```py
>>> from transformers import pipeline

>>> transcriber = pipeline(task="automatic-speech-recognition", model="openai/whisper-small")
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
```

## Visi칩n por computadora

Una de las primeras y exitosas tareas de visi칩n por computadora fue reconocer im치genes de n칰meros de c칩digo postal utilizando una [red neuronal convolucional](glossary#convolution) (CNN, por sus siglas en ingl칠s). Una imagen est치 compuesta por p칤xeles, y cada p칤xel tiene un valor num칠rico. Esto facilita representar una imagen como una matriz de valores de p칤xeles. Cada combinaci칩n particular de valores de p칤xeles describe los colores de una imagen.

Dos formas generales en las que se pueden resolver las tareas de visi칩n por computadora son:

1. Utilizar convoluciones para aprender las caracter칤sticas jer치rquicas de una imagen, desde caracter칤sticas de bajo nivel hasta cosas abstractas de alto nivel.
2. Dividir una imagen en parches y utilizar un Transformer para aprender gradualmente c칩mo cada parche de imagen se relaciona entre s칤 para formar una imagen. A diferencia del enfoque ascendente preferido por una CNN, esto es como comenzar con una imagen borrosa y luego enfocarla gradualmente.

### Clasificaci칩n de im치genes

La clasificaci칩n de im치genes etiqueta una imagen completa con un conjunto predefinido de clases. Como la mayor칤a de las tareas de clasificaci칩n, hay muchos casos pr치cticos para la clasificaci칩n de im치genes, algunos de los cuales incluyen:

* salud: etiquetar im치genes m칠dicas para detectar enfermedades o monitorear la salud del paciente
* medio ambiente: etiquetar im치genes de sat칠lite para monitorear la deforestaci칩n, informar la gesti칩n de 치reas silvestres o detectar incendios forestales
* agricultura: etiquetar im치genes de cultivos para monitorear la salud de las plantas o im치genes de sat칠lite para el monitoreo del uso del suelo
* ecolog칤a: etiquetar im치genes de especies animales o vegetales para monitorear poblaciones de vida silvestre o rastrear especies en peligro de extinci칩n

```py
>>> from transformers import pipeline

>>> classifier = pipeline(task="image-classification")
>>> preds = classifier(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> print(*preds, sep="\n")
{'score': 0.4335, 'label': 'lynx, catamount'}
{'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}
{'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}
{'score': 0.0239, 'label': 'Egyptian cat'}
{'score': 0.0229, 'label': 'tiger cat'}
```

### Detecci칩n de objetos

A diferencia de la clasificaci칩n de im치genes, la detecci칩n de objetos identifica m칰ltiples objetos dentro de una imagen y las posiciones de los objetos en la imagen (definidas por el cuadro delimitador). Algunas aplicaciones ejemplares de la detecci칩n de objetos incluyen:

* veh칤culos aut칩nomos: detectar objetos de tr치fico cotidianos como otros veh칤culos, peatones y sem치foros
* teledetecci칩n: monitoreo de desastres, planificaci칩n urbana y pron칩stico del tiempo
* detecci칩n de defectos: detectar grietas o da침os estructurales en edificios y defectos de fabricaci칩n

```py
>>> from transformers import pipeline

>>> detector = pipeline(task="object-detection")
>>> preds = detector(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"], "box": pred["box"]} for pred in preds]
>>> preds
[{'score': 0.9865,
  'label': 'cat',
  'box': {'xmin': 178, 'ymin': 154, 'xmax': 882, 'ymax': 598}}]
```

### Segmentaci칩n de im치genes

La segmentaci칩n de im치genes es una tarea a nivel de p칤xeles que asigna cada p칤xel en una imagen a una clase. A diferencia de la detecci칩n de objetos, que utiliza cuadros delimitadores para etiquetar y predecir objetos en una imagen, la segmentaci칩n es m치s granular. La segmentaci칩n puede detectar objetos a nivel de p칤xeles. Hay varios tipos de segmentaci칩n de im치genes:

* segmentaci칩n de instancias: adem치s de etiquetar la clase de un objeto, tambi칠n etiqueta cada instancia distinta de un objeto ("perro-1", "perro-2")
* segmentaci칩n pan칩ptica: una combinaci칩n de segmentaci칩n sem치ntica y de instancias; etiqueta cada p칤xel con una clase sem치ntica **y** cada instancia distinta de un objeto

Las tareas de segmentaci칩n son 칰tiles en veh칤culos aut칩nomos para crear un mapa a nivel de p칤xeles del mundo que los rodea para que puedan navegar de manera segura alrededor de peatones y otros veh칤culos. Tambi칠n es 칰til en im치genes m칠dicas, donde la mayor granularidad de la tarea puede ayudar a identificar c칠lulas anormales o caracter칤sticas de 칩rganos. La segmentaci칩n de im치genes tambi칠n se puede utilizar en comercio electr칩nico para probar virtualmente la ropa o crear experiencias de realidad aumentada superponiendo objetos en el mundo real a trav칠s de tu c치mara.

```py
>>> from transformers import pipeline

>>> segmenter = pipeline(task="image-segmentation")
>>> preds = segmenter(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> print(*preds, sep="\n")
{'score': 0.9879, 'label': 'LABEL_184'}
{'score': 0.9973, 'label': 'snow'}
{'score': 0.9972, 'label': 'cat'}
```

### Estimaci칩n de profundidad

La estimaci칩n de profundidad predice la distancia de cada p칤xel en una imagen desde la c치mara. Esta tarea de visi칩n por computadora es especialmente importante para la comprensi칩n y reconstrucci칩n de escenas. Por ejemplo, en los veh칤culos aut칩nomos, es necesario entender qu칠 tan lejos est치n los objetos como peatones, se침ales de tr치fico y otros veh칤culos para evitar obst치culos y colisiones. La informaci칩n de profundidad tambi칠n es 칰til para construir representaciones 3D a partir de im치genes 2D y se puede utilizar para crear representaciones 3D de alta calidad de estructuras biol칩gicas o edificios.

Hay dos enfoques para la estimaci칩n de profundidad:

* est칠reo: las profundidades se estiman comparando dos im치genes de la misma escena desde 치ngulos ligeramente diferentes
* monocular: las profundidades se estiman a partir de una sola imagen

```py
>>> from transformers import pipeline

>>> depth_estimator = pipeline(task="depth-estimation")
>>> preds = depth_estimator(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
```

## Procesamiento del lenguaje natural

Las tareas de procesamiento del lenguaje natural (NLP, por sus siglas en ingl칠s) est치n entre los tipos de tareas m치s comunes porque el texto es una forma natural de comunicaci칩n para nosotros. Para convertir el texto en un formato reconocido por un modelo, es necesario tokenizarlo. Esto significa dividir una secuencia de texto en palabras o subpalabras separadas (tokens) y luego convertir estos tokens en n칰meros. Como resultado, puedes representar una secuencia de texto como una secuencia de n칰meros, y una vez que tienes una secuencia de n칰meros, se puede ingresar a un modelo para resolver todo tipo de tareas de NLP.

### Clasificaci칩n de texto

Al igual que las tareas de clasificaci칩n en cualquier modalidad, la clasificaci칩n de texto etiqueta una secuencia de texto (puede ser a nivel de oraci칩n, p치rrafo o documento) de un conjunto predefinido de clases. Hay muchas aplicaciones pr치cticas para la clasificaci칩n de texto, algunas de las cuales incluyen:

* an치lisis de sentimientos: etiquetar texto seg칰n alguna polaridad como `positivo` o `negativo`, lo que puede informar y respaldar la toma de decisiones en campos como pol칤tica, finanzas y marketing
* clasificaci칩n de contenido: etiquetar texto seg칰n alg칰n tema para ayudar a organizar y filtrar informaci칩n en noticias y feeds de redes sociales (`clima`, `deportes`, `finanzas`, etc.)

```py
>>> from transformers import pipeline

>>> classifier = pipeline(task="sentiment-analysis")
>>> preds = classifier("Hugging Face is the best thing since sliced bread!")
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> preds
[{'score': 0.9991, 'label': 'POSITIVE'}]
```

### Token classification

In any NLP task, text is preprocessed by separating the sequence of text into individual words or subwords. These are known as [tokens](glossary#token). Token classification assigns each token a label from a predefined set of classes. 

Two common types of token classification are:

* named entity recognition (NER): label a token according to an entity category like organization, person, location or date. NER is especially popular in biomedical settings, where it can label genes, proteins, and drug names.
* part-of-speech tagging (POS): label a token according to its part-of-speech like noun, verb, or adjective. POS is useful for helping translation systems understand how two identical words are grammatically different (bank as a noun versus bank as a verb).

```py
>>> from transformers import pipeline

>>> classifier = pipeline(task="ner")
>>> preds = classifier("Hugging Face is a French company based in New York City.")
>>> preds = [
...     {
...         "entity": pred["entity"],
...         "score": round(pred["score"], 4),
...         "index": pred["index"],
...         "word": pred["word"],
...         "start": pred["start"],
...         "end": pred["end"],
...     }
...     for pred in preds
... ]
>>> print(*preds, sep="\n")
{'entity': 'I-ORG', 'score': 0.9968, 'index': 1, 'word': 'Hu', 'start': 0, 'end': 2}
{'entity': 'I-ORG', 'score': 0.9293, 'index': 2, 'word': '##gging', 'start': 2, 'end': 7}
{'entity': 'I-ORG', 'score': 0.9763, 'index': 3, 'word': 'Face', 'start': 8, 'end': 12}
{'entity': 'I-MISC', 'score': 0.9983, 'index': 6, 'word': 'French', 'start': 18, 'end': 24}
{'entity': 'I-LOC', 'score': 0.999, 'index': 10, 'word': 'New', 'start': 42, 'end': 45}
{'entity': 'I-LOC', 'score': 0.9987, 'index': 11, 'word': 'York', 'start': 46, 'end': 50}
{'entity': 'I-LOC', 'score': 0.9992, 'index': 12, 'word': 'City', 'start': 51, 'end': 55}
```

### Question answering

Question answering is another token-level task that returns an answer to a question, sometimes with context (open-domain) and other times without context (closed-domain). This task happens whenever we ask a virtual assistant something like whether a restaurant is open. It can also provide customer or technical support and help search engines retrieve the relevant information you're asking for. 

There are two common types of question answering:

* extractive: given a question and some context, the answer is a span of text from the context the model must extract
* abstractive: given a question and some context, the answer is generated from the context; this approach is handled by the [`Text2TextGenerationPipeline`] instead of the [`QuestionAnsweringPipeline`] shown below


```py
>>> from transformers import pipeline

>>> question_answerer = pipeline(task="question-answering")
>>> preds = question_answerer(
...     question="What is the name of the repository?",
...     context="The name of the repository is huggingface/transformers",
... )
>>> print(
...     f"score: {round(preds['score'], 4)}, start: {preds['start']}, end: {preds['end']}, answer: {preds['answer']}"
... )
score: 0.9327, start: 30, end: 54, answer: huggingface/transformers
```

### Summarization

Summarization creates a shorter version of a text from a longer one while trying to preserve most of the meaning of the original document. Summarization is a sequence-to-sequence task; it outputs a shorter text sequence than the input. There are a lot of long-form documents that can be summarized to help readers quickly understand the main points. Legislative bills, legal and financial documents, patents, and scientific papers are a few examples of documents that could be summarized to save readers time and serve as a reading aid.

Like question answering, there are two types of summarization:

* extractive: identify and extract the most important sentences from the original text
* abstractive: generate the target summary (which may include new words not in the input document) from the original text; the [`SummarizationPipeline`] uses the abstractive approach

```py
>>> from transformers import pipeline

>>> summarizer = pipeline(task="summarization")
>>> summarizer(
...     "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles."
... )
[{'summary_text': ' The Transformer is the first sequence transduction model based entirely on attention . It replaces the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention . For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .'}]
```

### Translation

Translation converts a sequence of text in one language to another. It is important in helping people from different backgrounds communicate with each other, help translate content to reach wider audiences, and even be a learning tool to help people learn a new language. Along with summarization, translation is a sequence-to-sequence task, meaning the model receives an input sequence and returns a target output sequence. 

In the early days, translation models were mostly monolingual, but recently, there has been increasing interest in multilingual models that can translate between many pairs of languages.

```py
>>> from transformers import pipeline

>>> text = "translate English to French: Hugging Face is a community-based open-source platform for machine learning."
>>> translator = pipeline(task="translation", model="t5-small")
>>> translator(text)
[{'translation_text': "Hugging Face est une tribune communautaire de l'apprentissage des machines."}]
```

### Language modeling

Language modeling is a task that predicts a word in a sequence of text. It has become a very popular NLP task because a pretrained language model can be finetuned for many other downstream tasks. Lately, there has been a lot of interest in large language models (LLMs) which demonstrate zero- or few-shot learning. This means the model can solve tasks it wasn't explicitly trained to do! Language models can be used to generate fluent and convincing text, though you need to be careful since the text may not always be accurate.

There are two types of language modeling:

* causal: the model's objective is to predict the next token in a sequence, and future tokens are masked

    ```py
    >>> from transformers import pipeline

    >>> prompt = "Hugging Face is a community-based open-source platform for machine learning."
    >>> generator = pipeline(task="text-generation")
    >>> generator(prompt)  # doctest: +SKIP
    ```

* masked: the model's objective is to predict a masked token in a sequence with full access to the tokens in the sequence
    
    ```py
    >>> text = "Hugging Face is a community-based open-source <mask> for machine learning."
    >>> fill_mask = pipeline(task="fill-mask")
    >>> preds = fill_mask(text, top_k=1)
    >>> preds = [
    ...     {
    ...         "score": round(pred["score"], 4),
    ...         "token": pred["token"],
    ...         "token_str": pred["token_str"],
    ...         "sequence": pred["sequence"],
    ...     }
    ...     for pred in preds
    ... ]
    >>> preds
    [{'score': 0.2236,
      'token': 1761,
      'token_str': ' platform',
      'sequence': 'Hugging Face is a community-based open-source platform for machine learning.'}]
    ```

## Multimodal

Multimodal tasks require a model to process multiple data modalities (text, image, audio, video) to solve a particular problem. Image captioning is an example of a multimodal task where the model takes an image as input and outputs a sequence of text describing the image or some properties of the image. 

Although multimodal models work with different data types or modalities, internally, the preprocessing steps help the model convert all the data types into embeddings (vectors or list of numbers that holds meaningful information about the data). For a task like image captioning, the model learns relationships between image embeddings and text embeddings.

### Document question answering

Document question answering is a task that answers natural language questions from a document. Unlike a token-level question answering task which takes text as input, document question answering takes an image of a document as input along with a question about the document and returns an answer. Document question answering can be used to parse structured documents and extract key information from it. In the example below, the total amount and change due can be extracted from a receipt.

```py
>>> from transformers import pipeline
>>> from PIL import Image
>>> import requests

>>> url = "https://datasets-server.huggingface.co/assets/hf-internal-testing/example-documents/--/hf-internal-testing--example-documents/test/2/image/image.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> doc_question_answerer = pipeline("document-question-answering", model="magorshunov/layoutlm-invoices")
>>> preds = doc_question_answerer(
...     question="What is the total amount?",
...     image=image,
... )
>>> preds
[{'score': 0.8531, 'answer': '17,000', 'start': 4, 'end': 4}]
```

Hopefully, this page has given you some more background information about all the types of tasks in each modality and the practical importance of each one. In the next [section](tasks_explained), you'll learn **how** 游뱅 Transformers work to solve these tasks.