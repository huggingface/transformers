<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

丘멆잺 Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# 쮺칩mo los 游뱅 Transformers resuelven tareas?

En [Lo que 游뱅 Transformers puede hacer](task_summary), aprendiste sobre el procesamiento de lenguaje natural (NLP), tareas de voz y audio, visi칩n por computadora y algunas aplicaciones importantes de ellas. Esta p치gina se centrar치 en c칩mo los modelos resuelven estas tareas y explicar치 lo que est치 sucediendo debajo de la superficie. Hay muchas maneras de resolver una tarea dada, y diferentes modelos pueden implementar ciertas t칠cnicas o incluso abordar la tarea desde un 치ngulo nuevo, pero para los modelos Transformer, la idea general es la misma. Debido a su arquitectura flexible, la mayor칤a de los modelos son una variante de una estructura de codificador, descodificador o codificador-descodificador. Adem치s de los modelos Transformer, nuestra biblioteca tambi칠n tiene varias redes neuronales convolucionales (CNNs) modernas, que todav칤a se utilizan hoy en d칤a para tareas de visi칩n por computadora. Tambi칠n explicaremos c칩mo funciona una CNN moderna.

Para explicar c칩mo se resuelven las tareas, caminaremos a trav칠s de lo que sucede dentro del modelo para generar predicciones 칰tiles.

- [Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2) para clasificaci칩n de audio y reconocimiento autom치tico de habla (ASR)
- [Transformador de Visi칩n (ViT)](https://huggingface.co/docs/transformers/model_doc/vit) y [ConvNeXT](https://huggingface.co/docs/transformers/model_doc/convnext) para clasificaci칩n de im치genes
- [DETR](https://huggingface.co/docs/transformers/model_doc/detr) para detecci칩n de objetos
- [Mask2Former](https://huggingface.co/docs/transformers/model_doc/mask2former) para segmentaci칩n de imagen
- [GLPN](https://huggingface.co/docs/transformers/model_doc/glpn) para estimaci칩n de profundidad
- [BERT](https://huggingface.co/docs/transformers/model_doc/bert) para tareas de NLP como clasificaci칩n de texto, clasificaci칩n de tokens y preguntas y respuestas que utilizan un codificador
- [GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2) para tareas de NLP como generaci칩n de texto que utilizan un descodificador
- [BART](https://huggingface.co/docs/transformers/model_doc/bart) para tareas de NLP como resumen y traducci칩n que utilizan un codificador-descodificador

<Tip>

Antes de continuar, es bueno tener un conocimiento b치sico de la arquitectura original del Transformer. Saber c칩mo funcionan los codificadores, decodificadores y la atenci칩n te ayudar치 a entender c칩mo funcionan los diferentes modelos de Transformer. Si est치s empezando o necesitas repasar, 춰echa un vistazo a nuestro [curso](https://huggingface.co/course/chapter1/4?fw=pt) para obtener m치s informaci칩n!

</Tip>

## Habla y audio

[Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2) es un modelo auto-supervisado preentrenado en datos de habla no etiquetados y ajustado en datos etiquetados para clasificaci칩n de audio y reconocimiento autom치tico de voz. 

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/wav2vec2_architecture.png"/>
</div>

Este modelo tiene cuatro componentes principales:

1. Un *codificador de caracter칤sticas* toma la forma de onda de audio cruda, la normaliza a media cero y varianza unitaria, y la convierte en una secuencia de vectores de caracter칤sticas, cada uno de 20 ms de duraci칩n.

2. Las formas de onda son continuas por naturaleza, por lo que no se pueden dividir en unidades separadas como una secuencia de texto se puede dividir en palabras. Por eso, los vectores de caracter칤sticas se pasan a un *m칩dulo de cuantificaci칩n*, que tiene como objetivo aprender unidades de habla discretas. La unidad de habla se elige de una colecci칩n de palabras de c칩digo, conocidas como *codebook* (puedes pensar en esto como el vocabulario). Del codebook, se elige el vector o unidad de habla que mejor representa la entrada de audio continua y se env칤a a trav칠s del modelo.

3. Alrededor de la mitad de los vectores de caracter칤sticas se enmascaran aleatoriamente, y el vector de caracter칤sticas enmascarado se alimenta a una *red de contexto*, que es un codificador Transformer que tambi칠n agrega incrustaciones posicionales relativas.

4. El objetivo del preentrenamiento de la red de contexto es una *tarea contrastiva*. El modelo tiene que predecir la verdadera representaci칩n de habla cuantizada de la predicci칩n enmascarada a partir de un conjunto de falsas, lo que anima al modelo a encontrar el vector de contexto y la unidad de habla cuantizada m치s similares (la etiqueta objetivo).

춰Ahora que wav2vec2 est치 preentrenado, puedes ajustarlo con tus datos para clasificaci칩n de audio o reconocimiento autom치tico de voz!

### Clasificaci칩n de audio

Para usar el modelo preentrenado para la clasificaci칩n de audio, a침ade una capa de clasificaci칩n de secuencia encima del modelo base de Wav2Vec2. La capa de clasificaci칩n es una capa lineal que acepta los estados ocultos del codificador. Los estados ocultos representan las caracter칤sticas aprendidas de cada fotograma de audio, que pueden tener longitudes variables. Para crear un vector de longitud fija, primero se agrupan los estados ocultos y luego se transforman en logits sobre las etiquetas de clase. La p칠rdida de entrop칤a cruzada se calcula entre los logits y el objetivo para encontrar la clase m치s probable.

쯃isto para probar la clasificaci칩n de audio? 춰Consulta nuestra gu칤a completa de [clasificaci칩n de audio](https://huggingface.co/docs/transformers/tasks/audio_classification) para aprender c칩mo ajustar Wav2Vec2 y usarlo para inferencia!

### Reconocimiento autom치tico de voz

Para usar el modelo preentrenado para el reconocimiento autom치tico de voz, a침ade una capa de modelado del lenguaje encima del modelo base de Wav2Vec2 para [CTC (clasificaci칩n temporal conexista)](glossary#connectionist-temporal-classification-ctc). La capa de modelado del lenguaje es una capa lineal que acepta los estados ocultos del codificador y los transforma en logits. Cada logit representa una clase de token (el n칰mero de tokens proviene del vocabulario de la tarea). La p칠rdida de CTC se calcula entre los logits y los objetivos para encontrar la secuencia de tokens m치s probable, que luego se decodifican en una transcripci칩n.

쯃isto para probar el reconocimiento autom치tico de voz? 춰Consulta nuestra gu칤a completa de [reconocimiento autom치tico de voz](tasks/asr) para aprender c칩mo ajustar Wav2Vec2 y usarlo para inferencia!

## Visi칩n por computadora

Hay dos formas de abordar las tareas de visi칩n por computadora:

1. Dividir una imagen en una secuencia de parches y procesarlos en paralelo con un Transformer.
2. Utilizar una CNN moderna, como [ConvNeXT](https://huggingface.co/docs/transformers/model_doc/convnext), que se basa en capas convolucionales pero adopta dise침os de redes modernas.

<Tip>

Un tercer enfoque combina Transformers con convoluciones (por ejemplo, [Convolutional Vision Transformer](https://huggingface.co/docs/transformers/model_doc/cvt) o [LeViT](https://huggingface.co/docs/transformers/model_doc/levit)). No discutiremos estos porque simplemente combinan los dos enfoques que examinamos aqu칤.

</Tip>

ViT y ConvNeXT se utilizan com칰nmente para la clasificaci칩n de im치genes, pero para otras tareas de visi칩n como la detecci칩n de objetos, la segmentaci칩n y la estimaci칩n de profundidad, veremos DETR, Mask2Former y GLPN, respectivamente; estos modelos son m치s adecuados para esas tareas.

### Clasificaci칩n de im치genes

ViT y ConvNeXT pueden usarse ambos para la clasificaci칩n de im치genes; la diferencia principal es que ViT utiliza un mecanismo de atenci칩n mientras que ConvNeXT utiliza convoluciones.

#### Transformer

[ViT](https://huggingface.co/docs/transformers/model_doc/vit) reemplaza completamente las convoluciones con una arquitectura de Transformer pura. Si est치s familiarizado con el Transformer original, entonces ya est치s en el camino para entender ViT.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vit_architecture.jpg"/>
</div>

El cambio principal que introdujo ViT fue en c칩mo se alimentan las im치genes a un Transformer:

1. Una imagen se divide en parches cuadrados no superpuestos, cada uno de los cuales se convierte en un vector o *incrustaci칩n de parche*(patch embedding). Las incrustaciones de parche se generan a partir de una capa convolucional 2D que crea las dimensiones de entrada adecuadas (que para un Transformer base son 768 valores para cada incrustaci칩n de parche). Si tuvieras una imagen de 224x224 p칤xeles, podr칤as dividirla en 196 parches de imagen de 16x16. Al igual que el texto se tokeniza en palabras, una imagen se "tokeniza" en una secuencia de parches.

2. Se agrega una *incrustaci칩n aprendida* - un token especial `[CLS]` - al principio de las incrustaciones del parche, al igual que en BERT. El estado oculto final del token `[CLS]` se utiliza como la entrada para la cabecera de clasificaci칩n adjunta; otras salidas se ignoran. Este token ayuda al modelo a aprender c칩mo codificar una representaci칩n de la imagen.

3. Lo 칰ltimo que se agrega a las incrustaciones de parche e incrustaciones aprendidas son las *incrustaciones de posici칩n* porque el modelo no sabe c칩mo est치n ordenados los parches de imagen. Las incrustaciones de posici칩n tambi칠n son aprendibles y tienen el mismo tama침o que las incrustaciones de parche. Finalmente, todas las incrustaciones se pasan al codificador Transformer.

4. La salida, espec칤ficamente solo la salida con el token `[CLS]`, se pasa a una cabecera de perceptr칩n multicapa (MLP). El objetivo del preentrenamiento de ViT es simplemente la clasificaci칩n. Al igual que otras cabeceras de clasificaci칩n, la cabecera de MLP convierte la salida en logits sobre las etiquetas de clase y calcula la p칠rdida de entrop칤a cruzada para encontrar la clase m치s probable.

쯃isto para probar la clasificaci칩n de im치genes? 춰Consulta nuestra gu칤a completa de [clasificaci칩n de im치genes](tasks/image_classification) para aprender c칩mo ajustar ViT y usarlo para inferencia!

#### CNN

<Tip>

Esta secci칩n explica brevemente las convoluciones, pero ser칤a 칰til tener un entendimiento previo de c칩mo cambian la forma y el tama침o de una imagen. Si no est치s familiarizado con las convoluciones, 춰echa un vistazo al [cap칤tulo de Redes Neuronales Convolucionales](https://github.com/fastai/fastbook/blob/master/13_convolutions.ipynb) del libro fastai!

</Tip>

[ConvNeXT](https://huggingface.co/docs/transformers/model_doc/convnext) es una arquitectura de CNN que adopta dise침os de redes nuevas y modernas para mejorar el rendimiento. Sin embargo, las convoluciones siguen siendo el n칰cleo del modelo. Desde una perspectiva de alto nivel, una [convoluci칩n](glossary#convolution) es una operaci칩n donde una matriz m치s peque침a (*kernel*) se multiplica por una peque침a ventana de p칤xeles de la imagen. Esta calcula algunas caracter칤sticas de ella, como una textura particular o la curvatura de una l칤nea. Luego, se desliza hacia la siguiente ventana de p칤xeles; la distancia que recorre la convoluci칩n se conoce como el *stride*. 

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/convolution.gif"/>
</div>

<small>Una convoluci칩n b치sica sin relleno ni paso, tomada de <a href="https://arxiv.org/abs/1603.07285">Una gu칤a para la aritm칠tica de convoluciones para el aprendizaje profundo.</a></small>

Puedes alimentar esta salida a otra capa convolucional, y con cada capa sucesiva, la red aprende cosas m치s complejas y abstractas como perros calientes o cohetes. Entre capas convolucionales, es com칰n a침adir una capa de agrupaci칩n para reducir la dimensionalidad y hacer que el modelo sea m치s robusto a las variaciones de la posici칩n de una caracter칤stica.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/convnext_architecture.png"/>
</div>

ConvNeXT moderniza una CNN de cinco maneras:

1. Cambia el n칰mero de bloques en cada etapa y "fragmenta" una imagen con un paso y tama침o de kernel m치s grandes. La ventana deslizante no superpuesta hace que esta estrategia de fragmentaci칩n sea similar a c칩mo ViT divide una imagen en parches.

2. Una capa de *cuello de botella* reduce el n칰mero de canales y luego lo restaura porque es m치s r치pido hacer una convoluci칩n de 1x1, y se puede aumentar la profundidad. Un cuello de botella invertido hace lo contrario al expandir el n칰mero de canales y luego reducirlos, lo cual es m치s eficiente en memoria.

3. Reemplaza la t칤pica capa convolucional de 3x3 en la capa de cuello de botella con una convoluci칩n *depthwise*, que aplica una convoluci칩n a cada canal de entrada por separado y luego los apila de nuevo al final. Esto ensancha el ancho de la red para mejorar el rendimiento.

4. ViT tiene un campo receptivo global, lo que significa que puede ver m치s de una imagen a la vez gracias a su mecanismo de atenci칩n. ConvNeXT intenta replicar este efecto aumentando el tama침o del kernel a 7x7.

5. ConvNeXT tambi칠n hace varios cambios en el dise침o de capas que imitan a los modelos Transformer. Hay menos capas de activaci칩n y normalizaci칩n, la funci칩n de activaci칩n se cambia a GELU en lugar de ReLU, y utiliza LayerNorm en lugar de BatchNorm.

La salida de los bloques convolucionales se pasa a una cabecera de clasificaci칩n que convierte las salidas en logits y calcula la p칠rdida de entrop칤a cruzada para encontrar la etiqueta m치s probable.

### Object detection

[DETR](https://huggingface.co/docs/transformers/model_doc/detr), *DEtection TRansformer*, es un modelo de detecci칩n de objetos de un extremo a otro que combina una CNN con un codificador-decodificador Transformer.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/detr_architecture.png"/>
</div>

1. Una CNN preentrenada *backbone* toma una imagen, representada por sus valores de p칤xeles, y crea un mapa de caracter칤sticas de baja resoluci칩n de la misma. A continuaci칩n, se aplica una convoluci칩n 1x1 al mapa de caracter칤sticas para reducir la dimensionalidad y se crea un nuevo mapa de caracter칤sticas con una representaci칩n de imagen de alto nivel. Dado que el Transformer es un modelo secuencial, el mapa de caracter칤sticas se aplana en una secuencia de vectores de caracter칤sticas que se combinan con incrustaciones posicionales.

2. Los vectores de caracter칤sticas se pasan al codificador, que aprende las representaciones de imagen usando sus capas de atenci칩n. A continuaci칩n, los estados ocultos del codificador se combinan con *consultas de objeto* en el decodificador. Las consultas de objeto son incrustaciones aprendidas que se enfocan en las diferentes regiones de una imagen, y se actualizan a medida que avanzan a trav칠s de cada capa de atenci칩n. Los estados ocultos del decodificador se pasan a una red feedforward que predice las coordenadas del cuadro delimitador y la etiqueta de clase para cada consulta de objeto, o `no objeto` si no hay ninguno.

    DETR descodifica cada consulta de objeto en paralelo para producir *N* predicciones finales, donde *N* es el n칰mero de consultas. A diferencia de un modelo autoregresivo t칤pico que predice un elemento a la vez, la detecci칩n de objetos es una tarea de predicci칩n de conjuntos (`cuadro delimitador`, `etiqueta de clase`) que hace *N* predicciones en un solo paso.

3. DETR utiliza una **p칠rdida de coincidencia bipartita** durante el entrenamiento para comparar un n칰mero fijo de predicciones con un conjunto fijo de etiquetas de verdad b치sica. Si hay menos etiquetas de verdad b치sica en el conjunto de *N* etiquetas, entonces se rellenan con una clase `no objeto`. Esta funci칩n de p칠rdida fomenta que DETR encuentre una asignaci칩n uno a uno entre las predicciones y las etiquetas de verdad b치sica. Si los cuadros delimitadores o las etiquetas de clase no son correctos, se incurre en una p칠rdida. Del mismo modo, si DETR predice un objeto que no existe, se penaliza. Esto fomenta que DETR encuentre otros objetos en una imagen en lugar de centrarse en un objeto realmente prominente.

Se a침ade una cabecera de detecci칩n de objetos encima de DETR para encontrar la etiqueta de clase y las coordenadas del cuadro delimitador. Hay dos componentes en la cabecera de detecci칩n de objetos: una capa lineal para transformar los estados ocultos del decodificador en logits sobre las etiquetas de clase, y una MLP para predecir el cuadro delimitador.

쯃isto para probar la detecci칩n de objetos? 춰Consulta nuestra gu칤a completa de [detecci칩n de objetos](https://huggingface.co/docs/transformers/tasks/object_detection) para aprender c칩mo ajustar DETR y usarlo para inferencia!

### Segmentaci칩n de im치genes

[Mask2Former](https://huggingface.co/docs/transformers/model_doc/mask2former) es una arquitectura universal para resolver todos los tipos de tareas de segmentaci칩n de im치genes. Los modelos de segmentaci칩n tradicionales suelen estar adaptados a una tarea particular de segmentaci칩n de im치genes, como la segmentaci칩n de instancias, sem치ntica o pan칩ptica. Mask2Former enmarca cada una de esas tareas como un problema de *clasificaci칩n de m치scaras*. La clasificaci칩n de m치scaras agrupa p칤xeles en *N* segmentos, y predice *N* m치scaras y su etiqueta de clase correspondiente para una imagen dada. Explicaremos c칩mo funciona Mask2Former en esta secci칩n, y luego podr치s probar el ajuste fino de SegFormer al final.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/mask2former_architecture.png"/>
</div>

Hay tres componentes principales en Mask2Former:

1. Un [backbone Swin](https://huggingface.co/docs/transformers/model_doc/swin) acepta una imagen y crea un mapa de caracter칤sticas de imagen de baja resoluci칩n a partir de 3 convoluciones consecutivas de 3x3.

2. El mapa de caracter칤sticas se pasa a un *decodificador de p칤xeles* que aumenta gradualmente las caracter칤sticas de baja resoluci칩n en incrustaciones de alta resoluci칩n por p칤xel. De hecho, el decodificador de p칤xeles genera caracter칤sticas multiescala (contiene caracter칤sticas de baja y alta resoluci칩n) con resoluciones de 1/32, 1/16 y 1/8 de la imagen original.

3. Cada uno de estos mapas de caracter칤sticas de diferentes escalas se alimenta sucesivamente a una capa decodificadora Transformer a la vez para capturar objetos peque침os de las caracter칤sticas de alta resoluci칩n. La clave de Mask2Former es el mecanismo de *atenci칩n enmascarada* en el decodificador. A diferencia de la atenci칩n cruzada que puede atender a toda la imagen, la atenci칩n enmascarada solo se centra en cierta 치rea de la imagen. Esto es m치s r치pido y conduce a un mejor rendimiento porque las caracter칤sticas locales de una imagen son suficientes para que el modelo aprenda.

4. Al igual que [DETR](tasks_explained#object-detection), Mask2Former tambi칠n utiliza consultas de objetos aprendidas y las combina con las caracter칤sticas de la imagen del decodificador de p칤xeles para hacer una predicci칩n de conjunto (`etiqueta de clase`, `predicci칩n de m치scara`). Los estados ocultos del decodificador se pasan a una capa lineal y se transforman en logits sobre las etiquetas de clase. Se calcula la p칠rdida de entrop칤a cruzada entre los logits y la etiqueta de clase para encontrar la m치s probable.

    Las predicciones de m치scara se generan combinando las incrustaciones de p칤xeles con los estados ocultos finales del decodificador. La p칠rdida de entrop칤a cruzada sigmoidea y de la p칠rdida DICE se calcula entre los logits y la m치scara de verdad b치sica para encontrar la m치scara m치s probable.

쯃isto para probar la detecci칩n de objetos? 춰Consulta nuestra gu칤a completa de [segmentaci칩n de im치genes](https://huggingface.co/docs/transformers/tasks/semantic_segmentation) para aprender c칩mo ajustar SegFormer y usarlo para inferencia!

### Estimaci칩n de profundidad

[GLPN](https://huggingface.co/docs/transformers/model_doc/glpn), *Global-Local Path Network*, es un Transformer para la estimaci칩n de profundidad que combina un codificador [SegFormer](https://huggingface.co/docs/transformers/model_doc/segformer) con un decodificador ligero.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/glpn_architecture.jpg"/>
</div>

1. Al igual que ViT, una imagen se divide en una secuencia de parches, excepto que estos parches de imagen son m치s peque침os. Esto es mejor para tareas de predicci칩n densa como la segmentaci칩n o la estimaci칩n de profundidad. Los parches de imagen se transforman en incrustaciones de parches (ver la secci칩n de [clasificaci칩n de im치genes](#clasificaci칩n-de-im치genes) para m치s detalles sobre c칩mo se crean las incrustaciones de parches), que se alimentan al codificador.

2. El codificador acepta las incrustaciones de parches y las pasa a trav칠s de varios bloques codificadores. Cada bloque consiste en capas de atenci칩n y Mix-FFN. El prop칩sito de este 칰ltimo es proporcionar informaci칩n posicional. Al final de cada bloque codificador hay una capa de *fusi칩n de parches* para crear representaciones jer치rquicas. Las caracter칤sticas de cada grupo de parches vecinos se concatenan, y se aplica una capa lineal a las caracter칤sticas concatenadas para reducir el n칰mero de parches a una resoluci칩n de 1/4. Esto se convierte en la entrada al siguiente bloque codificador, donde se repite todo este proceso hasta que tengas caracter칤sticas de imagen con resoluciones de 1/8, 1/16 y 1/32.

3. Un decodificador ligero toma el 칰ltimo mapa de caracter칤sticas (escala 1/32) del codificador y lo aumenta a una escala de 1/16. A partir de aqu칤, la caracter칤stica se pasa a un m칩dulo de *Fusi칩n Selectiva de Caracter칤sticas (SFF)*, que selecciona y combina caracter칤sticas locales y globales de un mapa de atenci칩n para cada caracter칤stica y luego la aumenta a 1/8. Este proceso se repite hasta que las caracter칤sticas decodificadas sean del mismo tama침o que la imagen original. La salida se pasa a trav칠s de dos capas de convoluci칩n y luego se aplica una activaci칩n sigmoide para predecir la profundidad de cada p칤xel.

## Procesamiento del lenguaje natural

El Transformer fue dise침ado inicialmente para la traducci칩n autom치tica, y desde entonces, pr치cticamente se ha convertido en la arquitectura predeterminada para resolver todas las tareas de procesamiento del lenguaje natural (NLP, por sus siglas en ingl칠s). Algunas tareas se prestan a la estructura del codificador del Transformer, mientras que otras son m치s adecuadas para el decodificador. Todav칤a hay otras tareas que hacen uso de la estructura codificador-decodificador del Transformer.

### Clasificaci칩n de texto

[BERT](https://huggingface.co/docs/transformers/model_doc/bert) es un modelo que solo tiene codificador y es el primer modelo en implementar efectivamente la bidireccionalidad profunda para aprender representaciones m치s ricas del texto al atender a las palabras en ambos lados.

1. BERT utiliza la tokenizaci칩n [WordPiece](https://huggingface.co/docs/transformers/tokenizer_summary#wordpiece) para generar una incrustaci칩n de tokens del texto. Para diferenciar entre una sola oraci칩n y un par de oraciones, se agrega un token especial `[SEP]` para diferenciarlos. Tambi칠n se agrega un token especial `[CLS]` al principio de cada secuencia de texto. La salida final con el token `[CLS]` se utiliza como la entrada a la cabeza de clasificaci칩n para tareas de clasificaci칩n. BERT tambi칠n agrega una incrustaci칩n de segmento para indicar si un token pertenece a la primera o segunda oraci칩n en un par de oraciones.

2. BERT se preentrena con dos objetivos: modelar el lenguaje enmascarado y predecir de pr칩xima oraci칩n. En el modelado de lenguaje enmascarado, un cierto porcentaje de los tokens de entrada se enmascaran aleatoriamente, y el modelo necesita predecir estos. Esto resuelve el problema de la bidireccionalidad, donde el modelo podr칤a hacer trampa y ver todas las palabras y "predecir" la siguiente palabra. Los estados ocultos finales de los tokens de m치scara predichos se pasan a una red feedforward con una softmax sobre el vocabulario para predecir la palabra enmascarada.

    El segundo objetivo de preentrenamiento es la predicci칩n de pr칩xima oraci칩n. El modelo debe predecir si la oraci칩n B sigue a la oraci칩n A. La mitad del tiempo, la oraci칩n B es la siguiente oraci칩n, y la otra mitad del tiempo, la oraci칩n B es una oraci칩n aleatoria. La predicci칩n, ya sea que sea la pr칩xima oraci칩n o no, se pasa a una red feedforward con una softmax sobre las dos clases (`EsSiguiente` y `NoSiguiente`).

3. Las incrustaciones de entrada se pasan a trav칠s de m칰ltiples capas codificadoras para producir algunos estados ocultos finales.

Para usar el modelo preentrenado para clasificaci칩n de texto, se a침ade una cabecera de clasificaci칩n de secuencia encima del modelo base de BERT. La cabecera de clasificaci칩n de secuencia es una capa lineal que acepta los estados ocultos finales y realiza una transformaci칩n lineal para convertirlos en logits. Se calcula la p칠rdida de entrop칤a cruzada entre los logits y el objetivo para encontrar la etiqueta m치s probable.

쯃isto para probar la clasificaci칩n de texto? 춰Consulta nuestra gu칤a completa de [clasificaci칩n de texto](https://huggingface.co/docs/transformers/tasks/sequence_classification) para aprender c칩mo ajustar DistilBERT y usarlo para inferencia!

### Clasificaci칩n de tokens

Para usar BERT en tareas de clasificaci칩n de tokens como el reconocimiento de entidades nombradas (NER), a침ade una cabecera de clasificaci칩n de tokens encima del modelo base de BERT. La cabecera de clasificaci칩n de tokens es una capa lineal que acepta los estados ocultos finales y realiza una transformaci칩n lineal para convertirlos en logits. Se calcula la p칠rdida de entrop칤a cruzada entre los logits y cada token para encontrar la etiqueta m치s probable.

쯃isto para probar la clasificaci칩n de tokens? 춰Consulta nuestra gu칤a completa de [clasificaci칩n de tokens](https://huggingface.co/docs/transformers/tasks/token_classification) para aprender c칩mo ajustar DistilBERT y usarlo para inferencia!

### Respuesta a preguntas

Para usar BERT en la respuesta a preguntas, a침ade una cabecera de clasificaci칩n de span encima del modelo base de BERT. Esta capa lineal acepta los estados ocultos finales y realiza una transformaci칩n lineal para calcular los logits de inicio y fin del `span` correspondiente a la respuesta. Se calcula la p칠rdida de entrop칤a cruzada entre los logits y la posici칩n de la etiqueta para encontrar el span m치s probable de texto correspondiente a la respuesta.

쯃isto para probar la respuesta a preguntas? 춰Consulta nuestra gu칤a completa de [respuesta a preguntas](tasks/question_answering) para aprender c칩mo ajustar DistilBERT y usarlo para inferencia!

<Tip>

游눠 춰Observa lo f치cil que es usar BERT para diferentes tareas una vez que ha sido preentrenado! 춰Solo necesitas a침adir una cabecera espec칤fica al modelo preentrenado para manipular los estados ocultos en tu salida deseada!

</Tip>

### Generaci칩n de texto

[GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2) es un modelo que solo tiene decodificador y se preentrena en una gran cantidad de texto. Puede generar texto convincente (춰aunque no siempre verdadero!) dado un est칤mulo y completar otras tareas de procesamiento del lenguaje natural como responder preguntas, a pesar de no haber sido entrenado expl칤citamente para ello.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gpt2_architecture.png"/>
</div>

1. GPT-2 utiliza [codificaci칩n de pares de bytes (BPE)](https://huggingface.co/docs/transformers/tokenizer_summary#bytepair-encoding-bpe) para tokenizar palabras y generar una incrustaci칩n de token. Se a침aden incrustaciones posicionales a las incrustaciones de token para indicar la posici칩n de cada token en la secuencia. Las incrustaciones de entrada se pasan a trav칠s de varios bloques decodificadores para producir alg칰n estado oculto final. Dentro de cada bloque decodificador, GPT-2 utiliza una capa de *autoatenci칩n enmascarada*, lo que significa que GPT-2 no puede atender a los tokens futuros. Solo puede atender a los tokens a la izquierda. Esto es diferente al token [`mask`] de BERT porque, en la autoatenci칩n enmascarada, se utiliza una m치scara de atenci칩n para establecer la puntuaci칩n en `0` para los tokens futuros.

2. La salida del decodificador se pasa a una cabecera de modelado de lenguaje, que realiza una transformaci칩n lineal para convertir los estados ocultos en logits. La etiqueta es el siguiente token en la secuencia, que se crea desplazando los logits a la derecha en uno. Se calcula la p칠rdida de entrop칤a cruzada entre los logits desplazados y las etiquetas para obtener el siguiente token m치s probable.

El objetivo del preentrenamiento de GPT-2 se basa completamente en el [modelado de lenguaje causal](glossary#causal-language-modeling), prediciendo la siguiente palabra en una secuencia. Esto hace que GPT-2 sea especialmente bueno en tareas que implican la generaci칩n de texto.

쯃isto para probar la generaci칩n de texto? 춰Consulta nuestra gu칤a completa de [modelado de lenguaje causal](tasks/language_modeling#modelado-de-lenguaje-causal) para aprender c칩mo ajustar DistilGPT-2 y usarlo para inferencia!

<Tip>

Para obtener m치s informaci칩n sobre la generaci칩n de texto, 춰consulta la gu칤a de [estrategias de generaci칩n de texto](https://huggingface.co/docs/transformers/generation_strategies)!

</Tip>

### Resumir

Los modelos codificador-decodificador como [BART](https://huggingface.co/docs/transformers/model_doc/bart) y [T5](https://huggingface.co/docs/transformers/model_doc/t5) est치n dise침ados para el patr칩n de secuencia a secuencia de una tarea de resumen. Explicaremos c칩mo funciona BART en esta secci칩n, y luego podr치s probar el ajuste fino de T5 al final.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bart_architecture.png"/>
</div>

1. La arquitectura del codificador de BART es muy similar a la de BERT y acepta una incrustaci칩n de token y posicional del texto. BART se preentrena corrompiendo la entrada y luego reconstruy칠ndola con el decodificador. A diferencia de otros codificadores con estrategias espec칤ficas de corrupci칩n, BART puede aplicar cualquier tipo de corrupci칩n. Sin embargo, la estrategia de corrupci칩n de *relleno de texto* funciona mejor. En el relleno de texto, varios fragmentos de texto se reemplazan con un **칰nico** token [`mask`]. Esto es importante porque el modelo tiene que predecir los tokens enmascarados, y le ense침a al modelo a predecir la cantidad de tokens faltantes. Las incrustaciones de entrada y los fragmentos enmascarados se pasan a trav칠s del codificador para producir algunos estados ocultos finales, pero a diferencia de BERT, BART no a침ade una red feedforward final al final para predecir una palabra.

2. La salida del codificador se pasa al decodificador, que debe predecir los tokens enmascarados y cualquier token no corrompido de la salida del codificador. Esto proporciona un contexto adicional para ayudar al decodificador a restaurar el texto original. La salida del decodificador se pasa a una cabeza de modelado de lenguaje, que realiza una transformaci칩n lineal para convertir los estados ocultos en logits. Se calcula la p칠rdida de entrop칤a cruzada entre los logits y la etiqueta, que es simplemente el token desplazado hacia la derecha.

쯃isto para probar la sumarizaci칩n? 춰Consulta nuestra gu칤a completa de [Generaci칩n de res칰menes](tasks/summarization) para aprender c칩mo ajustar T5 y usarlo para inferencia!

<Tip>

Para obtener m치s informaci칩n sobre la generaci칩n de texto, 춰consulta la gu칤a de [estrategias de generaci칩n de texto](https://huggingface.co/docs/transformers/generation_strategies)!

</Tip>

### Traducci칩n

La traducci칩n es otro ejemplo de una tarea de secuencia a secuencia, lo que significa que puedes usar un modelo codificador-decodificador como [BART](https://huggingface.co/docs/transformers/model_doc/bart) o [T5](https://huggingface.co/docs/transformers/model_doc/t5) para hacerlo. Explicaremos c칩mo funciona BART en esta secci칩n, y luego podr치s probar el ajuste fino de T5 al final.

BART se adapta a la traducci칩n a침adiendo un codificador separado inicializado aleatoriamente para mapear un idioma fuente a una entrada que pueda ser decodificada en el idioma objetivo. Las incrustaciones de este nuevo codificador se pasan al codificador preentrenado en lugar de las incrustaciones de palabras originales. El codificador de origen se entrena actualizando el codificador de origen, las incrustaciones posicionales y las incrustaciones de entrada con la p칠rdida de entrop칤a cruzada de la salida del modelo. Los par치metros del modelo est치n congelados en este primer paso, y todos los par치metros del modelo se entrenan juntos en el segundo paso.

Desde entonces, BART ha sido seguido por una versi칩n multiling칲e, mBART, destinada a la traducci칩n y preentrenada en muchos idiomas diferentes.

쯃isto para probar la traducci칩n? 춰Consulta nuestra gu칤a completa de [traducci칩n](https://huggingface.co/docs/transformers/tasks/translation) para aprender c칩mo ajustar T5 y usarlo para inferencia!

<Tip>

Para obtener m치s informaci칩n sobre la generaci칩n de texto, 춰consulta la gu칤a de [estrategias de generaci칩n de texto](https://huggingface.co/docs/transformers/generation_strategies)!

</Tip>