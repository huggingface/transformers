<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

丘멆잺 Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# 쮺칩mo los 游뱅 Transformers resuelven tareas?

En [Qu칠 pueden hacer los 游뱅 Transformers](task_summary), aprendiste sobre el procesamiento de lenguaje natural (NLP), tareas de voz y audio, visi칩n por computadora y algunas aplicaciones importantes de ellas. Esta p치gina se centrar치 en c칩mo los modelos resuelven estas tareas y explicar치 lo que est치 sucediendo debajo de la superficie. Hay muchas maneras de resolver una tarea dada, y diferentes modelos pueden implementar ciertas t칠cnicas o incluso abordar la tarea desde un 치ngulo nuevo, pero para los modelos Transformer, la idea general es la misma. Debido a su arquitectura flexible, la mayor칤a de los modelos son una variante de una estructura de codificador, descodificador o codificador-descodificador. Adem치s de los modelos Transformer, nuestra biblioteca tambi칠n tiene varias redes neuronales convolucionales (CNNs) modernas, que todav칤a se utilizan hoy en d칤a para tareas de visi칩n por computadora. Tambi칠n explicaremos c칩mo funciona una CNN moderna.

Para explicar c칩mo se resuelven las tareas, caminaremos a trav칠s de lo que sucede dentro del modelo para generar predicciones 칰tiles.

- [Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2) para clasificaci칩n de audio y reconocimiento autom치tico de habla (ASR)
- [Transformador de Visi칩n (ViT)](https://huggingface.co/docs/transformers/model_doc/vit) y [ConvNeXT](https://huggingface.co/docs/transformers/model_doc/convnext) para clasificaci칩n de im치genes
- [DETR](https://huggingface.co/docs/transformers/model_doc/detr) para detecci칩n de objetos
- [Mask2Former](https://huggingface.co/docs/transformers/model_doc/mask2former) para segmentaci칩n de imagen
- [GLPN](https://huggingface.co/docs/transformers/model_doc/glpn) para estimaci칩n de profundidad
- [BERT](https://huggingface.co/docs/transformers/model_doc/bert) para tareas de NLP como clasificaci칩n de texto, clasificaci칩n de tokens y preguntas y respuestas que utilizan un codificador
- [GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2) para tareas de NLP como generaci칩n de texto que utilizan un descodificador
- [BART](https://huggingface.co/docs/transformers/model_doc/bart) para tareas de NLP como resumen y traducci칩n que utilizan un codificador-descodificador

<Tip>

Antes de continuar, es bueno tener un conocimiento b치sico de la arquitectura original del Transformer. Saber c칩mo funcionan los codificadores, decodificadores y la atenci칩n te ayudar치 a entender c칩mo funcionan los diferentes modelos de Transformer. Si est치s empezando o necesitas repasar, 춰echa un vistazo a nuestro [curso](https://huggingface.co/course/chapter1/4?fw=pt) para obtener m치s informaci칩n!

</Tip>

## Habla y audio

[Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2) es un modelo auto-supervisado preentrenado en datos de habla no etiquetados y ajustado en datos etiquetados para clasificaci칩n de audio y reconocimiento autom치tico de voz. 

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/wav2vec2_architecture.png"/>
</div>

Este modelo tiene cuatro componentes principales:

1. Un *codificador de caracter칤sticas* toma la forma de onda de audio cruda, la normaliza a media cero y varianza unitaria, y la convierte en una secuencia de vectores de caracter칤sticas, cada uno de 20 ms de duraci칩n.

2. Las formas de onda son continuas por naturaleza, por lo que no se pueden dividir en unidades separadas como una secuencia de texto se puede dividir en palabras. Por eso, los vectores de caracter칤sticas se pasan a un *m칩dulo de cuantificaci칩n*, que tiene como objetivo aprender unidades de habla discretas. La unidad de habla se elige de una colecci칩n de palabras de c칩digo, conocidas como *codebook* (puedes pensar en esto como el vocabulario). Del codebook, se elige el vector o unidad de habla que mejor representa la entrada de audio continua y se env칤a a trav칠s del modelo.

3. Alrededor de la mitad de los vectores de caracter칤sticas se enmascaran aleatoriamente, y el vector de caracter칤sticas enmascarado se alimenta a una *red de contexto*, que es un codificador Transformer que tambi칠n agrega incrustaciones posicionales relativas.

4. El objetivo del preentrenamiento de la red de contexto es una *tarea contrastiva*. El modelo tiene que predecir la verdadera representaci칩n de habla cuantizada de la predicci칩n enmascarada a partir de un conjunto de falsas, lo que anima al modelo a encontrar el vector de contexto y la unidad de habla cuantizada m치s similares (la etiqueta objetivo).

춰Ahora que wav2vec2 est치 preentrenado, puedes ajustarlo con tus datos para clasificaci칩n de audio o reconocimiento autom치tico de voz!

### Clasificaci칩n de audio

Para usar el modelo preentrenado para la clasificaci칩n de audio, a침ade una capa de clasificaci칩n de secuencia encima del modelo base de Wav2Vec2. La capa de clasificaci칩n es una capa lineal que acepta los estados ocultos del codificador. Los estados ocultos representan las caracter칤sticas aprendidas de cada fotograma de audio, que pueden tener longitudes variables. Para crear un vector de longitud fija, primero se agrupan los estados ocultos y luego se transforman en logits sobre las etiquetas de clase. La p칠rdida de entrop칤a cruzada se calcula entre los logits y el objetivo para encontrar la clase m치s probable.

쯃isto para probar la clasificaci칩n de audio? 춰Consulta nuestra gu칤a completa de [clasificaci칩n de audio](https://huggingface.co/docs/transformers/tasks/audio_classification) para aprender c칩mo ajustar Wav2Vec2 y usarlo para inferencia!

### Reconocimiento autom치tico de voz

Para usar el modelo preentrenado para el reconocimiento autom치tico de voz, a침ade una capa de modelado del lenguaje encima del modelo base de Wav2Vec2 para [CTC (clasificaci칩n temporal conexista)](glossary#connectionist-temporal-classification-ctc). La capa de modelado del lenguaje es una capa lineal que acepta los estados ocultos del codificador y los transforma en logits. Cada logit representa una clase de token (el n칰mero de tokens proviene del vocabulario de la tarea). La p칠rdida de CTC se calcula entre los logits y los objetivos para encontrar la secuencia de tokens m치s probable, que luego se decodifican en una transcripci칩n.

쯃isto para probar el reconocimiento autom치tico de voz? 춰Consulta nuestra gu칤a completa de [reconocimiento autom치tico de voz](tasks/asr) para aprender c칩mo ajustar Wav2Vec2 y usarlo para inferencia!

## Visi칩n por computadora

Hay dos formas de abordar las tareas de visi칩n por computadora:

1. Dividir una imagen en una secuencia de parches y procesarlos en paralelo con un Transformer.
2. Utilizar una CNN moderna, como [ConvNeXT](https://huggingface.co/docs/transformers/model_doc/convnext), que se basa en capas convolucionales pero adopta dise침os de redes modernas.

<Tip>

Un tercer enfoque combina Transformers con convoluciones (por ejemplo, [Convolutional Vision Transformer](https://huggingface.co/docs/transformers/model_doc/cvt) o [LeViT](https://huggingface.co/docs/transformers/model_doc/levit)). No discutiremos estos porque simplemente combinan los dos enfoques que examinamos aqu칤.

</Tip>

ViT y ConvNeXT se utilizan com칰nmente para la clasificaci칩n de im치genes, pero para otras tareas de visi칩n como la detecci칩n de objetos, la segmentaci칩n y la estimaci칩n de profundidad, veremos DETR, Mask2Former y GLPN, respectivamente; estos modelos son m치s adecuados para esas tareas.

### Clasificaci칩n de im치genes

ViT y ConvNeXT pueden usarse ambos para la clasificaci칩n de im치genes; la diferencia principal es que ViT utiliza un mecanismo de atenci칩n mientras que ConvNeXT utiliza convoluciones.

#### Transformer

[ViT](https://huggingface.co/docs/transformers/model_doc/vit) reemplaza completamente las convoluciones con una arquitectura de Transformer pura. Si est치s familiarizado con el Transformer original, entonces ya est치s en el camino para entender ViT.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vit_architecture.jpg"/>
</div>

El cambio principal que introdujo ViT fue en c칩mo se alimentan las im치genes a un Transformer:

1. Una imagen se divide en parches cuadrados no superpuestos, cada uno de los cuales se convierte en un vector o *incrustaci칩n de parche*(patch embedding). Las incrustaciones de parche se generan a partir de una capa convolucional 2D que crea las dimensiones de entrada adecuadas (que para un Transformer base son 768 valores para cada incrustaci칩n de parche). Si tuvieras una imagen de 224x224 p칤xeles, podr칤as dividirla en 196 parches de imagen de 16x16. Al igual que el texto se tokeniza en palabras, una imagen se "tokeniza" en una secuencia de parches.

2. Se agrega una *incrustaci칩n aprendida* - un token especial `[CLS]` - al principio de las incrustaciones del parche, al igual que en BERT. El estado oculto final del token `[CLS]` se utiliza como la entrada para la cabecera de clasificaci칩n adjunta; otras salidas se ignoran. Este token ayuda al modelo a aprender c칩mo codificar una representaci칩n de la imagen.

3. Lo 칰ltimo que se agrega a las incrustaciones de parche e incrustaciones aprendidas son las *incrustaciones de posici칩n* porque el modelo no sabe c칩mo est치n ordenados los parches de imagen. Las incrustaciones de posici칩n tambi칠n son aprendibles y tienen el mismo tama침o que las incrustaciones de parche. Finalmente, todas las incrustaciones se pasan al codificador Transformer.

4. La salida, espec칤ficamente solo la salida con el token `[CLS]`, se pasa a una cabecera de perceptr칩n multicapa (MLP). El objetivo del preentrenamiento de ViT es simplemente la clasificaci칩n. Al igual que otras cabeceras de clasificaci칩n, la cabecera de MLP convierte la salida en logits sobre las etiquetas de clase y calcula la p칠rdida de entrop칤a cruzada para encontrar la clase m치s probable.

쯃isto para probar la clasificaci칩n de im치genes? 춰Consulta nuestra gu칤a completa de [clasificaci칩n de im치genes](tasks/image_classification) para aprender c칩mo ajustar ViT y usarlo para inferencia!

#### CNN

<Tip>

Esta secci칩n explica brevemente las convoluciones, pero ser칤a 칰til tener un entendimiento previo de c칩mo cambian la forma y el tama침o de una imagen. Si no est치s familiarizado con las convoluciones, 춰echa un vistazo al [cap칤tulo de Redes Neuronales Convolucionales](https://github.com/fastai/fastbook/blob/master/13_convolutions.ipynb) del libro fastai!

</Tip>

[ConvNeXT](https://huggingface.co/docs/transformers/model_doc/convnext) es una arquitectura de CNN que adopta dise침os de redes nuevas y modernas para mejorar el rendimiento. Sin embargo, las convoluciones siguen siendo el n칰cleo del modelo. Desde una perspectiva de alto nivel, una [convoluci칩n](glossary#convolution) es una operaci칩n donde una matriz m치s peque침a (*kernel*) se multiplica por una peque침a ventana de p칤xeles de la imagen. Esta calcula algunas caracter칤sticas de ella, como una textura particular o la curvatura de una l칤nea. Luego, se desliza hacia la siguiente ventana de p칤xeles; la distancia que recorre la convoluci칩n se conoce como el *stride*. 

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/convolution.gif"/>
</div>

<small>Una convoluci칩n b치sica sin relleno ni paso, tomada de <a href="https://arxiv.org/abs/1603.07285">Una gu칤a para la aritm칠tica de convoluciones para el aprendizaje profundo.</a></small>

Puedes alimentar esta salida a otra capa convolucional, y con cada capa sucesiva, la red aprende cosas m치s complejas y abstractas como perros calientes o cohetes. Entre capas convolucionales, es com칰n a침adir una capa de agrupaci칩n para reducir la dimensionalidad y hacer que el modelo sea m치s robusto a las variaciones de la posici칩n de una caracter칤stica.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/convnext_architecture.png"/>
</div>

ConvNeXT moderniza una CNN de cinco maneras:

1. Cambia el n칰mero de bloques en cada etapa y "fragmenta" una imagen con un paso y tama침o de kernel m치s grandes. La ventana deslizante no superpuesta hace que esta estrategia de fragmentaci칩n sea similar a c칩mo ViT divide una imagen en parches.

2. Una capa de *cuello de botella* reduce el n칰mero de canales y luego lo restaura porque es m치s r치pido hacer una convoluci칩n de 1x1, y se puede aumentar la profundidad. Un cuello de botella invertido hace lo contrario al expandir el n칰mero de canales y luego reducirlos, lo cual es m치s eficiente en memoria.

3. Reemplaza la t칤pica capa convolucional de 3x3 en la capa de cuello de botella con una convoluci칩n *depthwise*, que aplica una convoluci칩n a cada canal de entrada por separado y luego los apila de nuevo al final. Esto ensancha el ancho de la red para mejorar el rendimiento.

4. ViT tiene un campo receptivo global, lo que significa que puede ver m치s de una imagen a la vez gracias a su mecanismo de atenci칩n. ConvNeXT intenta replicar este efecto aumentando el tama침o del kernel a 7x7.

5. ConvNeXT tambi칠n hace varios cambios en el dise침o de capas que imitan a los modelos Transformer. Hay menos capas de activaci칩n y normalizaci칩n, la funci칩n de activaci칩n se cambia a GELU en lugar de ReLU, y utiliza LayerNorm en lugar de BatchNorm.

La salida de los bloques convolucionales se pasa a una cabecera de clasificaci칩n que convierte las salidas en logits y calcula la p칠rdida de entrop칤a cruzada para encontrar la etiqueta m치s probable.

### Object detection

[DETR](https://huggingface.co/docs/transformers/model_doc/detr), *DEtection TRansformer*, es un modelo de detecci칩n de objetos de un extremo a otro que combina una CNN con un codificador-decodificador Transformer.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/detr_architecture.png"/>
</div>

1. Una CNN preentrenada *backbone* toma una imagen, representada por sus valores de p칤xeles, y crea un mapa de caracter칤sticas de baja resoluci칩n de la misma. A continuaci칩n, se aplica una convoluci칩n 1x1 al mapa de caracter칤sticas para reducir la dimensionalidad y se crea un nuevo mapa de caracter칤sticas con una representaci칩n de imagen de alto nivel. Dado que el Transformer es un modelo secuencial, el mapa de caracter칤sticas se aplana en una secuencia de vectores de caracter칤sticas que se combinan con incrustaciones posicionales.

2. Los vectores de caracter칤sticas se pasan al codificador, que aprende las representaciones de imagen usando sus capas de atenci칩n. A continuaci칩n, los estados ocultos del codificador se combinan con *consultas de objeto* en el decodificador. Las consultas de objeto son incrustaciones aprendidas que se enfocan en las diferentes regiones de una imagen, y se actualizan a medida que avanzan a trav칠s de cada capa de atenci칩n. Los estados ocultos del decodificador se pasan a una red feedforward que predice las coordenadas del cuadro delimitador y la etiqueta de clase para cada consulta de objeto, o `no objeto` si no hay ninguno.

    DETR descodifica cada consulta de objeto en paralelo para producir *N* predicciones finales, donde *N* es el n칰mero de consultas. A diferencia de un modelo autoregresivo t칤pico que predice un elemento a la vez, la detecci칩n de objetos es una tarea de predicci칩n de conjuntos (`cuadro delimitador`, `etiqueta de clase`) que hace *N* predicciones en un solo paso.

3. DETR utiliza una **p칠rdida de coincidencia bipartita** durante el entrenamiento para comparar un n칰mero fijo de predicciones con un conjunto fijo de etiquetas de verdad b치sica. Si hay menos etiquetas de verdad b치sica en el conjunto de *N* etiquetas, entonces se rellenan con una clase `no objeto`. Esta funci칩n de p칠rdida fomenta que DETR encuentre una asignaci칩n uno a uno entre las predicciones y las etiquetas de verdad b치sica. Si los cuadros delimitadores o las etiquetas de clase no son correctos, se incurre en una p칠rdida. Del mismo modo, si DETR predice un objeto que no existe, se penaliza. Esto fomenta que DETR encuentre otros objetos en una imagen en lugar de centrarse en un objeto realmente prominente.

Se a침ade una cabecera de detecci칩n de objetos encima de DETR para encontrar la etiqueta de clase y las coordenadas del cuadro delimitador. Hay dos componentes en la cabecera de detecci칩n de objetos: una capa lineal para transformar los estados ocultos del decodificador en logits sobre las etiquetas de clase, y una MLP para predecir el cuadro delimitador.

쯃isto para probar la detecci칩n de objetos? 춰Consulta nuestra gu칤a completa de [detecci칩n de objetos](https://huggingface.co/docs/transformers/tasks/object_detection) para aprender c칩mo ajustar DETR y usarlo para inferencia!

### Segmentaci칩n de im치genes

[Mask2Former](https://huggingface.co/docs/transformers/model_doc/mask2former) es una arquitectura universal para resolver todos los tipos de tareas de segmentaci칩n de im치genes. Los modelos de segmentaci칩n tradicionales suelen estar adaptados a una tarea particular de segmentaci칩n de im치genes, como la segmentaci칩n de instancias, sem치ntica o pan칩ptica. Mask2Former enmarca cada una de esas tareas como un problema de *clasificaci칩n de m치scaras*. La clasificaci칩n de m치scaras agrupa p칤xeles en *N* segmentos, y predice *N* m치scaras y su etiqueta de clase correspondiente para una imagen dada. Explicaremos c칩mo funciona Mask2Former en esta secci칩n, y luego podr치s probar el ajuste fino de SegFormer al final.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/mask2former_architecture.png"/>
</div>

Hay tres componentes principales en Mask2Former:

1. Un [backbone Swin](https://huggingface.co/docs/transformers/model_doc/swin) acepta una imagen y crea un mapa de caracter칤sticas de imagen de baja resoluci칩n a partir de 3 convoluciones consecutivas de 3x3.

2. El mapa de caracter칤sticas se pasa a un *decodificador de p칤xeles* que aumenta gradualmente las caracter칤sticas de baja resoluci칩n en incrustaciones de alta resoluci칩n por p칤xel. De hecho, el decodificador de p칤xeles genera caracter칤sticas multiescala (contiene caracter칤sticas de baja y alta resoluci칩n) con resoluciones de 1/32, 1/16 y 1/8 de la imagen original.

3. Cada uno de estos mapas de caracter칤sticas de diferentes escalas se alimenta sucesivamente a una capa decodificadora Transformer a la vez para capturar objetos peque침os de las caracter칤sticas de alta resoluci칩n. La clave de Mask2Former es el mecanismo de *atenci칩n enmascarada* en el decodificador. A diferencia de la atenci칩n cruzada que puede atender a toda la imagen, la atenci칩n enmascarada solo se centra en cierta 치rea de la imagen. Esto es m치s r치pido y conduce a un mejor rendimiento porque las caracter칤sticas locales de una imagen son suficientes para que el modelo aprenda.

4. Al igual que [DETR](tasks_explained#object-detection), Mask2Former tambi칠n utiliza consultas de objetos aprendidas y las combina con las caracter칤sticas de la imagen del decodificador de p칤xeles para hacer una predicci칩n de conjunto (`etiqueta de clase`, `predicci칩n de m치scara`). Los estados ocultos del decodificador se pasan a una capa lineal y se transforman en logits sobre las etiquetas de clase. Se calcula la p칠rdida de entrop칤a cruzada entre los logits y la etiqueta de clase para encontrar la m치s probable.

    Las predicciones de m치scara se generan combinando las incrustaciones de p칤xeles con los estados ocultos finales del decodificador. La p칠rdida de entrop칤a cruzada sigmoidea y de la p칠rdida DICE se calcula entre los logits y la m치scara de verdad b치sica para encontrar la m치scara m치s probable.

쯃isto para probar la detecci칩n de objetos? 춰Consulta nuestra gu칤a completa de [segmentaci칩n de im치genes](https://huggingface.co/docs/transformers/tasks/semantic_segmentation) para aprender c칩mo ajustar SegFormer y usarlo para inferencia!

### Estimaci칩n de profundidad

[GLPN](https://huggingface.co/docs/transformers/model_doc/glpn), *Global-Local Path Network*, es un Transformer para la estimaci칩n de profundidad que combina un codificador [SegFormer](https://huggingface.co/docs/transformers/model_doc/segformer) con un decodificador ligero.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/glpn_architecture.jpg"/>
</div>

1. Al igual que ViT, una imagen se divide en una secuencia de parches, excepto que estos parches de imagen son m치s peque침os. Esto es mejor para tareas de predicci칩n densa como la segmentaci칩n o la estimaci칩n de profundidad. Los parches de imagen se transforman en incrustaciones de parches (ver la secci칩n de [clasificaci칩n de im치genes](#clasificaci칩n-de-im치genes) para m치s detalles sobre c칩mo se crean las incrustaciones de parches), que se alimentan al codificador.

2. El codificador acepta las incrustaciones de parches y las pasa a trav칠s de varios bloques codificadores. Cada bloque consiste en capas de atenci칩n y Mix-FFN. El prop칩sito de este 칰ltimo es proporcionar informaci칩n posicional. Al final de cada bloque codificador hay una capa de *fusi칩n de parches* para crear representaciones jer치rquicas. Las caracter칤sticas de cada grupo de parches vecinos se concatenan, y se aplica una capa lineal a las caracter칤sticas concatenadas para reducir el n칰mero de parches a una resoluci칩n de 1/4. Esto se convierte en la entrada al siguiente bloque codificador, donde se repite todo este proceso hasta que tengas caracter칤sticas de imagen con resoluciones de 1/8, 1/16 y 1/32.

3. Un decodificador ligero toma el 칰ltimo mapa de caracter칤sticas (escala 1/32) del codificador y lo aumenta a una escala de 1/16. A partir de aqu칤, la caracter칤stica se pasa a un m칩dulo de *Fusi칩n Selectiva de Caracter칤sticas (SFF)*, que selecciona y combina caracter칤sticas locales y globales de un mapa de atenci칩n para cada caracter칤stica y luego la aumenta a 1/8. Este proceso se repite hasta que las caracter칤sticas decodificadas sean del mismo tama침o que la imagen original. La salida se pasa a trav칠s de dos capas de convoluci칩n y luego se aplica una activaci칩n sigmoide para predecir la profundidad de cada p칤xel.

## Natural language processing

The Transformer was initially designed for machine translation, and since then, it has practically become the default architecture for solving all NLP tasks. Some tasks lend themselves to the Transformer's encoder structure, while others are better suited for the decoder. Still, other tasks make use of both the Transformer's encoder-decoder structure.

### Text classification

[BERT](model_doc/bert) is an encoder-only model and is the first model to effectively implement deep bidirectionality to learn richer representations of the text by attending to words on both sides.

1. BERT uses [WordPiece](tokenizer_summary#wordpiece) tokenization to generate a token embedding of the text. To tell the difference between a single sentence and a pair of sentences, a special `[SEP]` token is added to differentiate them. A special `[CLS]` token is added to the beginning of every sequence of text. The final output with the `[CLS]` token is used as the input to the classification head for classification tasks. BERT also adds a segment embedding to denote whether a token belongs to the first or second sentence in a pair of sentences.

2. BERT is pretrained with two objectives: masked language modeling and next-sentence prediction. In masked language modeling, some percentage of the input tokens are randomly masked, and the model needs to predict these. This solves the issue of bidirectionality, where the model could cheat and see all the words and "predict" the next word. The final hidden states of the predicted mask tokens are passed to a feedforward network with a softmax over the vocabulary to predict the masked word.

    The second pretraining object is next-sentence prediction. The model must predict whether sentence B follows sentence A. Half of the time sentence B is the next sentence, and the other half of the time, sentence B is a random sentence. The prediction, whether it is the next sentence or not, is passed to a feedforward network with a softmax over the two classes (`IsNext` and `NotNext`).

3. The input embeddings are passed through multiple encoder layers to output some final hidden states.

To use the pretrained model for text classification, add a sequence classification head on top of the base BERT model. The sequence classification head is a linear layer that accepts the final hidden states and performs a linear transformation to convert them into logits. The cross-entropy loss is calculated between the logits and target to find the most likely label.

Ready to try your hand at text classification? Check out our complete [text classification guide](tasks/sequence_classification) to learn how to finetune DistilBERT and use it for inference!

### Token classification

To use BERT for token classification tasks like named entity recognition (NER), add a token classification head on top of the base BERT model. The token classification head is a linear layer that accepts the final hidden states and performs a linear transformation to convert them into logits. The cross-entropy loss is calculated between the logits and each token to find the most likely label.

Ready to try your hand at token classification? Check out our complete [token classification guide](tasks/token_classification) to learn how to finetune DistilBERT and use it for inference!

### Question answering

To use BERT for question answering, add a span classification head on top of the base BERT model. This linear layer accepts the final hidden states and performs a linear transformation to compute the `span` start and end logits corresponding to the answer. The cross-entropy loss is calculated between the logits and the label position to find the most likely span of text corresponding to the answer.

Ready to try your hand at question answering? Check out our complete [question answering guide](tasks/question_answering) to learn how to finetune DistilBERT and use it for inference!

<Tip>

游눠 Notice how easy it is to use BERT for different tasks once it's been pretrained. You only need to add a specific head to the pretrained model to manipulate the hidden states into your desired output!

</Tip>

### Text generation

[GPT-2](model_doc/gpt2) is a decoder-only model pretrained on a large amount of text. It can generate convincing (though not always true!) text given a prompt and complete other NLP tasks like question answering despite not being explicitly trained to.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gpt2_architecture.png"/>
</div>

1. GPT-2 uses [byte pair encoding (BPE)](tokenizer_summary#bytepair-encoding-bpe) to tokenize words and generate a token embedding. Positional encodings are added to the token embeddings to indicate the position of each token in the sequence. The input embeddings are passed through multiple decoder blocks to output some final hidden state. Within each decoder block, GPT-2 uses a *masked self-attention* layer which means GPT-2 can't attend to future tokens. It is only allowed to attend to tokens on the left. This is different from BERT's [`mask`] token because, in masked self-attention, an attention mask is used to set the score to `0` for future tokens.

2. The output from the decoder is passed to a language modeling head, which performs a linear transformation to convert the hidden states into logits. The label is the next token in the sequence, which are created by shifting the logits to the right by one. The cross-entropy loss is calculated between the shifted logits and the labels to output the next most likely token.

GPT-2's pretraining objective is based entirely on [causal language modeling](glossary#causal-language-modeling), predicting the next word in a sequence. This makes GPT-2 especially good at tasks that involve generating text.

Ready to try your hand at text generation? Check out our complete [causal language modeling guide](tasks/language_modeling#causal-language-modeling) to learn how to finetune DistilGPT-2 and use it for inference!

<Tip>

For more information about text generation, check out the [text generation strategies](generation_strategies) guide!

</Tip>

### Summarization

Encoder-decoder models like [BART](model_doc/bart) and [T5](model_doc/t5) are designed for the sequence-to-sequence pattern of a summarization task. We'll explain how BART works in this section, and then you can try finetuning T5 at the end.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bart_architecture.png"/>
</div>

1. BART's encoder architecture is very similar to BERT and accepts a token and positional embedding of the text. BART is pretrained by corrupting the input and then reconstructing it with the decoder. Unlike other encoders with specific corruption strategies, BART can apply any type of corruption. The *text infilling* corruption strategy works the best though. In text infilling, a number of text spans are replaced with a **single** [`mask`] token. This is important because the model has to predict the masked tokens, and it teaches the model to predict the number of missing tokens. The input embeddings and masked spans are passed through the encoder to output some final hidden states, but unlike BERT, BART doesn't add a final feedforward network at the end to predict a word.

2. The encoder's output is passed to the decoder, which must predict the masked tokens and any uncorrupted tokens from the encoder's output. This gives additional context to help the decoder restore the original text. The output from the decoder is passed to a language modeling head, which performs a linear transformation to convert the hidden states into logits. The cross-entropy loss is calculated between the logits and the label, which is just the token shifted to the right.

Ready to try your hand at summarization? Check out our complete [summarization guide](tasks/summarization) to learn how to finetune T5 and use it for inference!

<Tip>

For more information about text generation, check out the [text generation strategies](generation_strategies) guide!

</Tip>

### Translation

Translation is another example of a sequence-to-sequence task, which means you can use an encoder-decoder model like [BART](model_doc/bart) or [T5](model_doc/t5) to do it. We'll explain how BART works in this section, and then you can try finetuning T5 at the end.

BART adapts to translation by adding a separate randomly initialized encoder to map a source language to an input that can be decoded into the target language. This new encoder's embeddings are passed to the pretrained encoder instead of the original word embeddings. The source encoder is trained by updating the source encoder, positional embeddings, and input embeddings with the cross-entropy loss from the model output. The model parameters are frozen in this first step, and all the model parameters are trained together in the second step.

BART has since been followed up by a multilingual version, mBART, intended for translation and pretrained on many different languages.

Ready to try your hand at translation? Check out our complete [translation guide](tasks/summarization) to learn how to finetune T5 and use it for inference!

<Tip>

For more information about text generation, check out the [text generation strategies](generation_strategies) guide!

</Tip>