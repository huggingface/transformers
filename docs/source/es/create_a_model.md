<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Crea una arquitectura personalizada

Una [`AutoClass`](model_doc/auto) infiere, autom√°ticamente, la arquitectura del modelo y descarga la configuraci√≥n y los pesos del modelo preentrenado. Normalmente, recomendamos usar una `AutoClass` para producir un c√≥digo agn√≥stico a puntos de guardado o checkpoints. Sin embargo, los usuarios que quieran m√°s control sobre los par√°metros espec√≠ficos de los modelos pueden crear su propio modelo ü§ó Transformers personalizado a partir de varias clases base. Esto puede ser particularmente √∫til para alguien que est√© interesado en estudiar, entrenar o experimentar con modelos ü§ó Transformers. En esta gu√≠a vamos a profundizar en la creaci√≥n de modelos personalizados sin usar `AutoClass`. Aprenderemos a:

- Cargar y personalizar una configuraci√≥n para un modelo.
- Crear una arquitectura para un modelo.
- Crear tokenizadores r√°pidos y lentos para textos.
- Crear un extractor de propiedades para tareas de audio o im√°genes.
- Crear un procesador para tareas multimodales.

## Configuraci√≥n

Una [configuraci√≥n](main_classes/configuration) es un conjunto de atributos espec√≠ficos de un modelo. Cada configuraci√≥n de modelo tiene atributos diferentes. Por ejemplo, todos los modelos de PLN tienen los atributos `hidden_size`, `num_attention_heads`, `num_hidden_layers` y `vocab_size` en com√∫n. Estos atributos especifican el n√∫mero de cabezas de atenci√≥n o de capas ocultas con las que se construyen los modelos.

Puedes echarle un vistazo a [DistilBERT](model_doc/distilbert) y sus atributos accediendo a [`DistilBertConfig`]:

```py
>>> from transformers import DistilBertConfig

>>> config = DistilBertConfig()
>>> print(config)
DistilBertConfig {
  "activation": "gelu",
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "transformers_version": "4.16.2",
  "vocab_size": 30522
}
```

[`DistilBertConfig`] muestra todos los atributos por defecto que se han usado para construir un modelo [`DistilBertModel`] base. Todos ellos son personalizables, lo que deja espacio para poder experimentar. Por ejemplo, puedes personalizar un modelo predeterminado para:

- Probar una funci√≥n de activaci√≥n diferente, usando el par√°metro `activation`.
- Usar un valor de abandono (tambi√©n conocido como _dropout_) m√°s alto para las probabilidades de las capas de atenci√≥n, usando el par√°metro `attention_dropout`.

```py
>>> my_config = DistilBertConfig(activation="relu", attention_dropout=0.4)
>>> print(my_config)
DistilBertConfig {
  "activation": "relu",
  "attention_dropout": 0.4,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "transformers_version": "4.16.2",
  "vocab_size": 30522
}
```

Los atributos de los modelos preentrenados pueden ser modificados con la funci√≥n [`~PretrainedConfig.from_pretrained`]:

```py
>>> my_config = DistilBertConfig.from_pretrained("distilbert/distilbert-base-uncased", activation="relu", attention_dropout=0.4)
```

Cuando est√©s satisfecho con la configuraci√≥n de tu modelo, puedes guardarlo con la funci√≥n [`~PretrainedConfig.save_pretrained`]. Tu configuraci√≥n se guardar√° en un archivo JSON dentro del directorio que le especifiques como par√°metro.

```py
>>> my_config.save_pretrained(save_directory="./your_model_save_path")
```

Para volver a usar el archivo de configuraci√≥n, puedes cargarlo usando [`~PretrainedConfig.from_pretrained`]:

```py
>>> my_config = DistilBertConfig.from_pretrained("./your_model_save_path/my_config.json")
```

<Tip>
  
Tambi√©n puedes guardar los archivos de configuraci√≥n como un diccionario; o incluso guardar solo la diferencia entre tu archivo personalizado y la configuraci√≥n por defecto. Consulta la [documentaci√≥n sobre configuraci√≥n](main_classes/configuration) para ver m√°s detalles.

</Tip>

## Modelo

El siguiente paso ser√° crear un [modelo](main_classes/models). El modelo, al que a veces tambi√©n nos referimos como arquitectura, es el encargado de definir cada capa y qu√© operaciones se realizan. Los atributos como `num_hidden_layers` de la configuraci√≥n se usan para definir la arquitectura. Todos los modelos comparten una clase base, [`PreTrainedModel`], y algunos m√©todos comunes que se pueden usar para redimensionar los _embeddings_ o para recortar cabezas de auto-atenci√≥n (tambi√©n llamadas _self-attention heads_). Adem√°s, todos los modelos son subclases de [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html), [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) o [`flax.linen.Module`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html), lo que significa que son compatibles con su respectivo framework. 

<frameworkcontent>
<pt>

Carga los atributos de tu configuraci√≥n personalizada en el modelo de la siguiente forma:

```py
>>> from transformers import DistilBertModel

>>> my_config = DistilBertConfig.from_pretrained("./your_model_save_path/my_config.json")
>>> model = DistilBertModel(my_config)
```
  
Esto crea un modelo con valores aleatorios, en lugar de crearlo con los pesos del preentrenamiento, por lo que no ser√°s capaz de usar este modelo para nada √∫til hasta que no lo entrenes. El entrenamiento es un proceso costoso, tanto en cuesti√≥n de recursos como de tiempo, por lo que generalmente es mejor usar un modelo preentrenado para obtener mejores resultados m√°s r√°pido, consumiendo una fracci√≥n de los recursos que un entrenamiento completo hubiera requerido. 

Puedes crear un modelo preentrenado con [`~PreTrainedModel.from_pretrained`]:

```py
>>> model = DistilBertModel.from_pretrained("distilbert/distilbert-base-uncased")
```

Cuando cargues tus pesos del preentrenamiento, el modelo por defecto se carga autom√°ticamente si nos lo proporciona ü§ó Transformers. Sin embargo, siempre puedes reemplazar (todos o algunos de) los atributos del modelo por defecto por los tuyos:

```py
>>> model = DistilBertModel.from_pretrained("distilbert/distilbert-base-uncased", config=my_config)
```
</pt>
<tf>
  
Carga los atributos de tu configuraci√≥n personalizada en el modelo de la siguiente forma:

```py
>>> from transformers import TFDistilBertModel

>>> my_config = DistilBertConfig.from_pretrained("./your_model_save_path/my_config.json")
>>> tf_model = TFDistilBertModel(my_config)
```

Esto crea un modelo con valores aleatorios, en lugar de crearlo con los pesos del preentrenamiento, por lo que no ser√°s capaz de usar este modelo para nada √∫til hasta que no lo entrenes. El entrenamiento es un proceso costoso, tanto en cuesti√≥n de recursos como de tiempo, por lo que generalmente es mejor usar un modelo preentrenado para obtener mejores resultados m√°s r√°pido, consumiendo solo una fracci√≥n de los recursos que un entrenamiento completo hubiera requerido. 

Puedes crear un modelo preentrenado con [`~TFPreTrainedModel.from_pretrained`]:

```py
>>> tf_model = TFDistilBertModel.from_pretrained("distilbert/distilbert-base-uncased")
```

Cuando cargues tus pesos del preentrenamiento, el modelo por defecto se carga autom√°ticamente si este nos lo proporciona ü§ó Transformers. Sin embargo, siempre puedes reemplazar (todos o algunos de) los atributos del modelo por defecto por los tuyos:

```py
>>> tf_model = TFDistilBertModel.from_pretrained("distilbert/distilbert-base-uncased", config=my_config)
```
</tf>
</frameworkcontent>

### Cabezas de modelo 

En este punto del tutorial, tenemos un modelo DistilBERT base que devuelve los *hidden states* o estados ocultos. Los *hidden states* se pasan como par√°metros de entrada a la cabeza del modelo para producir la salida. ü§ó Transformers ofrece una cabeza de modelo diferente para cada tarea, siempre y cuando el modelo sea compatible para la tarea (por ejemplo, no puedes usar DistilBERT para una tarea secuencia a secuencia como la traducci√≥n).


<frameworkcontent>
<pt>

Por ejemplo,  [`DistilBertForSequenceClassification`] es un modelo DistilBERT base con una cabeza de clasificaci√≥n de secuencias. La cabeza de clasificaci√≥n de secuencias es una capa superior que precede a la recolecci√≥n de las salidas.

```py
>>> from transformers import DistilBertForSequenceClassification

>>> model = DistilBertForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

Puedes reutilizar este punto de guardado o *checkpoint* para otra tarea f√°cilmente cambiando a una cabeza de un modelo diferente. Para una tarea de respuesta a preguntas, puedes usar la cabeza del modelo [`DistilBertForQuestionAnswering`]. La cabeza de respuesta a preguntas es similar a la de clasificaci√≥n de secuencias, excepto porque consta de una capa lineal delante de la salida de los *hidden states*. 


```py
>>> from transformers import DistilBertForQuestionAnswering

>>> model = DistilBertForQuestionAnswering.from_pretrained("distilbert/distilbert-base-uncased")
```
</pt>
<tf>

Por ejemplo,  [`TFDistilBertForSequenceClassification`] es un modelo DistilBERT base con una cabeza de clasificaci√≥n de secuencias. La cabeza de clasificaci√≥n de secuencias es una capa superior que precede a la recolecci√≥n de las salidas.

```py
>>> from transformers import TFDistilBertForSequenceClassification

>>> tf_model = TFDistilBertForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

Puedes reutilizar este punto de guardado o *checkpoint* para otra tarea f√°cilmente cambiando a una cabeza de un modelo diferente. Para una tarea de respuesta a preguntas, puedes usar la cabeza del modelo [`TFDistilBertForQuestionAnswering`]. La cabeza de respuesta a preguntas es similar a la de clasificaci√≥n de secuencias, excepto porque consta de una capa lineal delante de la salida de los *hidden states*. 


```py
>>> from transformers import TFDistilBertForQuestionAnswering

>>> tf_model = TFDistilBertForQuestionAnswering.from_pretrained("distilbert/distilbert-base-uncased")
```
</tf>
</frameworkcontent>

## Tokenizer

La ultima clase base que debes conocer antes de usar un modelo con datos textuales es la clase [tokenizer](main_classes/tokenizer), que convierte el texto bruto en tensores. Hay dos tipos de *tokenizers* que puedes usar con ü§ó Transformers:

- [`PreTrainedTokenizer`]:  una implementaci√≥n de un *tokenizer* hecha en Python.
- [`PreTrainedTokenizerFast`]: un *tokenizer* de nuestra librer√≠a [ü§ó Tokenizer](https://huggingface.co/docs/tokenizers/python/latest/), basada en Rust. Este tipo de *tokenizer* es bastante m√°s r√°pido, especialmente durante la tokenizaci√≥n por lotes, gracias a estar implementado en Rust. Esta r√°pida tokenizaci√≥n tambi√©n ofrece m√©todos adicionales como el *offset mapping*, que relaciona los tokens con sus palabras o caracteres originales.

Ambos *tokenizers* son compatibles con los m√©todos comunes, como los de encodificaci√≥n y decodificaci√≥n, los m√©todos para a√±adir tokens y aquellos que manejan tokens especiales. 

<Tip warning={true}>

No todos los modelos son compatibles con un *tokenizer* r√°pido. √âchale un vistazo a esta [tabla](index#supported-frameworks) para comprobar si un modelo espec√≠fico es compatible con un *tokenizer* r√°pido.

</Tip>

Si has entrenado tu propio *tokenizer*, puedes crear uno desde tu archivo de ‚Äúvocabulario‚Äù:

```py
>>> from transformers import DistilBertTokenizer

>>> my_tokenizer = DistilBertTokenizer(vocab_file="my_vocab_file.txt", do_lower_case=False, padding_side="left")
```

Es importante recordar que los vocabularios que provienen de un *tokenizer* personalizado ser√°n diferentes a los vocabularios generados por el *tokenizer* de un modelo preentrenado. Debes usar el vocabulario de un *tokenizer* preentrenado si vas a usar un modelo preentrenado, de lo contrario las entradas no tendr√°n sentido. Crea un *tokenizer* con el vocabulario de un modelo preentrenado usando la clase [`DistilBertTokenizer`]:


```py
>>> from transformers import DistilBertTokenizer

>>> slow_tokenizer = DistilBertTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
```

Crea un *tokenizer* r√°pido con la clase [`DistilBertTokenizerFast`]:


```py
>>> from transformers import DistilBertTokenizerFast

>>> fast_tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert/distilbert-base-uncased")
```

<Tip>

Por defecto, el [`AutoTokenizer`] intentar√° cargar un *tokenizer* r√°pido. Puedes desactivar este comportamiento cambiando el par√°metro `use_fast=False` de `from_pretrained`.


</Tip>

## Extractor de Caracter√≠sticas 

Un extractor de caracter√≠sticas procesa entradas de audio e imagen. Hereda de la clase base [`~feature_extraction_utils.FeatureExtractionMixin`] y tambi√©n puede heredar de la clase [`ImageFeatureExtractionMixin`] para el procesamiento de caracter√≠sticas de las im√°genes o de la clase [`SequenceFeatureExtractor`] para el procesamiento de entradas de audio.

Dependiendo de si trabajas en una tarea de audio o de video, puedes crear un extractor de caracter√≠sticas asociado al modelo que est√©s usando. Por ejemplo, podr√≠as crear un [`ViTFeatureExtractor`] por defecto si est√°s usando [ViT](model_doc/vit) para clasificaci√≥n de im√°genes:

```py
>>> from transformers import ViTFeatureExtractor

>>> vit_extractor = ViTFeatureExtractor()
>>> print(vit_extractor)
ViTFeatureExtractor {
  "do_normalize": true,
  "do_resize": true,
  "feature_extractor_type": "ViTFeatureExtractor",
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "resample": 2,
  "size": 224
}
```

<Tip>

Si no est√°s buscando ninguna personalizaci√≥n en espec√≠fico, usa el m√©todo `from_pretrained` para cargar los par√°metros del extractor de caracter√≠sticas por defecto del modelo.

</Tip>

Puedes modificar cualquier par√°metro de [`ViTFeatureExtractor`] para crear tu extractor de caracter√≠sticas personalizado:

```py
>>> from transformers import ViTFeatureExtractor

>>> my_vit_extractor = ViTFeatureExtractor(resample="PIL.Image.BOX", do_normalize=False, image_mean=[0.3, 0.3, 0.3])
>>> print(my_vit_extractor)
ViTFeatureExtractor {
  "do_normalize": false,
  "do_resize": true,
  "feature_extractor_type": "ViTFeatureExtractor",
  "image_mean": [
    0.3,
    0.3,
    0.3
  ],
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "resample": "PIL.Image.BOX",
  "size": 224
}
```

Para las entradas de audio, puedes crear un [`Wav2Vec2FeatureExtractor`] y personalizar los par√°metros de una forma similar:


```py
>>> from transformers import Wav2Vec2FeatureExtractor

>>> w2v2_extractor = Wav2Vec2FeatureExtractor()
>>> print(w2v2_extractor)
Wav2Vec2FeatureExtractor {
  "do_normalize": true,
  "feature_extractor_type": "Wav2Vec2FeatureExtractor",
  "feature_size": 1,
  "padding_side": "right",
  "padding_value": 0.0,
  "return_attention_mask": false,
  "sampling_rate": 16000
}
```

## Procesador

Para modelos que son compatibles con tareas multimodales, ü§ó Transformers ofrece una clase *procesador* que agrupa un extractor de caracter√≠sticas y un *tokenizer* en el mismo objeto. Por ejemplo, probemos a usar el procesador [`Wav2Vec2Processor`] para una tarea de reconocimiento de voz (ASR). Un ASR transcribe el audio a texto, por lo que necesitaremos un extractor de caracter√≠sticas y un *tokenizer*.

Crea un extractor de caracter√≠sticas para manejar la entrada de audio:


```py
>>> from transformers import Wav2Vec2FeatureExtractor

>>> feature_extractor = Wav2Vec2FeatureExtractor(padding_value=1.0, do_normalize=True)
```

Crea un *tokenizer* para manejar la entrada de texto:

```py
>>> from transformers import Wav2Vec2CTCTokenizer

>>> tokenizer = Wav2Vec2CTCTokenizer(vocab_file="my_vocab_file.txt")
```

Puedes combinar el extractor de caracter√≠sticas y el *tokenizer* en el [`Wav2Vec2Processor`]:


```py
>>> from transformers import Wav2Vec2Processor

>>> processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)
```
Con dos clases base (la configuraci√≥n y el modelo) y una clase de preprocesamiento adicional (*tokenizer*, extractor de caracter√≠sticas o procesador), puedes crear cualquiera de los modelos compatibles con ü§ó Transformers. Cada una de estas clases son configurables, permiti√©ndote usar sus atributos espec√≠ficos. Puedes crear un modelo para entrenarlo de una forma f√°cil, o modificar un modelo preentrenado disponible para especializarlo.
