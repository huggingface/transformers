<!--Copyright 2024 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

丘멆잺 Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Exportar a TorchScript

> [!TIP]
> Este es el comienzo de nuestros experimentos con TorchScript y todav칤a estamos explorando sus capacidades con modelos de variables de entrada. Es un tema de inter칠s para nosotros y profundizaremos en nuestro an치lisis en las pr칩ximas versiones, con m치s ejemplos de c칩digo, una implementaci칩n m치s flexible y comparativas de rendimiento comparando c칩digos basados en Python con TorchScript compilado.

De acuerdo con la documentaci칩n de TorchScript: 

> "TorchScript es una manera de crear modelos serializables y optimizables a partir del c칩digo PyTorch."

Hay dos m칩dulos de PyTorch, [JIT y TRACE](https://pytorch.org/docs/stable/jit.html), que permiten a los desarrolladores exportar sus modelos para ser reusados en otros programas, como los programas de C++ orientados a la eficiencia.

Nosotros proveemos una interface que te permite exportar los modelos 游뱅Transformers a TorchScript para que puedan ser reusados en un entorno diferente al de los programas Python basados en PyTorch. Aqu칤 explicamos como exportar y usar nuestros modelos utilizando TorchScript.

Exportar un modelo requiere de dos cosas:

- La instanciaci칩n del modelo con la bandera TorchScript.
- Un paso hacia adelante con entradas ficticias.

Estas necesidades implican varias cosas de las que los desarrolladores deben tener cuidado, como se detalla a continuaci칩n.

## Bandera TorchScript y pesos atados.

La bandera `torchscript` es necesaria porque la mayor칤a de los modelos de lenguaje de 游뱅Transformers tienen pesos atados entre su `capa de incrustaci칩n` (`Embedding`) y su `capa de decodificaci칩n` (`Decoding`). TorchScript no te permite exportar modelos que tienen pesos atados, por lo que es necesario desatar y clonar los pesos de antemano.

Los modelos instanciados con la bandera `torchscript` tienen su `capa de incrustaci칩n` (`Embedding`) y su `capa de decodificaci칩n` (`Decoding`) separadas, lo que significa que no deben ser entrenados m치s adelante. Entrenar desincronizar칤a las dos capas, lo que llevar칤a a resultados inesperados.

Esto no es as칤 para los modelos que no tienen una cabeza de modelo de lenguaje, ya que esos modelos no tienen pesos atados. Estos modelos pueden ser exportados de manera segura sin la bandera `torchscript`.

## Entradas ficticias y longitudes est치ndar

Las entradas ficticias se utilizan para un paso del modelo hacia adelante. Mientras los valores de las entradas se propagan a trav칠s de las capas, PyTorch realiza un seguimiento de las diferentes operaciones ejecutadas en cada tensor. Estas operaciones registradas se utilizan luego para crear *la traza* del modelo.
La traza se crea en relaci칩n con las dimensiones de las entradas. Por lo tanto, est치 limitada por las dimensiones de la entrada ficticia y no funcionar치 para ninguna otra longitud de secuencia o tama침o de lote. Cuando se intenta con un tama침o diferente, se genera el siguiente error:

```
`El tama침o expandido del tensor (3) debe coincidir con el tama침o existente (7) en la dimensi칩n no singleton 2`.
```

Recomendamos trazar el modelo con un tama침o de entrada ficticio al menos tan grande como la entrada m치s grande con la que se alimentar치 al modelo durante la inferencia. El relleno puede ayudar a completar los valores faltantes. Sin embargo, dado que el modelo se traza con un tama침o de entrada m치s grande, las dimensiones de la matriz tambi칠n ser치n grandes, lo que resultar치 en m치s c치lculos.

Ten cuidado con el n칰mero total de operaciones realizadas en cada entrada y sigue de cerca el rendimiento al exportar modelos con longitudes de secuencia variables.

## Usando TorchScript en Python

Esta secci칩n demuestra c칩mo guardar y cargar modelos, as칤 como c칩mo usar la traza para la inferencia.

### Guardando un modelo

Para exportar un `BertModel` con TorchScript, instancia `BertModel` a partir de la clase `BertConfig` y luego gu치rdalo en disco bajo el nombre de archivo `traced_bert.pt`:

```python
from transformers import BertModel, BertTokenizer, BertConfig
import torch

enc = BertTokenizer.from_pretrained("bert-base-uncased")

# Tokenizing input text
text = "[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]"
tokenized_text = enc.tokenize(text)

# Masking one of the input tokens
masked_index = 8
tokenized_text[masked_index] = "[MASK]"
indexed_tokens = enc.convert_tokens_to_ids(tokenized_text)
segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]

# Creating a dummy input
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])
dummy_input = [tokens_tensor, segments_tensors]

# Initializing the model with the torchscript flag
# Flag set to True even though it is not necessary as this model does not have an LM Head.
config = BertConfig(
    vocab_size_or_config_json_file=32000,
    hidden_size=768,
    num_hidden_layers=12,
    num_attention_heads=12,
    intermediate_size=3072,
    torchscript=True,
)

# Instantiating the model
model = BertModel(config)

# The model needs to be in evaluation mode
model.eval()

# If you are instantiating the model with *from_pretrained* you can also easily set the TorchScript flag
model = BertModel.from_pretrained("bert-base-uncased", torchscript=True)

# Creating the trace
traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])
torch.jit.save(traced_model, "traced_bert.pt")
```
### Cargando un modelo

Ahora puedes cargar el `BertModel` guardado anteriormente, `traced_bert.pt`, desde el disco y usarlo en la entrada ficticia (`dummy_input`) previamente inicializada:

```python
loaded_model = torch.jit.load("traced_bert.pt")
loaded_model.eval()

all_encoder_layers, pooled_output = loaded_model(*dummy_input)
```

## Usando un modelo trazado para inferencia

Utiliza el modelo trazado para inferencia utilizando su m칠todo `_call_` dunder:

```python
traced_model(tokens_tensor, segments_tensors)
```
## Despliega modelos TorchScript de Hugging Face en AWS con el Neuron SDK

AWS introdujo la familia de instancias [Amazon EC2 Inf1](https://aws.amazon.com/ec2/instance-types/inf1/) para inferencia de aprendizaje autom치tico de alto rendimiento y bajo costo en la nube. Las instancias Inf1 est치n alimentadas por el chip AWS Inferentia, un acelerador de hardware personalizado que se especializa en cargas de trabajo de inferencia de aprendizaje profundo. [AWS Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/#) es el SDK para Inferentia que admite el trazado y la optimizaci칩n de modelos de transformers para implementaci칩n en Inf1. El SDK Neuron proporciona:

1. Una API f치cil de usar con un solo cambio de l칤nea de c칩digo para trazar y optimizar un modelo TorchScript para inferencia en la nube.

2. Optimizaciones de rendimiento listas para usar [para mejorar el rendimiento y el costo](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/benchmark/>).

3. Soporte para modelos de transformers de Hugging Face construidos tanto con [PyTorch](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/bert_tutorial/tutorial_pretrained_bert.html) como con [TensorFlow](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/tensorflow/huggingface_bert/huggingface_bert.html).

### Implicaciones

Los modelos transformers basados en la arquitectura [BERT (Bidirectional Encoder Representations from Transformers)](https://huggingface.co/docs/transformers/main/model_doc/bert), o sus variantes como [distilBERT](https://huggingface.co/docs/transformers/main/model_doc/distilbert) y [roBERTa](https://huggingface.co/docs/transformers/main/model_doc/roberta), funcionan mejor en Inf1 para tareas no generativas como la respuesta a preguntas extractivas, la clasificaci칩n de secuencias y la clasificaci칩n de tokens. Sin embargo, las tareas de generaci칩n de texto a칰n pueden adaptarse para ejecutarse en Inf1 seg칰n este [tutorial de AWS Neuron MarianMT](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/transformers-marianmt.html). Se puede encontrar m치s informaci칩n sobre los modelos que se pueden convertir f치cilmente para usar en Inferentia en la secci칩n de [Model Architecture Fit](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/models/models-inferentia.html#models-inferentia) de la documentaci칩n de Neuron.

### Dependencias

El uso de AWS Neuron para convertir modelos requiere un [entorno de Neuron SDK](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-frameworks/pytorch-neuron/index.html#installation-guide) que viene preconfigurado en [la AMI de AWS Deep Learning](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-launching.html).

### Convertir un modelo para AWS Neuron

Convierte un modelo para AWS NEURON utilizando el mismo c칩digo de [Uso de TorchScript en Python](torchscript#using-torchscript-in-python) para trazar un `BertModel`. Importa la extensi칩n del framework `torch.neuron` para acceder a los componentes del Neuron SDK a trav칠s de una API de Python:

```python
from transformers import BertModel, BertTokenizer, BertConfig
import torch
import torch.neuron
```
Solo necesitas la linea sigueda:

```diff
- torch.jit.trace(model, [tokens_tensor, segments_tensors])
+ torch.neuron.trace(model, [token_tensor, segments_tensors])
```

Esto permite que el Neuron SDK trace el modelo y lo optimice para las instancias Inf1.

Para obtener m치s informaci칩n sobre las caracter칤sticas, herramientas, tutoriales de ejemplo y 칰ltimas actualizaciones del AWS Neuron SDK, consulta [la documentaci칩n de AWS NeuronSDK](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html).