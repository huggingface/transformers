<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Descripci√≥n general de los tokenizadores

[[open-in-colab]]

En esta p√°gina, veremos m√°s de cerca la tokenizaci√≥n.

<Youtube id="VFp38yj8h3A"/>

Como vimos en [el tutorial de preprocesamiento](preprocessing), tokenizar un texto es dividirlo en palabras o subpalabras, que luego se convierten en indices o ids a trav√©s de una tabla de b√∫squeda. Convertir palabras o subpalabras en ids es sencillo, as√≠ que en esta descripci√≥n general, nos centraremos en dividir un texto en palabras o subpalabras (es decir, tokenizar un texto). M√°s espec√≠ficamente, examinaremos los tres principales tipos de tokenizadores utilizados en ü§ó Transformers: [Byte-Pair Encoding (BPE)](#byte-pair-encoding), [WordPiece](#wordpiece) y [SentencePiece](#sentencepiece), y mostraremos ejemplos de qu√© tipo de tokenizador se utiliza en cada modelo.

Ten en cuenta que en las p√°ginas de los modelos, puedes ver la documentaci√≥n del tokenizador asociado para saber qu√© tipo de tokenizador se utiliz√≥ en el modelo preentrenado. Por ejemplo, si miramos [BertTokenizer](https://huggingface.co/docs/transformers/en/model_doc/bert#transformers.BertTokenizer), podemos ver que dicho modelo utiliza [WordPiece](#wordpiece).

## Introducci√≥n

Dividir un texto en trozos m√°s peque√±os es m√°s dif√≠cil de lo que parece, y hay m√∫ltiples formas de hacerlo. Por ejemplo, veamos la oraci√≥n `"Don't you love ü§ó Transformers? We sure do."`

<Youtube id="nhJxYji1aho"/>

Una forma sencilla de tokenizar este texto es dividirlo por espacios, lo que dar√≠a:

```
["Don't", "you", "love", "ü§ó", "Transformers?", "We", "sure", "do."]
```

Este es un primer paso sensato, pero si miramos los tokens `"Transformers?"` y `"do."`, notamos que las puntuaciones est√°n unidas a las palabras `"Transformer"` y `"do"`, lo que es sub√≥ptimo. Deber√≠amos tener en cuenta la puntuaci√≥n para que un modelo no tenga que aprender una representaci√≥n diferente de una palabra y cada posible s√≠mbolo de puntuaci√≥n que podr√≠a seguirle, lo que explotar√≠a el n√∫mero de representaciones que el modelo tiene que aprender. Teniendo en cuenta la puntuaci√≥n, tokenizar nuestro texto dar√≠a:

```
["Don", "'", "t", "you", "love", "ü§ó", "Transformers", "?", "We", "sure", "do", "."]
```

Mejor. Sin embargo, es desventajoso c√≥mo la tokenizaci√≥n trata la palabra `"Don't"`. `"Don't"` significa `"do not"`, as√≠ que ser√≠a mejor tokenizada como `["Do", "n't"]`. Aqu√≠ es donde las cosas comienzan a complicarse, y es la razon por la que cada modelo tiene su propio tipo de tokenizador. Dependiendo de las reglas que apliquemos para tokenizar un texto, se genera una salida tokenizada diferente para el mismo texto. Un modelo preentrenado solo se desempe√±a correctamente si se le proporciona una entrada que fue tokenizada con las mismas reglas que se utilizaron para tokenizar sus datos de entrenamiento.

[spaCy](https://spacy.io/) y [Moses](http://www.statmt.org/moses/?n=Development.GetStarted) son dos tokenizadores basados en reglas populares. Al aplicarlos en nuestro ejemplo, *spaCy* y *Moses* generar√≠an algo como:

```
["Do", "n't", "you", "love", "ü§ó", "Transformers", "?", "We", "sure", "do", "."]
```

Como se puede ver, aqu√≠ se utiliza tokenizaci√≥n de espacio y puntuaci√≥n, as√≠ como tokenizaci√≥n basada en reglas. La tokenizaci√≥n de espacio y puntuaci√≥n y la tokenizaci√≥n basada en reglas son ambos ejemplos de tokenizaci√≥n de palabras, que se define de manera simple como dividir oraciones en palabras. Aunque es la forma m√°s intuitiva de dividir textos en trozos m√°s peque√±os, este m√©todo de tokenizaci√≥n puede generar problemas para corpus de texto masivos. En este caso, la tokenizaci√≥n de espacio y puntuaci√≥n suele generar un vocabulario muy grande (el conjunto de todas las palabras y tokens √∫nicos utilizados). *Ej.*, [Transformer XL](https://huggingface.co/docs/transformers/main/en/model_doc/transfo-xl) utiliza tokenizaci√≥n de espacio y puntuaci√≥n, lo que resulta en un tama√±o de vocabulario de 267,735.

Un tama√±o de vocabulario tan grande fuerza al modelo a tener una matriz de embeddings enormemente grande como capa de entrada y salida, lo que causa un aumento tanto en la complejidad de memoria como en la complejidad de tiempo. En general, los modelos de transformadores rara vez tienen un tama√±o de vocabulario mayor que 50,000, especialmente si est√°n preentrenados solo en un idioma.

Entonces, si la simple tokenizaci√≥n de espacios y puntuaci√≥n es insatisfactoria, ¬øpor qu√© no tokenizar simplemente en caracteres?

<Youtube id="ssLq_EK2jLE"/>

Aunque la tokenizaci√≥n de caracteres es muy simple y reducir√≠a significativamente la complejidad de memoria y tiempo, hace que sea mucho m√°s dif√≠cil para el modelo aprender representaciones de entrada significativas. *Ej.* aprender una representaci√≥n independiente del contexto para la letra `"t"` es mucho m√°s dif√≠cil que aprender una representaci√≥n independiente del contexto para la palabra `"today"`. Por lo tanto, la tokenizaci√≥n de caracteres suele acompa√±arse de una p√©rdida de rendimiento. As√≠ que para obtener lo mejor de ambos mundos, los modelos de transformadores utilizan un h√≠brido entre la tokenizaci√≥n de nivel de palabra y de nivel de car√°cter llamada **tokenizaci√≥n de subpalabras**.

## Tokenizaci√≥n de subpalabras

<Youtube id="zHvTiHr506c"/>

Los algoritmos de tokenizaci√≥n de subpalabras se basan en el principio de que las palabras frecuentemente utilizadas no deber√≠an dividirse en subpalabras m√°s peque√±as, pero las palabras raras deber√≠an descomponerse en subpalabras significativas. Por ejemplo, `"annoyingly"` podr√≠a considerarse una palabra rara y descomponerse en `"annoying"` y `"ly"`. Ambas `"annoying"` y `"ly"` como subpalabras independientes aparecer√≠an con m√°s frecuencia al mismo tiempo que se mantiene el significado de `"annoyingly"` por el significado compuesto de `"annoying"` y `"ly"`. Esto es especialmente √∫til en lenguas aglutinantes como el turco, donde puedes formar palabras complejas (casi) arbitrariamente largas concatenando subpalabras.

La tokenizaci√≥n de subpalabras permite al modelo tener un tama√±o de vocabulario razonable mientras puede aprender representaciones contextuales independientes significativas. Adem√°s, la tokenizaci√≥n de subpalabras permite al modelo procesar palabras que nunca ha visto antes, descomponi√©ndolas en subpalabras conocidas. Por ejemplo, el tokenizador [BertTokenizer](https://huggingface.co/docs/transformers/en/model_doc/bert#transformers.BertTokenizer) tokeniza `"I have a new GPU!"` de la siguiente manera:

```py
>>> from transformers import BertTokenizer

>>> tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-uncased")
>>> tokenizer.tokenize("I have a new GPU!")
["i", "have", "a", "new", "gp", "##u", "!"]
```

Debido a que estamos considerando el modelo sin may√∫sculas, la oraci√≥n se convirti√≥ a min√∫sculas primero. Podemos ver que las palabras `["i", "have", "a", "new"]` est√°n presentes en el vocabulario del tokenizador, pero la palabra `"gpu"` no. En consecuencia, el tokenizador divide `"gpu"` en subpalabras conocidas: `["gp" y "##u"]`. `"##"` significa que el resto del token deber√≠a adjuntarse al anterior, sin espacio (para decodificar o revertir la tokenizaci√≥n).

Como otro ejemplo, el tokenizador [XLNetTokenizer](https://huggingface.co/docs/transformers/en/model_doc/xlnet#transformers.XLNetTokenizer) tokeniza nuestro texto de ejemplo anterior de la siguiente manera:

```py
>>> from transformers import XLNetTokenizer

>>> tokenizer = XLNetTokenizer.from_pretrained("xlnet/xlnet-base-cased")
>>> tokenizer.tokenize("Don't you love ü§ó Transformers? We sure do.")
["‚ñÅDon", "'", "t", "‚ñÅyou", "‚ñÅlove", "‚ñÅ", "ü§ó", "‚ñÅ", "Transform", "ers", "?", "‚ñÅWe", "‚ñÅsure", "‚ñÅdo", "."]
```

Hablaremos del significado de esos `"‚ñÅ"` cuando veamos [SentencePiece](#sentencepiece). Como se puede ver, la palabra rara `"Transformers"` se ha dividido en las subpalabras m√°s frecuentes `"Transform"` y `"ers"`.

Ahora, veamos c√≥mo funcionan los diferentes algoritmos de tokenizaci√≥n de subpalabras. Ten en cuenta que todos esos algoritmos de tokenizaci√≥n se basan en alguna forma de entrenamiento que usualmente se realiza en el corpus en el que se entrenar√° el modelo correspondiente.

<a id='byte-pair-encoding'></a>

### Byte-Pair Encoding (BPE)

La Codificaci√≥n por Pares de Bytes (BPE por sus siglas en ingl√©s) fue introducida en [Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015)](https://huggingface.co/papers/1508.07909). BPE se basa en un pre-tokenizador que divide los datos de entrenamiento en palabras. La pre-tokenizaci√≥n puede ser tan simple como la tokenizaci√≥n por espacio, por ejemplo, [GPT-2](https://huggingface.co/docs/transformers/en/model_doc/gpt2), [RoBERTa](https://huggingface.co/docs/transformers/en/model_doc/roberta). La pre-tokenizaci√≥n m√°s avanzada incluye la tokenizaci√≥n basada en reglas, por ejemplo, [XLM](https://huggingface.co/docs/transformers/en/model_doc/xlm), [FlauBERT](https://huggingface.co/docs/transformers/en/model_doc/flaubert) que utiliza Moses para la mayor√≠a de los idiomas, o [GPT](https://huggingface.co/docs/transformers/en/model_doc/openai-gpt) que utiliza spaCy y ftfy, para contar la frecuencia de cada palabra en el corpus de entrenamiento.

Despu√©s de la pre-tokenizaci√≥n, se ha creado un conjunto de palabras √∫nicas y ha determinado la frecuencia con la que cada palabra apareci√≥ en los datos de entrenamiento. A continuaci√≥n, BPE crea un vocabulario base que consiste en todos los s√≠mbolos que aparecen en el conjunto de palabras √∫nicas y aprende reglas de fusi√≥n para formar un nuevo s√≠mbolo a partir de dos s√≠mbolos del vocabulario base. Lo hace hasta que el vocabulario ha alcanzado el tama√±o de vocabulario deseado. Tenga en cuenta que el tama√±o de vocabulario deseado es un hiperpar√°metro que se debe definir antes de entrenar el tokenizador.

Por ejemplo, supongamos que despu√©s de la pre-tokenizaci√≥n, se ha determinado el siguiente conjunto de palabras, incluyendo su frecuencia:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

En consecuencia, el vocabulario base es `["b", "g", "h", "n", "p", "s", "u"]`. Dividiendo todas las palabras en s√≠mbolos del vocabulario base, obtenemos:

```
("h" "u" "g", 10), ("p" "u" "g", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "u" "g" "s", 5)
```

Luego, BPE cuenta la frecuencia de cada par de s√≠mbolos posible y selecciona el par de s√≠mbolos que ocurre con m√°s frecuencia. En el ejemplo anterior, `"h"` seguido de `"u"` est√° presente _10 + 5 = 15_ veces (10 veces en las 10 ocurrencias de `"hug"`, 5 veces en las 5 ocurrencias de `"hugs"`). Sin embargo, el par de s√≠mbolos m√°s frecuente es `"u"` seguido de `"g"`, que ocurre _10 + 5 + 5 = 20_ veces en total. Por lo tanto, la primera regla de fusi√≥n que aprende el tokenizador es agrupar todos los s√≠mbolos `"u"` seguidos de un s√≠mbolo `"g"` juntos. A continuaci√≥n, `"ug"` se agrega al vocabulario. El conjunto de palabras entonces se convierte en

```
("h" "ug", 10), ("p" "ug", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "ug" "s", 5)
```

Seguidamente, BPE identifica el pr√≥ximo par de s√≠mbolos m√°s com√∫n. Es `"u"` seguido de `"n"`, que ocurre 16 veces. `"u"`, `"n"` se fusionan en `"un"` y se agregan al vocabulario. El pr√≥ximo par de s√≠mbolos m√°s frecuente es `"h"` seguido de `"ug"`, que ocurre 15 veces. De nuevo, el par se fusiona y `"hug"` se puede agregar al vocabulario.

En este momento, el vocabulario es `["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"]` y nuestro conjunto de palabras √∫nicas se representa como:

```
("hug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("hug" "s", 5)
```

Suponiendo que el entrenamiento por Byte-Pair Encoding se detuviera en este punto, las reglas de combinaci√≥n aprendidas se aplicar√≠an entonces a nuevas palabras (siempre que esas nuevas palabras no incluyan s√≠mbolos que no estuvieran en el vocabulario base). Por ejemplo, la palabra `"bug"` se tokenizar√≠a como `["b", "ug"]`, pero `"mug"` se tokenizar√≠a como `["<unk>", "ug"]` ya que el s√≠mbolo `"m"` no est√° en el vocabulario base. En general, las letras individuales como `"m"` no se reemplazan por el s√≠mbolo `"<unk>"` porque los datos de entrenamiento usualmente incluyen al menos una ocurrencia de cada letra, pero es probable que suceda para caracteres especiales como los emojis.

Como se mencion√≥ anteriormente, el tama√±o del vocabulario, es decir, el tama√±o del vocabulario base + el n√∫mero de combinaciones, es un hiperpar√°metro que se debe elegir. Por ejemplo, [GPT](https://huggingface.co/docs/transformers/en/model_doc/openai-gpt) tiene un tama√±o de vocabulario de 40,478 ya que tienen 478 caracteres base y eligieron detener el entrenamiento despu√©s de 40,000 combinaciones.

#### Byte-level BPE

Un vocabulario base que incluya todos los caracteres base posibles puede ser bastante extenso si, por ejemplo, se consideran todos los caracteres unicode como caracteres base. Para tener un vocabulario base mejor, [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) utiliza bytes como vocabulario base, lo que es un truco astuto para forzar el vocabulario base a ser de tama√±o 256 mientras se asegura de que cada car√°cter base est√© incluido en el vocabulario. Con algunas reglas adicionales para tratar con la puntuaci√≥n, el tokenizador de GPT2 puede tokenizar cualquier texto sin la necesidad del s√≠mbolo `<unk>`. [GPT-2](https://huggingface.co/docs/transformers/en/model_doc/gpt2) tiene un tama√±o de vocabulario de 50,257, lo que corresponde a los 256 tokens base de bytes, un token especial de fin de texto y los s√≠mbolos aprendidos con 50,000 combinaciones.

<a id='wordpiece'></a>

### WordPiece

WordPiece es el algoritmo de tokenizaci√≥n de subpalabras utilizado por [BERT](https://huggingface.co/docs/transformers/en/model_doc/bert), [DistilBERT](https://huggingface.co/docs/transformers/main/en/model_doc/distilbert) y [Electra](https://huggingface.co/docs/transformers/main/en/model_doc/electra). El algoritmo fue descrito en [Japanese and Korean Voice Search (Schuster et al., 2012)](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf) y es muy similar a BPE. WordPiece inicializa el vocabulario para incluir cada car√°cter presente en los datos de entrenamiento y aprende progresivamente un n√∫mero determinado de reglas de fusi√≥n. A diferencia de BPE, WordPiece no elige el par de s√≠mbolos m√°s frecuente, sino el que maximiza la probabilidad de los datos de entrenamiento una vez agregado al vocabulario.

¬øQu√© significa esto exactamente? Refiri√©ndonos al ejemplo anterior, maximizar la probabilidad de los datos de entrenamiento es equivalente a encontrar el par de s√≠mbolos cuya probabilidad dividida entre las probabilidades de su primer s√≠mbolo seguido de su segundo s√≠mbolo es la mayor entre todos los pares de s√≠mbolos. *Ej.* `"u"` seguido de `"g"` solo habr√≠a sido combinado si la probabilidad de `"ug"` dividida entre `"u"` y `"g"` habr√≠a sido mayor que para cualquier otro par de s√≠mbolos. Intuitivamente, WordPiece es ligeramente diferente a BPE en que eval√∫a lo que _pierde_ al fusionar dos s√≠mbolos para asegurarse de que _valga la pena_.

<a id='unigram'></a>

### Unigram

Unigram es un algoritmo de tokenizaci√≥n de subpalabras introducido en [Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates (Kudo, 2018)](https://huggingface.co/papers/1804.10959). A diferencia de BPE o WordPiece, Unigram inicializa su vocabulario base con un gran n√∫mero de s√≠mbolos y progresivamente recorta cada s√≠mbolo para obtener un vocabulario m√°s peque√±o. El vocabulario base podr√≠a corresponder, por ejemplo, a todas las palabras pre-tokenizadas y las subcadenas m√°s comunes. Unigram no se utiliza directamente para ninguno de los modelos transformers, pero se utiliza en conjunto con [SentencePiece](#sentencepiece).

En cada paso de entrenamiento, el algoritmo Unigram define una p√©rdida (a menudo definida como la probabilidad logar√≠tmica) sobre los datos de entrenamiento dados el vocabulario actual y un modelo de lenguaje unigram. Luego, para cada s√≠mbolo en el vocabulario, el algoritmo calcula cu√°nto aumentar√≠a la p√©rdida general si el s√≠mbolo se eliminara del vocabulario. Luego, Unigram elimina un porcentaje `p` de los s√≠mbolos cuyo aumento de p√©rdida es el m√°s bajo (siendo `p` generalmente 10% o 20%), es decir, aquellos s√≠mbolos que menos afectan la p√©rdida general sobre los datos de entrenamiento. Este proceso se repite hasta que el vocabulario haya alcanzado el tama√±o deseado. El algoritmo Unigram siempre mantiene los caracteres base para que cualquier palabra pueda ser tokenizada.

Debido a que Unigram no se basa en reglas de combinaci√≥n (en contraste con BPE y WordPiece), el algoritmo tiene varias formas de tokenizar nuevo texto despu√©s del entrenamiento. Por ejemplo, si un tokenizador Unigram entrenado exhibe el vocabulario:

```
["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"],
```

`"hugs"` podr√≠a ser tokenizado tanto como `["hug", "s"]`, `["h", "ug", "s"]` o `["h", "u", "g", "s"]`. ¬øCu√°l elegir? Unigram guarda la probabilidad de cada token en el corpus de entrenamiento junto con el vocabulario, para que la probabilidad de que cada posible tokenizaci√≥n pueda ser computada despu√©s del entrenamiento. El algoritmo simplemente elige la tokenizaci√≥n m√°s probable en la pr√°ctica, pero tambi√©n ofrece la posibilidad de muestrear una posible tokenizaci√≥n seg√∫n sus probabilidades.

Esas probabilidades est√°n definidas por la p√©rdida en la que se entrena el tokenizador. Suponiendo que los datos de entrenamiento constan de las palabras \\(x_{1}, \dots, x_{N}\\) y que el conjunto de todas las posibles tokenizaciones para una palabra \\(x_{i}\\) se define como \\(S(x_{i})\\), entonces la p√©rdida general se define como:

$$\mathcal{L} = -\sum_{i=1}^{N} \log \left ( \sum_{x \in S(x_{i})} p(x) \right )$$

<a id='sentencepiece'></a>

### SentencePiece

Todos los algoritmos de tokenizaci√≥n descritos hasta ahora tienen el mismo problema: se asume que el texto de entrada utiliza espacios para separar palabras. Sin embargo, no todos los idiomas utilizan espacios para separar palabras. Una posible soluci√≥n es utilizar pre-tokenizadores espec√≠ficos del idioma, *ej.* [XLM](https://huggingface.co/docs/transformers/en/model_doc/xlm) utiliza un pre-tokenizador espec√≠fico para chino, japon√©s y tailand√©s. Para resolver este problema de manera m√°s general, [SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (Kudo et al., 2018)](https://huggingface.co/papers/1808.06226) trata el texto de entrada como una corriente de entrada bruta, por lo que incluye el espacio en el conjunto de caracteres para utilizar. Luego utiliza el algoritmo BPE o unigram para construir el vocabulario apropiado.

Por ejemplo, [`XLNetTokenizer`](https://huggingface.co/docs/transformers/en/model_doc/xlnet#transformers.XLNetTokenizer) utiliza SentencePiece, raz√≥n por la cual en el ejemplo anterior se incluy√≥ el car√°cter `"‚ñÅ"` en el vocabulario. Decodificar con SentencePiece es muy f√°cil, ya que todos los tokens pueden simplemente concatenarse y `"‚ñÅ"` se reemplaza por un espacio.

Todos los modelos transformers de nuestra biblioteca que utilizan SentencePiece lo utilizan en combinaci√≥n con Unigram. Ejemplos de los modelos que utilizan SentencePiece son [ALBERT](https://huggingface.co/docs/transformers/en/model_doc/albert), [XLNet](https://huggingface.co/docs/transformers/en/model_doc/xlnet), [Marian](https://huggingface.co/docs/transformers/en/model_doc/marian) y [T5](https://huggingface.co/docs/transformers/main/en/model_doc/t5).
