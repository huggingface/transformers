============================= test session starts ==============================
platform linux -- Python 3.10.12, pytest-8.4.2, pluggy-1.6.0 -- /home/ilyas/transformers/.docker/bin/python3
cachedir: .pytest_cache
hypothesis profile 'default'
rootdir: /home/ilyas/transformers
configfile: pyproject.toml
plugins: anyio-4.11.0, xdist-3.8.0, asyncio-1.3.0, timeout-2.4.0, rich-0.2.0, rerunfailures-15.1, hypothesis-6.148.2, order-1.3.0
asyncio: mode=strict, debug=False, asyncio_default_fixture_loop_scope=function, asyncio_default_test_loop_scope=function
created: 64/64 workers
64 workers [477 items]

scheduling tests via LoadScheduling

tests/models/align/test_modeling_align.py::AlignVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/aimv2/test_modeling_aimv2.py::Aimv2VisionModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/align/test_modeling_align.py::AlignModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/aimv2/test_modeling_aimv2.py::Aimv2ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/bart/test_modeling_bart.py::BartStandaloneDecoderModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/aria/test_modeling_aria.py::AriaForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/altclip/test_modeling_altclip.py::AltCLIPTextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/bark/test_modeling_bark.py::BarkSemanticModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/aya_vision/test_modeling_aya_vision.py::AyaVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/apertus/test_modeling_apertus.py::ApertusModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/audioflamingo3/test_modeling_audioflamingo3.py::AudioFlamingo3ForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/big_bird/test_modeling_big_bird.py::BigBirdModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/bit/test_modeling_bit.py::BitModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/bert/test_modeling_bert.py::BertModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/blip/test_modeling_blip.py::BlipVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/blip/test_modeling_blip.py::BlipTextRetrievalModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/blip_2/test_modeling_blip_2.py::Blip2TextModelWithProjectionTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/bark/test_modeling_bark.py::BarkFineModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/blip_2/test_modeling_blip_2.py::Blip2TextRetrievalModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/blenderbot_small/test_modeling_blenderbot_small.py::BlenderbotSmallModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/blt/test_modeling_blt.py::BltModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/blenderbot/test_modeling_blenderbot.py::BlenderbotModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/bigbird_pegasus/test_modeling_bigbird_pegasus.py::BigBirdPegasusStandaloneDecoderModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/blip/test_modeling_blip_text.py::BlipTextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/blip_2/test_modeling_blip_2.py::Blip2ForConditionalGenerationDecoderOnlyTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/blip/test_modeling_blip.py::BlipModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/bros/test_modeling_bros.py::BrosModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/chameleon/test_modeling_chameleon.py::ChameleonModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/clipseg/test_modeling_clipseg.py::CLIPSegVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/clap/test_modeling_clap.py::ClapTextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/clip/test_modeling_clip.py::CLIPVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/chinese_clip/test_modeling_chinese_clip.py::ChineseCLIPModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/codegen/test_modeling_codegen.py::CodeGenModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/convnextv2/test_modeling_convnextv2.py::ConvNextV2ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/clipseg/test_modeling_clipseg.py::CLIPSegModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/cohere2/test_modeling_cohere2.py::CohereModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/chinese_clip/test_modeling_chinese_clip.py::ChineseCLIPTextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/clip/test_modeling_clip.py::CLIPModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/convbert/test_modeling_convbert.py::ConvBertModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/csm/test_modeling_csm.py::CsmForConditionalGenerationTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/clvp/test_modeling_clvp.py::ClvpDecoderTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/cohere2_vision/test_modeling_cohere2_vision.py::Cohere2ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/cvt/test_modeling_cvt.py::CvtModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/colqwen2/test_modeling_colqwen2.py::ColQwen2ForRetrievalModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/d_fine/test_modeling_d_fine.py::DFineModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/dac/test_modeling_dac.py::DacModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/data2vec/test_modeling_data2vec_text.py::Data2VecTextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/deepseek_v2/test_modeling_deepseek_v2.py::DeepseekV2ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/deberta_v2/test_modeling_deberta_v2.py::DebertaV2ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/dbrx/test_modeling_dbrx.py::DbrxModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/deepseek_vl/test_modeling_deepseek_vl.py::DeepseekVLModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/deformable_detr/test_modeling_deformable_detr.py::DeformableDetrModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/dinov2/test_modeling_dinov2.py::Dinov2ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/dpr/test_modeling_dpr.py::DPRModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/donut/test_modeling_donut_swin.py::DonutSwinModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/detr/test_modeling_detr.py::DetrModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/edgetam/test_modeling_edgetam.py::EdgeTamModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/efficientnet/test_modeling_efficientnet.py::EfficientNetModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/dpt/test_modeling_dpt_auto_backbone.py::DPTModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/dinov3_convnext/test_modeling_dinov3_convnext.py::DINOv3ConvNextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/depth_anything/test_modeling_depth_anything.py::DepthAnythingModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/emu3/test_modeling_emu3.py::Emu3Text2TextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/distilbert/test_modeling_distilbert.py::DistilBertModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/diffllama/test_modeling_diffllama.py::DiffLlamaModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw11] [  0%] PASSED tests/models/altclip/test_modeling_altclip.py::AltCLIPTextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/altclip/test_modeling_altclip.py::AltCLIPModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw27] [  0%] PASSED tests/models/chinese_clip/test_modeling_chinese_clip.py::ChineseCLIPTextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/chinese_clip/test_modeling_chinese_clip.py::ChineseCLIPVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw5] [  0%] PASSED tests/models/bark/test_modeling_bark.py::BarkSemanticModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/bark/test_modeling_bark.py::BarkCoarseModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw43] [  0%] PASSED tests/models/bark/test_modeling_bark.py::BarkFineModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/bart/test_modeling_bart.py::BartModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw57] [  1%] PASSED tests/models/emu3/test_modeling_emu3.py::Emu3Text2TextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/emu3/test_modeling_emu3.py::Emu3Vision2TextModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw6] [  1%] PASSED tests/models/blip_2/test_modeling_blip_2.py::Blip2TextModelWithProjectionTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/blip_2/test_modeling_blip_2.py::Blip2VisionModelWithProjectionTest::test_torch_export <- tests/test_modeling_common.py 
[gw59] [  1%] PASSED tests/models/blip/test_modeling_blip.py::BlipVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/blip/test_modeling_blip.py::BlipTextModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw52] [  1%] PASSED tests/models/blip/test_modeling_blip_text.py::BlipTextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/blip_2/test_modeling_blip_2.py::Blip2VisionModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw32] [  1%] PASSED tests/models/clipseg/test_modeling_clipseg.py::CLIPSegVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/clipseg/test_modeling_clipseg.py::CLIPSegTextModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw1] [  2%] PASSED tests/models/aimv2/test_modeling_aimv2.py::Aimv2VisionModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/aimv2/test_modeling_aimv2.py::Aimv2TextModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw24] [  2%] PASSED tests/models/clap/test_modeling_clap.py::ClapTextModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw5] [  2%] PASSED tests/models/bark/test_modeling_bark.py::BarkCoarseModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/ernie/test_modeling_ernie.py::ErnieModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/clap/test_modeling_clap.py::ClapModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw61] [  2%] PASSED tests/models/colqwen2/test_modeling_colqwen2.py::ColQwen2ForRetrievalModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/conditional_detr/test_modeling_conditional_detr.py::ConditionalDetrModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw26] [  2%] PASSED tests/models/clip/test_modeling_clip.py::CLIPVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/clip/test_modeling_clip.py::CLIPTextModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw2] [  3%] PASSED tests/models/bart/test_modeling_bart.py::BartStandaloneDecoderModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/beit/test_modeling_beit.py::BeitModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw27] [  3%] PASSED tests/models/chinese_clip/test_modeling_chinese_clip.py::ChineseCLIPVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/eomt/test_modeling_eomt.py::EomtForUniversalSegmentationTest::test_torch_export <- tests/test_modeling_common.py 
[gw37] [  3%] PASSED tests/models/codegen/test_modeling_codegen.py::CodeGenModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/cohere/test_modeling_cohere.py::CohereModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw18] [  3%] PASSED tests/models/clvp/test_modeling_clvp.py::ClvpDecoderTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/clvp/test_modeling_clvp.py::ClvpModelForConditionalGenerationTest::test_torch_export <- tests/test_modeling_common.py 
[gw59] [  3%] PASSED tests/models/blip/test_modeling_blip.py::BlipTextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/esm/test_modeling_esmfold.py::EsmFoldModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw32] [  4%] PASSED tests/models/clipseg/test_modeling_clipseg.py::CLIPSegTextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/exaone4/test_modeling_exaone4.py::Exaone4ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw1] [  4%] PASSED tests/models/aimv2/test_modeling_aimv2.py::Aimv2TextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/falcon/test_modeling_falcon.py::FalconModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw52] [  4%] PASSED tests/models/blip_2/test_modeling_blip_2.py::Blip2VisionModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/evolla/test_modeling_evolla.py::EvollaModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw20] [  4%] PASSED tests/models/bigbird_pegasus/test_modeling_bigbird_pegasus.py::BigBirdPegasusStandaloneDecoderModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/biogpt/test_modeling_biogpt.py::BioGptModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw28] [  5%] PASSED tests/models/depth_anything/test_modeling_depth_anything.py::DepthAnythingModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/depth_pro/test_modeling_depth_pro.py::DepthProModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw34] [  5%] PASSED tests/models/dpt/test_modeling_dpt_auto_backbone.py::DPTModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/dpt/test_modeling_dpt_hybrid.py::DPTModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw11] [  5%] PASSED tests/models/altclip/test_modeling_altclip.py::AltCLIPModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/encodec/test_modeling_encodec.py::EncodecModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw31] [  5%] PASSED tests/models/blip/test_modeling_blip.py::BlipTextRetrievalModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/blip/test_modeling_blip.py::BlipTextImageModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw51] [  5%] PASSED tests/models/clip/test_modeling_clip.py::CLIPModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/clip/test_modeling_clip.py::CLIPForImageClassificationModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw46] [  6%] PASSED tests/models/dpr/test_modeling_dpr.py::DPRModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/dpt/test_modeling_dpt.py::DPTModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw53] [  6%] PASSED tests/models/cohere2/test_modeling_cohere2.py::CohereModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/cohere2/test_modeling_cohere2.py::Cohere2ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw9] [  6%] PASSED tests/models/blip/test_modeling_blip.py::BlipModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/blip/test_modeling_blip.py::BlipVQAModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw12] [  6%] PASSED tests/models/chinese_clip/test_modeling_chinese_clip.py::ChineseCLIPModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/clap/test_modeling_clap.py::ClapAudioModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw0] [  6%] PASSED tests/models/aimv2/test_modeling_aimv2.py::Aimv2ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw51] [  7%] PASSED tests/models/clip/test_modeling_clip.py::CLIPForImageClassificationModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/albert/test_modeling_albert.py::AlbertModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/funnel/test_modeling_funnel.py::FunnelBaseModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw15] [  7%] PASSED tests/models/blip_2/test_modeling_blip_2.py::Blip2ForConditionalGenerationDecoderOnlyTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/blip_2/test_modeling_blip_2.py::Blip2ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw17] [  7%] PASSED tests/models/audioflamingo3/test_modeling_audioflamingo3.py::AudioFlamingo3ForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/autoformer/test_modeling_autoformer.py::AutoformerModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw6] [  7%] PASSED tests/models/blip_2/test_modeling_blip_2.py::Blip2VisionModelWithProjectionTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/esm/test_modeling_esm.py::EsmModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw41] [  7%] PASSED tests/models/chameleon/test_modeling_chameleon.py::ChameleonModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/chameleon/test_modeling_chameleon.py::ChameleonVision2SeqModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw26] [  8%] PASSED tests/models/clip/test_modeling_clip.py::CLIPTextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/fastspeech2_conformer/test_modeling_fastspeech2_conformer.py::FastSpeech2ConformerWithHifiGanTest::test_torch_export <- tests/test_modeling_common.py 
[gw49] [  8%] PASSED tests/models/dinov2/test_modeling_dinov2.py::Dinov2ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/dinov2_with_registers/test_modeling_dinov2_with_registers.py::Dinov2WithRegistersModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw7] [  8%] PASSED tests/models/blenderbot_small/test_modeling_blenderbot_small.py::BlenderbotSmallModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/blenderbot_small/test_modeling_blenderbot_small.py::BlenderbotSmallStandaloneDecoderModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw40] [  8%] PASSED tests/models/csm/test_modeling_csm.py::CsmForConditionalGenerationTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/ctrl/test_modeling_ctrl.py::CTRLModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw16] [  9%] PASSED tests/models/blip_2/test_modeling_blip_2.py::Blip2TextRetrievalModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/bloom/test_modeling_bloom.py::BloomModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw14] [  9%] PASSED tests/models/apertus/test_modeling_apertus.py::ApertusModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/arcee/test_modeling_arcee.py::ArceeModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw42] [  9%] PASSED tests/models/dbrx/test_modeling_dbrx.py::DbrxModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/deberta/test_modeling_deberta.py::DebertaModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw13] [  9%] PASSED tests/models/blenderbot/test_modeling_blenderbot.py::BlenderbotModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/blenderbot/test_modeling_blenderbot.py::BlenderbotStandaloneDecoderModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw6] [  9%] PASSED tests/models/esm/test_modeling_esm.py::EsmModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/git/test_modeling_git.py::GitVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw23] [ 10%] PASSED tests/models/deepseek_vl/test_modeling_deepseek_vl.py::DeepseekVLModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/deepseek_vl_hybrid/test_modeling_deepseek_vl_hybrid.py::DeepseekVLHybridModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw39] [ 10%] PASSED tests/models/distilbert/test_modeling_distilbert.py::DistilBertModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/doge/test_modeling_doge.py::DogeModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw4] [ 10%] PASSED tests/models/align/test_modeling_align.py::AlignVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/align/test_modeling_align.py::AlignTextModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw31] [ 10%] PASSED tests/models/blip/test_modeling_blip.py::BlipTextImageModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw37] [ 10%] PASSED tests/models/cohere/test_modeling_cohere.py::CohereModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/funnel/test_modeling_funnel.py::FunnelModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/flava/test_modeling_flava.py::FlavaTextModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw11] [ 11%] PASSED tests/models/encodec/test_modeling_encodec.py::EncodecModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/fsmt/test_modeling_fsmt.py::FSMTModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw51] [ 11%] PASSED tests/models/funnel/test_modeling_funnel.py::FunnelBaseModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/gemma3n/test_modeling_gemma3n.py::Gemma3nAudioModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw51] [ 11%] SKIPPED tests/models/gemma3n/test_modeling_gemma3n.py::Gemma3nAudioModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/granite/test_modeling_granite.py::GraniteModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw38] [ 11%] PASSED tests/models/bit/test_modeling_bit.py::BitModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/bitnet/test_modeling_bitnet.py::BitNetModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw63] [ 11%] PASSED tests/models/clipseg/test_modeling_clipseg.py::CLIPSegModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/clvp/test_modeling_clvp.py::ClvpEncoderTest::test_torch_export <- tests/test_modeling_common.py 
[gw6] [ 12%] PASSED tests/models/git/test_modeling_git.py::GitVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/gpt_bigcode/test_modeling_gpt_bigcode.py::GPTBigCodeModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw29] [ 12%] PASSED tests/models/blt/test_modeling_blt.py::BltModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw7] [ 12%] PASSED tests/models/blenderbot_small/test_modeling_blenderbot_small.py::BlenderbotSmallStandaloneDecoderModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/glm4_moe/test_modeling_glm4_moe.py::Glm4MoeModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/bridgetower/test_modeling_bridgetower.py::BridgeTowerModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw59] [ 12%] PASSED tests/models/esm/test_modeling_esmfold.py::EsmFoldModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/flava/test_modeling_flava.py::FlavaImageCodebookTest::test_torch_export <- tests/test_modeling_common.py 
[gw58] [ 12%] PASSED tests/models/cohere2_vision/test_modeling_cohere2_vision.py::Cohere2ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/colpali/test_modeling_colpali.py::ColPaliForRetrievalModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw53] [ 13%] PASSED tests/models/cohere2/test_modeling_cohere2.py::Cohere2ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/gemma/test_modeling_gemma.py::GemmaModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw37] [ 13%] PASSED tests/models/flava/test_modeling_flava.py::FlavaTextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/gpt_oss/test_modeling_gpt_oss.py::GptOssModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw19] [ 13%] PASSED tests/models/donut/test_modeling_donut_swin.py::DonutSwinModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/dots1/test_modeling_dots1.py::Dots1ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw9] [ 13%] PASSED tests/models/blip/test_modeling_blip.py::BlipVQAModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/gemma2/test_modeling_gemma2.py::Gemma2ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw36] [ 14%] PASSED tests/models/deberta_v2/test_modeling_deberta_v2.py::DebertaV2ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/decision_transformer/test_modeling_decision_transformer.py::DecisionTransformerModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw4] [ 14%] PASSED tests/models/align/test_modeling_align.py::AlignTextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/gpt_neox/test_modeling_gpt_neox.py::GPTNeoXModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw13] [ 14%] PASSED tests/models/blenderbot/test_modeling_blenderbot.py::BlenderbotStandaloneDecoderModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/gpt2/test_modeling_gpt2.py::GPT2ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw30] [ 14%] PASSED tests/models/dac/test_modeling_dac.py::DacModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/data2vec/test_modeling_data2vec_audio.py::Data2VecAudioModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw8] [ 14%] PASSED tests/models/align/test_modeling_align.py::AlignModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/altclip/test_modeling_altclip.py::AltCLIPVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw24] [ 15%] PASSED tests/models/clap/test_modeling_clap.py::ClapModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/falcon_h1/test_modeling_falcon_h1.py::FalconH1ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw57] [ 15%] PASSED tests/models/emu3/test_modeling_emu3.py::Emu3Vision2TextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/ernie4_5_moe/test_modeling_ernie4_5_moe.py::Ernie4_5_MoeModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw48] [ 15%] PASSED tests/models/diffllama/test_modeling_diffllama.py::DiffLlamaModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/dinat/test_modeling_dinat.py::DinatModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw48] [ 15%] SKIPPED tests/models/dinat/test_modeling_dinat.py::DinatModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/idefics2/test_modeling_idefics2.py::Idefics2ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw63] [ 15%] PASSED tests/models/clvp/test_modeling_clvp.py::ClvpEncoderTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/granitemoehybrid/test_modeling_granitemoehybrid.py::BambaModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw20] [ 16%] PASSED tests/models/biogpt/test_modeling_biogpt.py::BioGptModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw21] [ 16%] PASSED tests/models/aya_vision/test_modeling_aya_vision.py::AyaVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/florence2/test_modeling_florence2.py::Florence2ForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/bamba/test_modeling_bamba.py::BambaModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw49] [ 16%] PASSED tests/models/dinov2_with_registers/test_modeling_dinov2_with_registers.py::Dinov2WithRegistersModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/glm4/test_modeling_glm4.py::Glm4ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw26] [ 16%] PASSED tests/models/fastspeech2_conformer/test_modeling_fastspeech2_conformer.py::FastSpeech2ConformerWithHifiGanTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/glm/test_modeling_glm.py::GlmModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw8] [ 16%] PASSED tests/models/altclip/test_modeling_altclip.py::AltCLIPVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/ibert/test_modeling_ibert.py::IBertModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw59] [ 17%] PASSED tests/models/flava/test_modeling_flava.py::FlavaImageCodebookTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/groupvit/test_modeling_groupvit.py::GroupViTVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw50] [ 17%] PASSED tests/models/dinov3_convnext/test_modeling_dinov3_convnext.py::DINOv3ConvNextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/dinov3_vit/test_modeling_dinov3_vit.py::Dinov3ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw51] [ 17%] PASSED tests/models/granite/test_modeling_granite.py::GraniteModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/granite_speech/test_modeling_granite_speech.py::GraniteSpeechForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw46] [ 17%] PASSED tests/models/dpt/test_modeling_dpt.py::DPTModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/fuyu/test_modeling_fuyu.py::FuyuModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw60] [ 18%] PASSED tests/models/data2vec/test_modeling_data2vec_text.py::Data2VecTextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/data2vec/test_modeling_data2vec_vision.py::Data2VecVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw40] [ 18%] PASSED tests/models/ctrl/test_modeling_ctrl.py::CTRLModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/glm4v/test_modeling_glm4v.py::Glm4vModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw27] [ 18%] FAILED tests/models/eomt/test_modeling_eomt.py::EomtForUniversalSegmentationTest::test_torch_export <- tests/test_modeling_common.py 
[gw55] [ 18%] PASSED tests/models/convbert/test_modeling_convbert.py::ConvBertModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/convnext/test_modeling_convnext.py::ConvNextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/flava/test_modeling_flava.py::FlavaImageModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw33] [ 18%] PASSED tests/models/bert/test_modeling_bert.py::BertModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/bert_generation/test_modeling_bert_generation.py::BertGenerationEncoderTest::test_torch_export <- tests/test_modeling_common.py 
[gw56] [ 19%] PASSED tests/models/deepseek_v2/test_modeling_deepseek_v2.py::DeepseekV2ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/deepseek_v3/test_modeling_deepseek_v3.py::DeepseekV3ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw31] [ 19%] PASSED tests/models/funnel/test_modeling_funnel.py::FunnelModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/gpt_neox_japanese/test_modeling_gpt_neox_japanese.py::GPTNeoXModelJapaneseTest::test_torch_export <- tests/test_modeling_common.py 
[gw27] [ 19%] PASSED tests/models/flava/test_modeling_flava.py::FlavaImageModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/jetmoe/test_modeling_jetmoe.py::JetMoeModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw36] [ 19%] PASSED tests/models/decision_transformer/test_modeling_decision_transformer.py::DecisionTransformerModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw52] [ 19%] PASSED tests/models/evolla/test_modeling_evolla.py::EvollaModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/hubert/test_modeling_hubert.py::HubertModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/flex_olmo/test_modeling_flex_olmo.py::FlexOlmoModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw38] [ 20%] PASSED tests/models/bitnet/test_modeling_bitnet.py::BitNetModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/granitemoe/test_modeling_granitemoe.py::GraniteMoeModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw12] [ 20%] PASSED tests/models/clap/test_modeling_clap.py::ClapAudioModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/gemma3/test_modeling_gemma3.py::Gemma3TextModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw1] [ 20%] PASSED tests/models/falcon/test_modeling_falcon.py::FalconModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/flava/test_modeling_flava.py::FlavaForPreTrainingTest::test_torch_export <- tests/test_modeling_common.py 
[gw11] [ 20%] PASSED tests/models/fsmt/test_modeling_fsmt.py::FSMTModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/gptj/test_modeling_gptj.py::GPTJModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw22] [ 20%] PASSED tests/models/bros/test_modeling_bros.py::BrosModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/canine/test_modeling_canine.py::CanineModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw58] [ 21%] PASSED tests/models/colpali/test_modeling_colpali.py::ColPaliForRetrievalModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/groupvit/test_modeling_groupvit.py::GroupViTTextModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw6] [ 21%] PASSED tests/models/gpt_bigcode/test_modeling_gpt_bigcode.py::GPTBigCodeModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/granitemoehybrid/test_modeling_granitemoehybrid.py::GraniteMoeHybridModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw16] [ 21%] PASSED tests/models/bloom/test_modeling_bloom.py::BloomModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/glm4v_moe/test_modeling_glm4v_moe.py::Glm4vMoeModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw10] [ 21%] PASSED tests/models/aria/test_modeling_aria.py::AriaForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/audio_spectrogram_transformer/test_modeling_audio_spectrogram_transformer.py::ASTModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw40] [ 22%] PASSED tests/models/glm4v/test_modeling_glm4v.py::Glm4vModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/janus/test_modeling_janus.py::JanusVisionText2TextModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw25] [ 22%] PASSED tests/models/efficientnet/test_modeling_efficientnet.py::EfficientNetModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/electra/test_modeling_electra.py::ElectraModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw33] [ 22%] PASSED tests/models/bert_generation/test_modeling_bert_generation.py::BertGenerationEncoderTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/kosmos2/test_modeling_kosmos2.py::Kosmos2ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw41] [ 22%] PASSED tests/models/chameleon/test_modeling_chameleon.py::ChameleonVision2SeqModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/git/test_modeling_git.py::GitModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw50] [ 22%] PASSED tests/models/dinov3_vit/test_modeling_dinov3_vit.py::Dinov3ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/instructblipvideo/test_modeling_instructblipvideo.py::InstructBlipVideoVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw39] [ 23%] PASSED tests/models/doge/test_modeling_doge.py::DogeModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/gpt_neo/test_modeling_gpt_neo.py::GPTNeoModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw14] [ 23%] PASSED tests/models/arcee/test_modeling_arcee.py::ArceeModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/glpn/test_modeling_glpn.py::GLPNModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw7] [ 23%] PASSED tests/models/glm4_moe/test_modeling_glm4_moe.py::Glm4MoeModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/grounding_dino/test_modeling_grounding_dino.py::GroundingDinoModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw58] [ 23%] PASSED tests/models/groupvit/test_modeling_groupvit.py::GroupViTTextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/lightglue/test_modeling_lightglue.py::LightGlueModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw0] [ 23%] PASSED tests/models/albert/test_modeling_albert.py::AlbertModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/gemma3/test_modeling_gemma3.py::Gemma3Vision2TextModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw5] [ 24%] PASSED tests/models/ernie/test_modeling_ernie.py::ErnieModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw28] [ 24%] PASSED tests/models/depth_pro/test_modeling_depth_pro.py::DepthProModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/falcon_mamba/test_modeling_falcon_mamba.py::FalconMambaModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/fnet/test_modeling_fnet.py::FNetModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw53] [ 24%] PASSED tests/models/gemma/test_modeling_gemma.py::GemmaModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/groupvit/test_modeling_groupvit.py::GroupViTModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw43] [ 24%] PASSED tests/models/bart/test_modeling_bart.py::BartModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/ernie4_5/test_modeling_ernie4_5.py::Ernie4_5ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw32] [ 24%] PASSED tests/models/exaone4/test_modeling_exaone4.py::Exaone4ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/flava/test_modeling_flava.py::FlavaModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw42] [ 25%] PASSED tests/models/deberta/test_modeling_deberta.py::DebertaModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/got_ocr2/test_modeling_got_ocr2.py::GotOcr2ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw46] [ 25%] PASSED tests/models/fuyu/test_modeling_fuyu.py::FuyuModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/internvl/test_modeling_internvl.py::InternVLModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw50] [ 25%] PASSED tests/models/instructblipvideo/test_modeling_instructblipvideo.py::InstructBlipVideoVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/longformer/test_modeling_longformer.py::LongformerModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw1] [ 25%] FAILED tests/models/flava/test_modeling_flava.py::FlavaForPreTrainingTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/lfm2/test_modeling_lfm2.py::Lfm2ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw10] [ 25%] PASSED tests/models/audio_spectrogram_transformer/test_modeling_audio_spectrogram_transformer.py::ASTModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/llava/test_modeling_llava.py::LlavaForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw23] [ 26%] PASSED tests/models/deepseek_vl_hybrid/test_modeling_deepseek_vl_hybrid.py::DeepseekVLHybridModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/gpt_bigcode/test_modeling_gpt_bigcode.py::GPTBigCodeMHAModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw16] [ 26%] PASSED tests/models/glm4v_moe/test_modeling_glm4v_moe.py::Glm4vMoeModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/llama/test_modeling_llama.py::LlamaModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw31] [ 26%] PASSED tests/models/gpt_neox_japanese/test_modeling_gpt_neox_japanese.py::GPTNeoXModelJapaneseTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/kyutai_speech_to_text/test_modeling_kyutai_speech_to_text.py::KyutaiSpeechToTextModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw58] [ 26%] PASSED tests/models/lightglue/test_modeling_lightglue.py::LightGlueModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/longt5/test_modeling_longt5.py::LongT5EncoderOnlyTGlobalModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw4] [ 27%] PASSED tests/models/gpt_neox/test_modeling_gpt_neox.py::GPTNeoXModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/hubert/test_modeling_hubert.py::HubertRobustModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw57] [ 27%] PASSED tests/models/ernie4_5_moe/test_modeling_ernie4_5_moe.py::Ernie4_5_MoeModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/idefics/test_modeling_idefics.py::IdeficsForVisionText2TextTest::test_torch_export <- tests/test_modeling_common.py 
[gw19] [ 27%] PASSED tests/models/dots1/test_modeling_dots1.py::Dots1ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/hgnet_v2/test_modeling_hgnet_v2.py::HGNetV2ForImageClassificationTest::test_torch_export <- tests/test_modeling_common.py 
[gw2] [ 27%] PASSED tests/models/beit/test_modeling_beit.py::BeitModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/flaubert/test_modeling_flaubert.py::FlaubertModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw54] [ 27%] PASSED tests/models/convnextv2/test_modeling_convnextv2.py::ConvNextV2ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/cpmant/test_modeling_cpmant.py::CpmAntModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw48] [ 28%] PASSED tests/models/idefics2/test_modeling_idefics2.py::Idefics2ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/idefics2/test_modeling_idefics2.py::Idefics2ForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw60] [ 28%] PASSED tests/models/data2vec/test_modeling_data2vec_vision.py::Data2VecVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/jamba/test_modeling_jamba.py::JambaModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw59] [ 28%] PASSED tests/models/groupvit/test_modeling_groupvit.py::GroupViTVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/instructblip/test_modeling_instructblip.py::InstructBlipForConditionalGenerationDecoderOnlyTest::test_torch_export <- tests/test_modeling_common.py 
[gw21] [ 28%] PASSED tests/models/bamba/test_modeling_bamba.py::BambaModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/ijepa/test_modeling_ijepa.py::IJepaModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw20] [ 28%] PASSED tests/models/florence2/test_modeling_florence2.py::Florence2ForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw15] [ 29%] PASSED tests/models/blip_2/test_modeling_blip_2.py::Blip2ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw63] [ 29%] PASSED tests/models/granitemoehybrid/test_modeling_granitemoehybrid.py::BambaModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/idefics3/test_modeling_idefics3.py::Idefics3ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/idefics3/test_modeling_idefics3.py::Idefics3ForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/gemma3n/test_modeling_gemma3n.py::Gemma3nTextModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw51] [ 29%] PASSED tests/models/granite_speech/test_modeling_granite_speech.py::GraniteSpeechForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/instructblipvideo/test_modeling_instructblipvideo.py::InstructBlipVideoForConditionalGenerationDecoderOnlyTest::test_torch_export <- tests/test_modeling_common.py 
[gw26] [ 29%] PASSED tests/models/glm/test_modeling_glm.py::GlmModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/informer/test_modeling_informer.py::InformerModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw1] [ 29%] PASSED tests/models/lfm2/test_modeling_lfm2.py::Lfm2ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/maskformer/test_modeling_maskformer.py::MaskFormerModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw37] [ 30%] PASSED tests/models/gpt_oss/test_modeling_gpt_oss.py::GptOssModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/helium/test_modeling_helium.py::HeliumModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw49] [ 30%] PASSED tests/models/glm4/test_modeling_glm4.py::Glm4ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/imagegpt/test_modeling_imagegpt.py::ImageGPTModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw50] [ 30%] PASSED tests/models/longformer/test_modeling_longformer.py::LongformerModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/mask2former/test_modeling_mask2former.py::Mask2FormerModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw32] [ 30%] PASSED tests/models/flava/test_modeling_flava.py::FlavaModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw13] [ 31%] PASSED tests/models/gpt2/test_modeling_gpt2.py::GPT2ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/marian/test_modeling_marian.py::MarianModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw9] [ 31%] PASSED tests/models/gemma2/test_modeling_gemma2.py::Gemma2ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/hunyuan_v1_dense/test_modeling_hunyuan_v1_dense.py::HunYuanDenseV1ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw36] [ 31%] PASSED tests/models/hubert/test_modeling_hubert.py::HubertModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/hiera/test_modeling_hiera.py::HieraModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/layoutlmv2/test_modeling_layoutlmv2.py::LayoutLMv2ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw36] [ 31%] SKIPPED tests/models/layoutlmv2/test_modeling_layoutlmv2.py::LayoutLMv2ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/moonshine/test_modeling_moonshine.py::MoonshineModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw41] [ 31%] PASSED tests/models/git/test_modeling_git.py::GitModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/longcat_flash/test_modeling_longcat_flash.py::LongcatFlashModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw45] [ 32%] PASSED tests/models/edgetam/test_modeling_edgetam.py::EdgeTamModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/efficientloftr/test_modeling_efficientloftr.py::EfficientLoFTRModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw52] [ 32%] PASSED tests/models/flex_olmo/test_modeling_flex_olmo.py::FlexOlmoModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/layoutlmv3/test_modeling_layoutlmv3.py::LayoutLMv3ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw43] [ 32%] PASSED tests/models/ernie4_5/test_modeling_ernie4_5.py::Ernie4_5ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/mamba2/test_modeling_mamba2.py::Mamba2ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw21] [ 32%] PASSED tests/models/ijepa/test_modeling_ijepa.py::IJepaModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/mistral3/test_modeling_mistral3.py::Mistral3ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw58] [ 32%] PASSED tests/models/longt5/test_modeling_longt5.py::LongT5EncoderOnlyTGlobalModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/metaclip_2/test_modeling_metaclip_2.py::MetaClip2VisionModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw33] [ 33%] PASSED tests/models/kosmos2/test_modeling_kosmos2.py::Kosmos2ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/llava_onevision/test_modeling_llava_onevision.py::LlavaOnevisionForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw11] [ 33%] PASSED tests/models/gptj/test_modeling_gptj.py::GPTJModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/lfm2_moe/test_modeling_lfm2_moe.py::Lfm2MoeModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw23] [ 33%] PASSED tests/models/gpt_bigcode/test_modeling_gpt_bigcode.py::GPTBigCodeMHAModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/mbart/test_modeling_mbart.py::MBartModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw21] [ 33%] PASSED tests/models/mistral3/test_modeling_mistral3.py::Mistral3ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/mt5/test_modeling_mt5.py::MT5ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw28] [ 33%] PASSED tests/models/fnet/test_modeling_fnet.py::FNetModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/m2m_100/test_modeling_m2m_100.py::M2M100ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw38] [ 34%] PASSED tests/models/granitemoe/test_modeling_granitemoe.py::GraniteMoeModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/led/test_modeling_led.py::LEDModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw56] [ 34%] PASSED tests/models/deepseek_v3/test_modeling_deepseek_v3.py::DeepseekV3ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/kosmos2_5/test_modeling_kosmos2_5.py::Kosmos2_5ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw57] [ 34%] PASSED tests/models/idefics/test_modeling_idefics.py::IdeficsForVisionText2TextTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/metaclip_2/test_modeling_metaclip_2.py::MetaClip2ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw10] [ 34%] PASSED tests/models/llava/test_modeling_llava.py::LlavaForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/maskformer/test_modeling_maskformer_swin.py::MaskFormerSwinModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw31] [ 35%] PASSED tests/models/kyutai_speech_to_text/test_modeling_kyutai_speech_to_text.py::KyutaiSpeechToTextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/megatron_bert/test_modeling_megatron_bert.py::MegatronBertModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw54] [ 35%] PASSED tests/models/cpmant/test_modeling_cpmant.py::CpmAntModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/mimi/test_modeling_mimi.py::MimiModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw46] [ 35%] PASSED tests/models/internvl/test_modeling_internvl.py::InternVLModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/markuplm/test_modeling_markuplm.py::MarkupLMModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw12] [ 35%] PASSED tests/models/gemma3/test_modeling_gemma3.py::Gemma3TextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/levit/test_modeling_levit.py::LevitModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw58] [ 35%] PASSED tests/models/metaclip_2/test_modeling_metaclip_2.py::MetaClip2VisionModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/mt5/test_modeling_mt5.py::MT5EncoderOnlyModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw33] [ 36%] PASSED tests/models/llava_onevision/test_modeling_llava_onevision.py::LlavaOnevisionForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/musicgen/test_modeling_musicgen.py::MusicgenDecoderTest::test_torch_export <- tests/test_modeling_common.py 
[gw30] [ 36%] PASSED tests/models/data2vec/test_modeling_data2vec_audio.py::Data2VecAudioModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/hunyuan_v1_moe/test_modeling_hunyuan_v1_moe.py::HunYuanMoEV1ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw4] [ 36%] PASSED tests/models/hubert/test_modeling_hubert.py::HubertRobustModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/metaclip_2/test_modeling_metaclip_2.py::MetaClip2TextModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw24] [ 36%] PASSED tests/models/falcon_h1/test_modeling_falcon_h1.py::FalconH1ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw9] [ 36%] PASSED tests/models/hiera/test_modeling_hiera.py::HieraModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/idefics/test_modeling_idefics.py::IdeficsModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/modernbert_decoder/test_modeling_modernbert_decoder.py::ModernBertDecoderModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw17] [ 37%] PASSED tests/models/autoformer/test_modeling_autoformer.py::AutoformerModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/gemma3n/test_modeling_gemma3n.py::Gemma3nVision2TextModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw17] [ 37%] SKIPPED tests/models/gemma3n/test_modeling_gemma3n.py::Gemma3nVision2TextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/owlv2/test_modeling_owlv2.py::Owlv2TextModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw5] [ 37%] PASSED tests/models/falcon_mamba/test_modeling_falcon_mamba.py::FalconMambaModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/lxmert/test_modeling_lxmert.py::LxmertModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw20] [ 37%] PASSED tests/models/idefics3/test_modeling_idefics3.py::Idefics3ForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/mixtral/test_modeling_mixtral.py::MixtralModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw57] [ 37%] PASSED tests/models/metaclip_2/test_modeling_metaclip_2.py::MetaClip2ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw8] [ 38%] PASSED tests/models/ibert/test_modeling_ibert.py::IBertModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/nllb_moe/test_modeling_nllb_moe.py::NllbMoeModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/instructblip/test_modeling_instructblip.py::InstructBlipVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw48] [ 38%] PASSED tests/models/idefics2/test_modeling_idefics2.py::Idefics2ForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/minimax/test_modeling_minimax.py::MiniMaxModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw39] [ 38%] PASSED tests/models/gpt_neo/test_modeling_gpt_neo.py::GPTNeoModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/longt5/test_modeling_longt5.py::LongT5ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw63] [ 38%] PASSED tests/models/idefics3/test_modeling_idefics3.py::Idefics3ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/mllama/test_modeling_mllama.py::MllamaForCausalLMModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw25] [ 38%] PASSED tests/models/electra/test_modeling_electra.py::ElectraModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/llava_next_video/test_modeling_llava_next_video.py::LlavaNextVideoForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw11] [ 39%] PASSED tests/models/lfm2_moe/test_modeling_lfm2_moe.py::Lfm2MoeModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/musicgen/test_modeling_musicgen.py::MusicgenTest::test_torch_export <- tests/test_modeling_common.py 
[gw43] [ 39%] PASSED tests/models/mamba2/test_modeling_mamba2.py::Mamba2ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/mra/test_modeling_mra.py::MraModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw55] [ 39%] PASSED tests/models/convnext/test_modeling_convnext.py::ConvNextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/janus/test_modeling_janus.py::JanusVQModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw49] [ 39%] PASSED tests/models/imagegpt/test_modeling_imagegpt.py::ImageGPTModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw40] [ 40%] PASSED tests/models/janus/test_modeling_janus.py::JanusVisionText2TextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/llava_next/test_modeling_llava_next.py::LlavaNextForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/mobilenet_v2/test_modeling_mobilenet_v2.py::MobileNetV2ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw8] [ 40%] PASSED tests/models/instructblip/test_modeling_instructblip.py::InstructBlipVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/owlvit/test_modeling_owlvit.py::OwlViTModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw19] [ 40%] PASSED tests/models/hgnet_v2/test_modeling_hgnet_v2.py::HGNetV2ForImageClassificationTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/metaclip_2/test_modeling_metaclip_2.py::MetaClip2ForImageClassificationModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw53] [ 40%] PASSED tests/models/groupvit/test_modeling_groupvit.py::GroupViTModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/mamba/test_modeling_mamba.py::MambaModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw38] [ 40%] PASSED tests/models/led/test_modeling_led.py::LEDModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/mvp/test_modeling_mvp.py::MvpStandaloneDecoderModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw62] [ 41%] PASSED tests/models/detr/test_modeling_detr.py::DetrModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw16] [ 41%] PASSED tests/models/llama/test_modeling_llama.py::LlamaModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/dia/test_modeling_dia.py::DiaModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/mbart/test_modeling_mbart.py::MBartStandaloneDecoderModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw60] [ 41%] PASSED tests/models/jamba/test_modeling_jamba.py::JambaModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/ministral/test_modeling_ministral.py::MinistralModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw42] [ 41%] PASSED tests/models/got_ocr2/test_modeling_got_ocr2.py::GotOcr2ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/marian/test_modeling_marian.py::MarianStandaloneDecoderModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw4] [ 41%] PASSED tests/models/metaclip_2/test_modeling_metaclip_2.py::MetaClip2TextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/opt/test_modeling_opt.py::OPTModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw32] [ 42%] PASSED tests/models/marian/test_modeling_marian.py::MarianModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/mobilevitv2/test_modeling_mobilevitv2.py::MobileViTV2ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw13] [ 42%] PASSED tests/models/hunyuan_v1_dense/test_modeling_hunyuan_v1_dense.py::HunYuanDenseV1ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/modernbert/test_modeling_modernbert.py::ModernBertModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw58] [ 42%] PASSED tests/models/mt5/test_modeling_mt5.py::MT5EncoderOnlyModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/omdet_turbo/test_modeling_omdet_turbo.py::OmDetTurboModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw63] [ 42%] PASSED tests/models/mllama/test_modeling_mllama.py::MllamaForCausalLMModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/paligemma2/test_modeling_paligemma2.py::PaliGemma2ForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw41] [ 42%] PASSED tests/models/longcat_flash/test_modeling_longcat_flash.py::LongcatFlashModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw34] [ 43%] PASSED tests/models/dpt/test_modeling_dpt_hybrid.py::DPTModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/focalnet/test_modeling_focalnet.py::FocalNetModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/moshi/test_modeling_moshi.py::MoshiTest::test_torch_export <- tests/test_modeling_common.py 
[gw25] [ 43%] PASSED tests/models/llava_next_video/test_modeling_llava_next_video.py::LlavaNextVideoForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/parakeet/test_modeling_parakeet.py::ParakeetEncoderModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw40] [ 43%] PASSED tests/models/llava_next/test_modeling_llava_next.py::LlavaNextForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/pegasus/test_modeling_pegasus.py::PegasusStandaloneDecoderModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw19] [ 43%] PASSED tests/models/metaclip_2/test_modeling_metaclip_2.py::MetaClip2ForImageClassificationModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/pegasus_x/test_modeling_pegasus_x.py::PegasusXStandaloneDecoderModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw59] [ 44%] PASSED tests/models/instructblip/test_modeling_instructblip.py::InstructBlipForConditionalGenerationDecoderOnlyTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/mistral/test_modeling_mistral.py::MistralModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw57] [ 44%] PASSED tests/models/nllb_moe/test_modeling_nllb_moe.py::NllbMoeModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/owlvit/test_modeling_owlvit.py::OwlViTTextModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw37] [ 44%] PASSED tests/models/helium/test_modeling_helium.py::HeliumModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/mobilenet_v1/test_modeling_mobilenet_v1.py::MobileNetV1ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw22] [ 44%] PASSED tests/models/canine/test_modeling_canine.py::CanineModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/lfm2_vl/test_modeling_lfm2_vl.py::Lfm2VlModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw35] [ 44%] PASSED tests/models/deformable_detr/test_modeling_deformable_detr.py::DeformableDetrModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/deit/test_modeling_deit.py::DeiTModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw27] [ 45%] PASSED tests/models/jetmoe/test_modeling_jetmoe.py::JetMoeModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/layoutlm/test_modeling_layoutlm.py::LayoutLMModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw36] [ 45%] PASSED tests/models/moonshine/test_modeling_moonshine.py::MoonshineModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/moshi/test_modeling_moshi.py::MoshiDecoderTest::test_torch_export <- tests/test_modeling_common.py 
[gw61] [ 45%] PASSED tests/models/conditional_detr/test_modeling_conditional_detr.py::ConditionalDetrModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/fastspeech2_conformer/test_modeling_fastspeech2_conformer.py::FastSpeech2ConformerModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw28] [ 45%] PASSED tests/models/m2m_100/test_modeling_m2m_100.py::M2M100ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/mvp/test_modeling_mvp.py::MvpModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw33] [ 45%] PASSED tests/models/musicgen/test_modeling_musicgen.py::MusicgenDecoderTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/oneformer/test_modeling_oneformer.py::OneFormerModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw19] [ 46%] PASSED tests/models/pegasus_x/test_modeling_pegasus_x.py::PegasusXStandaloneDecoderModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/prompt_depth_anything/test_modeling_prompt_depth_anything.py::PromptDepthAnythingModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw2] [ 46%] PASSED tests/models/flaubert/test_modeling_flaubert.py::FlaubertModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw26] [ 46%] PASSED tests/models/informer/test_modeling_informer.py::InformerModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/mgp_str/test_modeling_mgp_str.py::MgpstrModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/mm_grounding_dino/test_modeling_mm_grounding_dino.py::MMGroundingDinoModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw56] [ 46%] PASSED tests/models/kosmos2_5/test_modeling_kosmos2_5.py::Kosmos2_5ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/nemotron/test_modeling_nemotron.py::NemotronModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw38] [ 46%] PASSED tests/models/mvp/test_modeling_mvp.py::MvpStandaloneDecoderModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/perception_lm/test_modeling_perception_lm.py::PerceptionLMForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw16] [ 47%] PASSED tests/models/mbart/test_modeling_mbart.py::MBartStandaloneDecoderModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/phi/test_modeling_phi.py::PhiModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw51] [ 47%] PASSED tests/models/instructblipvideo/test_modeling_instructblipvideo.py::InstructBlipVideoForConditionalGenerationDecoderOnlyTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/mllama/test_modeling_mllama.py::MllamaForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw10] [ 47%] PASSED tests/models/maskformer/test_modeling_maskformer_swin.py::MaskFormerSwinModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/nystromformer/test_modeling_nystromformer.py::NystromformerModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw42] [ 47%] PASSED tests/models/marian/test_modeling_marian.py::MarianStandaloneDecoderModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/phi4_multimodal/test_modeling_phi4_multimodal.py::Phi4MultimodalModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw0] [ 48%] PASSED tests/models/gemma3/test_modeling_gemma3.py::Gemma3Vision2TextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/luke/test_modeling_luke.py::LukeModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw47] [ 48%] PASSED tests/models/cvt/test_modeling_cvt.py::CvtModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/cwm/test_modeling_cwm.py::CwmModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw9] [ 48%] PASSED tests/models/modernbert_decoder/test_modeling_modernbert_decoder.py::ModernBertDecoderModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/owlv2/test_modeling_owlv2.py::Owlv2VisionModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw58] [ 48%] PASSED tests/models/omdet_turbo/test_modeling_omdet_turbo.py::OmDetTurboModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/pix2struct/test_modeling_pix2struct.py::Pix2StructModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw46] [ 48%] PASSED tests/models/markuplm/test_modeling_markuplm.py::MarkupLMModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/olmo3/test_modeling_olmo3.py::Olmo3ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw40] [ 49%] PASSED tests/models/pegasus/test_modeling_pegasus.py::PegasusStandaloneDecoderModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw17] [ 49%] PASSED tests/models/owlv2/test_modeling_owlv2.py::Owlv2TextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/owlv2/test_modeling_owlv2.py::Owlv2ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/pop2piano/test_modeling_pop2piano.py::Pop2PianoModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw42] [ 49%] PASSED tests/models/phi4_multimodal/test_modeling_phi4_multimodal.py::Phi4MultimodalModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/reformer/test_modeling_reformer.py::ReformerLocalAttnModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw61] [ 49%] PASSED tests/models/fastspeech2_conformer/test_modeling_fastspeech2_conformer.py::FastSpeech2ConformerModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/qwen2_5_vl/test_modeling_qwen2_5_vl.py::Qwen2_5_VLModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw51] [ 49%] FAILED tests/models/mllama/test_modeling_mllama.py::MllamaForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/qwen3_vl_moe/test_modeling_qwen3_vl_moe.py::Qwen3VLMoeModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw9] [ 50%] PASSED tests/models/owlv2/test_modeling_owlv2.py::Owlv2VisionModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/rembert/test_modeling_rembert.py::RemBertModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw55] [ 50%] PASSED tests/models/janus/test_modeling_janus.py::JanusVQModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/patchtst/test_modeling_patchtst.py::PatchTSTModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw25] [ 50%] PASSED tests/models/parakeet/test_modeling_parakeet.py::ParakeetEncoderModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/poolformer/test_modeling_poolformer.py::PoolFormerModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw45] [ 50%] PASSED tests/models/efficientloftr/test_modeling_efficientloftr.py::EfficientLoFTRModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/mpnet/test_modeling_mpnet.py::MPNetModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw2] [ 50%] PASSED tests/models/mgp_str/test_modeling_mgp_str.py::MgpstrModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/qwen3/test_modeling_qwen3.py::Qwen3ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw4] [ 51%] PASSED tests/models/opt/test_modeling_opt.py::OPTModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/phimoe/test_modeling_phimoe.py::PhimoeModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw39] [ 51%] PASSED tests/models/longt5/test_modeling_longt5.py::LongT5ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/paligemma/test_modeling_paligemma.py::PaliGemmaForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw18] [ 51%] FAILED tests/models/clvp/test_modeling_clvp.py::ClvpModelForConditionalGenerationTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/flava/test_modeling_flava.py::FlavaMultimodalModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw52] [ 51%] PASSED tests/models/layoutlmv3/test_modeling_layoutlmv3.py::LayoutLMv3ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/mpt/test_modeling_mpt.py::MptModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw36] [ 51%] PASSED tests/models/moshi/test_modeling_moshi.py::MoshiDecoderTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/qwen2_5_omni/test_modeling_qwen2_5_omni.py::Qwen2_5OmniThinkerForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw24] [ 52%] PASSED tests/models/idefics/test_modeling_idefics.py::IdeficsModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/ovis2/test_modeling_ovis2.py::Ovis2ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw19] [ 52%] PASSED tests/models/prompt_depth_anything/test_modeling_prompt_depth_anything.py::PromptDepthAnythingModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/qwen2_vl/test_modeling_qwen2_vl.py::Qwen2VLModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw33] [ 52%] PASSED tests/models/oneformer/test_modeling_oneformer.py::OneFormerModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/qwen2_moe/test_modeling_qwen2_moe.py::Qwen2MoeModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw35] [ 52%] PASSED tests/models/deit/test_modeling_deit.py::DeiTModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/pvt_v2/test_modeling_pvt_v2.py::PvtV2ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw5] [ 53%] PASSED tests/models/lxmert/test_modeling_lxmert.py::LxmertModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/owlv2/test_modeling_owlv2.py::Owlv2ForObjectDetectionTest::test_torch_export <- tests/test_modeling_common.py 
[gw54] [ 53%] PASSED tests/models/mimi/test_modeling_mimi.py::MimiModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/olmo2/test_modeling_olmo2.py::Olmo2ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw8] [ 53%] PASSED tests/models/owlvit/test_modeling_owlvit.py::OwlViTModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/pegasus_x/test_modeling_pegasus_x.py::PegasusXModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw1] [ 53%] PASSED tests/models/maskformer/test_modeling_maskformer.py::MaskFormerModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/mobilebert/test_modeling_mobilebert.py::MobileBertModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw18] [ 53%] PASSED tests/models/flava/test_modeling_flava.py::FlavaMultimodalModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/seamless_m4t/test_modeling_seamless_m4t.py::SeamlessM4TModelWithSpeechInputTest::test_torch_export <- tests/test_modeling_common.py 
[gw41] [ 54%] PASSED tests/models/moshi/test_modeling_moshi.py::MoshiTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/plbart/test_modeling_plbart.py::PLBartModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw43] [ 54%] PASSED tests/models/mra/test_modeling_mra.py::MraModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/patchtsmixer/test_modeling_patchtsmixer.py::PatchTSMixerModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw63] [ 54%] PASSED tests/models/paligemma2/test_modeling_paligemma2.py::PaliGemma2ForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/pixtral/test_modeling_pixtral.py::PixtralVisionModelModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw51] [ 54%] PASSED tests/models/qwen3_vl_moe/test_modeling_qwen3_vl_moe.py::Qwen3VLMoeModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/rt_detr_v2/test_modeling_rt_detr_v2.py::RTDetrV2ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw29] [ 54%] PASSED tests/models/bridgetower/test_modeling_bridgetower.py::BridgeTowerModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/granitemoeshared/test_modeling_granitemoeshared.py::GraniteMoeSharedModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw63] [ 55%] PASSED tests/models/pixtral/test_modeling_pixtral.py::PixtralVisionModelModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/siglip2/test_modeling_siglip2.py::Siglip2TextModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw53] [ 55%] PASSED tests/models/mamba/test_modeling_mamba.py::MambaModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/perceiver/test_modeling_perceiver.py::PerceiverModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw40] [ 55%] PASSED tests/models/pop2piano/test_modeling_pop2piano.py::Pop2PianoModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw61] [ 55%] PASSED tests/models/qwen2_5_vl/test_modeling_qwen2_5_vl.py::Qwen2_5_VLModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/rt_detr/test_modeling_rt_detr.py::RTDetrModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw57] [ 55%] PASSED tests/models/owlvit/test_modeling_owlvit.py::OwlViTTextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/roc_bert/test_modeling_roc_bert.py::RoCBertModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/prophetnet/test_modeling_prophetnet.py::ProphetNetStandaloneDecoderModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw47] [ 56%] PASSED tests/models/cwm/test_modeling_cwm.py::CwmModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/regnet/test_modeling_regnet.py::RegNetModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw36] [ 56%] PASSED tests/models/qwen2_5_omni/test_modeling_qwen2_5_omni.py::Qwen2_5OmniThinkerForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/seamless_m4t_v2/test_modeling_seamless_m4t_v2.py::SeamlessM4Tv2ModelWithSpeechInputTest::test_torch_export <- tests/test_modeling_common.py 
[gw31] [ 56%] PASSED tests/models/megatron_bert/test_modeling_megatron_bert.py::MegatronBertModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/olmo/test_modeling_olmo.py::OlmoModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw19] [ 56%] PASSED tests/models/qwen2_vl/test_modeling_qwen2_vl.py::Qwen2VLModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/seed_oss/test_modeling_seed_oss.py::SeedOssModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw6] [ 57%] PASSED tests/models/granitemoehybrid/test_modeling_granitemoehybrid.py::GraniteMoeHybridModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw58] [ 57%] PASSED tests/models/pix2struct/test_modeling_pix2struct.py::Pix2StructModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/lilt/test_modeling_lilt.py::LiltModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/resnet/test_modeling_resnet.py::ResNetModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw14] [ 57%] PASSED tests/models/glpn/test_modeling_glpn.py::GLPNModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/longt5/test_modeling_longt5.py::LongT5TGlobalModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw46] [ 57%] PASSED tests/models/olmo3/test_modeling_olmo3.py::Olmo3ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/roberta/test_modeling_roberta.py::RobertaModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw16] [ 57%] PASSED tests/models/phi/test_modeling_phi.py::PhiModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/qwen3_vl/test_modeling_qwen3_vl.py::Qwen3VLModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw15] [ 58%] PASSED tests/models/gemma3n/test_modeling_gemma3n.py::Gemma3nTextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/mlcd/test_modeling_mlcd.py::MLCDVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw59] [ 58%] PASSED tests/models/mistral/test_modeling_mistral.py::MistralModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/prophetnet/test_modeling_prophetnet.py::ProphetNetModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw63] [ 58%] PASSED tests/models/siglip2/test_modeling_siglip2.py::Siglip2TextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/smollm3/test_modeling_smollm3.py::SmolLM3ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw30] [ 58%] PASSED tests/models/hunyuan_v1_moe/test_modeling_hunyuan_v1_moe.py::HunYuanMoEV1ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/openai/test_modeling_openai.py::OpenAIGPTModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw37] [ 58%] PASSED tests/models/mobilenet_v1/test_modeling_mobilenet_v1.py::MobileNetV1ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/prophetnet/test_modeling_prophetnet.py::ProphetNetStandaloneEncoderModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw62] [ 59%] PASSED tests/models/dia/test_modeling_dia.py::DiaModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw11] [ 59%] PASSED tests/models/musicgen/test_modeling_musicgen.py::MusicgenTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/persimmon/test_modeling_persimmon.py::PersimmonModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/parakeet/test_modeling_parakeet.py::ParakeetForCTCModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw23] [ 59%] PASSED tests/models/mbart/test_modeling_mbart.py::MBartModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/musicgen_melody/test_modeling_musicgen_melody.py::MusicgenMelodyDecoderTest::test_torch_export <- tests/test_modeling_common.py 
[gw25] [ 59%] PASSED tests/models/poolformer/test_modeling_poolformer.py::PoolFormerModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/sam/test_modeling_sam.py::SamModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw34] [ 59%] PASSED tests/models/focalnet/test_modeling_focalnet.py::FocalNetModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/plbart/test_modeling_plbart.py::PLBartStandaloneDecoderModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw60] [ 60%] PASSED tests/models/ministral/test_modeling_ministral.py::MinistralModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/phi3/test_modeling_phi3.py::Phi3ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw10] [ 60%] PASSED tests/models/nystromformer/test_modeling_nystromformer.py::NystromformerModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/recurrent_gemma/test_modeling_recurrent_gemma.py::RecurrentGemmaModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw54] [ 60%] PASSED tests/models/olmo2/test_modeling_olmo2.py::Olmo2ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/sew_d/test_modeling_sew_d.py::SEWDModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw15] [ 60%] PASSED tests/models/mlcd/test_modeling_mlcd.py::MLCDVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/superpoint/test_modeling_superpoint.py::SuperPointModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw37] [ 61%] PASSED tests/models/prophetnet/test_modeling_prophetnet.py::ProphetNetStandaloneEncoderModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/swinv2/test_modeling_swinv2.py::Swinv2ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw3] [ 61%] PASSED tests/models/big_bird/test_modeling_big_bird.py::BigBirdModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/bigbird_pegasus/test_modeling_bigbird_pegasus.py::BigBirdPegasusModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw17] [ 61%] PASSED tests/models/owlv2/test_modeling_owlv2.py::Owlv2ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/roberta_prelayernorm/test_modeling_roberta_prelayernorm.py::RobertaPreLayerNormModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw56] [ 61%] PASSED tests/models/nemotron/test_modeling_nemotron.py::NemotronModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/qwen3_next/test_modeling_qwen3_next.py::Qwen3NextModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw42] [ 61%] PASSED tests/models/reformer/test_modeling_reformer.py::ReformerLocalAttnModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/roformer/test_modeling_roformer.py::RoFormerModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw44] [ 62%] PASSED tests/models/d_fine/test_modeling_d_fine.py::DFineModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/dab_detr/test_modeling_dab_detr.py::DabDetrModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw16] [ 62%] PASSED tests/models/qwen3_vl/test_modeling_qwen3_vl.py::Qwen3VLModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/superglue/test_modeling_superglue.py::SuperGlueModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw15] [ 62%] PASSED tests/models/superpoint/test_modeling_superpoint.py::SuperPointModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/textnet/test_modeling_textnet.py::TextNetModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw52] [ 62%] PASSED tests/models/mpt/test_modeling_mpt.py::MptModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/seamless_m4t/test_modeling_seamless_m4t.py::SeamlessM4TModelWithTextInputTest::test_torch_export <- tests/test_modeling_common.py 
[gw38] [ 62%] PASSED tests/models/perception_lm/test_modeling_perception_lm.py::PerceptionLMForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/qwen3_omni_moe/test_modeling_qwen3_omni_moe.py::Qwen2_5OmniThinkerForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw31] [ 63%] PASSED tests/models/olmo/test_modeling_olmo.py::OlmoModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/speecht5/test_modeling_speecht5.py::SpeechT5ForSpeechToSpeechTest::test_torch_export <- tests/test_modeling_common.py 
[gw55] [ 63%] PASSED tests/models/patchtst/test_modeling_patchtst.py::PatchTSTModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/sam/test_modeling_sam.py::SamVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw21] [ 63%] PASSED tests/models/mt5/test_modeling_mt5.py::MT5ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/musicgen_melody/test_modeling_musicgen_melody.py::MusicgenMelodyTest::test_torch_export <- tests/test_modeling_common.py 
[gw23] [ 63%] PASSED tests/models/musicgen_melody/test_modeling_musicgen_melody.py::MusicgenMelodyDecoderTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/t5/test_modeling_t5.py::T5ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw57] [ 63%] PASSED tests/models/prophetnet/test_modeling_prophetnet.py::ProphetNetStandaloneDecoderModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/speecht5/test_modeling_speecht5.py::SpeechT5ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw24] [ 64%] PASSED tests/models/ovis2/test_modeling_ovis2.py::Ovis2ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/seamless_m4t_v2/test_modeling_seamless_m4t_v2.py::SeamlessM4Tv2ModelWithTextInputTest::test_torch_export <- tests/test_modeling_common.py 
[gw43] [ 64%] PASSED tests/models/patchtsmixer/test_modeling_patchtsmixer.py::PatchTSMixerModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/siglip2/test_modeling_siglip2.py::Siglip2VisionModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw34] [ 64%] PASSED tests/models/plbart/test_modeling_plbart.py::PLBartStandaloneDecoderModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/t5gemma/test_modeling_t5gemma.py::T5GemmaModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw45] [ 64%] PASSED tests/models/mpnet/test_modeling_mpnet.py::MPNetModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/sam2/test_modeling_sam2.py::Sam2VisionModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw47] [ 64%] PASSED tests/models/regnet/test_modeling_regnet.py::RegNetModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/speecht5/test_modeling_speecht5.py::SpeechT5ForSpeechToTextTest::test_torch_export <- tests/test_modeling_common.py 
[gw11] [ 65%] PASSED tests/models/parakeet/test_modeling_parakeet.py::ParakeetForCTCModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/switch_transformers/test_modeling_switch_transformers.py::SwitchTransformersEncoderOnlyModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw55] [ 65%] PASSED tests/models/sam/test_modeling_sam.py::SamVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/unispeech/test_modeling_unispeech.py::UniSpeechRobustModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw39] [ 65%] PASSED tests/models/paligemma/test_modeling_paligemma.py::PaliGemmaForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/sam_hq/test_modeling_sam_hq.py::SamHQModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw50] [ 65%] PASSED tests/models/mask2former/test_modeling_mask2former.py::Mask2FormerModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/mobilevit/test_modeling_mobilevit.py::MobileViTModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw2] [ 66%] PASSED tests/models/qwen3/test_modeling_qwen3.py::Qwen3ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/sam2/test_modeling_sam2.py::Sam2ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw9] [ 66%] PASSED tests/models/rembert/test_modeling_rembert.py::RemBertModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/rwkv/test_modeling_rwkv.py::RwkvModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw27] [ 66%] PASSED tests/models/layoutlm/test_modeling_layoutlm.py::LayoutLMModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/qwen2/test_modeling_qwen2.py::Qwen2ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw5] [ 66%] PASSED tests/models/owlv2/test_modeling_owlv2.py::Owlv2ForObjectDetectionTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/sew/test_modeling_sew.py::SEWModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw32] [ 66%] PASSED tests/models/mobilevitv2/test_modeling_mobilevitv2.py::MobileViTV2ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/pix2struct/test_modeling_pix2struct.py::Pix2StructVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw10] [ 67%] PASSED tests/models/recurrent_gemma/test_modeling_recurrent_gemma.py::RecurrentGemmaModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/table_transformer/test_modeling_table_transformer.py::TableTransformerModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw22] [ 67%] PASSED tests/models/lfm2_vl/test_modeling_lfm2_vl.py::Lfm2VlModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/pvt/test_modeling_pvt.py::PvtModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw11] [ 67%] PASSED tests/models/switch_transformers/test_modeling_switch_transformers.py::SwitchTransformersEncoderOnlyModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/videomae/test_modeling_videomae.py::VideoMAEModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw20] [ 67%] PASSED tests/models/mixtral/test_modeling_mixtral.py::MixtralModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/owlvit/test_modeling_owlvit.py::OwlViTVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw8] [ 67%] PASSED tests/models/pegasus_x/test_modeling_pegasus_x.py::PegasusXModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/siglip/test_modeling_siglip.py::SiglipVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw38] [ 68%] PASSED tests/models/qwen3_omni_moe/test_modeling_qwen3_omni_moe.py::Qwen2_5OmniThinkerForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/umt5/test_modeling_umt5.py::UMT5ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw58] [ 68%] PASSED tests/models/resnet/test_modeling_resnet.py::ResNetModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/squeezebert/test_modeling_squeezebert.py::SqueezeBertModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw30] [ 68%] PASSED tests/models/openai/test_modeling_openai.py::OpenAIGPTModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/swin2sr/test_modeling_swin2sr.py::Swin2SRModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw25] [ 68%] PASSED tests/models/sam/test_modeling_sam.py::SamModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/t5/test_modeling_t5.py::T5EncoderOnlyModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw28] [ 68%] PASSED tests/models/mvp/test_modeling_mvp.py::MvpModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/qwen2_audio/test_modeling_qwen2_audio.py::Qwen2AudioForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw57] [ 69%] PASSED tests/models/speecht5/test_modeling_speecht5.py::SpeechT5ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/univnet/test_modeling_univnet.py::UnivNetModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw62] [ 69%] PASSED tests/models/persimmon/test_modeling_persimmon.py::PersimmonModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/switch_transformers/test_modeling_switch_transformers.py::SwitchTransformersModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw15] [ 69%] PASSED tests/models/textnet/test_modeling_textnet.py::TextNetModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/udop/test_modeling_udop.py::UdopModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw19] [ 69%] PASSED tests/models/seed_oss/test_modeling_seed_oss.py::SeedOssModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/speecht5/test_modeling_speecht5.py::SpeechT5HifiGanTest::test_torch_export <- tests/test_modeling_common.py 
[gw29] [ 70%] PASSED tests/models/granitemoeshared/test_modeling_granitemoeshared.py::GraniteMoeSharedModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/siglip2/test_modeling_siglip2.py::Siglip2ForImageClassificationModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw41] [ 70%] PASSED tests/models/plbart/test_modeling_plbart.py::PLBartModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/siglip/test_modeling_siglip.py::SiglipForImageClassificationModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw20] [ 70%] PASSED tests/models/owlvit/test_modeling_owlvit.py::OwlViTVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/vits/test_modeling_vits.py::VitsModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw48] [ 70%] PASSED tests/models/minimax/test_modeling_minimax.py::MiniMaxModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/owlvit/test_modeling_owlvit.py::OwlViTForObjectDetectionTest::test_torch_export <- tests/test_modeling_common.py 
[gw32] [ 70%] PASSED tests/models/pix2struct/test_modeling_pix2struct.py::Pix2StructVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/vitdet/test_modeling_vitdet.py::VitDetModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw60] [ 71%] PASSED tests/models/phi3/test_modeling_phi3.py::Phi3ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/t5gemma/test_modeling_t5gemma.py::T5GemmaEncoderOnlyModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw63] [ 71%] PASSED tests/models/smollm3/test_modeling_smollm3.py::SmolLM3ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/swin/test_modeling_swin.py::SwinModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw49] [ 71%] PASSED tests/models/mobilenet_v2/test_modeling_mobilenet_v2.py::MobileNetV2ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/pegasus/test_modeling_pegasus.py::PegasusModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw14] [ 71%] PASSED tests/models/longt5/test_modeling_longt5.py::LongT5TGlobalModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw12] [ 71%] PASSED tests/models/levit/test_modeling_levit.py::LevitModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/stablelm/test_modeling_stablelm.py::StableLmModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/olmoe/test_modeling_olmoe.py::OlmoeModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw8] [ 72%] PASSED tests/models/siglip/test_modeling_siglip.py::SiglipVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/vivit/test_modeling_vivit.py::VivitModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw46] [ 72%] PASSED tests/models/roberta/test_modeling_roberta.py::RobertaModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/starcoder2/test_modeling_starcoder2.py::Starcoder2ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw40] [ 72%] PASSED tests/models/roc_bert/test_modeling_roc_bert.py::RoCBertModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/smolvlm/test_modeling_smolvlm.py::SmolVLMForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw59] [ 72%] PASSED tests/models/prophetnet/test_modeling_prophetnet.py::ProphetNetModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/swiftformer/test_modeling_swiftformer.py::SwiftFormerModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw25] [ 72%] PASSED tests/models/t5/test_modeling_t5.py::T5EncoderOnlyModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/wav2vec2/test_modeling_wav2vec2.py::Wav2Vec2RobustModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw41] [ 73%] PASSED tests/models/siglip/test_modeling_siglip.py::SiglipForImageClassificationModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/x_clip/test_modeling_x_clip.py::XCLIPVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw54] [ 73%] PASSED tests/models/sew_d/test_modeling_sew_d.py::SEWDModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/tapas/test_modeling_tapas.py::TapasModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw19] [ 73%] PASSED tests/models/speecht5/test_modeling_speecht5.py::SpeechT5HifiGanTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/whisper/test_modeling_whisper.py::WhisperEncoderModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw47] [ 73%] PASSED tests/models/speecht5/test_modeling_speecht5.py::SpeechT5ForSpeechToTextTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/video_llava/test_modeling_video_llava.py::VideoLlavaForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw32] [ 74%] PASSED tests/models/vitdet/test_modeling_vitdet.py::VitDetModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/xcodec/test_modeling_xcodec.py::XcodecModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw17] [ 74%] PASSED tests/models/roberta_prelayernorm/test_modeling_roberta_prelayernorm.py::RobertaPreLayerNormModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/timesformer/test_modeling_timesformer.py::TimesformerModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw4] [ 74%] PASSED tests/models/phimoe/test_modeling_phimoe.py::PhimoeModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/sam_hq/test_modeling_sam_hq.py::SamHQVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw28] [ 74%] PASSED tests/models/qwen2_audio/test_modeling_qwen2_audio.py::Qwen2AudioForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/wav2vec2_bert/test_modeling_wav2vec2_bert.py::Wav2Vec2BertModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw31] [ 74%] PASSED tests/models/speecht5/test_modeling_speecht5.py::SpeechT5ForSpeechToSpeechTest::test_torch_export <- tests/test_modeling_common.py 
[gw8] [ 75%] PASSED tests/models/vivit/test_modeling_vivit.py::VivitModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/xmod/test_modeling_xmod.py::XmodModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/umt5/test_modeling_umt5.py::UMT5EncoderOnlyModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw19] [ 75%] PASSED tests/models/whisper/test_modeling_whisper.py::WhisperEncoderModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw41] [ 75%] PASSED tests/models/x_clip/test_modeling_x_clip.py::XCLIPVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/zoedepth/test_modeling_zoedepth.py::ZoeDepthModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw45] [ 75%] PASSED tests/models/sam2/test_modeling_sam2.py::Sam2VisionModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw5] [ 75%] PASSED tests/models/sew/test_modeling_sew.py::SEWModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/video_llama_3/test_modeling_video_llama_3.py::VideoLlama3ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/vit_msn/test_modeling_vit_msn.py::ViTMSNModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw57] [ 76%] PASSED tests/models/univnet/test_modeling_univnet.py::UnivNetModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/wav2vec2_conformer/test_modeling_wav2vec2_conformer.py::Wav2Vec2ConformerModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw35] [ 76%] PASSED tests/models/pvt_v2/test_modeling_pvt_v2.py::PvtV2ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/seggpt/test_modeling_seggpt.py::SegGptModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw45] [ 76%] PASSED tests/models/video_llama_3/test_modeling_video_llama_3.py::VideoLlama3ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw6] [ 76%] PASSED tests/models/lilt/test_modeling_lilt.py::LiltModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/splinter/test_modeling_splinter.py::SplinterModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw43] [ 76%] PASSED tests/models/siglip2/test_modeling_siglip2.py::Siglip2VisionModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/vaultgemma/test_modeling_vaultgemma.py::VaultGemmaModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw21] [ 77%] PASSED tests/models/musicgen_melody/test_modeling_musicgen_melody.py::MusicgenMelodyTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/unispeech_sat/test_modeling_unispeech_sat.py::UniSpeechSatModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw4] [ 77%] PASSED tests/models/sam_hq/test_modeling_sam_hq.py::SamHQVisionModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw42] [ 77%] PASSED tests/models/roformer/test_modeling_roformer.py::RoFormerModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/timm_wrapper/test_modeling_timm_wrapper.py::TimmWrapperModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw8] [ 77%] PASSED tests/models/xmod/test_modeling_xmod.py::XmodModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw58] [ 77%] PASSED tests/models/squeezebert/test_modeling_squeezebert.py::SqueezeBertModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/voxtral/test_modeling_voxtral.py::VoxtralForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw0] [ 78%] PASSED tests/models/luke/test_modeling_luke.py::LukeModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/reformer/test_modeling_reformer.py::ReformerLSHAttnModelTest::test_torch_export 
[gw5] [ 78%] PASSED tests/models/vit_msn/test_modeling_vit_msn.py::ViTMSNModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw55] [ 78%] PASSED tests/models/unispeech/test_modeling_unispeech.py::UniSpeechRobustModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/vilt/test_modeling_vilt.py::ViltModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw60] [ 78%] PASSED tests/models/t5gemma/test_modeling_t5gemma.py::T5GemmaEncoderOnlyModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/xglm/test_modeling_xglm.py::XGLMModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw33] [ 79%] PASSED tests/models/qwen2_moe/test_modeling_qwen2_moe.py::Qwen2MoeModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/segformer/test_modeling_segformer.py::SegformerModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw14] [ 79%] PASSED tests/models/stablelm/test_modeling_stablelm.py::StableLmModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/xlnet/test_modeling_xlnet.py::XLNetModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw27] [ 79%] PASSED tests/models/qwen2/test_modeling_qwen2.py::Qwen2ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/vit_mae/test_modeling_vit_mae.py::ViTMAEModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw31] [ 79%] PASSED tests/models/umt5/test_modeling_umt5.py::UMT5EncoderOnlyModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw30] [ 79%] PASSED tests/models/swin2sr/test_modeling_swin2sr.py::Swin2SRModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/wav2vec2/test_modeling_wav2vec2.py::Wav2Vec2ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw1] [ 80%] PASSED tests/models/mobilebert/test_modeling_mobilebert.py::MobileBertModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/siglip/test_modeling_siglip.py::SiglipTextModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw40] [ 80%] PASSED tests/models/smolvlm/test_modeling_smolvlm.py::SmolVLMForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw9] [ 80%] PASSED tests/models/rwkv/test_modeling_rwkv.py::RwkvModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/yoso/test_modeling_yoso.py::YosoModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/vit/test_modeling_vit.py::ViTModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw46] [ 80%] PASSED tests/models/starcoder2/test_modeling_starcoder2.py::Starcoder2ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/yolos/test_modeling_yolos.py::YolosModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw16] [ 80%] PASSED tests/models/superglue/test_modeling_superglue.py::SuperGlueModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/tvp/test_modeling_tvp.py::TVPModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw49] [ 81%] PASSED tests/models/pegasus/test_modeling_pegasus.py::PegasusModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/xlm_roberta_xl/test_modeling_xlm_roberta_xl.py::XLMRobertaXLModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw17] [ 81%] PASSED tests/models/timesformer/test_modeling_timesformer.py::TimesformerModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw48] [ 81%] PASSED tests/models/owlvit/test_modeling_owlvit.py::OwlViTForObjectDetectionTest::test_torch_export <- tests/test_modeling_common.py 
[gw55] [ 81%] PASSED tests/models/vilt/test_modeling_vilt.py::ViltModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/x_clip/test_modeling_x_clip.py::XCLIPModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw12] [ 81%] PASSED tests/models/olmoe/test_modeling_olmoe.py::OlmoeModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/xlstm/test_modeling_xlstm.py::xLSTMModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw32] [ 82%] PASSED tests/models/xcodec/test_modeling_xcodec.py::XcodecModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw43] [ 82%] PASSED tests/models/vaultgemma/test_modeling_vaultgemma.py::VaultGemmaModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw29] [ 82%] PASSED tests/models/siglip2/test_modeling_siglip2.py::Siglip2ForImageClassificationModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/whisper/test_modeling_whisper.py::WhisperStandaloneDecoderModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw58] [ 82%] PASSED tests/models/voxtral/test_modeling_voxtral.py::VoxtralForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw41] [ 83%] PASSED tests/models/zoedepth/test_modeling_zoedepth.py::ZoeDepthModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw1] [ 83%] PASSED tests/models/siglip/test_modeling_siglip.py::SiglipTextModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw11] [ 83%] FAILED tests/models/videomae/test_modeling_videomae.py::VideoMAEModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/vitpose_backbone/test_modeling_vitpose_backbone.py::VitPoseBackboneModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw37] [ 83%] PASSED tests/models/swinv2/test_modeling_swinv2.py::Swinv2ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/time_series_transformer/test_modeling_time_series_transformer.py::TimeSeriesTransformerModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw15] [ 83%] PASSED tests/models/udop/test_modeling_udop.py::UdopModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/whisper/test_modeling_whisper.py::WhisperModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw2] [ 84%] PASSED tests/models/sam2/test_modeling_sam2.py::Sam2ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/visual_bert/test_modeling_visual_bert.py::VisualBertModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw60] [ 84%] PASSED tests/models/xglm/test_modeling_xglm.py::XGLMModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw7] [ 84%] PASSED tests/models/grounding_dino/test_modeling_grounding_dino.py::GroundingDinoModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/longt5/test_modeling_longt5.py::LongT5EncoderOnlyModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw24] [ 84%] PASSED tests/models/seamless_m4t_v2/test_modeling_seamless_m4t_v2.py::SeamlessM4Tv2ModelWithTextInputTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/upernet/test_modeling_upernet.py::UperNetModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw35] [ 84%] PASSED tests/models/seggpt/test_modeling_seggpt.py::SegGptModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw47] [ 85%] PASSED tests/models/video_llava/test_modeling_video_llava.py::VideoLlavaForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw46] [ 85%] PASSED tests/models/yolos/test_modeling_yolos.py::YolosModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw6] [ 85%] PASSED tests/models/splinter/test_modeling_splinter.py::SplinterModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw27] [ 85%] PASSED tests/models/vit_mae/test_modeling_vit_mae.py::ViTMAEModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw42] [ 85%] PASSED tests/models/timm_wrapper/test_modeling_timm_wrapper.py::TimmWrapperModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw9] [ 86%] PASSED tests/models/vit/test_modeling_vit.py::ViTModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw11] [ 86%] PASSED tests/models/vitpose_backbone/test_modeling_vitpose_backbone.py::VitPoseBackboneModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw62] [ 86%] PASSED tests/models/switch_transformers/test_modeling_switch_transformers.py::SwitchTransformersModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/wavlm/test_modeling_wavlm.py::WavLMModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw29] [ 86%] PASSED tests/models/whisper/test_modeling_whisper.py::WhisperStandaloneDecoderModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw22] [ 87%] PASSED tests/models/pvt/test_modeling_pvt.py::PvtModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/vitpose/test_modeling_vitpose.py::VitPoseModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw59] [ 87%] PASSED tests/models/swiftformer/test_modeling_swiftformer.py::SwiftFormerModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/zamba/test_modeling_zamba.py::ZambaModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw10] [ 87%] PASSED tests/models/table_transformer/test_modeling_table_transformer.py::TableTransformerModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/vitmatte/test_modeling_vitmatte.py::VitMatteModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw52] [ 87%] PASSED tests/models/seamless_m4t/test_modeling_seamless_m4t.py::SeamlessM4TModelWithTextInputTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/udop/test_modeling_udop.py::UdopEncoderOnlyModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw23] [ 87%] PASSED tests/models/t5/test_modeling_t5.py::T5ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/unispeech_sat/test_modeling_unispeech_sat.py::UniSpeechSatRobustModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw0] [ 88%] PASSED tests/models/reformer/test_modeling_reformer.py::ReformerLSHAttnModelTest::test_torch_export 
[gw7] [ 88%] PASSED tests/models/longt5/test_modeling_longt5.py::LongT5EncoderOnlyModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw48] [ 88%] PASSED tests/models/x_clip/test_modeling_x_clip.py::XCLIPModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw18] [ 88%] PASSED tests/models/seamless_m4t/test_modeling_seamless_m4t.py::SeamlessM4TModelWithSpeechInputTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/siglip/test_modeling_siglip.py::SiglipModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw61] [ 88%] PASSED tests/models/rt_detr/test_modeling_rt_detr.py::RTDetrModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/speech_to_text/test_modeling_speech_to_text.py::Speech2TextModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw63] [ 89%] PASSED tests/models/swin/test_modeling_swin.py::SwinModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/xlm/test_modeling_xlm.py::XLMModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw36] [ 89%] PASSED tests/models/seamless_m4t_v2/test_modeling_seamless_m4t_v2.py::SeamlessM4Tv2ModelWithSpeechInputTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/speecht5/test_modeling_speecht5.py::SpeechT5ForTextToSpeechTest::test_torch_export <- tests/test_modeling_common.py 
[gw44] [ 89%] PASSED tests/models/dab_detr/test_modeling_dab_detr.py::DabDetrModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/trocr/test_modeling_trocr.py::TrOCRStandaloneDecoderModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw22] [ 89%] PASSED tests/models/vitpose/test_modeling_vitpose.py::VitPoseModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw53] [ 89%] PASSED tests/models/perceiver/test_modeling_perceiver.py::PerceiverModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/smolvlm/test_modeling_smolvlm.py::SmolVLMModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw51] [ 90%] PASSED tests/models/rt_detr_v2/test_modeling_rt_detr_v2.py::RTDetrV2ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/siglip2/test_modeling_siglip2.py::Siglip2ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw52] [ 90%] PASSED tests/models/udop/test_modeling_udop.py::UdopEncoderOnlyModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw16] [ 90%] PASSED tests/models/tvp/test_modeling_tvp.py::TVPModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw20] [ 90%] PASSED tests/models/vits/test_modeling_vits.py::VitsModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/x_clip/test_modeling_x_clip.py::XCLIPTextModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw15] [ 90%] PASSED tests/models/whisper/test_modeling_whisper.py::WhisperModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw38] [ 91%] PASSED tests/models/umt5/test_modeling_umt5.py::UMT5ModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/vjepa2/test_modeling_vjepa2.py::VJEPA2ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw13] [ 91%] PASSED tests/models/modernbert/test_modeling_modernbert.py::ModernBertModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/pix2struct/test_modeling_pix2struct.py::Pix2StructTextModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw10] [ 91%] PASSED tests/models/vitmatte/test_modeling_vitmatte.py::VitMatteModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw54] [ 91%] PASSED tests/models/tapas/test_modeling_tapas.py::TapasModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw49] [ 92%] PASSED tests/models/xlm_roberta_xl/test_modeling_xlm_roberta_xl.py::XLMRobertaXLModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw21] [ 92%] PASSED tests/models/unispeech_sat/test_modeling_unispeech_sat.py::UniSpeechSatModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw20] [ 92%] PASSED tests/models/x_clip/test_modeling_x_clip.py::XCLIPTextModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw44] [ 92%] PASSED tests/models/trocr/test_modeling_trocr.py::TrOCRStandaloneDecoderModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw25] [ 92%] PASSED tests/models/wav2vec2/test_modeling_wav2vec2.py::Wav2Vec2RobustModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/zamba2/test_modeling_zamba2.py::Zamba2ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw40] [ 93%] PASSED tests/models/yoso/test_modeling_yoso.py::YosoModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw30] [ 93%] PASSED tests/models/wav2vec2/test_modeling_wav2vec2.py::Wav2Vec2ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw13] [ 93%] PASSED tests/models/pix2struct/test_modeling_pix2struct.py::Pix2StructTextModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw2] [ 93%] PASSED tests/models/visual_bert/test_modeling_visual_bert.py::VisualBertModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw37] [ 93%] PASSED tests/models/time_series_transformer/test_modeling_time_series_transformer.py::TimeSeriesTransformerModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw50] [ 94%] PASSED tests/models/mobilevit/test_modeling_mobilevit.py::MobileViTModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/vipllava/test_modeling_vipllava.py::VipLlavaForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw39] [ 94%] PASSED tests/models/sam_hq/test_modeling_sam_hq.py::SamHQModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw18] [ 94%] PASSED tests/models/siglip/test_modeling_siglip.py::SiglipModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/vilt/test_modeling_vilt.py::ViltForImagesAndTextClassificationModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw36] [ 94%] PASSED tests/models/speecht5/test_modeling_speecht5.py::SpeechT5ForTextToSpeechTest::test_torch_export <- tests/test_modeling_common.py 
[gw59] [ 94%] FAILED tests/models/zamba/test_modeling_zamba.py::ZambaModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw39] [ 95%] PASSED tests/models/vilt/test_modeling_vilt.py::ViltForImagesAndTextClassificationModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw28] [ 95%] PASSED tests/models/wav2vec2_bert/test_modeling_wav2vec2_bert.py::Wav2Vec2BertModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw26] [ 95%] PASSED tests/models/mm_grounding_dino/test_modeling_mm_grounding_dino.py::MMGroundingDinoModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/qwen3_moe/test_modeling_qwen3_moe.py::Qwen3MoeModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw23] [ 95%] PASSED tests/models/unispeech_sat/test_modeling_unispeech_sat.py::UniSpeechSatRobustModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw3] [ 96%] PASSED tests/models/bigbird_pegasus/test_modeling_bigbird_pegasus.py::BigBirdPegasusModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/timesfm/test_modeling_timesfm.py::TimesFmModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw53] [ 96%] PASSED tests/models/smolvlm/test_modeling_smolvlm.py::SmolVLMModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw12] [ 96%] PASSED tests/models/xlstm/test_modeling_xlstm.py::xLSTMModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw14] [ 96%] PASSED tests/models/xlnet/test_modeling_xlnet.py::XLNetModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw62] [ 96%] PASSED tests/models/wavlm/test_modeling_wavlm.py::WavLMModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw24] [ 97%] PASSED tests/models/upernet/test_modeling_upernet.py::UperNetModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw61] [ 97%] PASSED tests/models/speech_to_text/test_modeling_speech_to_text.py::Speech2TextModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw33] [ 97%] PASSED tests/models/segformer/test_modeling_segformer.py::SegformerModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw34] [ 97%] PASSED tests/models/t5gemma/test_modeling_t5gemma.py::T5GemmaModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/video_llama_3/test_modeling_video_llama_3.py::VideoLlama3VisionModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw3] [ 97%] PASSED tests/models/timesfm/test_modeling_timesfm.py::TimesFmModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw34] [ 98%] PASSED tests/models/video_llama_3/test_modeling_video_llama_3.py::VideoLlama3VisionModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw50] [ 98%] PASSED tests/models/vipllava/test_modeling_vipllava.py::VipLlavaForConditionalGenerationModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw63] [ 98%] PASSED tests/models/xlm/test_modeling_xlm.py::XLMModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw57] [ 98%] PASSED tests/models/wav2vec2_conformer/test_modeling_wav2vec2_conformer.py::Wav2Vec2ConformerModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw51] [ 98%] PASSED tests/models/siglip2/test_modeling_siglip2.py::Siglip2ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw38] [ 99%] PASSED tests/models/vjepa2/test_modeling_vjepa2.py::VJEPA2ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw25] [ 99%] PASSED tests/models/zamba2/test_modeling_zamba2.py::Zamba2ModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw26] [ 99%] PASSED tests/models/qwen3_moe/test_modeling_qwen3_moe.py::Qwen3MoeModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw56] [ 99%] PASSED tests/models/qwen3_next/test_modeling_qwen3_next.py::Qwen3NextModelTest::test_torch_export <- tests/test_modeling_common.py 
tests/models/timm_backbone/test_modeling_timm_backbone.py::TimmBackboneModelTest::test_torch_export <- tests/test_modeling_common.py 
[gw56] [100%] FAILED tests/models/timm_backbone/test_modeling_timm_backbone.py::TimmBackboneModelTest::test_torch_export <- tests/test_modeling_common.py 

=================================== FAILURES ===================================
______________ EomtForUniversalSegmentationTest.test_torch_export ______________
[gw27] linux -- Python 3.10.12 /home/ilyas/transformers/.docker/bin/python3

self = <tests.models.eomt.test_modeling_eomt.EomtForUniversalSegmentationTest testMethod=test_torch_export>
atol = 0.0001, rtol = 0.0001

    @slow
    @require_torch_greater_or_equal("2.5")
    @pytest.mark.torch_export_test
    def test_torch_export(self, atol=1e-4, rtol=1e-4):
        """
        Test if model can be exported with torch.export.export()
    
        Args:
            atol (`float`, *optional*, defaults to 1e-4): absolute tolerance for output comparison
            rtol (`float`, *optional*, defaults to 1e-4): relative tolerance for output comparison
        """
    
        exporter = DynamoExporter(export_config=DynamoConfig())
    
        for model_class in self.all_model_classes:
            with self.subTest(model_class.__name__):
                if hasattr(self.model_tester, "prepare_config_and_inputs_for_model_class"):
                    config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_model_class(model_class)
                else:
                    config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()
                inputs_dict = self._prepare_for_class(inputs_dict, model_class)
                model = model_class(config).eval().to(torch_device)
    
                # prepare cache inputs for auto-regressive models and include it for computing eager outputs
                # process output flags (e.g. use_cache, output_attentions, etc) to avoid passing them as inputs
                model, inputs_dict = prepare_for_export(model, inputs_dict)
    
                with torch.no_grad():
                    # Running the eager inference before the export to catch model/inputs comatibility issues, also sometimes after
                    # the export, the model used for export will return FakeTensors instead of real ones (torch cuda/inductor issue)
                    # This happens on cuda with (codegen, clvp, esm, gptj, levit, wav2vec2_bert and wav2vec2_conformer)
                    set_seed(1234)
                    eager_outputs = model(**copy.deepcopy(inputs_dict))
                    eager_outputs = get_leaf_tensors(eager_outputs)
                    self.assertTrue(eager_outputs, "Eager outputs is empty.")
    
                try:
>                   exported_program = exporter.export(model, inputs_dict)

tests/test_modeling_common.py:3532: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/exporters/exporter_dynamo.py:122: in export
    exported_program: ExportedProgram = torch.export.export(
.docker/lib/python3.10/site-packages/torch/export/__init__.py:311: in export
    raise e
.docker/lib/python3.10/site-packages/torch/export/__init__.py:277: in export
    return _export(
.docker/lib/python3.10/site-packages/torch/export/_trace.py:1163: in wrapper
    raise e
.docker/lib/python3.10/site-packages/torch/export/_trace.py:1129: in wrapper
    ep = fn(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/export/exported_program.py:124: in wrapper
    return fn(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/export/_trace.py:2255: in _export
    ep = _export_for_training(
.docker/lib/python3.10/site-packages/torch/export/_trace.py:1163: in wrapper
    raise e
.docker/lib/python3.10/site-packages/torch/export/_trace.py:1129: in wrapper
    ep = fn(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/export/exported_program.py:124: in wrapper
    return fn(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/export/_trace.py:2071: in _export_for_training
    export_artifact = export_func(
.docker/lib/python3.10/site-packages/torch/export/_trace.py:2002: in _non_strict_export
    aten_export_artifact = _to_aten_func(  # type: ignore[operator]
.docker/lib/python3.10/site-packages/torch/export/_trace.py:1793: in _export_to_aten_ir_make_fx
    gm, graph_signature = transform(_make_fx_helper)(
.docker/lib/python3.10/site-packages/torch/export/_trace.py:1922: in _aot_export_non_strict
    gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags)
.docker/lib/python3.10/site-packages/torch/export/_trace.py:1706: in _make_fx_helper
    gm = make_fx(
.docker/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:2429: in wrapped
    return make_fx_tracer.trace(f, *args)
.docker/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:2356: in trace
    return self._trace_inner(f, *args)
.docker/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:2318: in _trace_inner
    t = dispatch_trace(
.docker/lib/python3.10/site-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1303: in dispatch_trace
    graph = tracer.trace(root, concrete_args)  # type: ignore[arg-type]
.docker/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1908: in trace
    res = super().trace(root, concrete_args)
.docker/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:868: in trace
    (self.create_arg(fn(*args)),),
.docker/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1361: in wrapped
    out = f(*tensors)  # type:ignore[call-arg]
<string>:1: in <lambda>
    ???
.docker/lib/python3.10/site-packages/torch/export/_trace.py:1593: in wrapped_fn
    return tuple(flat_fn(*args))
.docker/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:187: in flat_fn
    tree_out = fn(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/graph_capture_wrappers.py:1354: in functional_call
    out = mod(*args[params_len:], **kwargs)
.docker/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:843: in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
.docker/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1997: in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
.docker/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:560: in call_module
    ret_val = forward(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:836: in forward
    return _orig_module_call(mod, *args, **kwargs)
.docker/lib/python3.10/site-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/export/_trace.py:1906: in forward
    tree_out = mod(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:843: in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
.docker/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1997: in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
.docker/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:560: in call_module
    ret_val = forward(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:836: in forward
    return _orig_module_call(mod, *args, **kwargs)
.docker/lib/python3.10/site-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
src/transformers/utils/generic.py:928: in wrapper
    outputs = func(self, *args, **kwargs)
src/transformers/models/eomt/modeling_eomt.py:1151: in forward
    attention_mask = self._disable_attention_mask(
src/transformers/models/eomt/modeling_eomt.py:1215: in _disable_attention_mask
    if prob < 1:
.docker/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1409: in __torch_function__
    return func(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1479: in __torch_function__
    return func(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/_export/non_strict_utils.py:1066: in __torch_function__
    return func(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/fx/experimental/sym_node.py:538: in guard_bool
    r = self.evaluate()
.docker/lib/python3.10/site-packages/torch/fx/experimental/sym_node.py:512: in evaluate
    return self.shape_env.evaluate_sym_node(self, size_oblivious)
.docker/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:7233: in evaluate_sym_node
    return self.evaluate_expr(
.docker/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:7333: in evaluate_expr
    return self._inner_evaluate_expr(
.docker/lib/python3.10/site-packages/torch/fx/experimental/recording.py:272: in wrapper
    return retlog(fn(*args, **kwargs))
.docker/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:7356: in _inner_evaluate_expr
    return self._evaluate_expr(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <torch.fx.experimental.symbolic_shapes.ShapeEnv object at 0x7f3e10bd6650>
orig_expr = Eq(u0, 1), hint = None, fx_node = False, size_oblivious = False
fallback_value = None

    def _evaluate_expr(
        self,
        orig_expr: sympy.Basic,
        hint: Optional[Union[bool, int, float]] = None,
        fx_node: Optional[torch.fx.Node] = None,
        size_oblivious: bool = False,
        fallback_value: Optional[bool] = None,
        *,
        forcing_spec: bool = False,
    ) -> sympy.Basic:
        # TODO: split conjunctions and evaluate them separately
    
        if isinstance(
            orig_expr,
            (sympy.logic.boolalg.BooleanTrue, sympy.logic.boolalg.BooleanFalse),
        ):
            return orig_expr
    
        # Don't track this one. (Because this cache is inside this function the
        # cache only lasts for the invocation of this function call)
        @functools.cache
        def compute_concrete_val() -> sympy.Basic:
            if hint is None:
                # This is only ever called for expressions WITHOUT unbacked
                # symbols
                r = self.size_hint(orig_expr)
                assert r is not None
                return r
            else:
                return sympy.sympify(hint)
    
        concrete_val: Optional[sympy.Basic]
    
        # Check if:
        #   1. 'translation_validation' is set
        #   2. the corresponding 'fx_node' is not 'None'
        #   3. the guard should not be suppressed
        #   4. the guard doesn't contain backed symfloat symbols
        #      since z3 can't handle floats
        #   5. fallback_value is none.
        # If all of the above check, we create an FX node representing the
        # actual expression to be guarded.
        node = None
        fresh = False
        if (
            self._translation_validation_enabled
            and fx_node is not None
            and not self._suppress_guards_tls()
            and not size_oblivious
            and not any(symbol_is_type(s, SymT.FLOAT) for s in orig_expr.free_symbols)
            and fallback_value is None
        ):
            # TODO: does this even worked with unbacked :think:
            concrete_val = compute_concrete_val()
            if concrete_val is sympy.true:
                node, fresh = self._create_fx_call_function(torch._assert, (fx_node,))
            elif concrete_val is sympy.false:
                neg, _ = self._create_fx_call_function(operator.not_, (fx_node,))
                node, fresh = self._create_fx_call_function(torch._assert, (neg,))
            else:
                eql, _ = self._create_fx_call_function(
                    operator.eq, (fx_node, concrete_val)
                )
                node, fresh = self._create_fx_call_function(torch._assert, (eql,))
    
            assert node is not None
            # If this is a fresh node, we have to remember the event index that
            # corresponds to this assertion node.
            # Reason: so that, given an assertion node, we can replay the ShapeEnv
            # events until the point where this assertion node was freshly created.
            if fresh:
                self._add_fx_node_metadata(node)
    
        # After creating the FX node corresponding to orig_expr, we must make sure that
        # no error will be raised until the end of this function.
        #
        # Reason: the translation validation may become invalid otherwise.
        #
        # If an error is raised before the end of this function, we remove the FX node
        # inserted, and re-raise the error.
        guard = None
    
        try:
            if orig_expr.is_number:
                self.log.debug("eval %s [trivial]", orig_expr)
                if hint is not None:
                    if isinstance(hint, bool):
                        assert orig_expr == hint, f"{orig_expr} != {hint}"
                    else:
                        assert sympy.Eq(orig_expr, hint), f"{orig_expr} != {hint}"
                return orig_expr
    
            expr = orig_expr
    
            static_expr = self._maybe_evaluate_static(
                expr, size_oblivious=size_oblivious
            )
            if static_expr is not None:
                self.log.debug(
                    "eval %s == %s [statically known]",
                    (
                        f"size_oblivious({orig_expr})"
                        if size_oblivious
                        else size_oblivious
                    ),
                    static_expr,
                )
                if (
                    not size_oblivious
                    and config.backed_size_oblivious
                    and hint is not None
                ):
                    # TODO: maybe reconcile this with use of counterfactual hints
                    # in unbacked case
                    assert static_expr == hint, f"{static_expr} != {hint}"
                return static_expr
    
            transmute_into_runtime_assert = False
    
            concrete_val = None
            if not (expr.free_symbols <= self.var_to_val.keys()):
                # TODO: dedupe this with _maybe_evaluate_static
                # Attempt to eliminate the unbacked SymInt
                new_expr = self._maybe_evaluate_static(expr, unbacked_only=True)
                assert new_expr is not None
                if not (new_expr.free_symbols <= self.var_to_val.keys()):
                    ok = False
    
                    # fallback_value is set when guard_or_true or guard_or_false are used.
                    if not ok and fallback_value is not None:
                        self._log_suppressed_dde(orig_expr, fallback_value)
                        return fallback_value
    
                    # oblivious_var_to_val will be defined iff we have sizes with DimDynamic.OBLIVIOUS_SIZE type.
                    # See https://github.com/pytorch/pytorch/issues/137100#issuecomment-2495778113
                    if (
                        self.oblivious_var_to_val
                        and not (
                            correct_hint := orig_expr.xreplace(
                                self.oblivious_var_to_val
                            )
                        ).free_symbols
                        and not (
                            counterfactual_hint := orig_expr.xreplace(
                                {
                                    k: max(2, v)
                                    for k, v in self.oblivious_var_to_val.items()
                                }
                            )
                        ).free_symbols
                        and correct_hint == counterfactual_hint
                    ):
                        # TODO: better logging
                        log.info(
                            "oblivious_size %s -> %s (passed counterfactual)",
                            orig_expr,
                            correct_hint,
                        )
                        concrete_val = correct_hint
                        # NB: do NOT transmute into runtime assert
                        ok = True
    
                    # unbacked_var_to_val is not None iff propagate_real_tensors is on.
                    # if propagate_real_tensors is on, we check the example values to generate (unsound_result)
                    # and if they pass we add a runtime assertions and continue.
                    if (
                        not ok
                        and self.unbacked_var_to_val
                        and not (
                            unsound_result := orig_expr.xreplace(
                                self.unbacked_var_to_val
                            ).xreplace(self.var_to_val)
                        ).free_symbols
                    ):
                        self._log_real_tensor_propagation(orig_expr, unsound_result)
                        transmute_into_runtime_assert = True
                        concrete_val = unsound_result
                        ok = True
    
                    # Check if this is coming from a python assert statement, if so, convert it to a runtime assertion
                    # instead of failing.
                    if not ok and self.trace_asserts and self._is_python_assert():
                        concrete_val = sympy.true
                        transmute_into_runtime_assert = True
                        ok = True
    
                    if not ok:
>                       raise self._make_data_dependent_error(
                            expr.xreplace(self.var_to_val),
                            expr,
                            expr_sym_node_id=self._expr_sym_node_id,
                        )
E                       torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode: Could not guard on data-dependent expression Eq(u0, 1) (unhinted: Eq(u0, 1)).  (Size-like symbols: none)
E                       
E                       consider using data-dependent friendly APIs such as guard_or_false, guard_or_true and statically_known_trueCaused by: (_export/non_strict_utils.py:1066 in __torch_function__)
E                       For more information, run with TORCH_LOGS="dynamic"
E                       For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="u0"
E                       If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
E                       For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing
E                       
E                       For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
E                       
E                       The following call raised this error:
E                         File "/home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py", line 1215, in _disable_attention_mask
E                           if prob < 1:
E                       
E                       
E                       The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.

.docker/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:7574: GuardOnDataDependentSymNode
----------------------------- Captured stderr call -----------------------------
W1121 07:11:16.101000 21967 .docker/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:7942] Unable to find user code corresponding to {u0}



def forward(self, arg0_1: "f32[1, 1, 8]", arg1_1: "f32[1, 19, 8]", arg2_1: "f32[8, 3, 2, 2]", arg3_1: "f32[8]", arg4_1: "f32[400, 8]", arg5_1: "f32[8]", arg6_1: "f32[8]", arg7_1: "f32[5, 8]", arg8_1: "f32[8]", arg9_1: "f32[8]", arg10_1: "f32[8, 8]", arg11_1: "f32[8]", arg12_1: "f32[8, 8]", arg13_1: "f32[8]", arg14_1: "f32[8, 8]", arg15_1: "f32[8]", arg16_1: "f32[8, 8]", arg17_1: "f32[8]", arg18_1: "f32[8]", arg19_1: "f32[8]", arg20_1: "f32[8]", arg21_1: "f32[32, 8]", arg22_1: "f32[32]", arg23_1: "f32[8, 32]", arg24_1: "f32[8]", arg25_1: "f32[8]", arg26_1: "f32[8]", arg27_1: "f32[8]", arg28_1: "f32[8, 8]", arg29_1: "f32[8]", arg30_1: "f32[8, 8]", arg31_1: "f32[8]", arg32_1: "f32[8, 8]", arg33_1: "f32[8]", arg34_1: "f32[8, 8]", arg35_1: "f32[8]", arg36_1: "f32[8]", arg37_1: "f32[8]", arg38_1: "f32[8]", arg39_1: "f32[32, 8]", arg40_1: "f32[32]", arg41_1: "f32[8, 32]", arg42_1: "f32[8]", arg43_1: "f32[8]", arg44_1: "f32[8, 8, 2, 2]", arg45_1: "f32[8]", arg46_1: "f32[8, 1, 3, 3]", arg47_1: "f32[8]", arg48_1: "f32[8]", arg49_1: "f32[8, 8, 2, 2]", arg50_1: "f32[8]", arg51_1: "f32[8, 1, 3, 3]", arg52_1: "f32[8]", arg53_1: "f32[8]", arg54_1: "f32[8, 8]", arg55_1: "f32[8]", arg56_1: "f32[8, 8]", arg57_1: "f32[8]", arg58_1: "f32[8, 8]", arg59_1: "f32[8]", arg60_1: "f32[5, 8]", arg61_1: "f32[5]", arg62_1: "f32[1]", arg63_1: "i64[1, 400]", arg64_1: "f32[5]", arg65_1: "f32[2, 3, 40, 40]"):
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:701 in forward, code: embeddings = self.patch_embeddings(pixel_values.to(dtype=target_dtype))
    to: "f32[2, 3, 40, 40]" = torch.ops.aten.to.dtype(arg65_1, torch.float32);  arg65_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
    conv2d: "f32[2, 8, 20, 20]" = torch.ops.aten.conv2d.default(to, arg2_1, arg3_1, [2, 2]);  to = arg2_1 = arg3_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:673 in forward, code: embeddings = self.projection(pixel_values).flatten(2).transpose(1, 2)
    flatten: "f32[2, 8, 400]" = torch.ops.aten.flatten.using_ints(conv2d, 2);  conv2d = None
    transpose: "f32[2, 400, 8]" = torch.ops.aten.transpose.int(flatten, 1, 2);  flatten = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:703 in forward, code: cls_tokens = self.cls_token.expand(batch_size, -1, -1)
    expand: "f32[2, 1, 8]" = torch.ops.aten.expand.default(arg0_1, [2, -1, -1]);  arg0_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:704 in forward, code: register_tokens = self.register_tokens.expand(batch_size, -1, -1)
    expand_1: "f32[2, 19, 8]" = torch.ops.aten.expand.default(arg1_1, [2, -1, -1]);  arg1_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: "f32[1, 400, 8]" = torch.ops.aten.embedding.default(arg4_1, arg63_1);  arg4_1 = arg63_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:706 in forward, code: embeddings = embeddings + self.position_embeddings(self.position_ids)
    add: "f32[2, 400, 8]" = torch.ops.aten.add.Tensor(transpose, embedding);  transpose = embedding = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:707 in forward, code: embeddings = torch.cat([cls_tokens, register_tokens, embeddings], dim=1)
    cat: "f32[2, 420, 8]" = torch.ops.aten.cat.default([expand, expand_1, add], 1);  expand = expand_1 = add = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
    dropout: "f32[2, 420, 8]" = torch.ops.aten.dropout.default(cat, 0.0, False);  cat = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(
    layer_norm: "f32[2, 420, 8]" = torch.ops.aten.layer_norm.default(dropout, [8], arg8_1, arg9_1, 1e-06);  arg8_1 = arg9_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear: "f32[2, 420, 8]" = torch.ops.aten.linear.default(layer_norm, arg14_1, arg15_1);  arg14_1 = arg15_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_1: "f32[2, 420, 8]" = torch.ops.aten.linear.default(layer_norm, arg10_1, arg11_1);  arg10_1 = arg11_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_2: "f32[2, 420, 8]" = torch.ops.aten.linear.default(layer_norm, arg12_1, arg13_1);  layer_norm = arg12_1 = arg13_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:774 in forward, code: queries = queries.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)
    view: "f32[2, 420, 2, 4]" = torch.ops.aten.view.default(linear, [2, 420, 2, 4]);  linear = None
    transpose_1: "f32[2, 2, 420, 4]" = torch.ops.aten.transpose.int(view, 1, 2);  view = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:775 in forward, code: keys = keys.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)
    view_1: "f32[2, 420, 2, 4]" = torch.ops.aten.view.default(linear_1, [2, 420, 2, 4]);  linear_1 = None
    transpose_2: "f32[2, 2, 420, 4]" = torch.ops.aten.transpose.int(view_1, 1, 2);  view_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:776 in forward, code: values = values.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)
    view_2: "f32[2, 420, 2, 4]" = torch.ops.aten.view.default(linear_2, [2, 420, 2, 4]);  linear_2 = None
    transpose_3: "f32[2, 2, 420, 4]" = torch.ops.aten.transpose.int(view_2, 1, 2);  view_2 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:782 in forward, code: attn_output, attn_weights = attention_interface(
    scaled_dot_product_attention: "f32[2, 2, 420, 4]" = torch.ops.aten.scaled_dot_product_attention.default(transpose_1, transpose_2, transpose_3, scale = 0.5);  transpose_1 = transpose_2 = transpose_3 = None
    transpose_4: "f32[2, 420, 2, 4]" = torch.ops.aten.transpose.int(scaled_dot_product_attention, 1, 2);  scaled_dot_product_attention = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:793 in forward, code: attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()
    reshape: "f32[2, 420, 8]" = torch.ops.aten.reshape.default(transpose_4, [2, 420, 8]);  transpose_4 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_3: "f32[2, 420, 8]" = torch.ops.aten.linear.default(reshape, arg16_1, arg17_1);  reshape = arg16_1 = arg17_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:805 in forward, code: return hidden_state * self.lambda1
    mul: "f32[2, 420, 8]" = torch.ops.aten.mul.Tensor(linear_3, arg18_1);  linear_3 = arg18_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:902 in forward, code: hidden_states = self.drop_path(self_attention_output) + hidden_states
    add_1: "f32[2, 420, 8]" = torch.ops.aten.add.Tensor(mul, dropout);  mul = dropout = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(
    layer_norm_1: "f32[2, 420, 8]" = torch.ops.aten.layer_norm.default(add_1, [8], arg19_1, arg20_1, 1e-06);  arg19_1 = arg20_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_4: "f32[2, 420, 32]" = torch.ops.aten.linear.default(layer_norm_1, arg21_1, arg22_1);  layer_norm_1 = arg21_1 = arg22_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/activations.py:89 in forward, code: return self.act(input)
    gelu: "f32[2, 420, 32]" = torch.ops.aten.gelu.default(linear_4);  linear_4 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_5: "f32[2, 420, 8]" = torch.ops.aten.linear.default(gelu, arg23_1, arg24_1);  gelu = arg23_1 = arg24_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:805 in forward, code: return hidden_state * self.lambda1
    mul_1: "f32[2, 420, 8]" = torch.ops.aten.mul.Tensor(linear_5, arg25_1);  linear_5 = arg25_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:910 in forward, code: layer_output = self.drop_path(layer_output) + hidden_states
    add_2: "f32[2, 420, 8]" = torch.ops.aten.add.Tensor(mul_1, add_1);  mul_1 = add_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:1117 in forward, code: query = self.query.weight[None, :, :].expand(hidden_states.shape[0], -1, -1).to(hidden_states.device)
    unsqueeze: "f32[1, 5, 8]" = torch.ops.aten.unsqueeze.default(arg7_1, 0);  arg7_1 = None
    expand_2: "f32[2, 5, 8]" = torch.ops.aten.expand.default(unsqueeze, [2, -1, -1]);  unsqueeze = None
    to_1: "f32[2, 5, 8]" = torch.ops.aten.to.dtype_layout(expand_2, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0));  expand_2 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:1118 in forward, code: hidden_states = torch.cat((query, hidden_states), dim=1)
    cat_1: "f32[2, 425, 8]" = torch.ops.aten.cat.default([to_1, add_2], 1);  to_1 = add_2 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(
    layer_norm_2: "f32[2, 425, 8]" = torch.ops.aten.layer_norm.default(cat_1, [8], arg5_1, arg6_1, 1e-06);  cat_1 = arg5_1 = arg6_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:1126 in forward, code: masks_queries_logits, class_queries_logits = self.predict(norm_hidden_states)
    slice_1: "f32[2, 5, 8]" = torch.ops.aten.slice.Tensor(layer_norm_2, 1, 0, 5)
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_6: "f32[2, 5, 5]" = torch.ops.aten.linear.default(slice_1, arg60_1, arg61_1);  arg60_1 = arg61_1 = linear_6 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:1126 in forward, code: masks_queries_logits, class_queries_logits = self.predict(norm_hidden_states)
    slice_2: "f32[2, 400, 8]" = torch.ops.aten.slice.Tensor(layer_norm_2, 1, 25, 9223372036854775807);  layer_norm_2 = None
    transpose_5: "f32[2, 8, 400]" = torch.ops.aten.transpose.int(slice_2, 1, 2);  slice_2 = None
    reshape_1: "f32[2, 8, 20, 20]" = torch.ops.aten.reshape.default(transpose_5, [2, -1, 20, 20]);  transpose_5 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_7: "f32[2, 5, 8]" = torch.ops.aten.linear.default(slice_1, arg54_1, arg55_1);  slice_1 = arg54_1 = arg55_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/activations.py:89 in forward, code: return self.act(input)
    gelu_1: "f32[2, 5, 8]" = torch.ops.aten.gelu.default(linear_7);  linear_7 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_8: "f32[2, 5, 8]" = torch.ops.aten.linear.default(gelu_1, arg56_1, arg57_1);  gelu_1 = arg56_1 = arg57_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/activations.py:89 in forward, code: return self.act(input)
    gelu_2: "f32[2, 5, 8]" = torch.ops.aten.gelu.default(linear_8);  linear_8 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_9: "f32[2, 5, 8]" = torch.ops.aten.linear.default(gelu_2, arg58_1, arg59_1);  gelu_2 = arg58_1 = arg59_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/conv.py:1161 in forward, code: return F.conv_transpose2d(
    conv_transpose2d: "f32[2, 8, 40, 40]" = torch.ops.aten.conv_transpose2d.input(reshape_1, arg44_1, arg45_1, [2, 2]);  reshape_1 = arg44_1 = arg45_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/activations.py:89 in forward, code: return self.act(input)
    gelu_3: "f32[2, 8, 40, 40]" = torch.ops.aten.gelu.default(conv_transpose2d);  conv_transpose2d = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
    conv2d_1: "f32[2, 8, 40, 40]" = torch.ops.aten.conv2d.default(gelu_3, arg46_1, None, [1, 1], [1, 1], [1, 1], 8);  gelu_3 = arg46_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:920 in forward, code: hidden_state = hidden_state.permute(0, 2, 3, 1)
    permute: "f32[2, 40, 40, 8]" = torch.ops.aten.permute.default(conv2d_1, [0, 2, 3, 1]);  conv2d_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:921 in forward, code: hidden_state = F.layer_norm(hidden_state, self.normalized_shape, self.weight, self.bias, self.eps)
    layer_norm_3: "f32[2, 40, 40, 8]" = torch.ops.aten.layer_norm.default(permute, [8], arg47_1, arg48_1, 1e-06);  permute = arg47_1 = arg48_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:922 in forward, code: hidden_state = hidden_state.permute(0, 3, 1, 2)
    permute_1: "f32[2, 8, 40, 40]" = torch.ops.aten.permute.default(layer_norm_3, [0, 3, 1, 2]);  layer_norm_3 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/conv.py:1161 in forward, code: return F.conv_transpose2d(
    conv_transpose2d_1: "f32[2, 8, 80, 80]" = torch.ops.aten.conv_transpose2d.input(permute_1, arg49_1, arg50_1, [2, 2]);  permute_1 = arg49_1 = arg50_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/activations.py:89 in forward, code: return self.act(input)
    gelu_4: "f32[2, 8, 80, 80]" = torch.ops.aten.gelu.default(conv_transpose2d_1);  conv_transpose2d_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
    conv2d_2: "f32[2, 8, 80, 80]" = torch.ops.aten.conv2d.default(gelu_4, arg51_1, None, [1, 1], [1, 1], [1, 1], 8);  gelu_4 = arg51_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:920 in forward, code: hidden_state = hidden_state.permute(0, 2, 3, 1)
    permute_2: "f32[2, 80, 80, 8]" = torch.ops.aten.permute.default(conv2d_2, [0, 2, 3, 1]);  conv2d_2 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:921 in forward, code: hidden_state = F.layer_norm(hidden_state, self.normalized_shape, self.weight, self.bias, self.eps)
    layer_norm_4: "f32[2, 80, 80, 8]" = torch.ops.aten.layer_norm.default(permute_2, [8], arg52_1, arg53_1, 1e-06);  permute_2 = arg52_1 = arg53_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:922 in forward, code: hidden_state = hidden_state.permute(0, 3, 1, 2)
    permute_3: "f32[2, 8, 80, 80]" = torch.ops.aten.permute.default(layer_norm_4, [0, 3, 1, 2]);  layer_norm_4 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:1126 in forward, code: masks_queries_logits, class_queries_logits = self.predict(norm_hidden_states)
    einsum: "f32[2, 5, 80, 80]" = torch.ops.aten.einsum.default('bqc, bchw -> bqhw', [linear_9, permute_3]);  linear_9 = permute_3 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:1131 in forward, code: attention_mask = torch.ones(
    ones: "b8[2, 425, 425]" = torch.ops.aten.ones.default([2, 425, 425], dtype = torch.bool, device = device(type='cuda', index=0), pin_memory = False)
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:1139 in forward, code: interpolated_logits = F.interpolate(masks_queries_logits, size=self.grid_size, mode="bilinear")
    upsample_bilinear2d: "f32[2, 5, 20, 20]" = torch.ops.aten.upsample_bilinear2d.vec(einsum, [20, 20], False, None);  einsum = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:1140 in forward, code: interpolated_logits = interpolated_logits.view(
    view_3: "f32[2, 5, 400]" = torch.ops.aten.view.default(upsample_bilinear2d, [2, 5, -1]);  upsample_bilinear2d = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:1148 in forward, code: attention_mask[:, :num_query_tokens, encoder_start_tokens:] = interpolated_logits > 0
    gt: "b8[2, 5, 400]" = torch.ops.aten.gt.Scalar(view_3, 0);  view_3 = None
    slice_3: "b8[2, 5, 425]" = torch.ops.aten.slice.Tensor(ones, 1, 0, 5);  ones = None
    slice_4: "b8[2, 5, 400]" = torch.ops.aten.slice.Tensor(slice_3, 2, 25, 9223372036854775807);  slice_3 = None
    copy_: "b8[2, 5, 400]" = torch.ops.aten.copy_.default(slice_4, gt);  slice_4 = gt = copy_ = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:1153 in forward, code: prob=self.attn_mask_probs[idx - self.num_hidden_layers + self.config.num_blocks],
    select: "f32[]" = torch.ops.aten.select.int(arg62_1, 0, 0);  arg62_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:1151 in forward, code: attention_mask = self._disable_attention_mask(
    lt: "b8[]" = torch.ops.aten.lt.Scalar(select, 1);  select = None
    ne: "b8[]" = torch.ops.aten.ne.Scalar(lt, 0);  lt = None
    item: "Sym(Eq(u0, 1))" = torch.ops.aten.item.default(ne);  ne = item = None
    



def forward(self, arg0_1: "f32[1, 1, 8]", arg1_1: "f32[1, 19, 8]", arg2_1: "f32[8, 3, 2, 2]", arg3_1: "f32[8]", arg4_1: "f32[400, 8]", arg5_1: "f32[8]", arg6_1: "f32[8]", arg7_1: "f32[5, 8]", arg8_1: "f32[8]", arg9_1: "f32[8]", arg10_1: "f32[8, 8]", arg11_1: "f32[8]", arg12_1: "f32[8, 8]", arg13_1: "f32[8]", arg14_1: "f32[8, 8]", arg15_1: "f32[8]", arg16_1: "f32[8, 8]", arg17_1: "f32[8]", arg18_1: "f32[8]", arg19_1: "f32[8]", arg20_1: "f32[8]", arg21_1: "f32[32, 8]", arg22_1: "f32[32]", arg23_1: "f32[8, 32]", arg24_1: "f32[8]", arg25_1: "f32[8]", arg26_1: "f32[8]", arg27_1: "f32[8]", arg28_1: "f32[8, 8]", arg29_1: "f32[8]", arg30_1: "f32[8, 8]", arg31_1: "f32[8]", arg32_1: "f32[8, 8]", arg33_1: "f32[8]", arg34_1: "f32[8, 8]", arg35_1: "f32[8]", arg36_1: "f32[8]", arg37_1: "f32[8]", arg38_1: "f32[8]", arg39_1: "f32[32, 8]", arg40_1: "f32[32]", arg41_1: "f32[8, 32]", arg42_1: "f32[8]", arg43_1: "f32[8]", arg44_1: "f32[8, 8, 2, 2]", arg45_1: "f32[8]", arg46_1: "f32[8, 1, 3, 3]", arg47_1: "f32[8]", arg48_1: "f32[8]", arg49_1: "f32[8, 8, 2, 2]", arg50_1: "f32[8]", arg51_1: "f32[8, 1, 3, 3]", arg52_1: "f32[8]", arg53_1: "f32[8]", arg54_1: "f32[8, 8]", arg55_1: "f32[8]", arg56_1: "f32[8, 8]", arg57_1: "f32[8]", arg58_1: "f32[8, 8]", arg59_1: "f32[8]", arg60_1: "f32[5, 8]", arg61_1: "f32[5]", arg62_1: "f32[1]", arg63_1: "i64[1, 400]", arg64_1: "f32[5]", arg65_1: "f32[2, 3, 40, 40]"):
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:701 in forward, code: embeddings = self.patch_embeddings(pixel_values.to(dtype=target_dtype))
    to: "f32[2, 3, 40, 40]" = torch.ops.aten.to.dtype(arg65_1, torch.float32);  arg65_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
    conv2d: "f32[2, 8, 20, 20]" = torch.ops.aten.conv2d.default(to, arg2_1, arg3_1, [2, 2]);  to = arg2_1 = arg3_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:673 in forward, code: embeddings = self.projection(pixel_values).flatten(2).transpose(1, 2)
    flatten: "f32[2, 8, 400]" = torch.ops.aten.flatten.using_ints(conv2d, 2);  conv2d = None
    transpose: "f32[2, 400, 8]" = torch.ops.aten.transpose.int(flatten, 1, 2);  flatten = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:703 in forward, code: cls_tokens = self.cls_token.expand(batch_size, -1, -1)
    expand: "f32[2, 1, 8]" = torch.ops.aten.expand.default(arg0_1, [2, -1, -1]);  arg0_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:704 in forward, code: register_tokens = self.register_tokens.expand(batch_size, -1, -1)
    expand_1: "f32[2, 19, 8]" = torch.ops.aten.expand.default(arg1_1, [2, -1, -1]);  arg1_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: "f32[1, 400, 8]" = torch.ops.aten.embedding.default(arg4_1, arg63_1);  arg4_1 = arg63_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:706 in forward, code: embeddings = embeddings + self.position_embeddings(self.position_ids)
    add: "f32[2, 400, 8]" = torch.ops.aten.add.Tensor(transpose, embedding);  transpose = embedding = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:707 in forward, code: embeddings = torch.cat([cls_tokens, register_tokens, embeddings], dim=1)
    cat: "f32[2, 420, 8]" = torch.ops.aten.cat.default([expand, expand_1, add], 1);  expand = expand_1 = add = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
    dropout: "f32[2, 420, 8]" = torch.ops.aten.dropout.default(cat, 0.0, False);  cat = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(
    layer_norm: "f32[2, 420, 8]" = torch.ops.aten.layer_norm.default(dropout, [8], arg8_1, arg9_1, 1e-06);  arg8_1 = arg9_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear: "f32[2, 420, 8]" = torch.ops.aten.linear.default(layer_norm, arg14_1, arg15_1);  arg14_1 = arg15_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_1: "f32[2, 420, 8]" = torch.ops.aten.linear.default(layer_norm, arg10_1, arg11_1);  arg10_1 = arg11_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_2: "f32[2, 420, 8]" = torch.ops.aten.linear.default(layer_norm, arg12_1, arg13_1);  layer_norm = arg12_1 = arg13_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:774 in forward, code: queries = queries.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)
    view: "f32[2, 420, 2, 4]" = torch.ops.aten.view.default(linear, [2, 420, 2, 4]);  linear = None
    transpose_1: "f32[2, 2, 420, 4]" = torch.ops.aten.transpose.int(view, 1, 2);  view = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:775 in forward, code: keys = keys.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)
    view_1: "f32[2, 420, 2, 4]" = torch.ops.aten.view.default(linear_1, [2, 420, 2, 4]);  linear_1 = None
    transpose_2: "f32[2, 2, 420, 4]" = torch.ops.aten.transpose.int(view_1, 1, 2);  view_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:776 in forward, code: values = values.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)
    view_2: "f32[2, 420, 2, 4]" = torch.ops.aten.view.default(linear_2, [2, 420, 2, 4]);  linear_2 = None
    transpose_3: "f32[2, 2, 420, 4]" = torch.ops.aten.transpose.int(view_2, 1, 2);  view_2 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:782 in forward, code: attn_output, attn_weights = attention_interface(
    scaled_dot_product_attention: "f32[2, 2, 420, 4]" = torch.ops.aten.scaled_dot_product_attention.default(transpose_1, transpose_2, transpose_3, scale = 0.5);  transpose_1 = transpose_2 = transpose_3 = None
    transpose_4: "f32[2, 420, 2, 4]" = torch.ops.aten.transpose.int(scaled_dot_product_attention, 1, 2);  scaled_dot_product_attention = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:793 in forward, code: attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()
    reshape: "f32[2, 420, 8]" = torch.ops.aten.reshape.default(transpose_4, [2, 420, 8]);  transpose_4 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_3: "f32[2, 420, 8]" = torch.ops.aten.linear.default(reshape, arg16_1, arg17_1);  reshape = arg16_1 = arg17_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:805 in forward, code: return hidden_state * self.lambda1
    mul: "f32[2, 420, 8]" = torch.ops.aten.mul.Tensor(linear_3, arg18_1);  linear_3 = arg18_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:902 in forward, code: hidden_states = self.drop_path(self_attention_output) + hidden_states
    add_1: "f32[2, 420, 8]" = torch.ops.aten.add.Tensor(mul, dropout);  mul = dropout = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(
    layer_norm_1: "f32[2, 420, 8]" = torch.ops.aten.layer_norm.default(add_1, [8], arg19_1, arg20_1, 1e-06);  arg19_1 = arg20_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_4: "f32[2, 420, 32]" = torch.ops.aten.linear.default(layer_norm_1, arg21_1, arg22_1);  layer_norm_1 = arg21_1 = arg22_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/activations.py:89 in forward, code: return self.act(input)
    gelu: "f32[2, 420, 32]" = torch.ops.aten.gelu.default(linear_4);  linear_4 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_5: "f32[2, 420, 8]" = torch.ops.aten.linear.default(gelu, arg23_1, arg24_1);  gelu = arg23_1 = arg24_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:805 in forward, code: return hidden_state * self.lambda1
    mul_1: "f32[2, 420, 8]" = torch.ops.aten.mul.Tensor(linear_5, arg25_1);  linear_5 = arg25_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:910 in forward, code: layer_output = self.drop_path(layer_output) + hidden_states
    add_2: "f32[2, 420, 8]" = torch.ops.aten.add.Tensor(mul_1, add_1);  mul_1 = add_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:1117 in forward, code: query = self.query.weight[None, :, :].expand(hidden_states.shape[0], -1, -1).to(hidden_states.device)
    unsqueeze: "f32[1, 5, 8]" = torch.ops.aten.unsqueeze.default(arg7_1, 0);  arg7_1 = None
    expand_2: "f32[2, 5, 8]" = torch.ops.aten.expand.default(unsqueeze, [2, -1, -1]);  unsqueeze = None
    to_1: "f32[2, 5, 8]" = torch.ops.aten.to.dtype_layout(expand_2, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0));  expand_2 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:1118 in forward, code: hidden_states = torch.cat((query, hidden_states), dim=1)
    cat_1: "f32[2, 425, 8]" = torch.ops.aten.cat.default([to_1, add_2], 1);  to_1 = add_2 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(
    layer_norm_2: "f32[2, 425, 8]" = torch.ops.aten.layer_norm.default(cat_1, [8], arg5_1, arg6_1, 1e-06);  cat_1 = arg5_1 = arg6_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:1126 in forward, code: masks_queries_logits, class_queries_logits = self.predict(norm_hidden_states)
    slice_1: "f32[2, 5, 8]" = torch.ops.aten.slice.Tensor(layer_norm_2, 1, 0, 5)
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_6: "f32[2, 5, 5]" = torch.ops.aten.linear.default(slice_1, arg60_1, arg61_1);  arg60_1 = arg61_1 = linear_6 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:1126 in forward, code: masks_queries_logits, class_queries_logits = self.predict(norm_hidden_states)
    slice_2: "f32[2, 400, 8]" = torch.ops.aten.slice.Tensor(layer_norm_2, 1, 25, 9223372036854775807);  layer_norm_2 = None
    transpose_5: "f32[2, 8, 400]" = torch.ops.aten.transpose.int(slice_2, 1, 2);  slice_2 = None
    reshape_1: "f32[2, 8, 20, 20]" = torch.ops.aten.reshape.default(transpose_5, [2, -1, 20, 20]);  transpose_5 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_7: "f32[2, 5, 8]" = torch.ops.aten.linear.default(slice_1, arg54_1, arg55_1);  slice_1 = arg54_1 = arg55_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/activations.py:89 in forward, code: return self.act(input)
    gelu_1: "f32[2, 5, 8]" = torch.ops.aten.gelu.default(linear_7);  linear_7 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_8: "f32[2, 5, 8]" = torch.ops.aten.linear.default(gelu_1, arg56_1, arg57_1);  gelu_1 = arg56_1 = arg57_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/activations.py:89 in forward, code: return self.act(input)
    gelu_2: "f32[2, 5, 8]" = torch.ops.aten.gelu.default(linear_8);  linear_8 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_9: "f32[2, 5, 8]" = torch.ops.aten.linear.default(gelu_2, arg58_1, arg59_1);  gelu_2 = arg58_1 = arg59_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/conv.py:1161 in forward, code: return F.conv_transpose2d(
    conv_transpose2d: "f32[2, 8, 40, 40]" = torch.ops.aten.conv_transpose2d.input(reshape_1, arg44_1, arg45_1, [2, 2]);  reshape_1 = arg44_1 = arg45_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/activations.py:89 in forward, code: return self.act(input)
    gelu_3: "f32[2, 8, 40, 40]" = torch.ops.aten.gelu.default(conv_transpose2d);  conv_transpose2d = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
    conv2d_1: "f32[2, 8, 40, 40]" = torch.ops.aten.conv2d.default(gelu_3, arg46_1, None, [1, 1], [1, 1], [1, 1], 8);  gelu_3 = arg46_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:920 in forward, code: hidden_state = hidden_state.permute(0, 2, 3, 1)
    permute: "f32[2, 40, 40, 8]" = torch.ops.aten.permute.default(conv2d_1, [0, 2, 3, 1]);  conv2d_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:921 in forward, code: hidden_state = F.layer_norm(hidden_state, self.normalized_shape, self.weight, self.bias, self.eps)
    layer_norm_3: "f32[2, 40, 40, 8]" = torch.ops.aten.layer_norm.default(permute, [8], arg47_1, arg48_1, 1e-06);  permute = arg47_1 = arg48_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:922 in forward, code: hidden_state = hidden_state.permute(0, 3, 1, 2)
    permute_1: "f32[2, 8, 40, 40]" = torch.ops.aten.permute.default(layer_norm_3, [0, 3, 1, 2]);  layer_norm_3 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/conv.py:1161 in forward, code: return F.conv_transpose2d(
    conv_transpose2d_1: "f32[2, 8, 80, 80]" = torch.ops.aten.conv_transpose2d.input(permute_1, arg49_1, arg50_1, [2, 2]);  permute_1 = arg49_1 = arg50_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/activations.py:89 in forward, code: return self.act(input)
    gelu_4: "f32[2, 8, 80, 80]" = torch.ops.aten.gelu.default(conv_transpose2d_1);  conv_transpose2d_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)
    conv2d_2: "f32[2, 8, 80, 80]" = torch.ops.aten.conv2d.default(gelu_4, arg51_1, None, [1, 1], [1, 1], [1, 1], 8);  gelu_4 = arg51_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:920 in forward, code: hidden_state = hidden_state.permute(0, 2, 3, 1)
    permute_2: "f32[2, 80, 80, 8]" = torch.ops.aten.permute.default(conv2d_2, [0, 2, 3, 1]);  conv2d_2 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:921 in forward, code: hidden_state = F.layer_norm(hidden_state, self.normalized_shape, self.weight, self.bias, self.eps)
    layer_norm_4: "f32[2, 80, 80, 8]" = torch.ops.aten.layer_norm.default(permute_2, [8], arg52_1, arg53_1, 1e-06);  permute_2 = arg52_1 = arg53_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:922 in forward, code: hidden_state = hidden_state.permute(0, 3, 1, 2)
    permute_3: "f32[2, 8, 80, 80]" = torch.ops.aten.permute.default(layer_norm_4, [0, 3, 1, 2]);  layer_norm_4 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:1126 in forward, code: masks_queries_logits, class_queries_logits = self.predict(norm_hidden_states)
    einsum: "f32[2, 5, 80, 80]" = torch.ops.aten.einsum.default('bqc, bchw -> bqhw', [linear_9, permute_3]);  linear_9 = permute_3 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:1131 in forward, code: attention_mask = torch.ones(
    ones: "b8[2, 425, 425]" = torch.ops.aten.ones.default([2, 425, 425], dtype = torch.bool, device = device(type='cuda', index=0), pin_memory = False)
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:1139 in forward, code: interpolated_logits = F.interpolate(masks_queries_logits, size=self.grid_size, mode="bilinear")
    upsample_bilinear2d: "f32[2, 5, 20, 20]" = torch.ops.aten.upsample_bilinear2d.vec(einsum, [20, 20], False, None);  einsum = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:1140 in forward, code: interpolated_logits = interpolated_logits.view(
    view_3: "f32[2, 5, 400]" = torch.ops.aten.view.default(upsample_bilinear2d, [2, 5, -1]);  upsample_bilinear2d = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:1148 in forward, code: attention_mask[:, :num_query_tokens, encoder_start_tokens:] = interpolated_logits > 0
    gt: "b8[2, 5, 400]" = torch.ops.aten.gt.Scalar(view_3, 0);  view_3 = None
    slice_3: "b8[2, 5, 425]" = torch.ops.aten.slice.Tensor(ones, 1, 0, 5);  ones = None
    slice_4: "b8[2, 5, 400]" = torch.ops.aten.slice.Tensor(slice_3, 2, 25, 9223372036854775807);  slice_3 = None
    copy_: "b8[2, 5, 400]" = torch.ops.aten.copy_.default(slice_4, gt);  slice_4 = gt = copy_ = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:1153 in forward, code: prob=self.attn_mask_probs[idx - self.num_hidden_layers + self.config.num_blocks],
    select: "f32[]" = torch.ops.aten.select.int(arg62_1, 0, 0);  arg62_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py:1151 in forward, code: attention_mask = self._disable_attention_mask(
    lt: "b8[]" = torch.ops.aten.lt.Scalar(select, 1);  select = None
    ne: "b8[]" = torch.ops.aten.ne.Scalar(lt, 0);  lt = None
    item: "Sym(Eq(u0, 1))" = torch.ops.aten.item.default(ne);  ne = item = None
    
__________________ FlavaForPreTrainingTest.test_torch_export ___________________
[gw1] linux -- Python 3.10.12 /home/ilyas/transformers/.docker/bin/python3

self = <tests.models.flava.test_modeling_flava.FlavaForPreTrainingTest testMethod=test_torch_export>
atol = 0.0001, rtol = 0.0001

    @slow
    @require_torch_greater_or_equal("2.5")
    @pytest.mark.torch_export_test
    def test_torch_export(self, atol=1e-4, rtol=1e-4):
        """
        Test if model can be exported with torch.export.export()
    
        Args:
            atol (`float`, *optional*, defaults to 1e-4): absolute tolerance for output comparison
            rtol (`float`, *optional*, defaults to 1e-4): relative tolerance for output comparison
        """
    
        exporter = DynamoExporter(export_config=DynamoConfig())
    
        for model_class in self.all_model_classes:
            with self.subTest(model_class.__name__):
                if hasattr(self.model_tester, "prepare_config_and_inputs_for_model_class"):
                    config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_model_class(model_class)
                else:
                    config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()
                inputs_dict = self._prepare_for_class(inputs_dict, model_class)
                model = model_class(config).eval().to(torch_device)
    
                # prepare cache inputs for auto-regressive models and include it for computing eager outputs
                # process output flags (e.g. use_cache, output_attentions, etc) to avoid passing them as inputs
                model, inputs_dict = prepare_for_export(model, inputs_dict)
    
                with torch.no_grad():
                    # Running the eager inference before the export to catch model/inputs comatibility issues, also sometimes after
                    # the export, the model used for export will return FakeTensors instead of real ones (torch cuda/inductor issue)
                    # This happens on cuda with (codegen, clvp, esm, gptj, levit, wav2vec2_bert and wav2vec2_conformer)
                    set_seed(1234)
>                   eager_outputs = model(**copy.deepcopy(inputs_dict))

tests/test_modeling_common.py:3527: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.docker/lib/python3.10/site-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = FlavaForPreTraining(
  (flava): FlavaModel(
    (text_model): FlavaTextModel(
      (embeddings): FlavaTextEmbeddings(
        (word_embeddings): Embedding(102, 32, padding_idx=0)
        (position_embeddings): Embedding(512, 32)
        (token_type_embeddings): Embedding(2, 32)
        (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (encoder): FlavaEncoder(
        (layer): ModuleList(
          (0-1): 2 x FlavaLayer(
            (attention): FlavaAttention(
              (attention): FlavaSelfAttention(
                (query): Linear(in_features=32, out_features=32, bias=True)
                (key): Linear(in_features=32, out_features=32, bias=True)
                (value): Linear(in_features=32, out_features=32, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): FlavaSelfOutput(
                (dense): Linear(in_features=32, out_features=32, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (intermediate): FlavaIntermediate(
              (dense): Linear(in_features=32, out_features=37, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): FlavaOutput(
              (dense): Linear(in_features=37, out_features=32, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (layernorm_before): LayerNorm((32,), eps=1e-12, elementwise_affine=True)
            (layernorm_after): LayerNorm((32,), eps=1e-12, elementwise_affine=True)
          )
        )
      )
      (layernorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)
      (pooler): FlavaPooler(
        (dense): Linear(in_features=32, out_features=32, bias=True)
        (activation): Tanh()
      )
    )
    (image_model): FlavaImageModel(
      (embeddings): FlavaImageEmbeddings(
        (patch_embeddings): PatchEmbeddings(
          (projection): Conv2d(3, 32, kernel_size=(2, 2), stride=(2, 2))
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (encoder): FlavaEncoder(
        (layer): ModuleList(
          (0-1): 2 x FlavaLayer(
            (attention): FlavaAttention(
              (attention): FlavaSelfAttention(
                (query): Linear(in_features=32, out_features=32, bias=True)
                (key): Linear(in_features=32, out_features=32, bias=True)
                (value): Linear(in_features=32, out_features=32, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): FlavaSelfOutput(
                (dense): Linear(in_features=32, out_features=32, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (intermediate): FlavaIntermediate(
              (dense): Linear(in_features=32, out_features=37, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): FlavaOutput(
              (dense): Linear(in_features=37, out_features=32, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (layernorm_before): LayerNorm((32,), eps=1e-12, elementwise_affine=True)
            (layernorm_after): LayerNorm((32,), eps=1e-12, elementwise_affine=True)
          )
        )
      )
      (layernorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)
      (pooler): FlavaPooler(
        (dense): Linear(in_features=32, out_features=32, bias=True)
        (activation): Tanh()
      )
    )
    (multimodal_model): FlavaMultimodalModel(
      (encoder): FlavaEncoder(
        (layer): ModuleList(
          (0-1): 2 x FlavaLayer(
            (attention): FlavaAttention(
              (attention): FlavaSelfAttention(
                (query): Linear(in_features=32, out_features=32, bias=True)
                (key): Linear(in_features=32, out_features=32, bias=True)
                (value): Linear(in_features=32, out_features=32, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
              (output): FlavaSelfOutput(
                (dense): Linear(in_features=32, out_features=32, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
              )
            )
            (intermediate): FlavaIntermediate(
              (dense): Linear(in_features=32, out_features=37, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): FlavaOutput(
              (dense): Linear(in_features=37, out_features=32, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (layernorm_before): LayerNorm((32,), eps=1e-12, elementwise_affine=True)
            (layernorm_after): LayerNorm((32,), eps=1e-12, elementwise_affine=True)
          )
        )
      )
      (layernorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)
      (pooler): FlavaPooler(
        (dense): Linear(in_features=32, out_features=32, bias=True)
        (activation): Tanh()
      )
    )
    (image_projection): Linear(in_features=32, out_features=32, bias=True)
    (text_projection): Linear(in_features=32, out_features=32, bias=True)
    (image_to_mm_projection): Linear(in_features=32, out_features=32, bias=True)
    (text_to_mm_projection): Linear(in_features=32, out_features=32, bias=True)
  )
  (image_codebook): FlavaImageCodebook(
    (blocks): Sequential(
      (input): Conv2d(3, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))
      (group_1): FlavaImageCodebookLayerGroup(
        (group): Sequential(
          (block_1): FlavaImageCodebookBlock(
            (id_path): Identity()
            (res_path): FlavaImageCodebookResPath(
              (path): Sequential(
                (relu_1): ReLU()
                (conv_1): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (relu_2): ReLU()
                (conv_2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (relu_3): ReLU()
                (conv_3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (relu_4): ReLU()
                (conv_4): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))
              )
            )
          )
          (block_2): FlavaImageCodebookBlock(
            (id_path): Identity()
            (res_path): FlavaImageCodebookResPath(
              (path): Sequential(
                (relu_1): ReLU()
                (conv_1): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (relu_2): ReLU()
                (conv_2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (relu_3): ReLU()
                (conv_3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (relu_4): ReLU()
                (conv_4): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))
              )
            )
          )
          (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        )
      )
      (group_2): FlavaImageCodebookLayerGroup(
        (group): Sequential(
          (block_1): FlavaImageCodebookBlock(
            (id_path): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))
            (res_path): FlavaImageCodebookResPath(
              (path): Sequential(
                (relu_1): ReLU()
                (conv_1): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (relu_2): ReLU()
                (conv_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (relu_3): ReLU()
                (conv_3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (relu_4): ReLU()
                (conv_4): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))
              )
            )
          )
          (block_2): FlavaImageCodebookBlock(
            (id_path): Identity()
            (res_path): FlavaImageCodebookResPath(
              (path): Sequential(
                (relu_1): ReLU()
                (conv_1): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (relu_2): ReLU()
                (conv_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (relu_3): ReLU()
                (conv_3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (relu_4): ReLU()
                (conv_4): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))
              )
            )
          )
          (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        )
      )
      (group_3): FlavaImageCodebookLayerGroup(
        (group): Sequential(
          (block_1): FlavaImageCodebookBlock(
            (id_path): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
            (res_path): FlavaImageCodebookResPath(
              (path): Sequential(
                (relu_1): ReLU()
                (conv_1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (relu_2): ReLU()
                (conv_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (relu_3): ReLU()
                (conv_3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (relu_4): ReLU()
                (conv_4): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
              )
            )
          )
          (block_2): FlavaImageCodebookBlock(
            (id_path): Identity()
            (res_path): FlavaImageCodebookResPath(
              (path): Sequential(
                (relu_1): ReLU()
                (conv_1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (relu_2): ReLU()
                (conv_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (relu_3): ReLU()
                (conv_3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (relu_4): ReLU()
                (conv_4): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
              )
            )
          )
          (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        )
      )
      (group_4): FlavaImageCodebookLayerGroup(
        (group): Sequential(
          (block_1): FlavaImageCodebookBlock(
            (id_path): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
            (res_path): FlavaImageCodebookResPath(
              (path): Sequential(
                (relu_1): ReLU()
                (conv_1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (relu_2): ReLU()
                (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (relu_3): ReLU()
                (conv_3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (relu_4): ReLU()
                (conv_4): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
              )
            )
          )
          (block_2): FlavaImageCodebookBlock(
            (id_path): Identity()
            (res_path): FlavaImageCodebookResPath(
              (path): Sequential(
                (relu_1): ReLU()
                (conv_1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (relu_2): ReLU()
                (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (relu_3): ReLU()
                (conv_3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (relu_4): ReLU()
                (conv_4): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
              )
            )
          )
        )
      )
      (output): Sequential(
        (relu): ReLU()
        (conv): Conv2d(256, 99, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
  (mim_head): FlavaMaskedPredictionHead(
    (transform): FlavaPredictionHeadTransform(
      (dense): Linear(in_features=32, out_features=32, bias=True)
      (transform_act_fn): GELUActivation()
      (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)
    )
    (decoder): Linear(in_features=32, out_features=99, bias=True)
  )
  (mlm_head): FlavaMaskedPredictionHead(
    (transform): FlavaPredictionHeadTransform(
      (dense): Linear(in_features=32, out_features=32, bias=True)
      (transform_act_fn): GELUActivation()
      (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)
    )
    (decoder): Linear(in_features=32, out_features=102, bias=True)
  )
  (itm_head): FlavaITMHead(
    (pooler): FlavaPooler(
      (dense): Linear(in_features=32, out_features=32, bias=True)
      (activation): Tanh()
    )
    (seq_relationship): Linear(in_features=32, out_features=2, bias=True)
  )
  (mmm_image_head): FlavaMaskedPredictionHead(
    (transform): FlavaPredictionHeadTransform(
      (dense): Linear(in_features=32, out_features=32, bias=True)
      (transform_act_fn): GELUActivation()
      (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)
    )
    (decoder): Linear(in_features=32, out_features=99, bias=True)
  )
  (mmm_text_head): FlavaMaskedPredictionHead(
    (transform): FlavaPredictionHeadTransform(
      (dense): Linear(in_features=32, out_features=32, bias=True)
      (transform_act_fn): GELUActivation()
      (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)
    )
    (decoder): Linear(in_features=32, out_features=102, bias=True)
  )
  (global_contrastive_head): FlavaGlobalContrastiveHead()
)
input_ids = tensor([[ 62,  29,  13,  33,  64,   9,   0],
        [ 93,  91,  49,  53,  52,  37,  28],
        [  3,  14,  17,  51,  84,  15,  95],
        [ 68,  83,  90,  51,  87,  92,  48],
        [ 66,  25,  23,  22,  67,  33,  99],
        [ 14,  48,  78,  67,   2,  76,  14],
        [ 78,   6,  32,  51,  90,   1,  73],
        [ 81,  37,  70,  48,  19,   8,  28],
        [ 27,  13,  70,  46,  45,   9,  43],
        [ 89,  91,  12,  23,  39,  78,  69],
        [ 15,  80, 100,  36,  39,  65,  78],
        [ 89,  16,  41,   5,  93,  74,  42]], device='cuda:0')
input_ids_masked = tensor([[ 62, 100, 100,  33,  64,   9,   0],
        [ 93, 100, 100,  53,  52,  37,  28],
        [  3, 100, 100,  51,  84,  15,  95],
        [ 68, 100, 100,  51,  87,  92,  48],
        [ 66, 100, 100,  22,  67,  33,  99],
        [ 14, 100, 100,  67,   2,  76,  14],
        [ 78, 100, 100,  51,  90,   1,  73],
        [ 81, 100, 100,  48,  19,   8,  28],
        [ 27, 100, 100,  46,  45,   9,  43],
        [ 89, 100, 100,  23,  39,  78,  69],
        [ 15, 100, 100,  36,  39,  65,  78],
        [ 89, 100, 100,   5,  93,  74,  42]], device='cuda:0')
pixel_values = tensor([[[[0.6292, 0.8716, 0.6683,  ..., 0.3767, 0.6904, 0.7981],
          [0.3908, 0.9846, 0.7483,  ..., 0.1696, 0.8879, 0.7089],
          [0.7249, 0.4461, 0.8334,  ..., 0.0508, 0.0179, 0.6384],
          ...,
          [0.1861, 0.7476, 0.7579,  ..., 0.9684, 0.3186, 0.7849],
          [0.3935, 0.5990, 0.0622,  ..., 0.1609, 0.7467, 0.0530],
          [0.9168, 0.2095, 0.7335,  ..., 0.8055, 0.2149, 0.6641]],

         [[0.7484, 0.0731, 0.6215,  ..., 0.6957, 0.2759, 0.9003],
          [0.2183, 0.7220, 0.1430,  ..., 0.8769, 0.2427, 0.8350],
          [0.2086, 0.7890, 0.6517,  ..., 0.1586, 0.2213, 0.6628],
          ...,
          [0.3217, 0.7107, 0.1626,  ..., 0.4554, 0.2442, 0.2111],
          [0.8533, 0.7032, 0.8450,  ..., 0.2984, 0.7128, 0.2493],
          [0.7110, 0.9347, 0.4971,  ..., 0.6250, 0.8522, 0.4984]],

         [[0.3851, 0.2204, 0.5361,  ..., 0.7143, 0.6962, 0.9324],
          [0.8066, 0.9100, 0.7011,  ..., 0.5469, 0.9310, 0.1949],
          [0.2574, 0.7340, 0.9674,  ..., 0.7432, 0.8618, 0.9953],
          ...,
          [0.5411, 0.3698, 0.9341,  ..., 0.1422, 0.8587, 0.2391],
          [0.7366, 0.9240, 0.5291,  ..., 0.5237, 0.1326, 0.1890],
          [0.0244, 0.5766, 0.8930,  ..., 0.2710, 0.8252, 0.6618]]],


        [[[0.1793, 0.5572, 0.8746,  ..., 0.6465, 0.4474, 0.9668],
          [0.2873, 0.9318, 0.5719,  ..., 0.5271, 0.9116, 0.1362],
          [0.1436, 0.7545, 0.1570,  ..., 0.5646, 0.5713, 0.4671],
          ...,
          [0.6436, 0.0269, 0.4038,  ..., 0.1637, 0.6720, 0.1134],
          [0.4384, 0.0364, 0.9222,  ..., 0.8222, 0.7309, 0.7260],
          [0.3022, 0.0068, 0.3564,  ..., 0.1945, 0.1066, 0.8987]],

         [[0.3653, 0.3397, 0.2521,  ..., 0.7212, 0.1215, 0.7432],
          [0.9139, 0.5647, 0.9769,  ..., 0.6015, 0.9137, 0.7931],
          [0.2564, 0.2413, 0.8623,  ..., 0.2654, 0.1010, 0.7177],
          ...,
          [0.4033, 0.5753, 0.1725,  ..., 0.2411, 0.1143, 0.6106],
          [0.0850, 0.3779, 0.8504,  ..., 0.0527, 0.9480, 0.0113],
          [0.8332, 0.4649, 0.6320,  ..., 0.3970, 0.7875, 0.1369]],

         [[0.2814, 0.2201, 0.7176,  ..., 0.2585, 0.6681, 0.4818],
          [0.2936, 0.6157, 0.1624,  ..., 0.1554, 0.0141, 0.7869],
          [0.0458, 0.5303, 0.5820,  ..., 0.1985, 0.8954, 0.0225],
          ...,
          [0.1355, 0.4102, 0.8886,  ..., 0.3601, 0.6013, 0.7448],
          [0.4709, 0.5097, 0.3078,  ..., 0.3869, 0.7628, 0.5096],
          [0.1854, 0.5670, 0.2710,  ..., 0.6932, 0.0449, 0.6732]]],


        [[[0.7653, 0.6628, 0.8601,  ..., 0.9314, 0.9845, 0.0416],
          [0.8790, 0.7814, 0.5364,  ..., 0.6851, 0.2497, 0.0453],
          [0.9827, 0.6827, 0.3502,  ..., 0.2150, 0.3424, 0.5498],
          ...,
          [0.1198, 0.1354, 0.9720,  ..., 0.1839, 0.8959, 0.7341],
          [0.4077, 0.7185, 0.2966,  ..., 0.2776, 0.7039, 0.2881],
          [0.9849, 0.4839, 0.1679,  ..., 0.2986, 0.3536, 0.3936]],

         [[0.9083, 0.9948, 0.0811,  ..., 0.2437, 0.9286, 0.0666],
          [0.4361, 0.4582, 0.5795,  ..., 0.3414, 0.9311, 0.8447],
          [0.3759, 0.4000, 0.6525,  ..., 0.9659, 0.4536, 0.0448],
          ...,
          [0.7409, 0.9720, 0.8362,  ..., 0.5176, 0.8124, 0.4665],
          [0.1749, 0.3469, 0.5199,  ..., 0.9859, 0.5292, 0.1520],
          [0.1366, 0.2604, 0.5352,  ..., 0.6141, 0.1176, 0.3503]],

         [[0.5760, 0.2803, 0.6079,  ..., 0.5799, 0.3994, 0.3483],
          [0.6684, 0.6101, 0.9960,  ..., 0.7446, 0.2633, 0.5865],
          [0.5663, 0.6466, 0.6713,  ..., 0.4675, 0.3487, 0.1293],
          ...,
          [0.0825, 0.2854, 0.5402,  ..., 0.3534, 0.4895, 0.7941],
          [0.7757, 0.3594, 0.6515,  ..., 0.7511, 0.8463, 0.4031],
          [0.3240, 0.7065, 0.1092,  ..., 0.1717, 0.5465, 0.8822]]],


        ...,


        [[[0.2889, 0.3245, 0.1381,  ..., 0.0864, 0.0293, 0.9145],
          [0.8320, 0.8577, 0.5652,  ..., 0.6462, 0.4274, 0.4543],
          [0.7471, 0.9809, 0.3220,  ..., 0.8652, 0.5233, 0.1267],
          ...,
          [0.5408, 0.6862, 0.9201,  ..., 0.1904, 0.5572, 0.1836],
          [0.9846, 0.6549, 0.4355,  ..., 0.8707, 0.0210, 0.8472],
          [0.2516, 0.4280, 0.5600,  ..., 0.7513, 0.5500, 0.7110]],

         [[0.4460, 0.9247, 0.0778,  ..., 0.3077, 0.7165, 0.7236],
          [0.9174, 0.5504, 0.9878,  ..., 0.8006, 0.8369, 0.8288],
          [0.2859, 0.9593, 0.8911,  ..., 0.7134, 0.2122, 0.8693],
          ...,
          [0.4192, 0.0108, 0.1958,  ..., 0.7337, 0.5669, 0.6294],
          [0.4748, 0.0685, 0.5897,  ..., 0.0987, 0.2686, 0.1826],
          [0.5069, 0.7769, 0.1596,  ..., 0.1050, 0.9029, 0.5413]],

         [[0.6685, 0.7940, 0.5925,  ..., 0.5054, 0.7059, 0.6449],
          [0.4936, 0.2318, 0.8742,  ..., 0.9634, 0.5140, 0.9133],
          [0.9838, 0.3310, 0.2104,  ..., 0.2558, 0.5598, 0.7884],
          ...,
          [0.5774, 0.8597, 0.0243,  ..., 0.4705, 0.7992, 0.3749],
          [0.4265, 0.9090, 0.0154,  ..., 0.2641, 0.2787, 0.3043],
          [0.5616, 0.3239, 0.4563,  ..., 0.0879, 0.4193, 0.6458]]],


        [[[0.3851, 0.0185, 0.1248,  ..., 0.7413, 0.7004, 0.5177],
          [0.6215, 0.7999, 0.3584,  ..., 0.3564, 0.7316, 0.9038],
          [0.9580, 0.3409, 0.6881,  ..., 0.4096, 0.9331, 0.2399],
          ...,
          [0.2306, 0.7796, 0.8632,  ..., 0.1324, 0.9065, 0.5160],
          [0.2124, 0.1860, 0.9373,  ..., 0.2102, 0.5173, 0.1642],
          [0.9408, 0.7783, 0.1629,  ..., 0.3868, 0.9248, 0.0364]],

         [[0.2019, 0.3986, 0.6394,  ..., 0.7571, 0.1054, 0.4351],
          [0.3601, 0.8004, 0.2177,  ..., 0.6511, 0.2513, 0.9321],
          [0.1546, 0.3719, 0.7555,  ..., 0.6899, 0.2393, 0.1868],
          ...,
          [0.6866, 0.0842, 0.1073,  ..., 0.8684, 0.8032, 0.7562],
          [0.4223, 0.7039, 0.2904,  ..., 0.8925, 0.8264, 0.3033],
          [0.3602, 0.6352, 0.6401,  ..., 0.8118, 0.0277, 0.7902]],

         [[0.2275, 0.3771, 0.2287,  ..., 0.2566, 0.7681, 0.8859],
          [0.8058, 0.8906, 0.8179,  ..., 0.9823, 0.0723, 0.2575],
          [0.7837, 0.6451, 0.0330,  ..., 0.6210, 0.7828, 0.3637],
          ...,
          [0.6396, 0.8428, 0.5167,  ..., 0.2638, 0.3469, 0.7458],
          [0.6445, 0.4670, 0.2735,  ..., 0.7548, 0.2613, 0.1529],
          [0.0719, 0.6972, 0.0974,  ..., 0.0990, 0.0617, 0.0496]]],


        [[[0.8052, 0.6915, 0.1883,  ..., 0.0292, 0.3569, 0.0484],
          [0.9930, 0.9263, 0.0351,  ..., 0.5428, 0.5920, 0.2357],
          [0.0715, 0.9819, 0.5387,  ..., 0.9806, 0.4581, 0.4973],
          ...,
          [0.4123, 0.9819, 0.9546,  ..., 0.7865, 0.7078, 0.7661],
          [0.9213, 0.5833, 0.7424,  ..., 0.4575, 0.8749, 0.2374],
          [0.4117, 0.1440, 0.1892,  ..., 0.6566, 0.1281, 0.2781]],

         [[0.4165, 0.3969, 0.4776,  ..., 0.5733, 0.3641, 0.5439],
          [0.8416, 0.8764, 0.5731,  ..., 0.3636, 0.1792, 0.5939],
          [0.2859, 0.2760, 0.2278,  ..., 0.5652, 0.0057, 0.9530],
          ...,
          [0.7265, 0.6232, 0.0918,  ..., 0.1352, 0.4900, 0.2076],
          [0.9489, 0.8312, 0.5680,  ..., 0.3970, 0.8690, 0.2395],
          [0.2553, 0.0466, 0.9826,  ..., 0.4020, 0.6619, 0.9937]],

         [[0.8875, 0.6890, 0.8179,  ..., 0.3612, 0.5154, 0.5877],
          [0.3736, 0.2616, 0.3532,  ..., 0.1041, 0.3671, 0.6640],
          [0.4967, 0.9197, 0.4010,  ..., 0.5007, 0.1943, 0.9064],
          ...,
          [0.2485, 0.0769, 0.0472,  ..., 0.2039, 0.9968, 0.2074],
          [0.8637, 0.9409, 0.7381,  ..., 0.8127, 0.3680, 0.8386],
          [0.1818, 0.3793, 0.1078,  ..., 0.5646, 0.8222, 0.5226]]]],
       device='cuda:0')
codebook_pixel_values = None
attention_mask = tensor([[1, 1, 1, 1, 0, 0, 0],
        [1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 0, 0],
        [1, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 0, 0],
        [1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 0, 0, 0, 0]], device='cuda:0')
token_type_ids = tensor([[0, 0, 0, 1, 0, 0, 0],
        [0, 1, 0, 1, 0, 0, 0],
        [0, 0, 0, 1, 1, 0, 1],
        [1, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 1, 1, 1],
        [1, 0, 1, 1, 0, 0, 1],
        [0, 0, 1, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 0, 0],
        [0, 0, 1, 0, 1, 0, 1],
        [1, 1, 1, 1, 0, 0, 0],
        [0, 1, 1, 1, 0, 1, 0],
        [0, 0, 0, 0, 0, 1, 0]], device='cuda:0')
bool_masked_pos = tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 0,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 0, 1,  ..., 0, 1, 1],
        [1, 1, 1,  ..., 1, 1, 0],
        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')
position_ids = None, image_attention_mask = None
skip_unmasked_multimodal_encoder = True
mlm_labels = tensor([[-100,   29,   13, -100, -100, -100, -100],
        [-100,   91,   49, -100, -100, -100, -100],
        [-100,   14,   17, -100, -100, -100, -100],
        [-100,   83,   90, -100, -100, -100, -100],
        [-100,   25,   23, -100, -100, -100, -100],
        [-100,   48,   78, -100, -100, -100, -100],
        [-100,    6,   32, -100, -100, -100, -100],
        [-100,   37,   70, -100, -100, -100, -100],
        [-100,   13,   70, -100, -100, -100, -100],
        [-100,   91,   12, -100, -100, -100, -100],
        [-100,   80,  100, -100, -100, -100, -100],
        [-100,   16,   41, -100, -100, -100, -100]], device='cuda:0')
mim_labels = tensor([[  65,   95,   77,  ...,   41,   34,   73],
        [  28,   70, -100,  ...,   64,   61,   11],
        [  51,    0,   65,  ...,   67,   59,   89],
        ...,
        [  70, -100,   67,  ..., -100,    6,   48],
        [  53,   30,   11,  ...,   71,   26, -100],
        [  39,   63,   19,  ...,   15,   50,   57]], device='cuda:0')
itm_labels = tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')
output_attentions = None, output_hidden_states = True, return_dict = True
return_loss = True

    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        input_ids_masked: Optional[torch.LongTensor] = None,
        pixel_values: Optional[torch.FloatTensor] = None,
        codebook_pixel_values: Optional[torch.FloatTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        token_type_ids: Optional[torch.Tensor] = None,
        bool_masked_pos: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        image_attention_mask: Optional[torch.Tensor] = None,
        skip_unmasked_multimodal_encoder: Optional[bool] = None,
        mlm_labels: Optional[torch.Tensor] = None,
        mim_labels: Optional[torch.Tensor] = None,
        itm_labels: Optional[torch.Tensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: bool = True,
        return_dict: Optional[bool] = None,
        return_loss: Optional[bool] = None,
    ) -> Union[tuple[torch.Tensor], FlavaForPreTrainingOutput]:
        r"""
        input_ids (`torch.LongTensor` of shape `(batch_size, text_seq_len)`):
            Indices of input sequence tokens in the vocabulary. Indices can be obtained using [`AutoTokenizer`]. See
            [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details. [What are input
            IDs?](../glossary#input-ids)
        input_ids_masked (`torch.LongTensor` of shape `(batch_size, text_seq_len)`):
            Indices of input sequence tokens in the vocabulary. These ones are the masked version of the original task
            to be used with MLM. Indices can be obtained using [`AutoTokenizer`] along with
            [`DataCollatorForMaskedLanguageModeling`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details. [What are input IDs?](../glossary#input-ids)
        codebook_pixel_values (`torch.FloatTensor` of shape `(batch_size, num_image_patches, patch_size, patch_size, 3)`, *optional*):
            Pixel values for image patches that are used to compute the image codebook labels for masked image modeling.
        token_type_ids (`torch.LongTensor` of shape `(batch_size, text_seq_len)`, *optional*):
            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,
            1]`:
            - 0 corresponds to a *sentence A* token,
            - 1 corresponds to a *sentence B* token.
            [What are token type IDs?](../glossary#token-type-ids)
        bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, image_num_patches)`):
            Boolean masked positions. Indicates which patches are masked (1) and which aren't (0).
        image_attention_mask (`torch.FloatTensor` of shape `(batch_size, image_num_patches)`, *optional*):
            Mask to avoid performing attention on padding token indices specifically for images. Mask values selected
            in `[0, 1]`:
            - 1 for tokens that are **not masked**,
            - 0 for tokens that are **masked**.
            [What are attention masks?](../glossary#attention-mask)
        skip_unmasked_multimodal_encoder (*bool*, *optional*):
            Skip any calculations for multimodal encoder for unmasked inputs. FLAVA pretraining doesn't need unmasked
            multimodal embeddings or outputs as of now.
        mlm_labels (`torch.LongTensor` of shape `(batch_size, text_seq_len)`, *optional*):
            Labels for computing the left-to-right language and multimodal masked modeling loss (next word prediction).
            Indices should be in `[-100, 0, ..., text_config.vocab_size - 1]` (see `input_ids` docstring). Tokens with
            indices set to `-100` are ignored (masked), the loss is only computed for the tokens with labels in `[0,
            ..., text_config.vocab_size - 1]`.
        mim_labels (`torch.LongTensor` of shape `(batch_size, image_num_patches)`, *optional*):
            Labels for computing the image and multimodal masked modeling loss. Indices should be in `[-100, 0, ...,
            image_config.vocab_size - 1]`. Tokens with indices set to `-100` are ignored (masked), the loss is only
            computed for the tokens with labels in `[0, ..., image_config.vocab_size - 1]`. If not passed, they are
            generated automatically using the image codebook assigned to the model. By default, it uses
            [`FlavaImageCodebook`]. See [`FlavaImageCodebook`] to understand how to generate mim_labels.
        itm_labels (`torch.LongTensor` of shape `(batch_size, 1)`, *optional*):
            Labels for computing the image-text matching loss. 0 means the pairs don't match and 1 means they match.
            The pairs with 0 will be skipped for calculation of MMM and global contrastive losses as well.
        return_loss (`bool`, *optional*, default to None):
            Whether to return calculated loss or not.
    
        Examples:
        ```python
        >>> from PIL import Image
        >>> import requests
        >>> from transformers import FlavaForPreTraining, AutoProcessor
    
        >>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
        >>> image = Image.open(requests.get(url, stream=True).raw)
    
        >>> model = FlavaForPreTraining.from_pretrained("facebook/flava-full")
        >>> processor = AutoProcessor.from_pretrained("facebook/flava-full")
    
        >>> text = ["a photo of a cat"]
    
        >>> inputs = processor(
        ...     images=[image],
        ...     text=text,
        ...     return_masks=True,
        ...     return_codebook_pixels=True,
        ...     padding=True,
        ...     max_length=77,
        ...     return_tensors="pt",
        ... )
    
    
        >>> output = model(**inputs)
        ```
        """
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        return_loss = return_loss if return_loss is not None else self.config.return_loss
    
        skip_unmasked_multimodal_encoder = (
            skip_unmasked_multimodal_encoder
            if skip_unmasked_multimodal_encoder is not None
            else self.skip_unmasked_multimodal_encoder
        )
    
        if input_ids_masked is None and input_ids is not None:
            logger.warning(
                "`input_ids_masked` isn't passed which means MLM loss won't be calculated correctlySetting it to"
                " `input_ids` so that model can work. Please pass it if this is unintentional. This is usually OKAY if"
                " you are doing inference on unmasked text..."
            )
            input_ids_masked = input_ids
    
        flava_output = self.flava(
            input_ids=input_ids,
            pixel_values=pixel_values,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            image_attention_mask=image_attention_mask,
            # Don't need unmasked multimodal embedding for anything so skip it
            # NOTE: ITM uses masked version
            skip_multimodal_encoder=skip_unmasked_multimodal_encoder,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            # Pass true to have deterministic outputs
            return_dict=True,
        )
    
        flava_masked_output = self.flava(
            input_ids=input_ids_masked,
            pixel_values=pixel_values,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            image_attention_mask=image_attention_mask,
            bool_masked_pos=bool_masked_pos,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=True,
        )
    
        pos_mask = None
    
        image_embeddings = flava_output.image_embeddings
        text_embeddings = flava_output.text_embeddings
        image_masked_embeddings = flava_masked_output.image_embeddings
        text_masked_embeddings = flava_masked_output.text_embeddings
        multimodal_masked_embeddings = flava_masked_output.multimodal_embeddings
    
        total_loss = mim_loss = mlm_loss = mmm_text_loss = mmm_image_loss = gc_loss = itm_loss = None
        mim_logits = mlm_logits = mmm_text_logits = mmm_image_logits = None
        itm_logits = logits_per_image = logits_per_text = None
    
        # Calculate mim_labels if necessary from the image_codebook
        if image_masked_embeddings is not None or multimodal_masked_embeddings is not None:
            if mim_labels is None and return_loss:
                if self.image_codebook is None:
                    raise RuntimeError(
                        "`return_loss` is set to True but the image codebook is not initialized and no `mim_labels` "
                        " have been passed. Reinstantiate the model with `init_codebook` set to True or "
                        "pass in your custom `mim_labels`"
                    )
                if codebook_pixel_values is None:
                    raise ValueError(
                        "`codebook_pixel_value` are required to generate `mim_labels` if loss is expected. "
                        "Call `AutoProcessor` with `return_codebook_pixels` set to True"
                    )
                mim_labels = self.image_codebook.get_codebook_indices(codebook_pixel_values)
        # Unimodal MIM Loss
        # If multimodal embeddings are present, we will calculate MMM loss
        if self.mim_weight > 0 and image_masked_embeddings is not None and multimodal_masked_embeddings is None:
            sequence_for_image = image_masked_embeddings
    
            if mim_labels is not None:
                mim_labels = self._resize_to_2d(mim_labels)
                bool_masked_pos = self._resize_to_2d(bool_masked_pos)
                mim_labels[bool_masked_pos.ne(True)] = self.ce_ignore_index
    
                sequence_for_image = sequence_for_image[:, -mim_labels.size(1) :, :]
                masked_tokens = mim_labels.ne(self.ce_ignore_index)
                mim_labels_filtered = mim_labels[masked_tokens]
                sequence_for_image = sequence_for_image[masked_tokens, :]
                mim_logits = self.mim_head(sequence_for_image)
                if return_loss:
                    mim_loss = nn.functional.cross_entropy(
                        mim_logits.view(-1, self.image_vocab_size), mim_labels_filtered.view(-1)
                    )
                    mim_loss *= self.mim_weight
            else:
                mim_logits = self.mim_head(sequence_for_image)
    
        # Unimodal MLM Loss
        if self.mlm_weight > 0 and text_masked_embeddings is not None and multimodal_masked_embeddings is None:
            sequence_for_text = text_masked_embeddings
            if mlm_labels is not None:
                mlm_labels = self._resize_to_2d(mlm_labels)
                sequence_for_text = sequence_for_text[:, -mlm_labels.size(1) :, :]
                masked_tokens = mlm_labels.ne(self.ce_ignore_index)
                mlm_labels_filtered = mlm_labels[masked_tokens]
                sequence_for_text = sequence_for_text[masked_tokens, :]
                mlm_logits = self.mlm_head(sequence_for_text)
                if return_loss:
                    mlm_loss = nn.functional.cross_entropy(
                        mlm_logits.view(-1, self.text_vocab_size), mlm_labels_filtered.view(-1)
                    )
                    mlm_loss *= self.mlm_weight
            else:
                mlm_logits = self.mlm_head(sequence_for_text)
    
        # ITM Loss
        if self.itm_weight > 0 and multimodal_masked_embeddings is not None:
            itm_logits = self.itm_head(multimodal_masked_embeddings)
    
            if itm_labels is not None:
                pos_pairs = itm_labels.ne(0)
                pos_mask = torch.where(pos_pairs.any(), pos_pairs, pos_pairs.new([True]))
                if return_loss:
                    itm_loss = nn.functional.cross_entropy(itm_logits, itm_labels)
                    itm_loss *= self.itm_weight
    
                if multimodal_masked_embeddings is not None:
                    multimodal_masked_embeddings = multimodal_masked_embeddings[pos_mask]
    
                if mlm_labels is not None:
                    mlm_labels = mlm_labels[pos_mask]
    
                if mim_labels is not None:
                    mim_labels = mim_labels[pos_mask]
                    bool_masked_pos = bool_masked_pos[pos_mask]
    
        # MMM Image Loss
        if multimodal_masked_embeddings is not None and self.mmm_image_weight > 0:
            sequence_for_image = multimodal_masked_embeddings
            end_index = image_masked_embeddings.size(1) - 1
            sequence_for_image = sequence_for_image[:, 2 : 2 + end_index, :]
    
            if mim_labels is not None:
                mim_labels = self._resize_to_2d(mim_labels)
                bool_masked_pos = self._resize_to_2d(bool_masked_pos)
                mim_labels[bool_masked_pos.ne(True)] = self.ce_ignore_index
    
                masked_tokens = mim_labels.ne(self.ce_ignore_index)
                mim_labels_filtered = mim_labels[masked_tokens]
                sequence_for_image = sequence_for_image[masked_tokens, :]
                mmm_image_logits = self.mmm_image_head(sequence_for_image)
                if return_loss:
                    torch._check(mmm_image_logits.size(0) > 0)
                    mmm_image_loss = nn.functional.cross_entropy(
                        mmm_image_logits.view(-1, self.image_vocab_size), mim_labels_filtered.view(-1)
                    )
                    mmm_image_loss *= self.mmm_image_weight
            else:
                mmm_image_logits = self.mmm_image_head(sequence_for_image)
    
        # MMM Text Loss
        if multimodal_masked_embeddings is not None and self.mmm_text_weight > 0:
            sequence_for_text = multimodal_masked_embeddings
            sequence_for_text = sequence_for_text[:, -text_masked_embeddings.size(1) :, :]
    
            if mlm_labels is not None:
                mlm_labels = self._resize_to_2d(mlm_labels)
                masked_tokens = mlm_labels.ne(self.ce_ignore_index)
                mlm_labels_filtered = mlm_labels[masked_tokens]
                sequence_for_text = sequence_for_text[masked_tokens, :]
                mmm_text_logits = self.mmm_text_head(sequence_for_text)
                if return_loss:
>                   torch._check(mmm_text_logits.ize(0) > 0)
E                   AttributeError: 'Tensor' object has no attribute 'ize'

src/transformers/models/flava/modeling_flava.py:1837: AttributeError
__________ MllamaForConditionalGenerationModelTest.test_torch_export ___________
[gw51] linux -- Python 3.10.12 /home/ilyas/transformers/.docker/bin/python3

self = <tests.models.mllama.test_modeling_mllama.MllamaForConditionalGenerationModelTest testMethod=test_torch_export>
atol = 0.0001, rtol = 0.0001

    @slow
    @require_torch_greater_or_equal("2.5")
    @pytest.mark.torch_export_test
    def test_torch_export(self, atol=1e-4, rtol=1e-4):
        """
        Test if model can be exported with torch.export.export()
    
        Args:
            atol (`float`, *optional*, defaults to 1e-4): absolute tolerance for output comparison
            rtol (`float`, *optional*, defaults to 1e-4): relative tolerance for output comparison
        """
    
        exporter = DynamoExporter(export_config=DynamoConfig())
    
        for model_class in self.all_model_classes:
            with self.subTest(model_class.__name__):
                if hasattr(self.model_tester, "prepare_config_and_inputs_for_model_class"):
                    config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_model_class(model_class)
                else:
                    config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()
                inputs_dict = self._prepare_for_class(inputs_dict, model_class)
                model = model_class(config).eval().to(torch_device)
    
                # prepare cache inputs for auto-regressive models and include it for computing eager outputs
                # process output flags (e.g. use_cache, output_attentions, etc) to avoid passing them as inputs
                model, inputs_dict = prepare_for_export(model, inputs_dict)
    
                with torch.no_grad():
                    # Running the eager inference before the export to catch model/inputs comatibility issues, also sometimes after
                    # the export, the model used for export will return FakeTensors instead of real ones (torch cuda/inductor issue)
                    # This happens on cuda with (codegen, clvp, esm, gptj, levit, wav2vec2_bert and wav2vec2_conformer)
                    set_seed(1234)
>                   eager_outputs = model(**copy.deepcopy(inputs_dict))

tests/test_modeling_common.py:3527: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.docker/lib/python3.10/site-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
src/transformers/utils/generic.py:928: in wrapper
    outputs = func(self, *args, **kwargs)
src/transformers/utils/generic.py:758: in wrapper
    output = func(self, *args, **kwargs)
src/transformers/models/mllama/modeling_mllama.py:1552: in forward
    outputs = self.language_model(
.docker/lib/python3.10/site-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
src/transformers/utils/generic.py:928: in wrapper
    outputs = func(self, *args, **kwargs)
src/transformers/utils/generic.py:758: in wrapper
    output = func(self, *args, **kwargs)
src/transformers/models/mllama/modeling_mllama.py:1298: in forward
    hidden_states = decoder_layer(
src/transformers/modeling_layers.py:94: in __call__
    return super().__call__(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
src/transformers/models/mllama/modeling_mllama.py:707: in forward
    hidden_states, attn_weights = self.cross_attn(
.docker/lib/python3.10/site-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
src/transformers/models/mllama/modeling_mllama.py:459: in forward
    attn_output, attn_weights = attention_interface(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

module = MllamaTextCrossAttention(
  (q_proj): Linear(in_features=32, out_features=32, bias=False)
  (k_proj): Linear(in_features=32, out_features=32, bias=False)
  (v_proj): Linear(in_features=32, out_features=32, bias=False)
  (o_proj): Linear(in_features=32, out_features=32, bias=False)
  (q_norm): MllamaTextRMSNorm((8,), eps=1e-05)
  (k_norm): MllamaTextRMSNorm((8,), eps=1e-05)
)
query = tensor([[[[ 5.1528e-01,  4.1364e-01, -1.1495e+00, -1.3661e+00, -1.1130e+00,
           -1.1983e+00,  1.0242e+00, -8.0110e-01],
          [ 1.0683e+00, -1.5140e-01, -1.2777e+00, -3.3733e-01, -1.6570e+00,
           -1.7607e-01, -3.5511e-01, -1.4755e+00],
          [ 6.8755e-01,  4.9130e-02,  1.5543e+00, -1.6147e+00, -8.4545e-01,
            1.1772e+00,  3.7280e-01,  4.9049e-01],
          [ 6.3425e-01, -2.1324e+00,  1.0114e+00, -2.3017e-01,  9.6028e-01,
            8.0278e-01,  1.9056e-01,  6.0636e-01],
          [-1.0328e+00, -5.7516e-01, -3.5780e-01,  7.4725e-02,  7.6216e-01,
           -8.6798e-01,  1.0332e+00,  2.0116e+00],
          [ 7.2822e-01,  1.6985e-02, -1.3843e-01, -9.4619e-01, -9.9088e-01,
            1.9008e+00, -1.3741e+00,  2.5873e-01],
          [ 2.5233e-02, -5.7433e-01,  1.8920e+00,  1.7391e+00,  1.5746e-01,
           -7.1333e-02,  3.2196e-03,  1.0148e+00]],

         [[-1.1988e+00,  6.7294e-01,  2.7668e-01,  9.8765e-01,  1.1450e+00,
           -4.2055e-01,  1.3768e+00, -1.2902e+00],
          [-3.7647e-01,  8.9273e-01, -9.5369e-02,  7.4733e-01,  1.6907e+00,
           -1.7562e+00,  7.3572e-01,  6.0744e-02],
          [ 1.3205e+00,  1.4974e+00, -2.7785e-01, -3.8103e-01, -9.5861e-01,
            4.0984e-02,  4.3515e-01, -1.6365e+00],
          [-7.0936e-01, -5.4133e-01,  4.1061e-01, -6.7480e-01,  8.5171e-02,
            5.7548e-01, -1.9839e-01, -2.4895e+00],
          [-1.0399e+00, -9.9321e-01,  1.6723e+00, -9.4216e-01,  2.1143e-01,
            1.2217e+00, -1.9948e-01, -8.1365e-01],
          [-1.7993e+00,  1.0726e+00,  1.9903e-02,  2.5820e-01,  9.7180e-02,
           -3.1520e-01, -1.5748e+00, -9.7391e-01],
          [ 1.0145e+00, -9.3129e-01, -2.0233e-01,  1.0574e-01, -4.7589e-01,
           -8.0179e-01, -1.4859e+00, -1.7210e+00]],

         [[ 2.2658e+00, -1.4369e+00, -1.0525e-01,  6.8155e-02, -3.2828e-01,
           -2.6593e-01, -6.9427e-01,  3.3463e-01],
          [ 5.9216e-01, -9.4121e-01,  4.5076e-01,  3.4519e-01, -8.1939e-01,
            8.8688e-03, -1.7763e+00,  1.6157e+00],
          [ 1.0061e+00, -5.9236e-01,  6.9660e-01,  1.1648e+00, -2.0288e-01,
            4.3011e-01, -2.1013e+00,  3.8292e-01],
          [ 8.4811e-01, -5.0617e-01,  7.7560e-01, -1.8682e+00,  1.0368e+00,
           -4.1385e-01,  9.9078e-01, -8.3246e-01],
          [-6.8651e-01, -1.9833e+00, -7.9099e-01, -8.6999e-01,  3.6077e-01,
           -6.1646e-01, -1.2828e+00, -2.1296e-01],
          [-5.6852e-01, -3.1563e-01,  2.1719e-01, -7.8713e-01,  1.6842e-01,
           -7.1995e-01,  2.4282e+00, -6.8149e-01],
          [ 1.8392e-01,  4.8978e-01,  3.2347e-01, -2.4201e+00, -3.1955e-01,
            3.4135e-01, -8.8298e-02, -1.2267e+00]],

         [[ 5.1377e-01, -3.5357e-01, -1.8824e+00,  4.2607e-01,  8.0684e-01,
            1.3736e+00,  1.0054e+00, -5.7576e-01],
          [-1.8755e+00,  1.0112e+00,  1.0512e+00,  5.3203e-01, -2.1801e-01,
            1.1449e+00, -2.2656e-01, -8.0947e-01],
          [ 1.0526e+00,  9.9013e-01,  7.6281e-01,  7.5718e-01, -1.0770e+00,
            1.1546e+00,  1.5014e+00,  5.8850e-02],
          [ 4.2791e-01,  5.8298e-02, -7.1008e-01,  1.0751e-01,  2.1453e+00,
           -1.2170e+00,  4.9066e-01, -9.8152e-01],
          [-5.7004e-01,  6.6447e-02, -2.9815e-01, -1.4705e-01, -1.4289e+00,
            2.2174e+00,  1.2643e-01,  7.6071e-01],
          [ 2.0714e-01, -1.8037e+00, -1.4169e+00, -8.2404e-01,  8.3428e-01,
           -8.1425e-02,  1.0587e+00,  4.2927e-01],
          [ 4.2266e-01,  9.4764e-01,  3.5697e-01, -1.4422e+00,  1.3409e+00,
           -1.3107e+00, -2.3498e-01, -1.0671e+00]]],


        [[[-1.3629e+00, -1.7967e+00,  3.6923e-01,  1.8467e-01,  6.1364e-01,
            1.2788e+00,  8.1744e-01,  2.4885e-01],
          [ 6.8008e-01,  4.7064e-02, -1.5429e+00,  1.5006e-03, -1.2377e+00,
           -1.9507e-01, -8.1837e-01, -1.7039e+00],
          [-1.3570e+00, -1.6921e-01, -3.8869e-01,  4.2638e-01,  1.2608e+00,
           -9.2209e-01,  6.0190e-01,  1.7258e+00],
          [-1.8131e+00, -1.4420e+00,  9.5197e-01,  3.5373e-01, -3.1879e-01,
            5.9319e-01, -9.0422e-01,  5.7098e-01],
          [-9.1647e-01, -1.3243e+00, -1.1739e+00,  1.8458e+00, -3.5117e-01,
            4.2209e-01, -4.5676e-01, -3.2865e-01],
          [ 1.3054e+00, -9.5891e-01,  1.5954e+00, -7.7679e-01,  1.0213e+00,
            5.8180e-01, -4.5613e-01,  7.9563e-01],
          [ 1.1713e+00, -1.4418e+00, -2.6380e-01,  4.4911e-01,  1.6407e+00,
            7.8627e-01,  6.9687e-01,  6.9060e-01]],

         [[-1.1279e-02, -1.6828e+00,  4.0172e-01, -7.5237e-01,  1.0927e+00,
            1.7314e+00,  4.1629e-01, -2.6739e-01],
          [-3.9462e-01,  6.8026e-01, -2.0970e-01,  3.2719e-01,  2.0898e+00,
           -1.4138e+00,  6.9328e-01,  6.1519e-01],
          [-9.4698e-01, -1.1042e+00,  1.5255e+00, -1.1919e+00,  5.8733e-01,
            1.2953e+00, -2.1410e-01, -2.4376e-01],
          [ 1.8421e-01, -1.1084e+00, -3.4248e-01,  1.5504e+00,  1.2495e+00,
            5.5992e-01,  1.4676e+00,  4.2873e-01],
          [-7.9651e-01, -5.1453e-01,  1.2350e+00, -1.1558e+00,  1.2613e+00,
           -6.6705e-01,  7.1082e-01, -1.3009e+00],
          [-1.7529e+00,  4.3630e-01,  9.3783e-01, -4.9259e-01,  1.1769e+00,
            4.9975e-01, -1.2141e+00,  6.9450e-01],
          [ 1.7051e+00,  1.7418e-02,  4.7330e-01, -3.1611e-01, -8.5656e-01,
           -4.5289e-01, -3.9410e-01, -1.9161e+00]],

         [[-3.7707e-01,  4.0779e-01,  9.2985e-01, -1.0617e+00,  1.5908e+00,
            3.0340e-01,  5.6752e-01, -1.6564e+00],
          [ 3.5642e-01, -9.3010e-01,  3.5693e-01,  4.6027e-01, -1.0946e+00,
            3.9852e-01, -1.7804e+00,  1.4620e+00],
          [-1.0262e+00, -1.8524e+00, -9.3568e-01, -8.9130e-01,  1.4446e-01,
           -2.3760e-01, -1.2047e+00, -5.5128e-01],
          [-1.9679e-02,  1.4590e-01,  4.5930e-01, -1.1837e+00, -8.0316e-01,
            1.1965e+00,  3.1290e-01, -2.0459e+00],
          [-1.0347e-01, -9.7868e-01,  1.3468e+00,  1.0856e+00, -1.4424e+00,
            8.1563e-02,  1.0763e+00,  8.8716e-01],
          [-3.3854e-01, -8.0799e-01,  4.8507e-01,  7.9533e-01,  1.5685e+00,
            1.6777e+00,  1.0095e+00, -2.4629e-01],
          [ 6.9901e-01,  8.3129e-01,  9.4028e-02,  1.5443e+00, -1.3034e+00,
           -7.6072e-01,  4.1906e-01,  1.4037e+00]],

         [[ 1.4163e+00, -5.6206e-01, -1.5072e+00,  1.6852e+00,  5.7755e-01,
            1.2999e-01, -4.3347e-01,  1.2064e-01],
          [-1.8788e+00,  4.3328e-01,  1.4327e+00,  6.4669e-01, -3.3048e-01,
            1.1685e+00, -2.7775e-01, -5.0284e-01],
          [-4.6636e-01, -3.8259e-01,  1.7921e-02, -2.4807e-03, -1.4934e+00,
            2.1096e+00,  8.0023e-02,  9.7078e-01],
          [-8.4713e-01, -1.4540e+00, -7.1332e-01, -4.7654e-02,  7.3644e-01,
           -1.9225e+00,  4.9590e-01, -4.0956e-01],
          [ 3.0744e-01, -1.7352e+00,  6.3310e-01,  1.2626e+00,  1.5557e+00,
            5.1547e-02, -2.9193e-01,  6.1176e-01],
          [ 4.6421e-01, -3.9441e-01,  3.3662e-01,  7.5904e-01,  9.9450e-01,
           -1.5807e-01,  1.7508e+00,  1.6873e+00],
          [ 9.9426e-01,  1.5184e+00,  1.7648e+00,  9.9891e-02,  5.5998e-01,
            3.5295e-01, -9.2265e-01,  5.3735e-01]]],


        [[[-1.3193e+00, -1.7186e-01, -2.7109e-01, -4.0615e-01,  1.1120e+00,
            8.5377e-02,  2.8469e-01, -2.1590e+00],
          [ 1.1607e+00,  5.7211e-01, -1.3770e+00, -2.0461e-01, -1.2757e+00,
           -5.9454e-01, -8.8693e-01, -1.2695e+00],
          [-1.3601e+00, -1.6125e-01, -2.9179e-01, -3.4613e-01,  1.0697e+00,
            3.6546e-02,  2.2967e-01, -2.1714e+00],
          [-5.7211e-01, -6.3321e-01, -7.9363e-01,  1.5299e+00, -1.2329e+00,
            1.5879e+00, -4.8077e-01,  1.4965e-01],
          [ 2.3891e+00, -1.5638e-01,  3.8518e-02,  3.9028e-01, -3.9134e-01,
           -8.0199e-01, -1.2205e-01,  1.1355e+00],
          [ 7.5135e-01, -1.6904e+00, -4.2254e-01,  3.5560e-01, -1.1681e+00,
            1.2247e+00, -3.4778e-01,  1.1325e+00],
          [ 6.8801e-01,  1.1684e+00, -1.2324e+00, -1.1052e+00, -7.4176e-01,
           -1.5258e+00,  3.4946e-01, -6.4261e-01]],

         [[ 5.1372e-01,  2.1414e-01, -7.5392e-02,  1.1400e+00, -8.4658e-01,
           -2.0336e+00,  8.7076e-01,  8.7616e-01],
          [-2.5589e-01,  8.6685e-01, -3.6754e-01,  4.7052e-01,  1.6221e+00,
           -1.8708e+00,  7.3669e-01,  3.8263e-01],
          [ 3.8730e-01,  2.8496e-01, -5.9018e-02,  1.1496e+00, -8.3442e-01,
           -2.0429e+00,  8.3881e-01,  9.2909e-01],
          [-2.3634e-01, -5.1089e-01,  8.3524e-01, -1.8612e+00, -1.0230e-01,
           -1.6162e+00,  4.0624e-01, -8.5144e-01],
          [ 3.6544e-01, -1.5424e+00,  1.3770e+00, -3.8396e-01, -1.9407e-01,
            1.3920e+00,  7.4088e-01,  9.5237e-01],
          [-3.0799e-01, -1.5682e+00, -4.7858e-01,  3.7290e-01,  3.1842e-01,
            9.3018e-01, -1.0949e+00,  1.7030e+00],
          [-1.0855e+00,  7.9378e-01, -3.8412e-02,  7.8792e-01,  1.2969e+00,
           -6.5812e-01,  1.6038e+00, -9.3232e-01]],

         [[ 1.1408e+00,  6.7321e-01,  4.4041e-01, -1.4243e+00,  4.0683e-01,
           -5.3708e-01, -1.8395e+00,  4.2269e-01],
          [ 3.4697e-01, -7.3299e-01,  4.7491e-01,  1.0750e-01, -8.6376e-01,
            2.7370e-01, -1.9730e+00,  1.5453e+00],
          [ 1.1737e+00,  6.7815e-01,  4.8407e-01, -1.5389e+00,  3.4222e-01,
           -5.1574e-01, -1.7462e+00,  3.4760e-01],
          [-7.5451e-01,  4.0456e-01,  6.4640e-01, -2.1006e+00, -1.3373e+00,
            7.4201e-01,  2.9922e-01, -4.5277e-02],
          [-1.2639e+00, -6.4345e-01, -7.3072e-01, -2.0666e+00, -3.5123e-01,
           -4.5704e-01, -2.5757e-01, -8.8338e-01],
          [-1.2695e+00, -3.9510e-01,  1.2335e-01, -1.4842e-01,  1.8596e+00,
            6.4738e-02,  1.5894e+00,  4.4978e-01],
          [ 2.0342e+00, -1.2546e+00,  5.1801e-02, -2.1213e-01, -5.6935e-01,
            2.3785e-01, -1.2892e+00,  4.2620e-01]],

         [[-4.3946e-01,  1.2952e+00,  2.0045e+00,  1.0772e+00, -4.4969e-01,
            5.0516e-01,  6.3035e-01,  3.0252e-01],
          [-1.6534e+00,  3.5563e-01,  1.5735e+00,  7.6287e-01, -6.0983e-01,
            1.1304e+00, -2.3641e-01, -6.0749e-01],
          [-4.0803e-01,  1.2406e+00,  1.9972e+00,  1.1307e+00, -4.0067e-01,
            4.8033e-01,  7.0543e-01,  3.6483e-01],
          [-8.7556e-01, -5.4097e-01,  5.0880e-01, -3.9636e-01,  1.3123e+00,
           -1.4882e+00, -4.3260e-01, -1.5469e+00],
          [-8.0154e-01, -1.9082e+00, -4.9711e-01,  3.4340e-01,  4.7683e-01,
           -5.2535e-01, -1.2491e+00,  1.1303e+00],
          [ 3.8290e-01, -1.0095e+00, -1.9013e+00, -4.1549e-01,  1.5838e+00,
           -2.6059e-01, -9.4424e-02, -6.7300e-01],
          [ 7.9805e-01, -1.0276e+00, -1.4490e+00,  6.6489e-01,  4.1832e-01,
            1.5539e+00,  1.0352e+00, -3.1293e-01]]]], device='cuda:0')
key = tensor([[[[-1.4019e+00,  1.2453e-01, -6.4971e-01,  ..., -6.3182e-01,
           -1.2288e+00, -3.2816e-01],
          [ 1.2996e-01, -3.7470e-01, -5.2884e-01,  ..., -9.5707e-01,
            4.4668e-01,  1.2447e+00],
          [-7.4767e-01, -8.0795e-01, -1.2677e+00,  ...,  1.1749e+00,
           -2.8812e-01,  2.5068e-01],
          ...,
          [ 1.4675e-01, -1.7772e+00, -1.3329e+00,  ..., -3.7982e-01,
           -4.5880e-01,  7.3329e-01],
          [-8.2354e-02,  6.8987e-02, -4.6422e-01,  ..., -7.7316e-01,
            7.5831e-02,  1.3809e+00],
          [ 7.5636e-01, -1.8346e+00, -9.2929e-01,  ..., -7.6746e-02,
            8.4120e-01,  7.5314e-01]],

         [[-1.4953e+00,  3.1882e-01, -9.2675e-01,  ...,  7.6565e-03,
           -1.2217e+00,  3.7364e-01],
          [-1.2341e+00, -2.0318e-01, -2.2617e+00,  ..., -6.4202e-01,
            3.0418e-01, -3.7686e-01],
          [-9.9608e-01, -4.5888e-01, -1.9809e+00,  ..., -1.4859e+00,
           -2.0752e-01, -4.0894e-01],
          ...,
          [-1.1390e+00, -3.6417e-01, -2.4343e+00,  ..., -4.2244e-02,
            2.9877e-01, -4.9270e-02],
          [-1.4104e+00, -1.9348e-01, -1.8669e+00,  ...,  1.8806e-01,
            1.1311e+00, -3.6895e-01],
          [-4.1031e-01, -9.4875e-01, -1.8528e+00,  ..., -4.8252e-01,
            7.2783e-01, -3.8987e-01]],

         [[-7.0929e-01, -9.5226e-01,  1.2674e+00,  ...,  1.5206e+00,
            1.2482e-02,  9.5320e-01],
          [ 1.7513e+00, -1.1389e+00,  5.5175e-01,  ..., -2.1151e-01,
           -7.7313e-03,  1.5731e-01],
          [ 3.4182e-01, -1.1665e+00,  1.3360e+00,  ...,  2.0837e-01,
           -2.6134e-01,  8.9895e-01],
          ...,
          [-9.3424e-02, -1.6196e+00,  6.8913e-01,  ...,  5.6179e-01,
           -6.7292e-01, -7.8963e-02],
          [ 6.3145e-01, -8.2962e-01,  6.4450e-02,  ..., -3.4392e-02,
           -3.2775e-02,  9.5774e-01],
          [ 8.6299e-01, -1.7643e+00,  7.0598e-01,  ...,  1.2499e-01,
           -3.2008e-01, -5.0594e-01]],

         [[-1.1788e+00, -2.8933e-01,  1.6267e+00,  ..., -4.0424e-01,
            1.1319e+00,  6.4685e-01],
          [-1.4529e-01, -7.6845e-01, -3.9335e-01,  ...,  1.5997e+00,
            6.4910e-01, -1.7660e+00],
          [ 2.5497e-01, -1.2459e+00,  1.0629e+00,  ...,  5.9304e-02,
            5.7081e-01, -9.6875e-01],
          ...,
          [-6.1852e-01, -6.3182e-01,  3.6876e-01,  ...,  7.6554e-02,
            4.8908e-01, -1.6545e+00],
          [-5.0949e-01, -5.7170e-01, -7.1597e-01,  ...,  1.2100e+00,
            1.3739e-01, -1.8638e+00],
          [ 1.5241e-01, -6.6911e-01,  6.3213e-01,  ..., -2.9349e-01,
            8.8248e-01, -1.2875e+00]]],


        [[[-1.4019e+00,  1.2469e-01, -6.4959e-01,  ..., -6.3165e-01,
           -1.2290e+00, -3.2807e-01],
          [ 5.2016e-01, -1.5752e+00, -6.8608e-01,  ..., -8.9808e-01,
            9.0064e-02,  1.2038e+00],
          [ 2.4174e-01, -3.6771e-01, -5.7963e-01,  ...,  2.6881e-01,
            1.5769e+00,  6.7053e-01],
          ...,
          [ 4.6098e-02, -1.5601e+00, -1.5064e+00,  ..., -1.3816e-01,
           -8.1910e-01,  7.4525e-01],
          [-5.7826e-01, -1.1579e-01, -7.0237e-01,  ..., -2.4805e-01,
           -4.4638e-01,  1.0747e+00],
          [ 8.0845e-01, -2.2091e+00, -1.3812e-01,  ...,  2.5222e-01,
           -5.8437e-01, -6.8061e-01]],

         [[-1.4952e+00,  3.1880e-01, -9.2668e-01,  ...,  7.5993e-03,
           -1.2218e+00,  3.7361e-01],
          [-6.9943e-01, -4.0173e-01, -2.5122e+00,  ...,  2.3677e-01,
            4.4449e-01,  1.4000e-02],
          [-1.6018e-01, -8.1609e-01, -1.5977e+00,  ..., -1.3997e+00,
            5.4393e-01, -5.2474e-01],
          ...,
          [-1.3970e+00, -4.4264e-01, -2.2821e+00,  ..., -2.6757e-01,
            2.4777e-01, -3.9558e-01],
          [-1.0216e+00, -2.4485e-01, -2.3100e+00,  ..., -4.9616e-01,
            8.1422e-01, -1.1823e-01],
          [-1.1130e+00, -3.4101e-01, -2.1657e+00,  ..., -2.2674e-01,
            8.7466e-01,  3.1906e-01]],

         [[-7.0935e-01, -9.5204e-01,  1.2675e+00,  ...,  1.5207e+00,
            1.2517e-02,  9.5319e-01],
          [ 8.3052e-01, -1.2327e+00, -1.2134e-01,  ...,  3.2942e-01,
           -3.6211e-01, -1.4632e+00],
          [ 1.7019e+00, -9.3553e-01,  1.1349e+00,  ..., -7.4874e-01,
           -1.8057e-01,  9.4703e-01],
          ...,
          [-5.7832e-01, -1.3780e+00,  7.9461e-01,  ...,  9.3853e-01,
           -5.5342e-01, -1.3986e-01],
          [ 3.3849e-01, -1.0188e+00,  2.0490e-01,  ..., -1.3938e-01,
            2.9316e-01,  1.4904e+00],
          [-6.6164e-02, -1.8121e+00, -1.2793e-01,  ...,  2.8023e-02,
           -1.0464e+00, -1.2449e+00]],

         [[-1.1788e+00, -2.8930e-01,  1.6268e+00,  ..., -4.0434e-01,
            1.1318e+00,  6.4690e-01],
          [-4.4336e-01, -1.5807e-01,  1.0880e-02,  ...,  1.0798e+00,
            9.8312e-01, -1.8872e+00],
          [ 9.2923e-01, -1.5439e+00, -3.9516e-02,  ...,  3.6558e-01,
           -1.7321e-01, -9.6952e-01],
          ...,
          [-5.0529e-01, -4.4187e-01,  3.1499e-01,  ...,  4.3390e-02,
            7.0488e-01, -1.9005e+00],
          [-3.7619e-01, -1.0494e+00, -3.5626e-01,  ...,  2.9374e-01,
           -2.6025e-01, -1.4309e+00],
          [-1.2607e+00, -1.8654e-01,  1.3293e+00,  ..., -5.3442e-01,
            8.9068e-01, -1.1254e+00]]],


        [[[-1.4018e+00,  1.2438e-01, -6.4964e-01,  ..., -6.3184e-01,
           -1.2288e+00, -3.2820e-01],
          [ 5.3703e-01, -1.4433e+00, -6.8873e-01,  ..., -1.0510e+00,
            4.2800e-03,  1.1143e+00],
          [-8.1508e-01, -9.2529e-01, -1.1509e+00,  ...,  1.3254e+00,
           -8.6485e-01, -1.7728e-03],
          ...,
          [ 3.6889e-02, -1.3431e+00, -1.0547e+00,  ...,  2.2067e-01,
           -4.8599e-01,  7.8563e-02],
          [ 6.6085e-01, -8.0675e-01, -8.2856e-01,  ..., -5.3414e-01,
            7.9448e-01,  1.4875e+00],
          [ 1.0616e+00, -1.4207e+00, -2.3594e-01,  ..., -7.0626e-01,
            8.5049e-01,  4.4317e-01]],

         [[-1.4953e+00,  3.1884e-01, -9.2670e-01,  ...,  7.6988e-03,
           -1.2218e+00,  3.7362e-01],
          [-7.0351e-01, -6.2821e-01, -2.3555e+00,  ...,  4.8323e-01,
            6.5682e-01, -4.5607e-02],
          [-1.2002e+00, -2.7269e-01, -2.0049e+00,  ..., -1.3644e+00,
           -2.0986e-01, -3.1151e-01],
          ...,
          [-7.6725e-01, -9.1600e-01, -2.0280e+00,  ..., -5.3473e-01,
            1.0409e+00, -1.6564e-01],
          [ 4.8224e-02, -7.5984e-01, -1.5085e+00,  ...,  4.4258e-01,
            1.3996e+00, -5.9026e-01],
          [-3.0774e-01, -9.8239e-01, -1.4587e+00,  ..., -7.9070e-02,
            1.4079e+00, -2.0091e-01]],

         [[-7.0947e-01, -9.5219e-01,  1.2675e+00,  ...,  1.5207e+00,
            1.2401e-02,  9.5299e-01],
          [ 7.5324e-01, -1.0100e+00, -4.2179e-01,  ...,  3.4358e-01,
           -6.1299e-01, -1.4656e+00],
          [-1.1793e-01, -1.3741e+00,  1.2378e+00,  ...,  3.4294e-01,
           -3.2489e-01,  7.9291e-01],
          ...,
          [ 1.7129e-01, -1.5992e+00,  1.6870e-01,  ..., -1.0418e-01,
           -8.9253e-01,  6.6988e-01],
          [ 1.0005e+00, -7.5124e-01,  2.4444e-01,  ..., -1.6813e-01,
           -2.8069e-01,  3.0346e-02],
          [ 1.0875e+00, -1.5577e+00,  2.4445e-02,  ..., -3.0586e-01,
           -6.7796e-01, -5.3799e-01]],

         [[-1.1788e+00, -2.8918e-01,  1.6268e+00,  ..., -4.0431e-01,
            1.1320e+00,  6.4685e-01],
          [-4.5451e-01, -5.0815e-02, -2.8446e-01,  ...,  1.3160e+00,
            6.0936e-01, -2.1122e+00],
          [-2.3039e-01, -1.0484e+00,  1.3870e+00,  ..., -1.4391e-01,
            8.2265e-01, -9.2209e-01],
          ...,
          [-5.2497e-01, -1.0024e+00,  3.8837e-01,  ..., -2.8755e-01,
           -3.0840e-01, -1.7304e+00],
          [ 5.3789e-01, -3.6145e-01, -1.2850e+00,  ...,  9.4656e-01,
           -2.5084e-01, -1.7360e+00],
          [-5.9960e-01, -6.6476e-01,  2.2515e-01,  ..., -7.0513e-02,
            2.7754e-01, -2.0113e+00]]]], device='cuda:0')
value = tensor([[[[ 1.1615e-02, -2.0869e-02,  7.7227e-03,  ...,  5.6541e-03,
           -1.1110e-02,  5.2474e-03],
          [-2.1020e-03, -3.3813e-03,  9.7583e-04,  ..., -2.9112e-03,
           -1.1796e-02, -3.9775e-03],
          [ 6.3632e-03,  4.2827e-03,  8.0546e-03,  ...,  2.6399e-03,
            9.6472e-04, -3.5516e-03],
          ...,
          [ 4.6160e-03, -1.3376e-02,  1.6532e-03,  ..., -8.3566e-03,
           -2.6247e-03,  4.1119e-03],
          [ 3.1663e-03, -4.8960e-03, -8.4467e-04,  ..., -8.3807e-03,
           -8.3743e-03, -3.8184e-03],
          [-4.3694e-04, -2.7341e-04,  6.2013e-03,  ..., -7.3613e-03,
            1.0605e-02, -4.8356e-03]],

         [[ 1.0055e-02, -1.0489e-02,  2.8094e-02,  ...,  1.5280e-02,
            1.1922e-02,  2.7412e-03],
          [ 5.3761e-03, -2.6631e-03,  1.1570e-02,  ..., -2.0739e-03,
            6.3212e-03,  2.5590e-03],
          [ 2.7907e-03, -4.0027e-03,  2.3903e-02,  ..., -1.0450e-02,
            1.1747e-02, -2.1098e-03],
          ...,
          [ 1.0921e-02, -2.5125e-02,  1.0875e-02,  ..., -1.3568e-02,
            9.2409e-04, -1.0498e-02],
          [ 1.0947e-02, -1.2538e-02, -5.1320e-03,  ...,  1.6126e-03,
            3.0642e-03,  2.3518e-04],
          [ 2.7015e-03, -5.6460e-03,  6.5697e-03,  ..., -1.7451e-02,
           -1.0734e-02, -7.6219e-03]],

         [[ 2.5361e-03, -5.6522e-03,  7.0029e-03,  ..., -1.7294e-02,
            6.3831e-03, -1.0577e-02],
          [ 9.2383e-03,  1.6258e-03, -4.7215e-03,  ..., -3.2895e-03,
            2.4391e-03,  1.1734e-02],
          [ 2.3334e-02, -1.5051e-02,  2.6005e-03,  ..., -2.5469e-02,
            1.0871e-02,  6.7779e-03],
          ...,
          [ 1.6077e-02, -6.1668e-03,  1.1067e-02,  ..., -9.8098e-03,
           -3.1580e-03, -1.7595e-03],
          [ 6.4897e-04,  7.4316e-03, -3.0490e-03,  ...,  1.2936e-02,
           -6.8090e-03,  7.2106e-03],
          [ 1.7759e-02, -1.4918e-02,  7.0005e-03,  ..., -1.6628e-02,
            5.4714e-03,  1.9778e-03]],

         [[-8.0953e-03,  1.1951e-02,  3.0324e-02,  ...,  1.6429e-02,
           -2.6243e-02, -2.5273e-03],
          [ 2.4258e-04,  6.7055e-03,  8.3071e-03,  ..., -7.7367e-03,
            2.1753e-03,  1.8441e-02],
          [ 6.7227e-03,  2.6937e-03,  3.6977e-03,  ..., -6.9424e-03,
            1.4322e-04,  1.4364e-02],
          ...,
          [ 7.9232e-03,  8.0042e-03,  1.9249e-02,  ..., -1.3949e-03,
            6.6543e-03,  8.7011e-03],
          [ 2.3729e-03,  1.2979e-02,  1.3881e-02,  ..., -7.6645e-03,
            9.6437e-03,  1.4026e-02],
          [ 2.0617e-02,  9.0415e-04,  3.5921e-03,  ...,  4.4406e-03,
            1.3923e-02,  8.7867e-03]]],


        [[[ 1.1616e-02, -2.0870e-02,  7.7229e-03,  ...,  5.6538e-03,
           -1.1109e-02,  5.2478e-03],
          [-2.2343e-03, -7.9560e-03,  2.8219e-04,  ..., -7.1792e-03,
           -1.3529e-02,  1.8725e-03],
          [ 6.0716e-05,  9.8087e-03,  3.7853e-03,  ...,  1.4649e-03,
            1.4549e-02, -1.0223e-02],
          ...,
          [ 2.4678e-03, -1.2868e-02,  6.7856e-03,  ..., -1.2696e-02,
           -1.6919e-03,  3.5422e-04],
          [ 9.5606e-03, -4.9920e-03,  4.4385e-04,  ..., -3.5425e-03,
           -3.0431e-03, -5.0723e-04],
          [-4.6415e-03, -1.2998e-02, -4.3035e-03,  ..., -8.7527e-03,
           -8.4063e-03,  1.6611e-02]],

         [[ 1.0055e-02, -1.0491e-02,  2.8095e-02,  ...,  1.5280e-02,
            1.1924e-02,  2.7416e-03],
          [ 5.3572e-03, -1.1900e-02,  4.2080e-04,  ..., -1.5150e-02,
           -9.3831e-04, -6.3823e-03],
          [ 1.7933e-03, -2.1441e-05,  1.8600e-02,  ..., -4.4341e-03,
           -2.9718e-03,  7.5933e-04],
          ...,
          [ 8.8256e-03, -2.0099e-02,  1.2064e-02,  ..., -1.7616e-02,
            1.8525e-05, -1.1626e-02],
          [ 9.8099e-03, -1.9044e-02,  9.4160e-03,  ...,  9.4321e-04,
            1.0332e-02,  4.2461e-04],
          [ 5.1240e-03, -2.4857e-02, -2.7761e-03,  ..., -1.6634e-02,
            1.9135e-03, -6.3796e-03]],

         [[ 2.5343e-03, -5.6532e-03,  7.0024e-03,  ..., -1.7296e-02,
            6.3831e-03, -1.0576e-02],
          [ 6.2652e-03, -2.0331e-04,  1.3774e-03,  ..., -4.1240e-03,
           -2.6845e-03,  5.1213e-03],
          [ 1.8449e-02, -1.5723e-02, -2.2728e-03,  ..., -2.0434e-02,
            9.2012e-03,  1.1979e-02],
          ...,
          [ 2.0785e-02, -6.1577e-03,  1.1806e-02,  ..., -9.0220e-03,
           -1.7813e-03,  1.0088e-04],
          [ 6.0945e-03, -5.3614e-04, -6.8380e-04,  ...,  1.4628e-04,
           -1.2071e-04,  5.3230e-03],
          [ 1.8148e-02, -4.3369e-03,  9.6656e-03,  ..., -1.4517e-02,
            3.2977e-04, -7.2913e-03]],

         [[-8.0944e-03,  1.1952e-02,  3.0324e-02,  ...,  1.6428e-02,
           -2.6244e-02, -2.5262e-03],
          [ 6.7827e-03,  4.8682e-03,  1.5702e-02,  ...,  9.9858e-04,
            9.1928e-03,  1.0854e-02],
          [ 1.7375e-02, -3.4973e-03, -7.3253e-03,  ..., -8.1180e-03,
            7.0509e-03,  2.0167e-02],
          ...,
          [ 2.7030e-03,  9.9470e-03,  1.7027e-02,  ..., -3.9144e-03,
            1.1326e-02,  5.7269e-03],
          [ 4.0630e-03,  1.1328e-02,  7.2893e-03,  ..., -1.0059e-02,
            4.6425e-03,  1.5983e-02],
          [ 1.2573e-02,  8.7777e-03,  2.0577e-02,  ...,  1.3357e-02,
            8.2340e-04,  8.5396e-03]]],


        [[[ 1.1614e-02, -2.0870e-02,  7.7232e-03,  ...,  5.6534e-03,
           -1.1109e-02,  5.2477e-03],
          [-4.1343e-03, -1.0457e-02, -1.7496e-03,  ..., -9.0546e-03,
           -1.3590e-02, -2.0671e-04],
          [ 7.4706e-03,  1.3042e-03,  7.3439e-03,  ...,  1.8564e-03,
           -4.2984e-03,  1.6137e-03],
          ...,
          [ 4.1304e-03, -8.4424e-03, -1.3533e-03,  ..., -7.0624e-03,
            8.2020e-03,  4.3130e-04],
          [-1.2512e-03,  2.7086e-03,  3.7468e-04,  ..., -1.2148e-02,
            7.3139e-03, -1.1009e-02],
          [-8.4020e-03, -7.8765e-03, -3.5262e-03,  ..., -1.2498e-02,
            1.0798e-02, -7.3789e-04]],

         [[ 1.0053e-02, -1.0489e-02,  2.8095e-02,  ...,  1.5278e-02,
            1.1922e-02,  2.7417e-03],
          [ 5.5589e-03, -1.6898e-02, -1.2662e-03,  ..., -1.7358e-02,
           -5.5634e-03, -8.7770e-03],
          [ 4.4875e-03, -7.8632e-03,  2.1450e-02,  ..., -1.1162e-02,
            1.7050e-02, -2.5020e-03],
          ...,
          [ 8.8752e-03, -3.2211e-02,  1.0280e-02,  ..., -1.3457e-02,
           -3.7068e-03, -1.0454e-02],
          [ 7.4882e-03, -9.1128e-03, -1.0504e-02,  ..., -1.4478e-02,
           -1.4496e-02, -1.1226e-02],
          [ 3.5318e-03, -2.0266e-02,  3.7263e-05,  ..., -1.3552e-02,
           -1.8306e-02, -4.5931e-03]],

         [[ 2.5362e-03, -5.6520e-03,  7.0038e-03,  ..., -1.7295e-02,
            6.3826e-03, -1.0577e-02],
          [ 4.0646e-03,  3.1674e-04,  1.8227e-03,  ...,  7.9809e-04,
           -6.6213e-03,  5.7439e-03],
          [ 2.4569e-02, -1.2846e-02,  4.4865e-03,  ..., -2.5574e-02,
            1.0998e-02,  3.8238e-03],
          ...,
          [ 1.6177e-02, -1.3019e-02,  9.0804e-03,  ..., -1.0497e-02,
           -8.8003e-04, -1.4287e-04],
          [ 3.5348e-03, -3.2387e-04,  1.8782e-03,  ...,  1.1218e-02,
           -9.3295e-03,  9.6126e-03],
          [ 1.3703e-02, -9.8873e-03,  4.5786e-03,  ..., -8.3509e-03,
           -1.6387e-03,  2.5144e-03]],

         [[-8.0963e-03,  1.1951e-02,  3.0323e-02,  ...,  1.6429e-02,
           -2.6242e-02, -2.5280e-03],
          [ 4.0197e-03,  4.6932e-03,  1.4654e-02,  ..., -1.6679e-04,
            1.0978e-02,  1.1307e-02],
          [ 6.1403e-03,  6.1523e-03,  9.5542e-03,  ..., -4.4307e-03,
           -2.4545e-03,  1.3171e-02],
          ...,
          [ 1.3259e-02,  5.4761e-03,  6.1613e-03,  ..., -1.9997e-03,
            7.5953e-03,  1.3811e-02],
          [ 1.4314e-02, -7.0902e-04,  5.0899e-04,  ..., -1.2390e-02,
            2.7212e-02,  8.6811e-03],
          [ 2.0547e-02,  5.0490e-03,  4.3325e-03,  ...,  6.6270e-03,
            1.3912e-02,  1.6894e-02]]]], device='cuda:0')
attention_mask = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]],


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')
dropout = 0.0, scaling = 0.3535533905932738, is_causal = False, kwargs = {}
sdpa_kwargs = {}

    def sdpa_attention_forward(
        module: torch.nn.Module,
        query: torch.Tensor,
        key: torch.Tensor,
        value: torch.Tensor,
        attention_mask: Optional[torch.Tensor],
        dropout: float = 0.0,
        scaling: Optional[float] = None,
        is_causal: Optional[bool] = None,
        **kwargs,
    ) -> tuple[torch.Tensor, None]:
        if kwargs.get("output_attentions", False):
            logger.warning_once(
                "`sdpa` attention does not support `output_attentions=True`."
                " Please set your attention to `eager` if you want any of these features."
            )
        sdpa_kwargs = {}
        if hasattr(module, "num_key_value_groups"):
            if not use_gqa_in_sdpa(attention_mask, key):
                key = repeat_kv(key, module.num_key_value_groups)
                value = repeat_kv(value, module.num_key_value_groups)
            else:
                sdpa_kwargs = {"enable_gqa": True}
    
        if attention_mask is not None and attention_mask.ndim == 4:
            attention_mask = attention_mask[:, :, :, : key.shape[-2]]
    
        # Instead of relying on the value set in the module directly, we use the is_causal passed in kwargs if it is presented
        is_causal = is_causal if is_causal is not None else getattr(module, "is_causal", True)
    
        # SDPA's Flash Attention (and cuDNN) kernels rely on the `is_causal` flag. However, there are certain conditions:
        # - Not in decoding phase (otherwise we want full attention on the single query token)
        # - Attention mask is not to be provided (even if it is a causal pattern)
        # - Internally, we marked this as compatible with causal, i.e. it is a decoder attention type
        #
        # Quirks on the conditionals:
        # - We avoid inline passing this to the SDPA function directly to support both torch.compile's dynamic shapes and
        #   full graph options. Otherwise, dynamic shapes are prevented from compiling.
        # - It is important to check first for the shape, otherwise compile will fail with
        #   `argument 'is_causal' must be bool, not SymBool`.
        is_causal = query.shape[2] > 1 and attention_mask is None and is_causal
    
        # Shapes (e.g. query.shape[2]) are tensors during jit tracing, resulting in `is_causal` being a tensor.
        # We convert it to a bool for the SDPA kernel that only accepts bools.
        if torch.jit.is_tracing() and isinstance(is_causal, torch.Tensor):
            is_causal = is_causal.item()
    
        # When `is_causal = False` and the `attention_mask` is not of boolean type, the Ascend NPU's SDPA interface cannot utilize the FlashAttentionScore operator
        # and falls back to small-operator concatenation. To invoke the FlashAttentionScore, the attention_mask must be converted to boolean type.
        # This adaptation ensures the `attention_mask` meets the requirement for using FlashAttentionScore.
        if _is_torch_npu_available:
            if attention_mask is not None and attention_mask.dtype != torch.bool:
                # Convert to boolean type, making sdpa to force call FlashAttentionScore to improve performance.
                attention_mask = torch.logical_not(attention_mask.bool()).to(query.device)
    
>       attn_output = torch.nn.functional.scaled_dot_product_attention(
            query,
            key,
            value,
            attn_mask=attention_mask,
            dropout_p=dropout,
            scale=scaling,
            is_causal=is_causal,
            **sdpa_kwargs,
        )
E       RuntimeError: The expanded size of the tensor (1808) must match the existing size (904) at non-singleton dimension 3.  Target sizes: [3, 4, 7, 1808].  Tensor sizes: [3, 1, 7, 904]

src/transformers/integrations/sdpa_attention.py:97: RuntimeError
___________ ClvpModelForConditionalGenerationTest.test_torch_export ____________
[gw18] linux -- Python 3.10.12 /home/ilyas/transformers/.docker/bin/python3

self = <tests.models.clvp.test_modeling_clvp.ClvpModelForConditionalGenerationTest testMethod=test_torch_export>
atol = 0.0001, rtol = 0.0001

    @slow
    @require_torch_greater_or_equal("2.5")
    @pytest.mark.torch_export_test
    def test_torch_export(self, atol=1e-4, rtol=1e-4):
        """
        Test if model can be exported with torch.export.export()
    
        Args:
            atol (`float`, *optional*, defaults to 1e-4): absolute tolerance for output comparison
            rtol (`float`, *optional*, defaults to 1e-4): relative tolerance for output comparison
        """
    
        exporter = DynamoExporter(export_config=DynamoConfig())
    
        for model_class in self.all_model_classes:
            with self.subTest(model_class.__name__):
                if hasattr(self.model_tester, "prepare_config_and_inputs_for_model_class"):
                    config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_model_class(model_class)
                else:
>                   config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()

tests/test_modeling_common.py:3514: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/models/clvp/test_modeling_clvp.py:381: in prepare_config_and_inputs_for_common
    config_and_inputs = self.prepare_config_and_inputs()
tests/models/clvp/test_modeling_clvp.py:357: in prepare_config_and_inputs
    ds = datasets.load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
.docker/lib/python3.10/site-packages/datasets/load.py:1397: in load_dataset
    builder_instance = load_dataset_builder(
.docker/lib/python3.10/site-packages/datasets/load.py:1137: in load_dataset_builder
    dataset_module = dataset_module_factory(
.docker/lib/python3.10/site-packages/datasets/load.py:1036: in dataset_module_factory
    raise e1 from None
.docker/lib/python3.10/site-packages/datasets/load.py:1009: in dataset_module_factory
    ).get_module()
.docker/lib/python3.10/site-packages/datasets/load.py:592: in get_module
    standalone_yaml_path = cached_path(
.docker/lib/python3.10/site-packages/datasets/utils/file_utils.py:180: in cached_path
    ).resolve_path(url_or_filename)
.docker/lib/python3.10/site-packages/huggingface_hub/hf_file_system.py:269: in resolve_path
    repo_and_revision_exist, err = self._repo_and_revision_exist(repo_type, repo_id, revision)
.docker/lib/python3.10/site-packages/huggingface_hub/hf_file_system.py:196: in _repo_and_revision_exist
    self._api.repo_info(
.docker/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:89: in _inner_fn
    return fn(*args, **kwargs)
.docker/lib/python3.10/site-packages/huggingface_hub/hf_api.py:2758: in repo_info
    return method(
.docker/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:89: in _inner_fn
    return fn(*args, **kwargs)
.docker/lib/python3.10/site-packages/huggingface_hub/hf_api.py:2622: in dataset_info
    hf_raise_for_status(r)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

response = <Response [429 Too Many Requests]>, endpoint_name = None

    def hf_raise_for_status(response: httpx.Response, endpoint_name: Optional[str] = None) -> None:
        """
        Internal version of `response.raise_for_status()` that will refine a potential HTTPError.
        Raised exception will be an instance of [`~errors.HfHubHTTPError`].
    
        This helper is meant to be the unique method to raise_for_status when making a call to the Hugging Face Hub.
    
        Args:
            response (`Response`):
                Response from the server.
            endpoint_name (`str`, *optional*):
                Name of the endpoint that has been called. If provided, the error message will be more complete.
    
        > [!WARNING]
        > Raises when the request has failed:
        >
        >     - [`~utils.RepositoryNotFoundError`]
        >         If the repository to download from cannot be found. This may be because it
        >         doesn't exist, because `repo_type` is not set correctly, or because the repo
        >         is `private` and you do not have access.
        >     - [`~utils.GatedRepoError`]
        >         If the repository exists but is gated and the user is not on the authorized
        >         list.
        >     - [`~utils.RevisionNotFoundError`]
        >         If the repository exists but the revision couldn't be found.
        >     - [`~utils.EntryNotFoundError`]
        >         If the repository exists but the entry (e.g. the requested file) couldn't be
        >         find.
        >     - [`~utils.BadRequestError`]
        >         If request failed with a HTTP 400 BadRequest error.
        >     - [`~utils.HfHubHTTPError`]
        >         If request failed for a reason not listed above.
        """
        try:
            response.raise_for_status()
        except httpx.HTTPStatusError as e:
            if response.status_code // 100 == 3:
                return  # Do not raise on redirects to stay consistent with `requests`
    
            error_code = response.headers.get("X-Error-Code")
            error_message = response.headers.get("X-Error-Message")
    
            if error_code == "RevisionNotFound":
                message = f"{response.status_code} Client Error." + "\n\n" + f"Revision Not Found for url: {response.url}."
                raise _format(RevisionNotFoundError, message, response) from e
    
            elif error_code == "EntryNotFound":
                message = f"{response.status_code} Client Error." + "\n\n" + f"Entry Not Found for url: {response.url}."
                raise _format(RemoteEntryNotFoundError, message, response) from e
    
            elif error_code == "GatedRepo":
                message = (
                    f"{response.status_code} Client Error." + "\n\n" + f"Cannot access gated repo for url {response.url}."
                )
                raise _format(GatedRepoError, message, response) from e
    
            elif error_message == "Access to this resource is disabled.":
                message = (
                    f"{response.status_code} Client Error."
                    + "\n\n"
                    + f"Cannot access repository for url {response.url}."
                    + "\n"
                    + "Access to this resource is disabled."
                )
                raise _format(DisabledRepoError, message, response) from e
    
            elif error_code == "RepoNotFound" or (
                response.status_code == 401
                and error_message != "Invalid credentials in Authorization header"
                and response.request is not None
                and response.request.url is not None
                and REPO_API_REGEX.search(str(response.request.url)) is not None
            ):
                # 401 is misleading as it is returned for:
                #    - private and gated repos if user is not authenticated
                #    - missing repos
                # => for now, we process them as `RepoNotFound` anyway.
                # See https://gist.github.com/Wauplin/46c27ad266b15998ce56a6603796f0b9
                message = (
                    f"{response.status_code} Client Error."
                    + "\n\n"
                    + f"Repository Not Found for url: {response.url}."
                    + "\nPlease make sure you specified the correct `repo_id` and"
                    " `repo_type`.\nIf you are trying to access a private or gated repo,"
                    " make sure you are authenticated. For more details, see"
                    " https://huggingface.co/docs/huggingface_hub/authentication"
                )
                raise _format(RepositoryNotFoundError, message, response) from e
    
            elif response.status_code == 400:
                message = (
                    f"\n\nBad request for {endpoint_name} endpoint:" if endpoint_name is not None else "\n\nBad request:"
                )
                raise _format(BadRequestError, message, response) from e
    
            elif response.status_code == 403:
                message = (
                    f"\n\n{response.status_code} Forbidden: {error_message}."
                    + f"\nCannot access content at: {response.url}."
                    + "\nMake sure your token has the correct permissions."
                )
                raise _format(HfHubHTTPError, message, response) from e
    
            elif response.status_code == 416:
                range_header = response.request.headers.get("Range")
                message = f"{e}. Requested range: {range_header}. Content-Range: {response.headers.get('Content-Range')}."
                raise _format(HfHubHTTPError, message, response) from e
    
            # Convert `HTTPError` into a `HfHubHTTPError` to display request information
            # as well (request id and/or server error message)
>           raise _format(HfHubHTTPError, str(e), response) from e
E           huggingface_hub.errors.HfHubHTTPError: Client error '429 Too Many Requests' for url 'https://huggingface.co/api/datasets/hf-internal-testing/librispeech_asr_dummy/revision/5be91486e11a2d616f4ec5db8d3fd248585ac07a' (Request ID: Root=1-6920109c-5f2b8cd538a184b873bf8590;5f2b8c49-34b1-480b-9a4e-8fb7917e18b6)
E           For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429
E           
E           We had to rate limit your IP (85.118.59.141). To continue using our service, create a HF account or login to your existing account, and make sure you pass a HF_TOKEN if you're using the API.

.docker/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:629: HfHubHTTPError
----------------------------- Captured stderr call -----------------------------
Test failed with Client error '429 Too Many Requests' for url 'https://huggingface.co/api/datasets/hf-internal-testing/librispeech_asr_dummy/revision/5be91486e11a2d616f4ec5db8d3fd248585ac07a' (Request ID: Root=1-69201092-0005d4e619d59a814c15b229;582b7d90-bfd7-4d32-a007-45f0c01ef0ba)
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

We had to rate limit your IP (85.118.59.141). To continue using our service, create a HF account or login to your existing account, and make sure you pass a HF_TOKEN if you're using the API. at try 1/5 as it couldn't connect to the specified Hub repository.
Test failed with Client error '429 Too Many Requests' for url 'https://huggingface.co/api/datasets/hf-internal-testing/librispeech_asr_dummy/revision/5be91486e11a2d616f4ec5db8d3fd248585ac07a' (Request ID: Root=1-69201095-7cce5bbb6d29815f36d91d9f;e4154e4c-651d-4be6-8967-0cf5a44d8ca1)
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

We had to rate limit your IP (85.118.59.141). To continue using our service, create a HF account or login to your existing account, and make sure you pass a HF_TOKEN if you're using the API. at try 2/5 as it couldn't connect to the specified Hub repository.
Test failed with Client error '429 Too Many Requests' for url 'https://huggingface.co/api/datasets/hf-internal-testing/librispeech_asr_dummy/revision/5be91486e11a2d616f4ec5db8d3fd248585ac07a' (Request ID: Root=1-69201097-579889626023373e40c54062;e6a13190-4108-4b23-86bc-4586c1198361)
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

We had to rate limit your IP (85.118.59.141). To continue using our service, create a HF account or login to your existing account, and make sure you pass a HF_TOKEN if you're using the API. at try 3/5 as it couldn't connect to the specified Hub repository.
Test failed with Client error '429 Too Many Requests' for url 'https://huggingface.co/api/datasets/hf-internal-testing/librispeech_asr_dummy/revision/5be91486e11a2d616f4ec5db8d3fd248585ac07a' (Request ID: Root=1-6920109a-4d3119464e1402465c962c3e;5ffc5fc4-8e91-4090-9ab8-a5e1342162e7)
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

We had to rate limit your IP (85.118.59.141). To continue using our service, create a HF account or login to your existing account, and make sure you pass a HF_TOKEN if you're using the API. at try 4/5 as it couldn't connect to the specified Hub repository.
_____________________ VideoMAEModelTest.test_torch_export ______________________
[gw11] linux -- Python 3.10.12 /home/ilyas/transformers/.docker/bin/python3

self = <tests.models.videomae.test_modeling_videomae.VideoMAEModelTest testMethod=test_torch_export>
atol = 0.0001, rtol = 0.0001

    @slow
    @require_torch_greater_or_equal("2.5")
    @pytest.mark.torch_export_test
    def test_torch_export(self, atol=1e-4, rtol=1e-4):
        """
        Test if model can be exported with torch.export.export()
    
        Args:
            atol (`float`, *optional*, defaults to 1e-4): absolute tolerance for output comparison
            rtol (`float`, *optional*, defaults to 1e-4): relative tolerance for output comparison
        """
    
        exporter = DynamoExporter(export_config=DynamoConfig())
    
        for model_class in self.all_model_classes:
            with self.subTest(model_class.__name__):
                if hasattr(self.model_tester, "prepare_config_and_inputs_for_model_class"):
                    config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_model_class(model_class)
                else:
                    config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()
                inputs_dict = self._prepare_for_class(inputs_dict, model_class)
                model = model_class(config).eval().to(torch_device)
    
                # prepare cache inputs for auto-regressive models and include it for computing eager outputs
                # process output flags (e.g. use_cache, output_attentions, etc) to avoid passing them as inputs
                model, inputs_dict = prepare_for_export(model, inputs_dict)
    
                with torch.no_grad():
                    # Running the eager inference before the export to catch model/inputs comatibility issues, also sometimes after
                    # the export, the model used for export will return FakeTensors instead of real ones (torch cuda/inductor issue)
                    # This happens on cuda with (codegen, clvp, esm, gptj, levit, wav2vec2_bert and wav2vec2_conformer)
                    set_seed(1234)
                    eager_outputs = model(**copy.deepcopy(inputs_dict))
                    eager_outputs = get_leaf_tensors(eager_outputs)
                    self.assertTrue(eager_outputs, "Eager outputs is empty.")
    
                try:
>                   exported_program = exporter.export(model, inputs_dict)

tests/test_modeling_common.py:3532: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/exporters/exporter_dynamo.py:122: in export
    exported_program: ExportedProgram = torch.export.export(
.docker/lib/python3.10/site-packages/torch/export/__init__.py:311: in export
    raise e
.docker/lib/python3.10/site-packages/torch/export/__init__.py:277: in export
    return _export(
.docker/lib/python3.10/site-packages/torch/export/_trace.py:1163: in wrapper
    raise e
.docker/lib/python3.10/site-packages/torch/export/_trace.py:1129: in wrapper
    ep = fn(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/export/exported_program.py:124: in wrapper
    return fn(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/export/_trace.py:2255: in _export
    ep = _export_for_training(
.docker/lib/python3.10/site-packages/torch/export/_trace.py:1163: in wrapper
    raise e
.docker/lib/python3.10/site-packages/torch/export/_trace.py:1129: in wrapper
    ep = fn(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/export/exported_program.py:124: in wrapper
    return fn(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/export/_trace.py:2071: in _export_for_training
    export_artifact = export_func(
.docker/lib/python3.10/site-packages/torch/export/_trace.py:2002: in _non_strict_export
    aten_export_artifact = _to_aten_func(  # type: ignore[operator]
.docker/lib/python3.10/site-packages/torch/export/_trace.py:1793: in _export_to_aten_ir_make_fx
    gm, graph_signature = transform(_make_fx_helper)(
.docker/lib/python3.10/site-packages/torch/export/_trace.py:1922: in _aot_export_non_strict
    gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags)
.docker/lib/python3.10/site-packages/torch/export/_trace.py:1706: in _make_fx_helper
    gm = make_fx(
.docker/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:2429: in wrapped
    return make_fx_tracer.trace(f, *args)
.docker/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:2356: in trace
    return self._trace_inner(f, *args)
.docker/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:2318: in _trace_inner
    t = dispatch_trace(
.docker/lib/python3.10/site-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1303: in dispatch_trace
    graph = tracer.trace(root, concrete_args)  # type: ignore[arg-type]
.docker/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1908: in trace
    res = super().trace(root, concrete_args)
.docker/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:868: in trace
    (self.create_arg(fn(*args)),),
.docker/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1361: in wrapped
    out = f(*tensors)  # type:ignore[call-arg]
<string>:1: in <lambda>
    ???
.docker/lib/python3.10/site-packages/torch/export/_trace.py:1593: in wrapped_fn
    return tuple(flat_fn(*args))
.docker/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:187: in flat_fn
    tree_out = fn(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/graph_capture_wrappers.py:1354: in functional_call
    out = mod(*args[params_len:], **kwargs)
.docker/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:843: in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
.docker/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1997: in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
.docker/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:560: in call_module
    ret_val = forward(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:836: in forward
    return _orig_module_call(mod, *args, **kwargs)
.docker/lib/python3.10/site-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/export/_trace.py:1906: in forward
    tree_out = mod(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:843: in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
.docker/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1997: in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
.docker/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:560: in call_module
    ret_val = forward(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:836: in forward
    return _orig_module_call(mod, *args, **kwargs)
.docker/lib/python3.10/site-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
src/transformers/utils/generic.py:758: in wrapper
    output = func(self, *args, **kwargs)
src/transformers/models/videomae/modeling_videomae.py:594: in forward
    decoder_outputs: VideoMAEDecoderOutput = self.decoder(x_full, pos_emb_mask.shape[1])
.docker/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:843: in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
.docker/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1997: in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
.docker/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:560: in call_module
    ret_val = forward(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:836: in forward
    return _orig_module_call(mod, *args, **kwargs)
.docker/lib/python3.10/site-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
src/transformers/models/videomae/modeling_videomae.py:505: in forward
    if return_token_num > 0:
.docker/lib/python3.10/site-packages/torch/__init__.py:762: in __bool__
    return self.node.bool_()
.docker/lib/python3.10/site-packages/torch/fx/experimental/sym_node.py:616: in bool_
    return self.guard_bool("", 0)
.docker/lib/python3.10/site-packages/torch/fx/experimental/sym_node.py:538: in guard_bool
    r = self.evaluate()
.docker/lib/python3.10/site-packages/torch/fx/experimental/sym_node.py:512: in evaluate
    return self.shape_env.evaluate_sym_node(self, size_oblivious)
.docker/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:7233: in evaluate_sym_node
    return self.evaluate_expr(
.docker/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:7333: in evaluate_expr
    return self._inner_evaluate_expr(
.docker/lib/python3.10/site-packages/torch/fx/experimental/recording.py:272: in wrapper
    return retlog(fn(*args, **kwargs))
.docker/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:7356: in _inner_evaluate_expr
    return self._evaluate_expr(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <torch.fx.experimental.symbolic_shapes.ShapeEnv object at 0x7f03020c0af0>
orig_expr = u5 > 0, hint = None, fx_node = False, size_oblivious = False
fallback_value = None

    def _evaluate_expr(
        self,
        orig_expr: sympy.Basic,
        hint: Optional[Union[bool, int, float]] = None,
        fx_node: Optional[torch.fx.Node] = None,
        size_oblivious: bool = False,
        fallback_value: Optional[bool] = None,
        *,
        forcing_spec: bool = False,
    ) -> sympy.Basic:
        # TODO: split conjunctions and evaluate them separately
    
        if isinstance(
            orig_expr,
            (sympy.logic.boolalg.BooleanTrue, sympy.logic.boolalg.BooleanFalse),
        ):
            return orig_expr
    
        # Don't track this one. (Because this cache is inside this function the
        # cache only lasts for the invocation of this function call)
        @functools.cache
        def compute_concrete_val() -> sympy.Basic:
            if hint is None:
                # This is only ever called for expressions WITHOUT unbacked
                # symbols
                r = self.size_hint(orig_expr)
                assert r is not None
                return r
            else:
                return sympy.sympify(hint)
    
        concrete_val: Optional[sympy.Basic]
    
        # Check if:
        #   1. 'translation_validation' is set
        #   2. the corresponding 'fx_node' is not 'None'
        #   3. the guard should not be suppressed
        #   4. the guard doesn't contain backed symfloat symbols
        #      since z3 can't handle floats
        #   5. fallback_value is none.
        # If all of the above check, we create an FX node representing the
        # actual expression to be guarded.
        node = None
        fresh = False
        if (
            self._translation_validation_enabled
            and fx_node is not None
            and not self._suppress_guards_tls()
            and not size_oblivious
            and not any(symbol_is_type(s, SymT.FLOAT) for s in orig_expr.free_symbols)
            and fallback_value is None
        ):
            # TODO: does this even worked with unbacked :think:
            concrete_val = compute_concrete_val()
            if concrete_val is sympy.true:
                node, fresh = self._create_fx_call_function(torch._assert, (fx_node,))
            elif concrete_val is sympy.false:
                neg, _ = self._create_fx_call_function(operator.not_, (fx_node,))
                node, fresh = self._create_fx_call_function(torch._assert, (neg,))
            else:
                eql, _ = self._create_fx_call_function(
                    operator.eq, (fx_node, concrete_val)
                )
                node, fresh = self._create_fx_call_function(torch._assert, (eql,))
    
            assert node is not None
            # If this is a fresh node, we have to remember the event index that
            # corresponds to this assertion node.
            # Reason: so that, given an assertion node, we can replay the ShapeEnv
            # events until the point where this assertion node was freshly created.
            if fresh:
                self._add_fx_node_metadata(node)
    
        # After creating the FX node corresponding to orig_expr, we must make sure that
        # no error will be raised until the end of this function.
        #
        # Reason: the translation validation may become invalid otherwise.
        #
        # If an error is raised before the end of this function, we remove the FX node
        # inserted, and re-raise the error.
        guard = None
    
        try:
            if orig_expr.is_number:
                self.log.debug("eval %s [trivial]", orig_expr)
                if hint is not None:
                    if isinstance(hint, bool):
                        assert orig_expr == hint, f"{orig_expr} != {hint}"
                    else:
                        assert sympy.Eq(orig_expr, hint), f"{orig_expr} != {hint}"
                return orig_expr
    
            expr = orig_expr
    
            static_expr = self._maybe_evaluate_static(
                expr, size_oblivious=size_oblivious
            )
            if static_expr is not None:
                self.log.debug(
                    "eval %s == %s [statically known]",
                    (
                        f"size_oblivious({orig_expr})"
                        if size_oblivious
                        else size_oblivious
                    ),
                    static_expr,
                )
                if (
                    not size_oblivious
                    and config.backed_size_oblivious
                    and hint is not None
                ):
                    # TODO: maybe reconcile this with use of counterfactual hints
                    # in unbacked case
                    assert static_expr == hint, f"{static_expr} != {hint}"
                return static_expr
    
            transmute_into_runtime_assert = False
    
            concrete_val = None
            if not (expr.free_symbols <= self.var_to_val.keys()):
                # TODO: dedupe this with _maybe_evaluate_static
                # Attempt to eliminate the unbacked SymInt
                new_expr = self._maybe_evaluate_static(expr, unbacked_only=True)
                assert new_expr is not None
                if not (new_expr.free_symbols <= self.var_to_val.keys()):
                    ok = False
    
                    # fallback_value is set when guard_or_true or guard_or_false are used.
                    if not ok and fallback_value is not None:
                        self._log_suppressed_dde(orig_expr, fallback_value)
                        return fallback_value
    
                    # oblivious_var_to_val will be defined iff we have sizes with DimDynamic.OBLIVIOUS_SIZE type.
                    # See https://github.com/pytorch/pytorch/issues/137100#issuecomment-2495778113
                    if (
                        self.oblivious_var_to_val
                        and not (
                            correct_hint := orig_expr.xreplace(
                                self.oblivious_var_to_val
                            )
                        ).free_symbols
                        and not (
                            counterfactual_hint := orig_expr.xreplace(
                                {
                                    k: max(2, v)
                                    for k, v in self.oblivious_var_to_val.items()
                                }
                            )
                        ).free_symbols
                        and correct_hint == counterfactual_hint
                    ):
                        # TODO: better logging
                        log.info(
                            "oblivious_size %s -> %s (passed counterfactual)",
                            orig_expr,
                            correct_hint,
                        )
                        concrete_val = correct_hint
                        # NB: do NOT transmute into runtime assert
                        ok = True
    
                    # unbacked_var_to_val is not None iff propagate_real_tensors is on.
                    # if propagate_real_tensors is on, we check the example values to generate (unsound_result)
                    # and if they pass we add a runtime assertions and continue.
                    if (
                        not ok
                        and self.unbacked_var_to_val
                        and not (
                            unsound_result := orig_expr.xreplace(
                                self.unbacked_var_to_val
                            ).xreplace(self.var_to_val)
                        ).free_symbols
                    ):
                        self._log_real_tensor_propagation(orig_expr, unsound_result)
                        transmute_into_runtime_assert = True
                        concrete_val = unsound_result
                        ok = True
    
                    # Check if this is coming from a python assert statement, if so, convert it to a runtime assertion
                    # instead of failing.
                    if not ok and self.trace_asserts and self._is_python_assert():
                        concrete_val = sympy.true
                        transmute_into_runtime_assert = True
                        ok = True
    
                    if not ok:
>                       raise self._make_data_dependent_error(
                            expr.xreplace(self.var_to_val),
                            expr,
                            expr_sym_node_id=self._expr_sym_node_id,
                        )
E                       torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode: Could not guard on data-dependent expression u5 > 0 (unhinted: u5 > 0).  (Size-like symbols: u5)
E                       
E                       consider using data-dependent friendly APIs such as guard_or_false, guard_or_true and statically_known_trueCaused by: (src/transformers/models/videomae/modeling_videomae.py:505 in forward)
E                       For more information, run with TORCH_LOGS="dynamic"
E                       For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="u5"
E                       If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
E                       For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing
E                       
E                       For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
E                       
E                       The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.

.docker/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:7574: GuardOnDataDependentSymNode
----------------------------- Captured stderr call -----------------------------



def forward(self, arg0_1: "f32[1, 1, 32]", arg1_1: "f32[32, 3, 2, 2, 2]", arg2_1: "f32[32]", arg3_1: "f32[32]", arg4_1: "f32[32]", arg5_1: "f32[32, 32]", arg6_1: "f32[32, 32]", arg7_1: "f32[32, 32]", arg8_1: "f32[32, 32]", arg9_1: "f32[32]", arg10_1: "f32[37, 32]", arg11_1: "f32[37]", arg12_1: "f32[32, 37]", arg13_1: "f32[32]", arg14_1: "f32[32]", arg15_1: "f32[32]", arg16_1: "f32[32]", arg17_1: "f32[32]", arg18_1: "f32[32]", arg19_1: "f32[32]", arg20_1: "f32[32, 32]", arg21_1: "f32[32, 32]", arg22_1: "f32[32, 32]", arg23_1: "f32[32, 32]", arg24_1: "f32[32]", arg25_1: "f32[37, 32]", arg26_1: "f32[37]", arg27_1: "f32[32, 37]", arg28_1: "f32[32]", arg29_1: "f32[32]", arg30_1: "f32[32]", arg31_1: "f32[32]", arg32_1: "f32[32]", arg33_1: "f32[32, 32]", arg34_1: "f32[32]", arg35_1: "f32[32]", arg36_1: "f32[32, 32]", arg37_1: "f32[32, 32]", arg38_1: "f32[32, 32]", arg39_1: "f32[32, 32]", arg40_1: "f32[32]", arg41_1: "f32[37, 32]", arg42_1: "f32[37]", arg43_1: "f32[32, 37]", arg44_1: "f32[32]", arg45_1: "f32[32]", arg46_1: "f32[32]", arg47_1: "f32[32]", arg48_1: "f32[32]", arg49_1: "f32[32]", arg50_1: "f32[32]", arg51_1: "f32[32, 32]", arg52_1: "f32[32, 32]", arg53_1: "f32[32, 32]", arg54_1: "f32[32, 32]", arg55_1: "f32[32]", arg56_1: "f32[37, 32]", arg57_1: "f32[37]", arg58_1: "f32[32, 37]", arg59_1: "f32[32]", arg60_1: "f32[32]", arg61_1: "f32[32]", arg62_1: "f32[32]", arg63_1: "f32[32]", arg64_1: "f32[32]", arg65_1: "f32[32]", arg66_1: "f32[24, 32]", arg67_1: "f32[24]", arg68_1: "f32[13, 2, 3, 10, 10]", arg69_1: "b8[13, 25]"):
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:176 in forward, code: pixel_values = pixel_values.permute(0, 2, 1, 3, 4)
    permute: "f32[13, 3, 2, 10, 10]" = torch.ops.aten.permute.default(arg68_1, [0, 2, 1, 3, 4])
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/conv.py:717 in forward, code: return self._conv_forward(input, self.weight, self.bias)
    conv3d: "f32[13, 32, 1, 5, 5]" = torch.ops.aten.conv3d.default(permute, arg1_1, arg2_1, [2, 2, 2]);  permute = arg1_1 = arg2_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:177 in forward, code: embeddings = self.projection(pixel_values).flatten(2).transpose(1, 2)
    flatten: "f32[13, 32, 25]" = torch.ops.aten.flatten.using_ints(conv3d, 2);  conv3d = None
    transpose: "f32[13, 25, 32]" = torch.ops.aten.transpose.int(flatten, 1, 2);  flatten = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:115 in forward, code: embeddings = embeddings + self.position_embeddings.detach().type_as(embeddings).to(
    _tensor_constant0: "f32[1, 25, 32]" = self._tensor_constant0
    detach: "f32[1, 25, 32]" = torch.ops.aten.detach.default(_tensor_constant0);  _tensor_constant0 = None
    type_as: "f32[1, 25, 32]" = torch.ops.aten.type_as.default(detach, transpose);  detach = None
    to: "f32[1, 25, 32]" = torch.ops.aten.to.dtype_layout(type_as, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), copy = True);  type_as = None
    add: "f32[13, 25, 32]" = torch.ops.aten.add.Tensor(transpose, to);  transpose = to = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:122 in forward, code: embeddings = embeddings[~bool_masked_pos]
    bitwise_not: "b8[13, 25]" = torch.ops.aten.bitwise_not.default(arg69_1)
    index: "f32[13*u1, 32]" = torch.ops.aten.index.Tensor(add, [bitwise_not]);  add = bitwise_not = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:123 in forward, code: embeddings = embeddings.reshape(batch_size, -1, num_channels)
    reshape: "f32[13, u1, 32]" = torch.ops.aten.reshape.default(index, [13, -1, 32]);  index = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(
    layer_norm: "f32[13, u1, 32]" = torch.ops.aten.layer_norm.default(reshape, [32], arg14_1, arg15_1, 1e-12);  arg14_1 = arg15_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:241 in forward, code: k_bias = torch.zeros_like(self.v_bias, requires_grad=False) if self.q_bias is not None else None
    zeros_like: "f32[32]" = torch.ops.aten.zeros_like.default(arg4_1, pin_memory = False)
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:242 in forward, code: keys = nn.functional.linear(input=hidden_states, weight=self.key.weight, bias=k_bias)
    linear: "f32[13, u1, 32]" = torch.ops.aten.linear.default(layer_norm, arg6_1, zeros_like);  arg6_1 = zeros_like = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:243 in forward, code: values = nn.functional.linear(input=hidden_states, weight=self.value.weight, bias=self.v_bias)
    linear_1: "f32[13, u1, 32]" = torch.ops.aten.linear.default(layer_norm, arg7_1, arg4_1);  arg7_1 = arg4_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:244 in forward, code: queries = nn.functional.linear(input=hidden_states, weight=self.query.weight, bias=self.q_bias)
    linear_2: "f32[13, u1, 32]" = torch.ops.aten.linear.default(layer_norm, arg5_1, arg3_1);  layer_norm = arg5_1 = arg3_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:246 in forward, code: key_layer = keys.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)
    view: "f32[13, u1, 4, 8]" = torch.ops.aten.view.default(linear, [13, -1, 4, 8]);  linear = None
    transpose_1: "f32[13, 4, u1, 8]" = torch.ops.aten.transpose.int(view, 1, 2);  view = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:247 in forward, code: value_layer = values.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)
    view_1: "f32[13, u1, 4, 8]" = torch.ops.aten.view.default(linear_1, [13, -1, 4, 8]);  linear_1 = None
    transpose_2: "f32[13, 4, u1, 8]" = torch.ops.aten.transpose.int(view_1, 1, 2);  view_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:248 in forward, code: query_layer = queries.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)
    view_2: "f32[13, u1, 4, 8]" = torch.ops.aten.view.default(linear_2, [13, -1, 4, 8]);  linear_2 = None
    transpose_3: "f32[13, 4, u1, 8]" = torch.ops.aten.transpose.int(view_2, 1, 2)
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:254 in forward, code: context_layer, attention_probs = attention_interface(
    transpose_4: "f32[13, 4, 8, u1]" = torch.ops.aten.transpose.int(transpose_1, 2, 3);  transpose_1 = None
    matmul: "f32[13, 4, u1, u1]" = torch.ops.aten.matmul.default(transpose_3, transpose_4);  transpose_3 = transpose_4 = None
    mul: "f32[13, 4, u1, u1]" = torch.ops.aten.mul.Tensor(matmul, 0.3535533905932738);  matmul = None
    softmax: "f32[13, 4, u1, u1]" = torch.ops.aten.softmax.int(mul, -1);  mul = None
    dropout: "f32[13, 4, u1, u1]" = torch.ops.aten.dropout.default(softmax, 0.0, False);  softmax = None
    matmul_1: "f32[13, 4, u1, 8]" = torch.ops.aten.matmul.default(dropout, transpose_2);  dropout = transpose_2 = None
    transpose_5: "f32[13, u1, 4, 8]" = torch.ops.aten.transpose.int(matmul_1, 1, 2)
    sym_numel_default: "Sym(416*u1)" = torch.ops.aten.sym_numel.default(transpose_5)
    eq: "Sym(Eq(416*u1, 0))" = sym_numel_default == 0;  eq = None
    sym_stride_int: "Sym(8*Max(1, u1))" = torch.ops.aten.sym_stride.int(matmul_1, 1)
    ne: "Sym(Ne(8*Max(1, u1), 8))" = sym_stride_int != 8;  ne = None
    ne_1: "Sym(Ne(8*Max(1, u1), 8))" = sym_stride_int != 8;  ne_1 = None
    eq_1: "Sym(Eq(416*u1, 0))" = sym_numel_default == 0;  sym_numel_default = None
    eq_2: "Sym(Eq(8*Max(1, u1), 8))" = sym_stride_int == 8;  sym_stride_int = None
    or_: "Sym(Eq(8*Max(1, u1), 8))" = False | eq_2;  eq_2 = None
    and_: "Sym(Eq(8*Max(1, u1), 8))" = True & or_;  or_ = None
    sym_size_int: "Sym(u1)" = torch.ops.aten.sym_size.int(view_2, 1);  view_2 = None
    eq_3: "Sym(Eq(u1, 1))" = sym_size_int == 1
    or__1: "Sym(Eq(u1, 1))" = eq_3 | False;  eq_3 = None
    and__1: "Sym(Eq(u1, 1) & Eq(8*Max(1, u1), 8))" = and_ & or__1;  and_ = or__1 = None
    mul_1: "Sym(32*u1)" = 32 * sym_size_int
    sym_stride_int_1: "Sym(32*Max(1, u1))" = torch.ops.aten.sym_stride.int(matmul_1, 0);  matmul_1 = None
    eq_4: "Sym(Eq(32*Max(1, u1), 32*u1))" = sym_stride_int_1 == mul_1;  sym_stride_int_1 = None
    or__2: "Sym(Eq(32*Max(1, u1), 32*u1))" = False | eq_4;  eq_4 = None
    and__2: "Sym(Eq(u1, 1) & Eq(8*Max(1, u1), 8) & Eq(32*Max(1, u1), 32*u1))" = and__1 & or__2;  and__1 = or__2 = None
    mul_2: "Sym(416*u1)" = mul_1 * 13;  mul_1 = mul_2 = None
    or__3: "Sym(Eq(416*u1, 0) | (Eq(u1, 1) & Eq(8*Max(1, u1), 8) & Eq(32*Max(1, u1), 32*u1)))" = and__2 | eq_1;  and__2 = eq_1 = or__3 = None
    contiguous: "f32[13, u1, 4, 8]" = torch.ops.aten.contiguous.default(transpose_5);  transpose_5 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:266 in forward, code: context_layer = context_layer.reshape(new_context_layer_shape)
    reshape_1: "f32[13, u1, 32]" = torch.ops.aten.reshape.default(contiguous, [13, sym_size_int, 32]);  contiguous = sym_size_int = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_3: "f32[13, u1, 32]" = torch.ops.aten.linear.default(reshape_1, arg8_1, arg9_1);  reshape_1 = arg8_1 = arg9_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
    dropout_1: "f32[13, u1, 32]" = torch.ops.aten.dropout.default(linear_3, 0.1, False);  linear_3 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:351 in forward, code: hidden_states = attention_output + hidden_states
    add_1: "f32[13, u1, 32]" = torch.ops.aten.add.Tensor(dropout_1, reshape);  dropout_1 = reshape = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(
    layer_norm_1: "f32[13, u1, 32]" = torch.ops.aten.layer_norm.default(add_1, [32], arg16_1, arg17_1, 1e-12);  arg16_1 = arg17_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_4: "f32[13, u1, 37]" = torch.ops.aten.linear.default(layer_norm_1, arg10_1, arg11_1);  layer_norm_1 = arg10_1 = arg11_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/activations.py:89 in forward, code: return self.act(input)
    gelu: "f32[13, u1, 37]" = torch.ops.aten.gelu.default(linear_4);  linear_4 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_5: "f32[13, u1, 32]" = torch.ops.aten.linear.default(gelu, arg12_1, arg13_1);  gelu = arg12_1 = arg13_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
    dropout_2: "f32[13, u1, 32]" = torch.ops.aten.dropout.default(linear_5, 0.1, False);  linear_5 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:328 in forward, code: hidden_states = hidden_states + input_tensor
    add_2: "f32[13, u1, 32]" = torch.ops.aten.add.Tensor(dropout_2, add_1);  dropout_2 = add_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(
    layer_norm_2: "f32[13, u1, 32]" = torch.ops.aten.layer_norm.default(add_2, [32], arg29_1, arg30_1, 1e-12);  arg29_1 = arg30_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:241 in forward, code: k_bias = torch.zeros_like(self.v_bias, requires_grad=False) if self.q_bias is not None else None
    zeros_like_1: "f32[32]" = torch.ops.aten.zeros_like.default(arg19_1, pin_memory = False)
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:242 in forward, code: keys = nn.functional.linear(input=hidden_states, weight=self.key.weight, bias=k_bias)
    linear_6: "f32[13, u1, 32]" = torch.ops.aten.linear.default(layer_norm_2, arg21_1, zeros_like_1);  arg21_1 = zeros_like_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:243 in forward, code: values = nn.functional.linear(input=hidden_states, weight=self.value.weight, bias=self.v_bias)
    linear_7: "f32[13, u1, 32]" = torch.ops.aten.linear.default(layer_norm_2, arg22_1, arg19_1);  arg22_1 = arg19_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:244 in forward, code: queries = nn.functional.linear(input=hidden_states, weight=self.query.weight, bias=self.q_bias)
    linear_8: "f32[13, u1, 32]" = torch.ops.aten.linear.default(layer_norm_2, arg20_1, arg18_1);  layer_norm_2 = arg20_1 = arg18_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:246 in forward, code: key_layer = keys.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)
    view_3: "f32[13, u1, 4, 8]" = torch.ops.aten.view.default(linear_6, [13, -1, 4, 8]);  linear_6 = None
    transpose_6: "f32[13, 4, u1, 8]" = torch.ops.aten.transpose.int(view_3, 1, 2);  view_3 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:247 in forward, code: value_layer = values.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)
    view_4: "f32[13, u1, 4, 8]" = torch.ops.aten.view.default(linear_7, [13, -1, 4, 8]);  linear_7 = None
    transpose_7: "f32[13, 4, u1, 8]" = torch.ops.aten.transpose.int(view_4, 1, 2);  view_4 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:248 in forward, code: query_layer = queries.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)
    view_5: "f32[13, u1, 4, 8]" = torch.ops.aten.view.default(linear_8, [13, -1, 4, 8]);  linear_8 = None
    transpose_8: "f32[13, 4, u1, 8]" = torch.ops.aten.transpose.int(view_5, 1, 2)
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:254 in forward, code: context_layer, attention_probs = attention_interface(
    transpose_9: "f32[13, 4, 8, u1]" = torch.ops.aten.transpose.int(transpose_6, 2, 3);  transpose_6 = None
    matmul_2: "f32[13, 4, u1, u1]" = torch.ops.aten.matmul.default(transpose_8, transpose_9);  transpose_8 = transpose_9 = None
    mul_3: "f32[13, 4, u1, u1]" = torch.ops.aten.mul.Tensor(matmul_2, 0.3535533905932738);  matmul_2 = None
    softmax_1: "f32[13, 4, u1, u1]" = torch.ops.aten.softmax.int(mul_3, -1);  mul_3 = None
    dropout_3: "f32[13, 4, u1, u1]" = torch.ops.aten.dropout.default(softmax_1, 0.0, False);  softmax_1 = None
    matmul_3: "f32[13, 4, u1, 8]" = torch.ops.aten.matmul.default(dropout_3, transpose_7);  dropout_3 = transpose_7 = None
    transpose_10: "f32[13, u1, 4, 8]" = torch.ops.aten.transpose.int(matmul_3, 1, 2)
    sym_numel_default_1: "Sym(416*u1)" = torch.ops.aten.sym_numel.default(transpose_10)
    eq_5: "Sym(Eq(416*u1, 0))" = sym_numel_default_1 == 0;  eq_5 = None
    sym_stride_int_2: "Sym(8*Max(1, u1))" = torch.ops.aten.sym_stride.int(matmul_3, 1)
    ne_2: "Sym(Ne(8*Max(1, u1), 8))" = sym_stride_int_2 != 8;  ne_2 = None
    ne_3: "Sym(Ne(8*Max(1, u1), 8))" = sym_stride_int_2 != 8;  ne_3 = None
    eq_6: "Sym(Eq(416*u1, 0))" = sym_numel_default_1 == 0;  sym_numel_default_1 = None
    eq_7: "Sym(Eq(8*Max(1, u1), 8))" = sym_stride_int_2 == 8;  sym_stride_int_2 = None
    or__4: "Sym(Eq(8*Max(1, u1), 8))" = False | eq_7;  eq_7 = None
    and__3: "Sym(Eq(8*Max(1, u1), 8))" = True & or__4;  or__4 = None
    sym_size_int_1: "Sym(u1)" = torch.ops.aten.sym_size.int(view_5, 1);  view_5 = None
    eq_8: "Sym(Eq(u1, 1))" = sym_size_int_1 == 1
    or__5: "Sym(Eq(u1, 1))" = eq_8 | False;  eq_8 = None
    and__4: "Sym(Eq(u1, 1) & Eq(8*Max(1, u1), 8))" = and__3 & or__5;  and__3 = or__5 = None
    mul_4: "Sym(32*u1)" = 32 * sym_size_int_1
    sym_stride_int_3: "Sym(32*Max(1, u1))" = torch.ops.aten.sym_stride.int(matmul_3, 0);  matmul_3 = None
    eq_9: "Sym(Eq(32*Max(1, u1), 32*u1))" = sym_stride_int_3 == mul_4;  sym_stride_int_3 = None
    or__6: "Sym(Eq(32*Max(1, u1), 32*u1))" = False | eq_9;  eq_9 = None
    and__5: "Sym(Eq(u1, 1) & Eq(8*Max(1, u1), 8) & Eq(32*Max(1, u1), 32*u1))" = and__4 & or__6;  and__4 = or__6 = None
    mul_5: "Sym(416*u1)" = mul_4 * 13;  mul_4 = mul_5 = None
    or__7: "Sym(Eq(416*u1, 0) | (Eq(u1, 1) & Eq(8*Max(1, u1), 8) & Eq(32*Max(1, u1), 32*u1)))" = and__5 | eq_6;  and__5 = eq_6 = or__7 = None
    contiguous_1: "f32[13, u1, 4, 8]" = torch.ops.aten.contiguous.default(transpose_10);  transpose_10 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:266 in forward, code: context_layer = context_layer.reshape(new_context_layer_shape)
    reshape_2: "f32[13, u1, 32]" = torch.ops.aten.reshape.default(contiguous_1, [13, sym_size_int_1, 32]);  contiguous_1 = sym_size_int_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_9: "f32[13, u1, 32]" = torch.ops.aten.linear.default(reshape_2, arg23_1, arg24_1);  reshape_2 = arg23_1 = arg24_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
    dropout_4: "f32[13, u1, 32]" = torch.ops.aten.dropout.default(linear_9, 0.1, False);  linear_9 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:351 in forward, code: hidden_states = attention_output + hidden_states
    add_3: "f32[13, u1, 32]" = torch.ops.aten.add.Tensor(dropout_4, add_2);  dropout_4 = add_2 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(
    layer_norm_3: "f32[13, u1, 32]" = torch.ops.aten.layer_norm.default(add_3, [32], arg31_1, arg32_1, 1e-12);  arg31_1 = arg32_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_10: "f32[13, u1, 37]" = torch.ops.aten.linear.default(layer_norm_3, arg25_1, arg26_1);  layer_norm_3 = arg25_1 = arg26_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/activations.py:89 in forward, code: return self.act(input)
    gelu_1: "f32[13, u1, 37]" = torch.ops.aten.gelu.default(linear_10);  linear_10 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_11: "f32[13, u1, 32]" = torch.ops.aten.linear.default(gelu_1, arg27_1, arg28_1);  gelu_1 = arg27_1 = arg28_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
    dropout_5: "f32[13, u1, 32]" = torch.ops.aten.dropout.default(linear_11, 0.1, False);  linear_11 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:328 in forward, code: hidden_states = hidden_states + input_tensor
    add_4: "f32[13, u1, 32]" = torch.ops.aten.add.Tensor(dropout_5, add_3);  dropout_5 = add_3 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_12: "f32[13, u1, 32]" = torch.ops.aten.linear.default(add_4, arg33_1);  add_4 = arg33_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:585 in forward, code: expanded_position_embeddings = self.position_embeddings.expand(batch_size, -1, -1).type_as(pixel_values)
    _tensor_constant1: "f32[1, 25, 32]" = self._tensor_constant1
    expand: "f32[13, 25, 32]" = torch.ops.aten.expand.default(_tensor_constant1, [13, -1, -1]);  _tensor_constant1 = None
    type_as_1: "f32[13, 25, 32]" = torch.ops.aten.type_as.default(expand, arg68_1);  expand = arg68_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:586 in forward, code: expanded_position_embeddings = expanded_position_embeddings.detach().to(device=pixel_values.device, copy=True)
    detach_1: "f32[13, 25, 32]" = torch.ops.aten.detach.default(type_as_1);  type_as_1 = None
    to_1: "f32[13, 25, 32]" = torch.ops.aten.to.dtype_layout(detach_1, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), copy = True);  detach_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:587 in forward, code: pos_emb_visible = expanded_position_embeddings[~bool_masked_pos].reshape(batch_size, -1, num_channels)
    bitwise_not_1: "b8[13, 25]" = torch.ops.aten.bitwise_not.default(arg69_1)
    index_1: "f32[13*u3, 32]" = torch.ops.aten.index.Tensor(to_1, [bitwise_not_1]);  bitwise_not_1 = None
    reshape_3: "f32[13, u3, 32]" = torch.ops.aten.reshape.default(index_1, [13, -1, 32]);  index_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:588 in forward, code: pos_emb_mask = expanded_position_embeddings[bool_masked_pos].reshape(batch_size, -1, num_channels)
    index_2: "f32[13*u5, 32]" = torch.ops.aten.index.Tensor(to_1, [arg69_1]);  to_1 = arg69_1 = None
    reshape_4: "f32[13, u5, 32]" = torch.ops.aten.reshape.default(index_2, [13, -1, 32]);  index_2 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:591 in forward, code: x_full = torch.cat([sequence_output + pos_emb_visible, self.mask_token + pos_emb_mask], dim=1)
    add_5: "f32[13, u1, 32]" = torch.ops.aten.add.Tensor(linear_12, reshape_3);  linear_12 = reshape_3 = None
    add_6: "f32[13, u5, 32]" = torch.ops.aten.add.Tensor(arg0_1, reshape_4);  arg0_1 = None
    cat: "f32[13, u1 + u5, 32]" = torch.ops.aten.cat.default([add_5, add_6], 1);  add_5 = add_6 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(
    layer_norm_4: "f32[13, u1 + u5, 32]" = torch.ops.aten.layer_norm.default(cat, [32], arg45_1, arg46_1, 1e-12);  arg45_1 = arg46_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:241 in forward, code: k_bias = torch.zeros_like(self.v_bias, requires_grad=False) if self.q_bias is not None else None
    zeros_like_2: "f32[32]" = torch.ops.aten.zeros_like.default(arg35_1, pin_memory = False)
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:242 in forward, code: keys = nn.functional.linear(input=hidden_states, weight=self.key.weight, bias=k_bias)
    linear_13: "f32[13, u1 + u5, 32]" = torch.ops.aten.linear.default(layer_norm_4, arg37_1, zeros_like_2);  arg37_1 = zeros_like_2 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:243 in forward, code: values = nn.functional.linear(input=hidden_states, weight=self.value.weight, bias=self.v_bias)
    linear_14: "f32[13, u1 + u5, 32]" = torch.ops.aten.linear.default(layer_norm_4, arg38_1, arg35_1);  arg38_1 = arg35_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:244 in forward, code: queries = nn.functional.linear(input=hidden_states, weight=self.query.weight, bias=self.q_bias)
    linear_15: "f32[13, u1 + u5, 32]" = torch.ops.aten.linear.default(layer_norm_4, arg36_1, arg34_1);  layer_norm_4 = arg36_1 = arg34_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:246 in forward, code: key_layer = keys.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)
    view_6: "f32[13, u1 + u5, 4, 8]" = torch.ops.aten.view.default(linear_13, [13, -1, 4, 8]);  linear_13 = None
    transpose_11: "f32[13, 4, u1 + u5, 8]" = torch.ops.aten.transpose.int(view_6, 1, 2);  view_6 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:247 in forward, code: value_layer = values.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)
    view_7: "f32[13, u1 + u5, 4, 8]" = torch.ops.aten.view.default(linear_14, [13, -1, 4, 8]);  linear_14 = None
    transpose_12: "f32[13, 4, u1 + u5, 8]" = torch.ops.aten.transpose.int(view_7, 1, 2);  view_7 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:248 in forward, code: query_layer = queries.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)
    view_8: "f32[13, u1 + u5, 4, 8]" = torch.ops.aten.view.default(linear_15, [13, -1, 4, 8]);  linear_15 = None
    transpose_13: "f32[13, 4, u1 + u5, 8]" = torch.ops.aten.transpose.int(view_8, 1, 2)
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:254 in forward, code: context_layer, attention_probs = attention_interface(
    transpose_14: "f32[13, 4, 8, u1 + u5]" = torch.ops.aten.transpose.int(transpose_11, 2, 3);  transpose_11 = None
    matmul_4: "f32[13, 4, u1 + u5, u1 + u5]" = torch.ops.aten.matmul.default(transpose_13, transpose_14);  transpose_13 = transpose_14 = None
    mul_6: "f32[13, 4, u1 + u5, u1 + u5]" = torch.ops.aten.mul.Tensor(matmul_4, 0.3535533905932738);  matmul_4 = None
    softmax_2: "f32[13, 4, u1 + u5, u1 + u5]" = torch.ops.aten.softmax.int(mul_6, -1);  mul_6 = None
    dropout_6: "f32[13, 4, u1 + u5, u1 + u5]" = torch.ops.aten.dropout.default(softmax_2, 0.0, False);  softmax_2 = None
    matmul_5: "f32[13, 4, u1 + u5, 8]" = torch.ops.aten.matmul.default(dropout_6, transpose_12);  dropout_6 = transpose_12 = None
    transpose_15: "f32[13, u1 + u5, 4, 8]" = torch.ops.aten.transpose.int(matmul_5, 1, 2)
    sym_numel_default_2: "Sym(416*u1 + 416*u5)" = torch.ops.aten.sym_numel.default(transpose_15)
    eq_10: "Sym(Eq(416*u1 + 416*u5, 0))" = sym_numel_default_2 == 0;  eq_10 = None
    sym_stride_int_4: "Sym(8*Max(1, u1 + u5))" = torch.ops.aten.sym_stride.int(matmul_5, 1)
    ne_4: "Sym(Ne(8*Max(1, u1 + u5), 8))" = sym_stride_int_4 != 8;  ne_4 = None
    ne_5: "Sym(Ne(8*Max(1, u1 + u5), 8))" = sym_stride_int_4 != 8;  ne_5 = None
    eq_11: "Sym(Eq(416*u1 + 416*u5, 0))" = sym_numel_default_2 == 0;  sym_numel_default_2 = None
    eq_12: "Sym(Eq(8*Max(1, u1 + u5), 8))" = sym_stride_int_4 == 8;  sym_stride_int_4 = None
    or__8: "Sym(Eq(8*Max(1, u1 + u5), 8))" = False | eq_12;  eq_12 = None
    and__6: "Sym(Eq(8*Max(1, u1 + u5), 8))" = True & or__8;  or__8 = None
    sym_size_int_2: "Sym(u1 + u5)" = torch.ops.aten.sym_size.int(view_8, 1);  view_8 = None
    eq_13: "Sym(Eq(u1 + u5, 1))" = sym_size_int_2 == 1
    or__9: "Sym(Eq(u1 + u5, 1))" = eq_13 | False;  eq_13 = None
    and__7: "Sym(Eq(u1 + u5, 1) & Eq(8*Max(1, u1 + u5), 8))" = and__6 & or__9;  and__6 = or__9 = None
    mul_7: "Sym(32*u1 + 32*u5)" = 32 * sym_size_int_2
    sym_stride_int_5: "Sym(32*Max(1, u1 + u5))" = torch.ops.aten.sym_stride.int(matmul_5, 0);  matmul_5 = None
    eq_14: "Sym(Eq(32*Max(1, u1 + u5), 32*u1 + 32*u5))" = sym_stride_int_5 == mul_7;  sym_stride_int_5 = None
    or__10: "Sym(Eq(32*Max(1, u1 + u5), 32*u1 + 32*u5))" = False | eq_14;  eq_14 = None
    and__8: "Sym(Eq(u1 + u5, 1) & Eq(8*Max(1, u1 + u5), 8) & Eq(32*Max(1, u1 + u5), 32*u1 + 32*u5))" = and__7 & or__10;  and__7 = or__10 = None
    mul_8: "Sym(416*u1 + 416*u5)" = mul_7 * 13;  mul_7 = mul_8 = None
    or__11: "Sym(Eq(416*u1 + 416*u5, 0) | (Eq(u1 + u5, 1) & Eq(8*Max(1, u1 + u5), 8) & Eq(32*Max(1, u1 + u5), 32*u1 + 32*u5)))" = and__8 | eq_11;  and__8 = eq_11 = or__11 = None
    contiguous_2: "f32[13, u1 + u5, 4, 8]" = torch.ops.aten.contiguous.default(transpose_15);  transpose_15 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:266 in forward, code: context_layer = context_layer.reshape(new_context_layer_shape)
    reshape_5: "f32[13, u1 + u5, 32]" = torch.ops.aten.reshape.default(contiguous_2, [13, sym_size_int_2, 32]);  contiguous_2 = sym_size_int_2 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_16: "f32[13, u1 + u5, 32]" = torch.ops.aten.linear.default(reshape_5, arg39_1, arg40_1);  reshape_5 = arg39_1 = arg40_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
    dropout_7: "f32[13, u1 + u5, 32]" = torch.ops.aten.dropout.default(linear_16, 0.1, False);  linear_16 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:351 in forward, code: hidden_states = attention_output + hidden_states
    add_7: "f32[13, u1 + u5, 32]" = torch.ops.aten.add.Tensor(dropout_7, cat);  dropout_7 = cat = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(
    layer_norm_5: "f32[13, u1 + u5, 32]" = torch.ops.aten.layer_norm.default(add_7, [32], arg47_1, arg48_1, 1e-12);  arg47_1 = arg48_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_17: "f32[13, u1 + u5, 37]" = torch.ops.aten.linear.default(layer_norm_5, arg41_1, arg42_1);  layer_norm_5 = arg41_1 = arg42_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/activations.py:89 in forward, code: return self.act(input)
    gelu_2: "f32[13, u1 + u5, 37]" = torch.ops.aten.gelu.default(linear_17);  linear_17 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_18: "f32[13, u1 + u5, 32]" = torch.ops.aten.linear.default(gelu_2, arg43_1, arg44_1);  gelu_2 = arg43_1 = arg44_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
    dropout_8: "f32[13, u1 + u5, 32]" = torch.ops.aten.dropout.default(linear_18, 0.1, False);  linear_18 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:328 in forward, code: hidden_states = hidden_states + input_tensor
    add_8: "f32[13, u1 + u5, 32]" = torch.ops.aten.add.Tensor(dropout_8, add_7);  dropout_8 = add_7 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(
    layer_norm_6: "f32[13, u1 + u5, 32]" = torch.ops.aten.layer_norm.default(add_8, [32], arg60_1, arg61_1, 1e-12);  arg60_1 = arg61_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:241 in forward, code: k_bias = torch.zeros_like(self.v_bias, requires_grad=False) if self.q_bias is not None else None
    zeros_like_3: "f32[32]" = torch.ops.aten.zeros_like.default(arg50_1, pin_memory = False)
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:242 in forward, code: keys = nn.functional.linear(input=hidden_states, weight=self.key.weight, bias=k_bias)
    linear_19: "f32[13, u1 + u5, 32]" = torch.ops.aten.linear.default(layer_norm_6, arg52_1, zeros_like_3);  arg52_1 = zeros_like_3 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:243 in forward, code: values = nn.functional.linear(input=hidden_states, weight=self.value.weight, bias=self.v_bias)
    linear_20: "f32[13, u1 + u5, 32]" = torch.ops.aten.linear.default(layer_norm_6, arg53_1, arg50_1);  arg53_1 = arg50_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:244 in forward, code: queries = nn.functional.linear(input=hidden_states, weight=self.query.weight, bias=self.q_bias)
    linear_21: "f32[13, u1 + u5, 32]" = torch.ops.aten.linear.default(layer_norm_6, arg51_1, arg49_1);  layer_norm_6 = arg51_1 = arg49_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:246 in forward, code: key_layer = keys.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)
    view_9: "f32[13, u1 + u5, 4, 8]" = torch.ops.aten.view.default(linear_19, [13, -1, 4, 8]);  linear_19 = None
    transpose_16: "f32[13, 4, u1 + u5, 8]" = torch.ops.aten.transpose.int(view_9, 1, 2);  view_9 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:247 in forward, code: value_layer = values.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)
    view_10: "f32[13, u1 + u5, 4, 8]" = torch.ops.aten.view.default(linear_20, [13, -1, 4, 8]);  linear_20 = None
    transpose_17: "f32[13, 4, u1 + u5, 8]" = torch.ops.aten.transpose.int(view_10, 1, 2);  view_10 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:248 in forward, code: query_layer = queries.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)
    view_11: "f32[13, u1 + u5, 4, 8]" = torch.ops.aten.view.default(linear_21, [13, -1, 4, 8]);  linear_21 = None
    transpose_18: "f32[13, 4, u1 + u5, 8]" = torch.ops.aten.transpose.int(view_11, 1, 2)
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:254 in forward, code: context_layer, attention_probs = attention_interface(
    transpose_19: "f32[13, 4, 8, u1 + u5]" = torch.ops.aten.transpose.int(transpose_16, 2, 3);  transpose_16 = None
    matmul_6: "f32[13, 4, u1 + u5, u1 + u5]" = torch.ops.aten.matmul.default(transpose_18, transpose_19);  transpose_18 = transpose_19 = None
    mul_9: "f32[13, 4, u1 + u5, u1 + u5]" = torch.ops.aten.mul.Tensor(matmul_6, 0.3535533905932738);  matmul_6 = None
    softmax_3: "f32[13, 4, u1 + u5, u1 + u5]" = torch.ops.aten.softmax.int(mul_9, -1);  mul_9 = None
    dropout_9: "f32[13, 4, u1 + u5, u1 + u5]" = torch.ops.aten.dropout.default(softmax_3, 0.0, False);  softmax_3 = None
    matmul_7: "f32[13, 4, u1 + u5, 8]" = torch.ops.aten.matmul.default(dropout_9, transpose_17);  dropout_9 = transpose_17 = None
    transpose_20: "f32[13, u1 + u5, 4, 8]" = torch.ops.aten.transpose.int(matmul_7, 1, 2)
    sym_numel_default_3: "Sym(416*u1 + 416*u5)" = torch.ops.aten.sym_numel.default(transpose_20)
    eq_15: "Sym(Eq(416*u1 + 416*u5, 0))" = sym_numel_default_3 == 0;  eq_15 = None
    sym_stride_int_6: "Sym(8*Max(1, u1 + u5))" = torch.ops.aten.sym_stride.int(matmul_7, 1)
    ne_6: "Sym(Ne(8*Max(1, u1 + u5), 8))" = sym_stride_int_6 != 8;  ne_6 = None
    ne_7: "Sym(Ne(8*Max(1, u1 + u5), 8))" = sym_stride_int_6 != 8;  ne_7 = None
    eq_16: "Sym(Eq(416*u1 + 416*u5, 0))" = sym_numel_default_3 == 0;  sym_numel_default_3 = None
    eq_17: "Sym(Eq(8*Max(1, u1 + u5), 8))" = sym_stride_int_6 == 8;  sym_stride_int_6 = None
    or__12: "Sym(Eq(8*Max(1, u1 + u5), 8))" = False | eq_17;  eq_17 = None
    and__9: "Sym(Eq(8*Max(1, u1 + u5), 8))" = True & or__12;  or__12 = None
    sym_size_int_3: "Sym(u1 + u5)" = torch.ops.aten.sym_size.int(view_11, 1);  view_11 = None
    eq_18: "Sym(Eq(u1 + u5, 1))" = sym_size_int_3 == 1
    or__13: "Sym(Eq(u1 + u5, 1))" = eq_18 | False;  eq_18 = None
    and__10: "Sym(Eq(u1 + u5, 1) & Eq(8*Max(1, u1 + u5), 8))" = and__9 & or__13;  and__9 = or__13 = None
    mul_10: "Sym(32*u1 + 32*u5)" = 32 * sym_size_int_3
    sym_stride_int_7: "Sym(32*Max(1, u1 + u5))" = torch.ops.aten.sym_stride.int(matmul_7, 0);  matmul_7 = None
    eq_19: "Sym(Eq(32*Max(1, u1 + u5), 32*u1 + 32*u5))" = sym_stride_int_7 == mul_10;  sym_stride_int_7 = None
    or__14: "Sym(Eq(32*Max(1, u1 + u5), 32*u1 + 32*u5))" = False | eq_19;  eq_19 = None
    and__11: "Sym(Eq(u1 + u5, 1) & Eq(8*Max(1, u1 + u5), 8) & Eq(32*Max(1, u1 + u5), 32*u1 + 32*u5))" = and__10 & or__14;  and__10 = or__14 = None
    mul_11: "Sym(416*u1 + 416*u5)" = mul_10 * 13;  mul_10 = mul_11 = None
    or__15: "Sym(Eq(416*u1 + 416*u5, 0) | (Eq(u1 + u5, 1) & Eq(8*Max(1, u1 + u5), 8) & Eq(32*Max(1, u1 + u5), 32*u1 + 32*u5)))" = and__11 | eq_16;  and__11 = eq_16 = or__15 = None
    contiguous_3: "f32[13, u1 + u5, 4, 8]" = torch.ops.aten.contiguous.default(transpose_20);  transpose_20 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:266 in forward, code: context_layer = context_layer.reshape(new_context_layer_shape)
    reshape_6: "f32[13, u1 + u5, 32]" = torch.ops.aten.reshape.default(contiguous_3, [13, sym_size_int_3, 32]);  contiguous_3 = sym_size_int_3 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_22: "f32[13, u1 + u5, 32]" = torch.ops.aten.linear.default(reshape_6, arg54_1, arg55_1);  reshape_6 = arg54_1 = arg55_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
    dropout_10: "f32[13, u1 + u5, 32]" = torch.ops.aten.dropout.default(linear_22, 0.1, False);  linear_22 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:351 in forward, code: hidden_states = attention_output + hidden_states
    add_9: "f32[13, u1 + u5, 32]" = torch.ops.aten.add.Tensor(dropout_10, add_8);  dropout_10 = add_8 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(
    layer_norm_7: "f32[13, u1 + u5, 32]" = torch.ops.aten.layer_norm.default(add_9, [32], arg62_1, arg63_1, 1e-12);  arg62_1 = arg63_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_23: "f32[13, u1 + u5, 37]" = torch.ops.aten.linear.default(layer_norm_7, arg56_1, arg57_1);  layer_norm_7 = arg56_1 = arg57_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/activations.py:89 in forward, code: return self.act(input)
    gelu_3: "f32[13, u1 + u5, 37]" = torch.ops.aten.gelu.default(linear_23);  linear_23 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_24: "f32[13, u1 + u5, 32]" = torch.ops.aten.linear.default(gelu_3, arg58_1, arg59_1);  gelu_3 = arg58_1 = arg59_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
    dropout_11: "f32[13, u1 + u5, 32]" = torch.ops.aten.dropout.default(linear_24, 0.1, False);  linear_24 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:328 in forward, code: hidden_states = hidden_states + input_tensor
    add_10: "f32[13, u1 + u5, 32]" = torch.ops.aten.add.Tensor(dropout_11, add_9);  dropout_11 = add_9 = add_10 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:505 in forward, code: if return_token_num > 0:
    sym_size_int_4: "Sym(u5)" = torch.ops.aten.sym_size.int(reshape_4, 1);  reshape_4 = None
    gt: "Sym(u5 > 0)" = sym_size_int_4 > 0;  sym_size_int_4 = gt = None
    



def forward(self, arg0_1: "f32[1, 1, 32]", arg1_1: "f32[32, 3, 2, 2, 2]", arg2_1: "f32[32]", arg3_1: "f32[32]", arg4_1: "f32[32]", arg5_1: "f32[32, 32]", arg6_1: "f32[32, 32]", arg7_1: "f32[32, 32]", arg8_1: "f32[32, 32]", arg9_1: "f32[32]", arg10_1: "f32[37, 32]", arg11_1: "f32[37]", arg12_1: "f32[32, 37]", arg13_1: "f32[32]", arg14_1: "f32[32]", arg15_1: "f32[32]", arg16_1: "f32[32]", arg17_1: "f32[32]", arg18_1: "f32[32]", arg19_1: "f32[32]", arg20_1: "f32[32, 32]", arg21_1: "f32[32, 32]", arg22_1: "f32[32, 32]", arg23_1: "f32[32, 32]", arg24_1: "f32[32]", arg25_1: "f32[37, 32]", arg26_1: "f32[37]", arg27_1: "f32[32, 37]", arg28_1: "f32[32]", arg29_1: "f32[32]", arg30_1: "f32[32]", arg31_1: "f32[32]", arg32_1: "f32[32]", arg33_1: "f32[32, 32]", arg34_1: "f32[32]", arg35_1: "f32[32]", arg36_1: "f32[32, 32]", arg37_1: "f32[32, 32]", arg38_1: "f32[32, 32]", arg39_1: "f32[32, 32]", arg40_1: "f32[32]", arg41_1: "f32[37, 32]", arg42_1: "f32[37]", arg43_1: "f32[32, 37]", arg44_1: "f32[32]", arg45_1: "f32[32]", arg46_1: "f32[32]", arg47_1: "f32[32]", arg48_1: "f32[32]", arg49_1: "f32[32]", arg50_1: "f32[32]", arg51_1: "f32[32, 32]", arg52_1: "f32[32, 32]", arg53_1: "f32[32, 32]", arg54_1: "f32[32, 32]", arg55_1: "f32[32]", arg56_1: "f32[37, 32]", arg57_1: "f32[37]", arg58_1: "f32[32, 37]", arg59_1: "f32[32]", arg60_1: "f32[32]", arg61_1: "f32[32]", arg62_1: "f32[32]", arg63_1: "f32[32]", arg64_1: "f32[32]", arg65_1: "f32[32]", arg66_1: "f32[24, 32]", arg67_1: "f32[24]", arg68_1: "f32[13, 2, 3, 10, 10]", arg69_1: "b8[13, 25]"):
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:176 in forward, code: pixel_values = pixel_values.permute(0, 2, 1, 3, 4)
    permute: "f32[13, 3, 2, 10, 10]" = torch.ops.aten.permute.default(arg68_1, [0, 2, 1, 3, 4])
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/conv.py:717 in forward, code: return self._conv_forward(input, self.weight, self.bias)
    conv3d: "f32[13, 32, 1, 5, 5]" = torch.ops.aten.conv3d.default(permute, arg1_1, arg2_1, [2, 2, 2]);  permute = arg1_1 = arg2_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:177 in forward, code: embeddings = self.projection(pixel_values).flatten(2).transpose(1, 2)
    flatten: "f32[13, 32, 25]" = torch.ops.aten.flatten.using_ints(conv3d, 2);  conv3d = None
    transpose: "f32[13, 25, 32]" = torch.ops.aten.transpose.int(flatten, 1, 2);  flatten = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:115 in forward, code: embeddings = embeddings + self.position_embeddings.detach().type_as(embeddings).to(
    _tensor_constant0: "f32[1, 25, 32]" = self._tensor_constant0
    detach: "f32[1, 25, 32]" = torch.ops.aten.detach.default(_tensor_constant0);  _tensor_constant0 = None
    type_as: "f32[1, 25, 32]" = torch.ops.aten.type_as.default(detach, transpose);  detach = None
    to: "f32[1, 25, 32]" = torch.ops.aten.to.dtype_layout(type_as, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), copy = True);  type_as = None
    add: "f32[13, 25, 32]" = torch.ops.aten.add.Tensor(transpose, to);  transpose = to = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:122 in forward, code: embeddings = embeddings[~bool_masked_pos]
    bitwise_not: "b8[13, 25]" = torch.ops.aten.bitwise_not.default(arg69_1)
    index: "f32[13*u1, 32]" = torch.ops.aten.index.Tensor(add, [bitwise_not]);  add = bitwise_not = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:123 in forward, code: embeddings = embeddings.reshape(batch_size, -1, num_channels)
    reshape: "f32[13, u1, 32]" = torch.ops.aten.reshape.default(index, [13, -1, 32]);  index = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(
    layer_norm: "f32[13, u1, 32]" = torch.ops.aten.layer_norm.default(reshape, [32], arg14_1, arg15_1, 1e-12);  arg14_1 = arg15_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:241 in forward, code: k_bias = torch.zeros_like(self.v_bias, requires_grad=False) if self.q_bias is not None else None
    zeros_like: "f32[32]" = torch.ops.aten.zeros_like.default(arg4_1, pin_memory = False)
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:242 in forward, code: keys = nn.functional.linear(input=hidden_states, weight=self.key.weight, bias=k_bias)
    linear: "f32[13, u1, 32]" = torch.ops.aten.linear.default(layer_norm, arg6_1, zeros_like);  arg6_1 = zeros_like = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:243 in forward, code: values = nn.functional.linear(input=hidden_states, weight=self.value.weight, bias=self.v_bias)
    linear_1: "f32[13, u1, 32]" = torch.ops.aten.linear.default(layer_norm, arg7_1, arg4_1);  arg7_1 = arg4_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:244 in forward, code: queries = nn.functional.linear(input=hidden_states, weight=self.query.weight, bias=self.q_bias)
    linear_2: "f32[13, u1, 32]" = torch.ops.aten.linear.default(layer_norm, arg5_1, arg3_1);  layer_norm = arg5_1 = arg3_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:246 in forward, code: key_layer = keys.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)
    view: "f32[13, u1, 4, 8]" = torch.ops.aten.view.default(linear, [13, -1, 4, 8]);  linear = None
    transpose_1: "f32[13, 4, u1, 8]" = torch.ops.aten.transpose.int(view, 1, 2);  view = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:247 in forward, code: value_layer = values.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)
    view_1: "f32[13, u1, 4, 8]" = torch.ops.aten.view.default(linear_1, [13, -1, 4, 8]);  linear_1 = None
    transpose_2: "f32[13, 4, u1, 8]" = torch.ops.aten.transpose.int(view_1, 1, 2);  view_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:248 in forward, code: query_layer = queries.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)
    view_2: "f32[13, u1, 4, 8]" = torch.ops.aten.view.default(linear_2, [13, -1, 4, 8]);  linear_2 = None
    transpose_3: "f32[13, 4, u1, 8]" = torch.ops.aten.transpose.int(view_2, 1, 2)
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:254 in forward, code: context_layer, attention_probs = attention_interface(
    transpose_4: "f32[13, 4, 8, u1]" = torch.ops.aten.transpose.int(transpose_1, 2, 3);  transpose_1 = None
    matmul: "f32[13, 4, u1, u1]" = torch.ops.aten.matmul.default(transpose_3, transpose_4);  transpose_3 = transpose_4 = None
    mul: "f32[13, 4, u1, u1]" = torch.ops.aten.mul.Tensor(matmul, 0.3535533905932738);  matmul = None
    softmax: "f32[13, 4, u1, u1]" = torch.ops.aten.softmax.int(mul, -1);  mul = None
    dropout: "f32[13, 4, u1, u1]" = torch.ops.aten.dropout.default(softmax, 0.0, False);  softmax = None
    matmul_1: "f32[13, 4, u1, 8]" = torch.ops.aten.matmul.default(dropout, transpose_2);  dropout = transpose_2 = None
    transpose_5: "f32[13, u1, 4, 8]" = torch.ops.aten.transpose.int(matmul_1, 1, 2)
    sym_numel_default: "Sym(416*u1)" = torch.ops.aten.sym_numel.default(transpose_5)
    eq: "Sym(Eq(416*u1, 0))" = sym_numel_default == 0;  eq = None
    sym_stride_int: "Sym(8*Max(1, u1))" = torch.ops.aten.sym_stride.int(matmul_1, 1)
    ne: "Sym(Ne(8*Max(1, u1), 8))" = sym_stride_int != 8;  ne = None
    ne_1: "Sym(Ne(8*Max(1, u1), 8))" = sym_stride_int != 8;  ne_1 = None
    eq_1: "Sym(Eq(416*u1, 0))" = sym_numel_default == 0;  sym_numel_default = None
    eq_2: "Sym(Eq(8*Max(1, u1), 8))" = sym_stride_int == 8;  sym_stride_int = None
    or_: "Sym(Eq(8*Max(1, u1), 8))" = False | eq_2;  eq_2 = None
    and_: "Sym(Eq(8*Max(1, u1), 8))" = True & or_;  or_ = None
    sym_size_int: "Sym(u1)" = torch.ops.aten.sym_size.int(view_2, 1);  view_2 = None
    eq_3: "Sym(Eq(u1, 1))" = sym_size_int == 1
    or__1: "Sym(Eq(u1, 1))" = eq_3 | False;  eq_3 = None
    and__1: "Sym(Eq(u1, 1) & Eq(8*Max(1, u1), 8))" = and_ & or__1;  and_ = or__1 = None
    mul_1: "Sym(32*u1)" = 32 * sym_size_int
    sym_stride_int_1: "Sym(32*Max(1, u1))" = torch.ops.aten.sym_stride.int(matmul_1, 0);  matmul_1 = None
    eq_4: "Sym(Eq(32*Max(1, u1), 32*u1))" = sym_stride_int_1 == mul_1;  sym_stride_int_1 = None
    or__2: "Sym(Eq(32*Max(1, u1), 32*u1))" = False | eq_4;  eq_4 = None
    and__2: "Sym(Eq(u1, 1) & Eq(8*Max(1, u1), 8) & Eq(32*Max(1, u1), 32*u1))" = and__1 & or__2;  and__1 = or__2 = None
    mul_2: "Sym(416*u1)" = mul_1 * 13;  mul_1 = mul_2 = None
    or__3: "Sym(Eq(416*u1, 0) | (Eq(u1, 1) & Eq(8*Max(1, u1), 8) & Eq(32*Max(1, u1), 32*u1)))" = and__2 | eq_1;  and__2 = eq_1 = or__3 = None
    contiguous: "f32[13, u1, 4, 8]" = torch.ops.aten.contiguous.default(transpose_5);  transpose_5 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:266 in forward, code: context_layer = context_layer.reshape(new_context_layer_shape)
    reshape_1: "f32[13, u1, 32]" = torch.ops.aten.reshape.default(contiguous, [13, sym_size_int, 32]);  contiguous = sym_size_int = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_3: "f32[13, u1, 32]" = torch.ops.aten.linear.default(reshape_1, arg8_1, arg9_1);  reshape_1 = arg8_1 = arg9_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
    dropout_1: "f32[13, u1, 32]" = torch.ops.aten.dropout.default(linear_3, 0.1, False);  linear_3 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:351 in forward, code: hidden_states = attention_output + hidden_states
    add_1: "f32[13, u1, 32]" = torch.ops.aten.add.Tensor(dropout_1, reshape);  dropout_1 = reshape = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(
    layer_norm_1: "f32[13, u1, 32]" = torch.ops.aten.layer_norm.default(add_1, [32], arg16_1, arg17_1, 1e-12);  arg16_1 = arg17_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_4: "f32[13, u1, 37]" = torch.ops.aten.linear.default(layer_norm_1, arg10_1, arg11_1);  layer_norm_1 = arg10_1 = arg11_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/activations.py:89 in forward, code: return self.act(input)
    gelu: "f32[13, u1, 37]" = torch.ops.aten.gelu.default(linear_4);  linear_4 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_5: "f32[13, u1, 32]" = torch.ops.aten.linear.default(gelu, arg12_1, arg13_1);  gelu = arg12_1 = arg13_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
    dropout_2: "f32[13, u1, 32]" = torch.ops.aten.dropout.default(linear_5, 0.1, False);  linear_5 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:328 in forward, code: hidden_states = hidden_states + input_tensor
    add_2: "f32[13, u1, 32]" = torch.ops.aten.add.Tensor(dropout_2, add_1);  dropout_2 = add_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(
    layer_norm_2: "f32[13, u1, 32]" = torch.ops.aten.layer_norm.default(add_2, [32], arg29_1, arg30_1, 1e-12);  arg29_1 = arg30_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:241 in forward, code: k_bias = torch.zeros_like(self.v_bias, requires_grad=False) if self.q_bias is not None else None
    zeros_like_1: "f32[32]" = torch.ops.aten.zeros_like.default(arg19_1, pin_memory = False)
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:242 in forward, code: keys = nn.functional.linear(input=hidden_states, weight=self.key.weight, bias=k_bias)
    linear_6: "f32[13, u1, 32]" = torch.ops.aten.linear.default(layer_norm_2, arg21_1, zeros_like_1);  arg21_1 = zeros_like_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:243 in forward, code: values = nn.functional.linear(input=hidden_states, weight=self.value.weight, bias=self.v_bias)
    linear_7: "f32[13, u1, 32]" = torch.ops.aten.linear.default(layer_norm_2, arg22_1, arg19_1);  arg22_1 = arg19_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:244 in forward, code: queries = nn.functional.linear(input=hidden_states, weight=self.query.weight, bias=self.q_bias)
    linear_8: "f32[13, u1, 32]" = torch.ops.aten.linear.default(layer_norm_2, arg20_1, arg18_1);  layer_norm_2 = arg20_1 = arg18_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:246 in forward, code: key_layer = keys.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)
    view_3: "f32[13, u1, 4, 8]" = torch.ops.aten.view.default(linear_6, [13, -1, 4, 8]);  linear_6 = None
    transpose_6: "f32[13, 4, u1, 8]" = torch.ops.aten.transpose.int(view_3, 1, 2);  view_3 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:247 in forward, code: value_layer = values.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)
    view_4: "f32[13, u1, 4, 8]" = torch.ops.aten.view.default(linear_7, [13, -1, 4, 8]);  linear_7 = None
    transpose_7: "f32[13, 4, u1, 8]" = torch.ops.aten.transpose.int(view_4, 1, 2);  view_4 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:248 in forward, code: query_layer = queries.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)
    view_5: "f32[13, u1, 4, 8]" = torch.ops.aten.view.default(linear_8, [13, -1, 4, 8]);  linear_8 = None
    transpose_8: "f32[13, 4, u1, 8]" = torch.ops.aten.transpose.int(view_5, 1, 2)
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:254 in forward, code: context_layer, attention_probs = attention_interface(
    transpose_9: "f32[13, 4, 8, u1]" = torch.ops.aten.transpose.int(transpose_6, 2, 3);  transpose_6 = None
    matmul_2: "f32[13, 4, u1, u1]" = torch.ops.aten.matmul.default(transpose_8, transpose_9);  transpose_8 = transpose_9 = None
    mul_3: "f32[13, 4, u1, u1]" = torch.ops.aten.mul.Tensor(matmul_2, 0.3535533905932738);  matmul_2 = None
    softmax_1: "f32[13, 4, u1, u1]" = torch.ops.aten.softmax.int(mul_3, -1);  mul_3 = None
    dropout_3: "f32[13, 4, u1, u1]" = torch.ops.aten.dropout.default(softmax_1, 0.0, False);  softmax_1 = None
    matmul_3: "f32[13, 4, u1, 8]" = torch.ops.aten.matmul.default(dropout_3, transpose_7);  dropout_3 = transpose_7 = None
    transpose_10: "f32[13, u1, 4, 8]" = torch.ops.aten.transpose.int(matmul_3, 1, 2)
    sym_numel_default_1: "Sym(416*u1)" = torch.ops.aten.sym_numel.default(transpose_10)
    eq_5: "Sym(Eq(416*u1, 0))" = sym_numel_default_1 == 0;  eq_5 = None
    sym_stride_int_2: "Sym(8*Max(1, u1))" = torch.ops.aten.sym_stride.int(matmul_3, 1)
    ne_2: "Sym(Ne(8*Max(1, u1), 8))" = sym_stride_int_2 != 8;  ne_2 = None
    ne_3: "Sym(Ne(8*Max(1, u1), 8))" = sym_stride_int_2 != 8;  ne_3 = None
    eq_6: "Sym(Eq(416*u1, 0))" = sym_numel_default_1 == 0;  sym_numel_default_1 = None
    eq_7: "Sym(Eq(8*Max(1, u1), 8))" = sym_stride_int_2 == 8;  sym_stride_int_2 = None
    or__4: "Sym(Eq(8*Max(1, u1), 8))" = False | eq_7;  eq_7 = None
    and__3: "Sym(Eq(8*Max(1, u1), 8))" = True & or__4;  or__4 = None
    sym_size_int_1: "Sym(u1)" = torch.ops.aten.sym_size.int(view_5, 1);  view_5 = None
    eq_8: "Sym(Eq(u1, 1))" = sym_size_int_1 == 1
    or__5: "Sym(Eq(u1, 1))" = eq_8 | False;  eq_8 = None
    and__4: "Sym(Eq(u1, 1) & Eq(8*Max(1, u1), 8))" = and__3 & or__5;  and__3 = or__5 = None
    mul_4: "Sym(32*u1)" = 32 * sym_size_int_1
    sym_stride_int_3: "Sym(32*Max(1, u1))" = torch.ops.aten.sym_stride.int(matmul_3, 0);  matmul_3 = None
    eq_9: "Sym(Eq(32*Max(1, u1), 32*u1))" = sym_stride_int_3 == mul_4;  sym_stride_int_3 = None
    or__6: "Sym(Eq(32*Max(1, u1), 32*u1))" = False | eq_9;  eq_9 = None
    and__5: "Sym(Eq(u1, 1) & Eq(8*Max(1, u1), 8) & Eq(32*Max(1, u1), 32*u1))" = and__4 & or__6;  and__4 = or__6 = None
    mul_5: "Sym(416*u1)" = mul_4 * 13;  mul_4 = mul_5 = None
    or__7: "Sym(Eq(416*u1, 0) | (Eq(u1, 1) & Eq(8*Max(1, u1), 8) & Eq(32*Max(1, u1), 32*u1)))" = and__5 | eq_6;  and__5 = eq_6 = or__7 = None
    contiguous_1: "f32[13, u1, 4, 8]" = torch.ops.aten.contiguous.default(transpose_10);  transpose_10 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:266 in forward, code: context_layer = context_layer.reshape(new_context_layer_shape)
    reshape_2: "f32[13, u1, 32]" = torch.ops.aten.reshape.default(contiguous_1, [13, sym_size_int_1, 32]);  contiguous_1 = sym_size_int_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_9: "f32[13, u1, 32]" = torch.ops.aten.linear.default(reshape_2, arg23_1, arg24_1);  reshape_2 = arg23_1 = arg24_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
    dropout_4: "f32[13, u1, 32]" = torch.ops.aten.dropout.default(linear_9, 0.1, False);  linear_9 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:351 in forward, code: hidden_states = attention_output + hidden_states
    add_3: "f32[13, u1, 32]" = torch.ops.aten.add.Tensor(dropout_4, add_2);  dropout_4 = add_2 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(
    layer_norm_3: "f32[13, u1, 32]" = torch.ops.aten.layer_norm.default(add_3, [32], arg31_1, arg32_1, 1e-12);  arg31_1 = arg32_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_10: "f32[13, u1, 37]" = torch.ops.aten.linear.default(layer_norm_3, arg25_1, arg26_1);  layer_norm_3 = arg25_1 = arg26_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/activations.py:89 in forward, code: return self.act(input)
    gelu_1: "f32[13, u1, 37]" = torch.ops.aten.gelu.default(linear_10);  linear_10 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_11: "f32[13, u1, 32]" = torch.ops.aten.linear.default(gelu_1, arg27_1, arg28_1);  gelu_1 = arg27_1 = arg28_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
    dropout_5: "f32[13, u1, 32]" = torch.ops.aten.dropout.default(linear_11, 0.1, False);  linear_11 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:328 in forward, code: hidden_states = hidden_states + input_tensor
    add_4: "f32[13, u1, 32]" = torch.ops.aten.add.Tensor(dropout_5, add_3);  dropout_5 = add_3 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_12: "f32[13, u1, 32]" = torch.ops.aten.linear.default(add_4, arg33_1);  add_4 = arg33_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:585 in forward, code: expanded_position_embeddings = self.position_embeddings.expand(batch_size, -1, -1).type_as(pixel_values)
    _tensor_constant1: "f32[1, 25, 32]" = self._tensor_constant1
    expand: "f32[13, 25, 32]" = torch.ops.aten.expand.default(_tensor_constant1, [13, -1, -1]);  _tensor_constant1 = None
    type_as_1: "f32[13, 25, 32]" = torch.ops.aten.type_as.default(expand, arg68_1);  expand = arg68_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:586 in forward, code: expanded_position_embeddings = expanded_position_embeddings.detach().to(device=pixel_values.device, copy=True)
    detach_1: "f32[13, 25, 32]" = torch.ops.aten.detach.default(type_as_1);  type_as_1 = None
    to_1: "f32[13, 25, 32]" = torch.ops.aten.to.dtype_layout(detach_1, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), copy = True);  detach_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:587 in forward, code: pos_emb_visible = expanded_position_embeddings[~bool_masked_pos].reshape(batch_size, -1, num_channels)
    bitwise_not_1: "b8[13, 25]" = torch.ops.aten.bitwise_not.default(arg69_1)
    index_1: "f32[13*u3, 32]" = torch.ops.aten.index.Tensor(to_1, [bitwise_not_1]);  bitwise_not_1 = None
    reshape_3: "f32[13, u3, 32]" = torch.ops.aten.reshape.default(index_1, [13, -1, 32]);  index_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:588 in forward, code: pos_emb_mask = expanded_position_embeddings[bool_masked_pos].reshape(batch_size, -1, num_channels)
    index_2: "f32[13*u5, 32]" = torch.ops.aten.index.Tensor(to_1, [arg69_1]);  to_1 = arg69_1 = None
    reshape_4: "f32[13, u5, 32]" = torch.ops.aten.reshape.default(index_2, [13, -1, 32]);  index_2 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:591 in forward, code: x_full = torch.cat([sequence_output + pos_emb_visible, self.mask_token + pos_emb_mask], dim=1)
    add_5: "f32[13, u1, 32]" = torch.ops.aten.add.Tensor(linear_12, reshape_3);  linear_12 = reshape_3 = None
    add_6: "f32[13, u5, 32]" = torch.ops.aten.add.Tensor(arg0_1, reshape_4);  arg0_1 = None
    cat: "f32[13, u1 + u5, 32]" = torch.ops.aten.cat.default([add_5, add_6], 1);  add_5 = add_6 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(
    layer_norm_4: "f32[13, u1 + u5, 32]" = torch.ops.aten.layer_norm.default(cat, [32], arg45_1, arg46_1, 1e-12);  arg45_1 = arg46_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:241 in forward, code: k_bias = torch.zeros_like(self.v_bias, requires_grad=False) if self.q_bias is not None else None
    zeros_like_2: "f32[32]" = torch.ops.aten.zeros_like.default(arg35_1, pin_memory = False)
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:242 in forward, code: keys = nn.functional.linear(input=hidden_states, weight=self.key.weight, bias=k_bias)
    linear_13: "f32[13, u1 + u5, 32]" = torch.ops.aten.linear.default(layer_norm_4, arg37_1, zeros_like_2);  arg37_1 = zeros_like_2 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:243 in forward, code: values = nn.functional.linear(input=hidden_states, weight=self.value.weight, bias=self.v_bias)
    linear_14: "f32[13, u1 + u5, 32]" = torch.ops.aten.linear.default(layer_norm_4, arg38_1, arg35_1);  arg38_1 = arg35_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:244 in forward, code: queries = nn.functional.linear(input=hidden_states, weight=self.query.weight, bias=self.q_bias)
    linear_15: "f32[13, u1 + u5, 32]" = torch.ops.aten.linear.default(layer_norm_4, arg36_1, arg34_1);  layer_norm_4 = arg36_1 = arg34_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:246 in forward, code: key_layer = keys.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)
    view_6: "f32[13, u1 + u5, 4, 8]" = torch.ops.aten.view.default(linear_13, [13, -1, 4, 8]);  linear_13 = None
    transpose_11: "f32[13, 4, u1 + u5, 8]" = torch.ops.aten.transpose.int(view_6, 1, 2);  view_6 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:247 in forward, code: value_layer = values.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)
    view_7: "f32[13, u1 + u5, 4, 8]" = torch.ops.aten.view.default(linear_14, [13, -1, 4, 8]);  linear_14 = None
    transpose_12: "f32[13, 4, u1 + u5, 8]" = torch.ops.aten.transpose.int(view_7, 1, 2);  view_7 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:248 in forward, code: query_layer = queries.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)
    view_8: "f32[13, u1 + u5, 4, 8]" = torch.ops.aten.view.default(linear_15, [13, -1, 4, 8]);  linear_15 = None
    transpose_13: "f32[13, 4, u1 + u5, 8]" = torch.ops.aten.transpose.int(view_8, 1, 2)
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:254 in forward, code: context_layer, attention_probs = attention_interface(
    transpose_14: "f32[13, 4, 8, u1 + u5]" = torch.ops.aten.transpose.int(transpose_11, 2, 3);  transpose_11 = None
    matmul_4: "f32[13, 4, u1 + u5, u1 + u5]" = torch.ops.aten.matmul.default(transpose_13, transpose_14);  transpose_13 = transpose_14 = None
    mul_6: "f32[13, 4, u1 + u5, u1 + u5]" = torch.ops.aten.mul.Tensor(matmul_4, 0.3535533905932738);  matmul_4 = None
    softmax_2: "f32[13, 4, u1 + u5, u1 + u5]" = torch.ops.aten.softmax.int(mul_6, -1);  mul_6 = None
    dropout_6: "f32[13, 4, u1 + u5, u1 + u5]" = torch.ops.aten.dropout.default(softmax_2, 0.0, False);  softmax_2 = None
    matmul_5: "f32[13, 4, u1 + u5, 8]" = torch.ops.aten.matmul.default(dropout_6, transpose_12);  dropout_6 = transpose_12 = None
    transpose_15: "f32[13, u1 + u5, 4, 8]" = torch.ops.aten.transpose.int(matmul_5, 1, 2)
    sym_numel_default_2: "Sym(416*u1 + 416*u5)" = torch.ops.aten.sym_numel.default(transpose_15)
    eq_10: "Sym(Eq(416*u1 + 416*u5, 0))" = sym_numel_default_2 == 0;  eq_10 = None
    sym_stride_int_4: "Sym(8*Max(1, u1 + u5))" = torch.ops.aten.sym_stride.int(matmul_5, 1)
    ne_4: "Sym(Ne(8*Max(1, u1 + u5), 8))" = sym_stride_int_4 != 8;  ne_4 = None
    ne_5: "Sym(Ne(8*Max(1, u1 + u5), 8))" = sym_stride_int_4 != 8;  ne_5 = None
    eq_11: "Sym(Eq(416*u1 + 416*u5, 0))" = sym_numel_default_2 == 0;  sym_numel_default_2 = None
    eq_12: "Sym(Eq(8*Max(1, u1 + u5), 8))" = sym_stride_int_4 == 8;  sym_stride_int_4 = None
    or__8: "Sym(Eq(8*Max(1, u1 + u5), 8))" = False | eq_12;  eq_12 = None
    and__6: "Sym(Eq(8*Max(1, u1 + u5), 8))" = True & or__8;  or__8 = None
    sym_size_int_2: "Sym(u1 + u5)" = torch.ops.aten.sym_size.int(view_8, 1);  view_8 = None
    eq_13: "Sym(Eq(u1 + u5, 1))" = sym_size_int_2 == 1
    or__9: "Sym(Eq(u1 + u5, 1))" = eq_13 | False;  eq_13 = None
    and__7: "Sym(Eq(u1 + u5, 1) & Eq(8*Max(1, u1 + u5), 8))" = and__6 & or__9;  and__6 = or__9 = None
    mul_7: "Sym(32*u1 + 32*u5)" = 32 * sym_size_int_2
    sym_stride_int_5: "Sym(32*Max(1, u1 + u5))" = torch.ops.aten.sym_stride.int(matmul_5, 0);  matmul_5 = None
    eq_14: "Sym(Eq(32*Max(1, u1 + u5), 32*u1 + 32*u5))" = sym_stride_int_5 == mul_7;  sym_stride_int_5 = None
    or__10: "Sym(Eq(32*Max(1, u1 + u5), 32*u1 + 32*u5))" = False | eq_14;  eq_14 = None
    and__8: "Sym(Eq(u1 + u5, 1) & Eq(8*Max(1, u1 + u5), 8) & Eq(32*Max(1, u1 + u5), 32*u1 + 32*u5))" = and__7 & or__10;  and__7 = or__10 = None
    mul_8: "Sym(416*u1 + 416*u5)" = mul_7 * 13;  mul_7 = mul_8 = None
    or__11: "Sym(Eq(416*u1 + 416*u5, 0) | (Eq(u1 + u5, 1) & Eq(8*Max(1, u1 + u5), 8) & Eq(32*Max(1, u1 + u5), 32*u1 + 32*u5)))" = and__8 | eq_11;  and__8 = eq_11 = or__11 = None
    contiguous_2: "f32[13, u1 + u5, 4, 8]" = torch.ops.aten.contiguous.default(transpose_15);  transpose_15 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:266 in forward, code: context_layer = context_layer.reshape(new_context_layer_shape)
    reshape_5: "f32[13, u1 + u5, 32]" = torch.ops.aten.reshape.default(contiguous_2, [13, sym_size_int_2, 32]);  contiguous_2 = sym_size_int_2 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_16: "f32[13, u1 + u5, 32]" = torch.ops.aten.linear.default(reshape_5, arg39_1, arg40_1);  reshape_5 = arg39_1 = arg40_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
    dropout_7: "f32[13, u1 + u5, 32]" = torch.ops.aten.dropout.default(linear_16, 0.1, False);  linear_16 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:351 in forward, code: hidden_states = attention_output + hidden_states
    add_7: "f32[13, u1 + u5, 32]" = torch.ops.aten.add.Tensor(dropout_7, cat);  dropout_7 = cat = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(
    layer_norm_5: "f32[13, u1 + u5, 32]" = torch.ops.aten.layer_norm.default(add_7, [32], arg47_1, arg48_1, 1e-12);  arg47_1 = arg48_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_17: "f32[13, u1 + u5, 37]" = torch.ops.aten.linear.default(layer_norm_5, arg41_1, arg42_1);  layer_norm_5 = arg41_1 = arg42_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/activations.py:89 in forward, code: return self.act(input)
    gelu_2: "f32[13, u1 + u5, 37]" = torch.ops.aten.gelu.default(linear_17);  linear_17 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_18: "f32[13, u1 + u5, 32]" = torch.ops.aten.linear.default(gelu_2, arg43_1, arg44_1);  gelu_2 = arg43_1 = arg44_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
    dropout_8: "f32[13, u1 + u5, 32]" = torch.ops.aten.dropout.default(linear_18, 0.1, False);  linear_18 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:328 in forward, code: hidden_states = hidden_states + input_tensor
    add_8: "f32[13, u1 + u5, 32]" = torch.ops.aten.add.Tensor(dropout_8, add_7);  dropout_8 = add_7 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(
    layer_norm_6: "f32[13, u1 + u5, 32]" = torch.ops.aten.layer_norm.default(add_8, [32], arg60_1, arg61_1, 1e-12);  arg60_1 = arg61_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:241 in forward, code: k_bias = torch.zeros_like(self.v_bias, requires_grad=False) if self.q_bias is not None else None
    zeros_like_3: "f32[32]" = torch.ops.aten.zeros_like.default(arg50_1, pin_memory = False)
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:242 in forward, code: keys = nn.functional.linear(input=hidden_states, weight=self.key.weight, bias=k_bias)
    linear_19: "f32[13, u1 + u5, 32]" = torch.ops.aten.linear.default(layer_norm_6, arg52_1, zeros_like_3);  arg52_1 = zeros_like_3 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:243 in forward, code: values = nn.functional.linear(input=hidden_states, weight=self.value.weight, bias=self.v_bias)
    linear_20: "f32[13, u1 + u5, 32]" = torch.ops.aten.linear.default(layer_norm_6, arg53_1, arg50_1);  arg53_1 = arg50_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:244 in forward, code: queries = nn.functional.linear(input=hidden_states, weight=self.query.weight, bias=self.q_bias)
    linear_21: "f32[13, u1 + u5, 32]" = torch.ops.aten.linear.default(layer_norm_6, arg51_1, arg49_1);  layer_norm_6 = arg51_1 = arg49_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:246 in forward, code: key_layer = keys.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)
    view_9: "f32[13, u1 + u5, 4, 8]" = torch.ops.aten.view.default(linear_19, [13, -1, 4, 8]);  linear_19 = None
    transpose_16: "f32[13, 4, u1 + u5, 8]" = torch.ops.aten.transpose.int(view_9, 1, 2);  view_9 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:247 in forward, code: value_layer = values.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)
    view_10: "f32[13, u1 + u5, 4, 8]" = torch.ops.aten.view.default(linear_20, [13, -1, 4, 8]);  linear_20 = None
    transpose_17: "f32[13, 4, u1 + u5, 8]" = torch.ops.aten.transpose.int(view_10, 1, 2);  view_10 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:248 in forward, code: query_layer = queries.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)
    view_11: "f32[13, u1 + u5, 4, 8]" = torch.ops.aten.view.default(linear_21, [13, -1, 4, 8]);  linear_21 = None
    transpose_18: "f32[13, 4, u1 + u5, 8]" = torch.ops.aten.transpose.int(view_11, 1, 2)
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:254 in forward, code: context_layer, attention_probs = attention_interface(
    transpose_19: "f32[13, 4, 8, u1 + u5]" = torch.ops.aten.transpose.int(transpose_16, 2, 3);  transpose_16 = None
    matmul_6: "f32[13, 4, u1 + u5, u1 + u5]" = torch.ops.aten.matmul.default(transpose_18, transpose_19);  transpose_18 = transpose_19 = None
    mul_9: "f32[13, 4, u1 + u5, u1 + u5]" = torch.ops.aten.mul.Tensor(matmul_6, 0.3535533905932738);  matmul_6 = None
    softmax_3: "f32[13, 4, u1 + u5, u1 + u5]" = torch.ops.aten.softmax.int(mul_9, -1);  mul_9 = None
    dropout_9: "f32[13, 4, u1 + u5, u1 + u5]" = torch.ops.aten.dropout.default(softmax_3, 0.0, False);  softmax_3 = None
    matmul_7: "f32[13, 4, u1 + u5, 8]" = torch.ops.aten.matmul.default(dropout_9, transpose_17);  dropout_9 = transpose_17 = None
    transpose_20: "f32[13, u1 + u5, 4, 8]" = torch.ops.aten.transpose.int(matmul_7, 1, 2)
    sym_numel_default_3: "Sym(416*u1 + 416*u5)" = torch.ops.aten.sym_numel.default(transpose_20)
    eq_15: "Sym(Eq(416*u1 + 416*u5, 0))" = sym_numel_default_3 == 0;  eq_15 = None
    sym_stride_int_6: "Sym(8*Max(1, u1 + u5))" = torch.ops.aten.sym_stride.int(matmul_7, 1)
    ne_6: "Sym(Ne(8*Max(1, u1 + u5), 8))" = sym_stride_int_6 != 8;  ne_6 = None
    ne_7: "Sym(Ne(8*Max(1, u1 + u5), 8))" = sym_stride_int_6 != 8;  ne_7 = None
    eq_16: "Sym(Eq(416*u1 + 416*u5, 0))" = sym_numel_default_3 == 0;  sym_numel_default_3 = None
    eq_17: "Sym(Eq(8*Max(1, u1 + u5), 8))" = sym_stride_int_6 == 8;  sym_stride_int_6 = None
    or__12: "Sym(Eq(8*Max(1, u1 + u5), 8))" = False | eq_17;  eq_17 = None
    and__9: "Sym(Eq(8*Max(1, u1 + u5), 8))" = True & or__12;  or__12 = None
    sym_size_int_3: "Sym(u1 + u5)" = torch.ops.aten.sym_size.int(view_11, 1);  view_11 = None
    eq_18: "Sym(Eq(u1 + u5, 1))" = sym_size_int_3 == 1
    or__13: "Sym(Eq(u1 + u5, 1))" = eq_18 | False;  eq_18 = None
    and__10: "Sym(Eq(u1 + u5, 1) & Eq(8*Max(1, u1 + u5), 8))" = and__9 & or__13;  and__9 = or__13 = None
    mul_10: "Sym(32*u1 + 32*u5)" = 32 * sym_size_int_3
    sym_stride_int_7: "Sym(32*Max(1, u1 + u5))" = torch.ops.aten.sym_stride.int(matmul_7, 0);  matmul_7 = None
    eq_19: "Sym(Eq(32*Max(1, u1 + u5), 32*u1 + 32*u5))" = sym_stride_int_7 == mul_10;  sym_stride_int_7 = None
    or__14: "Sym(Eq(32*Max(1, u1 + u5), 32*u1 + 32*u5))" = False | eq_19;  eq_19 = None
    and__11: "Sym(Eq(u1 + u5, 1) & Eq(8*Max(1, u1 + u5), 8) & Eq(32*Max(1, u1 + u5), 32*u1 + 32*u5))" = and__10 & or__14;  and__10 = or__14 = None
    mul_11: "Sym(416*u1 + 416*u5)" = mul_10 * 13;  mul_10 = mul_11 = None
    or__15: "Sym(Eq(416*u1 + 416*u5, 0) | (Eq(u1 + u5, 1) & Eq(8*Max(1, u1 + u5), 8) & Eq(32*Max(1, u1 + u5), 32*u1 + 32*u5)))" = and__11 | eq_16;  and__11 = eq_16 = or__15 = None
    contiguous_3: "f32[13, u1 + u5, 4, 8]" = torch.ops.aten.contiguous.default(transpose_20);  transpose_20 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:266 in forward, code: context_layer = context_layer.reshape(new_context_layer_shape)
    reshape_6: "f32[13, u1 + u5, 32]" = torch.ops.aten.reshape.default(contiguous_3, [13, sym_size_int_3, 32]);  contiguous_3 = sym_size_int_3 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_22: "f32[13, u1 + u5, 32]" = torch.ops.aten.linear.default(reshape_6, arg54_1, arg55_1);  reshape_6 = arg54_1 = arg55_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
    dropout_10: "f32[13, u1 + u5, 32]" = torch.ops.aten.dropout.default(linear_22, 0.1, False);  linear_22 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:351 in forward, code: hidden_states = attention_output + hidden_states
    add_9: "f32[13, u1 + u5, 32]" = torch.ops.aten.add.Tensor(dropout_10, add_8);  dropout_10 = add_8 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(
    layer_norm_7: "f32[13, u1 + u5, 32]" = torch.ops.aten.layer_norm.default(add_9, [32], arg62_1, arg63_1, 1e-12);  arg62_1 = arg63_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_23: "f32[13, u1 + u5, 37]" = torch.ops.aten.linear.default(layer_norm_7, arg56_1, arg57_1);  layer_norm_7 = arg56_1 = arg57_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/activations.py:89 in forward, code: return self.act(input)
    gelu_3: "f32[13, u1 + u5, 37]" = torch.ops.aten.gelu.default(linear_23);  linear_23 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear_24: "f32[13, u1 + u5, 32]" = torch.ops.aten.linear.default(gelu_3, arg58_1, arg59_1);  gelu_3 = arg58_1 = arg59_1 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
    dropout_11: "f32[13, u1 + u5, 32]" = torch.ops.aten.dropout.default(linear_24, 0.1, False);  linear_24 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:328 in forward, code: hidden_states = hidden_states + input_tensor
    add_10: "f32[13, u1 + u5, 32]" = torch.ops.aten.add.Tensor(dropout_11, add_9);  dropout_11 = add_9 = add_10 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/videomae/modeling_videomae.py:505 in forward, code: if return_token_num > 0:
    sym_size_int_4: "Sym(u5)" = torch.ops.aten.sym_size.int(reshape_4, 1);  reshape_4 = None
    gt: "Sym(u5 > 0)" = sym_size_int_4 > 0;  sym_size_int_4 = gt = None
    
_______________________ ZambaModelTest.test_torch_export _______________________
[gw59] linux -- Python 3.10.12 /home/ilyas/transformers/.docker/bin/python3

self = <tests.models.zamba.test_modeling_zamba.ZambaModelTest testMethod=test_torch_export>
atol = 0.0001, rtol = 0.0001

    @slow
    @require_torch_greater_or_equal("2.5")
    @pytest.mark.torch_export_test
    def test_torch_export(self, atol=1e-4, rtol=1e-4):
        """
        Test if model can be exported with torch.export.export()
    
        Args:
            atol (`float`, *optional*, defaults to 1e-4): absolute tolerance for output comparison
            rtol (`float`, *optional*, defaults to 1e-4): relative tolerance for output comparison
        """
    
        exporter = DynamoExporter(export_config=DynamoConfig())
    
        for model_class in self.all_model_classes:
            with self.subTest(model_class.__name__):
                if hasattr(self.model_tester, "prepare_config_and_inputs_for_model_class"):
                    config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_model_class(model_class)
                else:
                    config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()
                inputs_dict = self._prepare_for_class(inputs_dict, model_class)
                model = model_class(config).eval().to(torch_device)
    
                # prepare cache inputs for auto-regressive models and include it for computing eager outputs
                # process output flags (e.g. use_cache, output_attentions, etc) to avoid passing them as inputs
                model, inputs_dict = prepare_for_export(model, inputs_dict)
    
                with torch.no_grad():
                    # Running the eager inference before the export to catch model/inputs comatibility issues, also sometimes after
                    # the export, the model used for export will return FakeTensors instead of real ones (torch cuda/inductor issue)
                    # This happens on cuda with (codegen, clvp, esm, gptj, levit, wav2vec2_bert and wav2vec2_conformer)
                    set_seed(1234)
                    eager_outputs = model(**copy.deepcopy(inputs_dict))
                    eager_outputs = get_leaf_tensors(eager_outputs)
                    self.assertTrue(eager_outputs, "Eager outputs is empty.")
    
                try:
>                   exported_program = exporter.export(model, inputs_dict)

tests/test_modeling_common.py:3532: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/exporters/exporter_dynamo.py:122: in export
    exported_program: ExportedProgram = torch.export.export(
.docker/lib/python3.10/site-packages/torch/export/__init__.py:311: in export
    raise e
.docker/lib/python3.10/site-packages/torch/export/__init__.py:277: in export
    return _export(
.docker/lib/python3.10/site-packages/torch/export/_trace.py:1163: in wrapper
    raise e
.docker/lib/python3.10/site-packages/torch/export/_trace.py:1129: in wrapper
    ep = fn(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/export/exported_program.py:124: in wrapper
    return fn(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/export/_trace.py:2255: in _export
    ep = _export_for_training(
.docker/lib/python3.10/site-packages/torch/export/_trace.py:1163: in wrapper
    raise e
.docker/lib/python3.10/site-packages/torch/export/_trace.py:1129: in wrapper
    ep = fn(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/export/exported_program.py:124: in wrapper
    return fn(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/export/_trace.py:2071: in _export_for_training
    export_artifact = export_func(
.docker/lib/python3.10/site-packages/torch/export/_trace.py:2002: in _non_strict_export
    aten_export_artifact = _to_aten_func(  # type: ignore[operator]
.docker/lib/python3.10/site-packages/torch/export/_trace.py:1793: in _export_to_aten_ir_make_fx
    gm, graph_signature = transform(_make_fx_helper)(
.docker/lib/python3.10/site-packages/torch/export/_trace.py:1922: in _aot_export_non_strict
    gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags)
.docker/lib/python3.10/site-packages/torch/export/_trace.py:1706: in _make_fx_helper
    gm = make_fx(
.docker/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:2429: in wrapped
    return make_fx_tracer.trace(f, *args)
.docker/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:2356: in trace
    return self._trace_inner(f, *args)
.docker/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:2318: in _trace_inner
    t = dispatch_trace(
.docker/lib/python3.10/site-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1303: in dispatch_trace
    graph = tracer.trace(root, concrete_args)  # type: ignore[arg-type]
.docker/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1908: in trace
    res = super().trace(root, concrete_args)
.docker/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:868: in trace
    (self.create_arg(fn(*args)),),
.docker/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1361: in wrapped
    out = f(*tensors)  # type:ignore[call-arg]
<string>:1: in <lambda>
    ???
.docker/lib/python3.10/site-packages/torch/export/_trace.py:1593: in wrapped_fn
    return tuple(flat_fn(*args))
.docker/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:187: in flat_fn
    tree_out = fn(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/graph_capture_wrappers.py:1354: in functional_call
    out = mod(*args[params_len:], **kwargs)
.docker/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:843: in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
.docker/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1997: in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
.docker/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:560: in call_module
    ret_val = forward(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:836: in forward
    return _orig_module_call(mod, *args, **kwargs)
.docker/lib/python3.10/site-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/export/_trace.py:1906: in forward
    tree_out = mod(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:843: in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
.docker/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1997: in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
.docker/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:560: in call_module
    ret_val = forward(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:836: in forward
    return _orig_module_call(mod, *args, **kwargs)
.docker/lib/python3.10/site-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
src/transformers/models/zamba/modeling_zamba.py:947: in forward
    layer_outputs = layer(
src/transformers/modeling_layers.py:94: in __call__
    return super().__call__(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:843: in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
.docker/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1997: in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
.docker/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:560: in call_module
    ret_val = forward(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:836: in forward
    return _orig_module_call(mod, *args, **kwargs)
.docker/lib/python3.10/site-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
src/transformers/models/zamba/modeling_zamba.py:691: in forward
    hidden_states = self.mamba(
.docker/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:843: in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
.docker/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1997: in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
.docker/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:560: in call_module
    ret_val = forward(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:836: in forward
    return _orig_module_call(mod, *args, **kwargs)
.docker/lib/python3.10/site-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
src/transformers/models/zamba/modeling_zamba.py:561: in forward
    return self.slow_forward(hidden_states, cache_params, attention_mask=attention_mask)
src/transformers/models/zamba/modeling_zamba.py:508: in slow_forward
    if attention_mask is not None and not torch.all(attention_mask == 1):
.docker/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1409: in __torch_function__
    return func(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1479: in __torch_function__
    return func(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/_export/non_strict_utils.py:1066: in __torch_function__
    return func(*args, **kwargs)
.docker/lib/python3.10/site-packages/torch/fx/experimental/sym_node.py:538: in guard_bool
    r = self.evaluate()
.docker/lib/python3.10/site-packages/torch/fx/experimental/sym_node.py:512: in evaluate
    return self.shape_env.evaluate_sym_node(self, size_oblivious)
.docker/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:7233: in evaluate_sym_node
    return self.evaluate_expr(
.docker/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:7333: in evaluate_expr
    return self._inner_evaluate_expr(
.docker/lib/python3.10/site-packages/torch/fx/experimental/recording.py:272: in wrapper
    return retlog(fn(*args, **kwargs))
.docker/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:7356: in _inner_evaluate_expr
    return self._evaluate_expr(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <torch.fx.experimental.symbolic_shapes.ShapeEnv object at 0x7f8a0d9b0a60>
orig_expr = Eq(u1, 1), hint = None, fx_node = False, size_oblivious = False
fallback_value = None

    def _evaluate_expr(
        self,
        orig_expr: sympy.Basic,
        hint: Optional[Union[bool, int, float]] = None,
        fx_node: Optional[torch.fx.Node] = None,
        size_oblivious: bool = False,
        fallback_value: Optional[bool] = None,
        *,
        forcing_spec: bool = False,
    ) -> sympy.Basic:
        # TODO: split conjunctions and evaluate them separately
    
        if isinstance(
            orig_expr,
            (sympy.logic.boolalg.BooleanTrue, sympy.logic.boolalg.BooleanFalse),
        ):
            return orig_expr
    
        # Don't track this one. (Because this cache is inside this function the
        # cache only lasts for the invocation of this function call)
        @functools.cache
        def compute_concrete_val() -> sympy.Basic:
            if hint is None:
                # This is only ever called for expressions WITHOUT unbacked
                # symbols
                r = self.size_hint(orig_expr)
                assert r is not None
                return r
            else:
                return sympy.sympify(hint)
    
        concrete_val: Optional[sympy.Basic]
    
        # Check if:
        #   1. 'translation_validation' is set
        #   2. the corresponding 'fx_node' is not 'None'
        #   3. the guard should not be suppressed
        #   4. the guard doesn't contain backed symfloat symbols
        #      since z3 can't handle floats
        #   5. fallback_value is none.
        # If all of the above check, we create an FX node representing the
        # actual expression to be guarded.
        node = None
        fresh = False
        if (
            self._translation_validation_enabled
            and fx_node is not None
            and not self._suppress_guards_tls()
            and not size_oblivious
            and not any(symbol_is_type(s, SymT.FLOAT) for s in orig_expr.free_symbols)
            and fallback_value is None
        ):
            # TODO: does this even worked with unbacked :think:
            concrete_val = compute_concrete_val()
            if concrete_val is sympy.true:
                node, fresh = self._create_fx_call_function(torch._assert, (fx_node,))
            elif concrete_val is sympy.false:
                neg, _ = self._create_fx_call_function(operator.not_, (fx_node,))
                node, fresh = self._create_fx_call_function(torch._assert, (neg,))
            else:
                eql, _ = self._create_fx_call_function(
                    operator.eq, (fx_node, concrete_val)
                )
                node, fresh = self._create_fx_call_function(torch._assert, (eql,))
    
            assert node is not None
            # If this is a fresh node, we have to remember the event index that
            # corresponds to this assertion node.
            # Reason: so that, given an assertion node, we can replay the ShapeEnv
            # events until the point where this assertion node was freshly created.
            if fresh:
                self._add_fx_node_metadata(node)
    
        # After creating the FX node corresponding to orig_expr, we must make sure that
        # no error will be raised until the end of this function.
        #
        # Reason: the translation validation may become invalid otherwise.
        #
        # If an error is raised before the end of this function, we remove the FX node
        # inserted, and re-raise the error.
        guard = None
    
        try:
            if orig_expr.is_number:
                self.log.debug("eval %s [trivial]", orig_expr)
                if hint is not None:
                    if isinstance(hint, bool):
                        assert orig_expr == hint, f"{orig_expr} != {hint}"
                    else:
                        assert sympy.Eq(orig_expr, hint), f"{orig_expr} != {hint}"
                return orig_expr
    
            expr = orig_expr
    
            static_expr = self._maybe_evaluate_static(
                expr, size_oblivious=size_oblivious
            )
            if static_expr is not None:
                self.log.debug(
                    "eval %s == %s [statically known]",
                    (
                        f"size_oblivious({orig_expr})"
                        if size_oblivious
                        else size_oblivious
                    ),
                    static_expr,
                )
                if (
                    not size_oblivious
                    and config.backed_size_oblivious
                    and hint is not None
                ):
                    # TODO: maybe reconcile this with use of counterfactual hints
                    # in unbacked case
                    assert static_expr == hint, f"{static_expr} != {hint}"
                return static_expr
    
            transmute_into_runtime_assert = False
    
            concrete_val = None
            if not (expr.free_symbols <= self.var_to_val.keys()):
                # TODO: dedupe this with _maybe_evaluate_static
                # Attempt to eliminate the unbacked SymInt
                new_expr = self._maybe_evaluate_static(expr, unbacked_only=True)
                assert new_expr is not None
                if not (new_expr.free_symbols <= self.var_to_val.keys()):
                    ok = False
    
                    # fallback_value is set when guard_or_true or guard_or_false are used.
                    if not ok and fallback_value is not None:
                        self._log_suppressed_dde(orig_expr, fallback_value)
                        return fallback_value
    
                    # oblivious_var_to_val will be defined iff we have sizes with DimDynamic.OBLIVIOUS_SIZE type.
                    # See https://github.com/pytorch/pytorch/issues/137100#issuecomment-2495778113
                    if (
                        self.oblivious_var_to_val
                        and not (
                            correct_hint := orig_expr.xreplace(
                                self.oblivious_var_to_val
                            )
                        ).free_symbols
                        and not (
                            counterfactual_hint := orig_expr.xreplace(
                                {
                                    k: max(2, v)
                                    for k, v in self.oblivious_var_to_val.items()
                                }
                            )
                        ).free_symbols
                        and correct_hint == counterfactual_hint
                    ):
                        # TODO: better logging
                        log.info(
                            "oblivious_size %s -> %s (passed counterfactual)",
                            orig_expr,
                            correct_hint,
                        )
                        concrete_val = correct_hint
                        # NB: do NOT transmute into runtime assert
                        ok = True
    
                    # unbacked_var_to_val is not None iff propagate_real_tensors is on.
                    # if propagate_real_tensors is on, we check the example values to generate (unsound_result)
                    # and if they pass we add a runtime assertions and continue.
                    if (
                        not ok
                        and self.unbacked_var_to_val
                        and not (
                            unsound_result := orig_expr.xreplace(
                                self.unbacked_var_to_val
                            ).xreplace(self.var_to_val)
                        ).free_symbols
                    ):
                        self._log_real_tensor_propagation(orig_expr, unsound_result)
                        transmute_into_runtime_assert = True
                        concrete_val = unsound_result
                        ok = True
    
                    # Check if this is coming from a python assert statement, if so, convert it to a runtime assertion
                    # instead of failing.
                    if not ok and self.trace_asserts and self._is_python_assert():
                        concrete_val = sympy.true
                        transmute_into_runtime_assert = True
                        ok = True
    
                    if not ok:
>                       raise self._make_data_dependent_error(
                            expr.xreplace(self.var_to_val),
                            expr,
                            expr_sym_node_id=self._expr_sym_node_id,
                        )
E                       torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode: Could not guard on data-dependent expression Eq(u1, 1) (unhinted: Eq(u1, 1)).  (Size-like symbols: none)
E                       
E                       consider using data-dependent friendly APIs such as guard_or_false, guard_or_true and statically_known_trueCaused by: (_export/non_strict_utils.py:1066 in __torch_function__)
E                       For more information, run with TORCH_LOGS="dynamic"
E                       For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="u1"
E                       If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
E                       For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing
E                       
E                       For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
E                       
E                       The following call raised this error:
E                         File "/home/ilyas/transformers/src/transformers/models/zamba/modeling_zamba.py", line 508, in slow_forward
E                           if attention_mask is not None and not torch.all(attention_mask == 1):
E                       
E                       
E                       The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.

.docker/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:7574: GuardOnDataDependentSymNode
----------------------------- Captured stderr call -----------------------------
The fast path is not available because one of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)` is None. To install follow https://github.com/state-spaces/mamba/#installation and https://github.com/Dao-AILab/causal-conv1d. If you want to use the naive implementation, set `use_mamba_kernels=False` in the model config
Zamba requires an initialized `ZambaHybridDynamicCache` to return a cache. None was provided, so no cache will be returned.
W1121 07:11:35.511000 24079 .docker/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:7942] Unable to find user code corresponding to {u1}



def forward(self, arg0_1: "f32[99, 64]", arg1_1: "f32[2, 64, 64]", arg2_1: "f32[2, 64, 32]", arg3_1: "f32[2, 64]", arg4_1: "f32[2, 64, 16]", arg5_1: "f32[2, 64]", arg6_1: "f32[128, 1, 4]", arg7_1: "f32[128]", arg8_1: "f32[256, 64]", arg9_1: "f32[64, 128]", arg10_1: "f32[64]", arg11_1: "f32[2, 64, 64]", arg12_1: "f32[2, 64, 32]", arg13_1: "f32[2, 64]", arg14_1: "f32[2, 64, 16]", arg15_1: "f32[2, 64]", arg16_1: "f32[128, 1, 4]", arg17_1: "f32[128]", arg18_1: "f32[256, 64]", arg19_1: "f32[64, 128]", arg20_1: "f32[64]", arg21_1: "f32[128, 128]", arg22_1: "f32[128, 128]", arg23_1: "f32[128, 128]", arg24_1: "f32[64, 128]", arg25_1: "f32[37, 64]", arg26_1: "f32[37, 64]", arg27_1: "f32[64, 37]", arg28_1: "f32[128]", arg29_1: "f32[64]", arg30_1: "f32[64, 64]", arg31_1: "f32[2, 64, 64]", arg32_1: "f32[2, 64, 32]", arg33_1: "f32[2, 64]", arg34_1: "f32[2, 64, 16]", arg35_1: "f32[2, 64]", arg36_1: "f32[128, 1, 4]", arg37_1: "f32[128]", arg38_1: "f32[256, 64]", arg39_1: "f32[64, 128]", arg40_1: "f32[64]", arg41_1: "f32[2, 64, 64]", arg42_1: "f32[2, 64, 32]", arg43_1: "f32[2, 64]", arg44_1: "f32[2, 64, 16]", arg45_1: "f32[2, 64]", arg46_1: "f32[128, 1, 4]", arg47_1: "f32[128]", arg48_1: "f32[256, 64]", arg49_1: "f32[64, 128]", arg50_1: "f32[64]", arg51_1: "f32[128, 128]", arg52_1: "f32[128, 128]", arg53_1: "f32[128, 128]", arg54_1: "f32[64, 128]", arg55_1: "f32[37, 64]", arg56_1: "f32[37, 64]", arg57_1: "f32[64, 37]", arg58_1: "f32[128]", arg59_1: "f32[64]", arg60_1: "f32[64, 64]", arg61_1: "f32[2, 64, 64]", arg62_1: "f32[2, 64, 32]", arg63_1: "f32[2, 64]", arg64_1: "f32[2, 64, 16]", arg65_1: "f32[2, 64]", arg66_1: "f32[128, 1, 4]", arg67_1: "f32[128]", arg68_1: "f32[256, 64]", arg69_1: "f32[64, 128]", arg70_1: "f32[64]", arg71_1: "f32[64]", arg72_1: "i64[13, 7]", arg73_1: "i64[13, 7]"):
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: "f32[13, 7, 64]" = torch.ops.aten.embedding.default(arg0_1, arg72_1, 0);  arg0_1 = arg72_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/zamba/modeling_zamba.py:924 in forward, code: original_hidden_states = torch.clone(inputs_embeds)
    clone: "f32[13, 7, 64]" = torch.ops.aten.clone.default(embedding);  clone = None
    
     # File: /home/ilyas/transformers/src/transformers/models/zamba/modeling_zamba.py:934 in forward, code: cache_position = torch.arange(hidden_states.shape[1], device=hidden_states.device)
    arange: "i64[7]" = torch.ops.aten.arange.default(7, device = device(type='cuda', index=0), pin_memory = False)
    
     # File: /home/ilyas/transformers/src/transformers/models/zamba/modeling_zamba.py:936 in forward, code: position_ids = cache_position.unsqueeze(0)
    unsqueeze: "i64[1, 7]" = torch.ops.aten.unsqueeze.default(arange, 0);  unsqueeze = None
    
     # File: /home/ilyas/transformers/src/transformers/models/zamba/modeling_zamba.py:938 in forward, code: causal_mask = self._update_causal_mask(attention_mask, inputs_embeds, cache_position)
    select: "i64[]" = torch.ops.aten.select.int(arange, 0, -1)
    add: "i64[]" = torch.ops.aten.add.Tensor(select, 1);  select = None
    item: "Sym(u0)" = torch.ops.aten.item.default(add)
    full: "f32[7, u0]" = torch.ops.aten.full.default([7, item], -3.4028234663852886e+38, dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
    triu: "f32[7, u0]" = torch.ops.aten.triu.default(full, 1);  full = None
    item_1: "Sym(u0)" = torch.ops.aten.item.default(add);  add = item_1 = None
    arange_1: "i64[u0]" = torch.ops.aten.arange.default(item, device = device(type='cuda', index=0), pin_memory = False);  item = None
    reshape: "i64[7, 1]" = torch.ops.aten.reshape.default(arange, [-1, 1]);  arange = None
    gt: "b8[7, u0]" = torch.ops.aten.gt.Tensor(arange_1, reshape);  arange_1 = reshape = None
    to: "b8[7, u0]" = torch.ops.aten.to.dtype(gt, torch.bool);  gt = None
    mul_: "f32[7, u0]" = torch.ops.aten.mul_.Tensor(triu, to);  to = None
    unsqueeze_1: "f32[1, 7, u0]" = torch.ops.aten.unsqueeze.default(mul_, 0);  mul_ = None
    unsqueeze_2: "f32[1, 1, 7, u0]" = torch.ops.aten.unsqueeze.default(unsqueeze_1, 1);  unsqueeze_1 = None
    sym_size_int: "Sym(u0)" = torch.ops.aten.sym_size.int(triu, 1);  triu = None
    le: "Sym(u0 <= 9223372036854775807)" = sym_size_int <= 9223372036854775807;  sym_size_int = le = None
    slice_1: "f32[1, 1, 7, u0]" = torch.ops.aten.slice.Tensor(unsqueeze_2, 3, 0, 9223372036854775807);  unsqueeze_2 = None
    expand: "f32[13, 1, 7, u0]" = torch.ops.aten.expand.default(slice_1, [13, 1, -1, -1])
    clone_1: "f32[13, 1, 7, u0]" = torch.ops.aten.clone.default(expand);  expand = None
    sym_size_int_1: "Sym(u0)" = torch.ops.aten.sym_size.int(slice_1, 3);  slice_1 = None
    ge: "Sym(u0 >= 7)" = sym_size_int_1 >= 7;  ge = None
    le_1: "Sym(u0 <= 7)" = sym_size_int_1 <= 7;  le_1 = None
    slice_2: "f32[13, 1, 7, 7]" = torch.ops.aten.slice.Tensor(clone_1, 3, 0, 7)
    eq: "b8[13, 1, 7, 7]" = torch.ops.aten.eq.Scalar(slice_2, 0.0);  slice_2 = None
    unsqueeze_3: "i64[13, 1, 7]" = torch.ops.aten.unsqueeze.default(arg73_1, 1)
    unsqueeze_4: "i64[13, 1, 1, 7]" = torch.ops.aten.unsqueeze.default(unsqueeze_3, 2);  unsqueeze_3 = None
    eq_1: "b8[13, 1, 1, 7]" = torch.ops.aten.eq.Scalar(unsqueeze_4, 0.0);  unsqueeze_4 = None
    mul: "b8[13, 1, 7, 7]" = torch.ops.aten.mul.Tensor(eq, eq_1);  eq = eq_1 = None
    le_2: "Sym(u0 <= 7)" = sym_size_int_1 <= 7;  le_2 = None
    slice_3: "f32[13, 1, 7, 7]" = torch.ops.aten.slice.Tensor(clone_1, 3, 0, 7)
    masked_fill: "f32[13, 1, 7, 7]" = torch.ops.aten.masked_fill.Scalar(slice_3, mul, -3.4028234663852886e+38);  slice_3 = mul = None
    le_3: "Sym(u0 <= 7)" = sym_size_int_1 <= 7;  sym_size_int_1 = le_3 = None
    slice_4: "f32[13, 1, 7, 7]" = torch.ops.aten.slice.Tensor(clone_1, 3, 0, 7);  clone_1 = None
    copy_: "f32[13, 1, 7, 7]" = torch.ops.aten.copy_.default(slice_4, masked_fill);  slice_4 = masked_fill = copy_ = None
    
     # File: /home/ilyas/transformers/src/transformers/models/zamba/modeling_zamba.py:75 in forward, code: hidden_states = hidden_states.to(torch.float32)
    to_1: "f32[13, 7, 64]" = torch.ops.aten.to.dtype(embedding, torch.float32);  embedding = None
    
     # File: /home/ilyas/transformers/src/transformers/models/zamba/modeling_zamba.py:76 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
    pow_1: "f32[13, 7, 64]" = torch.ops.aten.pow.Tensor_Scalar(to_1, 2)
    mean: "f32[13, 7, 1]" = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/zamba/modeling_zamba.py:77 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
    add_1: "f32[13, 7, 1]" = torch.ops.aten.add.Tensor(mean, 1e-05);  mean = None
    rsqrt: "f32[13, 7, 1]" = torch.ops.aten.rsqrt.default(add_1);  add_1 = None
    mul_1: "f32[13, 7, 64]" = torch.ops.aten.mul.Tensor(to_1, rsqrt);  to_1 = rsqrt = None
    
     # File: /home/ilyas/transformers/src/transformers/models/zamba/modeling_zamba.py:78 in forward, code: return self.weight * hidden_states.to(input_dtype)
    to_2: "f32[13, 7, 64]" = torch.ops.aten.to.dtype(mul_1, torch.float32);  mul_1 = None
    mul_2: "f32[13, 7, 64]" = torch.ops.aten.mul.Tensor(arg10_1, to_2);  arg10_1 = to_2 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear: "f32[13, 7, 256]" = torch.ops.aten.linear.default(mul_2, arg8_1);  mul_2 = arg8_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/zamba/modeling_zamba.py:561 in forward, code: return self.slow_forward(hidden_states, cache_params, attention_mask=attention_mask)
    transpose: "f32[13, 256, 7]" = torch.ops.aten.transpose.int(linear, 1, 2);  linear = None
    view: "f32[13, 128, 2, 7]" = torch.ops.aten.view.default(transpose, [13, -1, 2, 7]);  transpose = None
    chunk = torch.ops.aten.chunk.default(view, 2, 2);  view = None
    getitem: "f32[13, 128, 1, 7]" = chunk[0]
    getitem_1: "f32[13, 128, 1, 7]" = chunk[1];  chunk = None
    squeeze: "f32[13, 128, 7]" = torch.ops.aten.squeeze.dim(getitem, 2);  getitem = None
    contiguous: "f32[13, 128, 7]" = torch.ops.aten.contiguous.default(squeeze);  squeeze = contiguous = None
    squeeze_1: "f32[13, 128, 7]" = torch.ops.aten.squeeze.dim(getitem_1, 2);  getitem_1 = None
    reshape_1: "f32[13, 2, 64, 7]" = torch.ops.aten.reshape.default(squeeze_1, [13, 2, -1, 7]);  squeeze_1 = None
    transpose_1: "f32[2, 13, 64, 7]" = torch.ops.aten.transpose.int(reshape_1, 0, 1);  reshape_1 = transpose_1 = None
    zeros: "f32[13, 2, 64, 16]" = torch.ops.aten.zeros.default([13, 2, 64, 16], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False);  zeros = None
    eq_2: "b8[13, 7]" = torch.ops.aten.eq.Scalar(arg73_1, 1);  arg73_1 = None
    all_1: "b8[]" = torch.ops.aten.all.default(eq_2);  eq_2 = None
    ne: "b8[]" = torch.ops.aten.ne.Scalar(all_1, 0);  all_1 = None
    item_2: "Sym(Eq(u1, 1))" = torch.ops.aten.item.default(ne);  ne = item_2 = None
    



def forward(self, arg0_1: "f32[99, 64]", arg1_1: "f32[2, 64, 64]", arg2_1: "f32[2, 64, 32]", arg3_1: "f32[2, 64]", arg4_1: "f32[2, 64, 16]", arg5_1: "f32[2, 64]", arg6_1: "f32[128, 1, 4]", arg7_1: "f32[128]", arg8_1: "f32[256, 64]", arg9_1: "f32[64, 128]", arg10_1: "f32[64]", arg11_1: "f32[2, 64, 64]", arg12_1: "f32[2, 64, 32]", arg13_1: "f32[2, 64]", arg14_1: "f32[2, 64, 16]", arg15_1: "f32[2, 64]", arg16_1: "f32[128, 1, 4]", arg17_1: "f32[128]", arg18_1: "f32[256, 64]", arg19_1: "f32[64, 128]", arg20_1: "f32[64]", arg21_1: "f32[128, 128]", arg22_1: "f32[128, 128]", arg23_1: "f32[128, 128]", arg24_1: "f32[64, 128]", arg25_1: "f32[37, 64]", arg26_1: "f32[37, 64]", arg27_1: "f32[64, 37]", arg28_1: "f32[128]", arg29_1: "f32[64]", arg30_1: "f32[64, 64]", arg31_1: "f32[2, 64, 64]", arg32_1: "f32[2, 64, 32]", arg33_1: "f32[2, 64]", arg34_1: "f32[2, 64, 16]", arg35_1: "f32[2, 64]", arg36_1: "f32[128, 1, 4]", arg37_1: "f32[128]", arg38_1: "f32[256, 64]", arg39_1: "f32[64, 128]", arg40_1: "f32[64]", arg41_1: "f32[2, 64, 64]", arg42_1: "f32[2, 64, 32]", arg43_1: "f32[2, 64]", arg44_1: "f32[2, 64, 16]", arg45_1: "f32[2, 64]", arg46_1: "f32[128, 1, 4]", arg47_1: "f32[128]", arg48_1: "f32[256, 64]", arg49_1: "f32[64, 128]", arg50_1: "f32[64]", arg51_1: "f32[128, 128]", arg52_1: "f32[128, 128]", arg53_1: "f32[128, 128]", arg54_1: "f32[64, 128]", arg55_1: "f32[37, 64]", arg56_1: "f32[37, 64]", arg57_1: "f32[64, 37]", arg58_1: "f32[128]", arg59_1: "f32[64]", arg60_1: "f32[64, 64]", arg61_1: "f32[2, 64, 64]", arg62_1: "f32[2, 64, 32]", arg63_1: "f32[2, 64]", arg64_1: "f32[2, 64, 16]", arg65_1: "f32[2, 64]", arg66_1: "f32[128, 1, 4]", arg67_1: "f32[128]", arg68_1: "f32[256, 64]", arg69_1: "f32[64, 128]", arg70_1: "f32[64]", arg71_1: "f32[64]", arg72_1: "i64[13, 7]", arg73_1: "i64[13, 7]"):
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(
    embedding: "f32[13, 7, 64]" = torch.ops.aten.embedding.default(arg0_1, arg72_1, 0);  arg0_1 = arg72_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/zamba/modeling_zamba.py:924 in forward, code: original_hidden_states = torch.clone(inputs_embeds)
    clone: "f32[13, 7, 64]" = torch.ops.aten.clone.default(embedding);  clone = None
    
     # File: /home/ilyas/transformers/src/transformers/models/zamba/modeling_zamba.py:934 in forward, code: cache_position = torch.arange(hidden_states.shape[1], device=hidden_states.device)
    arange: "i64[7]" = torch.ops.aten.arange.default(7, device = device(type='cuda', index=0), pin_memory = False)
    
     # File: /home/ilyas/transformers/src/transformers/models/zamba/modeling_zamba.py:936 in forward, code: position_ids = cache_position.unsqueeze(0)
    unsqueeze: "i64[1, 7]" = torch.ops.aten.unsqueeze.default(arange, 0);  unsqueeze = None
    
     # File: /home/ilyas/transformers/src/transformers/models/zamba/modeling_zamba.py:938 in forward, code: causal_mask = self._update_causal_mask(attention_mask, inputs_embeds, cache_position)
    select: "i64[]" = torch.ops.aten.select.int(arange, 0, -1)
    add: "i64[]" = torch.ops.aten.add.Tensor(select, 1);  select = None
    item: "Sym(u0)" = torch.ops.aten.item.default(add)
    full: "f32[7, u0]" = torch.ops.aten.full.default([7, item], -3.4028234663852886e+38, dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
    triu: "f32[7, u0]" = torch.ops.aten.triu.default(full, 1);  full = None
    item_1: "Sym(u0)" = torch.ops.aten.item.default(add);  add = item_1 = None
    arange_1: "i64[u0]" = torch.ops.aten.arange.default(item, device = device(type='cuda', index=0), pin_memory = False);  item = None
    reshape: "i64[7, 1]" = torch.ops.aten.reshape.default(arange, [-1, 1]);  arange = None
    gt: "b8[7, u0]" = torch.ops.aten.gt.Tensor(arange_1, reshape);  arange_1 = reshape = None
    to: "b8[7, u0]" = torch.ops.aten.to.dtype(gt, torch.bool);  gt = None
    mul_: "f32[7, u0]" = torch.ops.aten.mul_.Tensor(triu, to);  to = None
    unsqueeze_1: "f32[1, 7, u0]" = torch.ops.aten.unsqueeze.default(mul_, 0);  mul_ = None
    unsqueeze_2: "f32[1, 1, 7, u0]" = torch.ops.aten.unsqueeze.default(unsqueeze_1, 1);  unsqueeze_1 = None
    sym_size_int: "Sym(u0)" = torch.ops.aten.sym_size.int(triu, 1);  triu = None
    le: "Sym(u0 <= 9223372036854775807)" = sym_size_int <= 9223372036854775807;  sym_size_int = le = None
    slice_1: "f32[1, 1, 7, u0]" = torch.ops.aten.slice.Tensor(unsqueeze_2, 3, 0, 9223372036854775807);  unsqueeze_2 = None
    expand: "f32[13, 1, 7, u0]" = torch.ops.aten.expand.default(slice_1, [13, 1, -1, -1])
    clone_1: "f32[13, 1, 7, u0]" = torch.ops.aten.clone.default(expand);  expand = None
    sym_size_int_1: "Sym(u0)" = torch.ops.aten.sym_size.int(slice_1, 3);  slice_1 = None
    ge: "Sym(u0 >= 7)" = sym_size_int_1 >= 7;  ge = None
    le_1: "Sym(u0 <= 7)" = sym_size_int_1 <= 7;  le_1 = None
    slice_2: "f32[13, 1, 7, 7]" = torch.ops.aten.slice.Tensor(clone_1, 3, 0, 7)
    eq: "b8[13, 1, 7, 7]" = torch.ops.aten.eq.Scalar(slice_2, 0.0);  slice_2 = None
    unsqueeze_3: "i64[13, 1, 7]" = torch.ops.aten.unsqueeze.default(arg73_1, 1)
    unsqueeze_4: "i64[13, 1, 1, 7]" = torch.ops.aten.unsqueeze.default(unsqueeze_3, 2);  unsqueeze_3 = None
    eq_1: "b8[13, 1, 1, 7]" = torch.ops.aten.eq.Scalar(unsqueeze_4, 0.0);  unsqueeze_4 = None
    mul: "b8[13, 1, 7, 7]" = torch.ops.aten.mul.Tensor(eq, eq_1);  eq = eq_1 = None
    le_2: "Sym(u0 <= 7)" = sym_size_int_1 <= 7;  le_2 = None
    slice_3: "f32[13, 1, 7, 7]" = torch.ops.aten.slice.Tensor(clone_1, 3, 0, 7)
    masked_fill: "f32[13, 1, 7, 7]" = torch.ops.aten.masked_fill.Scalar(slice_3, mul, -3.4028234663852886e+38);  slice_3 = mul = None
    le_3: "Sym(u0 <= 7)" = sym_size_int_1 <= 7;  sym_size_int_1 = le_3 = None
    slice_4: "f32[13, 1, 7, 7]" = torch.ops.aten.slice.Tensor(clone_1, 3, 0, 7);  clone_1 = None
    copy_: "f32[13, 1, 7, 7]" = torch.ops.aten.copy_.default(slice_4, masked_fill);  slice_4 = masked_fill = copy_ = None
    
     # File: /home/ilyas/transformers/src/transformers/models/zamba/modeling_zamba.py:75 in forward, code: hidden_states = hidden_states.to(torch.float32)
    to_1: "f32[13, 7, 64]" = torch.ops.aten.to.dtype(embedding, torch.float32);  embedding = None
    
     # File: /home/ilyas/transformers/src/transformers/models/zamba/modeling_zamba.py:76 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
    pow_1: "f32[13, 7, 64]" = torch.ops.aten.pow.Tensor_Scalar(to_1, 2)
    mean: "f32[13, 7, 1]" = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/zamba/modeling_zamba.py:77 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
    add_1: "f32[13, 7, 1]" = torch.ops.aten.add.Tensor(mean, 1e-05);  mean = None
    rsqrt: "f32[13, 7, 1]" = torch.ops.aten.rsqrt.default(add_1);  add_1 = None
    mul_1: "f32[13, 7, 64]" = torch.ops.aten.mul.Tensor(to_1, rsqrt);  to_1 = rsqrt = None
    
     # File: /home/ilyas/transformers/src/transformers/models/zamba/modeling_zamba.py:78 in forward, code: return self.weight * hidden_states.to(input_dtype)
    to_2: "f32[13, 7, 64]" = torch.ops.aten.to.dtype(mul_1, torch.float32);  mul_1 = None
    mul_2: "f32[13, 7, 64]" = torch.ops.aten.mul.Tensor(arg10_1, to_2);  arg10_1 = to_2 = None
    
     # File: /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)
    linear: "f32[13, 7, 256]" = torch.ops.aten.linear.default(mul_2, arg8_1);  mul_2 = arg8_1 = None
    
     # File: /home/ilyas/transformers/src/transformers/models/zamba/modeling_zamba.py:561 in forward, code: return self.slow_forward(hidden_states, cache_params, attention_mask=attention_mask)
    transpose: "f32[13, 256, 7]" = torch.ops.aten.transpose.int(linear, 1, 2);  linear = None
    view: "f32[13, 128, 2, 7]" = torch.ops.aten.view.default(transpose, [13, -1, 2, 7]);  transpose = None
    chunk = torch.ops.aten.chunk.default(view, 2, 2);  view = None
    getitem: "f32[13, 128, 1, 7]" = chunk[0]
    getitem_1: "f32[13, 128, 1, 7]" = chunk[1];  chunk = None
    squeeze: "f32[13, 128, 7]" = torch.ops.aten.squeeze.dim(getitem, 2);  getitem = None
    contiguous: "f32[13, 128, 7]" = torch.ops.aten.contiguous.default(squeeze);  squeeze = contiguous = None
    squeeze_1: "f32[13, 128, 7]" = torch.ops.aten.squeeze.dim(getitem_1, 2);  getitem_1 = None
    reshape_1: "f32[13, 2, 64, 7]" = torch.ops.aten.reshape.default(squeeze_1, [13, 2, -1, 7]);  squeeze_1 = None
    transpose_1: "f32[2, 13, 64, 7]" = torch.ops.aten.transpose.int(reshape_1, 0, 1);  reshape_1 = transpose_1 = None
    zeros: "f32[13, 2, 64, 16]" = torch.ops.aten.zeros.default([13, 2, 64, 16], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False);  zeros = None
    eq_2: "b8[13, 7]" = torch.ops.aten.eq.Scalar(arg73_1, 1);  arg73_1 = None
    all_1: "b8[]" = torch.ops.aten.all.default(eq_2);  eq_2 = None
    ne: "b8[]" = torch.ops.aten.ne.Scalar(all_1, 0);  all_1 = None
    item_2: "Sym(Eq(u1, 1))" = torch.ops.aten.item.default(ne);  ne = item_2 = None
    
___________________ TimmBackboneModelTest.test_torch_export ____________________
[gw56] linux -- Python 3.10.12 /home/ilyas/transformers/.docker/bin/python3

response = <Response [429 Too Many Requests]>, endpoint_name = None

    def hf_raise_for_status(response: httpx.Response, endpoint_name: Optional[str] = None) -> None:
        """
        Internal version of `response.raise_for_status()` that will refine a potential HTTPError.
        Raised exception will be an instance of [`~errors.HfHubHTTPError`].
    
        This helper is meant to be the unique method to raise_for_status when making a call to the Hugging Face Hub.
    
        Args:
            response (`Response`):
                Response from the server.
            endpoint_name (`str`, *optional*):
                Name of the endpoint that has been called. If provided, the error message will be more complete.
    
        > [!WARNING]
        > Raises when the request has failed:
        >
        >     - [`~utils.RepositoryNotFoundError`]
        >         If the repository to download from cannot be found. This may be because it
        >         doesn't exist, because `repo_type` is not set correctly, or because the repo
        >         is `private` and you do not have access.
        >     - [`~utils.GatedRepoError`]
        >         If the repository exists but is gated and the user is not on the authorized
        >         list.
        >     - [`~utils.RevisionNotFoundError`]
        >         If the repository exists but the revision couldn't be found.
        >     - [`~utils.EntryNotFoundError`]
        >         If the repository exists but the entry (e.g. the requested file) couldn't be
        >         find.
        >     - [`~utils.BadRequestError`]
        >         If request failed with a HTTP 400 BadRequest error.
        >     - [`~utils.HfHubHTTPError`]
        >         If request failed for a reason not listed above.
        """
        try:
>           response.raise_for_status()

.docker/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:553: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Response [429 Too Many Requests]>

    def raise_for_status(self) -> Response:
        """
        Raise the `HTTPStatusError` if one occurred.
        """
        request = self._request
        if request is None:
            raise RuntimeError(
                "Cannot call `raise_for_status` as the request "
                "instance has not been set on this response."
            )
    
        if self.is_success:
            return self
    
        if self.has_redirect_location:
            message = (
                "{error_type} '{0.status_code} {0.reason_phrase}' for url '{0.url}'\n"
                "Redirect location: '{0.headers[location]}'\n"
                "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/{0.status_code}"
            )
        else:
            message = (
                "{error_type} '{0.status_code} {0.reason_phrase}' for url '{0.url}'\n"
                "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/{0.status_code}"
            )
    
        status_class = self.status_code // 100
        error_types = {
            1: "Informational response",
            3: "Redirect response",
            4: "Client error",
            5: "Server error",
        }
        error_type = error_types.get(status_class, "Invalid status code")
        message = message.format(self, error_type=error_type)
>       raise HTTPStatusError(message, request=request, response=self)
E       httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://huggingface.co/api/models/timm/resnet18.a1_in1k/xet-read-token/491b427b45c94c7fb0e78b5474cc919aff584bbf'
E       For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

.docker/lib/python3.10/site-packages/httpx/_models.py:763: HTTPStatusError

The above exception was the direct cause of the following exception:

self = <tests.models.timm_backbone.test_modeling_timm_backbone.TimmBackboneModelTest testMethod=test_torch_export>
atol = 0.0001, rtol = 0.0001

    @slow
    @require_torch_greater_or_equal("2.5")
    @pytest.mark.torch_export_test
    def test_torch_export(self, atol=1e-4, rtol=1e-4):
        """
        Test if model can be exported with torch.export.export()
    
        Args:
            atol (`float`, *optional*, defaults to 1e-4): absolute tolerance for output comparison
            rtol (`float`, *optional*, defaults to 1e-4): relative tolerance for output comparison
        """
    
        exporter = DynamoExporter(export_config=DynamoConfig())
    
        for model_class in self.all_model_classes:
            with self.subTest(model_class.__name__):
                if hasattr(self.model_tester, "prepare_config_and_inputs_for_model_class"):
                    config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_model_class(model_class)
                else:
                    config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()
                inputs_dict = self._prepare_for_class(inputs_dict, model_class)
>               model = model_class(config).eval().to(torch_device)

tests/test_modeling_common.py:3516: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/transformers/models/timm_backbone/modeling_timm_backbone.py:65: in __init__
    self._backbone = timm.create_model(
.docker/lib/python3.10/site-packages/timm/models/_factory.py:138: in create_model
    model = create_fn(
.docker/lib/python3.10/site-packages/timm/models/resnet.py:1444: in resnet18
    return _create_resnet('resnet18', pretrained, **dict(model_args, **kwargs))
.docker/lib/python3.10/site-packages/timm/models/resnet.py:740: in _create_resnet
    return build_model_with_cfg(ResNet, variant, pretrained, **kwargs)
.docker/lib/python3.10/site-packages/timm/models/_builder.py:457: in build_model_with_cfg
    load_pretrained(
.docker/lib/python3.10/site-packages/timm/models/_builder.py:226: in load_pretrained
    state_dict = load_state_dict_from_hf(pretrained_loc, weights_only=True, cache_dir=cache_dir)
.docker/lib/python3.10/site-packages/timm/models/_hub.py:229: in load_state_dict_from_hf
    cached_safe_file = hf_hub_download(
.docker/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:89: in _inner_fn
    return fn(*args, **kwargs)
.docker/lib/python3.10/site-packages/huggingface_hub/file_download.py:1007: in hf_hub_download
    return _hf_hub_download_to_cache_dir(
.docker/lib/python3.10/site-packages/huggingface_hub/file_download.py:1200: in _hf_hub_download_to_cache_dir
    _download_to_tmp_and_move(
.docker/lib/python3.10/site-packages/huggingface_hub/file_download.py:1791: in _download_to_tmp_and_move
    xet_get(
.docker/lib/python3.10/site-packages/huggingface_hub/file_download.py:535: in xet_get
    connection_info = refresh_xet_connection_info(file_data=xet_file_data, headers=headers)
.docker/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:89: in _inner_fn
    return fn(*args, **kwargs)
.docker/lib/python3.10/site-packages/huggingface_hub/utils/_xet.py:122: in refresh_xet_connection_info
    return _fetch_xet_connection_info_with_url(file_data.refresh_route, headers)
.docker/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:89: in _inner_fn
    return fn(*args, **kwargs)
.docker/lib/python3.10/site-packages/huggingface_hub/utils/_xet.py:204: in _fetch_xet_connection_info_with_url
    hf_raise_for_status(resp)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

response = <Response [429 Too Many Requests]>, endpoint_name = None

    def hf_raise_for_status(response: httpx.Response, endpoint_name: Optional[str] = None) -> None:
        """
        Internal version of `response.raise_for_status()` that will refine a potential HTTPError.
        Raised exception will be an instance of [`~errors.HfHubHTTPError`].
    
        This helper is meant to be the unique method to raise_for_status when making a call to the Hugging Face Hub.
    
        Args:
            response (`Response`):
                Response from the server.
            endpoint_name (`str`, *optional*):
                Name of the endpoint that has been called. If provided, the error message will be more complete.
    
        > [!WARNING]
        > Raises when the request has failed:
        >
        >     - [`~utils.RepositoryNotFoundError`]
        >         If the repository to download from cannot be found. This may be because it
        >         doesn't exist, because `repo_type` is not set correctly, or because the repo
        >         is `private` and you do not have access.
        >     - [`~utils.GatedRepoError`]
        >         If the repository exists but is gated and the user is not on the authorized
        >         list.
        >     - [`~utils.RevisionNotFoundError`]
        >         If the repository exists but the revision couldn't be found.
        >     - [`~utils.EntryNotFoundError`]
        >         If the repository exists but the entry (e.g. the requested file) couldn't be
        >         find.
        >     - [`~utils.BadRequestError`]
        >         If request failed with a HTTP 400 BadRequest error.
        >     - [`~utils.HfHubHTTPError`]
        >         If request failed for a reason not listed above.
        """
        try:
            response.raise_for_status()
        except httpx.HTTPStatusError as e:
            if response.status_code // 100 == 3:
                return  # Do not raise on redirects to stay consistent with `requests`
    
            error_code = response.headers.get("X-Error-Code")
            error_message = response.headers.get("X-Error-Message")
    
            if error_code == "RevisionNotFound":
                message = f"{response.status_code} Client Error." + "\n\n" + f"Revision Not Found for url: {response.url}."
                raise _format(RevisionNotFoundError, message, response) from e
    
            elif error_code == "EntryNotFound":
                message = f"{response.status_code} Client Error." + "\n\n" + f"Entry Not Found for url: {response.url}."
                raise _format(RemoteEntryNotFoundError, message, response) from e
    
            elif error_code == "GatedRepo":
                message = (
                    f"{response.status_code} Client Error." + "\n\n" + f"Cannot access gated repo for url {response.url}."
                )
                raise _format(GatedRepoError, message, response) from e
    
            elif error_message == "Access to this resource is disabled.":
                message = (
                    f"{response.status_code} Client Error."
                    + "\n\n"
                    + f"Cannot access repository for url {response.url}."
                    + "\n"
                    + "Access to this resource is disabled."
                )
                raise _format(DisabledRepoError, message, response) from e
    
            elif error_code == "RepoNotFound" or (
                response.status_code == 401
                and error_message != "Invalid credentials in Authorization header"
                and response.request is not None
                and response.request.url is not None
                and REPO_API_REGEX.search(str(response.request.url)) is not None
            ):
                # 401 is misleading as it is returned for:
                #    - private and gated repos if user is not authenticated
                #    - missing repos
                # => for now, we process them as `RepoNotFound` anyway.
                # See https://gist.github.com/Wauplin/46c27ad266b15998ce56a6603796f0b9
                message = (
                    f"{response.status_code} Client Error."
                    + "\n\n"
                    + f"Repository Not Found for url: {response.url}."
                    + "\nPlease make sure you specified the correct `repo_id` and"
                    " `repo_type`.\nIf you are trying to access a private or gated repo,"
                    " make sure you are authenticated. For more details, see"
                    " https://huggingface.co/docs/huggingface_hub/authentication"
                )
                raise _format(RepositoryNotFoundError, message, response) from e
    
            elif response.status_code == 400:
                message = (
                    f"\n\nBad request for {endpoint_name} endpoint:" if endpoint_name is not None else "\n\nBad request:"
                )
                raise _format(BadRequestError, message, response) from e
    
            elif response.status_code == 403:
                message = (
                    f"\n\n{response.status_code} Forbidden: {error_message}."
                    + f"\nCannot access content at: {response.url}."
                    + "\nMake sure your token has the correct permissions."
                )
                raise _format(HfHubHTTPError, message, response) from e
    
            elif response.status_code == 416:
                range_header = response.request.headers.get("Range")
                message = f"{e}. Requested range: {range_header}. Content-Range: {response.headers.get('Content-Range')}."
                raise _format(HfHubHTTPError, message, response) from e
    
            # Convert `HTTPError` into a `HfHubHTTPError` to display request information
            # as well (request id and/or server error message)
>           raise _format(HfHubHTTPError, str(e), response) from e
E           huggingface_hub.errors.HfHubHTTPError: Client error '429 Too Many Requests' for url 'https://huggingface.co/api/models/timm/resnet18.a1_in1k/xet-read-token/491b427b45c94c7fb0e78b5474cc919aff584bbf' (Request ID: Root=1-692010bd-56e663ad1d7ebbd118bb822d;1fc4fe4d-9ae0-4ae8-b8ba-6367a620a820)
E           For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429
E           
E           We had to rate limit your IP (85.118.59.141). To continue using our service, create a HF account or login to your existing account, and make sure you pass a HF_TOKEN if you're using the API.

.docker/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:629: HfHubHTTPError
----------------------------- Captured stderr call -----------------------------
Test failed with Client error '429 Too Many Requests' for url 'https://huggingface.co/api/models/timm/resnet18.a1_in1k/xet-read-token/491b427b45c94c7fb0e78b5474cc919aff584bbf' (Request ID: Root=1-692010b3-778c49fe618e86ff49a6ba72;e8bc5945-624f-4e58-8637-4fff2f726b4f)
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

We had to rate limit your IP (85.118.59.141). To continue using our service, create a HF account or login to your existing account, and make sure you pass a HF_TOKEN if you're using the API. at try 1/5 as it couldn't connect to the specified Hub repository.
Test failed with Client error '429 Too Many Requests' for url 'https://huggingface.co/api/models/timm/resnet18.a1_in1k/xet-read-token/491b427b45c94c7fb0e78b5474cc919aff584bbf' (Request ID: Root=1-692010b6-4319c39b5a1677c5467a6dea;6143f425-5f42-4c3d-87e9-5ee473074b74)
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

We had to rate limit your IP (85.118.59.141). To continue using our service, create a HF account or login to your existing account, and make sure you pass a HF_TOKEN if you're using the API. at try 2/5 as it couldn't connect to the specified Hub repository.
Test failed with Client error '429 Too Many Requests' for url 'https://huggingface.co/api/models/timm/resnet18.a1_in1k/xet-read-token/491b427b45c94c7fb0e78b5474cc919aff584bbf' (Request ID: Root=1-692010b8-1d1a59d26f8ec58a317b8195;bfd052f4-7c3c-475a-a5c6-0d7388cade96)
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

We had to rate limit your IP (85.118.59.141). To continue using our service, create a HF account or login to your existing account, and make sure you pass a HF_TOKEN if you're using the API. at try 3/5 as it couldn't connect to the specified Hub repository.
Test failed with Client error '429 Too Many Requests' for url 'https://huggingface.co/api/models/timm/resnet18.a1_in1k/xet-read-token/491b427b45c94c7fb0e78b5474cc919aff584bbf' (Request ID: Root=1-692010ba-670ee34f171063567466e823;f8470f6f-b47c-4f82-a5b2-51504b4dd5af)
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

We had to rate limit your IP (85.118.59.141). To continue using our service, create a HF account or login to your existing account, and make sure you pass a HF_TOKEN if you're using the API. at try 4/5 as it couldn't connect to the specified Hub repository.
=============================== warnings summary ===============================
tests/test_modeling_common.py:3549: 64 warnings
  /home/ilyas/transformers/tests/test_modeling_common.py:3549: PytestUnknownMarkWarning: Unknown pytest.mark.onnx_export_test - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.onnx_export_test

<frozen importlib._bootstrap>:241: 128 warnings
  <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute

<frozen importlib._bootstrap>:241: 128 warnings
  <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute

<frozen importlib._bootstrap>:241: 64 warnings
  <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type swigvarlink has no __module__ attribute

tests/models/funnel/test_modeling_funnel.py::FunnelBaseModelTest::test_torch_export
tests/models/funnel/test_modeling_funnel.py::FunnelModelTest::test_torch_export
  /home/ilyas/transformers/src/transformers/models/funnel/modeling_funnel.py:245: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
    tensor = torch.cat([tensor[cls_slice], tensor], axis=axis)

tests/models/funnel/test_modeling_funnel.py::FunnelBaseModelTest::test_torch_export
tests/models/funnel/test_modeling_funnel.py::FunnelModelTest::test_torch_export
  /home/ilyas/transformers/src/transformers/models/funnel/modeling_funnel.py:246: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
    return tensor[enc_slice]

tests/models/flava/test_modeling_flava.py: 4 warnings
tests/models/bros/test_modeling_bros.py: 4 warnings
tests/models/layoutlmv3/test_modeling_layoutlmv3.py: 4 warnings
  /home/ilyas/transformers/src/transformers/modeling_utils.py:1421: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
    warnings.warn(

tests/models/bloom/test_modeling_bloom.py::BloomModelTest::test_torch_export
  /home/ilyas/transformers/src/transformers/models/bloom/modeling_bloom.py:504: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.
    warnings.warn(

tests/models/led/test_modeling_led.py::LEDModelTest::test_torch_export
  /home/ilyas/transformers/src/transformers/models/led/modeling_led.py:2114: FutureWarning: The `transformers.LEDForSequenceClassification` class is deprecated and will be removed in version 5 of Transformers. No actual method were provided in the original paper on how to perform sequence classification.
    warnings.warn(

tests/models/oneformer/test_modeling_oneformer.py::OneFormerModelTest::test_torch_export
  /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/functional.py:505: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4317.)
    return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

tests/models/modernbert/test_modeling_modernbert.py::ModernBertModelTest::test_torch_export
  /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:312: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
    warnings.warn(

tests/models/wav2vec2/test_modeling_wav2vec2.py::Wav2Vec2RobustModelTest::test_torch_export
tests/models/wav2vec2/test_modeling_wav2vec2.py::Wav2Vec2ModelTest::test_torch_export
  /home/ilyas/transformers/src/transformers/models/wav2vec2/modeling_wav2vec2.py:1647: FutureWarning: The class `Wav2Vec2ForMaskedLM` is deprecated. Please use `Wav2Vec2ForCTC` instead.
    warnings.warn(

tests/models/wavlm/test_modeling_wavlm.py::WavLMModelTest::test_torch_export
tests/models/wavlm/test_modeling_wavlm.py::WavLMModelTest::test_torch_export
tests/models/wavlm/test_modeling_wavlm.py::WavLMModelTest::test_torch_export
tests/models/wavlm/test_modeling_wavlm.py::WavLMModelTest::test_torch_export
  /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/nn/functional.py:6044: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
    warnings.warn(

tests/models/timesfm/test_modeling_timesfm.py::TimesFmModelTest::test_torch_export
  /home/ilyas/transformers/src/transformers/models/timesfm/modeling_timesfm.py:641: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
    torch.tensor(freq[: len(inputs)], dtype=torch.int32).reshape(-1, 1),

tests/models/timesfm/test_modeling_timesfm.py::TimesFmModelTest::test_torch_export
  /home/ilyas/transformers/.docker/lib/python3.10/site-packages/torch/_export/non_strict_utils.py:1066: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
    return func(*args, **kwargs)

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/models/eomt/test_modeling_eomt.py::EomtForUniversalSegmentationTest::test_torch_export - torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode: Could not guard on data-dependent expression Eq(u0, 1) (unhinted: Eq(u0, 1)).  (Size-like symbols: none)

consider using data-dependent friendly APIs such as guard_or_false, guard_or_true and statically_known_trueCaused by: (_export/non_strict_utils.py:1066 in __torch_function__)
For more information, run with TORCH_LOGS="dynamic"
For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="u0"
If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing

For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1

The following call raised this error:
  File "/home/ilyas/transformers/src/transformers/models/eomt/modeling_eomt.py", line 1215, in _disable_attention_mask
    if prob < 1:


The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.
FAILED tests/models/flava/test_modeling_flava.py::FlavaForPreTrainingTest::test_torch_export - AttributeError: 'Tensor' object has no attribute 'ize'
FAILED tests/models/mllama/test_modeling_mllama.py::MllamaForConditionalGenerationModelTest::test_torch_export - RuntimeError: The expanded size of the tensor (1808) must match the existing size (904) at non-singleton dimension 3.  Target sizes: [3, 4, 7, 1808].  Tensor sizes: [3, 1, 7, 904]
FAILED tests/models/clvp/test_modeling_clvp.py::ClvpModelForConditionalGenerationTest::test_torch_export - huggingface_hub.errors.HfHubHTTPError: Client error '429 Too Many Requests' for url 'https://huggingface.co/api/datasets/hf-internal-testing/librispeech_asr_dummy/revision/5be91486e11a2d616f4ec5db8d3fd248585ac07a' (Request ID: Root=1-6920109c-5f2b8cd538a184b873bf8590;5f2b8c49-34b1-480b-9a4e-8fb7917e18b6)
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

We had to rate limit your IP (85.118.59.141). To continue using our service, create a HF account or login to your existing account, and make sure you pass a HF_TOKEN if you're using the API.
FAILED tests/models/videomae/test_modeling_videomae.py::VideoMAEModelTest::test_torch_export - torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode: Could not guard on data-dependent expression u5 > 0 (unhinted: u5 > 0).  (Size-like symbols: u5)

consider using data-dependent friendly APIs such as guard_or_false, guard_or_true and statically_known_trueCaused by: (src/transformers/models/videomae/modeling_videomae.py:505 in forward)
For more information, run with TORCH_LOGS="dynamic"
For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="u5"
If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing

For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1

The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.
FAILED tests/models/zamba/test_modeling_zamba.py::ZambaModelTest::test_torch_export - torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode: Could not guard on data-dependent expression Eq(u1, 1) (unhinted: Eq(u1, 1)).  (Size-like symbols: none)

consider using data-dependent friendly APIs such as guard_or_false, guard_or_true and statically_known_trueCaused by: (_export/non_strict_utils.py:1066 in __torch_function__)
For more information, run with TORCH_LOGS="dynamic"
For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL="u1"
If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing

For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1

The following call raised this error:
  File "/home/ilyas/transformers/src/transformers/models/zamba/modeling_zamba.py", line 508, in slow_forward
    if attention_mask is not None and not torch.all(attention_mask == 1):


The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.
FAILED tests/models/timm_backbone/test_modeling_timm_backbone.py::TimmBackboneModelTest::test_torch_export - huggingface_hub.errors.HfHubHTTPError: Client error '429 Too Many Requests' for url 'https://huggingface.co/api/models/timm/resnet18.a1_in1k/xet-read-token/491b427b45c94c7fb0e78b5474cc919aff584bbf' (Request ID: Root=1-692010bd-56e663ad1d7ebbd118bb822d;1fc4fe4d-9ae0-4ae8-b8ba-6367a620a820)
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

We had to rate limit your IP (85.118.59.141). To continue using our service, create a HF account or login to your existing account, and make sure you pass a HF_TOKEN if you're using the API.
====== 7 failed, 466 passed, 4 skipped, 412 warnings in 158.34s (0:02:38) ======
