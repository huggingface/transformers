from argparse import ArgumentParser
from pathlib import Path
import gradio as gr
import os
import tempfile
import torch
from transformers import AutoProcessor, Glm4vForConditionalGeneration
from transformers.video_utils import load_video
import cv2

MODEL_PATH = "/model/glm-4v-9b-0603"
MAX_VIDEO_DURATION = 3600
def _get_args():
    parser = ArgumentParser()
    parser.add_argument("--server-port", type=int, default=8000,
                        help="Demo server port.")
    parser.add_argument("--server-name", type=str, default="10.244.99.109",
                        help="Demo server name.")
    args = parser.parse_args()
    return args


def get_video_duration(video_path):
    """è·å–è§†é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰"""
    try:
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)
        duration = frame_count / fps
        cap.release()
        return duration
    except Exception as e:
        print(f"Error getting video duration: {e}")
        return 0


def is_video_file(filename):
    """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºè§†é¢‘æ–‡ä»¶"""
    video_extensions = ['.mp4', '.avi', '.mkv', '.mov', '.wmv', '.flv', '.webm', '.mpeg', '.m4v']
    return any(filename.lower().endswith(ext) for ext in video_extensions)


class GLM4VModel:
    def __init__(self):
        self.processor = None
        self.model = None
        self.device = "cuda:0" if torch.cuda.is_available() else "cpu"

    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        if self.model is None:
            print("Loading GLM-4V model...")
            self.processor = AutoProcessor.from_pretrained(
                MODEL_PATH,
                use_fast=True,
                trust_remote_code=True
            )
            self.model = Glm4vForConditionalGeneration.from_pretrained(
                pretrained_model_name_or_path=MODEL_PATH,
                torch_dtype=torch.bfloat16,
                device_map=self.device,
                attn_implementation="sdpa",
                trust_remote_code=True,
            )
            print("Model loaded successfully!")

    def analyze_video(self, video_path, question):
        """åˆ†æè§†é¢‘å¹¶è¿”å›ç»“æœ"""
        try:
            # åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœè¿˜æ²¡åŠ è½½ï¼‰
            self.load_model()

            # æ„å»ºæ¶ˆæ¯
            messages = [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "video_url",
                            "video_url": {
                                "url": f"file://{video_path}"
                            },
                        },
                        {
                            "type": "text",
                            "text": question,
                        },
                    ],
                }
            ]

            # åŠ è½½è§†é¢‘
            video_tensor, video_metadata = load_video(video_path)

            # å¤„ç†è¾“å…¥
            text = self.processor.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            inputs = self.processor(
                text=text,
                videos=[video_tensor],
                video_metadata=[video_metadata],
                return_tensors="pt"
            ).to(self.device)

            # ç”Ÿæˆå›ç­”
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=1024,
                    do_sample=True,
                    temperature=1.0
                )

            # è§£ç è¾“å‡º
            output_text = self.processor.decode(
                generated_ids[0][inputs["input_ids"].shape[1]:],
                skip_special_tokens=True
            )

            return output_text

        except Exception as e:
            return f"å¤„ç†è§†é¢‘æ—¶å‡ºç°é”™è¯¯: {str(e)}"


# å…¨å±€æ¨¡å‹å®ä¾‹
glm4v_model = GLM4VModel()


def _launch_demo(args):
    uploaded_file_dir = os.environ.get("GRADIO_TEMP_DIR") or str(
        Path(tempfile.gettempdir()) / "gradio"
    )

    def process_video_and_question(video_file, question, chatbot, history):
        """å¤„ç†è§†é¢‘å’Œé—®é¢˜"""
        if video_file is None:
            error_msg = "è¯·å…ˆä¸Šä¼ è§†é¢‘æ–‡ä»¶"
            chatbot.append({"role": "user", "content": question})
            chatbot.append({"role": "assistant", "content": error_msg})
            return chatbot, history

        if not question.strip():
            error_msg = "è¯·è¾“å…¥æ‚¨çš„é—®é¢˜"
            chatbot.append({"role": "assistant", "content": error_msg})
            return chatbot, history

        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        if not is_video_file(video_file.name):
            error_msg = "è¯·ä¸Šä¼ æœ‰æ•ˆçš„è§†é¢‘æ–‡ä»¶"
            chatbot.append({"role": "user", "content": question})
            chatbot.append({"role": "assistant", "content": error_msg})
            return chatbot, history

        # æ£€æŸ¥è§†é¢‘æ—¶é•¿
        duration = get_video_duration(video_file.name)
        if duration > MAX_VIDEO_DURATION:
            error_msg = f"è§†é¢‘æ—¶é•¿({duration / 60:.1f}åˆ†é’Ÿ)è¶…è¿‡é™åˆ¶(60åˆ†é’Ÿ)ï¼Œè¯·ä¸Šä¼ è¾ƒçŸ­çš„è§†é¢‘"
            chatbot.append({"role": "user", "content": question})
            chatbot.append({"role": "assistant", "content": error_msg})
            return chatbot, history

        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°èŠå¤©æ¡†
        user_message = f"ğŸ“¹ è§†é¢‘: {os.path.basename(video_file.name)} ({duration / 60:.1f}åˆ†é’Ÿ)\nğŸ’¬ é—®é¢˜: {question}"
        chatbot.append({"role": "user", "content": user_message})
        chatbot.append({"role": "assistant", "content": "æ­£åœ¨åˆ†æè§†é¢‘ï¼Œè¯·ç¨å€™..."})

        # åˆ†æè§†é¢‘
        try:
            response = glm4v_model.analyze_video(video_file.name, question)
            chatbot[-1] = {"role": "assistant", "content": response}

            # æ›´æ–°å†å²è®°å½•
            history.append({
                "video": video_file.name,
                "question": question,
                "response": response
            })

        except Exception as e:
            error_msg = f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"
            chatbot[-1] = {"role": "assistant", "content": error_msg}

        return chatbot, history

    def clear_all():
        """æ¸…é™¤æ‰€æœ‰å†…å®¹"""
        return [], [], "", None

    def regenerate_last(chatbot, history):
        """é‡æ–°ç”Ÿæˆæœ€åä¸€ä¸ªå›ç­”"""
        if not history:
            return chatbot

        last_item = history[-1]
        video_path = last_item["video"]
        question = last_item["question"]

        # æ›´æ–°èŠå¤©æ¡†æ˜¾ç¤ºæ­£åœ¨é‡æ–°ç”Ÿæˆ
        if chatbot and len(chatbot) >= 2:
            user_message = chatbot[-2]["content"]
            chatbot[-1] = {"role": "assistant", "content": "æ­£åœ¨é‡æ–°åˆ†æè§†é¢‘ï¼Œè¯·ç¨å€™..."}

        try:
            response = glm4v_model.analyze_video(video_path, question)
            chatbot[-1] = {"role": "assistant", "content": response}
            history[-1]["response"] = response
        except Exception as e:
            error_msg = f"é‡æ–°ç”Ÿæˆæ—¶å‡ºç°é”™è¯¯: {str(e)}"
            chatbot[-1] = {"role": "assistant", "content": error_msg}

        return chatbot

    # åˆ›å»ºGradioç•Œé¢
    with gr.Blocks(title="GLM-4V è§†é¢‘åˆ†æ") as demo:
        gr.Markdown("""
        <center>
        <h1>ğŸ¬ GLM-4V è§†é¢‘åˆ†æåŠ©æ‰‹</h1>
        <p>ä¸Šä¼ è§†é¢‘æ–‡ä»¶ï¼Œæå‡ºé—®é¢˜ï¼Œè®©AIä¸ºæ‚¨è¯¦ç»†åˆ†æè§†é¢‘å†…å®¹</p>
        <p><small>âš ï¸ è§†é¢‘æ—¶é•¿é™åˆ¶ï¼šæœ€é•¿60åˆ†é’Ÿ</small></p>
        </center>
        """)

        # çŠ¶æ€å˜é‡
        history = gr.State([])

        # ä¸»è¦ç»„ä»¶
        with gr.Row():
            with gr.Column(scale=1):
                video_input = gr.File(
                    label="ğŸ“ ä¸Šä¼ è§†é¢‘æ–‡ä»¶",
                    file_types=["video"],
                    type="filepath"
                )
                question_input = gr.Textbox(
                    label="ğŸ’­ è¾“å…¥æ‚¨çš„é—®é¢˜",
                    placeholder="ä¾‹å¦‚ï¼šè¯¦ç»†æè¿°ä¸€ä¸‹è¿™ä¸ªè§†é¢‘çš„å†…å®¹",
                    lines=3
                )

                with gr.Row():
                    submit_btn = gr.Button("ğŸš€ å¼€å§‹åˆ†æ", variant="primary")
                    clear_btn = gr.Button("ğŸ§¹ æ¸…é™¤æ‰€æœ‰")
                    regen_btn = gr.Button("ğŸ”„ é‡æ–°ç”Ÿæˆ")

            with gr.Column(scale=2):
                chatbot = gr.Chatbot(
                    label="åˆ†æç»“æœ",
                    height=500,
                    type="messages",
                    elem_classes="chatbot-container"
                )

        submit_btn.click(
            fn=process_video_and_question,
            inputs=[video_input, question_input, chatbot, history],
            outputs=[chatbot, history],
            show_progress=True
        ).then(
            fn=lambda: "",
            outputs=question_input
        )

        clear_btn.click(
            fn=clear_all,
            outputs=[chatbot, history, question_input, video_input]
        )

        regen_btn.click(
            fn=regenerate_last,
            inputs=[chatbot, history],
            outputs=chatbot,
        )

        question_input.submit(
            fn=process_video_and_question,
            inputs=[video_input, question_input, chatbot, history],
            outputs=[chatbot, history],
        ).then(
            fn=lambda: "",
            outputs=question_input
        )

    demo.queue().launch(server_port=args.server_port, server_name=args.server_name,share=True)


def main():
    args = _get_args()
    _launch_demo(args)


if __name__ == '__main__':
    main()