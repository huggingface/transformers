{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aad_1s7ybD5o"
   },
   "source": [
    "# `RoBERTa` --> `Longformer`: build a \"long\" version of pretrained models\n",
    "\n",
    "This notebook replicates the procedure descriped in the [Longformer paper](https://arxiv.org/abs/2004.05150) to train a Longformer model starting from the RoBERTa checkpoint. The same procedure can be applied to build the \"long\" version of other pretrained models as well. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BieXv0YUd7NF"
   },
   "source": [
    "### Data, libraries, and imports\n",
    "Our procedure requires a corpus for pretraining. For demonstration, we will use Wikitext103; a corpus of 100M tokens from wikipedia articles. Depending on your application, consider using a different corpus that is a better match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7AZZ4VmKSwvH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-09-13 21:45:47--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.99.238\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.99.238|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4721645 (4.5M) [application/zip]\n",
      "Saving to: â€˜wikitext-2-raw-v1.zip.2â€™\n",
      "\n",
      "wikitext-2-raw-v1.z 100%[===================>]   4.50M  9.09MB/s    in 0.5s    \n",
      "\n",
      "2022-09-13 21:45:48 (9.09 MB/s) - â€˜wikitext-2-raw-v1.zip.2â€™ saved [4721645/4721645]\n",
      "\n",
      "Archive:  wikitext-2-raw-v1.zip\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip\n",
    "!unzip -n wikitext-2-raw-v1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o3yjIYKXw3rL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==3.0.2\n",
      "  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m769.0/769.0 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /rscratch/ja/anaconda3/envs/bert/lib/python3.8/site-packages (from transformers==3.0.2) (1.23.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /rscratch/ja/anaconda3/envs/bert/lib/python3.8/site-packages (from transformers==3.0.2) (2022.8.17)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.53-py3-none-any.whl\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.8.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: packaging in /rscratch/ja/anaconda3/envs/bert/lib/python3.8/site-packages (from transformers==3.0.2) (21.3)\n",
      "Collecting requests\n",
      "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers==0.8.1.rc1\n",
      "  Downloading tokenizers-0.8.1rc1-cp38-cp38-manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
      "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /rscratch/ja/anaconda3/envs/bert/lib/python3.8/site-packages (from transformers==3.0.2) (4.64.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /rscratch/ja/anaconda3/envs/bert/lib/python3.8/site-packages (from packaging->transformers==3.0.2) (3.0.9)\n",
      "Collecting charset-normalizer<3,>=2\n",
      "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /rscratch/ja/anaconda3/envs/bert/lib/python3.8/site-packages (from requests->transformers==3.0.2) (2022.6.15)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m140.4/140.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting click\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Requirement already satisfied: six in /rscratch/ja/anaconda3/envs/bert/lib/python3.8/site-packages (from sacremoses->transformers==3.0.2) (1.16.0)\n",
      "Installing collected packages: tokenizers, sentencepiece, urllib3, joblib, idna, filelock, click, charset-normalizer, sacremoses, requests, transformers\n",
      "Successfully installed charset-normalizer-2.1.1 click-8.1.3 filelock-3.8.0 idna-3.4 joblib-1.1.0 requests-2.28.1 sacremoses-0.0.53 sentencepiece-0.1.97 tokenizers-0.8.1rc1 transformers-3.0.2 urllib3-1.26.12\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==3.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U0NnMMl6wy7Q"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rscratch/ja/anaconda3/envs/bert/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from transformers import RobertaForMaskedLM, RobertaTokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer\n",
    "from transformers import TrainingArguments, HfArgumentParser\n",
    "from transformers import RobertaConfig\n",
    "from transformers.modeling_longformer import LongformerSelfAttention\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xgoNVJYUbD59"
   },
   "source": [
    "### RobertaLong\n",
    "\n",
    "`RobertaLongForMaskedLM` represents the \"long\" version of the `RoBERTa` model. It replaces `BertSelfAttention` with `RobertaLongSelfAttention`, which is a thin wrapper around `LongformerSelfAttention`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J9EBISkRxPjO"
   },
   "outputs": [],
   "source": [
    "# class RobertaLongSelfAttention(LongformerSelfAttention):\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         hidden_states,\n",
    "#         attention_mask=None,\n",
    "#         head_mask=None,\n",
    "#         encoder_hidden_states=None,\n",
    "#         encoder_attention_mask=None,\n",
    "#         output_attentions=False,\n",
    "#     ):\n",
    "#         return super().forward(hidden_states, attention_mask=attention_mask, output_attentions=output_attentions)\n",
    "\n",
    "\n",
    "# class RobertaLongForMaskedLM(RobertaForMaskedLM):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__(config)\n",
    "#         for i, layer in enumerate(self.roberta.encoder.layer):\n",
    "#             # replace the `modeling_bert.BertSelfAttention` object with `LongformerSelfAttention`\n",
    "#             layer.attention.self = RobertaLongSelfAttention(config, layer_id=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4LRZa5s1bD6E"
   },
   "source": [
    "Starting from the `roberta-base` checkpoint, the following function converts it into an instance of `RobertaLong`. It makes the following changes:\n",
    "\n",
    "- extend the position embeddings from `512` positions to `max_pos`. In Longformer, we set `max_pos=4096`\n",
    "\n",
    "- initialize the additional position embeddings by copying the embeddings of the first `512` positions. This initialization is crucial for the model performance (check table 6 in [the paper](https://arxiv.org/pdf/2004.05150.pdf) for performance without this initialization)\n",
    "\n",
    "- replaces `modeling_bert.BertSelfAttention` objects with `modeling_longformer.LongformerSelfAttention` with a attention window size `attention_window`\n",
    "\n",
    "The output of this function works for long documents even without pretraining. Check tables 6 and 11 in [the paper](https://arxiv.org/pdf/2004.05150.pdf) to get a sense of the expected performance of this model before pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-m4A_ttixPuf"
   },
   "outputs": [],
   "source": [
    "# def create_long_model(save_model_to, attention_window, max_pos):\n",
    "#     model = RobertaForMaskedLM.from_pretrained('roberta-base')\n",
    "#     tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', model_max_length=max_pos)\n",
    "#     config = model.config\n",
    "\n",
    "#     # extend position embeddings\n",
    "#     tokenizer.model_max_length = max_pos\n",
    "#     tokenizer.init_kwargs['model_max_length'] = max_pos\n",
    "#     current_max_pos, embed_size = model.roberta.embeddings.position_embeddings.weight.shape\n",
    "#     max_pos += 2  # NOTE: RoBERTa has positions 0,1 reserved, so embedding size is max position + 2\n",
    "#     config.max_position_embeddings = max_pos\n",
    "#     assert max_pos > current_max_pos\n",
    "#     # allocate a larger position embedding matrix\n",
    "#     new_pos_embed = model.roberta.embeddings.position_embeddings.weight.new_empty(max_pos, embed_size)\n",
    "#     # copy position embeddings over and over to initialize the new position embeddings\n",
    "#     k = 2\n",
    "#     step = current_max_pos - 2\n",
    "#     while k < max_pos - 1:\n",
    "#         new_pos_embed[k:(k + step)] = model.roberta.embeddings.position_embeddings.weight[2:]\n",
    "#         k += step\n",
    "#     model.roberta.embeddings.position_embeddings.weight.data = new_pos_embed\n",
    "#     model.roberta.embeddings.position_ids.data = torch.tensor([i for i in range(max_pos)]).reshape(1, max_pos)\n",
    "\n",
    "#     # replace the `modeling_bert.BertSelfAttention` object with `LongformerSelfAttention`\n",
    "#     config.attention_window = [attention_window] * config.num_hidden_layers\n",
    "#     for i, layer in enumerate(model.roberta.encoder.layer):\n",
    "#         longformer_self_attn = LongformerSelfAttention(config, layer_id=i)\n",
    "#         longformer_self_attn.query = layer.attention.self.query\n",
    "#         longformer_self_attn.key = layer.attention.self.key\n",
    "#         longformer_self_attn.value = layer.attention.self.value\n",
    "\n",
    "#         longformer_self_attn.query_global = copy.deepcopy(layer.attention.self.query)\n",
    "#         longformer_self_attn.key_global = copy.deepcopy(layer.attention.self.key)\n",
    "#         longformer_self_attn.value_global = copy.deepcopy(layer.attention.self.value)\n",
    "\n",
    "#         layer.attention.self = longformer_self_attn\n",
    "\n",
    "#     logger.info(f'saving model to {save_model_to}')\n",
    "#     model.save_pretrained(save_model_to)\n",
    "#     tokenizer.save_pretrained(save_model_to)\n",
    "#     return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PkbQvhOMbD6L"
   },
   "source": [
    "Pretraining on Masked Language Modeling (MLM) doesn't update the global projection layers. After pretraining, the following function copies `query`, `key`, `value` to their global counterpart projection matrices.\n",
    "For more explanation on \"local\" vs. \"global\" attention, please refer to the documentation [here](https://huggingface.co/transformers/model_doc/longformer.html#longformer-self-attention)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CO3MoEgCxP9W"
   },
   "outputs": [],
   "source": [
    "# def copy_proj_layers(model):\n",
    "#     for i, layer in enumerate(model.roberta.encoder.layer):\n",
    "#         layer.attention.self.query_global = copy.deepcopy(layer.attention.self.query)\n",
    "#         layer.attention.self.key_global = copy.deepcopy(layer.attention.self.key)\n",
    "#         layer.attention.self.value_global = copy.deepcopy(layer.attention.self.value)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0KjI9f8BbD6S"
   },
   "source": [
    "### Pretrain and Evaluate on masked language modeling (MLM)\n",
    "\n",
    "The following function pretrains and evaluates a model on MLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wC2sddeLyVhA"
   },
   "outputs": [],
   "source": [
    "def pretrain_and_evaluate(args, model, tokenizer, eval_only, model_path):\n",
    "    val_dataset = TextDataset(tokenizer=tokenizer,\n",
    "                              file_path=args.val_datapath,\n",
    "                              block_size=tokenizer.max_len)\n",
    "    if eval_only:\n",
    "        train_dataset = val_dataset\n",
    "    else:\n",
    "        logger.info(f'Loading and tokenizing training data is usually slow: {args.train_datapath}')\n",
    "        train_dataset = TextDataset(tokenizer=tokenizer,\n",
    "                                    file_path=args.train_datapath,\n",
    "                                    block_size=tokenizer.max_len)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "    trainer = Trainer(model=model, args=args, data_collator=data_collator,\n",
    "                      train_dataset=train_dataset, eval_dataset=val_dataset, prediction_loss_only=True,)\n",
    "\n",
    "    eval_loss = trainer.evaluate()\n",
    "    eval_loss = eval_loss['eval_loss']\n",
    "    logger.info(f'Initial eval bpc: {eval_loss/math.log(2)}')\n",
    "    \n",
    "    if not eval_only:\n",
    "        trainer.train(model_path=model_path)\n",
    "        trainer.save_model()\n",
    "\n",
    "        eval_loss = trainer.evaluate()\n",
    "        eval_loss = eval_loss['eval_loss']\n",
    "        logger.info(f'Eval bpc after pretraining: {eval_loss/math.log(2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AqY_Hg5HbD6a"
   },
   "source": [
    "**Training hyperparameters**\n",
    "\n",
    "- Following RoBERTa pretraining setting, we set number of tokens per batch to be `2^18` tokens. Changing this number might require changes in the lr, lr-scheudler, #steps and #warmup steps. Therefor, it is a good idea to keep this number constant.\n",
    "\n",
    "- Note that: `#tokens/batch = batch_size x #gpus x gradient_accumulation x seqlen`\n",
    "   \n",
    "- In [the paper](https://arxiv.org/pdf/2004.05150.pdf), we train for 65k steps, but 3k is probably enough (check table 6)\n",
    "\n",
    "- **Important note**: The lr-scheduler in [the paper](https://arxiv.org/pdf/2004.05150.pdf) is polynomial_decay with power 3 over 65k steps. To train for 3k steps, use a constant lr-scheduler (after warmup). Both lr-scheduler are not supported in HF trainer, and at least **constant lr-scheduler** will need to be added. \n",
    "\n",
    "- Pretraining will take 2 days on 1 x 32GB GPU with fp32. Consider using fp16 and using more gpus to train faster (if you increase `#gpus`, reduce `gradient_accumulation` to maintain `#tokens/batch` as mentioned earlier).\n",
    "\n",
    "- As a demonstration, this notebook is training on wikitext103 but wikitext103 is rather small that it takes 7 epochs to train for 3k steps Consider doing a single epoch on a larger dataset (800M tokens) instead.\n",
    "\n",
    "- Set #gpus using `CUDA_VISIBLE_DEVICES`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zl_hDDlryVo2"
   },
   "outputs": [],
   "source": [
    "# @dataclass\n",
    "# class ModelArgs:\n",
    "#     attention_window: int = field(default=512, metadata={\"help\": \"Size of attention window\"})\n",
    "#     max_pos: int = field(default=4096, metadata={\"help\": \"Maximum position\"})\n",
    "\n",
    "parser = HfArgumentParser(TrainingArguments)\n",
    "\n",
    "\n",
    "training_args = parser.parse_args_into_dataclasses(look_for_args_file=False, args=[\n",
    "    '--output_dir', 'tmp',\n",
    "    '--warmup_steps', '500',\n",
    "    '--learning_rate', '0.00003',\n",
    "    '--weight_decay', '0.01',\n",
    "    '--adam_epsilon', '1e-6',\n",
    "    '--max_steps', '3000',\n",
    "    '--logging_steps', '500',\n",
    "    '--save_steps', '500',\n",
    "    '--max_grad_norm', '5.0',\n",
    "    '--per_gpu_eval_batch_size', '4',\n",
    "    '--per_gpu_train_batch_size', '4', \n",
    "    '--gradient_accumulation_steps', '4',\n",
    "    '--evaluate_during_training',\n",
    "    '--do_train',\n",
    "    '--do_eval',\n",
    "])[0]\n",
    "training_args.val_datapath = 'wikitext-2-raw/wiki.valid.raw'\n",
    "training_args.train_datapath = 'wikitext-2-raw/wiki.train.raw'\n",
    "\n",
    "# Choose GPU\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wi5ZIKcsbD6e"
   },
   "source": [
    "### Put it all together\n",
    "\n",
    "1) Evaluating `roberta-base` on MLM to establish a baseline. Validation `bpc` = `2.536` which is higher than the `bpc` values in table 6 [here](https://arxiv.org/pdf/2004.05150.pdf) because wikitext103 is harder than our pretraining corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "b59172f252764a3cae42171d649fa160"
     ]
    },
    "colab_type": "code",
    "id": "JiKj7D1c1ovy",
    "outputId": "dd430ba8-c69b-4110-e384-550a7b4875e6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/eecs/j.wat/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
      "INFO:transformers.configuration_utils:Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /home/eecs/j.wat/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
      "INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "WARNING:transformers.modeling_utils:Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /home/eecs/j.wat/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /home/eecs/j.wat/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "INFO:__main__:Evaluating roberta-base-pretrained (seqlen: 512) for refernece ...\n",
      "INFO:transformers.data.datasets.language_modeling:Loading features from cached file wikitext-2-raw/cached_lm_RobertaTokenizer_510_wiki.valid.raw [took 0.009 s]\n",
      "INFO:transformers.training_args:PyTorch: setting up devices\n",
      "INFO:transformers.trainer:You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n",
      "WARNING:transformers.training_args:Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "INFO:transformers.trainer:***** Running Evaluation *****\n",
      "INFO:transformers.trainer:  Num examples = 489\n",
      "INFO:transformers.trainer:  Batch size = 4\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 123/123 [00:08<00:00, 14.35it/s]\n",
      "INFO:transformers.trainer:{'eval_loss': 1.7777911550630399, 'step': 0}\n",
      "INFO:__main__:Initial eval bpc: 2.5648104831457097\n"
     ]
    }
   ],
   "source": [
    "roberta_base_pretrained = RobertaForMaskedLM.from_pretrained('roberta-base')\n",
    "roberta_base_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "logger.info('Evaluating roberta-base-pretrained (seqlen: 512) for refernece ...')\n",
    "pretrain_and_evaluate(training_args, roberta_base_pretrained, roberta_base_tokenizer, eval_only=True, model_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Evaluating roberta-base no train (seqlen: 512) for refernece ...\n",
      "INFO:transformers.data.datasets.language_modeling:Loading features from cached file wikitext-2-raw/cached_lm_RobertaTokenizer_510_wiki.valid.raw [took 0.021 s]\n",
      "INFO:transformers.trainer:You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n",
      "WARNING:transformers.training_args:Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "INFO:transformers.trainer:***** Running Evaluation *****\n",
      "INFO:transformers.trainer:  Num examples = 489\n",
      "INFO:transformers.trainer:  Batch size = 4\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 123/123 [00:08<00:00, 14.66it/s]\n",
      "INFO:transformers.trainer:{'eval_loss': 10.915725095485284, 'step': 0}\n",
      "INFO:__main__:Initial eval bpc: 15.748062462963826\n"
     ]
    }
   ],
   "source": [
    "roberta_base = RobertaForMaskedLM(config=roberta_base_pretrained.config)\n",
    "logger.info('Evaluating roberta-base no train (seqlen: 512) for refernece ...')\n",
    "pretrain_and_evaluate(training_args, roberta_base, roberta_base_tokenizer, eval_only=True, model_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MBXU3r69bD6l"
   },
   "source": [
    "2) As descriped in `create_long_model`, convert a `roberta-base` model into `roberta-base-4096` which is an instance of `RobertaLong`, then save it to the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c7tcsSfZ1-b9"
   },
   "outputs": [],
   "source": [
    "model_path = f'{training_args.output_dir}/roberta-base'\n",
    "# if not os.path.exists(model_path):\n",
    "#     os.makedirs(model_path)\n",
    "\n",
    "# logger.info(f'Converting roberta-base into roberta-base-{model_args.max_pos}')\n",
    "# model, tokenizer = create_long_model(\n",
    "#     save_model_to=model_path, attention_window=model_args.attention_window, max_pos=model_args.max_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JiftMH3-zPUS"
   },
   "source": [
    "3) Load `roberta-base-4096` from the disk. This model works for long sequences even without pretraining. If you don't want to pretrain, you can stop here and start finetuning your `roberta-base-4096` on downstream tasks ðŸŽ‰ðŸŽ‰ðŸŽ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H8vNeYdrzMd2"
   },
   "outputs": [],
   "source": [
    "# configuration = RobertaConfig()\n",
    "# tokenizer = RobertaTokenizerFast.from_pretrained(model_path)\n",
    "# model = RobertaForMaskedLM(config=config)\n",
    "model = roberta_base\n",
    "tokenizer = roberta_base_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kS0Np2F4bD6p"
   },
   "source": [
    "4) Pretrain `roberta-base-4096` for `3k` steps, each steps has `2^18` tokens. Notes: \n",
    "\n",
    "- The `training_args.max_steps = 3 ` is just for the demo. **Remove this line for the actual training**\n",
    "\n",
    "- Training for `3k` steps will take 2 days on a single 32GB gpu with `fp32`. Consider using `fp16` and more gpus to train faster. \n",
    "\n",
    "- Tokenizing the training data the first time is going to take 5-10 minutes.\n",
    "\n",
    "- MLM validation `bpc` **before** pretraining: **2.652**, a bit worse than the **2.536** of `roberta-base`. As discussed in [the paper](https://arxiv.org/pdf/2004.05150.pdf) this is expected because the model didn't learn yet to work with the sliding window attention. \n",
    "\n",
    "- MLM validation `bpc` after pretraining for a few number of steps: **2.628**. It is quickly getting better. By 3k steps, it should be better than the **2.536** of `roberta-base`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "f2d298034ac44de397e262fd9902f4cf",
      "dbff0a67283b41c7acc81c9bc7b3c3d2",
      "b8cfa9900fe745f58430dfbd1e86f25e",
      "21cf3ef186aa4895aa9741376834a84e"
     ]
    },
    "colab_type": "code",
    "id": "SHD7QMUWbD6q",
    "outputId": "684d79ed-2237-4c7d-f0b9-dd6ba6b45d00"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Pretraining roberta-base ... \n",
      "INFO:transformers.data.datasets.language_modeling:Loading features from cached file wikitext-2-raw/cached_lm_RobertaTokenizer_510_wiki.valid.raw [took 0.022 s]\n",
      "INFO:__main__:Loading and tokenizing training data is usually slow: wikitext-2-raw/wiki.train.raw\n",
      "INFO:transformers.data.datasets.language_modeling:Loading features from cached file wikitext-2-raw/cached_lm_RobertaTokenizer_510_wiki.train.raw [took 0.120 s]\n",
      "INFO:transformers.trainer:You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n",
      "WARNING:transformers.training_args:Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "INFO:transformers.trainer:***** Running Evaluation *****\n",
      "INFO:transformers.trainer:  Num examples = 489\n",
      "INFO:transformers.trainer:  Batch size = 4\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 123/123 [00:08<00:00, 14.71it/s]\n",
      "INFO:transformers.trainer:{'eval_loss': 10.915725095485284, 'step': 0}\n",
      "INFO:__main__:Initial eval bpc: 15.748062462963826\n",
      "WARNING:transformers.training_args:Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "WARNING:transformers.training_args:Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "INFO:transformers.trainer:***** Running training *****\n",
      "INFO:transformers.trainer:  Num examples = 4740\n",
      "INFO:transformers.trainer:  Num Epochs = 11\n",
      "INFO:transformers.trainer:  Instantaneous batch size per device = 8\n",
      "INFO:transformers.trainer:  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "INFO:transformers.trainer:  Gradient Accumulation steps = 4\n",
      "INFO:transformers.trainer:  Total optimization steps = 3000\n",
      "INFO:transformers.trainer:  Starting fine-tuning.\n",
      "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1185/1185 [03:49<00:00,  5.17it/s]\n",
      "Epoch:   9%|â–‰         | 1/11 [03:49<38:12, 229.28s/it]INFO:transformers.trainer:{'loss': 8.75536299419403, 'learning_rate': 3e-05, 'epoch': 1.688607594936709, 'step': 500}\n",
      "INFO:transformers.trainer:Saving model checkpoint to tmp/checkpoint-500\n",
      "INFO:transformers.configuration_utils:Configuration saved in tmp/checkpoint-500/config.json\n",
      "INFO:transformers.modeling_utils:Model weights saved in tmp/checkpoint-500/pytorch_model.bin\n",
      "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1185/1185 [03:55<00:00,  5.04it/s]\n",
      "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1185/1185 [03:52<00:00,  5.10it/s]\n",
      "Epoch:  27%|â–ˆâ–ˆâ–‹       | 3/11 [11:36<31:00, 232.54s/it]INFO:transformers.trainer:{'loss': 7.173312723875045, 'learning_rate': 2.4e-05, 'epoch': 3.378059071729958, 'step': 1000}\n",
      "WARNING:transformers.training_args:Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "INFO:transformers.trainer:***** Running Evaluation *****\n",
      "INFO:transformers.trainer:  Num examples = 489\n",
      "INFO:transformers.trainer:  Batch size = 4\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 123/123 [00:08<00:00, 14.13it/s]\n",
      "INFO:transformers.trainer:{'eval_loss': 7.065050121245346, 'epoch': 3.378059071729958, 'step': 1000}\n",
      "INFO:transformers.trainer:Saving model checkpoint to tmp/checkpoint-1000\n",
      "INFO:transformers.configuration_utils:Configuration saved in tmp/checkpoint-1000/config.json\n",
      "INFO:transformers.modeling_utils:Model weights saved in tmp/checkpoint-1000/pytorch_model.bin\n",
      "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1185/1185 [04:03<00:00,  4.87it/s]\n",
      "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1185/1185 [03:52<00:00,  5.11it/s]\n",
      "Epoch:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 5/11 [19:32<23:30, 235.14s/it]INFO:transformers.trainer:{'loss': 7.015567335605621, 'learning_rate': 1.8e-05, 'epoch': 5.067510548523207, 'step': 1500}\n",
      "INFO:transformers.trainer:Saving model checkpoint to tmp/checkpoint-1500\n",
      "INFO:transformers.configuration_utils:Configuration saved in tmp/checkpoint-1500/config.json\n",
      "INFO:transformers.modeling_utils:Model weights saved in tmp/checkpoint-1500/pytorch_model.bin\n",
      "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1185/1185 [03:54<00:00,  5.05it/s]\n",
      "Epoch:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 6/11 [23:27<19:35, 235.03s/it]INFO:transformers.trainer:{'loss': 6.940303267478943, 'learning_rate': 1.2e-05, 'epoch': 6.756118143459916, 'step': 2000}\n",
      "WARNING:transformers.training_args:Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "INFO:transformers.trainer:***** Running Evaluation *****\n",
      "INFO:transformers.trainer:  Num examples = 489\n",
      "INFO:transformers.trainer:  Batch size = 4\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 123/123 [00:08<00:00, 14.14it/s]\n",
      "INFO:transformers.trainer:{'eval_loss': 6.931762924039267, 'epoch': 6.756118143459916, 'step': 2000}\n",
      "INFO:transformers.trainer:Saving model checkpoint to tmp/checkpoint-2000\n",
      "INFO:transformers.configuration_utils:Configuration saved in tmp/checkpoint-2000/config.json\n",
      "INFO:transformers.modeling_utils:Model weights saved in tmp/checkpoint-2000/pytorch_model.bin\n",
      "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1185/1185 [04:03<00:00,  4.86it/s]\n",
      "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1185/1185 [03:51<00:00,  5.11it/s]\n",
      "Epoch:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 8/11 [31:22<11:47, 235.93s/it]INFO:transformers.trainer:{'loss': 6.888863539695739, 'learning_rate': 6e-06, 'epoch': 8.445569620253165, 'step': 2500}\n",
      "INFO:transformers.trainer:Saving model checkpoint to tmp/checkpoint-2500\n",
      "INFO:transformers.configuration_utils:Configuration saved in tmp/checkpoint-2500/config.json\n",
      "INFO:transformers.modeling_utils:Model weights saved in tmp/checkpoint-2500/pytorch_model.bin\n",
      "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1185/1185 [03:54<00:00,  5.04it/s]\n",
      "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1185/1185 [03:51<00:00,  5.11it/s]\n",
      "Epoch:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 10/11 [39:09<03:54, 234.50s/it]INFO:transformers.trainer:{'loss': 6.854470146894455, 'learning_rate': 0.0, 'epoch': 10.135021097046414, 'step': 3000}\n",
      "WARNING:transformers.training_args:Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "INFO:transformers.trainer:***** Running Evaluation *****\n",
      "INFO:transformers.trainer:  Num examples = 489\n",
      "INFO:transformers.trainer:  Batch size = 4\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 123/123 [00:08<00:00, 14.15it/s]\n",
      "INFO:transformers.trainer:{'eval_loss': 6.89024713562756, 'epoch': 10.135021097046414, 'step': 3000}\n",
      "INFO:transformers.trainer:Saving model checkpoint to tmp/checkpoint-3000\n",
      "INFO:transformers.configuration_utils:Configuration saved in tmp/checkpoint-3000/config.json\n",
      "INFO:transformers.modeling_utils:Model weights saved in tmp/checkpoint-3000/pytorch_model.bin\n",
      "Iteration:  14%|â–ˆâ–        | 163/1185 [00:43<04:33,  3.74it/s]\n",
      "Epoch:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 10/11 [39:53<03:59, 239.32s/it]\n",
      "INFO:transformers.trainer:\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "INFO:transformers.trainer:Saving model checkpoint to tmp\n",
      "INFO:transformers.configuration_utils:Configuration saved in tmp/config.json\n",
      "INFO:transformers.modeling_utils:Model weights saved in tmp/pytorch_model.bin\n",
      "WARNING:transformers.training_args:Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "INFO:transformers.trainer:***** Running Evaluation *****\n",
      "INFO:transformers.trainer:  Num examples = 489\n",
      "INFO:transformers.trainer:  Batch size = 4\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 123/123 [00:08<00:00, 14.41it/s]\n",
      "INFO:transformers.trainer:{'eval_loss': 6.869078837759126, 'epoch': 10.138396624472573, 'step': 3001}\n",
      "INFO:__main__:Eval bpc after pretraining: 9.909985974710416\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'Pretraining roberta-base ... ')\n",
    "\n",
    "training_args.max_steps = 3000   ## <<<<<<<<<<<<<<<<<<<<<<<< REMOVE THIS <<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "pretrain_and_evaluate(training_args, model, tokenizer, eval_only=False, model_path=training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6GOc0_6SbD6u"
   },
   "source": [
    "5) Copy global projection layers. MLM pretraining doesn't train global projections, so we need to call `copy_proj_layers` to copy the local projection layers to the global ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "obupoA0FbD6v",
    "outputId": "059fa657-a3f4-401d-8230-a9997ffaed67"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:Configuration saved in tmp/roberta-base/config.json\n",
      "INFO:transformers.modeling_utils:Model weights saved in tmp/roberta-base/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# logger.info(f'Copying local projection layers into global projection layers ... ')\n",
    "# model = copy_proj_layers(model)\n",
    "# logger.info(f'Saving model to {model_path}')\n",
    "model.save_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jtZ2sfWkbD60"
   },
   "source": [
    "ðŸŽ‰ðŸŽ‰ðŸŽ‰ðŸŽ‰ **DONE**. ðŸŽ‰ðŸŽ‰ðŸŽ‰ðŸŽ‰\n",
    "\n",
    "`model` can now be used for finetuning on downstream tasks after loading it from the disk. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NOBmcHTj3NPz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading the model from tmp/roberta-base\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'RobertaTokenizerFast' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/rscratch/ja/bert/bert-pretrain/longformer/scripts/convert_model_to_long.ipynb Cell 30\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Ba27.millennium.berkeley.edu/rscratch/ja/bert/bert-pretrain/longformer/scripts/convert_model_to_long.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLoading the model from \u001b[39m\u001b[39m{\u001b[39;00mmodel_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Ba27.millennium.berkeley.edu/rscratch/ja/bert/bert-pretrain/longformer/scripts/convert_model_to_long.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m RobertaTokenizerFast\u001b[39m.\u001b[39mfrom_pretrained(model_path)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Ba27.millennium.berkeley.edu/rscratch/ja/bert/bert-pretrain/longformer/scripts/convert_model_to_long.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m RobertaLongForMaskedLM\u001b[39m.\u001b[39mfrom_pretrained(model_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RobertaTokenizerFast' is not defined"
     ]
    }
   ],
   "source": [
    "logger.info(f'Loading the model from {model_path}')\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_path)\n",
    "model = RobertaLongForMaskedLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "convert_model_to_long.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('bert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ff29d5fdd83b2b90871590e39ee23c1cf255be8cb727cf41644f22e608afb29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
