{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca8cd0a9",
   "metadata": {},
   "source": [
    "# Verifying the SpeechT5 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5342da",
   "metadata": {},
   "source": [
    "I needed to do the following to be able to load the original model:\n",
    "\n",
    "- Clone the https://github.com/microsoft/SpeechT5 repo\n",
    "\n",
    "Install stuff:\n",
    "\n",
    "```\n",
    "pip install editdistance\n",
    "pip install -U sacrebleu==1.5.1\n",
    "\n",
    "git submodule update --init SpeechT5/fairseq\n",
    "cd SpeechT5\n",
    "pip install --editable fairseq/\n",
    "pip install espnet\n",
    "```\n",
    "\n",
    "Put this notebook at the same level as the `SpeechT5` repo.\n",
    "\n",
    "Hack the code:\n",
    "\n",
    "- Copy `speecht5/tasks/speecht5.py` into `fairseq/fairseq/tasks`\n",
    "\n",
    "- To run on CPU: In `speecht5/sequence_generator.py`, comment out where it does `.to(device=\"cuda\")`\n",
    "\n",
    "Additional stuff to download:\n",
    "\n",
    "- `dict.txt` from https://drive.google.com/uc?export=download&id=19hcQ58RHZ6CssxF8Qp6yEF1NW_AXxObK\n",
    "\n",
    "- `tokenizer` from https://drive.google.com/uc?export=download&id=1wClgQjXXoU2lmpbaEa1v2SqMbg7cAutq\n",
    "\n",
    "- `speecht5_base_asr.pt` and `t5_transformer_lm.pt` from https://huggingface.co/ajyy/SpeechT5\n",
    "\n",
    "You also need an input audio file, any WAV at 16 kHz will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a4d7c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65661728",
   "metadata": {},
   "source": [
    "Set Python path so it can find the `speecht5` and `fairseq` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f30402f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../SpeechT5/SpeechT5\")\n",
    "sys.path.insert(0, \"../SpeechT5/SpeechT5/fairseq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d398e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b45926",
   "metadata": {},
   "source": [
    "## Load audio and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c18f317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/AUDIO_DIR/dev_clean/1272/141231/1272-141231-0020.flac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63370005",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/AUDIO_DIR/dev_clean/1272/128104/1272-128104-0000.flac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a855d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((93680,), 16000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "wav_data, cur_sample_rate = sf.read(input_file)\n",
    "wav_data.shape, cur_sample_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e60de",
   "metadata": {},
   "source": [
    "NOTE: The `Wav2Vec2FeatureExtractor` does not make sure the audio file is mono. If it has shape `(2, length)` or even `(1, length)` then the output from the feature extractor is incorrect!\n",
    "\n",
    "The `do_normalize` option is False for the SpeechT5 ASR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9699780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(do_normalize=False, return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ea3da78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 93680])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = feature_extractor(wav_data, sampling_rate=cur_sample_rate, padding=True, return_tensors=\"pt\")\n",
    "inputs[\"input_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1a94286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing padding mask\n",
    "inputs[\"attention_mask\"][:, 40000:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62e522e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhAAAADLCAYAAAA/ZqpMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA5nklEQVR4nO3deVxUVf8H8M+wDaACKgiiILjivibiXpK4tFj9etQszac0TZ9UzIUWLc2w9WmzbDPrUXPLrTTUcFcCRVERxV3cQA1hXNnm/P5AhhlmvbMDn/frZTF3ztx7uMPM/d5zvuccmRBCgIiIiEgCF0dXgIiIiCofBhBEREQkGQMIIiIikowBBBEREUnGAIKIiIgkYwBBREREkjGAICIiIskYQBAREZFkDCCIiIhIMgYQREREJJlNA4hdu3bh8ccfR3BwMGQyGdatW2f0NTt27ECnTp0gl8vRtGlTLF682JZVJCIiIjPYNIC4c+cO2rdvjwULFphU/ty5cxg8eDAefvhhpKWlYfLkyXj55ZexefNmW1aTiIiIJJLZazEtmUyGtWvXYsiQIXrLzJgxAxs3bkR6erpq27Bhw5CXl4eEhAQ71JKIiIhM4eboCqhLSkpCdHS0xraYmBhMnjxZ72sKCgpQUFCgeqxUKpGbm4u6detCJpPZqqpERERVjhACt27dQnBwMFxcDHdSOFUAkZ2djcDAQI1tgYGBUCgUuHfvHry8vLReEx8fj3fffddeVSQiIqryLl68iIYNGxos41QBhDni4uIQGxurepyfn4/Q0FBcvHgRPj4+1jlITgZw87x19kXkaEV3gT8ml/48/Rzg6u7Q6hCR81AoFAgJCUGtWrWMlnWqACIoKAg5OTka23JycuDj46Oz9QEA5HI55HK51nYfHx/rBRA+3QB0s86+iBztXh6wdUrpz7VqAW4eDq0OETkfU1IAnGoeiKioKCQmJmps27p1K6KiohxUIyIiItLFpgHE7du3kZaWhrS0NAClwzTT0tKQlZUFoLT7YeTIkary48aNw9mzZzF9+nScOHECX3/9NVauXIkpU6bYsppE1QuTi4nICmwaQBw4cAAdO3ZEx44dAQCxsbHo2LEjZs2aBQC4evWqKpgAgPDwcGzcuBFbt25F+/bt8cknn+CHH35ATEyMLatJVI3ZZRQ3EVVBdpsHwl4UCgV8fX2Rn59vvRwIoqrkfj4wP7T057euAW7aOUREVD1JuYY6VQ4EEdkDuzCIyHIMIIiqs6rVAElEdsQAgoiIiCRjAEFU3WiMwmALBBGZhwEEERERScYAgqjaYRIlEVmOAQRRdcYkSiIyEwMIIiIikowBBFF1wyRKIrICBhBEREQkGQMIIiIikowBBFG1o9aFwSRKIjITAwgiIiKSjAEEUXUj4zwQRGQ5BhBE1Rq7MIjIPAwgiIiISDIGEETVDpMoichyDCCIiIhIMgYQRNUNkyiJyAoYQBBVa+zCICLzMIAgIiIiyRhAEFU77MIgIssxgCCqzjgKg4jMxACCiIiIJGMAQVTdaIzCYAsEEZmHAQQRERFJxgCCqNphEiURWY4BBFF1xiRKIjKTXQKIBQsWICwsDJ6enoiMjERKSoresosXL4ZMJtP45+npaY9qEhERkYlsHkCsWLECsbGxmD17Ng4ePIj27dsjJiYG165d0/saHx8fXL16VfXvwoULtq4mUfXBqayJyApsHkB8+umnGDNmDEaPHo1WrVph4cKF8Pb2xqJFi/S+RiaTISgoSPUvMDDQ1tUkIiIiCWwaQBQWFiI1NRXR0dHlB3RxQXR0NJKSkvS+7vbt22jUqBFCQkLw5JNP4tixY3rLFhQUQKFQaPwjIkPYAkFElrNpAHHjxg2UlJRotSAEBgYiOztb52tatGiBRYsWYf369ViyZAmUSiW6d++OS5cu6SwfHx8PX19f1b+QkBCr/x5EVRaTKInITE43CiMqKgojR45Ehw4d0KdPH6xZswYBAQH49ttvdZaPi4tDfn6+6t/FixftXGMiIqLqx82WO/f394erqytycnI0tufk5CAoKMikfbi7u6Njx444ffq0zuflcjnkcrnFdSWqNphESURWYNMWCA8PD3Tu3BmJiYmqbUqlEomJiYiKijJpHyUlJTh69Cjq169vq2oSVWPswiAi89i0BQIAYmNjMWrUKHTp0gVdu3bFZ599hjt37mD06NEAgJEjR6JBgwaIj48HAMyZMwfdunVD06ZNkZeXh48++ggXLlzAyy+/bOuqEhERkYlsHkAMHToU169fx6xZs5CdnY0OHTogISFBlViZlZUFF5fyhpCbN29izJgxyM7ORu3atdG5c2fs27cPrVq1snVViaoH9S4MJlESkZlkQlStbxCFQgFfX1/k5+fDx8fH0dUhck7v+Jb+//XTQM0Ax9aFiJyGlGuo043CoMrhYu5dXMy96+hqkJP4btcZbD6me2g2EVVNNu/CoKqnoLgEvT7cDgDIfG8A5G6uDq4Rmc/yBsiDWTfx/qYTAIDz8wdbvD8iqhzYAkGSXfinvOUh724Rtp3IwTc7zuDG7QKbH/uNtUcxalEKlMoq1fNWqV1T2P59JyLnwxYIkmzcklTVzxOWHsSBCzcBAFsysrH21R42Pfay5CwAQPqVfLRr6GfTY1VtMnAIJxFZgi0QJNnZ63dUP5cFDwBwKCvPbnUodsIWiGuK+5iyIg2paufE6VWtHGqqxu4Vlji6CtUOAwiyqtw7hTbbd1GJUvXzTRsex1wzfjuCtYcu45lv9jm6KkTVym+pl9ByVgLi1hxB1j9M7rYXBhBkVbYcmXEq57bq5/x7RTY7jil05WCcr0xfXKq5IGzTApF4PAcZV7gyLtnH1FWHAQC/plxE74+2O7g21QcDCLKqLBsGEEcv56l+dmTL+5eJp9B+zhacyrkFADh34w6KSpRcJBtA6oVcHLuSj5d+PoBBX+x2dHWoErpTUOzwGwQyDZMoyar+8+shPN4+2ObHcVT8cE1xH59sPQkAmL3hGJ7v1givLj2IXs38K9kaVbap7DPfJCHQx76L2+3IvIZtJ67hjUEt4enOIcWVmRACrWdvBgCcmDvA6PsphEDKuVx7VI10YAsE2d29whIUFFuW8OSoCVS7vl++MJxMBizeex4AsPvUDcgqVwRRygrnUVlhHzlqwzqnrEiz+Xv14k/78UvSBfz04L1wZrl3CjH2lwNYe+iSo6vilNT/VC7dvGe0/Ir9FzH0u79tWCMyhAEE2dX1WwVoOSsBHd7datF+nGHsgAwyCLWauFTC+MFSSqXAq0sP6n1+7aHLuJJ/3y51uZzn/Dko8ZuOY0tGDqasOIwXfkzGv75NcmhCcGGxEq+vOowNh684rA766f6Uq998zFxz1F6VIR0YQJBeQgjcLiiW/LpLN/V/kff7ZAcA4F6RhUOuTIggrD3ZVOe52kGP+h1TiRMOLdXLSkmUpkweVlJSic6LjV27VX6+dp+6gZRzueg4d6vDWtSW78/C6tRLeO3XQw45fkXG1nmbvvowWryVgHM37mg/qcftgmJkZt+yQu2oIgYQpNfM346izezNOJglbV6DvzJy9D6nuG84IPlo8wmEzdyIQ0aOKYxc+I5cykPbdzZj8d5zBsuZKvdOIf6pcKcok2lefs9cN/1LrTrZkmHdNTLuFhbr7AKrzFNaFKoNUbany2rdBJuOXnVIHfTR9XauPFDa9fPjnrMm7+fRT3ci5rNd+M+vhzhXhJUxgCC9Vhy4CABYsO20pNcdvpSvc3uOQrMpu2KgcTH3LhZsPwMAeOrrffgrIwf/+/sCih98ud4vKv+SNXaxmLbqCO4UluCd3zNUr7dE4nHdQVHF/v/Kw7L+lvtFJZiyIg1z/sgwWvaXpAv4cc85q7TQ3C8qQatZm9F1XqLxwk5IX+vc1bz7Wp8PW1MqBb7dVX4hfnXpQYe1hADaeU2WVEW9heLqgy603w9fQfyfx42+9krePWw5lo0z12/jwPlcq3x/VFUchUE6qX+YpQ6pWnvoMv47tIPqcVGJEkMW7MWxCvMCvPzLAY3Fl8YvTdV6Hij9ohvVPQyzNxxTPafvWiSEwLErCmTmlDdZNn3zT0wf0AKv9m0q6fdQN231EZ3b7Tn7pk2Y+S39360nsfbQZZPKZuXexdw/MuDl7ornIkPNOl6ZM9dL5wIp+5tU76Y6mWPfZur8e0U4mXMLnUJrw9XEBBh9rVR9P94BwL6L0/2cdF5r2/XbBahXy9Mux1d3u6AYAz7bhU6htVXbDLUyyiDDfQPdoOOXpCJhcm+t7VszcjDnyTYG69J9/jatbafmDYS7K++3K+IZIZ0u55U3bR6wYGpmIQT+ysjRCh7K9P1oO/Lvll4M0i/rLjN7wzGkXczT3K+eL5e/jl/DY1/u0dr+YUKmwXreLSzG3ULt7pVrivv47K+TBl9riuz8+yZ1BV3Ju4fBX+zGqgetP7YmIHDzTiG2Z15D7Io03LpvWrC40Yzm7uNXS99fpVLg15QsnMi2bKKpnSevY/KKNNXj/eftO4V4+3e34NmFSWjyxiaTguyyv3NDWryVYI2qmeTd37Vbj7rOS8T8P0/YrQ5l1hy8hEs372kkcxqLbT/765Te507oyXm4amZCb7M3/8S0B5NVUTm2QJBOSgtb7e4XlcDT3RXjlxxEwjH9feDn/7mL9nO2GN3fkAV7NR6/uTYdIyIbaZUzlE2uVAq46LhTLC5RotWs0rHnp+cNhJvancaY/6XicIXgRaqdJ69j1KIU1ePV46LQsr4Pdp28jo6htfHTvnN4sn0DtAr2wXsbM3DsigLTVh/Bs11CLDquPkqlgEwmgwxA3Jp0LM8sv5Nbc+iySUtymzLErqKyBLn1hy8j7kH2vNTlv9enlb+/6ufU3gqLNT8gaw5ewuge4QZf88QC7cBWl92nrsO/phxzfs9A0tl/8Hy3ULw3pK3ZdZVq4c4zmDkwwmb7//PoVSgFMLhdfdU2U1uzyshk2l2iFf2SdB5DH7LeZ2hV6iW0CvYx+j5XJ2yBqKbuF5Xg6KV8vX2eJRb2hU5dWRqtGwoe7G1Vqu67+ptqd4a3KiR5mhs8/JJ0HseulOaCVLzQ/d/CJLSevRnjlx5Et/hEfLvzrGrWxjsFtk/y+s+vh1QXwJ2Z17SeN9Q0bA0b1IKA09duSRot890u05PnbKliTswPu/Un6yqVAkqlwAUTpzp/4ccUDPx8N5LO/gMAWPJ3ltVH+KRf1p2nVObln/fjP78esnpOxL3CEoxfehATlh3UaO3S1RVYMUhTJwOMtpbNWn8M3+207t/Lu79nIO+u863D4ygMIKogIQTm/J6BN9ce1fshfP6HZDz+1R6sPHBR55fT+jRpdwQVmdPEbQ2/G2iBmPGb8THjUr4uDX23zlp/DIO/MO2O0x6KSpQY88sBTFh20Oh7Y6v5sMp2uz3zumpb9Ke7MPDz3djiRIFmRddvFaDT3K34alt5k/n4CnNfqHf5AaWfwcnLD6F7fCJavP0nGr+xyaI6WNrdU2bW+nQM+GwXks78Y7DcX8ev4ffDV7D3tOFyUqmPnjE2lPvJCq2O6mQyGf46rh38VpRyXnuWSkOBiSk6zLFsDpuqhAFEFXPzTiHC4zZh0d5zWJqcheZv/alz6FJZXsOM344i8v1ErWg+T09/rTnzQtjKBwkncFTPiA8pNMeemx5C7Dl9w2iZa7dM73M9fe22xuMvEk/har70roKKbt0vQrM3/8TWjBxsPFIWPMge/Ff797VVIr6+Vq3MnFsY+79UvL0u3TYHttC/vk1C7p1CfLzlpMlDmi/dvId1aVdwJf8+iqwwD8bk5WkW7wMoHRFzIvsW5m0yPhoBsP7kXOqjI97feFzSfA5nr982XsgECgMtF38cccYJtZwXA4gq5uMt2smCLWcl4MI/+j+oN24X4NeULJP2L2XWPF1Jidb0zY4zePyrPaphVuZm4dtyKGbyWdPn6Y/+dCd2niy/O/9060lExW9TdYWYQwiBtu8YzzFRd0bHF/U7G45hxA9/W9SUvuTvLPywW3+T8v/+vqD6+e+z/yBs5kZ0ee8vkxM7Adss865+kXv6630Im7nR6Gus3eVw/XaB3n3m3S3EjNVHsF/H3ba6XDPOjbVnenzq6/Kl7telXcHDH+/ANROHr6q38lRs8ZFiidrfWUUTlznHhFqVBQOIKuamnv65Ph/tMLj+xPubTMu8dnM1vX37Aztlc5d1TTxmZpfBDrUmdWuzRnfA9xb0++v73Qxd3qbrGLK6eN957D39j8ULF7230bQ732EP1je4cbtAUt7Dh5sNj7aRasV+0wJroHSkDQDsOXUDz31v3fUZ8u4W4Uk9SZhz/ziOFQcu4tmFSQb3Mfon6Umn9pgWQn19mYouqq3uqz7R1VYDk9Wp231Ku5XQ0OgNU1m6lk9VwQCiiqmYBKguy8QkLkD/hc9VwhXx5yT9kb41/XawdHY6U2bz05UApVAbgmft70trfAGvSzPcrCqE0DtEcIeOJEl1urow9A25BUq7jUy5A7cmKfOQmNqSZipT8mbK7DxZeq6f/zHZJut/6BvmrKvFqKJVBy7qneDNFBdz72L0TymqobjmMGcWSPXPdLETTYk+8HMuVQ8wgKhSpq8+rDPiLiPlblimb6ZCJ10waoaeiZ4qMmf4oSUMBXTWMn7JQbSfswVHL+UjM/sW1qddVuVypBsIBkyl3vdccT4Oa5u1Pl0rQPklqXQ2UkN9187Akgu0JU4Z6Lr7YfdZhM3cqHciNFP1+nA7tmdeN+nCeSrnFtIv5+PHPec0Arqvd0ib0RYAbqitHeJMS82c5bT1ABhAVCll88TrU/YBNKVJvGKwoRra50QfYnUrTJx4SdckU+rLcB84fxOtZyUYHeZmqjfW2n61wLKhsp8nnkLMZ7swaXkaouJLZ9NL1TMJmChLotQTEKovkjVr/THdhWzgFz2tVudu3MEEA6t+GlNcosSC7adVAZAQQiNhtkQpcPRSvsa0xYbyNXRZlmzd1g9ddLUiqienqudr/LjnnMldRlLrMGpRCvad0b5Z2X7iGh797y489uUezP0jA3FrjqLowTk1p/VCvQXCkdNs62LpaI6qgAFEFWFKtv/m9NILjakZ2Ooi3k7A1owcZNt5vn57UL+GjluSijuFJXjsyz1aoyKc3V9qcxNkK+5b1NUwRW2GR1NGm9jaf349ZLB1zZhlKVn4aHMmhizYCyEEBn6+G+Fxm/DtzjMIm7kRTd7YhMe/2oO31YIlW1x8LRX9351a29TXiFEPGOeasE6JKSqu39H7o+3YefI6nvs+WRUclBm9eL/W68sutAVmXHDVE0etmexsjZV6H/uS3RgMIKoIUxYXktL8rCsZc8wvB/DEV/rHZlc1Yx+sxeEM9C3CZKu7st2nbjjVhDn6piY21amc8mAwR1Gg2l98hUTfX1OyLLq4/GPC8uaWKCxWYp+BgO5/Ota3sNTP+/Tvs9mbfyJs5kYM+nw39ugJ8PLvFeFuYbFZAaB60GDNLgypKwzrcjKnct1g2IJdAogFCxYgLCwMnp6eiIyMREqK4WzgVatWISIiAp6enmjbti02bbJsEhYqlXjCcELdkw/uzgBgzUHLJpJyZrEr0zQe62vGPythjLqt6ZrFUHG/SGM1RSmEqt1F/7fyFhMz3Z2NrqBK/T3uFm842I7/8zj6frTdrGN3fu8vs14nxXM/JGP6at3rMhy+lG/1VT2/NzDLZpmMqwo8/2Oyzue6z9+mmipeKvUGDktnx1W3TEeyrbHJtUibzQOIFStWIDY2FrNnz8bBgwfRvn17xMTE4No13Rezffv2Yfjw4XjppZdw6NAhDBkyBEOGDEF6unNOMlOVHL6Yh/C4TU5152kLaw5e1rgDcdK8UA0jfkhW9duXKAXCZm5Eu3e22HTho0+3WL6ImCPoWiRKXy6ILt/vPofzEkYsOcLKA5f05ulEvp+IYd8ZHtJZWaScK7+oW7O1TdcN0nAzht5ae76PysbmAcSnn36KMWPGYPTo0WjVqhUWLlwIb29vLFq0SGf5zz//HAMGDMC0adPQsmVLzJ07F506dcJXX31l66oadK+wxKxhSPYgpa/elEx2Ry5SZC9Pf71Pb7eAswqP24TwuE1oYuG0yIBpubDZivv4VMfEZM5u8b7zqm6IsvH6hoamVlaPfblH70ysf0uYwMyZfb/7HPLvFSE7/z42HbXudOfFJgz7NqbJG5tUia1KpbBZYmVBcQmEKF1TxZmSSWXChrUpLCyEt7c3Vq9ejSFDhqi2jxo1Cnl5eVi/fr3Wa0JDQxEbG4vJkyerts2ePRvr1q3D4cPazXYFBQUoKCjvd1QoFAgJCUF+fj58fHys8ns8880+SXcw1tawthfq1PBAYbESl2/ewy0nmk6aKqdj8tGoISvAEWU4CuDu6OoQEQBPd1ezFrNzGfQhOnd72Cp1UCgU8PX1NekaatPlvG/cuIGSkhIEBgZqbA8MDMSJE7qbXrOzs3WWz87WHX3Gx8fj3XfftU6F9bhs57kDKrp0857d5y+gqi1b1EET2VW0czHev01EdlICs/oFMuGY1lSbBhD2EBcXh9jYWNXjshYIa9o78xGrNBvrUreGB7o1qYviEiXkbq5wc5Ghdg0PLNp7DkIAri4yvDekDYJ8POHmKoObiwvWp11G6oWbmDkwAt/vPltlmivJfoYVvoVOLpZP6UtEhjWs7Y2Amh7w9XbH2et3UNvbA40DamDXyevoGFobzQNr4uyNO/DxdIe7qwxLJc4nElBTjvfaRtqo9obZNIDw9/eHq6srcnI0s7lzcnIQFBSk8zVBQUGSysvlcsjlcutUWA9XFxnOzx9s02NU9PZjrfQ+F9Wkrurnfi1LW2tMHfN/fv5go2W3TOmN/v/dZdL+KqtJ/ZphyqPNAZQm2D3zzT4jr3C8F7uH4dyNOxoLbpnrOmpjs7KrwTLRLevhh1EP2X3qakv5ernj8Oz+GtsS0rMxbkmqg2pkO8fnDEDLWQmOrobNPBJRD4tefAhA6eq0n261XmJvxe90c//Ozb02DFD7ub3az72fNGt3DmHTJEoPDw907twZiYnlw6aUSiUSExMRFRWl8zVRUVEa5QFg69atesuTdXVpVBt+XlW7Tzzcv4YqeAAgaUlhR3n7sVZ454nW+PnfXXF+/mB8NrSDzY85c2CEzY9hC0tf1r4bC6jl4YCa2Nawh0Lg5eGqtX3b1D5YNa5qfF++3r+F6mdbrpoLABtf6yn5Nfa+sXQ2Nu/CiI2NxahRo9ClSxd07doVn332Ge7cuYPRo0cDAEaOHIkGDRogPj4eADBp0iT06dMHn3zyCQYPHozly5fjwIED+O6772xd1UqtYW0vo3kStTyNv91Lx0TCw7V6zS8WEVTL0VUwalRUI43HT3YIhtzNBfV8PG3WenLXSUcdGdO0Xk2tbR1CajugJrY1Obq5zu2NA2qicYCdK6PH4dn94evljtsFxRj4+S5czDU/l8ua8cNDYdp/D62Dfa13gGrC5leKoUOH4uOPP8asWbPQoUMHpKWlISEhQZUomZWVhatXy5dp7d69O5YtW4bvvvsO7du3x+rVq7Fu3Tq0adPG1lWt1J7sEGy0zDOdGhp8vnFADcjdXCGTyfBq3ybWqprTefuxlhqPWwf7IKSOl1Y5e9zlm0pWYbYrmUyGgW3ro11D233pBdSybdegrXi6a9+Vu7rIcC5+EM7FD8KBt6INvn7eU23w/cguZh3blCDdGhY+3wlBvp52OZY5ZgyIwNn3B8H3QWtmTbkbNr7WS/J+NGeitF4EsWBEJ4v38eMo8/5GqhK73GpOnDgRFy5cQEFBAZKTkxEZWd7EuGPHDixevFij/LPPPovMzEwUFBQgPT0dgwYNskc1K7XX+jUzWqbsGvR/nXUHEkE+5V9Iri66p1da+Hxnk+v0RHvjQY0jPBKhOcpHJpNh9/RHcH7+YGyZ0hsv9wzHwbcfxZCODSw6zrIx1kts0vd+uLu6YP+b0Uh5sx8+H9bBascDgMBapX8P/jWrRvO/TCaDTCaDf005Ds/uDw833V9/IyIboWdTf7OOsWVKb0uqaJKu4XUQ07o8J+y/Q8t70HdNKx/KN/Hhpjaviy49mtbFuD6N4VLhb7amh/TgSj1msOakTfVqWRZ89W0RoMo/q86qV1t1FSZ3077rqsjHs/Ru4P2n2up8vlez8nbPine8ZQa00Z3Mqou1L2j20DywFt56rBXq1LD8otm9iT9e6d3YCrUyLKCWHPVqeSK0jrfV9vlUxwaqC8CSCjkFm8y4k7TU/KfbYnhX642u8vVyx/43tFsikt/oBwA6cwtMUd9XuyXLmmrJ3bDylSiNz+fDLeqVPufphtC65X8DsY/q7uIwR3TLQLSq74OVr+jPrWgf4oekuEew9OVuOr8/KgYUphBqU55ZcyprfZbpyJ/RZdGoh2xck8qBAUQVMtbIxWpgW8MX/5d7hat+lle4O9s5rS9SjTT9VqQvCLGFL4Z3tNuxpKgpN/2uq56FXQYdQ2uruh0ea1cfv43vbva+1OeXa+yvmVPQKtg6E7RJMaxrKB5tZd07Pl9vzWThfTMfQaBaK9zgtvUl7c8eCXV7Zjyitc3P2wOHZ/XH/jc1P5/mXLD1+XePMGya1Atdw+tg9bgojIpqhGkx5QmOz3cLxbpXu1s9gFKPGayxgiYAgzle3U1sebLmua3MKv08EFTOWH94uH8NANDbdOuu9sFyd9X8gDSqW8PC2tnWE+2D8dqvh4yWa+Bn2zvEitz1nGtdPvy/dmgWWAvTVx/GP7cLVStG9mluekZcyhv9cDX/PoKN/J7+NT1w47b+NU/U/0bUf37tkdJmcV8vd+TfMz4tulShdbyx8bWeiP/zBJY9GA+fOLUPAKB3M+tnBgb7euJKfuniUxXPWZ8WAdh49KqulzlMxaDH2PY2DXyQftmyabwXPt9Z48LaJawOuoTVAQC81DMcbi4yuJmYeP3F8I4mfU518fO2TlfarumWzdjYpoH9A2hnxRaIKiTaSJ+cejfH4Vn98dbg8mTC6Jb1NMq62LH1wJ7WTjD/rtwcz0WGmly2b4t6aODnhaUvd8Om13phUNsgeLq74KvnTG9dkclkRoMHAFg+1vAwv6lqw+cAYEp0c/Rq5o9JDzL/9eVkWGrntL6o5emO959qi8Oz+uPvuH5oElDaAmLqRUqKr0Z0go+nG+Y/rd2tp55n4AxMbV5Xt3qc5X/vhrotPd1dJb0vUvOi2jYovykq66qxlKXJpyuMfHaqE7ZAVCG6ss/18fV2x8u9GmP5/ou48M8drS6AyhBAnJ8/GGev30aIhL7/Ola6izFVWd6JVC4uMnw9wvSEVX02TOyBJ77aq7W9sX8N1Pf1xNV83Us/qzflA8CkaM0kXXPjh4w5MQaXdlbv9vL1doevxHU6fhotrW+6U2htpM3qr7NJ2tfM+VBiWgdi8zHrL4VuavO6Ok93V6OtTYa4ObipXv19aVRXWo5P/1aB2HXqOu4XWXeBK28z82OqIrZAVHObJ/fG0Xdi4F0hQ7preB0H1UiaxgE1NbpejHHEOnZDu2gm/z0SUQ9vDNKcpOnF7mE2OXa7hn46t7u4yLBnxiNmjxpoZcaY+dXjorT+ztR9/Gx7vc+Zypy7VEP92Y9ESN/fV891wp+TrJtoaqx10ZAUHcmihvznkfLRG5ttMKpkdI8wjcc/vfiQSXPP1JC74fX+zTElurnOFqOKlAJaw3FrSchJ0uX0vIF2ze1ydmyBqGJ09U0vGxOJhn66o3dXFxlcXbQj6jYNKt+kKhMeboIF288YLCP1o//DyC4YtyQVxRITuD79V/nF8P2n22Jo1xB88OcJ9G8dhJd6liar/rtHOG7dL8b94hKNIbTWVvGOuGwSHVcXGZoHak+iZcocCB8/2w5d5yUaLaeurN9cXVLcIziVcxsXcu/qHV7sSAuf74zmb/1ptJz6++fu6oKW9X2wbkIPDFlQ2vrzat8m+HqH4b9NW3FxkWHntL74fvdZjO3VBBlX8zFuyUGdZaNbBmJydHP0bOqPf+4UqrqPrGn2463xYvcwzPjtCMb3bYo+zQOwZ8bD6Pq+8b+niY+Ut4Qdu6LA//6+oLesUgiNkWWAdvCii673alDbILQO9rVJN1plxrNRxfw2Xrt/rnsTf43hXfbQJMD+SZfPRTYyWkbqF0B0q0Ccfl/3PCQVWxHUqa9X4uoiQ6fQ2ljxSpQqeCirS+0aHqjv62XTu5r/qH3p9mhaVyv/oeIkZKaMdrBkHH1/tf3X9/VC7+YBeKGb8fcOsP/kXroSjlPfitYaMit0tG11CPHDkpcisem1Xpg+IAIn3xtos3oa06huDbw3pC1C63ojpnUQ3n2itUZLQ5kfRnWBq4sMkY3rYpDEUShS67N8bJQqQbiejyc2TOwhaR/6ksHL6Jw3woTP2fQBERpzn6x8JQpfj+iMCQ6aV8OZMYCoYprWq4WMOTGqJvEwKwQO5syJUNaCYey1855qgx5N6xosYyp3I/21Q0yYrVOfeB1NpmN765+t01BTvb21rO+D9g19EezriZ9e7KqVADlPz7wgtlLfgiS2ipN79Wpm3oRPlqhbU66Vyf/O4611lu3ZzF817NXYBc9eZDIZRnUPw9T+LTBSbYr0OAevfdKuoR/+3aM8wFYfJmqOsnU0TswdYKSktr0zH8HSlyPx5fCOlaY71xGc4y+arMrbww3vPNEaqW9FY8uUPmbvZ/+b0Zg5MAKbJ0vvBy0bv715cm+9d41NAmpgRGQj/GjGpCzqI0jKGBub/WwX8yciaiuxS8fcBDxbcHWRYd2EHtgX10/nRaym3A1vDio9n+qzGhpTW8/QQV3U1/KwtLVFfYrtumoBqq0acdQXpkqYrJ3bsGFiDww08W7d2Fwt+oxRm6PFVl7p4/jp62cMLA8apExaV9F7Q9qg7YNh7erJ5RXnt9FH7uaKHk398biTzqbrLBhAVGF1a8otuusJqCXHuD5NzFoToWxtiYBacr1TQpcFGZ7urpIn4dE1YY2hi3b7hr7oYeb0xEDpehld1frwy+aTaFjbvvNKmMvYRXtM78Y4P38wnupoeh6ClGGOY614cdozQ/PuP/LBHeL/GVnrxVwPhdXBmfcHIW3Wo4gIKp8DYPnYbljwXCe9iaq6zBxg+l3+L/8uXW59RGQoIhtbp5Wuolb1nWtOA1cJUeCQDvqnmq+4QN6MARFo39AXo2yUrFxdOU87K1UpFfsLh3YJwYoDFzULVfiueKlnOH7cc07vPsf0Csf3u0uf19Xn7O7qAm8PV41VJL09XJH61qNmT02sqqpMhpXjohA2cyOA0oACKP2Sf+STnRbtu7KS0k2jPoHX+L5NsOHwFTxrZtJkxWnbvx/VBbtOXke/CNutTeDqItOayKibGRd1U6+Po3uEoXfzAGTM0R4hZU3PdgnBvaISp2mml9I61bahL3o188fuUze0nqs48d34vk0wvgovEOgobIEgm6j4pafrw1uxa2OGgbuzOU+21kjqkukZTzGiwsRNGyb2sDh4UFc2Brzvg+GCjW2QpV5Z6ErCAzS7FXQJ9PHEgTejETdIuxvKVJP6NUPdGh6Y2r8FfDzd8Vi7YKu+z7ZiygWygZ8XZj/IqbB1Lo2riwyje4RX2qWsF4/uii91TGNfWVeSrWwYQJBdhPnXwMn3BuLE3AHo3qQuJjzcRKvp18PNBQfeioaHq4vGuPAxvcLxfGQjdAytrdrWQE/XwesVEq+a1tMepmiJHa/3xbcvdMbQh8rzKQZa0FdbmdWu4YHz8wdrdT9VnHRKF0vXEpjyaHPsfzNa0iRilQWnGShlytpZri4yPN4+WGeSM9keuzBIshe6NTI4/lrfBbUsH2PZmG56X+tfU46T80qHu205lo2s3Lt4qWe41p2bvu9YU1YltUQ9H0+tvv9vnu+M1Au5eHNtumr9iuqs4qRAC5/vZJPjVNYFjXo29cee09rN7mWqcwBh7q8+vGsohncNxS9J5xHm5Ov2VCUMIEgyQ19wyW/0s3hVyTL9LVyL4OlO+pOsrK1zozqY2r8FxvxyAFE2SnhzZttf74vNx7Ix/KFQyN1dsCwlC0cu5aN38wCnW1PC0Ywt6NaoTvW9AKp/t/h4Sb88jYwKs15lyCgGEGRVFddQcCRz16Ew16OtArFzWl+TFrOqasL9a2Cc2kiLDRN7OrA2zu31mBYaCcXTYlrgo82ZAIDB7erj7cGtHFU1h5PJZPh+ZBfcLSy2aLIysg/mQJDV6Bojb21lcw80C9SfvFg2udDwrqavhGktjerWkLQ2B1U/AbXkWDG2vBtPfeG6Bc91sni1yMru0VaBeNLAEE1yHmyBIMlqqw1ne+fxVnjn9wxEt6ynMUbeVpLfiEaxUmkwO/3n0V1x634xfCVMdERkT+rzOlTSVA4iBhAk3djejXHsSj4eiQjEc5GheLGH7WfJK+Ph5gIPIw1nLi4yBg9UacS0DkLCsWzUrcGhh1S5yIQwZbBM5aFQKODr64v8/Hz4+DjXLGtERGVO5tzCP7cLEdWkLoQQXCaanIKUayhbIIiIHKB5YC3gweSZDB6oMmK2FxEREUnGAIKIiIgkYwBBREREkjGAICIiIskYQBAREZFkNg0gcnNzMWLECPj4+MDPzw8vvfQSbt++bfA1ffv2hUwm0/g3btw4W1aTiIiIJLLpMM4RI0bg6tWr2Lp1K4qKijB69GiMHTsWy5YtM/i6MWPGYM6cOarH3t5Vb8leIiKiysxmAcTx48eRkJCA/fv3o0uXLgCAL7/8EoMGDcLHH3+M4OBgva/19vZGUBBX8CMiInJWNuvCSEpKgp+fnyp4AIDo6Gi4uLggOTnZ4GuXLl0Kf39/tGnTBnFxcbh7966tqklERERmsFkLRHZ2NurVq6d5MDc31KlTB9nZ2Xpf99xzz6FRo0YIDg7GkSNHMGPGDGRmZmLNmjU6yxcUFKCgoED1WKFQWOcXICIiIr0kBxAzZ87EBx98YLDM8ePHza7Q2LFjVT+3bdsW9evXR79+/XDmzBk0adJEq3x8fDzeffdds49HRERE0kkOIKZOnYoXX3zRYJnGjRsjKCgI165d09heXFyM3NxcSfkNkZGRAIDTp0/rDCDi4uIQGxureqxQKBASEmLy/omIiEg6yQFEQEAAAgICjJaLiopCXl4eUlNT0blzZwDAtm3boFQqVUGBKdLS0gAA9evX1/m8XC6HXM5lcImIiOzJZkmULVu2xIABAzBmzBikpKRg7969mDhxIoYNG6YagXH58mVEREQgJSUFAHDmzBnMnTsXqampOH/+PDZs2ICRI0eid+/eaNeuna2qSkRERBLZdCKppUuXIiIiAv369cOgQYPQs2dPfPfdd6rni4qKkJmZqRpl4eHhgb/++gv9+/dHREQEpk6dimeeeQa///67LatJREREEsmEEMLRlbAmhUIBX19f5Ofnw8fHx9HVISIiqjSkXEO5FgYRERFJxgCCiIiIJGMAQURERJIxgCAiIiLJGEAQERGRZAwgiIiISDIGEERERCQZAwgiIiKSjAEEERERScYAgoiIiCRjAEFERESSMYAgIiIiyRhAEBERkWQMIIiIiEgyBhBEREQkGQMIIiIikowBBBEREUnGAIKIiIgkYwBBREREkjGAICIiIskYQBAREZFkDCCIiIhIMgYQREREJBkDCCIiIpKMAQQRERFJxgCCiIiIJGMAQURERJIxgCAiIiLJbBZAzJs3D927d4e3tzf8/PxMeo0QArNmzUL9+vXh5eWF6OhonDp1ylZVJCIiIjPZLIAoLCzEs88+i/Hjx5v8mg8//BBffPEFFi5ciOTkZNSoUQMxMTG4f/++rapJREREZpAJIYQtD7B48WJMnjwZeXl5BssJIRAcHIypU6fi9ddfBwDk5+cjMDAQixcvxrBhw0w6nkKhgK+vL/Lz8+Hj42Np9YmIiKoNKddQNzvVyahz584hOzsb0dHRqm2+vr6IjIxEUlKS3gCioKAABQUFqsf5+fkASk8CERERma7s2mlK24LTBBDZ2dkAgMDAQI3tgYGBqud0iY+Px7vvvqu1PSQkxLoVJCIiqiZu3boFX19fg2UkBRAzZ87EBx98YLDM8ePHERERIWW3FomLi0NsbKzqsVKpRG5uLurWrQuZTGaVYygUCoSEhODixYvsFrExnmv74Hm2D55n++B5th4hBG7duoXg4GCjZSUFEFOnTsWLL75osEzjxo2l7FIlKCgIAJCTk4P69eurtufk5KBDhw56XyeXyyGXyzW2mTrqQyofHx/+cdoJz7V98DzbB8+zffA8W4exlocykgKIgIAABAQEmFUhY8LDwxEUFITExERVwKBQKJCcnCxpJAcRERHZns2GcWZlZSEtLQ1ZWVkoKSlBWloa0tLScPv2bVWZiIgIrF27FgAgk8kwefJkvPfee9iwYQOOHj2KkSNHIjg4GEOGDLFVNYmIiMgMNkuinDVrFn7++WfV444dOwIAtm/fjr59+wIAMjMzVaMmAGD69Om4c+cOxo4di7y8PPTs2RMJCQnw9PS0VTVNIpfLMXv2bK2uErI+nmv74Hm2D55n++B5dgybzwNBREREVQ/XwiAiIiLJGEAQERGRZAwgiIiISDIGEERERCQZAwgTLFiwAGFhYfD09ERkZCRSUlIcXSWnER8fj4ceegi1atVCvXr1MGTIEGRmZmqUuX//PiZMmIC6deuiZs2aeOaZZ5CTk6NRJisrC4MHD4a3tzfq1auHadOmobi4WKPMjh070KlTJ8jlcjRt2hSLFy/Wqk91ea/mz5+vGvpchufZOi5fvoznn38edevWhZeXF9q2bYsDBw6onhdCYNasWahfvz68vLwQHR2NU6dOaewjNzcXI0aMgI+PD/z8/PDSSy9pDGEHgCNHjqBXr17w9PRESEgIPvzwQ626rFq1ChEREfD09ETbtm2xadMm2/zSDlBSUoK3334b4eHh8PLyQpMmTTB37lyNNRh4rp2cIIOWL18uPDw8xKJFi8SxY8fEmDFjhJ+fn8jJyXF01ZxCTEyM+Omnn0R6erpIS0sTgwYNEqGhoeL27duqMuPGjRMhISEiMTFRHDhwQHTr1k10795d9XxxcbFo06aNiI6OFocOHRKbNm0S/v7+Ii4uTlXm7NmzwtvbW8TGxoqMjAzx5ZdfCldXV5GQkKAqU13eq5SUFBEWFibatWsnJk2apNrO82y53Nxc0ahRI/Hiiy+K5ORkcfbsWbF582Zx+vRpVZn58+cLX19fsW7dOnH48GHxxBNPiPDwcHHv3j1VmQEDBoj27duLv//+W+zevVs0bdpUDB8+XPV8fn6+CAwMFCNGjBDp6eni119/FV5eXuLbb79Vldm7d69wdXUVH374ocjIyBBvvfWWcHd3F0ePHrXPybCxefPmibp164o//vhDnDt3TqxatUrUrFlTfP7556oyPNfOjQGEEV27dhUTJkxQPS4pKRHBwcEiPj7egbVyXteuXRMAxM6dO4UQQuTl5Ql3d3exatUqVZnjx48LACIpKUkIIcSmTZuEi4uLyM7OVpX55ptvhI+PjygoKBBCCDF9+nTRunVrjWMNHTpUxMTEqB5Xh/fq1q1bolmzZmLr1q2iT58+qgCC59k6ZsyYIXr27Kn3eaVSKYKCgsRHH32k2paXlyfkcrn49ddfhRBCZGRkCABi//79qjJ//vmnkMlk4vLly0IIIb7++mtRu3Zt1XkvO3aLFi1Uj//1r3+JwYMHaxw/MjJSvPLKK5b9kk5i8ODB4t///rfGtqefflqMGDFCCMFzXRmwC8OAwsJCpKamaiwx7uLigujoaCQlJTmwZs6rbGKwOnXqAABSU1NRVFSkcQ4jIiIQGhqqOodJSUlo27atxkqsMTExUCgUOHbsmKqM+j7KypTto7q8VxMmTMDgwYO1zgXPs3Vs2LABXbp0wbPPPot69eqhY8eO+P7771XPnzt3DtnZ2Rq/v6+vLyIjIzXOs5+fH7p06aIqEx0dDRcXFyQnJ6vK9O7dGx4eHqoyMTExyMzMxM2bN1VlDL0XlV337t2RmJiIkydPAgAOHz6MPXv2YODAgQB4risDp1nO2xnduHEDJSUlOpcYP3HihINq5byUSiUmT56MHj16oE2bNgBKl2n38PDQWuBMfZn27Oxsnee47DlDZRQKBe7du4ebN29W+fdq+fLlOHjwIPbv36/1HM+zdZw9exbffPMNYmNj8cYbb2D//v147bXX4OHhgVGjRqnOk67fX/0c1qtXT+N5Nzc31KlTR6NMeHi41j7Knqtdu7be96JsH5XdzJkzoVAoEBERAVdXV5SUlGDevHkYMWIEAPBcVwIMIMhqJkyYgPT0dOzZs8fRValyLl68iEmTJmHr1q0On9q9KlMqlejSpQvef/99AKVT8Kenp2PhwoUYNWqUg2tXtaxcuRJLly7FsmXL0Lp1a6SlpWHy5MkIDg7mua4k2IVhgL+/P1xdXbUy2XNyclTLj1OpiRMn4o8//sD27dvRsGFD1fagoCAUFhYiLy9Po7z6OQwKCtJ5jsueM1TGx8cHXl5eVf69Sk1NxbVr19CpUye4ubnBzc0NO3fuxBdffAE3NzcEBgbyPFtB/fr10apVK41tLVu2RFZWFoDy82To9w8KCsK1a9c0ni8uLkZubq5V3ouqcJ4BYNq0aZg5cyaGDRuGtm3b4oUXXsCUKVMQHx8PgOe6MmAAYYCHhwc6d+6MxMRE1TalUonExERERUU5sGbOQwiBiRMnYu3atdi2bZtWU2Hnzp3h7u6ucQ4zMzORlZWlOodRUVE4evSoxhfB1q1b4ePjo/oyj4qK0thHWZmyfVT196pfv344evSoalXbtLQ0dOnSBSNGjFD9zPNsuR49emgNQz558iQaNWoEAAgPD0dQUJDG769QKJCcnKxxnvPy8pCamqoqs23bNiiVSkRGRqrK7Nq1C0VFRaoyW7duRYsWLVC7dm1VGUPvRWV39+5duLhoXoJcXV2hVCoB8FxXCo7O4nR2y5cvF3K5XCxevFhkZGSIsWPHCj8/P41M9ups/PjxwtfXV+zYsUNcvXpV9e/u3buqMuPGjROhoaFi27Zt4sCBAyIqKkpERUWpni8bXti/f3+RlpYmEhISREBAgM7hhdOmTRPHjx8XCxYs0Dm8sDq9V+qjMITgebaGlJQU4ebmJubNmydOnTolli5dKry9vcWSJUtUZebPny/8/PzE+vXrxZEjR8STTz6pc2hhx44dRXJystizZ49o1qyZxtDCvLw8ERgYKF544QWRnp4uli9fLry9vbWGFrq5uYmPP/5YHD9+XMyePbtKDS0cNWqUaNCggWoY55o1a4S/v7+YPn26qgzPtXNjAGGCL7/8UoSGhgoPDw/RtWtX8ffffzu6Sk4DgM5/P/30k6rMvXv3xKuvvipq164tvL29xVNPPSWuXr2qsZ/z58+LgQMHCi8vL+Hv7y+mTp0qioqKNMps375ddOjQQXh4eIjGjRtrHKNMdXqvKgYQPM/W8fvvv4s2bdoIuVwuIiIixHfffafxvFKpFG+//bYIDAwUcrlc9OvXT2RmZmqU+eeff8Tw4cNFzZo1hY+Pjxg9erS4deuWRpnDhw+Lnj17CrlcLho0aCDmz5+vVZeVK1eK5s2bCw8PD9G6dWuxceNG6//CDqJQKMSkSZNEaGio8PT0FI0bNxZvvvmmxnBLnmvnxuW8iYiISDLmQBAREZFkDCCIiIhIMgYQREREJBkDCCIiIpKMAQQRERFJxgCCiIiIJGMAQURERJIxgCAiIiLJGEAQERGRZAwgiIiISDIGEERERCQZAwgiIiKS7P8BLunbSsr/XeIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 600x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 2))\n",
    "plt.plot(inputs[\"input_values\"][0])\n",
    "plt.plot(inputs[\"attention_mask\"][0])\n",
    "plt.ylim(-1, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f594f3",
   "metadata": {},
   "source": [
    "Transform multiple inputs into a single padded batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbb99431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128632,), 16000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file2 = \"/Users/matthijs/Documents/FILES/HuggingFace/S2S/textless/AUDIO_DIR/selfdestruct.wav\"\n",
    "wav_data2, cur_sample_rate2 = sf.read(input_file2)\n",
    "wav_data2.shape, cur_sample_rate2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1daee2cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128632])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs2 = feature_extractor([wav_data, wav_data2], sampling_rate=cur_sample_rate, padding=True, return_tensors=\"pt\")\n",
    "inputs2[\"input_values\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb653d87",
   "metadata": {},
   "source": [
    "The original model used a `padding_mask` as input, where False means no padding. The `Wav2Vec2FeatureExtractor` can return an `attention_mask`, where 1 means no padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d3ab7e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs2[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "7163b1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs2   # use the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2205900b",
   "metadata": {},
   "source": [
    "## Load the Transformers model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc8c0f8",
   "metadata": {},
   "source": [
    "To convert the original checkpoint weights to Transformers:\n",
    "\n",
    "First download the checkpoint. I used `speecht5_base_asr.pt` from https://huggingface.co/ajyy/SpeechT5\n",
    "\n",
    "Then run the following, using your own `--checkpoint_path` and `--pytorch_dump_folder_path`:\n",
    "\n",
    "```nohighlight\n",
    "cd transformers/src/transformers/models/speecht5\n",
    "\n",
    "python convert_speecht5_original_pytorch_checkpoint_to_pytorch.py \\\n",
    "  --task s2t \\\n",
    "  --checkpoint_path /path/to/SpeechT5/speecht5_base_asr.pt \n",
    "  --pytorch_dump_folder_path /some/other/path\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "552fd307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    SpeechT5Config, \n",
    "    SpeechT5CTCTokenizer,\n",
    "    SpeechT5Processor,\n",
    "    SpeechT5Model, \n",
    "    SpeechT5ForConditionalGeneration, \n",
    "    SpeechT5ForCTC, \n",
    "    Wav2Vec2FeatureExtractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69aa6aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SpeechT5Config()\n",
    "hf_model = SpeechT5Model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ba8f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_base_asr\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2884601a",
   "metadata": {},
   "source": [
    "Note that loading should work OK for both the base class `SpeechT5Model` and `SpeechT5ForCTC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5df4bac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model = SpeechT5ForConditionalGeneration.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7dd7606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeechT5ForConditionalGeneration(\n",
       "  (speecht5): SpeechT5Model(\n",
       "    (encoder): SpeechT5SpeechEncoder(\n",
       "      (prenet): SpeechT5SpeechEncoderPrenet(\n",
       "        (feature_encoder): SpeechT5FeatureEncoder(\n",
       "          (conv_layers): ModuleList(\n",
       "            (0): SpeechT5GroupNormConvLayer(\n",
       "              (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "              (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "            )\n",
       "            (1): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (2): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (3): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (4): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (5): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (6): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (feature_projection): SpeechT5FeatureProjection(\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (pos_conv_embed): SpeechT5PositionalConvEmbedding(\n",
       "          (conv): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "          (padding): SpeechT5SamePadLayer()\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (embed_positions): SpeechT5SinusoidalPositionalEmbedding()\n",
       "      )\n",
       "      (wrapped_encoder): SpeechT5Encoder(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (embed_positions): SpeechT5RelativePositionalEncoding(\n",
       "          (pe_k): Embedding(320, 64)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): SpeechT5TextDecoder(\n",
       "      (prenet): SpeechT5TextDecoderPrenet(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (embed_tokens): Embedding(81, 768, padding_idx=1)\n",
       "        (embed_positions): SpeechT5SinusoidalPositionalEmbedding()\n",
       "      )\n",
       "      (wrapped_decoder): SpeechT5Decoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (text_decoder_postnet): SpeechT5TextDecoderPostnet(\n",
       "    (lm_head): Linear(in_features=768, out_features=81, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "09c1c4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_base_asr were not used when initializing SpeechT5Model: ['speecht5.encoder.wrapped_encoder.layers.11.attention.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.feed_forward.output_dense.weight', 'speecht5.encoder.wrapped_encoder.layers.6.feed_forward.intermediate_dense.weight', 'speecht5.encoder.wrapped_encoder.layers.9.attention.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn_layer_norm.weight', 'speecht5.encoder.wrapped_encoder.layers.9.feed_forward.output_dense.bias', 'speecht5.decoder.prenet.embed_tokens.weight', 'speecht5.encoder.wrapped_encoder.layers.5.attention.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.3.final_layer_norm.weight', 'speecht5.encoder.wrapped_encoder.layers.8.final_layer_norm.weight', 'speecht5.encoder.wrapped_encoder.layers.10.attention.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn_layer_norm.weight', 'speecht5.encoder.wrapped_encoder.layers.0.attention.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn.v_proj.bias', 'speecht5.encoder.prenet.pos_conv_embed.conv.weight_v', 'speecht5.encoder.wrapped_encoder.layers.3.feed_forward.intermediate_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.feed_forward.intermediate_dense.weight', 'speecht5.encoder.wrapped_encoder.layers.3.feed_forward.output_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.2.feed_forward.output_dense.bias', 'speecht5.encoder.wrapped_encoder.layers.3.attention.k_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.5.layer_norm.weight', 'speecht5.encoder.wrapped_encoder.layers.7.attention.out_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.4.feed_forward.intermediate_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn.v_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.11.feed_forward.output_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn.q_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.0.feed_forward.intermediate_dense.weight', 'speecht5.encoder.wrapped_encoder.layers.2.attention.out_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.2.attention.q_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.6.attention.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn.v_proj.weight', 'speecht5.encoder.prenet.feature_encoder.conv_layers.3.conv.weight', 'speecht5.decoder.wrapped_decoder.layers.0.final_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn_layer_norm.weight', 'speecht5.encoder.wrapped_encoder.layers.0.feed_forward.output_dense.weight', 'speecht5.encoder.wrapped_encoder.layers.7.attention.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn.k_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.11.final_layer_norm.weight', 'speecht5.encoder.wrapped_encoder.layers.3.feed_forward.output_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.4.feed_forward.intermediate_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn_layer_norm.bias', 'speecht5.encoder.wrapped_encoder.layers.0.feed_forward.intermediate_dense.bias', 'speecht5.encoder.wrapped_encoder.layers.1.final_layer_norm.weight', 'speecht5.encoder.wrapped_encoder.layers.3.attention.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.0.final_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn.k_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.7.attention.v_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.11.feed_forward.intermediate_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.feed_forward.intermediate_dense.bias', 'speecht5.encoder.wrapped_encoder.layers.11.layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn_layer_norm.weight', 'speecht5.encoder.wrapped_encoder.layers.7.attention.v_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.11.feed_forward.output_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn.out_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.0.layer_norm.weight', 'speecht5.encoder.wrapped_encoder.layers.1.attention.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn_layer_norm.weight', 'speecht5.encoder.wrapped_encoder.layers.4.attention.q_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.4.final_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.0.feed_forward.intermediate_dense.weight', 'speecht5.encoder.wrapped_encoder.layers.4.layer_norm.weight', 'speecht5.encoder.wrapped_encoder.layers.7.feed_forward.intermediate_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn.v_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.10.attention.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn.k_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.7.feed_forward.output_dense.weight', 'speecht5.encoder.wrapped_encoder.layers.2.layer_norm.weight', 'speecht5.encoder.wrapped_encoder.layers.2.final_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn_layer_norm.bias', 'speecht5.encoder.wrapped_encoder.layers.5.attention.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn.q_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.2.feed_forward.intermediate_dense.bias', 'speecht5.encoder.wrapped_encoder.layers.2.final_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn_layer_norm.weight', 'speecht5.encoder.wrapped_encoder.layers.4.attention.q_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.9.final_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.0.feed_forward.output_dense.bias', 'speecht5.encoder.wrapped_encoder.layers.3.attention.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn.out_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.1.attention.out_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.10.feed_forward.output_dense.bias', 'speecht5.encoder.prenet.feature_encoder.conv_layers.5.conv.weight', 'speecht5.encoder.wrapped_encoder.layers.1.attention.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn.v_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.6.feed_forward.output_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn.k_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.2.feed_forward.output_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.feed_forward.intermediate_dense.bias', 'speecht5.encoder.wrapped_encoder.layers.1.feed_forward.intermediate_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn.q_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.4.layer_norm.bias', 'speecht5.encoder.wrapped_encoder.layers.3.attention.out_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.1.feed_forward.output_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn.out_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.9.attention.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.final_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn.out_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.11.attention.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn.v_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.3.layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn.q_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.9.attention.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.5.feed_forward.intermediate_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn.k_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.8.attention.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn.v_proj.weight', 'speecht5.encoder.prenet.feature_encoder.conv_layers.1.conv.weight', 'speecht5.encoder.wrapped_encoder.layers.4.attention.out_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.7.attention.out_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.1.attention.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn_layer_norm.bias', 'speecht5.encoder.wrapped_encoder.layers.4.feed_forward.intermediate_dense.weight', 'speecht5.encoder.wrapped_encoder.layers.11.attention.v_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.2.attention.k_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.5.attention.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn.v_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.6.attention.v_proj.weight', 'speecht5.encoder.prenet.embed_positions.weights', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn.q_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.1.attention.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.4.final_layer_norm.weight', 'speecht5.encoder.prenet.pos_conv_embed.conv.bias', 'speecht5.encoder.wrapped_encoder.layers.6.attention.q_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.2.attention.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn_layer_norm.weight', 'speecht5.encoder.wrapped_encoder.layers.3.attention.v_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.8.attention.v_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.1.feed_forward.intermediate_dense.bias', 'speecht5.encoder.wrapped_encoder.layers.7.attention.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn.out_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.7.final_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.5.feed_forward.intermediate_dense.weight', 'speecht5.encoder.wrapped_encoder.layers.5.final_layer_norm.bias', 'speecht5.encoder.wrapped_encoder.embed_positions.pe_k.weight', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn.v_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.0.attention.out_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.8.attention.q_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.7.attention.q_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.11.attention.k_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.9.attention.out_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.9.layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn.v_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.11.attention.out_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.5.final_layer_norm.weight', 'speecht5.encoder.wrapped_encoder.layers.7.layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn.out_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.6.layer_norm.bias', 'speecht5.encoder.wrapped_encoder.layers.0.final_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn.k_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.11.layer_norm.weight', 'speecht5.encoder.wrapped_encoder.layers.2.attention.k_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.7.feed_forward.intermediate_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.5.feed_forward.output_dense.weight', 'speecht5.encoder.wrapped_encoder.layers.3.attention.q_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.1.attention.k_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.1.attention.k_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.5.layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.4.final_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn.q_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.11.attention.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn.q_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.2.feed_forward.intermediate_dense.weight', 'speecht5.encoder.wrapped_encoder.layers.3.final_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn.v_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.0.final_layer_norm.bias', 'speecht5.encoder.wrapped_encoder.layers.10.final_layer_norm.bias', 'speecht5.encoder.wrapped_encoder.layers.10.feed_forward.intermediate_dense.bias', 'speecht5.encoder.wrapped_encoder.layers.10.attention.v_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.0.attention.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.final_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.2.feed_forward.intermediate_dense.bias', 'speecht5.encoder.wrapped_encoder.layers.3.attention.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn.k_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.1.feed_forward.output_dense.weight', 'speecht5.encoder.wrapped_encoder.layers.5.feed_forward.intermediate_dense.weight', 'speecht5.encoder.wrapped_encoder.layers.5.attention.out_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.8.attention.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.1.final_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.4.feed_forward.output_dense.weight', 'speecht5.encoder.wrapped_encoder.layers.2.attention.v_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.5.attention.q_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.0.attention.q_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.4.attention.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn.k_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.10.attention.v_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.4.final_layer_norm.weight', 'speecht5.encoder.prenet.feature_encoder.conv_layers.4.conv.weight', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn.k_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.3.feed_forward.intermediate_dense.bias', 'speecht5.encoder.wrapped_encoder.layers.2.layer_norm.bias', 'speecht5.encoder.wrapped_encoder.layers.8.feed_forward.intermediate_dense.bias', 'speecht5.encoder.wrapped_encoder.layers.8.attention.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn.q_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.4.feed_forward.output_dense.weight', 'speecht5.encoder.prenet.feature_projection.projection.bias', 'speecht5.encoder.wrapped_encoder.layers.9.feed_forward.intermediate_dense.bias', 'speecht5.encoder.wrapped_encoder.layers.8.feed_forward.output_dense.weight', 'speecht5.encoder.wrapped_encoder.layers.8.feed_forward.output_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn.q_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.6.attention.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn.v_proj.weight', 'speecht5.encoder.wrapped_encoder.layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.5.feed_forward.output_dense.bias', 'speecht5.encoder.wrapped_encoder.layers.5.attention.out_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.2.attention.q_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.0.attention.v_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.10.layer_norm.weight', 'speecht5.encoder.prenet.feature_projection.projection.weight', 'speecht5.encoder.wrapped_encoder.layers.1.layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn.q_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.5.feed_forward.intermediate_dense.bias', 'speecht5.encoder.wrapped_encoder.layers.6.final_layer_norm.bias', 'speecht5.encoder.wrapped_encoder.layers.0.attention.k_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.7.attention.k_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.10.feed_forward.intermediate_dense.weight', 'speecht5.encoder.wrapped_encoder.layers.8.final_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.2.feed_forward.output_dense.weight', 'speecht5.encoder.wrapped_encoder.layers.1.attention.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.3.feed_forward.output_dense.weight', 'speecht5.encoder.wrapped_encoder.layers.11.final_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn.q_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.9.attention.q_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.4.feed_forward.output_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn.q_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.10.attention.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn.v_proj.bias', 'speecht5.encoder.prenet.feature_encoder.conv_layers.0.layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.4.feed_forward.output_dense.bias', 'speecht5.encoder.wrapped_encoder.layers.8.attention.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn.v_proj.weight', 'speecht5.encoder.prenet.feature_encoder.conv_layers.0.layer_norm.weight', 'speecht5.encoder.wrapped_encoder.layers.0.feed_forward.output_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.0.feed_forward.output_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.1.final_layer_norm.weight', 'speecht5.encoder.wrapped_encoder.layers.11.attention.k_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.5.attention.k_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.10.attention.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.final_layer_norm.weight', 'speecht5.encoder.wrapped_encoder.layers.4.attention.v_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.11.feed_forward.intermediate_dense.weight', 'speecht5.encoder.wrapped_encoder.layers.8.attention.k_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.9.feed_forward.output_dense.weight', 'speecht5.encoder.wrapped_encoder.layers.10.feed_forward.output_dense.weight', 'speecht5.encoder.wrapped_encoder.layers.1.final_layer_norm.bias', 'speecht5.encoder.wrapped_encoder.layers.6.layer_norm.weight', 'speecht5.encoder.wrapped_encoder.layers.10.final_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn.v_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.9.attention.k_proj.bias', 'speecht5.encoder.prenet.feature_encoder.conv_layers.0.conv.weight', 'speecht5.encoder.wrapped_encoder.layers.2.attention.out_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.10.layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.4.feed_forward.intermediate_dense.bias', 'speecht5.encoder.wrapped_encoder.layers.5.attention.v_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.8.layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn.k_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.6.attention.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn.v_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.6.feed_forward.output_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.5.final_layer_norm.weight', 'speecht5.encoder.wrapped_encoder.layers.3.attention.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.feed_forward.output_dense.bias', 'speecht5.encoder.wrapped_encoder.layers.3.final_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn.k_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.7.feed_forward.output_dense.bias', 'speecht5.encoder.wrapped_encoder.layers.4.attention.v_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.2.feed_forward.output_dense.bias', 'speecht5.encoder.wrapped_encoder.layers.10.attention.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn.q_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.3.layer_norm.weight', 'speecht5.encoder.wrapped_encoder.layers.10.attention.out_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.9.attention.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.3.final_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn_layer_norm.weight', 'text_decoder_postnet.lm_head.weight', 'speecht5.encoder.wrapped_encoder.layers.1.layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn.q_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.6.attention.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn.out_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.9.layer_norm.bias', 'speecht5.encoder.wrapped_encoder.layers.6.attention.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn.out_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.8.layer_norm.bias', 'speecht5.encoder.wrapped_encoder.layers.7.final_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn.k_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.9.feed_forward.intermediate_dense.weight', 'speecht5.encoder.wrapped_encoder.layers.6.attention.k_proj.weight', 'speecht5.encoder.prenet.pos_conv_embed.conv.weight_g', 'speecht5.encoder.wrapped_encoder.layers.5.feed_forward.output_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn.v_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.6.feed_forward.intermediate_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn_layer_norm.weight', 'speecht5.encoder.prenet.feature_encoder.conv_layers.6.conv.weight', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn_layer_norm.weight', 'speecht5.decoder.prenet.embed_positions.weights', 'speecht5.encoder.wrapped_encoder.layers.4.attention.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.feed_forward.intermediate_dense.weight', 'speecht5.encoder.wrapped_encoder.layers.8.attention.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn.k_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.8.feed_forward.intermediate_dense.weight', 'speecht5.encoder.wrapped_encoder.layers.0.attention.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.3.feed_forward.output_dense.bias', 'speecht5.encoder.wrapped_encoder.layers.9.final_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.0.feed_forward.intermediate_dense.bias', 'speecht5.encoder.prenet.feature_projection.layer_norm.bias', 'speecht5.encoder.wrapped_encoder.layers.4.attention.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn.k_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.7.layer_norm.bias', 'speecht5.encoder.wrapped_encoder.layers.0.layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn.out_proj.bias', 'speecht5.encoder.prenet.feature_encoder.conv_layers.2.conv.weight', 'speecht5.encoder.wrapped_encoder.layers.0.attention.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn_layer_norm.bias', 'speecht5.encoder.wrapped_encoder.layer_norm.bias', 'speecht5.encoder.wrapped_encoder.layers.6.final_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn.q_proj.weight', 'speecht5.encoder.wrapped_encoder.layers.11.attention.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.feed_forward.intermediate_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn.v_proj.bias', 'speecht5.encoder.prenet.feature_projection.layer_norm.weight', 'speecht5.encoder.prenet.masked_spec_embed', 'speecht5.encoder.wrapped_encoder.layers.9.attention.v_proj.bias', 'speecht5.encoder.wrapped_encoder.layers.5.feed_forward.output_dense.bias']\n",
      "- This IS expected if you are initializing SpeechT5Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SpeechT5Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SpeechT5Model were not initialized from the model checkpoint at /Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_base_asr and are newly initialized: ['speecht5.encoder.layers.1.feed_forward.intermediate_dense.weight', 'speecht5.decoder.layers.5.feed_forward.intermediate_dense.weight', 'speecht5.encoder.layers.4.attention.v_proj.weight', 'speecht5.encoder.layers.5.attention.k_proj.weight', 'speecht5.encoder.layers.2.attention.out_proj.weight', 'speecht5.decoder.layers.2.self_attn.v_proj.weight', 'speecht5.encoder.layers.8.final_layer_norm.weight', 'speecht5.decoder.layers.3.encoder_attn.q_proj.bias', 'speecht5.encoder.layers.3.attention.k_proj.weight', 'speecht5.encoder.layers.0.final_layer_norm.bias', 'speecht5.decoder.layers.0.encoder_attn.k_proj.bias', 'speecht5.encoder.layers.6.attention.q_proj.weight', 'speecht5.encoder.layers.7.attention.k_proj.bias', 'speecht5.encoder.layers.10.feed_forward.intermediate_dense.weight', 'speecht5.encoder.layers.0.layer_norm.weight', 'speecht5.encoder.layers.0.attention.k_proj.bias', 'speecht5.encoder.layers.7.layer_norm.weight', 'speecht5.encoder.layers.3.attention.q_proj.bias', 'speecht5.decoder.layers.4.self_attn_layer_norm.weight', 'speecht5.encoder.layers.7.feed_forward.output_dense.weight', 'speecht5.decoder.layers.4.self_attn.out_proj.weight', 'speecht5.decoder.layers.4.encoder_attn.v_proj.bias', 'speecht5.encoder.layers.1.attention.out_proj.weight', 'speecht5.decoder.layers.5.feed_forward.output_dense.bias', 'speecht5.decoder.layers.0.self_attn.k_proj.bias', 'speecht5.decoder.layers.3.encoder_attn.k_proj.bias', 'speecht5.encoder.layers.11.feed_forward.intermediate_dense.weight', 'speecht5.decoder.layers.4.encoder_attn.k_proj.bias', 'speecht5.decoder.layers.5.self_attn_layer_norm.bias', 'speecht5.decoder.layers.2.self_attn.out_proj.bias', 'speecht5.decoder.layers.3.encoder_attn.out_proj.bias', 'speecht5.decoder.layers.5.encoder_attn.v_proj.weight', 'speecht5.encoder.layers.11.attention.q_proj.bias', 'speecht5.encoder.layers.6.feed_forward.output_dense.bias', 'speecht5.decoder.layers.4.self_attn_layer_norm.bias', 'speecht5.decoder.layers.3.encoder_attn.v_proj.weight', 'speecht5.decoder.layers.1.self_attn.out_proj.bias', 'speecht5.decoder.layers.1.encoder_attn.k_proj.weight', 'speecht5.decoder.layers.3.feed_forward.output_dense.bias', 'speecht5.encoder.layers.8.layer_norm.bias', 'speecht5.encoder.layers.2.final_layer_norm.bias', 'speecht5.encoder.layers.11.attention.k_proj.weight', 'speecht5.decoder.layers.2.encoder_attn_layer_norm.bias', 'speecht5.decoder.layers.4.feed_forward.intermediate_dense.bias', 'speecht5.encoder.layers.9.feed_forward.intermediate_dense.bias', 'speecht5.encoder.layers.7.final_layer_norm.weight', 'speecht5.decoder.layers.2.encoder_attn_layer_norm.weight', 'speecht5.encoder.layers.11.attention.out_proj.weight', 'speecht5.encoder.layers.11.layer_norm.weight', 'speecht5.encoder.layers.9.attention.k_proj.weight', 'speecht5.encoder.layers.0.attention.v_proj.weight', 'speecht5.encoder.layers.1.attention.k_proj.bias', 'speecht5.encoder.layers.10.attention.v_proj.weight', 'speecht5.decoder.layers.4.encoder_attn.v_proj.weight', 'speecht5.decoder.layers.1.feed_forward.output_dense.bias', 'speecht5.encoder.layers.3.layer_norm.bias', 'speecht5.encoder.layers.7.attention.out_proj.bias', 'speecht5.encoder.layers.8.attention.k_proj.bias', 'speecht5.encoder.layers.9.final_layer_norm.weight', 'speecht5.encoder.layers.0.attention.q_proj.bias', 'speecht5.encoder.layers.7.attention.out_proj.weight', 'speecht5.decoder.layers.5.encoder_attn.k_proj.weight', 'speecht5.encoder.layers.3.feed_forward.output_dense.weight', 'speecht5.decoder.layers.3.self_attn.q_proj.bias', 'speecht5.encoder.layers.2.feed_forward.intermediate_dense.bias', 'speecht5.decoder.layers.2.encoder_attn.v_proj.bias', 'speecht5.encoder.layers.4.feed_forward.intermediate_dense.bias', 'speecht5.decoder.layers.5.self_attn_layer_norm.weight', 'speecht5.encoder.layers.1.attention.v_proj.weight', 'speecht5.encoder.layers.5.final_layer_norm.weight', 'speecht5.encoder.layers.9.attention.q_proj.bias', 'speecht5.encoder.layers.0.layer_norm.bias', 'speecht5.encoder.layers.10.final_layer_norm.bias', 'speecht5.decoder.layers.1.self_attn_layer_norm.bias', 'speecht5.encoder.layers.3.attention.out_proj.weight', 'speecht5.decoder.layers.3.self_attn.k_proj.bias', 'speecht5.decoder.layers.4.self_attn.out_proj.bias', 'speecht5.decoder.layers.4.encoder_attn.k_proj.weight', 'speecht5.decoder.layers.3.feed_forward.output_dense.weight', 'speecht5.encoder.layers.2.feed_forward.output_dense.weight', 'speecht5.decoder.layers.5.self_attn.k_proj.weight', 'speecht5.decoder.layers.1.feed_forward.output_dense.weight', 'speecht5.decoder.layers.2.encoder_attn.k_proj.bias', 'speecht5.decoder.layers.4.self_attn.q_proj.weight', 'speecht5.encoder.layers.7.attention.q_proj.bias', 'speecht5.encoder.layers.8.feed_forward.intermediate_dense.bias', 'speecht5.decoder.layers.5.final_layer_norm.weight', 'speecht5.encoder.layers.1.attention.q_proj.bias', 'speecht5.decoder.layers.0.self_attn.q_proj.bias', 'speecht5.decoder.layers.2.self_attn.k_proj.bias', 'speecht5.decoder.layers.3.self_attn_layer_norm.bias', 'speecht5.encoder.layers.0.final_layer_norm.weight', 'speecht5.decoder.layers.3.encoder_attn_layer_norm.weight', 'speecht5.encoder.layers.6.attention.k_proj.weight', 'speecht5.encoder.layers.8.attention.out_proj.weight', 'speecht5.encoder.layer_norm.bias', 'speecht5.decoder.layers.1.encoder_attn_layer_norm.bias', 'speecht5.encoder.layers.2.attention.v_proj.weight', 'speecht5.decoder.layers.0.encoder_attn.v_proj.bias', 'speecht5.encoder.layers.2.attention.k_proj.bias', 'speecht5.decoder.layers.5.feed_forward.output_dense.weight', 'speecht5.encoder.layers.1.attention.out_proj.bias', 'speecht5.encoder.layers.0.attention.v_proj.bias', 'speecht5.encoder.layers.5.attention.v_proj.bias', 'speecht5.decoder.layers.3.final_layer_norm.bias', 'speecht5.encoder.layers.11.feed_forward.output_dense.weight', 'speecht5.decoder.layers.5.final_layer_norm.bias', 'speecht5.encoder.layers.6.attention.out_proj.weight', 'speecht5.decoder.layers.1.self_attn.v_proj.weight', 'speecht5.decoder.layers.5.self_attn.k_proj.bias', 'speecht5.encoder.layers.9.attention.out_proj.weight', 'speecht5.encoder.layers.3.final_layer_norm.bias', 'speecht5.encoder.layers.11.attention.k_proj.bias', 'speecht5.encoder.layers.7.final_layer_norm.bias', 'speecht5.encoder.layers.10.attention.q_proj.bias', 'speecht5.decoder.layers.1.encoder_attn.q_proj.bias', 'speecht5.decoder.layers.3.self_attn.q_proj.weight', 'speecht5.decoder.layers.1.encoder_attn.v_proj.weight', 'speecht5.encoder.layers.8.attention.q_proj.bias', 'speecht5.decoder.layers.5.encoder_attn.q_proj.weight', 'speecht5.encoder.layers.0.attention.q_proj.weight', 'speecht5.encoder.layers.5.layer_norm.bias', 'speecht5.decoder.layers.4.feed_forward.intermediate_dense.weight', 'speecht5.decoder.layers.0.encoder_attn.out_proj.bias', 'speecht5.encoder.layers.5.feed_forward.intermediate_dense.bias', 'speecht5.decoder.layers.5.self_attn.v_proj.weight', 'speecht5.encoder.layers.8.attention.q_proj.weight', 'speecht5.encoder.layers.4.attention.out_proj.bias', 'speecht5.decoder.layers.2.self_attn.q_proj.bias', 'speecht5.encoder.layers.4.feed_forward.intermediate_dense.weight', 'speecht5.decoder.layers.0.final_layer_norm.weight', 'speecht5.decoder.layers.5.encoder_attn.out_proj.bias', 'speecht5.encoder.layers.4.feed_forward.output_dense.weight', 'speecht5.decoder.layers.0.self_attn_layer_norm.weight', 'speecht5.encoder.layers.4.attention.q_proj.bias', 'speecht5.decoder.layers.2.self_attn.q_proj.weight', 'speecht5.encoder.layers.5.feed_forward.output_dense.bias', 'speecht5.decoder.layers.3.feed_forward.intermediate_dense.weight', 'speecht5.decoder.layers.5.self_attn.out_proj.bias', 'speecht5.decoder.layers.2.encoder_attn.v_proj.weight', 'speecht5.encoder.layers.7.attention.k_proj.weight', 'speecht5.encoder.layers.10.attention.out_proj.weight', 'speecht5.encoder.layers.2.feed_forward.output_dense.bias', 'speecht5.encoder.layers.4.attention.v_proj.bias', 'speecht5.encoder.layers.6.layer_norm.bias', 'speecht5.decoder.layers.1.encoder_attn_layer_norm.weight', 'speecht5.decoder.layers.1.self_attn_layer_norm.weight', 'speecht5.encoder.layers.10.attention.q_proj.weight', 'speecht5.decoder.layers.2.feed_forward.intermediate_dense.bias', 'speecht5.encoder.layers.3.final_layer_norm.weight', 'speecht5.encoder.layers.10.feed_forward.output_dense.bias', 'speecht5.decoder.layers.0.final_layer_norm.bias', 'speecht5.encoder.layer_norm.weight', 'speecht5.encoder.layers.8.final_layer_norm.bias', 'speecht5.decoder.layers.1.self_attn.k_proj.weight', 'speecht5.encoder.layers.7.attention.v_proj.weight', 'speecht5.encoder.layers.0.feed_forward.intermediate_dense.weight', 'speecht5.encoder.layers.0.attention.out_proj.weight', 'speecht5.decoder.layers.3.self_attn.out_proj.weight', 'speecht5.decoder.layers.4.self_attn.v_proj.weight', 'speecht5.encoder.layers.9.layer_norm.weight', 'speecht5.encoder.layers.2.layer_norm.bias', 'speecht5.decoder.layers.2.self_attn.k_proj.weight', 'speecht5.encoder.layers.10.final_layer_norm.weight', 'speecht5.decoder.layers.1.encoder_attn.v_proj.bias', 'speecht5.decoder.layers.2.self_attn_layer_norm.weight', 'speecht5.decoder.layers.1.self_attn.q_proj.bias', 'speecht5.encoder.layers.9.attention.v_proj.bias', 'speecht5.decoder.layers.0.self_attn.out_proj.bias', 'speecht5.decoder.layers.3.self_attn_layer_norm.weight', 'speecht5.encoder.layers.5.attention.q_proj.weight', 'speecht5.encoder.layers.9.feed_forward.output_dense.bias', 'speecht5.encoder.layers.3.attention.v_proj.weight', 'speecht5.encoder.layers.3.attention.out_proj.bias', 'speecht5.encoder.layers.2.attention.out_proj.bias', 'speecht5.decoder.layers.2.encoder_attn.k_proj.weight', 'speecht5.decoder.layers.5.encoder_attn.out_proj.weight', 'speecht5.encoder.layers.9.final_layer_norm.bias', 'speecht5.encoder.layers.10.layer_norm.weight', 'speecht5.decoder.layers.5.encoder_attn.q_proj.bias', 'speecht5.decoder.layers.2.self_attn_layer_norm.bias', 'speecht5.decoder.layers.2.feed_forward.output_dense.bias', 'speecht5.decoder.layers.0.encoder_attn.k_proj.weight', 'speecht5.decoder.layers.2.encoder_attn.out_proj.weight', 'speecht5.decoder.layers.4.final_layer_norm.weight', 'speecht5.decoder.layers.1.feed_forward.intermediate_dense.weight', 'speecht5.decoder.layers.1.self_attn.v_proj.bias', 'speecht5.encoder.layers.6.layer_norm.weight', 'speecht5.encoder.layers.5.feed_forward.intermediate_dense.weight', 'speecht5.decoder.layers.4.encoder_attn_layer_norm.weight', 'speecht5.encoder.layers.1.layer_norm.weight', 'speecht5.encoder.layers.5.layer_norm.weight', 'speecht5.encoder.layers.1.attention.v_proj.bias', 'speecht5.encoder.layers.10.attention.k_proj.bias', 'speecht5.decoder.layers.0.feed_forward.output_dense.weight', 'speecht5.decoder.layers.0.feed_forward.intermediate_dense.weight', 'speecht5.decoder.layers.1.encoder_attn.out_proj.bias', 'speecht5.encoder.layers.4.attention.q_proj.weight', 'speecht5.encoder.layers.6.feed_forward.intermediate_dense.bias', 'speecht5.decoder.layers.5.self_attn.v_proj.bias', 'speecht5.encoder.layers.10.feed_forward.output_dense.weight', 'speecht5.decoder.layers.4.encoder_attn_layer_norm.bias', 'speecht5.decoder.layers.0.encoder_attn.out_proj.weight', 'speecht5.encoder.layers.8.layer_norm.weight', 'speecht5.encoder.layers.10.feed_forward.intermediate_dense.bias', 'speecht5.decoder.layers.3.final_layer_norm.weight', 'speecht5.encoder.layers.2.layer_norm.weight', 'speecht5.encoder.layers.2.final_layer_norm.weight', 'speecht5.encoder.layers.0.attention.out_proj.bias', 'speecht5.encoder.layers.9.attention.q_proj.weight', 'speecht5.encoder.layers.2.feed_forward.intermediate_dense.weight', 'speecht5.encoder.layers.11.attention.v_proj.bias', 'speecht5.encoder.layers.8.attention.k_proj.weight', 'speecht5.encoder.layers.10.layer_norm.bias', 'speecht5.decoder.layers.2.self_attn.v_proj.bias', 'speecht5.decoder.layers.0.self_attn.k_proj.weight', 'speecht5.decoder.layers.2.feed_forward.intermediate_dense.weight', 'speecht5.decoder.layers.3.encoder_attn.k_proj.weight', 'speecht5.decoder.layers.5.feed_forward.intermediate_dense.bias', 'speecht5.encoder.layers.7.feed_forward.intermediate_dense.bias', 'speecht5.encoder.layers.5.attention.out_proj.bias', 'speecht5.decoder.layers.2.encoder_attn.q_proj.bias', 'speecht5.encoder.layers.11.attention.out_proj.bias', 'speecht5.encoder.layers.5.final_layer_norm.bias', 'speecht5.encoder.layers.4.attention.out_proj.weight', 'speecht5.decoder.layers.1.self_attn.out_proj.weight', 'speecht5.encoder.layers.11.layer_norm.bias', 'speecht5.decoder.layers.1.self_attn.k_proj.bias', 'speecht5.encoder.layers.4.layer_norm.weight', 'speecht5.decoder.layers.4.feed_forward.output_dense.bias', 'speecht5.encoder.layers.5.attention.out_proj.weight', 'speecht5.encoder.layers.11.attention.q_proj.weight', 'speecht5.decoder.layers.3.self_attn.v_proj.bias', 'speecht5.encoder.layers.5.attention.v_proj.weight', 'speecht5.encoder.layers.8.attention.out_proj.bias', 'speecht5.decoder.layers.4.feed_forward.output_dense.weight', 'speecht5.encoder.layers.1.final_layer_norm.bias', 'speecht5.encoder.layers.10.attention.k_proj.weight', 'speecht5.encoder.layers.11.attention.v_proj.weight', 'speecht5.decoder.layers.2.encoder_attn.q_proj.weight', 'speecht5.decoder.layers.3.encoder_attn.v_proj.bias', 'speecht5.decoder.layers.1.final_layer_norm.bias', 'speecht5.decoder.layers.4.self_attn.k_proj.bias', 'speecht5.decoder.layers.1.self_attn.q_proj.weight', 'speecht5.decoder.layers.3.self_attn.out_proj.bias', 'speecht5.decoder.layers.2.feed_forward.output_dense.weight', 'speecht5.encoder.layers.6.attention.q_proj.bias', 'speecht5.decoder.layers.4.self_attn.q_proj.bias', 'speecht5.decoder.layers.0.self_attn.v_proj.bias', 'speecht5.encoder.layers.0.feed_forward.output_dense.weight', 'speecht5.encoder.layers.8.attention.v_proj.bias', 'speecht5.encoder.layers.9.feed_forward.intermediate_dense.weight', 'speecht5.encoder.layers.5.attention.k_proj.bias', 'speecht5.decoder.layers.0.self_attn.out_proj.weight', 'speecht5.decoder.layers.2.final_layer_norm.weight', 'speecht5.encoder.layers.4.feed_forward.output_dense.bias', 'speecht5.encoder.layers.7.feed_forward.intermediate_dense.weight', 'speecht5.encoder.layers.11.feed_forward.intermediate_dense.bias', 'speecht5.decoder.layers.1.encoder_attn.k_proj.bias', 'speecht5.encoder.layers.1.layer_norm.bias', 'speecht5.encoder.layers.5.attention.q_proj.bias', 'speecht5.encoder.layers.9.layer_norm.bias', 'speecht5.decoder.layers.5.self_attn.q_proj.bias', 'speecht5.encoder.layers.11.feed_forward.output_dense.bias', 'speecht5.encoder.layers.1.feed_forward.output_dense.weight', 'speecht5.encoder.layers.9.attention.k_proj.bias', 'speecht5.encoder.layers.6.attention.out_proj.bias', 'speecht5.decoder.layers.4.self_attn.k_proj.weight', 'speecht5.encoder.layers.2.attention.v_proj.bias', 'speecht5.encoder.layers.0.attention.k_proj.weight', 'speecht5.encoder.layers.10.attention.v_proj.bias', 'speecht5.encoder.layers.8.feed_forward.intermediate_dense.weight', 'speecht5.encoder.layers.1.attention.k_proj.weight', 'speecht5.encoder.layers.6.attention.k_proj.bias', 'speecht5.encoder.layers.1.feed_forward.intermediate_dense.bias', 'speecht5.encoder.layers.1.final_layer_norm.weight', 'speecht5.encoder.layers.6.final_layer_norm.bias', 'speecht5.decoder.layers.0.feed_forward.output_dense.bias', 'speecht5.decoder.layers.0.encoder_attn.q_proj.bias', 'speecht5.encoder.layers.11.final_layer_norm.weight', 'speecht5.decoder.layers.1.final_layer_norm.weight', 'speecht5.encoder.layers.4.final_layer_norm.weight', 'speecht5.encoder.layers.6.final_layer_norm.weight', 'speecht5.decoder.layers.3.encoder_attn_layer_norm.bias', 'speecht5.decoder.layers.5.self_attn.out_proj.weight', 'speecht5.decoder.layers.1.feed_forward.intermediate_dense.bias', 'speecht5.encoder.layers.4.final_layer_norm.bias', 'speecht5.encoder.layers.6.attention.v_proj.weight', 'speecht5.decoder.layers.0.feed_forward.intermediate_dense.bias', 'speecht5.encoder.embed_positions.pe_k.weight', 'speecht5.decoder.layers.2.final_layer_norm.bias', 'speecht5.decoder.layers.0.self_attn.v_proj.weight', 'speecht5.encoder.layers.6.feed_forward.output_dense.weight', 'speecht5.decoder.layers.0.encoder_attn_layer_norm.weight', 'speecht5.decoder.layers.3.feed_forward.intermediate_dense.bias', 'speecht5.encoder.layers.1.feed_forward.output_dense.bias', 'speecht5.encoder.layers.9.attention.v_proj.weight', 'speecht5.encoder.layers.9.feed_forward.output_dense.weight', 'speecht5.decoder.layers.0.self_attn_layer_norm.bias', 'speecht5.encoder.layers.7.attention.v_proj.bias', 'speecht5.encoder.layers.9.attention.out_proj.bias', 'speecht5.encoder.layers.7.attention.q_proj.weight', 'speecht5.decoder.layers.1.encoder_attn.q_proj.weight', 'speecht5.encoder.layers.7.layer_norm.bias', 'speecht5.decoder.layers.4.encoder_attn.q_proj.weight', 'speecht5.encoder.layers.11.final_layer_norm.bias', 'speecht5.decoder.layers.4.self_attn.v_proj.bias', 'speecht5.decoder.layers.0.self_attn.q_proj.weight', 'speecht5.decoder.layers.0.encoder_attn.q_proj.weight', 'speecht5.decoder.layers.0.encoder_attn.v_proj.weight', 'speecht5.decoder.layers.3.self_attn.k_proj.weight', 'speecht5.decoder.layers.4.final_layer_norm.bias', 'speecht5.encoder.layers.8.feed_forward.output_dense.bias', 'speecht5.encoder.layers.3.feed_forward.intermediate_dense.bias', 'speecht5.encoder.layers.5.feed_forward.output_dense.weight', 'speecht5.encoder.layers.7.feed_forward.output_dense.bias', 'speecht5.encoder.layers.4.attention.k_proj.bias', 'speecht5.encoder.layers.8.attention.v_proj.weight', 'speecht5.encoder.layers.0.feed_forward.output_dense.bias', 'speecht5.decoder.layers.5.self_attn.q_proj.weight', 'speecht5.decoder.layers.5.encoder_attn_layer_norm.weight', 'speecht5.decoder.layers.3.self_attn.v_proj.weight', 'speecht5.decoder.layers.4.encoder_attn.out_proj.weight', 'speecht5.encoder.layers.3.attention.k_proj.bias', 'speecht5.encoder.layers.4.attention.k_proj.weight', 'speecht5.encoder.layers.1.attention.q_proj.weight', 'speecht5.encoder.layers.4.layer_norm.bias', 'speecht5.decoder.layers.4.encoder_attn.out_proj.bias', 'speecht5.encoder.layers.3.layer_norm.weight', 'speecht5.encoder.layers.3.attention.q_proj.weight', 'speecht5.decoder.layers.3.encoder_attn.out_proj.weight', 'speecht5.decoder.layers.5.encoder_attn.k_proj.bias', 'speecht5.decoder.layers.0.encoder_attn_layer_norm.bias', 'speecht5.decoder.layers.3.encoder_attn.q_proj.weight', 'speecht5.encoder.layers.6.feed_forward.intermediate_dense.weight', 'speecht5.encoder.layers.3.feed_forward.intermediate_dense.weight', 'speecht5.encoder.layers.0.feed_forward.intermediate_dense.bias', 'speecht5.decoder.layers.1.encoder_attn.out_proj.weight', 'speecht5.encoder.layers.3.attention.v_proj.bias', 'speecht5.decoder.layers.5.encoder_attn.v_proj.bias', 'speecht5.encoder.layers.2.attention.q_proj.bias', 'speecht5.encoder.layers.2.attention.k_proj.weight', 'speecht5.decoder.layers.5.encoder_attn_layer_norm.bias', 'speecht5.decoder.layers.2.self_attn.out_proj.weight', 'speecht5.encoder.layers.10.attention.out_proj.bias', 'speecht5.encoder.layers.3.feed_forward.output_dense.bias', 'speecht5.encoder.layers.2.attention.q_proj.weight', 'speecht5.decoder.layers.2.encoder_attn.out_proj.bias', 'speecht5.decoder.layers.4.encoder_attn.q_proj.bias', 'speecht5.encoder.layers.6.attention.v_proj.bias', 'speecht5.encoder.layers.8.feed_forward.output_dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "hf_model_naked = SpeechT5Model.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "7628cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the attention layer weights are correct\n",
    "# for i in range(len(hf_model.speecht5.encoder.layers)):\n",
    "#     print(i, \"k_proj weight\", torch.all(hf_model.speecht5.encoder.layers[i].attention.k_proj.weight == orig_model.encoder.layers[i].self_attn.k_proj.weight))\n",
    "#     print(i, \"k_proj bias\", torch.all(hf_model.speecht5.encoder.layers[i].attention.k_proj.bias == orig_model.encoder.layers[i].self_attn.k_proj.bias))\n",
    "#     print(i, \"v_proj weight\", torch.all(hf_model.speecht5.encoder.layers[i].attention.v_proj.weight == orig_model.encoder.layers[i].self_attn.v_proj.weight))\n",
    "#     print(i, \"v_proj bias\", torch.all(hf_model.speecht5.encoder.layers[i].attention.v_proj.bias == orig_model.encoder.layers[i].self_attn.v_proj.bias))\n",
    "#     print(i, \"q_proj weight\", torch.all(hf_model.speecht5.encoder.layers[i].attention.q_proj.weight == orig_model.encoder.layers[i].self_attn.q_proj.weight))\n",
    "#     print(i, \"q_proj bias\", torch.all(hf_model.speecht5.encoder.layers[i].attention.q_proj.bias == orig_model.encoder.layers[i].self_attn.q_proj.bias))\n",
    "#     print(i, \"out_proj weight\", torch.all(hf_model.speecht5.encoder.layers[i].attention.out_proj.weight == orig_model.encoder.layers[i].self_attn.out_proj.weight))\n",
    "#     print(i, \"out_proj bias\", torch.all(hf_model.speecht5.encoder.layers[i].attention.out_proj.bias == orig_model.encoder.layers[i].self_attn.out_proj.bias))\n",
    "#     print(\"---\")\n",
    "\n",
    "# print(\"pos_emb weight\", torch.all(hf_model.speecht5.encoder.pos_emb.pe_k.weight == orig_model.encoder.pos_emb.pe_k.weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82233c52",
   "metadata": {},
   "source": [
    "Run a single forward pass. This should run the encoder, decoder, and the relevant pre- and postnets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29bd2f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.encoder.prenet(**inputs)\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e1f8501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using no attention_mask\n",
    "# with torch.no_grad():\n",
    "#      hf_outputs = hf_model.speech_encoder_prenet(input_values=inputs[\"input_values\"])\n",
    "\n",
    "# type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "359778a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97a77055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_outputs[\"extract_features\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7db34f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_outputs[\"extract_features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41afd19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_outputs[\"hidden_states\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fcef1339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_outputs[\"hidden_states\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d01d920d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 401, 768]), torch.Size([2, 401])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in hf_outputs if hasattr(x, \"shape\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d5ff106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6.9419e+00,  6.0887e+00,  5.0580e+00,  ...,  5.8203e-01,\n",
       "           2.5973e+00, -1.8086e-01],\n",
       "         [ 7.1978e+00,  1.6047e+00,  9.1351e+00,  ...,  1.2387e+00,\n",
       "           2.8825e+00,  3.4502e+00],\n",
       "         [ 3.0047e+00, -7.6195e-01,  1.1497e+01,  ...,  1.3287e+00,\n",
       "           2.6715e+00,  2.3741e+00],\n",
       "         ...,\n",
       "         [-1.2140e-01, -9.2100e-02, -1.5428e-01,  ..., -2.4324e-01,\n",
       "          -2.2983e-01, -2.0718e-01],\n",
       "         [ 4.0435e-02, -9.1658e-02, -1.5430e-01,  ..., -2.5205e-01,\n",
       "          -2.0630e-01, -2.5674e-01],\n",
       "         [ 8.2007e-03, -1.3189e-01, -1.6461e-01,  ..., -2.0494e-01,\n",
       "          -3.0676e-01, -3.1394e-01]],\n",
       "\n",
       "        [[ 1.4333e+00, -1.8173e+00,  9.1156e-01,  ...,  7.4279e-01,\n",
       "          -3.9927e-01,  1.0221e+00],\n",
       "         [ 5.6487e-01, -2.0062e+00,  2.2312e-02,  ...,  6.6407e-01,\n",
       "           9.8154e-02,  1.4818e+00],\n",
       "         [-2.7547e-01, -2.7482e+00, -7.6423e-01,  ...,  4.7994e-01,\n",
       "           2.4423e-01,  1.1358e+00],\n",
       "         ...,\n",
       "         [ 1.2788e-01, -1.4121e+00, -8.5273e-01,  ...,  5.4640e-01,\n",
       "          -2.7472e-01,  8.5026e-01],\n",
       "         [ 6.6787e-02, -1.0637e+00, -1.1532e+00,  ...,  5.7377e-01,\n",
       "          -1.1603e-01,  1.2345e+00],\n",
       "         [ 5.1934e-01, -1.9056e+00, -5.7697e-01,  ...,  4.9755e-01,\n",
       "           1.5985e-01,  1.6285e+00]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d24920f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_encoder_input = hf_outputs[0]\n",
    "hf_encoder_attention_mask = hf_outputs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64823660",
   "metadata": {},
   "source": [
    "## Load the original model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1697ab0",
   "metadata": {},
   "source": [
    "Load the dictionary. This adds `<s>, <pad>, </s>, <unk>` tokens to the front and `<mask>` and `<ctc_blank>` to the end. **dict.txt** was [downloaded from here](https://drive.google.com/uc?export=download&id=19hcQ58RHZ6CssxF8Qp6yEF1NW_AXxObK). This is the Vocabulary link from the main SpeechT5 README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24e0f251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size: 81\n"
     ]
    }
   ],
   "source": [
    "from fairseq.data import Dictionary\n",
    "tgt_dict = Dictionary.load(\"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/DATA_ROOT/dict.txt\")\n",
    "tgt_dict.add_symbol(\"<mask>\")\n",
    "tgt_dict.add_symbol(\"<ctc_blank>\")\n",
    "print(f\"dictionary size: \" f\"{len(tgt_dict):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85935250",
   "metadata": {},
   "source": [
    "To load the model we need the `SpeechT5Task` object but constructing it is annoying. Fortunately, `build_model` only reads two properties from the task object, so we can fake it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea4b4380",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeTask:\n",
    "    def __init__(self):\n",
    "        self.dicts = { \"text\": tgt_dict }\n",
    "        self.t5_task = \"s2t\"\n",
    "        \n",
    "task = FakeTask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dc6c7a",
   "metadata": {},
   "source": [
    "Load the fine-tuned ASR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a79fbfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from speecht5.models.speecht5 import T5TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b9b2818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"../weights/SpeechT5/speecht5_base_asr.pt\")\n",
    "\n",
    "orig_model = T5TransformerModel.build_model(checkpoint[\"cfg\"][\"model\"], task)\n",
    "\n",
    "orig_model.load_state_dict(checkpoint[\"model\"])\n",
    "orig_model = orig_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bda7a201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speecht5.models.speecht5.T5TransformerModel"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(orig_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "64d53b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speecht5.models.modules.encoder.TransformerEncoder"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(orig_model.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bad44136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(checkpoint[\"model\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c1faddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fairseq.data.encoders.sentencepiece_bpe.SentencepieceBPE"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fairseq.data import encoders\n",
    "from argparse import Namespace\n",
    "tokenizer = encoders.build_bpe(\n",
    "    Namespace(\n",
    "        bpe='sentencepiece', \n",
    "        sentencepiece_model='/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/MODEL_DIR/spm_char.model'\n",
    "    )\n",
    ")\n",
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "61dd5619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orig_model.decoder.layers[0].encoder_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56503f04",
   "metadata": {},
   "source": [
    "## Verify speech encoder prenet output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e2f9ff",
   "metadata": {},
   "source": [
    "This first uses the `speech_encoder_prenet` to convert the raw audio data into embeddings of shape `(batch, sequence_length, 768)`. The sequence length is roughly `number of audio samples / 320`, so there is one vector every 20 ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "01638115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128632])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = inputs[\"input_values\"]\n",
    "source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35b0a089",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_mask = torch.BoolTensor(source.shape).fill_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94586252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ...,  True,  True,  True],\n",
       "        [False, False, False,  ..., False, False, False]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask = torch.BoolTensor((1 - inputs[\"attention_mask\"]).bool())\n",
    "padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "374cf769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This doesn't work on the original model\n",
    "#padding_mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "31d1d2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input, encoder_padding_mask = orig_model.speech_encoder_prenet(\n",
    "    source, padding_mask=padding_mask, mask=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "36334be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_input = orig_model.speech_encoder_prenet.feature_extractor(source)\n",
    "# encoder_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "26c4a65d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 401, 768]), torch.Size([2, 401]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input.shape, encoder_padding_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "058de538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6.9419e+00,  6.0887e+00,  5.0580e+00,  ...,  5.8203e-01,\n",
       "           2.5973e+00, -1.8086e-01],\n",
       "         [ 7.1978e+00,  1.6047e+00,  9.1351e+00,  ...,  1.2387e+00,\n",
       "           2.8825e+00,  3.4502e+00],\n",
       "         [ 3.0047e+00, -7.6195e-01,  1.1497e+01,  ...,  1.3287e+00,\n",
       "           2.6715e+00,  2.3741e+00],\n",
       "         ...,\n",
       "         [-1.2140e-01, -9.2100e-02, -1.5428e-01,  ..., -2.4324e-01,\n",
       "          -2.2983e-01, -2.0718e-01],\n",
       "         [ 4.0435e-02, -9.1658e-02, -1.5430e-01,  ..., -2.5205e-01,\n",
       "          -2.0630e-01, -2.5674e-01],\n",
       "         [ 8.2007e-03, -1.3189e-01, -1.6461e-01,  ..., -2.0494e-01,\n",
       "          -3.0676e-01, -3.1394e-01]],\n",
       "\n",
       "        [[ 1.4333e+00, -1.8173e+00,  9.1156e-01,  ...,  7.4279e-01,\n",
       "          -3.9927e-01,  1.0221e+00],\n",
       "         [ 5.6487e-01, -2.0062e+00,  2.2312e-02,  ...,  6.6407e-01,\n",
       "           9.8154e-02,  1.4818e+00],\n",
       "         [-2.7547e-01, -2.7482e+00, -7.6423e-01,  ...,  4.7994e-01,\n",
       "           2.4423e-01,  1.1358e+00],\n",
       "         ...,\n",
       "         [ 1.2788e-01, -1.4121e+00, -8.5273e-01,  ...,  5.4640e-01,\n",
       "          -2.7472e-01,  8.5026e-01],\n",
       "         [ 6.6787e-02, -1.0637e+00, -1.1532e+00,  ...,  5.7377e-01,\n",
       "          -1.1603e-01,  1.2345e+00],\n",
       "         [ 5.1934e-01, -1.9056e+00, -5.7697e-01,  ...,  4.9755e-01,\n",
       "           1.5985e-01,  1.6285e+00]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3c07884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dbc6dd",
   "metadata": {},
   "source": [
    "If the weights and model were converted correctly, this should report zero or a very small number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4b099ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.max(torch.abs(encoder_input - hf_outputs[\"hidden_states\"]))\n",
    "torch.max(torch.abs(encoder_input - hf_outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5533cb26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f965177ee20>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.imshow(torch.abs(encoder_input - hf_outputs[\"hidden_states\"]).numpy()[0] > 1e-5)\n",
    "plt.imshow(torch.abs(encoder_input - hf_outputs[0]).numpy()[0] > 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bf93df",
   "metadata": {},
   "source": [
    "The line that is different is where the padding mask goes from 1 to 0; the original model handles this a little different than we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b92b758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.max(torch.abs(encoder_input - hf_outputs[\"extract_features\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d18286fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.abs(encoder_input - hf_outputs[\"last_hidden_state\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b76dc23",
   "metadata": {},
   "source": [
    "## Verify Transformer encoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "47c86f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run it with the original's speech prenet input:\n",
    "# with torch.no_grad():\n",
    "#     encoder_output = orig_model.encoder(encoder_input, encoder_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8b8b4df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run it with our input, which is slightly different (see above)\n",
    "with torch.no_grad():\n",
    "    encoder_output = orig_model.encoder(hf_encoder_input, ~hf_encoder_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "41ff76cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_out shape torch.Size([401, 2, 768])\n",
      "encoder_padding_mask shape torch.Size([2, 401])\n",
      "encoder_states []\n",
      "src_tokens []\n",
      "decoder_input [None]\n",
      "encoder_out_for_ctc shape torch.Size([401, 2, 81])\n"
     ]
    }
   ],
   "source": [
    "print(\"encoder_out shape\", encoder_output[\"encoder_out\"][0].shape)\n",
    "print(\"encoder_padding_mask shape\", encoder_output[\"encoder_padding_mask\"][0].shape)\n",
    "print(\"encoder_states\", encoder_output[\"encoder_states\"])  # []\n",
    "print(\"src_tokens\", encoder_output[\"src_tokens\"])  # []\n",
    "print(\"decoder_input\", encoder_output[\"decoder_input\"])  # [None]\n",
    "print(\"encoder_out_for_ctc shape\", encoder_output[\"encoder_out_for_ctc\"][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a38979d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3491, -0.0853, -0.4962,  ...,  0.2933, -0.6834,  0.0838],\n",
       "         [-0.2567, -0.0870, -0.5613,  ...,  0.2358, -0.6816,  0.0205],\n",
       "         [-0.3327, -0.1112, -0.6037,  ...,  0.2785, -0.6297, -0.1314],\n",
       "         ...,\n",
       "         [-0.0580, -0.3146, -0.0368,  ..., -0.2898, -0.1898,  0.2105],\n",
       "         [-0.2231, -0.3032,  0.0296,  ...,  0.0974, -0.2742,  0.1361],\n",
       "         [-0.1851, -0.3024,  0.0322,  ...,  0.1357, -0.2357,  0.0736]],\n",
       "\n",
       "        [[-0.2545, -0.2163, -0.4848,  ...,  0.3813, -0.5618,  0.3422],\n",
       "         [-0.1782, -0.2388, -0.4575,  ...,  0.3282, -0.5424,  0.2551],\n",
       "         [-0.1515, -0.3072, -0.3812,  ...,  0.2877, -0.4253,  0.1172],\n",
       "         ...,\n",
       "         [ 0.5395,  0.1442, -0.0540,  ...,  0.3096, -0.5798,  0.3530],\n",
       "         [ 0.2973,  0.0378, -0.0507,  ...,  0.2811, -0.5635,  0.2517],\n",
       "         [ 0.1936, -0.1404, -0.1436,  ...,  0.3061, -0.5138,  0.1664]]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[\"encoder_out\"][0].permute((1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f9b9dda4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -9.7539,  -9.9639,  -9.7654,  ...,  -9.7121,  -9.6935,  13.1617],\n",
       "         [-10.2644, -10.4033, -10.2593,  ..., -10.2150, -10.2457,  14.2349],\n",
       "         [-11.2071, -11.2572, -11.2052,  ..., -11.2506, -11.1760,  15.3922],\n",
       "         ...,\n",
       "         [-13.4234, -14.0800, -13.6217,  ..., -13.8993, -13.3328,   8.9910],\n",
       "         [ -9.9343, -10.3540, -10.0413,  ..., -10.0725,  -9.6708,   9.4658],\n",
       "         [ -9.1374,  -9.5370,  -9.2425,  ...,  -9.2380,  -8.8825,   8.8501]],\n",
       "\n",
       "        [[ -9.9849, -10.1272, -10.0026,  ..., -10.0058,  -9.7552,  12.7793],\n",
       "         [-10.4426, -10.5391, -10.4868,  ..., -10.5314, -10.2572,  13.7857],\n",
       "         [-12.3113, -12.4078, -12.2937,  ..., -12.4932, -12.1741,  15.2556],\n",
       "         ...,\n",
       "         [ -9.1622,  -9.3113,  -9.2084,  ...,  -9.3590,  -9.0228,  11.5350],\n",
       "         [ -9.9882, -10.2963,  -9.9973,  ..., -10.0581,  -9.7548,  11.7940],\n",
       "         [-10.3344, -10.8017, -10.4995,  ..., -10.4289,  -9.9483,  10.9756]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[\"encoder_out_for_ctc\"][0].permute((1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cde6dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use exact same inputs as the original model:\n",
    "# with torch.no_grad():\n",
    "#      hf_outputs = hf_model.speecht5(\n",
    "#          inputs_embeds=encoder_input,\n",
    "#          attention_mask=(~encoder_padding_mask),\n",
    "#      )\n",
    "\n",
    "# type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "28f529a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutput"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.encoder.wrapped_encoder(\n",
    "         hidden_states=hf_encoder_input,\n",
    "         attention_mask=hf_encoder_attention_mask,\n",
    "#          input_values=inputs.input_values,\n",
    "#          attention_mask=inputs.attention_mask,\n",
    "         return_dict=True,\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fb12e71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[-0.3491, -0.0853, -0.4962,  ...,  0.2933, -0.6834,  0.0838],\n",
       "         [-0.2567, -0.0870, -0.5613,  ...,  0.2358, -0.6816,  0.0205],\n",
       "         [-0.3327, -0.1112, -0.6037,  ...,  0.2785, -0.6297, -0.1314],\n",
       "         ...,\n",
       "         [-0.0580, -0.3146, -0.0368,  ..., -0.2898, -0.1898,  0.2105],\n",
       "         [-0.2231, -0.3032,  0.0296,  ...,  0.0974, -0.2742,  0.1361],\n",
       "         [-0.1851, -0.3024,  0.0322,  ...,  0.1357, -0.2357,  0.0736]],\n",
       "\n",
       "        [[-0.2545, -0.2163, -0.4848,  ...,  0.3813, -0.5618,  0.3422],\n",
       "         [-0.1782, -0.2388, -0.4575,  ...,  0.3282, -0.5424,  0.2551],\n",
       "         [-0.1515, -0.3072, -0.3812,  ...,  0.2877, -0.4253,  0.1172],\n",
       "         ...,\n",
       "         [ 0.5395,  0.1442, -0.0540,  ...,  0.3096, -0.5798,  0.3530],\n",
       "         [ 0.2973,  0.0378, -0.0507,  ...,  0.2811, -0.5635,  0.2517],\n",
       "         [ 0.1936, -0.1404, -0.1436,  ...,  0.3061, -0.5138,  0.1664]]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "06fb675b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last_hidden_state']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8fe9914e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 401, 768])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "69eda6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3491, -0.0853, -0.4962,  ...,  0.2933, -0.6834,  0.0838],\n",
       "         [-0.2567, -0.0870, -0.5613,  ...,  0.2358, -0.6816,  0.0205],\n",
       "         [-0.3327, -0.1112, -0.6037,  ...,  0.2785, -0.6297, -0.1314],\n",
       "         ...,\n",
       "         [-0.0580, -0.3146, -0.0368,  ..., -0.2898, -0.1898,  0.2105],\n",
       "         [-0.2231, -0.3032,  0.0296,  ...,  0.0974, -0.2742,  0.1361],\n",
       "         [-0.1851, -0.3024,  0.0322,  ...,  0.1357, -0.2357,  0.0736]],\n",
       "\n",
       "        [[-0.2545, -0.2163, -0.4848,  ...,  0.3813, -0.5618,  0.3422],\n",
       "         [-0.1782, -0.2388, -0.4575,  ...,  0.3282, -0.5424,  0.2551],\n",
       "         [-0.1515, -0.3072, -0.3812,  ...,  0.2877, -0.4253,  0.1172],\n",
       "         ...,\n",
       "         [ 0.5395,  0.1442, -0.0540,  ...,  0.3096, -0.5798,  0.3530],\n",
       "         [ 0.2973,  0.0378, -0.0507,  ...,  0.2811, -0.5635,  0.2517],\n",
       "         [ 0.1936, -0.1404, -0.1436,  ...,  0.3061, -0.5138,  0.1664]]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a747e547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9073e-06)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(encoder_output[\"encoder_out\"][0].permute((1, 0, 2)) - hf_outputs[\"last_hidden_state\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7518e152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca490a88",
   "metadata": {},
   "source": [
    "## Verify CTC model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98055be9",
   "metadata": {},
   "source": [
    "This model only needs the encoder portion.\n",
    "\n",
    "This uses the same checkpoint as before: `speecht5_base_asr.pt` from https://huggingface.co/ajyy/SpeechT5\n",
    "\n",
    "Run the following to convert, using your own `--checkpoint_path` and `--pytorch_dump_folder_path`:\n",
    "\n",
    "```nohighlight\n",
    "cd transformers/src/transformers/models/speecht5\n",
    "\n",
    "python convert_speecht5_original_pytorch_checkpoint_to_pytorch.py \\\n",
    "  --task ctc \\\n",
    "  --checkpoint_path /path/to/SpeechT5/speecht5_base_asr.pt \n",
    "  --pytorch_dump_folder_path /some/other/path\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "69ca25a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_model_ctc = SpeechT5ForCTC(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5a32978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_ctc = \"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_base_ctc\"\n",
    "hf_model_ctc = SpeechT5ForCTC.from_pretrained(model_checkpoint_ctc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "45272066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure if this should work, since SpeechT5ForCTC does not have a speecht5 property...\n",
    "#hf_model_ctc = SpeechT5Model.from_pretrained(model_checkpoint_ctc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8d81d7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.CausalLMOutput"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the full model:\n",
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model_ctc(**inputs)\n",
    "\n",
    "# Run without attention_mask:\n",
    "# with torch.no_grad():\n",
    "#     hf_outputs = hf_model_ctc(input_values=inputs[\"input_values\"])\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "de48ff1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.5367e-06)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(encoder_output[\"encoder_out_for_ctc\"][0].permute((1, 0, 2)) - hf_outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "32fabb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.nn.functional.softmax(hf_outputs[0], dim=-1, dtype=torch.float32)\n",
    "probs = probs.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "267f80ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[80, 80, 80, 80, 80,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "          4,  4,  4,  4,  4,  4,  4,  4,  4, 18, 18, 10, 10, 80, 12, 12, 80,  6,\n",
       "         80,  5, 13, 13,  4,  4, 45, 45, 16, 16, 10, 10, 15, 15, 15, 80, 80,  6,\n",
       "          6, 80,  5, 13, 13, 80, 80,  4,  4, 80, 10, 10, 80, 12, 12,  4,  4,  6,\n",
       "         11, 11,  5,  5,  4,  4, 80,  7, 24, 80, 80, 24, 80, 80, 80, 80,  8, 12,\n",
       "         12, 80, 80, 80, 12, 80, 80, 80,  5, 15, 15, 15, 80, 80, 80,  4,  4, 80,\n",
       "         80,  8, 19, 19,  4,  4,  6, 11, 11,  5,  4,  4, 18, 10, 10, 14, 80, 14,\n",
       "         14, 15, 15,  5, 80,  4,  4, 17, 17, 15, 15, 80, 80, 80,  7, 12, 12, 12,\n",
       "         80, 80, 12, 12, 80, 80,  5,  5, 80, 80, 80, 12, 12, 80, 80, 80, 80, 80,\n",
       "         80,  4,  4,  4, 80,  7,  9,  9, 14, 80,  4,  4, 20, 20,  5,  5,  4,  4,\n",
       "          7,  7, 13,  5,  5,  4, 21, 21, 15, 15, 80, 80, 80,  7,  7,  7, 80, 80,\n",
       "         14, 14, 80, 80,  4,  4,  6,  6,  8,  8, 80,  4,  4, 20, 20, 80,  5, 15,\n",
       "         15, 15, 15, 80, 80, 17, 17,  8,  8, 18,  5,  5, 80, 80, 80,  4,  4, 11,\n",
       "         11, 11, 10, 80, 12, 12, 80, 80,  4,  4,  4, 21, 21, 80, 80,  8,  8, 80,\n",
       "         80, 80, 12, 12, 80, 80, 80, 24, 24, 80, 80, 15, 15, 15, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80],\n",
       "        [80, 80, 80, 80,  4,  4,  4,  4,  4,  4,  4, 12, 12, 80, 80, 80,  5,  5,\n",
       "         15, 15, 15, 19, 19, 80,  4,  4, 14, 14, 80,  5, 80, 80, 12, 12, 12, 80,\n",
       "          6,  6, 13, 13, 16, 16, 16, 80, 17, 17,  6,  4,  4, 20, 10, 15, 80, 80,\n",
       "         15,  4, 17, 17,  8,  8, 18, 80, 80, 18, 80,  5,  9,  9, 80, 80, 17,  5,\n",
       "         80,  4, 80, 10,  9, 80, 80,  4,  4, 19, 19, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 10, 10, 80, 80, 27,  5,  5, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80,  4,  4,  4,  4,  4,  4,  4, 19, 19, 80, 80,\n",
       "         80,  8,  8, 16, 16, 13, 13, 13, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80,  4,  4,  4,  4,  4,  4,  6,  6, 11, 11, 13, 13,\n",
       "         13,  5,  5, 80, 80,  5,  5,  5, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80,  4,  4,  4,  4,  4,  4,  6,  6,  6, 20, 20, 80, 80,\n",
       "          8,  8, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80,  4,  4,  4,  4,  4,  4,  4,  4, 80, 80, 80,  8,  9,  9,\n",
       "          5,  5,  4,  4, 80,  7,  9,  9, 14,  4,  4,  4,  7,  4,  4, 11, 11, 80,\n",
       "         80, 80,  7,  7,  7, 15, 15, 15, 19, 19, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80,  4,  4,  4,  4,  4,  4,  4, 80, 80,  8,  8,  9,\n",
       "          9,  5,  5, 80,  4,  4, 80, 80,  7,  9,  9, 14,  4,  4,  7,  7,  4,  4,\n",
       "         45, 45, 16, 16, 16,  7,  7, 13, 13, 80, 80,  6, 80,  5, 13, 13, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,  4,  4,  4,  4,  4,\n",
       "          4,  4, 80, 80,  8,  8,  9,  9,  5,  4,  4, 80,  7,  9,  9, 14,  4,  4,\n",
       "          7,  4,  4, 15, 10,  6,  6, 80,  6,  6, 15,  5,  5,  4, 25, 25, 80, 10,\n",
       "         10, 80,  6,  6, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "90ca3c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tgt_dict.string(probs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5bcf370d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      mmiissterr  qquuiillltterr  iiss  thhee  appossselll  off  thhe  miidddlle  ccllassssseess   annd  wwee  aaree ggllaaadd  ttoo  wwellllccoomee  hhhiss   ggoosspplll\n",
      "       sseelllff  ddesssttrruuucct  will ccoommennce in  ffiivee       ffoouurrr      tthhrrreeeee      tttwwoo        onnee  annd   a  hhaaalllff       oonnee  annd  aa  qquuuaarrterr       oonne  annd  a  littttlee bbiitt\n"
     ]
    }
   ],
   "source": [
    "for i in range(probs.shape[0]):\n",
    "    print(tokenizer.decode(tgt_dict.string(probs[i])).replace(\"<ctc_blank>\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d998839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0843c78d",
   "metadata": {},
   "source": [
    "Calculate loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488b057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model_ctc(\n",
    "         **inputs, \n",
    "         labels=torch.tensor(\n",
    "           [[ 46, 16, 12,  6,  4,  6, 11, 13, 16, 12,  6,  4,  7,  9, 14,  4,\n",
    "         24,  7, 13, 13, 22,  4,  7,  9, 14,  4, 27, 10, 17,  6,  8, 13, 22,  4,\n",
    "          6,  8,  4,  6, 11,  5,  4, 12,  6, 13,  8,  9, 21,  5, 13, ]]\n",
    "         ),\n",
    "         output_hidden_states=True,\n",
    "         return_dict=True,\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9be9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a115aa44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b17bb840",
   "metadata": {},
   "source": [
    "## Verify text decoder prenet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9ba48d",
   "metadata": {},
   "source": [
    "First this calls `text_decoder_prenet`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "731e18db",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = inputs.input_values.size(0)\n",
    "beam_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "28dcb259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = torch.tensor([2, 4, 46, 16, 12, 16] * beam_size * batch_size).reshape(beam_size * batch_size, -1)\n",
    "tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2296db22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one has padding (token_id = 1)\n",
    "tokens = torch.tensor([2, 4, 46, 16, 1, 12] * beam_size * batch_size).reshape(beam_size * batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f09f744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    prev_output_tokens, tgt_mask, incremental_state = orig_model.text_decoder_prenet(tokens, incremental_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "28e885e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This returns a sequence length of 1\n",
    "# with torch.no_grad():\n",
    "#     prev_output_tokens, tgt_mask, incremental_state = orig_model.text_decoder_prenet(tokens, incremental_state={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "725cfc62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6, 768])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: misleading name; these are not the actual tokens but their embeddings!\n",
    "prev_output_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "84764c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "81026d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "incremental_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d180d86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.decoder.prenet(\n",
    "         input_ids=tokens,\n",
    "#          attention_mask=hf_encoder_attention_mask,\n",
    "#          attention_mask=inputs.attention_mask,\n",
    "#          past_key_values=[(torch.ones(5, 1, 1), torch.ones(5, 1, 1))],\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6ba48750",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeds_hf, decoder_attention_mask = hf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "265eaee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6, 768])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeds_hf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "308e46e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(prev_output_tokens - token_embeds_hf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1d4c3d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9c84dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fec354d",
   "metadata": {},
   "source": [
    "## Verify Transformer decoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "42a4fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = source.size(0)\n",
    "new_order = torch.arange(bsz).view(-1, 1).repeat(1, beam_size).view(-1)\n",
    "encoder_outs = orig_model.encoder.reorder_encoder_out(encoder_output, new_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a98652fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1074,  1.1042,  1.0387,  ...,  0.9619,  1.0463,  0.8678],\n",
       "         [ 0.0890,  0.1702,  0.3139,  ...,  0.8629,  0.7833,  0.9348],\n",
       "         [-0.7402, -0.4274, -0.5774,  ...,  1.3103,  0.5403,  0.7292],\n",
       "         [-0.8579, -0.9214, -0.9064,  ...,  1.0331,  0.8949,  0.9723],\n",
       "         [-0.2291, -0.4457, -0.5671,  ...,  0.9991,  0.8768,  0.9128],\n",
       "         [ 0.7580,  0.5875,  0.4706,  ...,  1.0331,  0.8949,  0.9723]],\n",
       "\n",
       "        [[ 1.1074,  1.1042,  1.0387,  ...,  0.9619,  1.0463,  0.8678],\n",
       "         [ 0.0890,  0.1702,  0.3139,  ...,  0.8629,  0.7833,  0.9348],\n",
       "         [-0.7402, -0.4274, -0.5774,  ...,  1.3103,  0.5403,  0.7292],\n",
       "         [-0.8579, -0.9214, -0.9064,  ...,  1.0331,  0.8949,  0.9723],\n",
       "         [-0.2291, -0.4457, -0.5671,  ...,  0.9991,  0.8768,  0.9128],\n",
       "         [ 0.7580,  0.5875,  0.4706,  ...,  1.0331,  0.8949,  0.9723]],\n",
       "\n",
       "        [[ 1.1074,  1.1042,  1.0387,  ...,  0.9619,  1.0463,  0.8678],\n",
       "         [ 0.0890,  0.1702,  0.3139,  ...,  0.8629,  0.7833,  0.9348],\n",
       "         [-0.7402, -0.4274, -0.5774,  ...,  1.3103,  0.5403,  0.7292],\n",
       "         [-0.8579, -0.9214, -0.9064,  ...,  1.0331,  0.8949,  0.9723],\n",
       "         [-0.2291, -0.4457, -0.5671,  ...,  0.9991,  0.8768,  0.9128],\n",
       "         [ 0.7580,  0.5875,  0.4706,  ...,  1.0331,  0.8949,  0.9723]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.1074,  1.1042,  1.0387,  ...,  0.9619,  1.0463,  0.8678],\n",
       "         [ 0.0890,  0.1702,  0.3139,  ...,  0.8629,  0.7833,  0.9348],\n",
       "         [-0.7402, -0.4274, -0.5774,  ...,  1.3103,  0.5403,  0.7292],\n",
       "         [-0.8579, -0.9214, -0.9064,  ...,  1.0331,  0.8949,  0.9723],\n",
       "         [-0.2291, -0.4457, -0.5671,  ...,  0.9991,  0.8768,  0.9128],\n",
       "         [ 0.7580,  0.5875,  0.4706,  ...,  1.0331,  0.8949,  0.9723]],\n",
       "\n",
       "        [[ 1.1074,  1.1042,  1.0387,  ...,  0.9619,  1.0463,  0.8678],\n",
       "         [ 0.0890,  0.1702,  0.3139,  ...,  0.8629,  0.7833,  0.9348],\n",
       "         [-0.7402, -0.4274, -0.5774,  ...,  1.3103,  0.5403,  0.7292],\n",
       "         [-0.8579, -0.9214, -0.9064,  ...,  1.0331,  0.8949,  0.9723],\n",
       "         [-0.2291, -0.4457, -0.5671,  ...,  0.9991,  0.8768,  0.9128],\n",
       "         [ 0.7580,  0.5875,  0.4706,  ...,  1.0331,  0.8949,  0.9723]],\n",
       "\n",
       "        [[ 1.1074,  1.1042,  1.0387,  ...,  0.9619,  1.0463,  0.8678],\n",
       "         [ 0.0890,  0.1702,  0.3139,  ...,  0.8629,  0.7833,  0.9348],\n",
       "         [-0.7402, -0.4274, -0.5774,  ...,  1.3103,  0.5403,  0.7292],\n",
       "         [-0.8579, -0.9214, -0.9064,  ...,  1.0331,  0.8949,  0.9723],\n",
       "         [-0.2291, -0.4457, -0.5671,  ...,  0.9991,  0.8768,  0.9128],\n",
       "         [ 0.7580,  0.5875,  0.4706,  ...,  1.0331,  0.8949,  0.9723]]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6229b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    decoder_output, extra = orig_model.decoder(\n",
    "        prev_output_tokens,\n",
    "        tgt_mask,\n",
    "        encoder_out=encoder_outs,\n",
    "        incremental_state=incremental_state,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0a190958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6, 768])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "fea0351e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4042, -0.3464, -0.1654,  ..., -0.2053, -0.4237,  0.0788],\n",
       "         [-0.4293, -0.6809, -1.6442,  ..., -0.3035,  0.1346,  0.0505],\n",
       "         [ 0.2834, -0.7885, -1.0885,  ...,  0.2483,  0.1978,  0.5294],\n",
       "         [ 0.7349, -1.1453, -1.6684,  ...,  0.3394,  0.5876, -0.4216],\n",
       "         [ 0.6584, -1.1614, -0.8039,  ..., -0.5429,  0.5048, -0.2464],\n",
       "         [ 0.5320, -0.6075, -0.9035,  ...,  0.6435,  0.6379, -0.0853]],\n",
       "\n",
       "        [[-0.4042, -0.3464, -0.1654,  ..., -0.2053, -0.4237,  0.0788],\n",
       "         [-0.4293, -0.6809, -1.6442,  ..., -0.3035,  0.1346,  0.0505],\n",
       "         [ 0.2834, -0.7885, -1.0885,  ...,  0.2483,  0.1978,  0.5294],\n",
       "         [ 0.7349, -1.1453, -1.6684,  ...,  0.3394,  0.5876, -0.4216],\n",
       "         [ 0.6584, -1.1614, -0.8039,  ..., -0.5429,  0.5048, -0.2464],\n",
       "         [ 0.5320, -0.6075, -0.9035,  ...,  0.6435,  0.6379, -0.0853]],\n",
       "\n",
       "        [[-0.4042, -0.3464, -0.1654,  ..., -0.2053, -0.4237,  0.0788],\n",
       "         [-0.4293, -0.6809, -1.6442,  ..., -0.3035,  0.1346,  0.0505],\n",
       "         [ 0.2834, -0.7885, -1.0885,  ...,  0.2483,  0.1978,  0.5294],\n",
       "         [ 0.7349, -1.1453, -1.6684,  ...,  0.3394,  0.5876, -0.4216],\n",
       "         [ 0.6584, -1.1614, -0.8039,  ..., -0.5429,  0.5048, -0.2464],\n",
       "         [ 0.5320, -0.6075, -0.9035,  ...,  0.6435,  0.6379, -0.0853]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.4469, -0.2283, -0.0834,  ..., -0.2076, -0.4499,  0.0166],\n",
       "         [ 0.0655, -0.4424, -1.4536,  ..., -0.2090, -0.1812, -0.1850],\n",
       "         [ 0.6125, -0.1873, -0.7856,  ...,  0.1821, -0.3962,  0.9259],\n",
       "         [ 0.8581, -0.5432, -1.7023,  ..., -0.0384,  0.2454,  0.0275],\n",
       "         [ 0.8709, -0.2559, -0.4044,  ..., -0.4054, -0.4117, -0.0879],\n",
       "         [ 0.6939, -0.6869, -0.7209,  ...,  0.2299,  0.0294,  0.2049]],\n",
       "\n",
       "        [[-0.4469, -0.2283, -0.0834,  ..., -0.2076, -0.4499,  0.0166],\n",
       "         [ 0.0655, -0.4424, -1.4536,  ..., -0.2090, -0.1812, -0.1850],\n",
       "         [ 0.6125, -0.1873, -0.7856,  ...,  0.1821, -0.3962,  0.9259],\n",
       "         [ 0.8581, -0.5432, -1.7023,  ..., -0.0384,  0.2454,  0.0275],\n",
       "         [ 0.8709, -0.2559, -0.4044,  ..., -0.4054, -0.4117, -0.0879],\n",
       "         [ 0.6939, -0.6869, -0.7209,  ...,  0.2299,  0.0294,  0.2049]],\n",
       "\n",
       "        [[-0.4469, -0.2283, -0.0834,  ..., -0.2076, -0.4499,  0.0166],\n",
       "         [ 0.0655, -0.4424, -1.4536,  ..., -0.2090, -0.1812, -0.1850],\n",
       "         [ 0.6125, -0.1873, -0.7856,  ...,  0.1821, -0.3962,  0.9259],\n",
       "         [ 0.8581, -0.5432, -1.7023,  ..., -0.0384,  0.2454,  0.0275],\n",
       "         [ 0.8709, -0.2559, -0.4044,  ..., -0.4054, -0.4117, -0.0879],\n",
       "         [ 0.6939, -0.6869, -0.7209,  ...,  0.2299,  0.0294,  0.2049]]])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "05117956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[x.shape for x in extra[\"attn\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "b593692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[x.shape for x in extra[\"inner_states\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "9d486f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.decoder.wrapped_decoder(\n",
    "         hidden_states=prev_output_tokens,\n",
    "         attention_mask=decoder_attention_mask,\n",
    "         encoder_hidden_states=encoder_outs[\"encoder_out\"][0].permute((1, 0, 2)),\n",
    "         encoder_attention_mask=hf_encoder_attention_mask.repeat((1, beam_size)).view(beam_size * batch_size, -1),\n",
    "         return_dict=True,\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "ea512cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last_hidden_state', 'past_key_values']"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d82c369a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6, 768])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "eb6282bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4042, -0.3464, -0.1654,  ..., -0.2053, -0.4237,  0.0788],\n",
       "         [-0.4293, -0.6809, -1.6442,  ..., -0.3035,  0.1346,  0.0505],\n",
       "         [ 0.2834, -0.7885, -1.0885,  ...,  0.2483,  0.1978,  0.5294],\n",
       "         [ 0.7349, -1.1453, -1.6684,  ...,  0.3394,  0.5876, -0.4216],\n",
       "         [ 0.6584, -1.1614, -0.8039,  ..., -0.5429,  0.5048, -0.2464],\n",
       "         [ 0.5320, -0.6075, -0.9035,  ...,  0.6435,  0.6379, -0.0853]],\n",
       "\n",
       "        [[-0.4042, -0.3464, -0.1654,  ..., -0.2053, -0.4237,  0.0788],\n",
       "         [-0.4293, -0.6809, -1.6442,  ..., -0.3035,  0.1346,  0.0505],\n",
       "         [ 0.2834, -0.7885, -1.0885,  ...,  0.2483,  0.1978,  0.5294],\n",
       "         [ 0.7349, -1.1453, -1.6684,  ...,  0.3394,  0.5876, -0.4216],\n",
       "         [ 0.6584, -1.1614, -0.8039,  ..., -0.5429,  0.5048, -0.2464],\n",
       "         [ 0.5320, -0.6075, -0.9035,  ...,  0.6435,  0.6379, -0.0853]],\n",
       "\n",
       "        [[-0.4042, -0.3464, -0.1654,  ..., -0.2053, -0.4237,  0.0788],\n",
       "         [-0.4293, -0.6809, -1.6442,  ..., -0.3035,  0.1346,  0.0505],\n",
       "         [ 0.2834, -0.7885, -1.0885,  ...,  0.2483,  0.1978,  0.5294],\n",
       "         [ 0.7349, -1.1453, -1.6684,  ...,  0.3394,  0.5876, -0.4216],\n",
       "         [ 0.6584, -1.1614, -0.8039,  ..., -0.5429,  0.5048, -0.2464],\n",
       "         [ 0.5320, -0.6075, -0.9035,  ...,  0.6435,  0.6379, -0.0853]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.4469, -0.2283, -0.0834,  ..., -0.2076, -0.4499,  0.0166],\n",
       "         [ 0.0655, -0.4424, -1.4536,  ..., -0.2090, -0.1812, -0.1850],\n",
       "         [ 0.6125, -0.1873, -0.7856,  ...,  0.1821, -0.3962,  0.9259],\n",
       "         [ 0.8581, -0.5432, -1.7023,  ..., -0.0384,  0.2454,  0.0275],\n",
       "         [ 0.8709, -0.2559, -0.4044,  ..., -0.4054, -0.4117, -0.0879],\n",
       "         [ 0.6939, -0.6869, -0.7209,  ...,  0.2299,  0.0294,  0.2049]],\n",
       "\n",
       "        [[-0.4469, -0.2283, -0.0834,  ..., -0.2076, -0.4499,  0.0166],\n",
       "         [ 0.0655, -0.4424, -1.4536,  ..., -0.2090, -0.1812, -0.1850],\n",
       "         [ 0.6125, -0.1873, -0.7856,  ...,  0.1821, -0.3962,  0.9259],\n",
       "         [ 0.8581, -0.5432, -1.7023,  ..., -0.0384,  0.2454,  0.0275],\n",
       "         [ 0.8709, -0.2559, -0.4044,  ..., -0.4054, -0.4117, -0.0879],\n",
       "         [ 0.6939, -0.6869, -0.7209,  ...,  0.2299,  0.0294,  0.2049]],\n",
       "\n",
       "        [[-0.4469, -0.2283, -0.0834,  ..., -0.2076, -0.4499,  0.0166],\n",
       "         [ 0.0655, -0.4424, -1.4536,  ..., -0.2090, -0.1812, -0.1850],\n",
       "         [ 0.6125, -0.1873, -0.7856,  ...,  0.1821, -0.3962,  0.9259],\n",
       "         [ 0.8581, -0.5432, -1.7023,  ..., -0.0384,  0.2454,  0.0275],\n",
       "         [ 0.8709, -0.2559, -0.4044,  ..., -0.4054, -0.4117, -0.0879],\n",
       "         [ 0.6939, -0.6869, -0.7209,  ...,  0.2299,  0.0294,  0.2049]]])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "f3e254cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6689e-06)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(decoder_output - hf_outputs[\"last_hidden_state\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef206524",
   "metadata": {},
   "source": [
    "Close enough! I would rather see 0.0 but this looks like numerical precision differences for 32-bit floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d3d1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d9113f7",
   "metadata": {},
   "source": [
    "## Verify text decoder postnet output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a20be43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    lprobs = orig_model.text_decoder_postnet(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a7a143bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6, 81])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lprobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "7d6de14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.4168e+01, -2.2556e+01,  5.4097e-01, -2.2586e+01,  1.7349e+01,\n",
       "          4.6760e-02,  6.2041e-01, -9.4606e-01, -4.4315e-01, -1.8812e+00,\n",
       "         -3.9952e-01,  2.1893e-01,  3.9134e-01, -1.3906e+00, -2.2705e+00,\n",
       "         -1.5500e+00, -1.0440e+00, -2.6232e+00, -7.7046e-01, -4.0442e-02,\n",
       "          1.1401e+00,  2.4883e-01, -3.7950e+00, -2.2715e+01, -8.6929e-01,\n",
       "          6.3732e-01, -2.2659e+01, -2.8817e+00, -3.3690e+00, -2.2606e+01,\n",
       "         -2.2637e+01,  9.3877e-02, -2.2744e+01, -2.2613e+01, -2.2609e+01,\n",
       "         -2.2670e+01, -2.2651e+01, -5.3913e+00, -2.2602e+01, -2.2722e+01,\n",
       "         -2.2586e+01, -2.2674e+01, -2.2655e+01, -2.2676e+01, -2.2658e+01,\n",
       "         -2.4368e+00, -2.3410e+00, -2.2580e+01, -2.2609e+01, -2.2735e+01,\n",
       "         -2.2613e+01, -2.2698e+01, -2.2605e+01, -2.2590e+01, -2.2633e+01,\n",
       "         -2.2558e+01, -2.2639e+01, -6.2336e+00, -2.2628e+01, -2.2604e+01,\n",
       "         -2.2654e+01, -2.2722e+01, -2.2647e+01, -2.2622e+01, -2.2642e+01,\n",
       "         -2.2666e+01, -2.2624e+01, -2.2692e+01, -2.2651e+01, -2.2621e+01,\n",
       "         -2.2545e+01, -2.2643e+01, -2.2597e+01, -2.2617e+01, -2.2579e+01,\n",
       "         -2.2647e+01, -2.2589e+01, -2.2632e+01, -2.2609e+01, -2.1264e+01,\n",
       "         -2.2597e+01],\n",
       "        [-1.9043e+01, -1.6790e+01, -1.0073e+01, -1.6704e+01, -2.1654e+00,\n",
       "         -1.6095e-01,  2.0542e+00,  1.3301e+00,  8.8607e-01,  3.1429e+00,\n",
       "          1.7273e+00,  1.7962e+00,  2.6194e+00,  1.9622e+00,  1.1515e+00,\n",
       "          2.9886e-01, -3.7260e+00, -1.7935e+00,  1.7473e+01,  1.4869e+00,\n",
       "          2.9626e+00, -5.8204e-01, -1.0603e+00, -1.6751e+01, -2.1045e-01,\n",
       "          1.1617e+00, -1.6724e+01,  1.3194e-01, -1.3141e+00, -1.6618e+01,\n",
       "         -1.6784e+01, -7.2883e+00, -1.6691e+01, -1.6631e+01, -1.6725e+01,\n",
       "         -1.6714e+01, -1.6689e+01, -8.9997e+00, -1.6611e+01, -1.6721e+01,\n",
       "         -1.6792e+01, -1.6688e+01, -1.6791e+01, -1.6748e+01, -1.6654e+01,\n",
       "         -4.4007e+00,  1.5568e-02, -1.6610e+01, -1.6773e+01, -1.6620e+01,\n",
       "         -1.6670e+01, -1.6731e+01, -1.6700e+01, -1.6577e+01, -1.6680e+01,\n",
       "         -1.6566e+01, -1.6698e+01, -3.7797e+00, -1.6731e+01, -1.6726e+01,\n",
       "         -1.6751e+01, -1.6781e+01, -1.6745e+01, -1.6605e+01, -1.6571e+01,\n",
       "         -1.6787e+01, -1.6671e+01, -1.6678e+01, -1.6674e+01, -1.6768e+01,\n",
       "         -1.6603e+01, -1.6593e+01, -1.6689e+01, -1.6522e+01, -1.6743e+01,\n",
       "         -1.6653e+01, -1.6763e+01, -1.6809e+01, -1.6687e+01, -1.5258e+01,\n",
       "         -1.6675e+01],\n",
       "        [-2.3547e+01, -1.7763e+01, -3.8741e+00, -1.8379e+01,  4.6679e+00,\n",
       "          8.2100e+00, -1.0609e+00,  8.2360e+00,  6.7864e+00, -5.4316e-01,\n",
       "          1.1049e+01, -5.6321e+00, -1.6484e+00, -1.5931e+00, -5.5817e+00,\n",
       "         -4.7685e+00,  1.2519e+01, -4.8425e+00,  6.5959e-01, -5.7096e+00,\n",
       "         -7.5465e+00, -7.1944e+00,  1.1202e-01, -1.8384e+01, -2.8636e+00,\n",
       "         -8.6657e+00, -1.8400e+01, -2.7559e+00, -7.7308e+00, -1.8319e+01,\n",
       "         -1.8435e+01,  1.0387e+00, -1.8136e+01, -1.8234e+01, -1.8274e+01,\n",
       "         -1.8311e+01, -1.8356e+01, -1.0118e+01, -1.8370e+01, -1.8282e+01,\n",
       "         -1.8412e+01, -1.8292e+01, -1.8378e+01, -1.8441e+01, -1.8355e+01,\n",
       "         -8.4691e+00, -6.2354e+00, -1.8301e+01, -1.8260e+01, -1.8281e+01,\n",
       "         -1.8252e+01, -1.8347e+01, -1.8376e+01, -1.8136e+01, -1.8382e+01,\n",
       "         -1.8192e+01, -1.8309e+01, -7.9916e+00, -1.8421e+01, -1.8375e+01,\n",
       "         -1.8315e+01, -1.8357e+01, -1.8441e+01, -1.8317e+01, -1.8351e+01,\n",
       "         -1.8351e+01, -1.8408e+01, -1.8292e+01, -1.8241e+01, -1.8416e+01,\n",
       "         -1.8368e+01, -1.8337e+01, -1.8390e+01, -1.8279e+01, -1.8236e+01,\n",
       "         -1.8186e+01, -1.8362e+01, -1.8411e+01, -1.8322e+01, -1.7774e+01,\n",
       "         -1.8253e+01],\n",
       "        [-2.2486e+01, -1.6404e+01, -6.4348e+00, -1.6746e+01,  8.9949e-01,\n",
       "         -4.2801e+00,  1.9602e+00,  1.0625e+00, -6.0304e+00,  6.0616e+00,\n",
       "          3.1471e+00, -1.6299e+00,  1.1612e+01,  2.5203e+00,  7.0812e+00,\n",
       "          9.1620e+00, -4.3276e+00, -3.3373e+00,  5.9013e+00, -1.2608e+00,\n",
       "         -5.8013e+00, -2.0902e-01, -3.5005e+00, -1.6746e+01,  6.6611e+00,\n",
       "          3.9378e+00, -1.6607e+01,  1.7803e+00, -4.4496e+00, -1.6785e+01,\n",
       "         -1.6694e+01, -5.0086e+00, -1.6821e+01, -1.6751e+01, -1.6729e+01,\n",
       "         -1.6822e+01, -1.6882e+01, -4.3096e+00, -1.6694e+01, -1.6740e+01,\n",
       "         -1.6745e+01, -1.6832e+01, -1.6796e+01, -1.6810e+01, -1.6876e+01,\n",
       "         -4.3033e+00, -3.1710e+00, -1.6942e+01, -1.6760e+01, -1.6683e+01,\n",
       "         -1.6764e+01, -1.6752e+01, -1.6687e+01, -1.6698e+01, -1.6927e+01,\n",
       "         -1.6769e+01, -1.6844e+01, -3.7531e+00, -1.6823e+01, -1.6781e+01,\n",
       "         -1.6878e+01, -1.6840e+01, -1.6772e+01, -1.6743e+01, -1.6781e+01,\n",
       "         -1.6845e+01, -1.6749e+01, -1.6924e+01, -1.6653e+01, -1.6760e+01,\n",
       "         -1.6856e+01, -1.6797e+01, -1.6971e+01, -1.6886e+01, -1.6766e+01,\n",
       "         -1.6745e+01, -1.6744e+01, -1.6852e+01, -1.6832e+01, -1.5578e+01,\n",
       "         -1.6644e+01],\n",
       "        [-9.5296e+00, -1.2752e+01, -3.4893e+00, -1.2767e+01,  3.0611e+00,\n",
       "         -2.2461e-01,  2.1057e+01, -1.7268e+00, -2.7778e-01, -8.3309e-01,\n",
       "          1.2230e+00,  4.0795e-01,  4.5440e+00, -3.5393e+00,  1.2538e+00,\n",
       "         -4.1438e+00,  1.6193e+00,  3.5282e+00, -2.9716e+00,  1.2647e-01,\n",
       "         -4.9344e+00, -3.9908e+00, -5.4732e+00, -1.2774e+01,  7.8165e+00,\n",
       "         -3.4972e+00, -1.2680e+01, -5.0850e+00,  2.7922e+00, -1.2757e+01,\n",
       "         -1.2756e+01, -2.4174e+00, -1.2711e+01, -1.2820e+01, -1.2787e+01,\n",
       "         -1.2801e+01, -1.2719e+01, -7.6775e+00, -1.2788e+01, -1.2685e+01,\n",
       "         -1.2682e+01, -1.2738e+01, -1.2773e+01, -1.2815e+01, -1.2762e+01,\n",
       "          2.4289e+00, -5.5745e+00, -1.2719e+01, -1.2909e+01, -1.2759e+01,\n",
       "         -1.2833e+01, -1.2759e+01, -1.2749e+01, -1.2570e+01, -1.2817e+01,\n",
       "         -1.2820e+01, -1.2875e+01, -3.7295e+00, -1.2872e+01, -1.2890e+01,\n",
       "         -1.2752e+01, -1.2635e+01, -1.2750e+01, -1.2593e+01, -1.2744e+01,\n",
       "         -1.2788e+01, -1.2700e+01, -1.2930e+01, -1.2680e+01, -1.2771e+01,\n",
       "         -1.2743e+01, -1.2986e+01, -1.2817e+01, -1.2855e+01, -1.2810e+01,\n",
       "         -1.2831e+01, -1.2743e+01, -1.2848e+01, -1.2743e+01, -1.2448e+01,\n",
       "         -1.2683e+01],\n",
       "        [-2.4212e+01, -1.6132e+01, -1.8114e+00, -1.6146e+01,  5.8716e+00,\n",
       "          2.2145e+00,  2.4628e-01,  5.1427e+00, -7.3206e+00, -1.6881e+00,\n",
       "          1.3354e+00,  6.1891e-01,  4.9448e+00,  4.5401e+00, -2.6384e+00,\n",
       "          2.3586e+00, -6.9156e+00,  7.5051e-01,  2.8483e+00,  1.1872e+00,\n",
       "         -4.4748e+00,  3.9956e-01, -7.1030e+00, -1.6008e+01,  7.3428e+00,\n",
       "         -1.7044e+00, -1.5921e+01, -1.6534e+00,  1.4140e+00, -1.6134e+01,\n",
       "         -1.6120e+01, -2.2550e+00, -1.6123e+01, -1.6095e+01, -1.5970e+01,\n",
       "         -1.6187e+01, -1.6026e+01, -5.2942e+00, -1.5873e+01, -1.6145e+01,\n",
       "         -1.6020e+01, -1.6156e+01, -1.6066e+01, -1.6032e+01, -1.6148e+01,\n",
       "         -8.2226e-01, -6.0372e+00, -1.6184e+01, -1.6105e+01, -1.5980e+01,\n",
       "         -1.6257e+01, -1.6009e+01, -1.6110e+01, -1.5955e+01, -1.6270e+01,\n",
       "         -1.6133e+01, -1.6255e+01, -9.2853e+00, -1.6042e+01, -1.6079e+01,\n",
       "         -1.6159e+01, -1.6172e+01, -1.5961e+01, -1.6141e+01, -1.6138e+01,\n",
       "         -1.6220e+01, -1.6157e+01, -1.6292e+01, -1.6040e+01, -1.6088e+01,\n",
       "         -1.6151e+01, -1.6006e+01, -1.6257e+01, -1.6041e+01, -1.6106e+01,\n",
       "         -1.6049e+01, -1.6141e+01, -1.6144e+01, -1.6224e+01, -1.4362e+01,\n",
       "         -1.6027e+01]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lprobs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "ba7571e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    lprobs_hf = hf_model.text_decoder_postnet(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "5623a791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6, 81])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lprobs_hf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "64f0c976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.4168e+01, -2.2556e+01,  5.4097e-01, -2.2586e+01,  1.7349e+01,\n",
       "          4.6760e-02,  6.2041e-01, -9.4606e-01, -4.4315e-01, -1.8812e+00,\n",
       "         -3.9952e-01,  2.1893e-01,  3.9134e-01, -1.3906e+00, -2.2705e+00,\n",
       "         -1.5500e+00, -1.0440e+00, -2.6232e+00, -7.7046e-01, -4.0442e-02,\n",
       "          1.1401e+00,  2.4883e-01, -3.7950e+00, -2.2715e+01, -8.6929e-01,\n",
       "          6.3732e-01, -2.2659e+01, -2.8817e+00, -3.3690e+00, -2.2606e+01,\n",
       "         -2.2637e+01,  9.3877e-02, -2.2744e+01, -2.2613e+01, -2.2609e+01,\n",
       "         -2.2670e+01, -2.2651e+01, -5.3913e+00, -2.2602e+01, -2.2722e+01,\n",
       "         -2.2586e+01, -2.2674e+01, -2.2655e+01, -2.2676e+01, -2.2658e+01,\n",
       "         -2.4368e+00, -2.3410e+00, -2.2580e+01, -2.2609e+01, -2.2735e+01,\n",
       "         -2.2613e+01, -2.2698e+01, -2.2605e+01, -2.2590e+01, -2.2633e+01,\n",
       "         -2.2558e+01, -2.2639e+01, -6.2336e+00, -2.2628e+01, -2.2604e+01,\n",
       "         -2.2654e+01, -2.2722e+01, -2.2647e+01, -2.2622e+01, -2.2642e+01,\n",
       "         -2.2666e+01, -2.2624e+01, -2.2692e+01, -2.2651e+01, -2.2621e+01,\n",
       "         -2.2545e+01, -2.2643e+01, -2.2597e+01, -2.2617e+01, -2.2579e+01,\n",
       "         -2.2647e+01, -2.2589e+01, -2.2632e+01, -2.2609e+01, -2.1264e+01,\n",
       "         -2.2597e+01],\n",
       "        [-1.9043e+01, -1.6790e+01, -1.0073e+01, -1.6704e+01, -2.1654e+00,\n",
       "         -1.6095e-01,  2.0542e+00,  1.3301e+00,  8.8607e-01,  3.1429e+00,\n",
       "          1.7273e+00,  1.7962e+00,  2.6194e+00,  1.9622e+00,  1.1515e+00,\n",
       "          2.9886e-01, -3.7260e+00, -1.7935e+00,  1.7473e+01,  1.4869e+00,\n",
       "          2.9626e+00, -5.8204e-01, -1.0603e+00, -1.6751e+01, -2.1045e-01,\n",
       "          1.1617e+00, -1.6724e+01,  1.3194e-01, -1.3141e+00, -1.6618e+01,\n",
       "         -1.6784e+01, -7.2883e+00, -1.6691e+01, -1.6631e+01, -1.6725e+01,\n",
       "         -1.6714e+01, -1.6689e+01, -8.9997e+00, -1.6611e+01, -1.6721e+01,\n",
       "         -1.6792e+01, -1.6688e+01, -1.6791e+01, -1.6748e+01, -1.6654e+01,\n",
       "         -4.4007e+00,  1.5568e-02, -1.6610e+01, -1.6773e+01, -1.6620e+01,\n",
       "         -1.6670e+01, -1.6731e+01, -1.6700e+01, -1.6577e+01, -1.6680e+01,\n",
       "         -1.6566e+01, -1.6698e+01, -3.7797e+00, -1.6731e+01, -1.6726e+01,\n",
       "         -1.6751e+01, -1.6781e+01, -1.6745e+01, -1.6605e+01, -1.6571e+01,\n",
       "         -1.6787e+01, -1.6671e+01, -1.6678e+01, -1.6674e+01, -1.6768e+01,\n",
       "         -1.6603e+01, -1.6593e+01, -1.6689e+01, -1.6522e+01, -1.6743e+01,\n",
       "         -1.6653e+01, -1.6763e+01, -1.6809e+01, -1.6687e+01, -1.5258e+01,\n",
       "         -1.6675e+01],\n",
       "        [-2.3547e+01, -1.7763e+01, -3.8741e+00, -1.8379e+01,  4.6679e+00,\n",
       "          8.2100e+00, -1.0609e+00,  8.2360e+00,  6.7864e+00, -5.4316e-01,\n",
       "          1.1049e+01, -5.6321e+00, -1.6484e+00, -1.5931e+00, -5.5817e+00,\n",
       "         -4.7685e+00,  1.2519e+01, -4.8425e+00,  6.5959e-01, -5.7096e+00,\n",
       "         -7.5465e+00, -7.1944e+00,  1.1202e-01, -1.8384e+01, -2.8636e+00,\n",
       "         -8.6657e+00, -1.8400e+01, -2.7559e+00, -7.7308e+00, -1.8319e+01,\n",
       "         -1.8435e+01,  1.0387e+00, -1.8136e+01, -1.8234e+01, -1.8274e+01,\n",
       "         -1.8311e+01, -1.8356e+01, -1.0118e+01, -1.8370e+01, -1.8282e+01,\n",
       "         -1.8412e+01, -1.8292e+01, -1.8378e+01, -1.8441e+01, -1.8355e+01,\n",
       "         -8.4691e+00, -6.2354e+00, -1.8301e+01, -1.8260e+01, -1.8281e+01,\n",
       "         -1.8252e+01, -1.8347e+01, -1.8376e+01, -1.8136e+01, -1.8382e+01,\n",
       "         -1.8192e+01, -1.8309e+01, -7.9916e+00, -1.8421e+01, -1.8375e+01,\n",
       "         -1.8315e+01, -1.8357e+01, -1.8441e+01, -1.8317e+01, -1.8351e+01,\n",
       "         -1.8351e+01, -1.8408e+01, -1.8292e+01, -1.8241e+01, -1.8416e+01,\n",
       "         -1.8368e+01, -1.8337e+01, -1.8390e+01, -1.8279e+01, -1.8236e+01,\n",
       "         -1.8186e+01, -1.8362e+01, -1.8411e+01, -1.8322e+01, -1.7774e+01,\n",
       "         -1.8253e+01],\n",
       "        [-2.2486e+01, -1.6404e+01, -6.4348e+00, -1.6746e+01,  8.9949e-01,\n",
       "         -4.2801e+00,  1.9602e+00,  1.0625e+00, -6.0304e+00,  6.0616e+00,\n",
       "          3.1471e+00, -1.6299e+00,  1.1612e+01,  2.5203e+00,  7.0812e+00,\n",
       "          9.1620e+00, -4.3276e+00, -3.3373e+00,  5.9013e+00, -1.2608e+00,\n",
       "         -5.8013e+00, -2.0902e-01, -3.5005e+00, -1.6746e+01,  6.6611e+00,\n",
       "          3.9378e+00, -1.6607e+01,  1.7803e+00, -4.4496e+00, -1.6785e+01,\n",
       "         -1.6694e+01, -5.0086e+00, -1.6821e+01, -1.6751e+01, -1.6729e+01,\n",
       "         -1.6822e+01, -1.6882e+01, -4.3096e+00, -1.6694e+01, -1.6740e+01,\n",
       "         -1.6745e+01, -1.6832e+01, -1.6796e+01, -1.6810e+01, -1.6876e+01,\n",
       "         -4.3033e+00, -3.1710e+00, -1.6942e+01, -1.6760e+01, -1.6683e+01,\n",
       "         -1.6764e+01, -1.6752e+01, -1.6687e+01, -1.6698e+01, -1.6927e+01,\n",
       "         -1.6769e+01, -1.6844e+01, -3.7531e+00, -1.6823e+01, -1.6781e+01,\n",
       "         -1.6878e+01, -1.6840e+01, -1.6772e+01, -1.6743e+01, -1.6781e+01,\n",
       "         -1.6845e+01, -1.6749e+01, -1.6924e+01, -1.6653e+01, -1.6760e+01,\n",
       "         -1.6856e+01, -1.6797e+01, -1.6971e+01, -1.6886e+01, -1.6766e+01,\n",
       "         -1.6745e+01, -1.6744e+01, -1.6852e+01, -1.6832e+01, -1.5578e+01,\n",
       "         -1.6644e+01],\n",
       "        [-9.5296e+00, -1.2752e+01, -3.4893e+00, -1.2767e+01,  3.0611e+00,\n",
       "         -2.2461e-01,  2.1057e+01, -1.7268e+00, -2.7778e-01, -8.3309e-01,\n",
       "          1.2230e+00,  4.0795e-01,  4.5440e+00, -3.5393e+00,  1.2538e+00,\n",
       "         -4.1438e+00,  1.6193e+00,  3.5282e+00, -2.9716e+00,  1.2647e-01,\n",
       "         -4.9344e+00, -3.9908e+00, -5.4732e+00, -1.2774e+01,  7.8165e+00,\n",
       "         -3.4972e+00, -1.2680e+01, -5.0850e+00,  2.7922e+00, -1.2757e+01,\n",
       "         -1.2756e+01, -2.4174e+00, -1.2711e+01, -1.2820e+01, -1.2787e+01,\n",
       "         -1.2801e+01, -1.2719e+01, -7.6775e+00, -1.2788e+01, -1.2685e+01,\n",
       "         -1.2682e+01, -1.2738e+01, -1.2773e+01, -1.2815e+01, -1.2762e+01,\n",
       "          2.4289e+00, -5.5745e+00, -1.2719e+01, -1.2909e+01, -1.2759e+01,\n",
       "         -1.2833e+01, -1.2759e+01, -1.2749e+01, -1.2570e+01, -1.2817e+01,\n",
       "         -1.2820e+01, -1.2875e+01, -3.7295e+00, -1.2872e+01, -1.2890e+01,\n",
       "         -1.2752e+01, -1.2635e+01, -1.2750e+01, -1.2593e+01, -1.2744e+01,\n",
       "         -1.2788e+01, -1.2700e+01, -1.2930e+01, -1.2680e+01, -1.2771e+01,\n",
       "         -1.2743e+01, -1.2986e+01, -1.2817e+01, -1.2855e+01, -1.2810e+01,\n",
       "         -1.2831e+01, -1.2743e+01, -1.2848e+01, -1.2743e+01, -1.2448e+01,\n",
       "         -1.2683e+01],\n",
       "        [-2.4212e+01, -1.6132e+01, -1.8114e+00, -1.6146e+01,  5.8716e+00,\n",
       "          2.2145e+00,  2.4628e-01,  5.1427e+00, -7.3206e+00, -1.6881e+00,\n",
       "          1.3354e+00,  6.1891e-01,  4.9448e+00,  4.5401e+00, -2.6384e+00,\n",
       "          2.3586e+00, -6.9156e+00,  7.5051e-01,  2.8483e+00,  1.1872e+00,\n",
       "         -4.4748e+00,  3.9956e-01, -7.1030e+00, -1.6008e+01,  7.3428e+00,\n",
       "         -1.7044e+00, -1.5921e+01, -1.6534e+00,  1.4140e+00, -1.6134e+01,\n",
       "         -1.6120e+01, -2.2550e+00, -1.6123e+01, -1.6095e+01, -1.5970e+01,\n",
       "         -1.6187e+01, -1.6026e+01, -5.2942e+00, -1.5873e+01, -1.6145e+01,\n",
       "         -1.6020e+01, -1.6156e+01, -1.6066e+01, -1.6032e+01, -1.6148e+01,\n",
       "         -8.2226e-01, -6.0372e+00, -1.6184e+01, -1.6105e+01, -1.5980e+01,\n",
       "         -1.6257e+01, -1.6009e+01, -1.6110e+01, -1.5955e+01, -1.6270e+01,\n",
       "         -1.6133e+01, -1.6255e+01, -9.2853e+00, -1.6042e+01, -1.6079e+01,\n",
       "         -1.6159e+01, -1.6172e+01, -1.5961e+01, -1.6141e+01, -1.6138e+01,\n",
       "         -1.6220e+01, -1.6157e+01, -1.6292e+01, -1.6040e+01, -1.6088e+01,\n",
       "         -1.6151e+01, -1.6006e+01, -1.6257e+01, -1.6041e+01, -1.6106e+01,\n",
       "         -1.6049e+01, -1.6141e+01, -1.6144e+01, -1.6224e+01, -1.4362e+01,\n",
       "         -1.6027e+01]])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lprobs_hf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "6eeff463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(lprobs - lprobs_hf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24926eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32e98799",
   "metadata": {},
   "source": [
    "## Use the `transformers` generator loop:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdaec2d",
   "metadata": {},
   "source": [
    "Run the full model to make sure this doesn't give any errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "b4b889ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.Seq2SeqLMOutput"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model(\n",
    "         input_values=inputs.input_values,\n",
    "         attention_mask=inputs.attention_mask,\n",
    "         decoder_input_ids=torch.tensor([[3, 4, 5]]),\n",
    "         #decoder_input_ids=torch.tensor([[3, 4, 5], [2, 2, 2]]),\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "677718c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logits', 'past_key_values', 'encoder_last_hidden_state']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "7f2bcec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 81])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d1eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19619c18",
   "metadata": {},
   "source": [
    "Also calculate loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "bf72acbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.Seq2SeqLMOutput"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model(\n",
    "         input_values=inputs.input_values,\n",
    "         attention_mask=inputs.attention_mask,\n",
    "         #decoder_input_ids=torch.tensor([[2,  4, 18, 10, 12,  6,  5]]),\n",
    "         labels=torch.tensor([[4, 18, 10, 12,  6,  5, 13]]),\n",
    "         #labels=torch.tensor([[4, 18, 10, 12,  6,  5, 13], [4, 18, 10, 12,  6,  5, 13]]),  # batch\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "5e15f966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7, 81])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "c9429b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9692)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7733304c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27209b72",
   "metadata": {},
   "source": [
    "Generator loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "9b6b1ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128632])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "b2fc2baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs = hf_model.generate(inputs.input_values, max_length=100)\n",
    "# hf_outputs = hf_model.generate(inputs.input_values, num_beams=5, max_length=100) #, bos_token_id=2)\n",
    "# hf_outputs = hf_model.generate(torch.rand(1, 10000), num_beams=5, max_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "015c80b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "ce7e1899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  4, 18, 10, 12,  6,  5, 13,  4, 45, 16, 10, 15,  6,  5, 13,  4, 10,\n",
       "         12,  4,  6, 11,  5,  4,  7, 24,  8, 12,  6, 15,  5,  4,  8, 19,  4,  6,\n",
       "         11,  5,  4, 18, 10, 14, 14, 15,  5,  4, 17, 15,  7, 12, 12,  5, 12,  4,\n",
       "          7,  9, 14,  4, 20,  5,  4,  7, 13,  5,  4, 21, 15,  7, 14,  4,  6,  8,\n",
       "          4, 20,  5, 15, 17,  8, 18,  5,  4, 11, 10, 12,  4, 21,  8, 12, 24,  5,\n",
       "         15,  2,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "        [ 2,  4, 12,  5, 15, 19,  4, 14,  5, 12,  6, 13, 16, 17,  6, 10,  8,  9,\n",
       "          4, 17,  8, 18, 18,  5,  9, 17,  5,  4, 10,  9,  4, 19, 10, 27,  5,  4,\n",
       "         19,  8, 16, 13,  4,  6, 11, 13,  5,  5,  4,  6, 20,  8,  4,  8,  9,  5,\n",
       "          4,  7,  9, 14,  4,  7,  4, 11,  7, 15, 19,  4,  8,  9,  5,  4,  7,  9,\n",
       "         14,  4,  7,  4, 45, 16,  7, 13,  6,  5, 13,  4,  8,  9,  5,  4,  7,  9,\n",
       "         14,  4,  7,  4, 15, 10,  6,  6, 15,  5]])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "11fce9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " m i s t e r  q u i l t e r  i s  t h e  a p o s t l e  o f  t h e  m i d d l e  c l a s s e s  a n d  w e  a r e  g l a d  t o  w e l c o m e  h i s  g o s p e l <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "mister quilter is the apostle of the middle classes and we are glad to welcome his gospel<pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "\n",
      " s e l f  d e s t r u c t i o n  c o m m e n c e  i n  f i v e  f o u r  t h r e e  t w o  o n e  a n d  a  h a l f  o n e  a n d  a  q u a r t e r  o n e  a n d  a  l i t t l e\n",
      "self destruction commence in five four three two one and a half one and a quarter one and a little\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(hf_outputs.shape[0]):\n",
    "    print(tgt_dict.string(hf_outputs[i]))\n",
    "    print(tokenizer.decode(tgt_dict.string(hf_outputs[i])))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "997fd48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '<pad>', '</s>', '<unk>', '']"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tgt_dict[x] for x in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04fb295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d592263e",
   "metadata": {},
   "source": [
    "For comparison, Speech2Text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340e218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n",
    "\n",
    "s2t_model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "s2t_processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "df390fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthijs/anaconda3/envs/t5/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'dataset_info': token. Will not be supported from version '0.12'.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Reusing dataset librispeech_asr_dummy (/Users/matthijs/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 584, 80])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "inputs = s2t_processor(\n",
    "    ds[0][\"audio\"][\"array\"], sampling_rate=ds[0][\"audio\"][\"sampling_rate\"], return_tensors=\"pt\"\n",
    ")\n",
    "input_features = inputs.input_features\n",
    "input_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "08b6652a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 584, 80])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test batch\n",
    "# input_features = torch.tile(input_features, dims=(2, 1, 1))\n",
    "# input_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "daf06a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/transformers/src/transformers/models/speech_to_text/modeling_speech_to_text.py:561: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  input_lengths = (input_lengths - 1) // 2 + 1\n",
      "/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/transformers/src/transformers/generation_utils.py:1296: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 200 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'mister quilter is the apostle of the middle classes and we are glad to welcome his gospel'"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids = s2t_model.generate(inputs=input_features)\n",
    "\n",
    "transcription = s2t_processor.batch_decode(generated_ids)[0]\n",
    "transcription\n",
    "\n",
    "#'mister quilter is the apostle of the middle classes and we are glad to welcome his gospel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49667e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d50884f2",
   "metadata": {},
   "source": [
    "Test other methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1cd38a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BART, Speech2Text, Wav2Vec2 don't have pruning\n",
    "#hf_model.prune_heads({1: [0, 2], 2: [2,3 ]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc11e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41c735ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(100, 768)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e42d707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=100, bias=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.get_output_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6f954bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(100, 768)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.resize_token_embeddings(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "856c523b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(100, 768)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed13692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8c420142",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2560fe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_ctc.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6d95fe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_naked.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8ba29c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
