{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca8cd0a9",
   "metadata": {},
   "source": [
    "# Verifying the SpeechT5 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5342da",
   "metadata": {},
   "source": [
    "I needed to do the following to be able to load the original model:\n",
    "\n",
    "- Clone the https://github.com/microsoft/SpeechT5 repo\n",
    "\n",
    "Install stuff:\n",
    "\n",
    "```\n",
    "pip install editdistance\n",
    "pip install -U sacrebleu==1.5.1\n",
    "\n",
    "git submodule update --init SpeechT5/fairseq\n",
    "cd SpeechT5\n",
    "pip install --editable fairseq/\n",
    "pip install espnet\n",
    "```\n",
    "\n",
    "Put this notebook at the same level as the `SpeechT5` repo.\n",
    "\n",
    "Hack the code:\n",
    "\n",
    "- Copy `speecht5/tasks/speecht5.py` into `fairseq/fairseq/tasks`\n",
    "\n",
    "- To run on CPU: In `speecht5/sequence_generator.py`, comment out where it does `.to(device=\"cuda\")`\n",
    "\n",
    "Additional stuff to download:\n",
    "\n",
    "- `dict.txt` from https://drive.google.com/uc?export=download&id=19hcQ58RHZ6CssxF8Qp6yEF1NW_AXxObK\n",
    "\n",
    "- `tokenizer` from https://drive.google.com/uc?export=download&id=1wClgQjXXoU2lmpbaEa1v2SqMbg7cAutq\n",
    "\n",
    "- `speecht5_base_asr.pt` and `t5_transformer_lm.pt` from https://huggingface.co/ajyy/SpeechT5\n",
    "\n",
    "You also need an input audio file, any WAV at 16 kHz will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a4d7c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65661728",
   "metadata": {},
   "source": [
    "Set Python path so it can find the `speecht5` and `fairseq` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f30402f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../SpeechT5/SpeechT5\")\n",
    "sys.path.insert(0, \"../SpeechT5/SpeechT5/fairseq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d398e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b45926",
   "metadata": {},
   "source": [
    "## Load audio and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c18f317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/AUDIO_DIR/dev_clean/1272/141231/1272-141231-0020.flac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49919308",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/AUDIO_DIR/dev_clean/1272/128104/1272-128104-0000.flac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a855d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((69120,), 16000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "wav_data, cur_sample_rate = sf.read(input_file)\n",
    "wav_data.shape, cur_sample_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e60de",
   "metadata": {},
   "source": [
    "NOTE: The `Wav2Vec2FeatureExtractor` does not make sure the audio file is mono. If it has shape `(2, length)` or even `(1, length)` then the output from the feature extractor is incorrect!\n",
    "\n",
    "The `do_normalize` option is False for the SpeechT5 ASR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9699780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(do_normalize=False, return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ea3da78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 69120])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = feature_extractor(wav_data, sampling_rate=cur_sample_rate, padding=True, return_tensors=\"pt\")\n",
    "inputs[\"input_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1a94286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing padding mask\n",
    "inputs[\"attention_mask\"][:, 40000:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62e522e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 2))\n",
    "plt.plot(inputs[\"input_values\"][0])\n",
    "plt.plot(inputs[\"attention_mask\"][0])\n",
    "plt.ylim(-1, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f594f3",
   "metadata": {},
   "source": [
    "Transform multiple inputs into a single padded batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbb99431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128632,), 16000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file2 = \"/Users/matthijs/Documents/FILES/HuggingFace/S2S/textless/AUDIO_DIR/selfdestruct.wav\"\n",
    "wav_data2, cur_sample_rate2 = sf.read(input_file2)\n",
    "wav_data2.shape, cur_sample_rate2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1daee2cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128632])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs2 = feature_extractor([wav_data, wav_data2], sampling_rate=cur_sample_rate, padding=True, return_tensors=\"pt\")\n",
    "inputs2[\"input_values\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb653d87",
   "metadata": {},
   "source": [
    "The original model used a `padding_mask` as input, where False means no padding. The `Wav2Vec2FeatureExtractor` can return an `attention_mask`, where 1 means no padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d3ab7e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs2[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7163b1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs2   # use the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2205900b",
   "metadata": {},
   "source": [
    "## Load the Transformers model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc8c0f8",
   "metadata": {},
   "source": [
    "To convert the original checkpoint weights to Transformers:\n",
    "\n",
    "First download the checkpoint. I used `speecht5_base_asr.pt` from https://huggingface.co/ajyy/SpeechT5\n",
    "\n",
    "Then run the following, using your own `--checkpoint_path` and `--pytorch_dump_folder_path`:\n",
    "\n",
    "```nohighlight\n",
    "cd transformers/src/transformers/models/speecht5\n",
    "\n",
    "python convert_speecht5_original_pytorch_checkpoint_to_pytorch.py \\\n",
    "  --task s2t \\\n",
    "  --checkpoint_path /path/to/SpeechT5/speecht5_base_asr.pt \n",
    "  --pytorch_dump_folder_path /some/other/path\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "552fd307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    SpeechT5Config, \n",
    "    SpeechT5CTCTokenizer,\n",
    "    SpeechT5Processor,\n",
    "    SpeechT5Model, \n",
    "    SpeechT5ForConditionalGeneration, \n",
    "    SpeechT5ForCTC, \n",
    "    Wav2Vec2FeatureExtractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69aa6aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SpeechT5Config()\n",
    "hf_model = SpeechT5Model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ba8f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_base_asr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5df4bac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model = SpeechT5ForConditionalGeneration.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7dd7606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeechT5ForConditionalGeneration(\n",
       "  (speecht5): SpeechT5Model(\n",
       "    (encoder): SpeechT5SpeechEncoder(\n",
       "      (prenet): SpeechT5SpeechEncoderPrenet(\n",
       "        (feature_encoder): SpeechT5FeatureEncoder(\n",
       "          (conv_layers): ModuleList(\n",
       "            (0): SpeechT5GroupNormConvLayer(\n",
       "              (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "              (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "            )\n",
       "            (1): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (2): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (3): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (4): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (5): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (6): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (feature_projection): SpeechT5FeatureProjection(\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (pos_conv_embed): SpeechT5PositionalConvEmbedding(\n",
       "          (conv): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "          (padding): SpeechT5SamePadLayer()\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (embed_positions): SpeechT5SinusoidalPositionalEmbedding()\n",
       "      )\n",
       "      (wrapped_encoder): SpeechT5Encoder(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (embed_positions): SpeechT5RelativePositionalEncoding(\n",
       "          (pe_k): Embedding(320, 64)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): SpeechT5TextDecoder(\n",
       "      (prenet): SpeechT5TextDecoderPrenet(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (embed_tokens): Embedding(81, 768, padding_idx=1)\n",
       "        (embed_positions): SpeechT5SinusoidalPositionalEmbedding()\n",
       "      )\n",
       "      (wrapped_decoder): SpeechT5Decoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (text_decoder_postnet): SpeechT5TextDecoderPostnet(\n",
       "    (lm_head): Linear(in_features=768, out_features=81, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09c1c4b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Normally loading should work OK for class `SpeechT5Model` too. \n",
    "# However, since that doesn't have the same encoder and decoder (not wrapped)\n",
    "# the weights have the wrong paths.\n",
    "# hf_model_naked = SpeechT5Model.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7628cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the attention layer weights are correct\n",
    "# for i in range(len(hf_model.speecht5.encoder.layers)):\n",
    "#     print(i, \"k_proj weight\", torch.all(hf_model.speecht5.encoder.layers[i].attention.k_proj.weight == orig_model.encoder.layers[i].self_attn.k_proj.weight))\n",
    "#     print(i, \"k_proj bias\", torch.all(hf_model.speecht5.encoder.layers[i].attention.k_proj.bias == orig_model.encoder.layers[i].self_attn.k_proj.bias))\n",
    "#     print(i, \"v_proj weight\", torch.all(hf_model.speecht5.encoder.layers[i].attention.v_proj.weight == orig_model.encoder.layers[i].self_attn.v_proj.weight))\n",
    "#     print(i, \"v_proj bias\", torch.all(hf_model.speecht5.encoder.layers[i].attention.v_proj.bias == orig_model.encoder.layers[i].self_attn.v_proj.bias))\n",
    "#     print(i, \"q_proj weight\", torch.all(hf_model.speecht5.encoder.layers[i].attention.q_proj.weight == orig_model.encoder.layers[i].self_attn.q_proj.weight))\n",
    "#     print(i, \"q_proj bias\", torch.all(hf_model.speecht5.encoder.layers[i].attention.q_proj.bias == orig_model.encoder.layers[i].self_attn.q_proj.bias))\n",
    "#     print(i, \"out_proj weight\", torch.all(hf_model.speecht5.encoder.layers[i].attention.out_proj.weight == orig_model.encoder.layers[i].self_attn.out_proj.weight))\n",
    "#     print(i, \"out_proj bias\", torch.all(hf_model.speecht5.encoder.layers[i].attention.out_proj.bias == orig_model.encoder.layers[i].self_attn.out_proj.bias))\n",
    "#     print(\"---\")\n",
    "\n",
    "# print(\"pos_emb weight\", torch.all(hf_model.speecht5.encoder.pos_emb.pe_k.weight == orig_model.encoder.pos_emb.pe_k.weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82233c52",
   "metadata": {},
   "source": [
    "Run a single forward pass. This should run the encoder, decoder, and the relevant pre- and postnets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29bd2f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.encoder.prenet(**inputs)\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e1f8501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using no attention_mask\n",
    "# with torch.no_grad():\n",
    "#      hf_outputs = hf_model.speech_encoder_prenet(input_values=inputs[\"input_values\"])\n",
    "\n",
    "# type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "359778a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97a77055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_outputs[\"extract_features\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7db34f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_outputs[\"extract_features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41afd19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_outputs[\"hidden_states\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fcef1339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_outputs[\"hidden_states\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d01d920d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 401, 768]), torch.Size([2, 401])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in hf_outputs if hasattr(x, \"shape\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d5ff106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.9166, -2.5645, 13.4132,  ...,  0.9873,  2.0047,  0.4048],\n",
       "         [ 9.2179, -2.3052, 14.9278,  ..., -0.3634,  1.1286,  0.5824],\n",
       "         [-3.0500,  4.4579, -1.0452,  ...,  0.5873, -0.4209,  2.4890],\n",
       "         ...,\n",
       "         [-0.6976, -1.4458, -1.5213,  ..., -0.2206,  0.3340,  0.1747],\n",
       "         [-0.7179, -1.4457, -1.5213,  ..., -0.2218,  0.3617,  0.1673],\n",
       "         [-0.7312, -1.4713, -1.5220,  ..., -0.1875,  0.2293, -0.0329]],\n",
       "\n",
       "        [[ 1.4333, -1.8173,  0.9116,  ...,  0.7428, -0.3993,  1.0221],\n",
       "         [ 0.5649, -2.0062,  0.0223,  ...,  0.6641,  0.0982,  1.4818],\n",
       "         [-0.2755, -2.7482, -0.7642,  ...,  0.4799,  0.2442,  1.1358],\n",
       "         ...,\n",
       "         [ 0.1279, -1.4121, -0.8527,  ...,  0.5464, -0.2747,  0.8503],\n",
       "         [ 0.0668, -1.0637, -1.1532,  ...,  0.5738, -0.1160,  1.2345],\n",
       "         [ 0.5193, -1.9056, -0.5770,  ...,  0.4975,  0.1598,  1.6285]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d24920f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_encoder_input = hf_outputs[0]\n",
    "hf_encoder_attention_mask = hf_outputs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64823660",
   "metadata": {},
   "source": [
    "## Load the original model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1697ab0",
   "metadata": {},
   "source": [
    "Load the dictionary. This adds `<s>, <pad>, </s>, <unk>` tokens to the front and `<mask>` and `<ctc_blank>` to the end. **dict.txt** was [downloaded from here](https://drive.google.com/uc?export=download&id=19hcQ58RHZ6CssxF8Qp6yEF1NW_AXxObK). This is the Vocabulary link from the main SpeechT5 README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "24e0f251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size: 81\n"
     ]
    }
   ],
   "source": [
    "from fairseq.data import Dictionary\n",
    "tgt_dict = Dictionary.load(\"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/DATA_ROOT/dict.txt\")\n",
    "tgt_dict.add_symbol(\"<mask>\")\n",
    "tgt_dict.add_symbol(\"<ctc_blank>\")\n",
    "print(f\"dictionary size: \" f\"{len(tgt_dict):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85935250",
   "metadata": {},
   "source": [
    "To load the model we need the `SpeechT5Task` object but constructing it is annoying. Fortunately, `build_model` only reads two properties from the task object, so we can fake it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea4b4380",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeTask:\n",
    "    def __init__(self):\n",
    "        self.dicts = { \"text\": tgt_dict }\n",
    "        self.t5_task = \"s2t\"\n",
    "        \n",
    "task = FakeTask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dc6c7a",
   "metadata": {},
   "source": [
    "Load the fine-tuned ASR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a79fbfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from speecht5.models.speecht5 import T5TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9b2818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"../weights/SpeechT5/speecht5_base_asr.pt\")\n",
    "\n",
    "orig_model = T5TransformerModel.build_model(checkpoint[\"cfg\"][\"model\"], task)\n",
    "\n",
    "orig_model.load_state_dict(checkpoint[\"model\"])\n",
    "orig_model = orig_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bda7a201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speecht5.models.speecht5.T5TransformerModel"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(orig_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "64d53b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speecht5.models.modules.encoder.TransformerEncoder"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(orig_model.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bad44136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(checkpoint[\"model\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c1faddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fairseq.data.encoders.sentencepiece_bpe.SentencepieceBPE"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fairseq.data import encoders\n",
    "from argparse import Namespace\n",
    "tokenizer = encoders.build_bpe(\n",
    "    Namespace(\n",
    "        bpe='sentencepiece', \n",
    "        sentencepiece_model='/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/MODEL_DIR/spm_char.model'\n",
    "    )\n",
    ")\n",
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "61dd5619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orig_model.decoder.layers[0].encoder_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56503f04",
   "metadata": {},
   "source": [
    "## Verify speech encoder prenet output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e2f9ff",
   "metadata": {},
   "source": [
    "This first uses the `speech_encoder_prenet` to convert the raw audio data into embeddings of shape `(batch, sequence_length, 768)`. The sequence length is roughly `number of audio samples / 320`, so there is one vector every 20 ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "01638115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128632])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = inputs[\"input_values\"]\n",
    "source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "35b0a089",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_mask = torch.BoolTensor(source.shape).fill_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "94586252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ...,  True,  True,  True],\n",
       "        [False, False, False,  ..., False, False, False]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask = torch.BoolTensor((1 - inputs[\"attention_mask\"]).bool())\n",
    "padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "374cf769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This doesn't work on the original model\n",
    "#padding_mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "31d1d2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input, encoder_padding_mask = orig_model.speech_encoder_prenet(\n",
    "    source, padding_mask=padding_mask, mask=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "36334be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_input = orig_model.speech_encoder_prenet.feature_extractor(source)\n",
    "# encoder_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "26c4a65d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 401, 768]), torch.Size([2, 401]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input.shape, encoder_padding_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "058de538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.9166, -2.5645, 13.4132,  ...,  0.9873,  2.0047,  0.4048],\n",
       "         [ 9.2179, -2.3052, 14.9278,  ..., -0.3634,  1.1286,  0.5824],\n",
       "         [-3.0500,  4.4579, -1.0452,  ...,  0.5873, -0.4209,  2.4890],\n",
       "         ...,\n",
       "         [-0.6976, -1.4458, -1.5213,  ..., -0.2206,  0.3340,  0.1747],\n",
       "         [-0.7179, -1.4457, -1.5213,  ..., -0.2218,  0.3617,  0.1673],\n",
       "         [-0.7312, -1.4713, -1.5220,  ..., -0.1875,  0.2293, -0.0329]],\n",
       "\n",
       "        [[ 1.4333, -1.8173,  0.9116,  ...,  0.7428, -0.3993,  1.0221],\n",
       "         [ 0.5649, -2.0062,  0.0223,  ...,  0.6641,  0.0982,  1.4818],\n",
       "         [-0.2755, -2.7482, -0.7642,  ...,  0.4799,  0.2442,  1.1358],\n",
       "         ...,\n",
       "         [ 0.1279, -1.4121, -0.8527,  ...,  0.5464, -0.2747,  0.8503],\n",
       "         [ 0.0668, -1.0637, -1.1532,  ...,  0.5738, -0.1160,  1.2345],\n",
       "         [ 0.5193, -1.9056, -0.5770,  ...,  0.4975,  0.1598,  1.6285]]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3c07884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dbc6dd",
   "metadata": {},
   "source": [
    "If the weights and model were converted correctly, this should report zero or a very small number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4b099ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.max(torch.abs(encoder_input - hf_outputs[\"hidden_states\"]))\n",
    "torch.max(torch.abs(encoder_input - hf_outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5533cb26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7930eaf940>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.imshow(torch.abs(encoder_input - hf_outputs[\"hidden_states\"]).numpy()[0] > 1e-5)\n",
    "plt.imshow(torch.abs(encoder_input - hf_outputs[0]).numpy()[0] > 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bf93df",
   "metadata": {},
   "source": [
    "The line that is different is where the padding mask goes from 1 to 0; the original model handles this a little different than we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b92b758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.max(torch.abs(encoder_input - hf_outputs[\"extract_features\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d18286fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.abs(encoder_input - hf_outputs[\"last_hidden_state\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b76dc23",
   "metadata": {},
   "source": [
    "## Verify Transformer encoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "47c86f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run it with the original's speech prenet input:\n",
    "# with torch.no_grad():\n",
    "#     encoder_output = orig_model.encoder(encoder_input, encoder_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8b8b4df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run it with our input, which is slightly different (see above)\n",
    "with torch.no_grad():\n",
    "    encoder_output = orig_model.encoder(hf_encoder_input, ~hf_encoder_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "41ff76cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_out shape torch.Size([401, 2, 768])\n",
      "encoder_padding_mask shape torch.Size([2, 401])\n",
      "encoder_states []\n",
      "src_tokens []\n",
      "decoder_input [None]\n",
      "encoder_out_for_ctc shape torch.Size([401, 2, 81])\n"
     ]
    }
   ],
   "source": [
    "print(\"encoder_out shape\", encoder_output[\"encoder_out\"][0].shape)\n",
    "print(\"encoder_padding_mask shape\", encoder_output[\"encoder_padding_mask\"][0].shape)\n",
    "print(\"encoder_states\", encoder_output[\"encoder_states\"])  # []\n",
    "print(\"src_tokens\", encoder_output[\"src_tokens\"])  # []\n",
    "print(\"decoder_input\", encoder_output[\"decoder_input\"])  # [None]\n",
    "print(\"encoder_out_for_ctc shape\", encoder_output[\"encoder_out_for_ctc\"][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a38979d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2958, -0.1868, -0.5756,  ...,  0.3251, -0.8031,  0.2546],\n",
       "         [-0.3142, -0.1687, -0.6311,  ...,  0.2724, -0.8867,  0.1609],\n",
       "         [-0.3856, -0.1547, -0.7009,  ...,  0.2810, -0.7849,  0.0047],\n",
       "         ...,\n",
       "         [-0.2996, -0.2572, -0.2176,  ..., -0.0371, -0.3114,  0.2447],\n",
       "         [-0.2781, -0.2788, -0.1919,  ...,  0.1189, -0.2927,  0.1661],\n",
       "         [-0.2763, -0.2893, -0.2026,  ...,  0.1521, -0.2810,  0.1230]],\n",
       "\n",
       "        [[-0.2545, -0.2163, -0.4848,  ...,  0.3813, -0.5618,  0.3422],\n",
       "         [-0.1782, -0.2388, -0.4575,  ...,  0.3282, -0.5424,  0.2551],\n",
       "         [-0.1515, -0.3072, -0.3812,  ...,  0.2877, -0.4253,  0.1172],\n",
       "         ...,\n",
       "         [ 0.5395,  0.1442, -0.0540,  ...,  0.3096, -0.5798,  0.3530],\n",
       "         [ 0.2973,  0.0378, -0.0507,  ...,  0.2811, -0.5635,  0.2517],\n",
       "         [ 0.1936, -0.1404, -0.1436,  ...,  0.3061, -0.5138,  0.1664]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[\"encoder_out\"][0].permute((1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f9b9dda4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -9.7003,  -9.8230,  -9.8102,  ...,  -9.7416,  -9.6264,  12.8432],\n",
       "         [-10.0260, -10.0707, -10.1447,  ..., -10.0677,  -9.9987,  13.4569],\n",
       "         [-11.1218, -11.1262, -11.1428,  ..., -11.1431, -11.0712,  14.8944],\n",
       "         ...,\n",
       "         [-11.9023, -12.3399, -12.1404,  ..., -12.2093, -11.7069,   9.8889],\n",
       "         [-10.3436, -10.7269, -10.5387,  ..., -10.5104, -10.1166,   8.9614],\n",
       "         [ -9.7999, -10.1612,  -9.9818,  ...,  -9.9216,  -9.5704,   8.4546]],\n",
       "\n",
       "        [[ -9.9849, -10.1272, -10.0026,  ..., -10.0058,  -9.7552,  12.7793],\n",
       "         [-10.4426, -10.5391, -10.4868,  ..., -10.5314, -10.2572,  13.7857],\n",
       "         [-12.3113, -12.4078, -12.2937,  ..., -12.4932, -12.1741,  15.2556],\n",
       "         ...,\n",
       "         [ -9.1622,  -9.3113,  -9.2084,  ...,  -9.3590,  -9.0228,  11.5350],\n",
       "         [ -9.9882, -10.2963,  -9.9973,  ..., -10.0581,  -9.7548,  11.7940],\n",
       "         [-10.3344, -10.8017, -10.4995,  ..., -10.4289,  -9.9483,  10.9756]]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[\"encoder_out_for_ctc\"][0].permute((1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cde6dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use exact same inputs as the original model:\n",
    "# with torch.no_grad():\n",
    "#      hf_outputs = hf_model.speecht5(\n",
    "#          inputs_embeds=encoder_input,\n",
    "#          attention_mask=(~encoder_padding_mask),\n",
    "#      )\n",
    "\n",
    "# type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "28f529a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutput"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.encoder.wrapped_encoder(\n",
    "         hidden_states=hf_encoder_input,\n",
    "         attention_mask=hf_encoder_attention_mask,\n",
    "#          input_values=inputs.input_values,\n",
    "#          attention_mask=inputs.attention_mask,\n",
    "         return_dict=True,\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fb12e71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[-0.2958, -0.1868, -0.5756,  ...,  0.3251, -0.8031,  0.2546],\n",
       "         [-0.3142, -0.1687, -0.6311,  ...,  0.2724, -0.8867,  0.1609],\n",
       "         [-0.3856, -0.1547, -0.7009,  ...,  0.2810, -0.7849,  0.0047],\n",
       "         ...,\n",
       "         [-0.2996, -0.2572, -0.2176,  ..., -0.0371, -0.3114,  0.2447],\n",
       "         [-0.2781, -0.2788, -0.1919,  ...,  0.1189, -0.2927,  0.1661],\n",
       "         [-0.2763, -0.2893, -0.2026,  ...,  0.1521, -0.2810,  0.1230]],\n",
       "\n",
       "        [[-0.2545, -0.2163, -0.4848,  ...,  0.3813, -0.5618,  0.3422],\n",
       "         [-0.1782, -0.2388, -0.4575,  ...,  0.3282, -0.5424,  0.2551],\n",
       "         [-0.1515, -0.3072, -0.3812,  ...,  0.2877, -0.4253,  0.1172],\n",
       "         ...,\n",
       "         [ 0.5395,  0.1442, -0.0540,  ...,  0.3096, -0.5798,  0.3530],\n",
       "         [ 0.2973,  0.0378, -0.0507,  ...,  0.2811, -0.5635,  0.2517],\n",
       "         [ 0.1936, -0.1404, -0.1436,  ...,  0.3061, -0.5138,  0.1664]]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "06fb675b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last_hidden_state']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8fe9914e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 401, 768])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "69eda6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2958, -0.1868, -0.5756,  ...,  0.3251, -0.8031,  0.2546],\n",
       "         [-0.3142, -0.1687, -0.6311,  ...,  0.2724, -0.8867,  0.1609],\n",
       "         [-0.3856, -0.1547, -0.7009,  ...,  0.2810, -0.7849,  0.0047],\n",
       "         ...,\n",
       "         [-0.2996, -0.2572, -0.2176,  ..., -0.0371, -0.3114,  0.2447],\n",
       "         [-0.2781, -0.2788, -0.1919,  ...,  0.1189, -0.2927,  0.1661],\n",
       "         [-0.2763, -0.2893, -0.2026,  ...,  0.1521, -0.2810,  0.1230]],\n",
       "\n",
       "        [[-0.2545, -0.2163, -0.4848,  ...,  0.3813, -0.5618,  0.3422],\n",
       "         [-0.1782, -0.2388, -0.4575,  ...,  0.3282, -0.5424,  0.2551],\n",
       "         [-0.1515, -0.3072, -0.3812,  ...,  0.2877, -0.4253,  0.1172],\n",
       "         ...,\n",
       "         [ 0.5395,  0.1442, -0.0540,  ...,  0.3096, -0.5798,  0.3530],\n",
       "         [ 0.2973,  0.0378, -0.0507,  ...,  0.2811, -0.5635,  0.2517],\n",
       "         [ 0.1936, -0.1404, -0.1436,  ...,  0.3061, -0.5138,  0.1664]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a747e547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3246e-06)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(encoder_output[\"encoder_out\"][0].permute((1, 0, 2)) - hf_outputs[\"last_hidden_state\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7518e152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca490a88",
   "metadata": {},
   "source": [
    "## Verify CTC model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98055be9",
   "metadata": {},
   "source": [
    "This model only needs the encoder portion.\n",
    "\n",
    "This uses the same checkpoint as before: `speecht5_base_asr.pt` from https://huggingface.co/ajyy/SpeechT5\n",
    "\n",
    "Run the following to convert, using your own `--checkpoint_path` and `--pytorch_dump_folder_path`:\n",
    "\n",
    "```nohighlight\n",
    "cd transformers/src/transformers/models/speecht5\n",
    "\n",
    "python convert_speecht5_original_pytorch_checkpoint_to_pytorch.py \\\n",
    "  --task ctc \\\n",
    "  --checkpoint_path /path/to/SpeechT5/speecht5_base_asr.pt \n",
    "  --pytorch_dump_folder_path /some/other/path\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "69ca25a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_model_ctc = SpeechT5ForCTC(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5a32978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_ctc = \"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_base_ctc\"\n",
    "hf_model_ctc = SpeechT5ForCTC.from_pretrained(model_checkpoint_ctc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "06126c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure if this should work, since SpeechT5ForCTC does not have a speecht5 property...\n",
    "#hf_model_ctc = SpeechT5Model.from_pretrained(model_checkpoint_ctc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8d81d7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.CausalLMOutput"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the full model:\n",
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model_ctc(**inputs)\n",
    "\n",
    "# Run without attention_mask:\n",
    "# with torch.no_grad():\n",
    "#     hf_outputs = hf_model_ctc(input_values=inputs[\"input_values\"])\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "de48ff1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.5367e-06)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(encoder_output[\"encoder_out_for_ctc\"][0].permute((1, 0, 2)) - hf_outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "32fabb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.nn.functional.softmax(hf_outputs[0], dim=-1, dtype=torch.float32)\n",
    "probs = probs.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "267f80ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[80, 80, 80, 80, 80, 80,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "          4,  4,  4,  4, 46, 80, 80, 16, 80, 80, 12, 12,  6,  6,  6, 80, 80, 80,\n",
       "         80,  4,  4,  4,  6,  6, 11, 11, 13, 13, 13, 80, 16, 16, 80, 80, 80, 80,\n",
       "         12, 12,  6,  6, 80, 80, 80, 80, 80, 80, 80, 80,  4,  4,  4, 80, 80,  7,\n",
       "          9, 14, 14,  4,  4, 80, 24, 80, 80, 80, 80,  5, 13, 13, 13, 13, 13, 13,\n",
       "         80, 22, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80,  4,  4,  4,  4,  4,  4,  4, 80,  7,  7,  9, 14,\n",
       "         14, 80,  4,  4,  4, 27, 80, 80, 10, 80, 17, 17,  6,  6,  6,  8, 13, 13,\n",
       "         13, 22, 80, 80, 80,  4,  4,  6,  8,  8, 80,  4,  6,  6, 11,  5,  5,  4,\n",
       "         12, 12, 80,  6,  6, 13, 13, 80, 80,  8, 80,  9,  9, 80, 21, 21, 80, 80,\n",
       "          5, 13, 13, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80],\n",
       "        [80, 80, 80, 80,  4,  4,  4,  4,  4,  4,  4, 12, 12, 80, 80, 80,  5,  5,\n",
       "         15, 15, 15, 19, 19, 80,  4,  4, 14, 14, 80,  5, 80, 80, 12, 12, 12, 80,\n",
       "          6,  6, 13, 13, 16, 16, 16, 80, 17, 17,  6,  4,  4, 20, 10, 15, 80, 80,\n",
       "         15,  4, 17, 17,  8,  8, 18, 80, 80, 18, 80,  5,  9,  9, 80, 80, 17,  5,\n",
       "         80,  4, 80, 10,  9, 80, 80,  4,  4, 19, 19, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 10, 10, 80, 80, 27,  5,  5, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80,  4,  4,  4,  4,  4,  4,  4, 19, 19, 80, 80,\n",
       "         80,  8,  8, 16, 16, 13, 13, 13, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80,  4,  4,  4,  4,  4,  4,  6,  6, 11, 11, 13, 13,\n",
       "         13,  5,  5, 80, 80,  5,  5,  5, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80,  4,  4,  4,  4,  4,  4,  6,  6,  6, 20, 20, 80, 80,\n",
       "          8,  8, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80,  4,  4,  4,  4,  4,  4,  4,  4, 80, 80, 80,  8,  9,  9,\n",
       "          5,  5,  4,  4, 80,  7,  9,  9, 14,  4,  4,  4,  7,  4,  4, 11, 11, 80,\n",
       "         80, 80,  7,  7,  7, 15, 15, 15, 19, 19, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80,  4,  4,  4,  4,  4,  4,  4, 80, 80,  8,  8,  9,\n",
       "          9,  5,  5, 80,  4,  4, 80, 80,  7,  9,  9, 14,  4,  4,  7,  7,  4,  4,\n",
       "         45, 45, 16, 16, 16,  7,  7, 13, 13, 80, 80,  6, 80,  5, 13, 13, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,  4,  4,  4,  4,  4,\n",
       "          4,  4, 80, 80,  8,  8,  9,  9,  5,  4,  4, 80,  7,  9,  9, 14,  4,  4,\n",
       "          7,  4,  4, 15, 10,  6,  6, 80,  6,  6, 15,  5,  5,  4, 25, 25, 80, 10,\n",
       "         10, 80,  6,  6, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "90ca3c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tgt_dict.string(probs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5bcf370d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                jussttt   tthhrrruusstt   andd  perrrrrry       aandd   vicctttorrry  too tthee ssttrronnggerr\n",
      "\n",
      "       sseelllff  ddesssttrruuucct  will ccoommennce in  ffiivee       ffoouurrr      tthhrrreeeee      tttwwoo        onnee  annd   a  hhaaalllff       oonnee  annd  aa  qquuuaarrterr       oonne  annd  a  littttlee bbiitt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(probs.shape[0]):\n",
    "    print(tokenizer.decode(tgt_dict.string(probs[i])).replace(\"<ctc_blank>\", \"\"))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3a62ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c2e133f",
   "metadata": {},
   "source": [
    "Calculate loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed2bca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model_ctc(\n",
    "         **inputs, \n",
    "         labels=torch.tensor(\n",
    "           [[ 46, 16, 12,  6,  4,  6, 11, 13, 16, 12,  6,  4,  7,  9, 14,  4,\n",
    "         24,  7, 13, 13, 22,  4,  7,  9, 14,  4, 27, 10, 17,  6,  8, 13, 22,  4,\n",
    "          6,  8,  4,  6, 11,  5,  4, 12,  6, 13,  8,  9, 21,  5, 13, ]]\n",
    "         ),\n",
    "         output_hidden_states=True,\n",
    "         return_dict=True,\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ab7f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135848ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b17bb840",
   "metadata": {},
   "source": [
    "## Verify text decoder prenet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9ba48d",
   "metadata": {},
   "source": [
    "First this calls `text_decoder_prenet`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3aad9e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = inputs.input_values.size(0)\n",
    "beam_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "28dcb259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = torch.tensor([2, 4, 46, 16, 12, 16] * beam_size * batch_size).reshape(beam_size * batch_size, -1)\n",
    "tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f335b565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one has padding (token_id = 1)\n",
    "# The results will be different with the HF implementation because\n",
    "# we don't set the attention_mask to 0 for padding tokens\n",
    "# tokens = torch.tensor([2, 4, 46, 16, 1, 12] * beam_size * batch_size).reshape(beam_size * batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f09f744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    prev_output_tokens, tgt_mask, incremental_state = orig_model.text_decoder_prenet(tokens, incremental_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "75d7d036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This returns a sequence length of 1\n",
    "# with torch.no_grad():\n",
    "#     prev_output_tokens, tgt_mask, incremental_state = orig_model.text_decoder_prenet(tokens, incremental_state={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "725cfc62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6, 768])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: misleading name; these are not the actual tokens but their embeddings!\n",
    "prev_output_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "84764c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "81026d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "incremental_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "79399ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.decoder.prenet(\n",
    "         input_ids=tokens,\n",
    "#          attention_mask=hf_encoder_attention_mask,\n",
    "#          attention_mask=inputs.attention_mask,\n",
    "#          past_key_values=[(torch.ones(5, 1, 1), torch.ones(5, 1, 1))],\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6ca35f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeds_hf, decoder_attention_mask = hf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5fe38c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6, 768])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeds_hf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fc01ea7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(prev_output_tokens - token_embeds_hf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a7f8ff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671824d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fec354d",
   "metadata": {},
   "source": [
    "## Verify Transformer decoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "42a4fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = source.size(0)\n",
    "new_order = torch.arange(bsz).view(-1, 1).repeat(1, beam_size).view(-1)\n",
    "encoder_outs = orig_model.encoder.reorder_encoder_out(encoder_output, new_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a98652fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1074,  1.1042,  1.0387,  ...,  0.9619,  1.0463,  0.8678],\n",
       "         [ 0.0890,  0.1702,  0.3139,  ...,  0.8629,  0.7833,  0.9348],\n",
       "         [-0.7402, -0.4274, -0.5774,  ...,  1.3103,  0.5403,  0.7292],\n",
       "         [-0.8579, -0.9214, -0.9064,  ...,  1.0331,  0.8949,  0.9723],\n",
       "         [-0.2291, -0.4457, -0.5671,  ...,  0.9991,  0.8768,  0.9128],\n",
       "         [ 0.7580,  0.5875,  0.4706,  ...,  1.0331,  0.8949,  0.9723]],\n",
       "\n",
       "        [[ 1.1074,  1.1042,  1.0387,  ...,  0.9619,  1.0463,  0.8678],\n",
       "         [ 0.0890,  0.1702,  0.3139,  ...,  0.8629,  0.7833,  0.9348],\n",
       "         [-0.7402, -0.4274, -0.5774,  ...,  1.3103,  0.5403,  0.7292],\n",
       "         [-0.8579, -0.9214, -0.9064,  ...,  1.0331,  0.8949,  0.9723],\n",
       "         [-0.2291, -0.4457, -0.5671,  ...,  0.9991,  0.8768,  0.9128],\n",
       "         [ 0.7580,  0.5875,  0.4706,  ...,  1.0331,  0.8949,  0.9723]],\n",
       "\n",
       "        [[ 1.1074,  1.1042,  1.0387,  ...,  0.9619,  1.0463,  0.8678],\n",
       "         [ 0.0890,  0.1702,  0.3139,  ...,  0.8629,  0.7833,  0.9348],\n",
       "         [-0.7402, -0.4274, -0.5774,  ...,  1.3103,  0.5403,  0.7292],\n",
       "         [-0.8579, -0.9214, -0.9064,  ...,  1.0331,  0.8949,  0.9723],\n",
       "         [-0.2291, -0.4457, -0.5671,  ...,  0.9991,  0.8768,  0.9128],\n",
       "         [ 0.7580,  0.5875,  0.4706,  ...,  1.0331,  0.8949,  0.9723]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.1074,  1.1042,  1.0387,  ...,  0.9619,  1.0463,  0.8678],\n",
       "         [ 0.0890,  0.1702,  0.3139,  ...,  0.8629,  0.7833,  0.9348],\n",
       "         [-0.7402, -0.4274, -0.5774,  ...,  1.3103,  0.5403,  0.7292],\n",
       "         [-0.8579, -0.9214, -0.9064,  ...,  1.0331,  0.8949,  0.9723],\n",
       "         [-0.2291, -0.4457, -0.5671,  ...,  0.9991,  0.8768,  0.9128],\n",
       "         [ 0.7580,  0.5875,  0.4706,  ...,  1.0331,  0.8949,  0.9723]],\n",
       "\n",
       "        [[ 1.1074,  1.1042,  1.0387,  ...,  0.9619,  1.0463,  0.8678],\n",
       "         [ 0.0890,  0.1702,  0.3139,  ...,  0.8629,  0.7833,  0.9348],\n",
       "         [-0.7402, -0.4274, -0.5774,  ...,  1.3103,  0.5403,  0.7292],\n",
       "         [-0.8579, -0.9214, -0.9064,  ...,  1.0331,  0.8949,  0.9723],\n",
       "         [-0.2291, -0.4457, -0.5671,  ...,  0.9991,  0.8768,  0.9128],\n",
       "         [ 0.7580,  0.5875,  0.4706,  ...,  1.0331,  0.8949,  0.9723]],\n",
       "\n",
       "        [[ 1.1074,  1.1042,  1.0387,  ...,  0.9619,  1.0463,  0.8678],\n",
       "         [ 0.0890,  0.1702,  0.3139,  ...,  0.8629,  0.7833,  0.9348],\n",
       "         [-0.7402, -0.4274, -0.5774,  ...,  1.3103,  0.5403,  0.7292],\n",
       "         [-0.8579, -0.9214, -0.9064,  ...,  1.0331,  0.8949,  0.9723],\n",
       "         [-0.2291, -0.4457, -0.5671,  ...,  0.9991,  0.8768,  0.9128],\n",
       "         [ 0.7580,  0.5875,  0.4706,  ...,  1.0331,  0.8949,  0.9723]]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6229b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    decoder_output, extra = orig_model.decoder(\n",
    "        prev_output_tokens,\n",
    "        tgt_mask,\n",
    "        encoder_out=encoder_outs,\n",
    "        incremental_state=incremental_state,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0a190958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6, 768])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fea0351e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5030, -0.3445, -0.2332,  ..., -0.2787, -0.4045,  0.0066],\n",
       "         [-0.2348, -0.0063, -0.7550,  ..., -0.1974, -0.6192, -0.0027],\n",
       "         [-0.0907, -0.0461, -0.4361,  ...,  0.1990, -0.2789,  0.1868],\n",
       "         [ 0.8854, -0.3577, -1.4784,  ...,  0.1855,  0.1554, -0.3780],\n",
       "         [ 0.5764, -0.4550, -0.2252,  ..., -0.4585, -0.1639, -0.5217],\n",
       "         [ 0.7981, -0.3895, -0.8105,  ...,  0.1244,  0.2380, -0.2574]],\n",
       "\n",
       "        [[-0.5030, -0.3445, -0.2332,  ..., -0.2787, -0.4045,  0.0066],\n",
       "         [-0.2348, -0.0063, -0.7550,  ..., -0.1974, -0.6192, -0.0027],\n",
       "         [-0.0907, -0.0461, -0.4361,  ...,  0.1990, -0.2789,  0.1868],\n",
       "         [ 0.8854, -0.3577, -1.4784,  ...,  0.1855,  0.1554, -0.3780],\n",
       "         [ 0.5764, -0.4550, -0.2252,  ..., -0.4585, -0.1639, -0.5217],\n",
       "         [ 0.7981, -0.3895, -0.8105,  ...,  0.1244,  0.2380, -0.2574]],\n",
       "\n",
       "        [[-0.5030, -0.3445, -0.2332,  ..., -0.2787, -0.4045,  0.0066],\n",
       "         [-0.2348, -0.0063, -0.7550,  ..., -0.1974, -0.6192, -0.0027],\n",
       "         [-0.0907, -0.0461, -0.4361,  ...,  0.1990, -0.2789,  0.1868],\n",
       "         [ 0.8854, -0.3577, -1.4784,  ...,  0.1855,  0.1554, -0.3780],\n",
       "         [ 0.5764, -0.4550, -0.2252,  ..., -0.4585, -0.1639, -0.5217],\n",
       "         [ 0.7981, -0.3895, -0.8105,  ...,  0.1244,  0.2380, -0.2574]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.4469, -0.2283, -0.0834,  ..., -0.2076, -0.4499,  0.0166],\n",
       "         [ 0.0655, -0.4424, -1.4536,  ..., -0.2090, -0.1812, -0.1850],\n",
       "         [ 0.6125, -0.1873, -0.7856,  ...,  0.1821, -0.3962,  0.9259],\n",
       "         [ 0.8581, -0.5432, -1.7023,  ..., -0.0384,  0.2454,  0.0275],\n",
       "         [ 0.8709, -0.2559, -0.4044,  ..., -0.4054, -0.4117, -0.0879],\n",
       "         [ 0.6939, -0.6869, -0.7209,  ...,  0.2299,  0.0294,  0.2049]],\n",
       "\n",
       "        [[-0.4469, -0.2283, -0.0834,  ..., -0.2076, -0.4499,  0.0166],\n",
       "         [ 0.0655, -0.4424, -1.4536,  ..., -0.2090, -0.1812, -0.1850],\n",
       "         [ 0.6125, -0.1873, -0.7856,  ...,  0.1821, -0.3962,  0.9259],\n",
       "         [ 0.8581, -0.5432, -1.7023,  ..., -0.0384,  0.2454,  0.0275],\n",
       "         [ 0.8709, -0.2559, -0.4044,  ..., -0.4054, -0.4117, -0.0879],\n",
       "         [ 0.6939, -0.6869, -0.7209,  ...,  0.2299,  0.0294,  0.2049]],\n",
       "\n",
       "        [[-0.4469, -0.2283, -0.0834,  ..., -0.2076, -0.4499,  0.0166],\n",
       "         [ 0.0655, -0.4424, -1.4536,  ..., -0.2090, -0.1812, -0.1850],\n",
       "         [ 0.6125, -0.1873, -0.7856,  ...,  0.1821, -0.3962,  0.9259],\n",
       "         [ 0.8581, -0.5432, -1.7023,  ..., -0.0384,  0.2454,  0.0275],\n",
       "         [ 0.8709, -0.2559, -0.4044,  ..., -0.4054, -0.4117, -0.0879],\n",
       "         [ 0.6939, -0.6869, -0.7209,  ...,  0.2299,  0.0294,  0.2049]]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "05117956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[x.shape for x in extra[\"attn\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b593692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[x.shape for x in extra[\"inner_states\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9d486f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.decoder.wrapped_decoder(\n",
    "         hidden_states=prev_output_tokens,\n",
    "         attention_mask=decoder_attention_mask,\n",
    "         encoder_hidden_states=encoder_outs[\"encoder_out\"][0].permute((1, 0, 2)),\n",
    "         encoder_attention_mask=hf_encoder_attention_mask.repeat((1, beam_size)).view(beam_size * batch_size, -1),\n",
    "         return_dict=True,\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ea512cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last_hidden_state', 'past_key_values']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d82c369a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6, 768])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "eb6282bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5030, -0.3445, -0.2332,  ..., -0.2787, -0.4045,  0.0066],\n",
       "         [-0.2348, -0.0063, -0.7550,  ..., -0.1974, -0.6192, -0.0027],\n",
       "         [-0.0907, -0.0461, -0.4361,  ...,  0.1990, -0.2789,  0.1868],\n",
       "         [ 0.8854, -0.3577, -1.4784,  ...,  0.1855,  0.1554, -0.3780],\n",
       "         [ 0.5764, -0.4550, -0.2252,  ..., -0.4585, -0.1639, -0.5217],\n",
       "         [ 0.7981, -0.3895, -0.8105,  ...,  0.1244,  0.2380, -0.2574]],\n",
       "\n",
       "        [[-0.5030, -0.3445, -0.2332,  ..., -0.2787, -0.4045,  0.0066],\n",
       "         [-0.2348, -0.0063, -0.7550,  ..., -0.1974, -0.6192, -0.0027],\n",
       "         [-0.0907, -0.0461, -0.4361,  ...,  0.1990, -0.2789,  0.1868],\n",
       "         [ 0.8854, -0.3577, -1.4784,  ...,  0.1855,  0.1554, -0.3780],\n",
       "         [ 0.5764, -0.4550, -0.2252,  ..., -0.4585, -0.1639, -0.5217],\n",
       "         [ 0.7981, -0.3895, -0.8105,  ...,  0.1244,  0.2380, -0.2574]],\n",
       "\n",
       "        [[-0.5030, -0.3445, -0.2332,  ..., -0.2787, -0.4045,  0.0066],\n",
       "         [-0.2348, -0.0063, -0.7550,  ..., -0.1974, -0.6192, -0.0027],\n",
       "         [-0.0907, -0.0461, -0.4361,  ...,  0.1990, -0.2789,  0.1868],\n",
       "         [ 0.8854, -0.3577, -1.4784,  ...,  0.1855,  0.1554, -0.3780],\n",
       "         [ 0.5764, -0.4550, -0.2252,  ..., -0.4585, -0.1639, -0.5217],\n",
       "         [ 0.7981, -0.3895, -0.8105,  ...,  0.1244,  0.2380, -0.2574]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.4469, -0.2283, -0.0834,  ..., -0.2076, -0.4499,  0.0166],\n",
       "         [ 0.0655, -0.4424, -1.4536,  ..., -0.2090, -0.1812, -0.1850],\n",
       "         [ 0.6125, -0.1873, -0.7856,  ...,  0.1821, -0.3962,  0.9259],\n",
       "         [ 0.8581, -0.5432, -1.7023,  ..., -0.0384,  0.2454,  0.0275],\n",
       "         [ 0.8709, -0.2559, -0.4044,  ..., -0.4054, -0.4117, -0.0879],\n",
       "         [ 0.6939, -0.6869, -0.7209,  ...,  0.2299,  0.0294,  0.2049]],\n",
       "\n",
       "        [[-0.4469, -0.2283, -0.0834,  ..., -0.2076, -0.4499,  0.0166],\n",
       "         [ 0.0655, -0.4424, -1.4536,  ..., -0.2090, -0.1812, -0.1850],\n",
       "         [ 0.6125, -0.1873, -0.7856,  ...,  0.1821, -0.3962,  0.9259],\n",
       "         [ 0.8581, -0.5432, -1.7023,  ..., -0.0384,  0.2454,  0.0275],\n",
       "         [ 0.8709, -0.2559, -0.4044,  ..., -0.4054, -0.4117, -0.0879],\n",
       "         [ 0.6939, -0.6869, -0.7209,  ...,  0.2299,  0.0294,  0.2049]],\n",
       "\n",
       "        [[-0.4469, -0.2283, -0.0834,  ..., -0.2076, -0.4499,  0.0166],\n",
       "         [ 0.0655, -0.4424, -1.4536,  ..., -0.2090, -0.1812, -0.1850],\n",
       "         [ 0.6125, -0.1873, -0.7856,  ...,  0.1821, -0.3962,  0.9259],\n",
       "         [ 0.8581, -0.5432, -1.7023,  ..., -0.0384,  0.2454,  0.0275],\n",
       "         [ 0.8709, -0.2559, -0.4044,  ..., -0.4054, -0.4117, -0.0879],\n",
       "         [ 0.6939, -0.6869, -0.7209,  ...,  0.2299,  0.0294,  0.2049]]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f3e254cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4305e-06)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(decoder_output - hf_outputs[\"last_hidden_state\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d3d1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d9113f7",
   "metadata": {},
   "source": [
    "## Verify text decoder postnet output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a20be43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    lprobs = orig_model.text_decoder_postnet(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a7a143bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6, 81])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lprobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7d6de14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1653e+01, -2.1903e+01,  1.1883e+00, -2.1865e+01,  1.6534e+01,\n",
       "          5.5673e-02,  3.0473e-01, -1.1260e+00, -2.9652e-01, -1.6258e+00,\n",
       "         -8.2225e-02, -4.3624e-02, -1.7976e-01, -8.3170e-01, -2.3726e+00,\n",
       "         -1.1654e+00, -8.4005e-01, -2.3911e+00, -9.3855e-01, -6.0906e-01,\n",
       "          9.2375e-01, -6.6507e-02, -2.8258e+00, -2.1986e+01, -1.3389e+00,\n",
       "          7.0116e-01, -2.1933e+01, -3.1611e+00, -2.5848e+00, -2.1882e+01,\n",
       "         -2.1913e+01,  3.2156e-01, -2.2011e+01, -2.1879e+01, -2.1878e+01,\n",
       "         -2.1948e+01, -2.1919e+01, -5.7559e+00, -2.1884e+01, -2.1979e+01,\n",
       "         -2.1856e+01, -2.1932e+01, -2.1922e+01, -2.1933e+01, -2.1903e+01,\n",
       "         -2.5441e+00, -1.3407e+00, -2.1845e+01, -2.1888e+01, -2.2011e+01,\n",
       "         -2.1885e+01, -2.1964e+01, -2.1863e+01, -2.1848e+01, -2.1898e+01,\n",
       "         -2.1841e+01, -2.1898e+01, -5.9434e+00, -2.1915e+01, -2.1874e+01,\n",
       "         -2.1929e+01, -2.1979e+01, -2.1919e+01, -2.1892e+01, -2.1897e+01,\n",
       "         -2.1925e+01, -2.1900e+01, -2.1952e+01, -2.1941e+01, -2.1896e+01,\n",
       "         -2.1816e+01, -2.1927e+01, -2.1859e+01, -2.1899e+01, -2.1847e+01,\n",
       "         -2.1901e+01, -2.1867e+01, -2.1888e+01, -2.1875e+01, -2.0533e+01,\n",
       "         -2.1867e+01],\n",
       "        [-1.1627e+01, -1.2799e+01, -8.0016e+00, -1.2712e+01, -2.6730e+00,\n",
       "          1.4944e-01,  4.1157e+00,  2.9334e+00, -4.6105e-01, -3.7645e-01,\n",
       "          2.4234e+00, -2.4231e-01,  1.9914e+00, -4.2823e-01,  4.5774e+00,\n",
       "         -2.0381e+00,  8.0717e-04,  3.7897e+00,  2.1376e+00,  1.2677e+00,\n",
       "          5.6208e-01,  5.3169e+00,  2.8548e+00, -1.2709e+01, -1.3019e-02,\n",
       "          2.0820e+00, -1.2704e+01, -5.1777e-01, -3.9188e+00, -1.2682e+01,\n",
       "         -1.2723e+01, -5.5099e+00, -1.2664e+01, -1.2626e+01, -1.2692e+01,\n",
       "         -1.2715e+01, -1.2719e+01, -5.1495e+00, -1.2636e+01, -1.2715e+01,\n",
       "         -1.2696e+01, -1.2669e+01, -1.2717e+01, -1.2687e+01, -1.2687e+01,\n",
       "         -2.3778e+00,  1.3126e+01, -1.2674e+01, -1.2721e+01, -1.2773e+01,\n",
       "         -1.2729e+01, -1.2723e+01, -1.2683e+01, -1.2615e+01, -1.2660e+01,\n",
       "         -1.2657e+01, -1.2647e+01, -1.0827e+00, -1.2692e+01, -1.2722e+01,\n",
       "         -1.2708e+01, -1.2722e+01, -1.2799e+01, -1.2665e+01, -1.2673e+01,\n",
       "         -1.2692e+01, -1.2710e+01, -1.2660e+01, -1.2735e+01, -1.2736e+01,\n",
       "         -1.2635e+01, -1.2713e+01, -1.2631e+01, -1.2619e+01, -1.2689e+01,\n",
       "         -1.2717e+01, -1.2739e+01, -1.2699e+01, -1.2722e+01, -1.2031e+01,\n",
       "         -1.2639e+01],\n",
       "        [-1.2027e+01, -1.2743e+01, -2.7108e+00, -1.2900e+01,  2.1201e+00,\n",
       "          7.8642e+00,  2.2893e-01,  3.8414e+00,  4.2161e+00,  1.0947e-01,\n",
       "          6.8731e+00, -8.1462e-01,  1.1705e+00, -1.6191e+00, -3.6515e+00,\n",
       "         -4.0879e+00,  1.3789e+01, -4.1417e+00, -3.8301e+00, -3.5975e+00,\n",
       "         -4.1099e+00, -3.9104e+00,  2.7459e+00, -1.3010e+01, -3.6717e+00,\n",
       "         -5.0438e+00, -1.2909e+01, -2.2472e+00, -6.0670e+00, -1.2901e+01,\n",
       "         -1.2984e+01,  2.7568e+00, -1.2809e+01, -1.2877e+01, -1.2889e+01,\n",
       "         -1.2881e+01, -1.2984e+01, -4.1106e+00, -1.2971e+01, -1.2929e+01,\n",
       "         -1.2969e+01, -1.2914e+01, -1.2959e+01, -1.2892e+01, -1.2964e+01,\n",
       "         -6.6020e+00, -1.5731e-01, -1.2941e+01, -1.2904e+01, -1.2929e+01,\n",
       "         -1.2882e+01, -1.2922e+01, -1.2992e+01, -1.2769e+01, -1.2937e+01,\n",
       "         -1.2851e+01, -1.2887e+01, -2.9979e+00, -1.2990e+01, -1.2956e+01,\n",
       "         -1.2922e+01, -1.2869e+01, -1.3010e+01, -1.2913e+01, -1.2968e+01,\n",
       "         -1.2911e+01, -1.2929e+01, -1.2871e+01, -1.2905e+01, -1.2999e+01,\n",
       "         -1.2951e+01, -1.2986e+01, -1.2890e+01, -1.2871e+01, -1.2864e+01,\n",
       "         -1.2835e+01, -1.2918e+01, -1.2959e+01, -1.2958e+01, -1.2740e+01,\n",
       "         -1.2845e+01],\n",
       "        [-1.8595e+01, -1.5520e+01, -1.2537e+00, -1.5532e+01,  2.9053e+00,\n",
       "         -3.8047e+00,  5.3872e+00,  6.7947e-01, -2.4162e+00,  2.1910e+00,\n",
       "          1.4852e+00, -1.7036e+00,  1.8714e+01,  1.6990e+00,  3.1962e+00,\n",
       "          1.8808e+00, -1.6671e+00, -1.9222e+00,  7.7780e-01, -1.3891e+00,\n",
       "         -5.1268e+00,  9.8080e-01, -2.8854e+00, -1.5707e+01,  1.9282e-02,\n",
       "          1.5668e-01, -1.5553e+01, -1.4532e+00, -4.8829e+00, -1.5710e+01,\n",
       "         -1.5660e+01,  6.1362e-01, -1.5660e+01, -1.5689e+01, -1.5613e+01,\n",
       "         -1.5637e+01, -1.5718e+01, -1.4889e+00, -1.5593e+01, -1.5605e+01,\n",
       "         -1.5612e+01, -1.5696e+01, -1.5674e+01, -1.5620e+01, -1.5673e+01,\n",
       "         -4.9521e+00, -4.2685e+00, -1.5845e+01, -1.5708e+01, -1.5597e+01,\n",
       "         -1.5682e+01, -1.5695e+01, -1.5622e+01, -1.5515e+01, -1.5800e+01,\n",
       "         -1.5591e+01, -1.5645e+01,  1.2637e-01, -1.5715e+01, -1.5595e+01,\n",
       "         -1.5694e+01, -1.5657e+01, -1.5636e+01, -1.5613e+01, -1.5733e+01,\n",
       "         -1.5695e+01, -1.5622e+01, -1.5722e+01, -1.5575e+01, -1.5668e+01,\n",
       "         -1.5697e+01, -1.5739e+01, -1.5733e+01, -1.5769e+01, -1.5603e+01,\n",
       "         -1.5610e+01, -1.5524e+01, -1.5721e+01, -1.5710e+01, -1.5128e+01,\n",
       "         -1.5508e+01],\n",
       "        [-1.1863e+01, -1.4547e+01,  2.2033e+00, -1.4225e+01,  9.2466e+00,\n",
       "         -1.5381e+00,  1.8949e+01, -1.9246e+00, -1.9959e+00, -1.0645e+00,\n",
       "         -1.5536e+00, -5.2282e-01,  5.3303e+00, -4.8001e+00, -1.2240e+00,\n",
       "         -2.8890e+00, -1.8417e+00, -1.6757e-01, -3.9733e+00, -3.3923e-01,\n",
       "         -3.7919e+00,  5.9325e-01, -1.7867e+00, -1.4290e+01,  2.6420e+00,\n",
       "         -2.8597e+00, -1.4238e+01, -2.4263e+00,  4.5340e-02, -1.4275e+01,\n",
       "         -1.4268e+01,  3.6714e+00, -1.4260e+01, -1.4363e+01, -1.4257e+01,\n",
       "         -1.4302e+01, -1.4241e+01, -3.6297e+00, -1.4286e+01, -1.4217e+01,\n",
       "         -1.4212e+01, -1.4250e+01, -1.4274e+01, -1.4315e+01, -1.4312e+01,\n",
       "          3.6341e-01, -6.0877e+00, -1.4245e+01, -1.4421e+01, -1.4286e+01,\n",
       "         -1.4330e+01, -1.4328e+01, -1.4275e+01, -1.4162e+01, -1.4297e+01,\n",
       "         -1.4322e+01, -1.4229e+01, -8.2361e-01, -1.4335e+01, -1.4328e+01,\n",
       "         -1.4316e+01, -1.4197e+01, -1.4256e+01, -1.4219e+01, -1.4303e+01,\n",
       "         -1.4277e+01, -1.4276e+01, -1.4405e+01, -1.4259e+01, -1.4276e+01,\n",
       "         -1.4203e+01, -1.4461e+01, -1.4276e+01, -1.4371e+01, -1.4253e+01,\n",
       "         -1.4313e+01, -1.4214e+01, -1.4323e+01, -1.4276e+01, -1.3283e+01,\n",
       "         -1.4253e+01],\n",
       "        [-2.1113e+01, -1.6734e+01,  5.7924e-01, -1.6765e+01,  7.7072e+00,\n",
       "         -1.5670e-01,  6.1144e+00,  9.5721e-01, -8.1220e+00,  6.7769e-01,\n",
       "         -3.9384e-01,  9.6266e-01,  1.0624e+01,  1.4706e+00,  4.8160e-01,\n",
       "          4.1931e+00, -7.0178e+00,  4.5149e-01, -4.7414e-01, -2.0390e-01,\n",
       "         -7.3246e+00,  2.5347e+00, -8.1403e+00, -1.6765e+01,  3.1180e+00,\n",
       "         -2.8665e+00, -1.6682e+01, -1.9554e+00, -1.2680e+00, -1.6870e+01,\n",
       "         -1.6891e+01,  1.3927e+00, -1.6831e+01, -1.6869e+01, -1.6648e+01,\n",
       "         -1.6839e+01, -1.6798e+01, -2.6841e+00, -1.6656e+01, -1.6778e+01,\n",
       "         -1.6682e+01, -1.6842e+01, -1.6842e+01, -1.6688e+01, -1.6873e+01,\n",
       "         -6.0640e+00, -4.6538e+00, -1.6940e+01, -1.6871e+01, -1.6724e+01,\n",
       "         -1.6940e+01, -1.6846e+01, -1.6825e+01, -1.6674e+01, -1.7015e+01,\n",
       "         -1.6862e+01, -1.6814e+01, -3.6806e+00, -1.6807e+01, -1.6768e+01,\n",
       "         -1.6820e+01, -1.6859e+01, -1.6670e+01, -1.6824e+01, -1.6886e+01,\n",
       "         -1.6886e+01, -1.6834e+01, -1.6963e+01, -1.6751e+01, -1.6790e+01,\n",
       "         -1.6835e+01, -1.6818e+01, -1.6977e+01, -1.6829e+01, -1.6774e+01,\n",
       "         -1.6769e+01, -1.6748e+01, -1.6865e+01, -1.6948e+01, -1.5303e+01,\n",
       "         -1.6689e+01]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lprobs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ba7571e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    lprobs_hf = hf_model.text_decoder_postnet(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5623a791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6, 81])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lprobs_hf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "64f0c976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1653e+01, -2.1903e+01,  1.1883e+00, -2.1865e+01,  1.6534e+01,\n",
       "          5.5673e-02,  3.0473e-01, -1.1260e+00, -2.9652e-01, -1.6258e+00,\n",
       "         -8.2225e-02, -4.3624e-02, -1.7976e-01, -8.3170e-01, -2.3726e+00,\n",
       "         -1.1654e+00, -8.4005e-01, -2.3911e+00, -9.3855e-01, -6.0906e-01,\n",
       "          9.2375e-01, -6.6507e-02, -2.8258e+00, -2.1986e+01, -1.3389e+00,\n",
       "          7.0116e-01, -2.1933e+01, -3.1611e+00, -2.5848e+00, -2.1882e+01,\n",
       "         -2.1913e+01,  3.2156e-01, -2.2011e+01, -2.1879e+01, -2.1878e+01,\n",
       "         -2.1948e+01, -2.1919e+01, -5.7559e+00, -2.1884e+01, -2.1979e+01,\n",
       "         -2.1856e+01, -2.1932e+01, -2.1922e+01, -2.1933e+01, -2.1903e+01,\n",
       "         -2.5441e+00, -1.3407e+00, -2.1845e+01, -2.1888e+01, -2.2011e+01,\n",
       "         -2.1885e+01, -2.1964e+01, -2.1863e+01, -2.1848e+01, -2.1898e+01,\n",
       "         -2.1841e+01, -2.1898e+01, -5.9434e+00, -2.1915e+01, -2.1874e+01,\n",
       "         -2.1929e+01, -2.1979e+01, -2.1919e+01, -2.1892e+01, -2.1897e+01,\n",
       "         -2.1925e+01, -2.1900e+01, -2.1952e+01, -2.1941e+01, -2.1896e+01,\n",
       "         -2.1816e+01, -2.1927e+01, -2.1859e+01, -2.1899e+01, -2.1847e+01,\n",
       "         -2.1901e+01, -2.1867e+01, -2.1888e+01, -2.1875e+01, -2.0533e+01,\n",
       "         -2.1867e+01],\n",
       "        [-1.1627e+01, -1.2799e+01, -8.0016e+00, -1.2712e+01, -2.6730e+00,\n",
       "          1.4944e-01,  4.1157e+00,  2.9334e+00, -4.6105e-01, -3.7645e-01,\n",
       "          2.4234e+00, -2.4231e-01,  1.9914e+00, -4.2823e-01,  4.5774e+00,\n",
       "         -2.0381e+00,  8.0717e-04,  3.7897e+00,  2.1376e+00,  1.2677e+00,\n",
       "          5.6208e-01,  5.3169e+00,  2.8548e+00, -1.2709e+01, -1.3019e-02,\n",
       "          2.0820e+00, -1.2704e+01, -5.1777e-01, -3.9188e+00, -1.2682e+01,\n",
       "         -1.2723e+01, -5.5099e+00, -1.2664e+01, -1.2626e+01, -1.2692e+01,\n",
       "         -1.2715e+01, -1.2719e+01, -5.1495e+00, -1.2636e+01, -1.2715e+01,\n",
       "         -1.2696e+01, -1.2669e+01, -1.2717e+01, -1.2687e+01, -1.2687e+01,\n",
       "         -2.3778e+00,  1.3126e+01, -1.2674e+01, -1.2721e+01, -1.2773e+01,\n",
       "         -1.2729e+01, -1.2723e+01, -1.2683e+01, -1.2615e+01, -1.2660e+01,\n",
       "         -1.2657e+01, -1.2647e+01, -1.0827e+00, -1.2692e+01, -1.2722e+01,\n",
       "         -1.2708e+01, -1.2722e+01, -1.2799e+01, -1.2665e+01, -1.2673e+01,\n",
       "         -1.2692e+01, -1.2710e+01, -1.2660e+01, -1.2735e+01, -1.2736e+01,\n",
       "         -1.2635e+01, -1.2713e+01, -1.2631e+01, -1.2619e+01, -1.2689e+01,\n",
       "         -1.2717e+01, -1.2739e+01, -1.2699e+01, -1.2722e+01, -1.2031e+01,\n",
       "         -1.2639e+01],\n",
       "        [-1.2027e+01, -1.2743e+01, -2.7108e+00, -1.2900e+01,  2.1201e+00,\n",
       "          7.8642e+00,  2.2893e-01,  3.8414e+00,  4.2161e+00,  1.0947e-01,\n",
       "          6.8731e+00, -8.1462e-01,  1.1705e+00, -1.6191e+00, -3.6515e+00,\n",
       "         -4.0879e+00,  1.3789e+01, -4.1417e+00, -3.8301e+00, -3.5975e+00,\n",
       "         -4.1099e+00, -3.9104e+00,  2.7459e+00, -1.3010e+01, -3.6717e+00,\n",
       "         -5.0438e+00, -1.2909e+01, -2.2472e+00, -6.0670e+00, -1.2901e+01,\n",
       "         -1.2984e+01,  2.7568e+00, -1.2809e+01, -1.2877e+01, -1.2889e+01,\n",
       "         -1.2881e+01, -1.2984e+01, -4.1106e+00, -1.2971e+01, -1.2929e+01,\n",
       "         -1.2969e+01, -1.2914e+01, -1.2959e+01, -1.2892e+01, -1.2964e+01,\n",
       "         -6.6020e+00, -1.5731e-01, -1.2941e+01, -1.2904e+01, -1.2929e+01,\n",
       "         -1.2882e+01, -1.2922e+01, -1.2992e+01, -1.2769e+01, -1.2937e+01,\n",
       "         -1.2851e+01, -1.2887e+01, -2.9979e+00, -1.2990e+01, -1.2956e+01,\n",
       "         -1.2922e+01, -1.2869e+01, -1.3010e+01, -1.2913e+01, -1.2968e+01,\n",
       "         -1.2911e+01, -1.2929e+01, -1.2871e+01, -1.2905e+01, -1.2999e+01,\n",
       "         -1.2951e+01, -1.2986e+01, -1.2890e+01, -1.2871e+01, -1.2864e+01,\n",
       "         -1.2835e+01, -1.2918e+01, -1.2959e+01, -1.2958e+01, -1.2740e+01,\n",
       "         -1.2845e+01],\n",
       "        [-1.8595e+01, -1.5520e+01, -1.2537e+00, -1.5532e+01,  2.9053e+00,\n",
       "         -3.8047e+00,  5.3872e+00,  6.7947e-01, -2.4162e+00,  2.1910e+00,\n",
       "          1.4852e+00, -1.7036e+00,  1.8714e+01,  1.6990e+00,  3.1962e+00,\n",
       "          1.8808e+00, -1.6671e+00, -1.9222e+00,  7.7780e-01, -1.3891e+00,\n",
       "         -5.1268e+00,  9.8080e-01, -2.8854e+00, -1.5707e+01,  1.9282e-02,\n",
       "          1.5668e-01, -1.5553e+01, -1.4532e+00, -4.8829e+00, -1.5710e+01,\n",
       "         -1.5660e+01,  6.1362e-01, -1.5660e+01, -1.5689e+01, -1.5613e+01,\n",
       "         -1.5637e+01, -1.5718e+01, -1.4889e+00, -1.5593e+01, -1.5605e+01,\n",
       "         -1.5612e+01, -1.5696e+01, -1.5674e+01, -1.5620e+01, -1.5673e+01,\n",
       "         -4.9521e+00, -4.2685e+00, -1.5845e+01, -1.5708e+01, -1.5597e+01,\n",
       "         -1.5682e+01, -1.5695e+01, -1.5622e+01, -1.5515e+01, -1.5800e+01,\n",
       "         -1.5591e+01, -1.5645e+01,  1.2637e-01, -1.5715e+01, -1.5595e+01,\n",
       "         -1.5694e+01, -1.5657e+01, -1.5636e+01, -1.5613e+01, -1.5733e+01,\n",
       "         -1.5695e+01, -1.5622e+01, -1.5722e+01, -1.5575e+01, -1.5668e+01,\n",
       "         -1.5697e+01, -1.5739e+01, -1.5733e+01, -1.5769e+01, -1.5603e+01,\n",
       "         -1.5610e+01, -1.5524e+01, -1.5721e+01, -1.5710e+01, -1.5128e+01,\n",
       "         -1.5508e+01],\n",
       "        [-1.1863e+01, -1.4547e+01,  2.2033e+00, -1.4225e+01,  9.2466e+00,\n",
       "         -1.5381e+00,  1.8949e+01, -1.9246e+00, -1.9959e+00, -1.0645e+00,\n",
       "         -1.5536e+00, -5.2282e-01,  5.3303e+00, -4.8001e+00, -1.2240e+00,\n",
       "         -2.8890e+00, -1.8417e+00, -1.6757e-01, -3.9733e+00, -3.3923e-01,\n",
       "         -3.7919e+00,  5.9325e-01, -1.7867e+00, -1.4290e+01,  2.6420e+00,\n",
       "         -2.8597e+00, -1.4238e+01, -2.4263e+00,  4.5340e-02, -1.4275e+01,\n",
       "         -1.4268e+01,  3.6714e+00, -1.4260e+01, -1.4363e+01, -1.4257e+01,\n",
       "         -1.4302e+01, -1.4241e+01, -3.6297e+00, -1.4286e+01, -1.4217e+01,\n",
       "         -1.4212e+01, -1.4250e+01, -1.4274e+01, -1.4315e+01, -1.4312e+01,\n",
       "          3.6341e-01, -6.0877e+00, -1.4245e+01, -1.4421e+01, -1.4286e+01,\n",
       "         -1.4330e+01, -1.4328e+01, -1.4275e+01, -1.4162e+01, -1.4297e+01,\n",
       "         -1.4322e+01, -1.4229e+01, -8.2361e-01, -1.4335e+01, -1.4328e+01,\n",
       "         -1.4316e+01, -1.4197e+01, -1.4256e+01, -1.4219e+01, -1.4303e+01,\n",
       "         -1.4277e+01, -1.4276e+01, -1.4405e+01, -1.4259e+01, -1.4276e+01,\n",
       "         -1.4203e+01, -1.4461e+01, -1.4276e+01, -1.4371e+01, -1.4253e+01,\n",
       "         -1.4313e+01, -1.4214e+01, -1.4323e+01, -1.4276e+01, -1.3283e+01,\n",
       "         -1.4253e+01],\n",
       "        [-2.1113e+01, -1.6734e+01,  5.7924e-01, -1.6765e+01,  7.7072e+00,\n",
       "         -1.5670e-01,  6.1144e+00,  9.5721e-01, -8.1220e+00,  6.7769e-01,\n",
       "         -3.9384e-01,  9.6266e-01,  1.0624e+01,  1.4706e+00,  4.8160e-01,\n",
       "          4.1931e+00, -7.0178e+00,  4.5149e-01, -4.7414e-01, -2.0390e-01,\n",
       "         -7.3246e+00,  2.5347e+00, -8.1403e+00, -1.6765e+01,  3.1180e+00,\n",
       "         -2.8665e+00, -1.6682e+01, -1.9554e+00, -1.2680e+00, -1.6870e+01,\n",
       "         -1.6891e+01,  1.3927e+00, -1.6831e+01, -1.6869e+01, -1.6648e+01,\n",
       "         -1.6839e+01, -1.6798e+01, -2.6841e+00, -1.6656e+01, -1.6778e+01,\n",
       "         -1.6682e+01, -1.6842e+01, -1.6842e+01, -1.6688e+01, -1.6873e+01,\n",
       "         -6.0640e+00, -4.6538e+00, -1.6940e+01, -1.6871e+01, -1.6724e+01,\n",
       "         -1.6940e+01, -1.6846e+01, -1.6825e+01, -1.6674e+01, -1.7015e+01,\n",
       "         -1.6862e+01, -1.6814e+01, -3.6806e+00, -1.6807e+01, -1.6768e+01,\n",
       "         -1.6820e+01, -1.6859e+01, -1.6670e+01, -1.6824e+01, -1.6886e+01,\n",
       "         -1.6886e+01, -1.6834e+01, -1.6963e+01, -1.6751e+01, -1.6790e+01,\n",
       "         -1.6835e+01, -1.6818e+01, -1.6977e+01, -1.6829e+01, -1.6774e+01,\n",
       "         -1.6769e+01, -1.6748e+01, -1.6865e+01, -1.6948e+01, -1.5303e+01,\n",
       "         -1.6689e+01]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lprobs_hf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6eeff463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(lprobs - lprobs_hf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24926eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32e98799",
   "metadata": {},
   "source": [
    "## Use the `transformers` generator loop:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ac3746",
   "metadata": {},
   "source": [
    "Run the full model to make sure this doesn't give any errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b4b889ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.Seq2SeqLMOutput"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model(\n",
    "         input_values=inputs.input_values,\n",
    "         attention_mask=inputs.attention_mask,\n",
    "         #decoder_input_ids=torch.tensor([[3, 4, 5]]),\n",
    "         decoder_input_ids=torch.tensor([[3, 4, 5], [2, 2, 2]]),  # batch\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "677718c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logits', 'past_key_values', 'encoder_last_hidden_state']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7f2bcec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 81])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d1eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e372583d",
   "metadata": {},
   "source": [
    "Also calculate loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ac885958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.Seq2SeqLMOutput"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model(\n",
    "         input_values=inputs.input_values,\n",
    "         attention_mask=inputs.attention_mask,\n",
    "         #decoder_input_ids=torch.tensor([[2,  4, 18, 10, 12,  6,  5]]),\n",
    "         #labels=torch.tensor([[4, 18, 10, 12,  6,  5, 13]]),\n",
    "         labels=torch.tensor([[4, 18, 10, 12,  6,  5, 13], [4, 18, 10, 12,  6,  5, 13]]),  # batch\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "95c7bef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7, 81])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "70ea70f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4196)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390f1998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e21df7f",
   "metadata": {},
   "source": [
    "Generator loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "804366f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128632])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b2fc2baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs = hf_model.generate(inputs.input_values, max_length=100)\n",
    "# hf_outputs = hf_model.generate(inputs.input_values, num_beams=5, max_length=100) #, bos_token_id=2)\n",
    "# hf_outputs = hf_model.generate(torch.rand(1, 10000), num_beams=5, max_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "015c80b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ce7e1899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  4, 46, 16, 12,  6,  4,  6, 11, 13, 16, 12,  6,  4,  7,  9, 14,  4,\n",
       "         24,  7, 13, 13, 22,  4,  7,  9, 14,  4, 27, 10, 17,  6,  8, 13, 22,  4,\n",
       "          6,  8,  4,  6, 11,  5,  4, 12,  6, 13,  8,  9, 21,  5, 13,  4, 46, 16,\n",
       "         12,  6,  4,  6, 11, 13, 16, 12,  6,  4,  7,  9, 14,  4, 24,  7, 13, 13,\n",
       "         22,  4,  7,  9, 14,  4, 27, 10, 17,  6,  8, 13, 22,  4,  6,  8,  4,  6,\n",
       "         11,  5,  4, 12,  6, 13,  8,  9, 21,  5],\n",
       "        [ 2,  4, 12,  5, 15, 19,  4, 14,  5, 12,  6, 13, 16, 17,  6, 10,  8,  9,\n",
       "          4, 17,  8, 18, 18,  5,  9, 17,  5,  4, 10,  9,  4, 19, 10, 27,  5,  4,\n",
       "         19,  8, 16, 13,  4,  6, 11, 13,  5,  5,  4,  6, 20,  8,  4,  8,  9,  5,\n",
       "          4,  7,  9, 14,  4,  7,  4, 11,  7, 15, 19,  4,  8,  9,  5,  4,  7,  9,\n",
       "         14,  4,  7,  4, 45, 16,  7, 13,  6,  5, 13,  4,  8,  9,  5,  4,  7,  9,\n",
       "         14,  4,  7,  4, 15, 10,  6,  6, 15,  5]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "11fce9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " j u s t  t h r u s t  a n d  p a r r y  a n d  v i c t o r y  t o  t h e  s t r o n g e r  j u s t  t h r u s t  a n d  p a r r y  a n d  v i c t o r y  t o  t h e  s t r o n g e\n",
      "just thrust and parry and victory to the stronger just thrust and parry and victory to the stronge\n",
      "\n",
      " s e l f  d e s t r u c t i o n  c o m m e n c e  i n  f i v e  f o u r  t h r e e  t w o  o n e  a n d  a  h a l f  o n e  a n d  a  q u a r t e r  o n e  a n d  a  l i t t l e\n",
      "self destruction commence in five four three two one and a half one and a quarter one and a little\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(hf_outputs.shape[0]):\n",
    "    print(tgt_dict.string(hf_outputs[i]))\n",
    "    print(tokenizer.decode(tgt_dict.string(hf_outputs[i])))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "997fd48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '<pad>', '</s>', '<unk>', '']"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tgt_dict[x] for x in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf8f058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b716efd",
   "metadata": {},
   "source": [
    "For comparison, Speech2Text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa11a599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n",
    "\n",
    "s2t_model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "s2t_processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "1c5b35cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthijs/anaconda3/envs/t5/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'dataset_info': token. Will not be supported from version '0.12'.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Reusing dataset librispeech_asr_dummy (/Users/matthijs/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 584, 80])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "inputs = s2t_processor(\n",
    "    ds[0][\"audio\"][\"array\"], sampling_rate=ds[0][\"audio\"][\"sampling_rate\"], return_tensors=\"pt\"\n",
    ")\n",
    "input_features = inputs.input_features\n",
    "input_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "c6adcf4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 584, 80])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test batch\n",
    "# input_features = torch.tile(input_features, dims=(2, 1, 1))\n",
    "# input_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "daf06a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/transformers/src/transformers/models/speech_to_text/modeling_speech_to_text.py:561: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  input_lengths = (input_lengths - 1) // 2 + 1\n",
      "/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/transformers/src/transformers/generation_utils.py:1296: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 200 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'mister quilter is the apostle of the middle classes and we are glad to welcome his gospel'"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids = s2t_model.generate(inputs=input_features)\n",
    "\n",
    "transcription = s2t_processor.batch_decode(generated_ids)[0]\n",
    "transcription\n",
    "\n",
    "#'mister quilter is the apostle of the middle classes and we are glad to welcome his gospel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "9a958526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/s2t-small-librispeech-asr were not used when initializing Speech2TextModel: ['lm_head.weight']\n",
      "- This IS expected if you are initializing Speech2TextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Speech2TextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Speech2TextModel were not initialized from the model checkpoint at facebook/s2t-small-librispeech-asr and are newly initialized: ['model.decoder.embed_positions.weights', 'model.encoder.embed_positions.weights']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** has_prefix_module True expects_prefix_module False\n",
      "*** remove_prefix_from_model False add_prefix_to_model True\n"
     ]
    }
   ],
   "source": [
    "from transformers import Speech2TextModel\n",
    "\n",
    "s2t_model = Speech2TextModel.from_pretrained(\"facebook/s2t-small-librispeech-asr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49667e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d50884f2",
   "metadata": {},
   "source": [
    "Test other methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1cd38a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BART, Speech2Text, Wav2Vec2 don't have pruning\n",
    "#hf_model.prune_heads({1: [0, 2], 2: [2,3 ]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc11e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "717f3625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(100, 768)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca84dc88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=100, bias=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.get_output_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6f954bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(100, 768)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.resize_token_embeddings(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e7528a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(100, 768)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecff9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "517c322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4355fa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_ctc.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8a0f3c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_naked.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712666c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
