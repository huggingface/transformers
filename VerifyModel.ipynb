{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca8cd0a9",
   "metadata": {},
   "source": [
    "# Verifying the SpeechT5 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5342da",
   "metadata": {},
   "source": [
    "I needed to do the following to be able to load the original model:\n",
    "\n",
    "- Clone the https://github.com/microsoft/SpeechT5 repo\n",
    "\n",
    "Install stuff:\n",
    "\n",
    "```\n",
    "pip install editdistance\n",
    "pip install -U sacrebleu==1.5.1\n",
    "\n",
    "git submodule update --init SpeechT5/fairseq\n",
    "cd SpeechT5\n",
    "pip install --editable fairseq/\n",
    "pip install espnet\n",
    "```\n",
    "\n",
    "Put this notebook at the same level as the `SpeechT5` repo.\n",
    "\n",
    "Hack the code:\n",
    "\n",
    "- Copy `speecht5/tasks/speecht5.py` into `fairseq/fairseq/tasks`\n",
    "\n",
    "- To run on CPU: In `speecht5/sequence_generator.py`, comment out where it does `.to(device=\"cuda\")`\n",
    "\n",
    "Additional stuff to download:\n",
    "\n",
    "- `dict.txt` from https://drive.google.com/uc?export=download&id=19hcQ58RHZ6CssxF8Qp6yEF1NW_AXxObK\n",
    "\n",
    "- `tokenizer` from https://drive.google.com/uc?export=download&id=1wClgQjXXoU2lmpbaEa1v2SqMbg7cAutq\n",
    "\n",
    "- `speecht5_base_asr.pt` and `t5_transformer_lm.pt` from https://huggingface.co/ajyy/SpeechT5\n",
    "\n",
    "You also need an input audio file, any WAV at 16 kHz will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a4d7c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65661728",
   "metadata": {},
   "source": [
    "Set Python path so it can find the `speecht5` and `fairseq` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f30402f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../SpeechT5/SpeechT5\")\n",
    "sys.path.insert(0, \"../SpeechT5/SpeechT5/fairseq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d398e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b45926",
   "metadata": {},
   "source": [
    "## Load audio and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "c18f317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/AUDIO_DIR/dev_clean/1272/141231/1272-141231-0020.flac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "a0350910",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/AUDIO_DIR/dev_clean/1272/128104/1272-128104-0000.flac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "4a855d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((69120,), 16000)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "wav_data, cur_sample_rate = sf.read(input_file)\n",
    "wav_data.shape, cur_sample_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e60de",
   "metadata": {},
   "source": [
    "NOTE: The `Wav2Vec2FeatureExtractor` does not make sure the audio file is mono. If it has shape `(2, length)` or even `(1, length)` then the output from the feature extractor is incorrect!\n",
    "\n",
    "The `do_normalize` option is False for the SpeechT5 ASR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "e9699780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(do_normalize=False, return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "5ea3da78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 69120])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = feature_extractor(wav_data, sampling_rate=cur_sample_rate, padding=True, return_tensors=\"pt\")\n",
    "inputs[\"input_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "d1a94286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing padding mask\n",
    "inputs[\"attention_mask\"][:, 40000:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "62e522e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 2))\n",
    "plt.plot(inputs[\"input_values\"][0])\n",
    "plt.plot(inputs[\"attention_mask\"][0])\n",
    "plt.ylim(-1, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f594f3",
   "metadata": {},
   "source": [
    "Transform multiple inputs into a single padded batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbb99431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128632,), 16000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file2 = \"/Users/matthijs/Documents/FILES/HuggingFace/S2S/textless/AUDIO_DIR/selfdestruct.wav\"\n",
    "wav_data2, cur_sample_rate2 = sf.read(input_file2)\n",
    "wav_data2.shape, cur_sample_rate2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1daee2cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128632])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs2 = feature_extractor([wav_data, wav_data2], sampling_rate=cur_sample_rate, padding=True, return_tensors=\"pt\")\n",
    "inputs2[\"input_values\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb653d87",
   "metadata": {},
   "source": [
    "The original model used a `padding_mask` as input, where False means no padding. The `Wav2Vec2FeatureExtractor` can return an `attention_mask`, where 1 means no padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d3ab7e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs2[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7163b1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = inputs2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2205900b",
   "metadata": {},
   "source": [
    "## Load the Transformers model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc8c0f8",
   "metadata": {},
   "source": [
    "To convert the original checkpoint weights to Transformers:\n",
    "\n",
    "First download the checkpoint. I used `speecht5_base_asr.pt` from https://huggingface.co/ajyy/SpeechT5\n",
    "\n",
    "Then run the following, using your own `--checkpoint_path` and `--pytorch_dump_folder_path`:\n",
    "\n",
    "```nohighlight\n",
    "cd transformers/src/transformers/models/speecht5\n",
    "\n",
    "python convert_speecht5_original_pytorch_checkpoint_to_pytorch.py \\\n",
    "  --task s2t \\\n",
    "  --checkpoint_path /path/to/SpeechT5/speecht5_base_asr.pt \n",
    "  --pytorch_dump_folder_path /some/other/path\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "552fd307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    SpeechT5Config, \n",
    "    SpeechT5CTCTokenizer,\n",
    "    SpeechT5Processor,\n",
    "    SpeechT5Model, \n",
    "    SpeechT5ForConditionalGeneration, \n",
    "    SpeechT5ForCTC, \n",
    "    Wav2Vec2FeatureExtractor,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "69aa6aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SpeechT5Config()\n",
    "hf_model = SpeechT5Model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "1ba8f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_base_asr\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2884601a",
   "metadata": {},
   "source": [
    "Note that loading should work OK for both the base class `SpeechT5Model` and `SpeechT5ForCTC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "09c1c4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hf_model = SpeechT5Model.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "5df4bac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model = SpeechT5ForConditionalGeneration.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "b7dd7606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeechT5ForConditionalGeneration(\n",
       "  (speecht5): SpeechT5Model(\n",
       "    (encoder): SpeechT5SpeechEncoder(\n",
       "      (prenet): SpeechT5SpeechEncoderPrenet(\n",
       "        (feature_encoder): SpeechT5FeatureEncoder(\n",
       "          (conv_layers): ModuleList(\n",
       "            (0): SpeechT5GroupNormConvLayer(\n",
       "              (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "              (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "            )\n",
       "            (1): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (2): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (3): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (4): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (5): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (6): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (feature_projection): SpeechT5FeatureProjection(\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (pos_conv_embed): SpeechT5PositionalConvEmbedding(\n",
       "          (conv): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "          (padding): SpeechT5SamePadLayer()\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (embed_positions): SpeechT5SinusoidalPositionalEmbedding()\n",
       "      )\n",
       "      (wrapped_encoder): SpeechT5Encoder(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (embed_positions): SpeechT5RelativePositionalEncoding(\n",
       "          (pe_k): Embedding(320, 64)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): SpeechT5TextDecoder(\n",
       "      (prenet): SpeechT5TextDecoderPrenet(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (embed_tokens): Embedding(81, 768, padding_idx=1)\n",
       "        (embed_positions): SpeechT5SinusoidalPositionalEmbedding()\n",
       "      )\n",
       "      (wrapped_decoder): SpeechT5Decoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (text_decoder_postnet): SpeechT5TextDecoderPostnet(\n",
       "    (lm_head): Linear(in_features=768, out_features=81, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "7628cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the attention layer weights are correct\n",
    "# for i in range(len(hf_model.speecht5.encoder.layers)):\n",
    "#     print(i, \"k_proj weight\", torch.all(hf_model.speecht5.encoder.layers[i].attention.k_proj.weight == orig_model.encoder.layers[i].self_attn.k_proj.weight))\n",
    "#     print(i, \"k_proj bias\", torch.all(hf_model.speecht5.encoder.layers[i].attention.k_proj.bias == orig_model.encoder.layers[i].self_attn.k_proj.bias))\n",
    "#     print(i, \"v_proj weight\", torch.all(hf_model.speecht5.encoder.layers[i].attention.v_proj.weight == orig_model.encoder.layers[i].self_attn.v_proj.weight))\n",
    "#     print(i, \"v_proj bias\", torch.all(hf_model.speecht5.encoder.layers[i].attention.v_proj.bias == orig_model.encoder.layers[i].self_attn.v_proj.bias))\n",
    "#     print(i, \"q_proj weight\", torch.all(hf_model.speecht5.encoder.layers[i].attention.q_proj.weight == orig_model.encoder.layers[i].self_attn.q_proj.weight))\n",
    "#     print(i, \"q_proj bias\", torch.all(hf_model.speecht5.encoder.layers[i].attention.q_proj.bias == orig_model.encoder.layers[i].self_attn.q_proj.bias))\n",
    "#     print(i, \"out_proj weight\", torch.all(hf_model.speecht5.encoder.layers[i].attention.out_proj.weight == orig_model.encoder.layers[i].self_attn.out_proj.weight))\n",
    "#     print(i, \"out_proj bias\", torch.all(hf_model.speecht5.encoder.layers[i].attention.out_proj.bias == orig_model.encoder.layers[i].self_attn.out_proj.bias))\n",
    "#     print(\"---\")\n",
    "\n",
    "# print(\"pos_emb weight\", torch.all(hf_model.speecht5.encoder.pos_emb.pe_k.weight == orig_model.encoder.pos_emb.pe_k.weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82233c52",
   "metadata": {},
   "source": [
    "Run a single forward pass. This should run the encoder, decoder, and the relevant pre- and postnets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "29bd2f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.encoder.prenet(**inputs)\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "7e1f8501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using no attention_mask\n",
    "# with torch.no_grad():\n",
    "#      hf_outputs = hf_model.speech_encoder_prenet(input_values=inputs[\"input_values\"])\n",
    "\n",
    "# type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "359778a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "97a77055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_outputs[\"extract_features\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "7db34f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_outputs[\"extract_features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "41afd19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_outputs[\"hidden_states\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "fcef1339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_outputs[\"hidden_states\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "d01d920d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 215, 768]), torch.Size([1, 215])]"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in hf_outputs if hasattr(x, \"shape\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "2d5ff106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.4147, -3.6930, 10.6119,  ...,  1.3639,  2.2972,  0.8101],\n",
       "         [ 8.1118, -4.2742, 12.2622,  ...,  0.1729,  1.6734,  0.6476],\n",
       "         [-2.8165,  2.4560, -1.3008,  ...,  0.9983, -0.4333,  2.5262],\n",
       "         ...,\n",
       "         [ 8.1539, -5.4359,  5.8689,  ...,  1.0349,  4.1734,  1.4926],\n",
       "         [ 0.8112, -3.6817,  5.7369,  ..., -0.6757,  2.3840,  1.2404],\n",
       "         [ 2.7066, -3.5683,  6.5834,  ...,  0.4324,  2.7151,  1.5720]]])"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "d24920f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_encoder_input = hf_outputs[0]\n",
    "hf_encoder_attention_mask = hf_outputs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64823660",
   "metadata": {},
   "source": [
    "## Load the original model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1697ab0",
   "metadata": {},
   "source": [
    "Load the dictionary. This adds `<s>, <pad>, </s>, <unk>` tokens to the front and `<mask>` and `<ctc_blank>` to the end. **dict.txt** was [downloaded from here](https://drive.google.com/uc?export=download&id=19hcQ58RHZ6CssxF8Qp6yEF1NW_AXxObK). This is the Vocabulary link from the main SpeechT5 README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "24e0f251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size: 81\n"
     ]
    }
   ],
   "source": [
    "from fairseq.data import Dictionary\n",
    "tgt_dict = Dictionary.load(\"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/DATA_ROOT/dict.txt\")\n",
    "tgt_dict.add_symbol(\"<mask>\")\n",
    "tgt_dict.add_symbol(\"<ctc_blank>\")\n",
    "print(f\"dictionary size: \" f\"{len(tgt_dict):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85935250",
   "metadata": {},
   "source": [
    "To load the model we need the `SpeechT5Task` object but constructing it is annoying. Fortunately, `build_model` only reads two properties from the task object, so we can fake it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "ea4b4380",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeTask:\n",
    "    def __init__(self):\n",
    "        self.dicts = { \"text\": tgt_dict }\n",
    "        self.t5_task = \"s2t\"\n",
    "        \n",
    "task = FakeTask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dc6c7a",
   "metadata": {},
   "source": [
    "Load the fine-tuned ASR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "a79fbfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from speecht5.models.speecht5 import T5TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "b9b2818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"../weights/SpeechT5/speecht5_base_asr.pt\")\n",
    "\n",
    "orig_model = T5TransformerModel.build_model(checkpoint[\"cfg\"][\"model\"], task)\n",
    "\n",
    "orig_model.load_state_dict(checkpoint[\"model\"])\n",
    "orig_model = orig_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "bda7a201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speecht5.models.speecht5.T5TransformerModel"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(orig_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "64d53b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speecht5.models.modules.encoder.TransformerEncoder"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(orig_model.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "bad44136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(checkpoint[\"model\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "4c1faddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fairseq.data.encoders.sentencepiece_bpe.SentencepieceBPE"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fairseq.data import encoders\n",
    "from argparse import Namespace\n",
    "tokenizer = encoders.build_bpe(\n",
    "    Namespace(\n",
    "        bpe='sentencepiece', \n",
    "        sentencepiece_model='/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/MODEL_DIR/spm_char.model'\n",
    "    )\n",
    ")\n",
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "61dd5619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orig_model.decoder.layers[0].encoder_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56503f04",
   "metadata": {},
   "source": [
    "## Verify speech encoder prenet output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e2f9ff",
   "metadata": {},
   "source": [
    "This first uses the `speech_encoder_prenet` to convert the raw audio data into embeddings of shape `(batch, sequence_length, 768)`. The sequence length is roughly `number of audio samples / 320`, so there is one vector every 20 ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "01638115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 69120])"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = inputs[\"input_values\"]\n",
    "source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "35b0a089",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_mask = torch.BoolTensor(source.shape).fill_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "94586252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False, False, False]])"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask = torch.BoolTensor((1 - inputs[\"attention_mask\"]).bool())\n",
    "padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "374cf769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This doesn't work on the original model\n",
    "#padding_mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "31d1d2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input, encoder_padding_mask = orig_model.speech_encoder_prenet(\n",
    "    source, padding_mask=padding_mask, mask=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "36334be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_input = orig_model.speech_encoder_prenet.feature_extractor(source)\n",
    "# encoder_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "26c4a65d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 215, 768]), torch.Size([1, 215]))"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input.shape, encoder_padding_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "058de538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.4147, -3.6930, 10.6119,  ...,  1.3639,  2.2972,  0.8101],\n",
       "         [ 8.1118, -4.2742, 12.2622,  ...,  0.1729,  1.6734,  0.6476],\n",
       "         [-2.8165,  2.4560, -1.3008,  ...,  0.9983, -0.4333,  2.5262],\n",
       "         ...,\n",
       "         [ 8.1539, -5.4359,  5.8689,  ...,  1.0349,  4.1734,  1.4926],\n",
       "         [ 0.8112, -3.6817,  5.7369,  ..., -0.6757,  2.3840,  1.2404],\n",
       "         [ 2.7066, -3.5683,  6.5834,  ...,  0.4324,  2.7151,  1.5720]]])"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "3c07884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dbc6dd",
   "metadata": {},
   "source": [
    "If the weights and model were converted correctly, this should report zero or a very small number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "4b099ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.max(torch.abs(encoder_input - hf_outputs[\"hidden_states\"]))\n",
    "torch.max(torch.abs(encoder_input - hf_outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "5533cb26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f94e911a040>"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.imshow(torch.abs(encoder_input - hf_outputs[\"hidden_states\"]).numpy()[0] > 1e-5)\n",
    "plt.imshow(torch.abs(encoder_input - hf_outputs[0]).numpy()[0] > 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bf93df",
   "metadata": {},
   "source": [
    "The line that is different is where the padding mask goes from 1 to 0; the original model handles this a little different than we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "b92b758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.max(torch.abs(encoder_input - hf_outputs[\"extract_features\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "d18286fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.abs(encoder_input - hf_outputs[\"last_hidden_state\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b76dc23",
   "metadata": {},
   "source": [
    "## Verify Transformer encoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "47c86f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run it with the original's speech prenet input:\n",
    "# with torch.no_grad():\n",
    "#     encoder_output = orig_model.encoder(encoder_input, encoder_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "8b8b4df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run it with our input, which is slightly different (see above)\n",
    "with torch.no_grad():\n",
    "    encoder_output = orig_model.encoder(hf_encoder_input, ~hf_encoder_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "41ff76cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_out shape torch.Size([215, 1, 768])\n",
      "encoder_padding_mask shape torch.Size([1, 215])\n",
      "encoder_states []\n",
      "src_tokens []\n",
      "decoder_input [None]\n",
      "encoder_out_for_ctc shape torch.Size([215, 1, 81])\n"
     ]
    }
   ],
   "source": [
    "print(\"encoder_out shape\", encoder_output[\"encoder_out\"][0].shape)\n",
    "print(\"encoder_padding_mask shape\", encoder_output[\"encoder_padding_mask\"][0].shape)\n",
    "print(\"encoder_states\", encoder_output[\"encoder_states\"])  # []\n",
    "print(\"src_tokens\", encoder_output[\"src_tokens\"])  # []\n",
    "print(\"decoder_input\", encoder_output[\"decoder_input\"])  # [None]\n",
    "print(\"encoder_out_for_ctc shape\", encoder_output[\"encoder_out_for_ctc\"][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "a38979d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3412, -0.1983, -0.5504,  ...,  0.3371, -0.7611,  0.2468],\n",
       "         [-0.3532, -0.1819, -0.6027,  ...,  0.2886, -0.8502,  0.1505],\n",
       "         [-0.4093, -0.1701, -0.6913,  ...,  0.2886, -0.7586, -0.0191],\n",
       "         ...,\n",
       "         [ 0.2191,  0.1308, -0.2119,  ...,  0.2059, -0.7364,  0.2486],\n",
       "         [ 0.0395,  0.0048, -0.3112,  ...,  0.1692, -0.7535,  0.2188],\n",
       "         [-0.0338, -0.1440, -0.3480,  ...,  0.2844, -0.7159,  0.1947]]])"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[\"encoder_out\"][0].permute((1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "f9b9dda4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -9.4632,  -9.6230,  -9.6073,  ...,  -9.5406,  -9.3881,  12.4264],\n",
       "         [ -9.8169,  -9.9027,  -9.9673,  ...,  -9.8993,  -9.7949,  13.0776],\n",
       "         [-10.9712, -11.0219, -11.0252,  ..., -11.0365, -10.9394,  14.6879],\n",
       "         ...,\n",
       "         [ -8.2366,  -8.3445,  -8.3155,  ...,  -8.4186,  -8.0964,  10.4814],\n",
       "         [ -8.7586,  -8.9771,  -8.8052,  ...,  -8.8178,  -8.5105,  11.1125],\n",
       "         [ -9.6384, -10.0383,  -9.7550,  ...,  -9.7551,  -9.3112,  11.0622]]])"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[\"encoder_out_for_ctc\"][0].permute((1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "cde6dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use exact same inputs as the original model:\n",
    "# with torch.no_grad():\n",
    "#      hf_outputs = hf_model.speecht5(\n",
    "#          inputs_embeds=encoder_input,\n",
    "#          attention_mask=(~encoder_padding_mask),\n",
    "#      )\n",
    "\n",
    "# type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "28f529a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutput"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.encoder.wrapped_encoder(\n",
    "         hidden_states=hf_encoder_input,\n",
    "         attention_mask=hf_encoder_attention_mask,\n",
    "#          input_values=inputs.input_values,\n",
    "#          attention_mask=inputs.attention_mask,\n",
    "         return_dict=True,\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "fb12e71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[-0.3412, -0.1983, -0.5504,  ...,  0.3371, -0.7611,  0.2468],\n",
       "         [-0.3532, -0.1819, -0.6027,  ...,  0.2886, -0.8502,  0.1505],\n",
       "         [-0.4093, -0.1701, -0.6913,  ...,  0.2886, -0.7586, -0.0191],\n",
       "         ...,\n",
       "         [ 0.2191,  0.1308, -0.2119,  ...,  0.2059, -0.7364,  0.2486],\n",
       "         [ 0.0395,  0.0048, -0.3112,  ...,  0.1692, -0.7535,  0.2188],\n",
       "         [-0.0338, -0.1440, -0.3480,  ...,  0.2844, -0.7159,  0.1947]]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "06fb675b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last_hidden_state']"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "8fe9914e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 215, 768])"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "69eda6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3412, -0.1983, -0.5504,  ...,  0.3371, -0.7611,  0.2468],\n",
       "         [-0.3532, -0.1819, -0.6027,  ...,  0.2886, -0.8502,  0.1505],\n",
       "         [-0.4093, -0.1701, -0.6913,  ...,  0.2886, -0.7586, -0.0191],\n",
       "         ...,\n",
       "         [ 0.2191,  0.1308, -0.2119,  ...,  0.2059, -0.7364,  0.2486],\n",
       "         [ 0.0395,  0.0048, -0.3112,  ...,  0.1692, -0.7535,  0.2188],\n",
       "         [-0.0338, -0.1440, -0.3480,  ...,  0.2844, -0.7159,  0.1947]]])"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "a747e547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(encoder_output[\"encoder_out\"][0].permute((1, 0, 2)) - hf_outputs[\"last_hidden_state\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7518e152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca490a88",
   "metadata": {},
   "source": [
    "## Verify CTC model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98055be9",
   "metadata": {},
   "source": [
    "This model only needs the encoder portion.\n",
    "\n",
    "This uses the same checkpoint as before: `speecht5_base_asr.pt` from https://huggingface.co/ajyy/SpeechT5\n",
    "\n",
    "Run the following to convert, using your own `--checkpoint_path` and `--pytorch_dump_folder_path`:\n",
    "\n",
    "```nohighlight\n",
    "cd transformers/src/transformers/models/speecht5\n",
    "\n",
    "python convert_speecht5_original_pytorch_checkpoint_to_pytorch.py \\\n",
    "  --task ctc \\\n",
    "  --checkpoint_path /path/to/SpeechT5/speecht5_base_asr.pt \n",
    "  --pytorch_dump_folder_path /some/other/path\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "69ca25a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_model_ctc = SpeechT5ForCTC(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "5a32978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_ctc = \"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_base_ctc\"\n",
    "hf_model_ctc = SpeechT5ForCTC.from_pretrained(model_checkpoint_ctc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "8d81d7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.CausalLMOutput"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the full model:\n",
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model_ctc(**inputs)\n",
    "\n",
    "# Run without attention_mask:\n",
    "# with torch.no_grad():\n",
    "#     hf_outputs = hf_model_ctc(input_values=inputs[\"input_values\"])\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "de48ff1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(encoder_output[\"encoder_out_for_ctc\"][0].permute((1, 0, 2)) - hf_outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "32fabb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.nn.functional.softmax(hf_outputs[0], dim=-1, dtype=torch.float32)\n",
    "probs = probs.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "267f80ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[80, 80, 80, 80, 80, 80,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "          4,  4,  4,  4, 46, 80, 80, 16, 80, 80, 12, 12,  6,  6, 80, 80, 80, 80,\n",
       "         80,  4,  4,  4,  6,  6, 11, 11, 13, 13, 13, 80, 16, 16, 80, 80, 80, 80,\n",
       "         12, 12,  6,  6, 80, 80, 80, 80, 80, 80, 80, 80,  4,  4,  4, 80, 80,  7,\n",
       "          9, 14, 14,  4,  4, 80, 24, 80, 80, 80, 80,  5, 13, 13, 80, 80, 13, 13,\n",
       "         80, 22, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80,  4,  4,  4,  4,  4,  4,  4, 80,  7,  7,  9, 14,\n",
       "         14, 80,  4,  4,  4, 27, 80, 80, 10, 80, 17, 17,  6,  6,  6,  8, 13, 13,\n",
       "         13, 22, 22, 80, 80,  4,  4,  6,  8,  8, 80,  4,  6,  6, 11,  5,  5,  4,\n",
       "         12, 12, 80,  6,  6, 13, 13, 80, 80,  8, 80,  9,  9, 80, 21, 21, 80, 80,\n",
       "          5, 13, 13, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80]])"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "90ca3c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tgt_dict.string(probs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "5bcf370d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                jusstt   tthhrrruusstt   andd  perrrry       aandd   vicctttorrryy  too tthee ssttrronnggerr\n"
     ]
    }
   ],
   "source": [
    "for i in range(probs.shape[0]):\n",
    "    print(tokenizer.decode(tgt_dict.string(probs[i])).replace(\"<ctc_blank>\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ade9d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8337214f",
   "metadata": {},
   "source": [
    "Calculate loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "9108bed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model_ctc(\n",
    "         **inputs, \n",
    "         labels=torch.tensor(\n",
    "           [ 46, 16, 12,  6,  4,  6, 11, 13, 16, 12,  6,  4,  7,  9, 14,  4,\n",
    "         24,  7, 13, 13, 22,  4,  7,  9, 14,  4, 27, 10, 17,  6,  8, 13, 22,  4,\n",
    "          6,  8,  4,  6, 11,  5,  4, 12,  6, 13,  8,  9, 21,  5, 13, ]\n",
    "         ),\n",
    "         output_hidden_states=True,\n",
    "         return_dict=True,\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "43f62849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1208.0262)"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22b8af9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b17bb840",
   "metadata": {},
   "source": [
    "## Verify text decoder prenet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9ba48d",
   "metadata": {},
   "source": [
    "First this calls `text_decoder_prenet`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "28dcb259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1])"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_size = 5\n",
    "tokens = torch.tensor([2] * beam_size).reshape(beam_size, -1)\n",
    "tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "e620930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "incremental_state = {}  # no incremental state on first step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "f09f744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    prev_output_tokens, tgt_mask, incremental_state = orig_model.text_decoder_prenet(tokens, incremental_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "725cfc62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 768])"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: misleading name; these are not the actual tokens but their embeddings!\n",
    "prev_output_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "84764c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "81026d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incremental_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "da00162a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.decoder.prenet(\n",
    "         input_ids=tokens,\n",
    "#          attention_mask=hf_encoder_attention_mask,\n",
    "#          attention_mask=inputs.attention_mask,\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "3dd961ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 768])"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "50efd1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(prev_output_tokens - hf_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c1a42c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fec354d",
   "metadata": {},
   "source": [
    "## Verify Transformer decoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "42a4fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = source.size(0)\n",
    "new_order = torch.arange(bsz).view(-1, 1).repeat(1, beam_size).view(-1)\n",
    "encoder_outs = orig_model.encoder.reorder_encoder_out(encoder_output, new_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "a98652fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.1074, 1.1042, 1.0387,  ..., 0.9619, 1.0463, 0.8678]],\n",
       "\n",
       "        [[1.1074, 1.1042, 1.0387,  ..., 0.9619, 1.0463, 0.8678]],\n",
       "\n",
       "        [[1.1074, 1.1042, 1.0387,  ..., 0.9619, 1.0463, 0.8678]],\n",
       "\n",
       "        [[1.1074, 1.1042, 1.0387,  ..., 0.9619, 1.0463, 0.8678]],\n",
       "\n",
       "        [[1.1074, 1.1042, 1.0387,  ..., 0.9619, 1.0463, 0.8678]]])"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "6229b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    decoder_output, extra = orig_model.decoder(\n",
    "        prev_output_tokens,\n",
    "        tgt_mask,\n",
    "        encoder_out=encoder_outs,\n",
    "        incremental_state=incremental_state,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "0a190958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 768])"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "fea0351e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4891, -0.3359, -0.2168,  ..., -0.2537, -0.4110, -0.0280]],\n",
       "\n",
       "        [[-0.4891, -0.3359, -0.2168,  ..., -0.2537, -0.4110, -0.0280]],\n",
       "\n",
       "        [[-0.4891, -0.3359, -0.2168,  ..., -0.2537, -0.4110, -0.0280]],\n",
       "\n",
       "        [[-0.4891, -0.3359, -0.2168,  ..., -0.2537, -0.4110, -0.0280]],\n",
       "\n",
       "        [[-0.4891, -0.3359, -0.2168,  ..., -0.2537, -0.4110, -0.0280]]])"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "05117956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[x.shape for x in extra[\"attn\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "b593692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [x.shape for x in extra[\"inner_states\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "b4fba04d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.speecht5.decoder.wrapped_decoder.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "9d486f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.decoder.wrapped_decoder(\n",
    "         hidden_states=prev_output_tokens,\n",
    "         #attention_mask=tgt_mask,\n",
    "         encoder_hidden_states=encoder_outs[\"encoder_out\"][0].permute((1, 0, 2)),\n",
    "         encoder_attention_mask=torch.tile(hf_encoder_attention_mask, (5, 1)),\n",
    "         return_dict=True,\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "ea512cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last_hidden_state', 'past_key_values']"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "d82c369a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 768])"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "eb6282bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4891, -0.3359, -0.2168,  ..., -0.2537, -0.4110, -0.0280]],\n",
       "\n",
       "        [[-0.4891, -0.3359, -0.2168,  ..., -0.2537, -0.4110, -0.0280]],\n",
       "\n",
       "        [[-0.4891, -0.3359, -0.2168,  ..., -0.2537, -0.4110, -0.0280]],\n",
       "\n",
       "        [[-0.4891, -0.3359, -0.2168,  ..., -0.2537, -0.4110, -0.0280]],\n",
       "\n",
       "        [[-0.4891, -0.3359, -0.2168,  ..., -0.2537, -0.4110, -0.0280]]])"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "f3e254cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.7684e-07)"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(decoder_output - hf_outputs[\"last_hidden_state\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef206524",
   "metadata": {},
   "source": [
    "Close enough! I would rather see 0.0 but this looks like numerical precision differences for 32-bit floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d3d1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d9113f7",
   "metadata": {},
   "source": [
    "## Verify text decoder postnet output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "a20be43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    lprobs = orig_model.text_decoder_postnet(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "a7a143bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 81])"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lprobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "7d6de14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-21.8423, -21.9258,   1.3216, -21.8823,  16.4646,   0.0780,   0.3366,\n",
       "          -1.1414,  -0.4092,  -1.6770,  -0.1935,   0.0583,  -0.1176,  -0.8975,\n",
       "          -2.3240,  -1.1076,  -0.8370,  -2.3857,  -0.8850,  -0.6924,   1.0384,\n",
       "          -0.1168,  -2.9756, -21.9991,  -1.3356,   0.6164, -21.9502,  -3.0913,\n",
       "          -2.4927, -21.8967, -21.9306,   0.2664, -22.0258, -21.8972, -21.8948,\n",
       "         -21.9652, -21.9363,  -5.7837, -21.9041, -21.9971, -21.8730, -21.9515,\n",
       "         -21.9392, -21.9500, -21.9247,  -2.4618,  -1.3141, -21.8651, -21.9030,\n",
       "         -22.0258, -21.9018, -21.9822, -21.8786, -21.8657, -21.9147, -21.8601,\n",
       "         -21.9148,  -5.8765, -21.9306, -21.8924, -21.9506, -21.9957, -21.9345,\n",
       "         -21.9068, -21.9166, -21.9429, -21.9176, -21.9675, -21.9592, -21.9133,\n",
       "         -21.8348, -21.9442, -21.8758, -21.9156, -21.8621, -21.9204, -21.8856,\n",
       "         -21.9069, -21.8934, -20.5559, -21.8836]])"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lprobs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "ba7571e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    lprobs_hf = hf_model.text_decoder_postnet(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "5623a791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 81])"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lprobs_hf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "64f0c976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-21.8423, -21.9258,   1.3216, -21.8823,  16.4646,   0.0780,   0.3366,\n",
       "          -1.1414,  -0.4092,  -1.6770,  -0.1935,   0.0583,  -0.1176,  -0.8975,\n",
       "          -2.3240,  -1.1076,  -0.8370,  -2.3857,  -0.8850,  -0.6924,   1.0384,\n",
       "          -0.1168,  -2.9756, -21.9991,  -1.3356,   0.6164, -21.9502,  -3.0913,\n",
       "          -2.4927, -21.8967, -21.9306,   0.2664, -22.0258, -21.8972, -21.8948,\n",
       "         -21.9652, -21.9363,  -5.7837, -21.9041, -21.9971, -21.8730, -21.9515,\n",
       "         -21.9392, -21.9500, -21.9247,  -2.4618,  -1.3141, -21.8651, -21.9030,\n",
       "         -22.0258, -21.9018, -21.9822, -21.8786, -21.8657, -21.9147, -21.8601,\n",
       "         -21.9148,  -5.8765, -21.9306, -21.8924, -21.9506, -21.9957, -21.9345,\n",
       "         -21.9068, -21.9166, -21.9429, -21.9176, -21.9675, -21.9592, -21.9133,\n",
       "         -21.8348, -21.9442, -21.8758, -21.9156, -21.8621, -21.9204, -21.8856,\n",
       "         -21.9069, -21.8934, -20.5559, -21.8836]])"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lprobs_hf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "6eeff463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(lprobs - lprobs_hf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24926eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32e98799",
   "metadata": {},
   "source": [
    "## Use the `transformers` generator loop:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8196542c",
   "metadata": {},
   "source": [
    "Run the full model to make sure this doesn't give any errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "b4b889ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.Seq2SeqLMOutput"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model(\n",
    "         input_values=inputs.input_values,\n",
    "         attention_mask=inputs.attention_mask,\n",
    "         decoder_input_ids=torch.tensor([[3, 4, 5]]),\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "677718c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logits', 'past_key_values', 'encoder_last_hidden_state']"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "7f2bcec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 81])"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d1eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dddb22cc",
   "metadata": {},
   "source": [
    "Also calculate loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "695cbd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.Seq2SeqLMOutput"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model(\n",
    "         input_values=inputs.input_values,\n",
    "         attention_mask=inputs.attention_mask,\n",
    "         #decoder_input_ids=torch.tensor([[2,  4, 18, 10, 12,  6,  5]]),\n",
    "         labels=torch.tensor([[4, 18, 10, 12,  6,  5, 13]]),\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "0bd14d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 81])"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "828d317d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8065)"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93712794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2684d34",
   "metadata": {},
   "source": [
    "Generator loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "b2fc2baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs = hf_model.generate(inputs.input_values, max_length=100)\n",
    "# hf_outputs = hf_model.generate(inputs.input_values, num_beams=5, max_length=100) #, bos_token_id=2)\n",
    "# hf_outputs = hf_model.generate(torch.rand(1, 10000), num_beams=5, max_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "015c80b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 52])"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "ce7e1899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  4, 46, 16, 12,  6,  4,  6, 11, 13, 16, 12,  6,  4,  7,  9, 14,  4,\n",
       "         24,  7, 13, 13, 22,  4,  7,  9, 14,  4, 27, 10, 17,  6,  8, 13, 22,  4,\n",
       "          6,  8,  4,  6, 11,  5,  4, 12,  6, 13,  8,  9, 21,  5, 13,  2]])"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "11fce9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " j u s t  t h r u s t  a n d  p a r r y  a n d  v i c t o r y  t o  t h e  s t r o n g e r\n",
      "just thrust and parry and victory to the stronger\n"
     ]
    }
   ],
   "source": [
    "for i in range(probs.shape[0]):\n",
    "    print(tgt_dict.string(hf_outputs))\n",
    "    print(tokenizer.decode(tgt_dict.string(hf_outputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "997fd48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '<pad>', '</s>', '<unk>', '']"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tgt_dict[x] for x in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8289a41b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "793497aa",
   "metadata": {},
   "source": [
    "For comparison, Speech2Text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "daf06a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset librispeech_asr_dummy (/Users/matthijs/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'mister quilter is the apostle of the middle classes and we are glad to welcome his gospel'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> import torch\n",
    ">>> from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n",
    ">>> from datasets import load_dataset\n",
    "\n",
    ">>> model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    ">>> processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "\n",
    ">>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    ">>> inputs = processor(\n",
    "...     ds[0][\"audio\"][\"array\"], sampling_rate=ds[0][\"audio\"][\"sampling_rate\"], return_tensors=\"pt\"\n",
    "... )\n",
    ">>> input_features = inputs.input_features\n",
    "\n",
    ">>> generated_ids = model.generate(inputs=input_features)\n",
    "\n",
    ">>> transcription = processor.batch_decode(generated_ids)[0]\n",
    ">>> transcription\n",
    "#'mister quilter is the apostle of the middle classes and we are glad to welcome his gospel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49667e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d50884f2",
   "metadata": {},
   "source": [
    "Test other methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1cd38a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_model.prune_heads({1: [0, 2], 2: [2,3 ]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc11e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f954bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
