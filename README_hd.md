<!---
Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

<!---
A useful guide for English-Hindi translation of Hugging Face documentation
- Add space around English words and numbers when they appear between Hindi characters. E.g., рдХреБрд▓ рдорд┐рд▓рд╛рдХрд░ 100 рд╕реЗ рдЕрдзрд┐рдХ рднрд╛рд╖рд╛рдПрдБ; рдЯреНрд░рд╛рдВрд╕рдлреЙрд░реНрдорд░ рд▓рд╛рдЗрдмреНрд░реЗрд░реА рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рддрд╛ рд╣реИред
- рд╡рд░реНрдЧрд╛рдХрд╛рд░ рдЙрджреНрдзрд░рдгреЛрдВ рдХрд╛ рдкреНрд░рдпреЛрдЧ рдХрд░реЗрдВ, рдЬреИрд╕реЗ, "рдЙрджреНрдзрд░рдг"

Dictionary

Hugging Face: рдЧрд▓реЗ рд▓рдЧрд╛рдУ рдЪреЗрд╣рд░рд╛
token: рд╢рдмреНрдж (рдФрд░ рдореВрд▓ рдЕрдВрдЧреНрд░реЗрдЬреА рдХреЛ рдХреЛрд╖реНрдардХ рдореЗрдВ рдЪрд┐рд╣реНрдирд┐рдд рдХрд░реЗрдВя╝Й
tokenize: рдЯреЛрдХрдирдирд╛рдЗрдЬрд╝ рдХрд░реЗрдВ (рдФрд░ рдореВрд▓ рдЕрдВрдЧреНрд░реЗрдЬрд╝реА рдХреЛ рдЪрд┐рд╣реНрдирд┐рдд рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдХреЛрд╖реНрдардХ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВ)
tokenizer: Tokenizer (рдореВрд▓ рдЕрдВрдЧреНрд░реЗрдЬреА рдореЗрдВ рдХреЛрд╖реНрдардХ рдХреЗ рд╕рд╛рде)
transformer: transformer
pipeline: рд╕рдордиреБрдХреНрд░рдо
API: API (рдЕрдиреБрд╡рд╛рдж рдХреЗ рдмрд┐рдирд╛)
inference: рд╡рд┐рдЪрд╛рд░
Trainer: рдкреНрд░рд╢рд┐рдХреНрд╖рдХред рдХрдХреНрд╖рд╛ рдХреЗ рдирд╛рдо рдХреЗ рд░реВрдк рдореЗрдВ рдкреНрд░рд╕реНрддреБрдд рдХрд┐рдП рдЬрд╛рдиреЗ рдкрд░ рдЕрдиреБрд╡рд╛рджрд┐рдд рдирд╣реАрдВ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
pretrained/pretrain: рдкреВрд░реНрд╡ рдкреНрд░рд╢рд┐рдХреНрд╖рдг
finetune: рдлрд╝рд╛рдЗрди рдЯреНрдпреВрдирд┐рдВрдЧ
community: рд╕рдореБрджрд╛рдп
example: рдЬрдм рд╡рд┐рд╢рд┐рд╖реНрдЯ рдЧреЛрджрд╛рдо example рдХреИрдЯрд▓реЙрдЧ рдХрд░рддреЗ рд╕рдордп "рдХреЗрд╕ рдХреЗрд╕" рдХреЗ рд░реВрдк рдореЗрдВ рдЕрдиреБрд╡рд╛рджрд┐рдд
Python data structures (e.g., list, set, dict): рдореВрд▓ рдЕрдВрдЧреНрд░реЗрдЬреА рдХреЛ рдЪрд┐рд╣реНрдирд┐рдд рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рд╕реВрдЪрд┐рдпреЛрдВ, рд╕реЗрдЯреЛрдВ, рд╢рдмреНрджрдХреЛрд╢реЛрдВ рдореЗрдВ рдЕрдиреБрд╡рд╛рдж рдХрд░реЗрдВ рдФрд░ рдХреЛрд╖реНрдардХ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВ
NLP/Natural Language Processing: рджреНрд╡рд╛рд░рд╛ NLP рдЕрдиреБрд╡рд╛рдж рдХреЗ рдмрд┐рдирд╛ рдкреНрд░рдХрдЯ рд╣реЛрддреЗ рд╣реИрдВ Natural Language Processing рдкреНрд░рд╕реНрддреБрдд рдХрд┐рдП рдЬрд╛рдиреЗ рдкрд░ рдкреНрд░рд╛рдХреГрддрд┐рдХ рднрд╛рд╖рд╛ рд╕рдВрд╕рд╛рдзрди рдореЗрдВ рдЕрдиреБрд╡рд╛рдж рдХрд░реЗрдВ
checkpoint: рдЬрд╛рдБрдЪ рдмрд┐рдВрджреБ
-->

<p align="center">
    <br>
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers_logo_name.png" width="400"/>
    <br>
</p>
<p align="center">
    <a href="https://circleci.com/gh/huggingface/transformers">
        <img alt="Build" src="https://img.shields.io/circleci/build/github/huggingface/transformers/main">
    </a>
    <a href="https://github.com/huggingface/transformers/blob/main/LICENSE">
        <img alt="GitHub" src="https://img.shields.io/github/license/huggingface/transformers.svg?color=blue">
    </a>
    <a href="https://huggingface.co/docs/transformers/index">
        <img alt="Documentation" src="https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online">
    </a>
    <a href="https://github.com/huggingface/transformers/releases">
        <img alt="GitHub release" src="https://img.shields.io/github/release/huggingface/transformers.svg">
    </a>
    <a href="https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md">
        <img alt="Contributor Covenant" src="https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg">
    </a>
    <a href="https://zenodo.org/badge/latestdoi/155220641"><img src="https://zenodo.org/badge/155220641.svg" alt="DOI"></a>
</p>

<h4 align="center">
    <p>
        <a href="https://github.com/huggingface/transformers/">English</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/README_zh-hans.md">чоАф╜Уф╕нцЦЗ</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/README_zh-hant.md">ч╣БщлФф╕нцЦЗ</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/README_ko.md">эХЬъ╡ньЦ┤</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/README_es.md">Espa├▒ol</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/README_ja.md">цЧецЬмшкЮ</a> |
        <b>рд╣рд┐рдиреНрджреА</b> |
        <a href="https://github.com/huggingface/transformers/blob/main/README_ru.md">╨а╤Г╤Б╤Б╨║╨╕╨╣</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/README_pt-br.md">╨аortugu├кs</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/README_te.md">р░др▒Жр░▓р▒Бр░Чр▒Б</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/README_fr.md">Fran├зais</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/README_de.md">Deutsch</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/README_vi.md">Tiс║┐ng Viс╗Зt</a> |
    </p>
</h4>

<h3 align="center">
    <p>Jax, PyTorch рдФрд░ TensorFlow рдХреЗ рд▓рд┐рдП рдЙрдиреНрдирдд рдорд╢реАрди рд▓рд░реНрдирд┐рдВрдЧ</p>
</h3>

<h3 align="center">
    <a href="https://hf.co/course"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png"></a>
</h3>

ЁЯдЧ Transformers 100 рд╕реЗ рдЕрдзрд┐рдХ рднрд╛рд╖рд╛рдУрдВ рдореЗрдВ рдкрд╛рда рд╡рд░реНрдЧреАрдХрд░рдг, рд╕реВрдЪрдирд╛ рдирд┐рд╖реНрдХрд░реНрд╖рдг, рдкреНрд░рд╢реНрди рдЙрддреНрддрд░, рд╕рд╛рд░рд╛рдВрд╢реАрдХрд░рдг, рдЕрдиреБрд╡рд╛рдж, рдкрд╛рда рдирд┐рд░реНрдорд╛рдг рдХрд╛ рд╕рдорд░реНрдерди рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рд╣рдЬрд╛рд░реЛрдВ рдкреВрд░реНрд╡-рдкреНрд░рд╢рд┐рдХреНрд╖рд┐рдд рдореЙрдбрд▓ рдкреНрд░рджрд╛рди рдХрд░рддрд╛ рд╣реИред рдЗрд╕рдХрд╛ рдЙрджреНрджреЗрд╢реНрдп рд╕рдмрд╕реЗ рдЙрдиреНрдирдд рдПрдирдПрд▓рдкреА рддрдХрдиреАрдХ рдХреЛ рд╕рднреА рдХреЗ рд▓рд┐рдП рд╕реБрд▓рдн рдмрдирд╛рдирд╛ рд╣реИред

ЁЯдЧ Transformers рддреНрд╡рд░рд┐рдд рдбрд╛рдЙрдирд▓реЛрдб рдФрд░ рдЙрдкрдпреЛрдЧ рдХреЗ рд▓рд┐рдП рдПрдХ рдПрдкреАрдЖрдИ рдкреНрд░рджрд╛рди рдХрд░рддрд╛ рд╣реИ, рдЬрд┐рд╕рд╕реЗ рдЖрдк рдХрд┐рд╕реА рджрд┐рдП рдЧрдП рдкрд╛рда рдкрд░ рдПрдХ рдкреВрд░реНрд╡-рдкреНрд░рд╢рд┐рдХреНрд╖рд┐рдд рдореЙрдбрд▓ рд▓реЗ рд╕рдХрддреЗ рд╣реИрдВ, рдЗрд╕реЗ рдЕрдкрдиреЗ рдбреЗрдЯрд╛рд╕реЗрдЯ рдкрд░ рдареАрдХ рдХрд░ рд╕рдХрддреЗ рд╣реИрдВ рдФрд░ рдЗрд╕реЗ [рдореЙрдбрд▓ рд╣рдм](https://huggingface.co/models) рдХреЗ рдорд╛рдзреНрдпрдо рд╕реЗ рд╕рдореБрджрд╛рдп рдХреЗ рд╕рд╛рде рд╕рд╛рдЭрд╛ рдХрд░ рд╕рдХрддреЗ рд╣реИрдВред рдЗрд╕реА рд╕рдордп, рдкреНрд░рддреНрдпреЗрдХ рдкрд░рд┐рднрд╛рд╖рд┐рдд рдкрд╛рдпрдерди рдореЙрдбреНрдпреВрд▓ рдкреВрд░реА рддрд░рд╣ рд╕реЗ рд╕реНрд╡рддрдВрддреНрд░ рд╣реИ, рдЬреЛ рд╕рдВрд╢реЛрдзрди рдФрд░ рддреЗрдЬреА рд╕реЗ рдЕрдиреБрд╕рдВрдзрд╛рди рдкреНрд░рдпреЛрдЧреЛрдВ рдХреЗ рд▓рд┐рдП рд╕реБрд╡рд┐рдзрд╛рдЬрдирдХ рд╣реИред

ЁЯдЧ Transformers рддреАрди рд╕рдмрд╕реЗ рд▓реЛрдХрдкреНрд░рд┐рдп рдЧрд╣рди рд╢рд┐рдХреНрд╖рдг рдкреБрд╕реНрддрдХрд╛рд▓рдпреЛрдВ рдХрд╛ рд╕рдорд░реНрдерди рдХрд░рддрд╛ рд╣реИя╝Ъ [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/) тАФ рдФрд░ рдЗрд╕рдХреЗ рд╕рд╛рде рдирд┐рд░реНрдмрд╛рдз рд░реВрдк рд╕реЗ рдПрдХреАрдХреГрдд рд╣реЛрддрд╛ рд╣реИред рдЖрдк рдЕрдкрдиреЗ рдореЙрдбрд▓ рдХреЛ рд╕реАрдзреЗ рдПрдХ рдврд╛рдВрдЪреЗ рдХреЗ рд╕рд╛рде рдкреНрд░рд╢рд┐рдХреНрд╖рд┐рдд рдХрд░ рд╕рдХрддреЗ рд╣реИрдВ рдФрд░ рджреВрд╕рд░реЗ рдХреЗ рд╕рд╛рде рд▓реЛрдб рдФрд░ рдЕрдиреБрдорд╛рди рд▓рдЧрд╛ рд╕рдХрддреЗ рд╣реИрдВред

## рдСрдирд▓рд╛рдЗрди рдбреЗрдореЛ

рдЖрдк рд╕рдмрд╕реЗ рд╕реАрдзреЗ рдореЙрдбрд▓ рдкреГрд╖реНрда рдкрд░ рдкрд░реАрдХреНрд╖рдг рдХрд░ рд╕рдХрддреЗ рд╣реИрдВ [model hub](https://huggingface.co/models) рдореЙрдбрд▓ рдкрд░ред рд╣рдо [рдирд┐рдЬреА рдореЙрдбрд▓ рд╣реЛрд╕реНрдЯрд┐рдВрдЧ, рдореЙрдбрд▓ рд╕рдВрд╕реНрдХрд░рдг, рдФрд░ рдЕрдиреБрдорд╛рди рдПрдкреАрдЖрдИ](https://huggingface.co/pricing) рднреА рдкреНрд░рджрд╛рди рдХрд░рддреЗ рд╣реИрдВредуАВ

рдпрд╣рд╛рдБ рдХреБрдЫ рдЙрджрд╛рд╣рд░рдг рд╣реИрдВя╝Ъ
- [рд╢рдмреНрдж рдХреЛ рднрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдорд╛рд╕реНрдХ рдХреЗ рд░реВрдк рдореЗрдВ BERT рдХрд╛ рдкреНрд░рдпреЛрдЧ рдХрд░реЗрдВ](https://huggingface.co/google-bert/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France)
- [рдЗрд▓реЗрдХреНрдЯреНрд░рд╛ рдХреЗ рд╕рд╛рде рдирд╛рдорд┐рдд рдЗрдХрд╛рдИ рдкрд╣рдЪрд╛рди](https://huggingface.co/dbmdz/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city)
- [рдЬреАрдкреАрдЯреА-2 рдХреЗ рд╕рд╛рде рдЯреЗрдХреНрд╕реНрдЯ рдЬрдирд░реЗрд╢рди](https://huggingface.co/openai-community/gpt2?text=A+long+time+ago%2C+)
- [рд░реЙрдмрд░реНрдЯрд╛ рдХреЗ рд╕рд╛рде рдкреНрд░рд╛рдХреГрддрд┐рдХ рднрд╛рд╖рд╛ рдирд┐рд╖реНрдХрд░реНрд╖](https://huggingface.co/FacebookAI/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal)
- [рдмрд╛рд░реНрдЯ рдХреЗ рд╕рд╛рде рдкрд╛рда рд╕рд╛рд░рд╛рдВрд╢](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+was+the+first+structure+to+reach+a+height+of+300+metres.+Due+to+the+addition+of+a+broadcasting+aerial+at+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct)
- [рдбрд┐рд╕реНрдЯрд┐рд▓рдмрд░реНрдЯ рдХреЗ рд╕рд╛рде рдкреНрд░рд╢реНрдиреЛрддреНрддрд░](https://huggingface.co/distilbert/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species)
- [рдЕрдиреБрд╡рд╛рдж рдХреЗ рд▓рд┐рдП T5 рдХрд╛ рдкреНрд░рдпреЛрдЧ рдХрд░реЗрдВ](https://huggingface.co/google-t5/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)

**[Write With Transformer](https://transformer.huggingface.co)**я╝Мрд╣рдЧрд┐рдВрдЧ рдлреЗрд╕ рдЯреАрдо рджреНрд╡рд╛рд░рд╛ рдмрдирд╛рдпрд╛ рдЧрдпрд╛, рдпрд╣ рдПрдХ рдЖрдзрд┐рдХрд╛рд░рд┐рдХ рдкрд╛рда рдкреАрдврд╝реА рд╣реИ demoуАВ

## рдпрджрд┐ рдЖрдк рд╣рдЧрд┐рдВрдЧ рдлреЗрд╕ рдЯреАрдо рд╕реЗ рдмреАрд╕реНрдкреЛрдХ рд╕рдорд░реНрдерди рдХреА рддрд▓рд╛рд╢ рдХрд░ рд░рд╣реЗ рд╣реИрдВ

<a target="_blank" href="https://huggingface.co/support">
    <img alt="HuggingFace Expert Acceleration Program" src="https://huggingface.co/front/thumbnails/support.png" style="max-width: 600px; border: 1px solid #eee; border-radius: 4px; box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);">
</a><br>

## рдЬрд▓реНрджреА рд╢реБрд░реВ рдХрд░реЗрдВ

рд╣рдо рддреНрд╡рд░рд┐рдд рдЙрдкрдпреЛрдЧ рдХреЗ рд▓рд┐рдП рдореЙрдбрд▓ рдкреНрд░рджрд╛рди рдХрд░рддреЗ рд╣реИрдВ `pipeline` (рдкрд╛рдЗрдкрд▓рд╛рдЗрди) рдПрдкреАрдЖрдИред рдкрд╛рдЗрдкрд▓рд╛рдЗрди рдкреВрд░реНрд╡-рдкреНрд░рд╢рд┐рдХреНрд╖рд┐рдд рдореЙрдбрд▓ рдФрд░ рд╕рдВрдмрдВрдзрд┐рдд рдкрд╛рда рдкреНрд░реАрдкреНрд░реЛрд╕реЗрд╕рд┐рдВрдЧ рдХреЛ рдПрдХрддреНрд░рд┐рдд рдХрд░рддреА рд╣реИред рд╕рдХрд╛рд░рд╛рддреНрдордХ рдФрд░ рдирдХрд╛рд░рд╛рддреНрдордХ рднрд╛рд╡рдирд╛ рдХреЛ рдирд┐рд░реНрдзрд╛рд░рд┐рдд рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдкрд╛рдЗрдкрд▓рд╛рдЗрдиреЛрдВ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рдиреЗ рдХрд╛ рдПрдХ рддреНрд╡рд░рд┐рдд рдЙрджрд╛рд╣рд░рдг рдпрд╣рд╛рдВ рджрд┐рдпрд╛ рдЧрдпрд╛ рд╣реИ:

```python
>>> from transformers import pipeline

# рднрд╛рд╡рдирд╛ рд╡рд┐рд╢реНрд▓реЗрд╖рдг рдкрд╛рдЗрдкрд▓рд╛рдЗрди рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рдирд╛
>>> classifier = pipeline('sentiment-analysis')
>>> classifier('We are very happy to introduce pipeline to the transformers repository.')
[{'label': 'POSITIVE', 'score': 0.9996980428695679}]
```

рдХреЛрдб рдХреА рджреВрд╕рд░реА рдкрдВрдХреНрддрд┐ рдкрд╛рдЗрдкрд▓рд╛рдЗрди рджреНрд╡рд╛рд░рд╛ рдЙрдкрдпреЛрдЧ рдХрд┐рдП рдЧрдП рдкреВрд░реНрд╡-рдкреНрд░рд╢рд┐рдХреНрд╖рд┐рдд рдореЙрдбрд▓ рдХреЛ рдбрд╛рдЙрдирд▓реЛрдб рдФрд░ рдХреИрд╢ рдХрд░рддреА рд╣реИ, рдЬрдмрдХрд┐ рдХреЛрдб рдХреА рддреАрд╕рд░реА рдкрдВрдХреНрддрд┐ рджрд┐рдП рдЧрдП рдкрд╛рда рдкрд░ рдореВрд▓реНрдпрд╛рдВрдХрди рдХрд░рддреА рд╣реИред рдпрд╣рд╛рдВ рдЙрддреНрддрд░ 99 рдЖрддреНрдорд╡рд┐рд╢реНрд╡рд╛рд╕ рдХреЗ рд╕реНрддрд░ рдХреЗ рд╕рд╛рде "рд╕рдХрд╛рд░рд╛рддреНрдордХ" рд╣реИред

рдХрдИ рдПрдирдПрд▓рдкреА рдХрд╛рд░реНрдпреЛрдВ рдореЗрдВ рдЖрдЙрдЯ рдСреЮ рдж рдмреЙрдХреНрд╕ рдкрд╛рдЗрдкрд▓рд╛рдЗрдиреЛрдВ рдХрд╛ рдкреВрд░реНрд╡-рдкреНрд░рд╢рд┐рдХреНрд╖рдг рд╣реЛрддрд╛ рд╣реИред рдЙрджрд╛рд╣рд░рдг рдХреЗ рд▓рд┐рдП, рд╣рдо рдХрд┐рд╕реА рджрд┐рдП рдЧрдП рдкрд╛рда рд╕реЗ рдХрд┐рд╕реА рдкреНрд░рд╢реНрди рдХрд╛ рдЙрддреНрддрд░ рдЖрд╕рд╛рдиреА рд╕реЗ рдирд┐рдХрд╛рд▓ рд╕рдХрддреЗ рд╣реИрдВ:

``` python
>>> from transformers import pipeline

# рдкреНрд░рд╢реНрдиреЛрддреНрддрд░ рдкрд╛рдЗрдкрд▓рд╛рдЗрди рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рдирд╛
>>> question_answerer = pipeline('question-answering')
>>> question_answerer({
...     'question': 'What is the name of the repository ?',
...     'context': 'Pipeline has been included in the huggingface/transformers repository'
... })
{'score': 0.30970096588134766, 'start': 34, 'end': 58, 'answer': 'huggingface/transformers'}

```

рдЙрддреНрддрд░ рджреЗрдиреЗ рдХреЗ рдЕрд▓рд╛рд╡рд╛, рдкреВрд░реНрд╡-рдкреНрд░рд╢рд┐рдХреНрд╖рд┐рдд рдореЙрдбрд▓ рд╕рдВрдЧрдд рдЖрддреНрдорд╡рд┐рд╢реНрд╡рд╛рд╕ рд╕реНрдХреЛрд░ рднреА рджреЗрддрд╛ рд╣реИ, рдЬрд╣рд╛рдВ рдЙрддреНрддрд░ рдЯреЛрдХрдирдпреБрдХреНрдд рдкрд╛рда рдореЗрдВ рд╢реБрд░реВ рдФрд░ рд╕рдорд╛рдкреНрдд рд╣реЛрддрд╛ рд╣реИред рдЖрдк [рдЗрд╕ рдЯреНрдпреВрдЯреЛрд░рд┐рдпрд▓](https://huggingface.co/docs/transformers/task_summary) рд╕реЗ рдкрд╛рдЗрдкрд▓рд╛рдЗрди рдПрдкреАрдЖрдИ рджреНрд╡рд╛рд░рд╛ рд╕рдорд░реНрдерд┐рдд рдХрд╛рд░реНрдпреЛрдВ рдХреЗ рдмрд╛рд░реЗ рдореЗрдВ рдЕрдзрд┐рдХ рдЬрд╛рди рд╕рдХрддреЗ рд╣реИрдВред

рдЕрдкрдиреЗ рдХрд╛рд░реНрдп рдкрд░ рдХрд┐рд╕реА рднреА рдкреВрд░реНрд╡-рдкреНрд░рд╢рд┐рдХреНрд╖рд┐рдд рдореЙрдбрд▓ рдХреЛ рдбрд╛рдЙрдирд▓реЛрдб рдХрд░рдирд╛ рдФрд░ рдЙрд╕рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рдирд╛ рднреА рдХреЛрдб рдХреА рддреАрди рдкрдВрдХреНрддрд┐рдпреЛрдВ рдХреА рддрд░рд╣ рд╕рд░рд▓ рд╣реИред рдпрд╣рд╛рдБ PyTorch рд╕рдВрд╕реНрдХрд░рдг рдХреЗ рд▓рд┐рдП рдПрдХ рдЙрджрд╛рд╣рд░рдг рджрд┐рдпрд╛ рдЧрдпрд╛ рд╣реИ:
```python
>>> from transformers import AutoTokenizer, AutoModel

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
>>> model = AutoModel.from_pretrained("google-bert/bert-base-uncased")

>>> inputs = tokenizer("Hello world!", return_tensors="pt")
>>> outputs = model(**inputs)
```
рдпрд╣рд╛рдБ рд╕рдордХрдХреНрд╖ рд╣реИ TensorFlow рдХреЛрдб:
```python
>>> from transformers import AutoTokenizer, TFAutoModel

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
>>> model = TFAutoModel.from_pretrained("google-bert/bert-base-uncased")

>>> inputs = tokenizer("Hello world!", return_tensors="tf")
>>> outputs = model(**inputs)
```

рдЯреЛрдХрдирдирд╛рдЗрдЬрд╝рд░ рд╕рднреА рдкреВрд░реНрд╡-рдкреНрд░рд╢рд┐рдХреНрд╖рд┐рдд рдореЙрдбрд▓реЛрдВ рдХреЗ рд▓рд┐рдП рдкреНрд░реАрдкреНрд░реЛрд╕реЗрд╕рд┐рдВрдЧ рдкреНрд░рджрд╛рди рдХрд░рддрд╛ рд╣реИ рдФрд░ рдЗрд╕реЗ рд╕реАрдзреЗ рдПрдХ рд╕реНрдЯреНрд░рд┐рдВрдЧ (рдЬреИрд╕реЗ рдКрдкрд░ рджрд┐рдП рдЧрдП рдЙрджрд╛рд╣рд░рдг) рдпрд╛ рдХрд┐рд╕реА рд╕реВрдЪреА рдкрд░ рдмреБрд▓рд╛рдпрд╛ рдЬрд╛ рд╕рдХрддрд╛ рд╣реИред рдпрд╣ рдПрдХ рдбрд┐рдХреНрд╢рдирд░реА (рддрд╛рдирд╛рд╢рд╛рд╣реА) рдХреЛ рдЖрдЙрдЯрдкреБрдЯ рдХрд░рддрд╛ рд╣реИ рдЬрд┐рд╕реЗ рдЖрдк рдбрд╛рдЙрдирд╕реНрдЯреНрд░реАрдо рдХреЛрдб рдореЗрдВ рдЙрдкрдпреЛрдЧ рдХрд░ рд╕рдХрддреЗ рд╣реИрдВ рдпрд╛ `**` рдЕрдирдкреИрдХрд┐рдВрдЧ рдПрдХреНрд╕рдкреНрд░реЗрд╢рди рдХреЗ рдорд╛рдзреНрдпрдо рд╕реЗ рд╕реАрдзреЗ рдореЙрдбрд▓ рдХреЛ рдкрд╛рд╕ рдХрд░ рд╕рдХрддреЗ рд╣реИрдВред

рдореЙрдбрд▓ рд╕реНрд╡рдпрдВ рдПрдХ рдирд┐рдпрдорд┐рдд [Pytorch `nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) рдпрд╛ [TensorFlow `tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) (рдЖрдкрдХреЗ рдмреИрдХрдПрдВрдб рдХреЗ рдЖрдзрд╛рд░ рдкрд░), рдЬреЛ рд╣реЛ рд╕рдХрддрд╛ рд╣реИ рд╕рд╛рдорд╛рдиреНрдп рддрд░реАрдХреЗ рд╕реЗ рдЙрдкрдпреЛрдЧ рдХрд┐рдпрд╛ рдЬрд╛рддрд╛ рд╣реИред [рдпрд╣ рдЯреНрдпреВрдЯреЛрд░рд┐рдпрд▓](https://huggingface.co/transformers/training.html) рдмрддрд╛рддрд╛ рд╣реИ рдХрд┐ рдЗрд╕ рддрд░рд╣ рдХреЗ рдореЙрдбрд▓ рдХреЛ рдХреНрд▓рд╛рд╕рд┐рдХ PyTorch рдпрд╛ TensorFlow рдкреНрд░рд╢рд┐рдХреНрд╖рдг рд▓реВрдк рдореЗрдВ рдХреИрд╕реЗ рдПрдХреАрдХреГрдд рдХрд┐рдпрд╛ рдЬрд╛рдП, рдпрд╛ рд╣рдорд╛рд░реЗ `рдЯреНрд░реЗрдирд░` рдПрдкреАрдЖрдИ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХреИрд╕реЗ рдХрд░реЗрдВ рддрд╛рдХрд┐ рдЗрд╕реЗ рдЬрд▓реНрджреА рд╕реЗ рдлрд╝рд╛рдЗрди рдЯреНрдпреВрди рдХрд┐рдпрд╛ рдЬрд╛ рд╕рдХреЗредрдПрдХ рдирдпрд╛ рдбреЗрдЯрд╛рд╕реЗрдЯ рдкреЗред

## рдЯреНрд░рд╛рдВрд╕рдлрд╛рд░реНрдорд░ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХреНрдпреЛрдВ рдХрд░реЗрдВ?

1. рдЙрдкрдпреЛрдЧ рдореЗрдВ рдЖрд╕рд╛рдиреА рдХреЗ рд▓рд┐рдП рдЙрдиреНрдирдд рдореЙрдбрд▓:
    - рдПрдирдПрд▓рдпреВ рдФрд░ рдПрдирдПрд▓рдЬреА рдкрд░ рдмреЗрд╣рддрд░ рдкреНрд░рджрд░реНрд╢рди
    - рдкреНрд░рд╡реЗрд╢ рдХреЗ рд▓рд┐рдП рдХрдо рдмрд╛рдзрд╛рдУрдВ рдХреЗ рд╕рд╛рде рд╢рд┐рдХреНрд╖рдг рдФрд░ рдЕрднреНрдпрд╛рд╕ рдХреЗ рдЕрдиреБрдХреВрд▓
    - рдЙрдкрдпреЛрдЧрдХрд░реНрддрд╛-рд╕рд╛рдордирд╛ рдХрд░рдиреЗ рд╡рд╛рд▓реЗ рд╕рд╛рд░ рддрддреНрд╡, рдХреЗрд╡рд▓ рддреАрди рд╡рд░реНрдЧреЛрдВ рдХреЛ рдЬрд╛рдирдиреЗ рдХреА рдЬрд░реВрд░рдд рд╣реИ
    - рд╕рднреА рдореЙрдбрд▓реЛрдВ рдХреЗ рд▓рд┐рдП рдПрдХреАрдХреГрдд рдПрдкреАрдЖрдИ

1. рдХрдо рдХрдореНрдкреНрдпреВрдЯреЗрд╢рдирд▓ рдУрд╡рд░рд╣реЗрдб рдФрд░ рдХрдо рдХрд╛рд░реНрдмрди рдЙрддреНрд╕рд░реНрдЬрди:
    - рд╢реЛрдзрдХрд░реНрддрд╛ рд╣рд░ рдмрд╛рд░ рдирдП рд╕рд┐рд░реЗ рд╕реЗ рдкреНрд░рд╢рд┐рдХреНрд╖рдг рджреЗрдиреЗ рдХреЗ рдмрдЬрд╛рдп рдкреНрд░рд╢рд┐рдХреНрд╖рд┐рдд рдореЙрдбрд▓ рд╕рд╛рдЭрд╛ рдХрд░ рд╕рдХрддреЗ рд╣реИрдВ
    - рдЗрдВрдЬреАрдирд┐рдпрд░ рдЧрдгрдирд╛ рд╕рдордп рдФрд░ рдЙрддреНрдкрд╛рджрди рдУрд╡рд░рд╣реЗрдб рдХреЛ рдХрдо рдХрд░ рд╕рдХрддреЗ рд╣реИрдВ
    - рджрд░реНрдЬрдиреЛрдВ рдореЙрдбрд▓ рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░, 2,000 рд╕реЗ рдЕрдзрд┐рдХ рдкреВрд░реНрд╡-рдкреНрд░рд╢рд┐рдХреНрд╖рд┐рдд рдореЙрдбрд▓, 100 рд╕реЗ рдЕрдзрд┐рдХ рднрд╛рд╖рд╛рдУрдВ рдХрд╛ рд╕рдорд░реНрдерди

1.рдореЙрдбрд▓ рдЬреАрд╡рдирдЪрдХреНрд░ рдХреЗ рд╣рд░ рд╣рд┐рд╕реНрд╕реЗ рдХреЛ рд╢рд╛рдорд┐рд▓ рдХрд░рддрд╛ рд╣реИ:
    - рдХреЛрдб рдХреА рдХреЗрд╡рд▓ 3 рдкрдВрдХреНрддрд┐рдпреЛрдВ рдореЗрдВ рдЙрдиреНрдирдд рдореЙрдбрд▓реЛрдВ рдХреЛ рдкреНрд░рд╢рд┐рдХреНрд╖рд┐рдд рдХрд░реЗрдВ
    - рдореЙрдбрд▓ рдХреЛ рдордирдорд╛рдиреЗ рдврдВрдЧ рд╕реЗ рд╡рд┐рднрд┐рдиреНрди рдбреАрдк рд▓рд░реНрдирд┐рдВрдЧ рдлреНрд░реЗрдорд╡рд░реНрдХ рдХреЗ рдмреАрдЪ рд╕реНрдерд╛рдирд╛рдВрддрд░рд┐рдд рдХрд┐рдпрд╛ рдЬрд╛ рд╕рдХрддрд╛ рд╣реИ, рдЬреИрд╕рд╛ рдЖрдк рдЪрд╛рд╣рддреЗ рд╣реИрдВ
    - рдирд┐рд░реНрдмрд╛рдз рд░реВрдк рд╕реЗ рдкреНрд░рд╢рд┐рдХреНрд╖рдг, рдореВрд▓реНрдпрд╛рдВрдХрди рдФрд░ рдЙрддреНрдкрд╛рджрди рдХреЗ рд▓рд┐рдП рд╕рдмрд╕реЗ рдЙрдкрдпреБрдХреНрдд рдврд╛рдВрдЪрд╛ рдЪреБрдиреЗрдВ

1. рдЖрд╕рд╛рдиреА рд╕реЗ рдЕрдирдиреНрдп рдореЙрдбрд▓ рдХреЛ рдЕрдиреБрдХреВрд▓рд┐рдд рдХрд░реЗрдВ рдФрд░ рдЕрдкрдиреА рдЖрд╡рд╢реНрдпрдХрддрд╛рдУрдВ рдХреЗ рд▓рд┐рдП рдорд╛рдорд▓реЛрдВ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВ:
    - рд╣рдо рдореВрд▓ рдкреЗрдкрд░ рдкрд░рд┐рдгрд╛рдореЛрдВ рдХреЛ рдкреБрди: рдкреЗрд╢ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдкреНрд░рддреНрдпреЗрдХ рдореЙрдбрд▓ рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░ рдХреЗ рд▓рд┐рдП рдХрдИ рдЙрдкрдпреЛрдЧ рдХреЗ рдорд╛рдорд▓реЗ рдкреНрд░рджрд╛рди рдХрд░рддреЗ рд╣реИрдВ
    - рдореЙрдбрд▓ рдХреА рдЖрдВрддрд░рд┐рдХ рд╕рдВрд░рдЪрдирд╛ рдкрд╛рд░рджрд░реНрд╢реА рдФрд░ рд╕реБрд╕рдВрдЧрдд рд░рд╣рддреА рд╣реИ
    - рдореЙрдбрд▓ рдлрд╝рд╛рдЗрд▓ рдХреЛ рдЕрд▓рдЧ рд╕реЗ рдЗрд╕реНрддреЗрдорд╛рд▓ рдХрд┐рдпрд╛ рдЬрд╛ рд╕рдХрддрд╛ рд╣реИ, рдЬреЛ рд╕рдВрд╢реЛрдзрди рдФрд░ рддреНрд╡рд░рд┐рдд рдкреНрд░рдпреЛрдЧ рдХреЗ рд▓рд┐рдП рд╕реБрд╡рд┐рдзрд╛рдЬрдирдХ рд╣реИ

## рдореБрдЭреЗ рдЯреНрд░рд╛рдВрд╕рдлреЙрд░реНрдорд░ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрдм рдирд╣реАрдВ рдХрд░рдирд╛ рдЪрд╛рд╣рд┐рдП?

- рдпрд╣ рд▓рд╛рдЗрдмреНрд░реЗрд░реА рдореЙрдбреНрдпреВрд▓рд░ рдиреНрдпреВрд░рд▓ рдиреЗрдЯрд╡рд░реНрдХ рдЯреВрд▓рдмреЙрдХреНрд╕ рдирд╣реАрдВ рд╣реИред рдореЙрдбрд▓ рдлрд╝рд╛рдЗрд▓ рдореЗрдВ рдХреЛрдб рдЬрд╛рдирдмреВрдЭрдХрд░ рдЕрд▓реНрдкрд╡рд┐рдХрд╕рд┐рдд рд╣реИ, рдмрд┐рдирд╛ рдЕрддрд┐рд░рд┐рдХреНрдд рд╕рд╛рд░ рдЗрдирдХреИрдкреНрд╕реБрд▓реЗрд╢рди рдХреЗ, рддрд╛рдХрд┐ рд╢реЛрдзрдХрд░реНрддрд╛ рдЕрдореВрд░реНрддрддрд╛ рдФрд░ рдлрд╝рд╛рдЗрд▓ рдЬрдВрдкрд┐рдВрдЧ рдореЗрдВ рд╢рд╛рдорд┐рд▓ рд╣реБрдП рдЬрд▓реНрджреА рд╕реЗ рдкреБрдирд░рд╛рд╡реГрддрд┐ рдХрд░ рд╕рдХреЗрдВред
- `рдЯреНрд░реЗрдирд░` рдПрдкреАрдЖрдИ рдХрд┐рд╕реА рднреА рдореЙрдбрд▓ рдХреЗ рд╕рд╛рде рд╕рдВрдЧрдд рдирд╣реАрдВ рд╣реИ, рдпрд╣ рдХреЗрд╡рд▓ рдЗрд╕ рдкреБрд╕реНрддрдХрд╛рд▓рдп рдХреЗ рдореЙрдбрд▓ рдХреЗ рд▓рд┐рдП рдЕрдиреБрдХреВрд▓рд┐рдд рд╣реИред рдпрджрд┐ рдЖрдк рд╕рд╛рдорд╛рдиреНрдп рдорд╢реАрди рд▓рд░реНрдирд┐рдВрдЧ рдХреЗ рд▓рд┐рдП рдЙрдкрдпреБрдХреНрдд рдкреНрд░рд╢рд┐рдХреНрд╖рдг рд▓реВрдк рдХрд╛рд░реНрдпрд╛рдиреНрд╡рдпрди рдХреА рддрд▓рд╛рд╢ рдореЗрдВ рд╣реИрдВ, рддреЛ рдХрд╣реАрдВ рдФрд░ рджреЗрдЦреЗрдВред
- рд╣рдорд╛рд░реЗ рд╕рд░реНрд╡реЛрддреНрддрдо рдкреНрд░рдпрд╛рд╕реЛрдВ рдХреЗ рдмрд╛рд╡рдЬреВрдж, [рдЙрджрд╛рд╣рд░рдг рдирд┐рд░реНрджреЗрд╢рд┐рдХрд╛](https://github.com/huggingface/transformers/tree/main/examples) рдореЗрдВ рд╕реНрдХреНрд░рд┐рдкреНрдЯ рдХреЗрд╡рд▓ рдЙрдкрдпреЛрдЧ рдХреЗ рдорд╛рдорд▓реЗ рд╣реИрдВред рдЖрдкрдХреА рд╡рд┐рд╢рд┐рд╖реНрдЯ рд╕рдорд╕реНрдпрд╛ рдХреЗ рд▓рд┐рдП, рд╡реЗ рдЬрд░реВрд░реА рдирд╣реАрдВ рдХрд┐ рдмреЙрдХреНрд╕ рд╕реЗ рдмрд╛рд╣рд░ рдХрд╛рдо рдХрд░реЗрдВ, рдФрд░ рдЖрдкрдХреЛ рдХреЛрдб рдХреА рдХреБрдЫ рдкрдВрдХреНрддрд┐рдпреЛрдВ рдХреЛ рд╕реВрдЯ рдХрд░рдиреЗ рдХреА рдЖрд╡рд╢реНрдпрдХрддрд╛ рд╣реЛ рд╕рдХрддреА рд╣реИред

## рд╕реНрдерд╛рдкрд┐рдд рдХрд░рдирд╛

### рдкрд┐рдк рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рдирд╛

рдЗрд╕ рд░рд┐рдкреЙрдЬрд┐рдЯрд░реА рдХрд╛ рдкрд░реАрдХреНрд╖рдг Python 3.8+, Flax 0.4.1+, PyTorch 1.11+ рдФрд░ TensorFlow 2.6+ рдХреЗ рддрд╣рдд рдХрд┐рдпрд╛ рдЧрдпрд╛ рд╣реИред

рдЖрдк [рд╡рд░реНрдЪреБрдЕрд▓ рдПрдирд╡рд╛рдпрд░рдирдореЗрдВрдЯ](https://docs.python.org/3/library/venv.html) рдореЗрдВ ЁЯдЧ рдЯреНрд░рд╛рдВрд╕рдлреЙрд░реНрдорд░ рдЗрдВрд╕реНрдЯреЙрд▓ рдХрд░ рд╕рдХрддреЗ рд╣реИрдВред рдпрджрд┐ рдЖрдк рдЕрднреА рддрдХ рдкрд╛рдпрдерди рдХреЗ рд╡рд░реНрдЪреБрдЕрд▓ рдПрдирд╡рд╛рдпрд░рдирдореЗрдВрдЯ рд╕реЗ рдкрд░рд┐рдЪрд┐рдд рдирд╣реАрдВ рд╣реИрдВ, рддреЛ рдХреГрдкрдпрд╛ рдЗрд╕реЗ [рдЙрдкрдпреЛрдЧрдХрд░реНрддрд╛ рдирд┐рд░реНрджреЗрд╢](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/) рдкрдврд╝реЗрдВред

рд╕рдмрд╕реЗ рдкрд╣рд▓реЗ, рдкрд╛рдпрдерди рдХреЗ рдЙрд╕ рд╕рдВрд╕реНрдХрд░рдг рдХреЗ рд╕рд╛рде рдПрдХ рдЖрднрд╛рд╕реА рд╡рд╛рддрд╛рд╡рд░рдг рдмрдирд╛рдПрдВ рдЬрд┐рд╕рдХрд╛ рдЖрдк рдЙрдкрдпреЛрдЧ рдХрд░рдиреЗ рдФрд░ рдЙрд╕реЗ рд╕рдХреНрд░рд┐рдп рдХрд░рдиреЗ рдХреА рдпреЛрдЬрдирд╛ рдмрдирд╛ рд░рд╣реЗ рд╣реИрдВред

рдлрд┐рд░, рдЖрдкрдХреЛ Flax, PyTorch рдпрд╛ TensorFlow рдореЗрдВ рд╕реЗ рдХрд┐рд╕реА рдПрдХ рдХреЛ рд╕реНрдерд╛рдкрд┐рдд рдХрд░рдиреЗ рдХреА рдЖрд╡рд╢реНрдпрдХрддрд╛ рд╣реИред рдЕрдкрдиреЗ рдкреНрд▓реЗрдЯрдлрд╝реЙрд░реНрдо рдкрд░ рдЗрди рдлрд╝реНрд░реЗрдорд╡рд░реНрдХ рдХреЛ рд╕реНрдерд╛рдкрд┐рдд рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП, [TensorFlow рд╕реНрдерд╛рдкрдирд╛ рдкреГрд╖реНрда](https://www.tensorflow.org/install/), [PyTorch рд╕реНрдерд╛рдкрдирд╛ рдкреГрд╖реНрда](https://pytorch.org/get-started/locally)

рджреЗрдЦреЗрдВ start-locally рдпрд╛ [Flax рд╕реНрдерд╛рдкрдирд╛ рдкреГрд╖реНрда](https://github.com/google/flax#quick-install).

рдЬрдм рдЗрдирдореЗрдВ рд╕реЗ рдХреЛрдИ рдПрдХ рдмреИрдХрдПрдВрдб рд╕рдлрд▓рддрд╛рдкреВрд░реНрд╡рдХ рд╕реНрдерд╛рдкрд┐рдд рд╣реЛ рдЬрд╛рддрд╛ рд╣реИ, рддреЛ рдЯреНрд░рд╛рдВрд╕рдлреЙрд░реНрдорд░ рдирд┐рдореНрдирд╛рдиреБрд╕рд╛рд░ рд╕реНрдерд╛рдкрд┐рдд рдХрд┐рдП рдЬрд╛ рд╕рдХрддреЗ рд╣реИрдВ:

```bash
pip install transformers
```

рдпрджрд┐ рдЖрдк рдЙрдкрдпреЛрдЧ рдХреЗ рдорд╛рдорд▓реЛрдВ рдХреЛ рдЖрдЬрд╝рдорд╛рдирд╛ рдЪрд╛рд╣рддреЗ рд╣реИрдВ рдпрд╛ рдЖрдзрд┐рдХрд╛рд░рд┐рдХ рд░рд┐рд▓реАрдЬрд╝ рд╕реЗ рдкрд╣рд▓реЗ рдирд╡реАрдирддрдо рдЗрди-рдбреЗрд╡рд▓рдкрдореЗрдВрдЯ рдХреЛрдб рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рдирд╛ рдЪрд╛рд╣рддреЗ рд╣реИрдВ, рддреЛ рдЖрдкрдХреЛ [рд╕реЛрд░реНрд╕ рд╕реЗ рдЗрдВрд╕реНрдЯреЙрд▓ рдХрд░рдирд╛ рд╣реЛрдЧрд╛](https://huggingface.co/docs/transformers/installation#installing-from-) рд╕реНрд░реЛрддред

### рдХреЛрдВрдбрд╛ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рдирд╛

рдЯреНрд░рд╛рдВрд╕рдлреЙрд░реНрдорд░ рдХреЛрдВрдбрд╛ рдХреЗ рдорд╛рдзреНрдпрдо рд╕реЗ рдирд┐рдореНрдирд╛рдиреБрд╕рд╛рд░ рд╕реНрдерд╛рдкрд┐рдд рдХрд┐рдпрд╛ рдЬрд╛ рд╕рдХрддрд╛ рд╣реИ:

```shell script
conda install conda-forge::transformers
```

> **_рдиреЛрдЯ:_** `huggingface` рдЪреИрдирд▓ рд╕реЗ `transformers` рдЗрдВрд╕реНрдЯреЙрд▓ рдХрд░рдирд╛ рдкреБрд░рд╛рдирд╛ рдкрдбрд╝ рдЪреБрдХрд╛ рд╣реИред

рдХреЛрдВрдбрд╛ рдХреЗ рдорд╛рдзреНрдпрдо рд╕реЗ Flax, PyTorch, рдпрд╛ TensorFlow рдореЗрдВ рд╕реЗ рдХрд┐рд╕реА рдПрдХ рдХреЛ рд╕реНрдерд╛рдкрд┐рдд рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП, рдирд┐рд░реНрджреЗрд╢реЛрдВ рдХреЗ рд▓рд┐рдП рдЙрдирдХреЗ рд╕рдВрдмрдВрдзрд┐рдд рд╕реНрдерд╛рдкрдирд╛ рдкреГрд╖реНрда рджреЗрдЦреЗрдВред

## рдореЙрдбрд▓ рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░
[рдЙрдкрдпреЛрдЧрдХрд░реНрддрд╛](https://huggingface.co/users) рдФрд░ [organization](https://huggingface.co) рджреНрд╡рд╛рд░рд╛ рдЯреНрд░рд╛рдВрд╕рдлреЙрд░реНрдорд░ рд╕рдорд░реНрдерд┐рдд [**рд╕рднреА рдореЙрдбрд▓ рдЪреМрдХрд┐рдпреЛрдВ**](https://huggingface.co/models/users) рд╣рдЧрд┐рдВрдЧрдлреЗрд╕.рдХреЛ/рдСрд░реНрдЧрдирд╛рдЗрдЬреЗрд╢рди), рд╕рднреА рдХреЛ рдмрд┐рдирд╛ рдХрд┐рд╕реА рдмрд╛рдзрд╛ рдХреЗ рд╣рдЧрд┐рдВрдЧрдлреЗрд╕.рдХреЛ [рдореЙрдбрд▓ рд╣рдм](https://huggingface.co) рдХреЗ рд╕рд╛рде рдПрдХреАрдХреГрдд рдХрд┐рдпрд╛ рдЧрдпрд╛ рд╣реИред

рдЪреМрдХрд┐рдпреЛрдВ рдХреА рд╡рд░реНрддрдорд╛рди рд╕рдВрдЦреНрдпрд╛: ![](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen)

ЁЯдЧ рдЯреНрд░рд╛рдВрд╕рдлреЙрд░реНрдорд░ рд╡рд░реНрддрдорд╛рди рдореЗрдВ рдирд┐рдореНрдирд▓рд┐рдЦрд┐рдд рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░ рдХрд╛ рд╕рдорд░реНрдерди рдХрд░рддреЗ рд╣реИрдВ (рдореЙрдбрд▓ рдХреЗ рдЕрд╡рд▓реЛрдХрди рдХреЗ рд▓рд┐рдП [рдпрд╣рд╛рдВ рджреЗрдЦреЗрдВ](https://huggingface.co/docs/transformers/model_summary))я╝Ъ

1. **[ALBERT](https://huggingface.co/docs/transformers/model_doc/albert)** (Google Research and the Toyota Technological Institute at Chicago) рд╕рд╛рде рдереАрд╕рд┐рд╕ [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942), рдЭреЗрдВрдЭреЛрдВрдЧ рд▓реИрди, рдорд┐рдВрдЧрджрд╛ рдЪреЗрди, рд╕реЗрдмреЗрд╕реНрдЯрд┐рдпрди рдЧреБрдбрдореИрди, рдХреЗрд╡рд┐рди рдЧрд┐рдореНрдкреЗрд▓, рдкреАрдпреВрд╖ рд╢рд░реНрдорд╛, рд░рд╛рдбреВ рд╕реЛрд░рд┐рдХрдЯ
1. **[ALIGN](https://huggingface.co/docs/transformers/model_doc/align)** (Google Research рд╕реЗ) Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](https://arxiv.org/abs/2102.05918) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[AltCLIP](https://huggingface.co/docs/transformers/model_doc/altclip)** (from BAAI) released with the paper [AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities](https://arxiv.org/abs/2211.06679) by Chen, Zhongzhi and Liu, Guang and Zhang, Bo-Wen and Ye, Fulong and Yang, Qinghong and Wu, Ledell.
1. **[Audio Spectrogram Transformer](https://huggingface.co/docs/transformers/model_doc/audio-spectrogram-transformer)** (from MIT) released with the paper [AST: Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778) by Yuan Gong, Yu-An Chung, James Glass.
1. **[Autoformer](https://huggingface.co/docs/transformers/model_doc/autoformer)** (from Tsinghua University) released with the paper [Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting](https://arxiv.org/abs/2106.13008) by Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long.
1. **[Bark](https://huggingface.co/docs/transformers/model_doc/bark)** (from Suno) released in the repository [suno-ai/bark](https://github.com/suno-ai/bark) by Suno AI team.
1. **[BART](https://huggingface.co/docs/transformers/model_doc/bart)** (рдлреЗрд╕рдмреБрдХ) рд╕рд╛рде рдереАрд╕рд┐рд╕ [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) рдкрд░ рдирд┐рд░реНрднрд░ рдорд╛рдЗрдХ рд▓реБрдИрд╕, рдпрд┐рдирд╣рд╛рди рд▓рд┐рдпреВ, рдирдорди рдЧреЛрдпрд▓, рдорд╛рд░реНрдЬрди рдЧрд╝рдЬрд╝рд╡рд┐рдирд┐рдиреЗрдЬрд╛рдж, рдЕрдмреНрджреЗрд▓рд░рд╣рдорд╛рди рдореЛрд╣рдореНрдордж, рдУрдорд░ рд▓реЗрд╡реА, рд╡реЗрд╕ рд╕реНрдЯреЛрдпрд╛рдиреЛрд╡ рдФрд░ рд▓реНрдпреВрдХ рдЬрд╝реЗрдЯрд▓рдореЙрдпрд░
1. **[BARThez](https://huggingface.co/docs/transformers/model_doc/barthez)** (рд╕реЗ ├Йcole polytechnique) рд╕рд╛рде рдереАрд╕рд┐рд╕ [BARThez: a Skilled Pretrained French Sequence-to-Sequence Model](https://arxiv.org/abs/2010.12321) рдкрд░ рдирд┐рд░реНрднрд░ Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis рд░рд┐рд╣рд╛рдИред
1. **[BARTpho](https://huggingface.co/docs/transformers/model_doc/bartpho)** (VinAI Research рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese](https://arxiv.org/abs/2109.09701)рдЧреБрдпреЗрди рд▓реБрдУрдВрдЧ рдЯреНрд░рд╛рди, рдбреБрдУрдВрдЧ рдорд┐рдиреНрд╣ рд▓реЗ рдФрд░ рдбрд╛рдЯ рдХреНрд╡реЛрдХ рдЧреБрдпреЗрди рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[BEiT](https://huggingface.co/docs/transformers/model_doc/beit)** (Microsoft рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдХрд╛рдЧрдЬ [BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254) Hangbo Bao, Li Dong, Furu Wei рджреНрд╡рд╛рд░рд╛ред
1. **[BERT](https://huggingface.co/docs/transformers/model_doc/bert)** (рдЧреВрдЧрд▓ рд╕реЗ) рд╕рд╛рде рд╡рд╛рд▓рд╛ рдкреЗрдкрд░ [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) рдЬреИрдХрдм рдбреЗрд╡рд▓рд┐рди, рдорд┐рдВрдЧ-рд╡реЗрдИ рдЪрд╛рдВрдЧ, рдХреЗрдВрдЯрди рд▓реА рдФрд░ рдХреНрд░рд┐рд╕реНрдЯреАрдирд╛ рдЯреМрдЯрд╛рдиреЛрд╡рд╛ рджреНрд╡рд╛рд░рд╛ рдкреНрд░рдХрд╛рд╢рд┐рдд рдХрд┐рдпрд╛ рдЧрдпрд╛ рдерд╛ред .
1. **[BERT For Sequence Generation](https://huggingface.co/docs/transformers/model_doc/bert-generation)** (рдЧреВрдЧрд▓ рд╕реЗ) рд╕рд╛рде рджреЗрдиреЗ рд╡рд╛рд▓рд╛ рдкреЗрдкрд░ [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) рд╕рд╛рд╢рд╛ рд░реЛрдареЗ, рд╢рд╢рд┐ рдирд╛рд░рд╛рдпрдг, рдЕрд▓рд┐рдпрд╛рдХреНрд╕рд┐ рд╕реЗрд╡реЗрд░рд┐рди рджреНрд╡рд╛рд░рд╛ред
1. **[BERTweet](https://huggingface.co/docs/transformers/model_doc/bertweet)** (VinAI Research рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [BERTweet: A pre-trained language model for English Tweets](https://aclanthology.org/2020.emnlp-demos.2/) рдбрд╛рдЯ рдХреНрд╡реЛрдХ рдЧреБрдпреЗрди, рдерд╛рди рд╡реБ рдФрд░ рдЕрдиреНрд╣ рддреБрдЖрди рдЧреБрдпреЗрди рджреНрд╡рд╛рд░рд╛ рдкреНрд░рдХрд╛рд╢рд┐рддред
1. **[BigBird-Pegasus](https://huggingface.co/docs/transformers/model_doc/bigbird_pegasus)** (рдЧреВрдЧрд▓ рд░рд┐рд╕рд░реНрдЪ рд╕реЗ) рд╕рд╛рде рд╡рд╛рд▓рд╛ рдкреЗрдкрд░ [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) рдордВрдЬрд╝рд┐рд▓ рдЬрд╝рд╣реАрд░, рдЧреБрд░реБ рдЧреБрд░реБрдЧрдгреЗрд╢, рдЕрд╡рд┐рдирд╛рд╡рд╛ рджреБрдмреЗ, рдЬреЛрд╢реБрдЖ рдЖрдЗрдВрд╕реНрд▓реА, рдХреНрд░рд┐рд╕ рдЕрд▓реНрдмрд░реНрдЯреА, рд╕реИрдВрдЯрд┐рдпрд╛рдЧреЛ рдУрдВрдЯрд╛рдиреЛрди, рдлрд┐рд▓рд┐рдк рдлрд╛рдо, рдЕрдирд┐рд░реБрджреНрдз рд░рд╛рд╡реБрд▓рд╛, рдХрд┐рдлрд╝рд╛рди рд╡рд╛рдВрдЧ, рд▓реА рдпрд╛рдВрдЧ, рдЕрдорд░ рдЕрд╣рдордж рджреНрд╡рд╛рд░рд╛ред
1. **[BigBird-RoBERTa](https://huggingface.co/docs/transformers/model_doc/big_bird)** (рдЧреВрдЧрд▓ рд░рд┐рд╕рд░реНрдЪ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) рдордВрдЬрд╝рд┐рд▓ рдЬрд╝рд╣реАрд░, рдЧреБрд░реБ рдЧреБрд░реБрдЧрдгреЗрд╢, рдЕрд╡рд┐рдирд╛рд╡рд╛ рджреБрдмреЗ, рдЬреЛрд╢реБрдЖ рдЖрдЗрдВрд╕реНрд▓реА, рдХреНрд░рд┐рд╕ рдЕрд▓реНрдмрд░реНрдЯреА, рд╕реИрдВрдЯрд┐рдпрд╛рдЧреЛ рдУрдВрдЯрд╛рдирди, рдлрд┐рд▓рд┐рдк рдлрд╛рдо рджреНрд╡рд╛рд░рд╛ , рдЕрдирд┐рд░реБрджреНрдз рд░рд╛рд╡реБрд▓рд╛, рдХрд┐рдлрд╝рд╛рди рд╡рд╛рдВрдЧ, рд▓реА рдпрд╛рдВрдЧ, рдЕрдорд░ рдЕрд╣рдордж рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[BioGpt](https://huggingface.co/docs/transformers/model_doc/biogpt)** (from Microsoft Research AI4Science) released with the paper [BioGPT: generative pre-trained transformer for biomedical text generation and mining](https://academic.oup.com/bib/advance-article/doi/10.1093/bib/bbac409/6713511?guestAccessKey=a66d9b5d-4f83-4017-bb52-405815c907b9) by Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon and Tie-Yan Liu.
1. **[BiT](https://huggingface.co/docs/transformers/model_doc/bit)** (from Google AI) released with the paper [Big Transfer (BiT) by Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby.
1. **[Blenderbot](https://huggingface.co/docs/transformers/model_doc/blenderbot)** (рдлреЗрд╕рдмреБрдХ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдХрд╛рдЧрдЬ [Recipes for building an open-domain chatbot](https://arxiv.org/abs/2004.13637) рд╕реНрдЯреАрдлрди рд░реЛрд▓рд░, рдПрдорд┐рд▓реА рджреАрдирди, рдирдорди рдЧреЛрдпрд▓, рджрд╛ рдЬреВ, рдореИрд░реА рд╡рд┐рд▓рд┐рдпрдорд╕рди, рдпрд┐рдирд╣рд╛рди рд▓рд┐рдпреВ, рдЬрд┐рдВрдЧ рдЬреВ, рдорд╛рдпрд▓ рдУрдЯ, рдХрд░реНрдЯ рд╢рд╕реНрдЯрд░, рдПрд░рд┐рдХ рдПрдоред рд╕реНрдорд┐рде, рд╡рд╛рдИ-рд▓реИрди рдмреЙрд░реЛ, рдЬреЗрд╕рди рд╡реЗрд╕реНрдЯрди рджреНрд╡рд╛рд░рд╛ред
1. **[BlenderbotSmall](https://huggingface.co/docs/transformers/model_doc/blenderbot-small)** (рдлреЗрд╕рдмреБрдХ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [Recipes for building an open-domain chatbot](https://arxiv.org/abs/2004.13637) рд╕реНрдЯреАрдлрди рд░реЛрд▓рд░, рдПрдорд┐рд▓реА рджреАрдирди, рдирдорди рдЧреЛрдпрд▓, рджрд╛ рдЬреВ, рдореИрд░реА рд╡рд┐рд▓рд┐рдпрдорд╕рди, рдпрд┐рдирд╣рд╛рди рд▓рд┐рдпреВ, рдЬрд┐рдВрдЧ рдЬреВ, рдорд╛рдпрд▓ рдУрдЯ, рдХрд░реНрдЯ рд╢рд╕реНрдЯрд░, рдПрд░рд┐рдХ рдПрдо рд╕реНрдорд┐рде, рд╡рд╛рдИ-рд▓реИрди рдмреЙрд░реЛ, рдЬреЗрд╕рди рд╡реЗрд╕реНрдЯрди рджреНрд╡рд╛рд░рд╛ред
1. **[BLIP](https://huggingface.co/docs/transformers/model_doc/blip)** (from Salesforce) released with the paper [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086) by Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi.
1. **[BLIP-2](https://huggingface.co/docs/transformers/model_doc/blip-2)** (Salesforce рд╕реЗ) Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[BLOOM](https://huggingface.co/docs/transformers/model_doc/bloom)** (from BigScience workshop) released by the [BigScience Workshop](https://bigscience.huggingface.co/).
1. **[BORT](https://huggingface.co/docs/transformers/model_doc/bort)** (рдПрд▓реЗрдХреНрд╕рд╛ рд╕реЗ) рдХрд╛рдЧрдЬ рдХреЗ рд╕рд╛рде [Optimal Subarchitecture Extraction For BERT](https://arxiv.org/abs/2010.10499) рдПрдбреНрд░рд┐рдпрди рдбреА рд╡рд┐рдВрдЯрд░ рдФрд░ рдбреИрдирд┐рдпрд▓ рдЬреЗ рдкреЗрд░реА рджреНрд╡рд╛рд░рд╛ред
1. **[BridgeTower](https://huggingface.co/docs/transformers/model_doc/bridgetower)** (рд╣рд░рдмрд┐рди рдЗрдВрд╕реНрдЯрд┐рдЯреНрдпреВрдЯ рдСреЮ рдЯреЗрдХреНрдиреЛрд▓реЙрдЬреА/рдорд╛рдЗрдХреНрд░реЛрд╕реЙрдлреНрдЯ рд░рд┐рд╕рд░реНрдЪ рдПрд╢рд┐рдпрд╛/рдЗрдВрдЯреЗрд▓ рд▓реИрдмреНрд╕ рд╕реЗ) рдХрд╛рдЧрдЬ рдХреЗ рд╕рд╛рде [BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning](https://arxiv.org/abs/2206.08657) by Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan.
1. **[BROS](https://huggingface.co/docs/transformers/model_doc/bros)** (NAVER CLOVA рд╕реЗ) Teakgyu Hong, Donghyun Kim, Mingi Ji, Wonseok Hwang, Daehyun Nam, Sungrae Park. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents](https://arxiv.org/abs/2108.04539) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[ByT5](https://huggingface.co/docs/transformers/model_doc/byt5)** (Google рдЕрдиреБрд╕рдВрдзрд╛рди рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдХрд╛рдЧрдЬ [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) Linting Xue, Aditya Barua, Noah Constant, рд░рд╛рдореА рдЕрд▓-рд░рдлреВ, рд╢рд░рдг рдирд╛рд░рдВрдЧ, рдорд┐рд╣рд┐рд░ рдХрд╛рд▓реЗ, рдПрдбрдо рд░реЙрдмрд░реНрдЯреНрд╕, рдХреЙрд▓рд┐рди рд░реИрдлреЗрд▓ рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[CamemBERT](https://huggingface.co/docs/transformers/model_doc/camembert)** (рдЗрдирд░рд┐рдпрд╛/рдлреЗрд╕рдмреБрдХ/рд╕реЛрд░рдмреЛрди рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдХрд╛рдЧрдЬ [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) рд▓реБрдИ рдорд╛рд░реНрдЯрд┐рди*, рдмреЗрдВрдЬрд╛рдорд┐рди рдореБрд▓рд░*, рдкреЗрдбреНрд░реЛ рдЬреЗрд╡рд┐рдпрд░ рдСрд░реНрдЯрд┐рдЬрд╝ рд╕реБрдЖрд░реЗрдЬрд╝*, рдпреЛрдЖрди рдбреНрдпреВрдкреЙрдиреНрдЯ, рд▓реЙрд░реЗрдВрдЯ рд░реЛрдорд░реА, рдПрд░рд┐рдХ рд╡рд┐рд▓реЗрдореЛрдиреНрдЯреЗ рдбреЗ рд▓рд╛ рдХреНрд▓рд░реНрдЬрд░реА, рдЬреИрдореЗ рд╕реЗрдбрд╛рд╣ рдФрд░ рдмреЗрдиреЛрдЗрдЯ рд╕рдЧреЛрдЯ рджреНрд╡рд╛рд░рд╛ред
1. **[CANINE](https://huggingface.co/docs/transformers/model_doc/canine)** (Google рд░рд┐рд╕рд░реНрдЪ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рджрд┐рдпрд╛ рдЧрдпрд╛ рдкреЗрдкрд░ [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://arxiv.org/abs/2103.06874) рдЬреЛрдирд╛рдерди рдПрдЪ рдХреНрд▓рд╛рд░реНрдХ, рдбреИрди рдЧреИрд░реЗрдЯ, рдпреВрд▓рд┐рдпрд╛ рдЯрд░реНрдХ, рдЬреЙрди рд╡рд┐рдПрдЯрд┐рдВрдЧ рджреНрд╡рд╛рд░рд╛ред
1. **[Chinese-CLIP](https://huggingface.co/docs/transformers/model_doc/chinese_clip)** (from OFA-Sys) released with the paper [Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese](https://arxiv.org/abs/2211.01335) by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou.
1. **[CLAP](https://huggingface.co/docs/transformers/model_doc/clap)** (LAION-AI рд╕реЗ) Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, Shlomo Dubnov. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation](https://arxiv.org/abs/2211.06687) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[CLIP](https://huggingface.co/docs/transformers/model_doc/clip)** (OpenAI рд╕реЗ) рд╕рд╛рде рд╡рд╛рд▓рд╛ рдкреЗрдкрд░ [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) рдПрд▓реЗрдХ рд░реИрдбрдлреЛрд░реНрдб, рдЬреЛрдВрдЧ рд╡реВрдХ рдХрд┐рдо, рдХреНрд░рд┐рд╕ рд╣реИрд▓рд╛рд╕реА, рдЖрджрд┐рддреНрдп рд░рдореЗрд╢, рдЧреЗрдмреНрд░рд┐рдпрд▓ рдЧреЛрд╣, рд╕рдВрдзреНрдпрд╛ рдЕрдЧреНрд░рд╡рд╛рд▓, рдЧрд┐рд░реАрд╢ рд╢рд╛рд╕реНрддреНрд░реА, рдЕрдорд╛рдВрдбрд╛ рдПрд╕реНрдХреЗрд▓, рдкрд╛рдореЗрд▓рд╛ рдорд┐рд╢реНрдХрд┐рди, рдЬреИрдХ рдХреНрд▓рд╛рд░реНрдХ, рдЧреНрд░реЗрдЪреЗрди рдХреНрд░реБрдПрдЧрд░, рдЗрд▓реНрдпрд╛ рд╕реБрддреНрд╕реНрдХреЗрд╡рд░ рджреНрд╡рд╛рд░рд╛ред
1. **[CLIPSeg](https://huggingface.co/docs/transformers/model_doc/clipseg)** (from University of G├╢ttingen) released with the paper [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Timo L├╝ddecke and Alexander Ecker.
1. **[CLVP](https://huggingface.co/docs/transformers/model_doc/clvp)** released with the paper [Better speech synthesis through scaling](https://arxiv.org/abs/2305.07243) by James Betker.
1. **[CodeGen](https://huggingface.co/docs/transformers/model_doc/codegen)** (рд╕реЗрд▓реНрд╕рдлреЛрд░реНрд╕ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [A Conversational Paradigm for Program Synthesis](https://arxiv.org/abs/2203.13474) рдПрд░рд┐рдХ рдирд┐рдЬрдХреИрдВрдк, рдмреЛ рдкреИрдВрдЧ, рд╣рд┐рд░реЛрдЖрдХреА рд╣рдпрд╛рд╢реА, рд▓рд┐рдлреВ рддреВ, рд╣реБрдЖрди рд╡рд╛рдВрдЧ, рдпрд┐рдВрдЧрдмреЛ рдЭреЛрдЙ, рд╕рд┐рд▓реНрд╡рд┐рдпреЛ рд╕рд╛рд╡рд░реЗрд╕, рдХреИрдорд┐рдВрдЧ рдЬрд┐рдУрдВрдЧ рд░рд┐рд▓реАрдЬред
1. **[CodeLlama](https://huggingface.co/docs/transformers/model_doc/llama_code)** (MetaAI рд╕реЗ) Baptiste Rozi├иre, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J├йr├йmy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D├йfossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, Gabriel Synnaeve. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [Code Llama: Open Foundation Models for Code](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[Cohere](https://huggingface.co/docs/transformers/model_doc/cohere)** (Cohere рд╕реЗ) Cohere.  рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [Command-R: Retrieval Augmented Generation at Production Scale](<https://txt.cohere.com/command-r/>) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[Conditional DETR](https://huggingface.co/docs/transformers/model_doc/conditional_detr)** (рдорд╛рдЗрдХреНрд░реЛрд╕реЙрдлреНрдЯ рд░рд┐рд╕рд░реНрдЪ рдПрд╢рд┐рдпрд╛ рд╕реЗ) рдХрд╛рдЧрдЬ рдХреЗ рд╕рд╛рде [Conditional DETR for Fast Training Convergence](https://arxiv.org/abs/2108.06152) рдбреЗрдкреВ рдореЗрдВрдЧ, рдЬрд╝рд┐рдпрд╛рдУрдХрд╛рдВрдЧ рдЪреЗрди, рдЬрд╝реЗрдЬрд┐рдпрд╛ рдлреИрди, рдЧреИрдВрдЧ рдЬрд╝реЗрдВрдЧ, рд╣реЛрдЙрдХрд┐рдпрд╛рдВрдЧ рд▓реА, рдпреБрд╣реБрдИ рдпреБрдЖрди, рд▓реЗрдИ рд╕рди, рдЬрд┐рдВрдЧрдбреЛрдВрдЧ рд╡рд╛рдВрдЧ рджреНрд╡рд╛рд░рд╛ред
1. **[ConvBERT](https://huggingface.co/docs/transformers/model_doc/convbert)** (YituTech рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдХрд╛рдЧрдЬ [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496) рдЬрд┐рд╣рд╛рдВрдЧ рдЬрд┐рдпрд╛рдВрдЧ, рд╡реАрд╣рд╛рдУ рдпреВ, рдбрд╛рдХрд╛рди рдЭреЛрдЙ, рдпреБрдирдкреЗрдВрдЧ рдЪреЗрди, рдЬрд┐рдпрд╛рд╢реА рдлреЗрдВрдЧ, рд╢реБрдЗрдЪреЗрдВрдЧ рдпрд╛рди рджреНрд╡рд╛рд░рд╛ред
1. **[ConvNeXT](https://huggingface.co/docs/transformers/model_doc/convnext)** (Facebook AI рд╕реЗ) рд╕рд╛рде рд╡рд╛рд▓рд╛ рдкреЗрдкрд░ [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545) рдЬрд╝реБрдЖрдВрдЧ рд▓рд┐рдпреВ, рд╣реЗрдВрдЬрд╝реА рдорд╛рдУ, рдЪрд╛рдУ-рдпреБрдЖрди рд╡реВ, рдХреНрд░рд┐рд╕реНрдЯреЛрдлрд╝ рдлреАрдЪрдЯреЗрдирд╣реЛрдлрд╝рд░, рдЯреНрд░реЗрд╡рд░ рдбреЗрд░реЗрд▓, рд╕реИрдирд┐рдВрдЧ рдЬрд╝реА рджреНрд╡рд╛рд░рд╛ред
1. **[ConvNeXTV2](https://huggingface.co/docs/transformers/model_doc/convnextv2)** (from Facebook AI) released with the paper [ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders](https://arxiv.org/abs/2301.00808) by Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie.
1. **[CPM](https://huggingface.co/docs/transformers/model_doc/cpm)** (рд╕рд┐рдВрдШреБрдЖ рдпреВрдирд┐рд╡рд░реНрд╕рд┐рдЯреА рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [CPM: A Large-scale Generative Chinese Pre-trained Language Model](https://arxiv.org/abs/2012.00413) рдЭреЗрдВрдЧреНрдпрд╛рди рдЭрд╛рдВрдЧ, рдЬреВ рд╣рд╛рди, рд╣рд╛рдУ рдЭреЛрдЙ, рдкреЗрдИ рдХреЗ, рдпреБрдХреНрд╕рд┐рдпрди рдЧреБ, рдбреЗрдорд┐рдВрдЧ рдпреЗ, рдпреБрдЬрд┐рдпрд╛ рдХрд┐рди, рдпреБрд╢реЗрдВрдЧ рд╕реБ, рд╣рд╛рдУрдЭреЗ рдЬреА, рдЬрд┐рдпрд╛рди рдЧреБрдЖрди, рдлреИрдВрдЪрд╛рдУ рдХреНрдпреВрдИ, рдЬрд╝рд┐рдпрд╛рдУрдЭреА рд╡рд╛рдВрдЧ, рдпрд╛рдирд╛рди рдЭреЗрдВрдЧ рджреНрд╡рд╛рд░рд╛ , рдЧреБрдУрдпрд╛рдВрдЧ рдЬрд╝реЗрдВрдЧ, рд╣реБрдЖрдирдХреА рдХрд╛рдУ, рд╢реЗрдВрдЧрдХреА рдЪреЗрди, рдбрд╛рдЗрдХреНрд╕реБрдЖрди рд▓реА, рдЬрд╝реЗрдирдмреЛ рд╕рди, рдЬрд╝рд┐рдпреБрдЖрди рд▓рд┐рдпреВ, рдорд┐рдирд▓реА рд╣реБрдЖрдВрдЧ, рд╡реЗрдВрдЯрд╛рдУ рд╣рд╛рди, рдЬреА рддрд╛рдВрдЧ, рдЬреБрдЖрдирдЬрд╝реА рд▓реА, рдЬрд╝рд┐рдпрд╛рдУрдпрд╛рди рдЭреВ, рдорд╛рдУрд╕реЛрдВрдЧ рд╕рдиред
1. **[CPM-Ant](https://huggingface.co/docs/transformers/model_doc/cpmant)** (from OpenBMB) released by the [OpenBMB](https://www.openbmb.org/).
1. **[CTRL](https://huggingface.co/docs/transformers/model_doc/ctrl)** (рд╕реЗрд▓реНрд╕рдлреЛрд░реНрд╕ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://arxiv.org/abs/1909.05858) рдиреАрддреАрд╢ рд╢рд┐рд░реАрд╖ рдХреЗрд╕рдХрд░*, рдмреНрд░рд╛рдпрди рдореИрдХрдХреИрди*, рд▓рд╡ рдЖрд░. рд╡рд╛рд░реНрд╖реНрдгреЗрдп, рдХреИрдорд┐рдВрдЧ рдЬрд┐рдУрдВрдЧ рдФрд░ рд░рд┐рдЪрд░реНрдб рджреНрд╡рд╛рд░рд╛ рд╕реЛрдЪрд░ рджреНрд╡рд╛рд░рд╛ рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[CvT](https://huggingface.co/docs/transformers/model_doc/cvt)** (Microsoft рд╕реЗ) рд╕рд╛рде рдореЗрдВ рджрд┐рдпрд╛ рдЧрдпрд╛ рдкреЗрдкрд░ [CvT: Introducing Convolutions to Vision Transformers](https://arxiv.org/abs/2103.15808) рд╣реИрдкрд┐рдВрдЧ рд╡реВ, рдмрд┐рди рдЬрд┐рдУ, рдиреЛрдПрд▓ рдХреЛрдбреЗрд▓рд╛, рдореЗрдВрдЧрдЪреЗрди рд▓рд┐рдпреВ, рдЬрд┐рдпрд╛рдВрдЧ рджрд╛рдИ, рд▓реВ рдпреБрдЖрди, рд▓реЗрдИ рдЭрд╛рдВрдЧ рджреНрд╡рд╛рд░рд╛ред
1. **[Data2Vec](https://huggingface.co/docs/transformers/model_doc/data2vec)** (рдлреЗрд╕рдмреБрдХ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдХрд╛рдЧрдЬ [Data2Vec:  A General Framework for Self-supervised Learning in Speech, Vision and Language](https://arxiv.org/abs/2202.03555) рдПрд▓реЗрдХреНрд╕реА рдмрд╛рдПрд╡реНрд╕реНрдХреА, рд╡реЗрдИ-рдирд┐рдВрдЧ рд╕реВ, рдХрд┐рдпрд╛рдирдЯреЛрдВрдЧ рдЬреВ, рдЕрд░реБрдг рдмрд╛рдмреВ, рдЬрд┐рдпрд╛рддрд╛рдУ рдЧреБ, рдорд╛рдЗрдХрд▓ рдФрд▓реА рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[DBRX](https://huggingface.co/docs/transformers/model_doc/dbrx)** (from Databricks) released with the paper [Introducing DBRX: A New State-of-the-Art Open LLM](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm) by the Mosaic Research Team.
1. **[DeBERTa](https://huggingface.co/docs/transformers/model_doc/deberta)** (Microsoft рд╕реЗ) рд╕рд╛рде рдореЗрдВ рджрд┐рдпрд╛ рдЧрдпрд╛ рдкреЗрдкрд░ [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) рдкреЗрдВрдЧрдЪреЗрдВрдЧ рд╣реЗ, рдЬрд╝рд┐рдпрд╛рдУрдбреЛрдВрдЧ рд▓рд┐рдпреВ, рдЬрд┐рдпрд╛рдирдлреЗрдВрдЧ рдЧрд╛рдУ, рд╡реАрдЬрд╝реВ рдЪреЗрди рджреНрд╡рд╛рд░рд╛ред
1. **[DeBERTa-v2](https://huggingface.co/docs/transformers/model_doc/deberta-v2)** (Microsoft рд╕реЗ) рд╕рд╛рде рдореЗрдВ рджрд┐рдпрд╛ рдЧрдпрд╛ рдкреЗрдкрд░ [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) рдкреЗрдВрдЧрдЪреЗрдВрдЧ рд╣реЗ, рдЬрд╝рд┐рдпрд╛рдУрдбреЛрдВрдЧ рд▓рд┐рдпреВ, рдЬрд┐рдпрд╛рдирдлреЗрдВрдЧ рдЧрд╛рдУ, рд╡реАрдЬрд╝реВ рдЪреЗрди рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[Decision Transformer](https://huggingface.co/docs/transformers/model_doc/decision_transformer)** (рдмрд░реНрдХрд▓реЗ/рдлреЗрд╕рдмреБрдХ/рдЧреВрдЧрд▓ рд╕реЗ) рдкреЗрдкрд░ рдХреЗ рд╕рд╛рде [Decision Transformer: Reinforcement Learning via Sequence Modeling](https://arxiv.org/abs/2106.01345) рд▓рд┐рд▓реА рдЪреЗрди, рдХреЗрд╡рд┐рди рд▓реВ, рдЕрд░рд╡рд┐рдВрдж рд░рд╛рдЬреЗрд╢реНрд╡рд░рди, рдХрд┐рдорд┐рди рд▓реА, рдЖрджрд┐рддреНрдп рдЧреНрд░реЛрд╡рд░, рдорд╛рдЗрдХрд▓ рд▓рд╛рд╕реНрдХрд┐рди, рдкреАрдЯрд░ рдПрдмреАрд▓, рдЕрд░рд╡рд┐рдВрдж рд╢реНрд░реАрдирд┐рд╡рд╛рд╕, рдЗрдЧреЛрд░ рдореЛрд░реНрдбрдЪ рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[Deformable DETR](https://huggingface.co/docs/transformers/model_doc/deformable_detr)** (рд╕реЗрдВрд╕рдЯрд╛рдЗрдо рд░рд┐рд╕рд░реНрдЪ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [Deformable DETR: Deformable Transformers for End-to-End Object Detection](https://arxiv.org/abs/2010.04159) Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, рдЬрд┐рдлреЗрдВрдЧ рджрд╛рдИ рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[DeiT](https://huggingface.co/docs/transformers/model_doc/deit)** (рдлреЗрд╕рдмреБрдХ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877) рд╣реНрдпреВрдЧреЛ рдЯреМрд╡реНрд░реЛрди, рдореИрдереНрдпреВ рдХреЙрд░реНрдб, рдореИрдерд┐рдЬреНрд╕ рдбреВрдЬрд╝, рдлрд╝реНрд░рд╛рдВрд╕рд┐рд╕реНрдХреЛ рдорд╕реНрд╕рд╛, рдПрд▓реЗрдХреНрдЬрд╝реЗрдВрдбрд░ рд╕рдмрд▓реЗрд░реЛрд▓реНрд╕, рд╣рд░реНрд╡реЗ рдЬреЗрдЧреМ рджреНрд╡рд╛рд░рд╛ред
1. **[DePlot](https://huggingface.co/docs/transformers/model_doc/deplot)** (Google AI рд╕реЗ) Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, Yasemin Altun. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [DePlot: One-shot visual language reasoning by plot-to-table translation](https://arxiv.org/abs/2212.10505) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[Depth Anything](https://huggingface.co/docs/transformers/model_doc/depth_anything)** (University of Hong Kong and TikTok рд╕реЗ) Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, Hengshuang Zhao. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data](https://arxiv.org/abs/2401.10891) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[DETA](https://huggingface.co/docs/transformers/model_doc/deta)** (from The University of Texas at Austin) released with the paper [NMS Strikes Back](https://arxiv.org/abs/2212.06137) by Jeffrey Ouyang-Zhang, Jang Hyun Cho, Xingyi Zhou, Philipp Kr├дhenb├╝hl.
1. **[DETR](https://huggingface.co/docs/transformers/model_doc/detr)** (рдлреЗрд╕рдмреБрдХ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдХрд╛рдЧрдЬ [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) рдирд┐рдХреЛрд▓рд╕ рдХреИрд░рд┐рдпрди, рдлрд╝реНрд░рд╛рдВрд╕рд┐рд╕реНрдХреЛ рдорд╕реНрд╕рд╛, рдЧреЗрдмреНрд░рд┐рдпрд▓ рд╕рд┐рдиреЗрд╡, рдирд┐рдХреЛрд▓рд╕ рдЙрд╕реБрдирд┐рдпрд░, рдЕрд▓реЗрдХреНрдЬреЗрдВрдбрд░ рдХрд┐рд░рд┐рд▓реЛрд╡, рд╕рд░реНрдЧреЗрдИ рдЬрд╝рд╛рдЧреЛрд░реБрдпрдХреЛ рджреНрд╡рд╛рд░рд╛ред
1. **[DialoGPT](https://huggingface.co/docs/transformers/model_doc/dialogpt)** (рдорд╛рдЗрдХреНрд░реЛрд╕реЙрдлреНрдЯ рд░рд┐рд╕рд░реНрдЪ рд╕реЗ) рдХрд╛рдЧрдЬ рдХреЗ рд╕рд╛рде [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://arxiv.org/abs/1911.00536) рдпрд┐рдЬрд╝реЗ рдЭрд╛рдВрдЧ, рд╕рд┐рдХреА рд╕рди, рдорд┐рд╢реЗрд▓ рдЧреИрд▓реА, рдпреЗрди-рдЪреБрди рдЪреЗрди, рдХреНрд░рд┐рд╕ рдмреНрд░реЛрдХреЗрдЯ, рдЬрд┐рдпрд╛рдВрдЧ рдЧрд╛рдУ, рдЬрд┐рдпрд╛рдирдлреЗрдВрдЧ рдЧрд╛рдУ, рдЬрд┐рдВрдЧрдЬрд┐рдВрдЧ рд▓рд┐рдпреВ, рдмрд┐рд▓ рдбреЛрд▓рди рджреНрд╡рд╛рд░рд╛ред
1. **[DiNAT](https://huggingface.co/docs/transformers/model_doc/dinat)** (from SHI Labs) released with the paper [Dilated Neighborhood Attention Transformer](https://arxiv.org/abs/2209.15001) by Ali Hassani and Humphrey Shi.
1. **[DINOv2](https://huggingface.co/docs/transformers/model_doc/dinov2)** (Meta AI рд╕реЗ) Maxime Oquab, Timoth├йe Darcet, Th├йo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv├й Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, Piotr Bojanowski. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)** (рд╣рдЧрд┐рдВрдЧрдлреЗрд╕ рд╕реЗ), рд╕рд╛рде рдореЗрдВ рдХрд╛рдЧрдЬ [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108) рд╡рд┐рдХреНрдЯрд░ рд╕рдирд╣, рд▓рд┐рд╕рд╛рдВрдбреНрд░реЗ рдбреЗрдмреНрдпреВ рдФрд░ рдереЙрдорд╕ рд╡реБрд▓реНрдл рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред рдпрд╣реА рддрд░реАрдХрд╛ GPT-2 рдХреЛ [DistilGPT2](https://github.com/huggingface/transformers/tree/main/examples/distillation), RoBERta рд╕реЗ [DistilRoBERta](https://github.com) рдкрд░ рдХрдВрдкреНрд░реЗрд╕ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рднреА рд▓рд╛рдЧреВ рдХрд┐рдпрд╛ рдЬрд╛рддрд╛ рд╣реИред / рд╣рдЧрд┐рдВрдЧрдлреЗрд╕/рдЯреНрд░рд╛рдВрд╕рдлреЙрд░реНрдорд░реНрд╕/рдЯреНрд░реА/рдореЗрди/рдЙрджрд╛рд╣рд░рдг/рдбрд┐рд╕реНрдЯрд┐рд▓реЗрд╢рди), рдмрд╣реБрднрд╛рд╖реА BERT рд╕реЗ [DistilmBERT](https://github.com/huggingface/transformers/tree/main/examples/distillation) рдФрд░ рдбрд┐рд╕реНрдЯрд┐рд▓рдмрд░реНрдЯ рдХрд╛ рдЬрд░реНрдорди рд╕рдВрд╕реНрдХрд░рдгред
1. **[DiT](https://huggingface.co/docs/transformers/model_doc/dit)** (рдорд╛рдЗрдХреНрд░реЛрд╕реЙрдлреНрдЯ рд░рд┐рд╕рд░реНрдЪ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [DiT: Self-supervised Pre-training for Document Image Transformer](https://arxiv.org/abs/2203.02378) рдЬреБрдирд▓реЙрдиреНрдЧ рд▓реА, рдпрд┐рд╣реЗрдВрдЧ рдЬреВ, рдЯреЗрдВрдЧрдЪрд╛рдУ рд▓рд╡, рд▓реЗрдИ рдХреБрдИ, рдЪрд╛ рдЭрд╛рдВрдЧ рджреНрд╡рд╛рд░рд╛ рдлреБрд░реБ рд╡реЗрдИ рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[Donut](https://huggingface.co/docs/transformers/model_doc/donut)** (NAVER рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдХрд╛рдЧрдЬ [OCR-free Document Understanding Transformer](https://arxiv.org/abs/2111.15664) рдЧреАрд╡реВрдХ рдХрд┐рдо, рдЯреАрдХрдЧреНрдпреВ рд╣реЛрдВрдЧ, рдореВрдирдмрд┐рди рдпрд┐рдо, рдЬрд┐рдпреЛрдВрдЧреНрдпреЛрди рдирд╛рдо, рдЬрд┐рдирдпреЙрдиреНрдЧ рдкрд╛рд░реНрдХ, рдЬрд┐рдирдпреЙрдиреНрдЧ рдпрд┐рдо, рд╡реЛрдирд╕реЗрдУрдХ рд╣реНрд╡рд╛рдВрдЧ, рд╕рд╛рдВрдЧрдбреВ рдпреВрдВ, рдбреЛрдВрдЧрдпреВрди рд╣рд╛рди, рд╕реЗрдЙрдВрдЧреНрдпреБрди рдкрд╛рд░реНрдХ рджреНрд╡рд╛рд░рд╛ред
1. **[DPR](https://huggingface.co/docs/transformers/model_doc/dpr)** (рдлреЗрд╕рдмреБрдХ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906) рд╡реНрд▓рд╛рджрд┐рдореАрд░ рдХрд░рдкреБрдЦрд┐рди, рдмрд░рд▓рд╛рд╕ рдУрдЬрд╝реБрдЬрд╝, рд╕реЗрд╡рди рдорд┐рди, рдкреИрдЯреНрд░рд┐рдХ рд▓реБрдИрд╕, рд▓реЗрдбреЗрд▓ рд╡реВ, рд╕рд░реНрдЧреЗрдИ рдПрдбреБрдиреЛрд╡, рдбреИрдирдХреА рдЪреЗрди, рдФрд░ рд╡реЗрди-рддрд╛рдК рдпрд┐рд╣ рджреНрд╡рд╛рд░рд╛ред
1. **[DPT](https://huggingface.co/docs/transformers/master/model_doc/dpt)** (рдЗрдВрдЯреЗрд▓ рд▓реИрдмреНрд╕ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдХрд╛рдЧрдЬ [Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413) рд░реЗрдиреЗ рд░реИрдирдлреНрдЯрд▓, рдПрд▓реЗрдХреНрд╕реА рдмреЛрдЪрдХреЛрд╡рд╕реНрдХреА, рд╡реНрд▓рд╛рджрд▓реЗрди рдХреЛрд▓реНрдЯрди рджреНрд╡рд╛рд░рд╛ред
1. **[EfficientFormer](https://huggingface.co/docs/transformers/model_doc/efficientformer)** (from Snap Research) released with the paper [EfficientFormer: Vision Transformers at MobileNetSpeed](https://arxiv.org/abs/2206.01191) by Yanyu Li, Geng Yuan, Yang Wen, Ju Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, Jian Ren.
1. **[EfficientNet](https://huggingface.co/docs/transformers/model_doc/efficientnet)** (from Google Brain) released with the paper [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946) by Mingxing Tan, Quoc V. Le.
1. **[ELECTRA](https://huggingface.co/docs/transformers/model_doc/electra)** (Google рд░рд┐рд╕рд░реНрдЪ/рд╕реНрдЯреИрдирдлреЛрд░реНрдб рдпреВрдирд┐рд╡рд░реНрд╕рд┐рдЯреА рд╕реЗ) рд╕рд╛рде рдореЗрдВ рджрд┐рдпрд╛ рдЧрдпрд╛ рдкреЗрдкрд░ [ELECTRA: Pre-training text encoders as discriminators rather than generators](https://arxiv.org/abs/2003.10555) рдХреЗрд╡рд┐рди рдХреНрд▓рд╛рд░реНрдХ, рдорд┐рдиреНрд╣-рдерд╛рдВрдЧ рд▓реБрдУрдВрдЧ, рдХреНрд╡реЛрдХ рд╡реА. рд▓реЗ, рдХреНрд░рд┐рд╕реНрдЯреЛрдлрд░ рдбреА. рдореИрдирд┐рдВрдЧ рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[EnCodec](https://huggingface.co/docs/transformers/model_doc/encodec)** (Meta AI рд╕реЗ) Alexandre D├йfossez, Jade Copet, Gabriel Synnaeve, Yossi Adi. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [High Fidelity Neural Audio Compression](https://arxiv.org/abs/2210.13438) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[EncoderDecoder](https://huggingface.co/docs/transformers/model_doc/encoder-decoder)** (Google рд░рд┐рд╕рд░реНрдЪ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рджрд┐рдпрд╛ рдЧрдпрд╛ рдкреЗрдкрд░ [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) рд╕рд╛рд╢рд╛ рд░реЛрдареЗ, рд╢рд╢рд┐ рдирд╛рд░рд╛рдпрдг, рдЕрд▓рд┐рдпрд╛рдХреНрд╕рд┐ рд╕реЗрд╡реЗрд░рд┐рди рджреНрд╡рд╛рд░рд╛ред
1. **[ERNIE](https://huggingface.co/docs/transformers/model_doc/ernie)**(Baidu рд╕реЗ) рд╕рд╛рде рджреЗрдиреЗ рд╡рд╛рд▓рд╛ рдкреЗрдкрд░ [ERNIE: Enhanced Representation through Knowledge Integration](https://arxiv.org/abs/1904.09223) рдпреВ рд╕рди, рд╢реБрдУрд╣реБрдЖрди рд╡рд╛рдВрдЧ, рдпреБрдХреБрди рд▓реА, рд╢рд┐рдХреБрди рдлреЗрдВрдЧ, рдЬрд╝реБрдИ рдЪреЗрди, рд╣рд╛рди рдЭрд╛рдВрдЧ, рд╢рд┐рди рддрд┐рдпрд╛рди, рдбреИрдирдХреНрд╕рд┐рдпрд╛рдВрдЧ рдЭреВ, рд╣рд╛рдУ рддрд┐рдпрд╛рди, рд╣реБрдЖ рд╡реВ рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[ErnieM](https://huggingface.co/docs/transformers/model_doc/ernie_m)** (Baidu рд╕реЗ) Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora](https://arxiv.org/abs/2012.15674) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[ESM](https://huggingface.co/docs/transformers/model_doc/esm)** (рдореЗрдЯрд╛ AI рд╕реЗ) рдЯреНрд░рд╛рдВрд╕рдлреЙрд░реНрдорд░ рдкреНрд░реЛрдЯреАрди рднрд╛рд╖рд╛ рдореЙрдбрд▓ рд╣реИрдВред **ESM-1b** рдкреЗрдкрд░ рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛ рдерд╛ [Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences](https://www.pnas.org/content/118/15/e2016239118) рдЬреЗрд╕рди рд▓рд┐рдпреВ, рдбреЗрдореА рдЧреБрдУ, рдорд╛рдпрд▓ рдУрдЯ, рд╕реА. рд▓реЙрд░реЗрдВрд╕ рдЬрд╝рд┐рдЯрдирд┐рдХ, рдЬреЗрд░реА рдорд╛ рдФрд░ рд░реЙрдм рдлрд░реНрдЧрд╕ред **ESM-1v** рдХреЛ рдкреЗрдкрд░ рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛ рдерд╛ [рднрд╛рд╖рд╛ рдореЙрдбрд▓ рдкреНрд░реЛрдЯреАрди рдлрд╝рдВрдХреНрд╢рди рдкрд░ рдЙрддреНрдкрд░рд┐рд╡рд░реНрддрди рдХреЗ рдкреНрд░рднрд╛рд╡реЛрдВ рдХреА рд╢реВрдиреНрдп-рд╢реЙрдЯ рднрд╡рд┐рд╖реНрдпрд╡рд╛рдгреА рдХреЛ рд╕рдХреНрд╖рдо рдХрд░рддреЗ рд╣реИрдВ](https://doi.org/10.1101/2021.07.09.450648) рдЬреЛрд╢реБрдЖ рдореЗрдпрд░, рд░реЛрд╢рди рд░рд╛рд╡, рд░реЙрдмрд░реНрдЯ рд╡реЗрд░рдХреБрдЗрд▓, рдЬреЗрд╕рди рд▓рд┐рдпреВ, рдЯреЙрдо рд╕рд░реНрдХреБ рдФрд░ рдЕрд▓реЗрдХреНрдЬреЗрдВрдбрд░ рд░рд╛рдЗрд╡реНрд╕ рджреНрд╡рд╛рд░рд╛ред **ESM-2** рдХреЛ рдкреЗрдкрд░ рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛ рдерд╛ [рднрд╛рд╖рд╛ рдореЙрдбрд▓ рд╡рд┐рдХрд╛рд╕ рдХреЗ рдкреИрдорд╛рдиреЗ рдкрд░ рдкреНрд░реЛрдЯреАрди рдЕрдиреБрдХреНрд░рдо рд╕рдЯреАрдХ рд╕рдВрд░рдЪрдирд╛ рднрд╡рд┐рд╖реНрдпрд╡рд╛рдгреА рдХреЛ рд╕рдХреНрд╖рдо рдХрд░рддреЗ рд╣реИрдВ](https://doi.org/10.1101/2022.07.20.500902) рдЬрд╝реЗрдорд┐рдВрдЧ рд▓рд┐рди, рд╣рд▓реАрд▓ рдЕрдХрд┐рди, рд░реЛрд╢рди рд░рд╛рд╡, рдмреНрд░рд╛рдпрди рд╣реА, рдЭреЛрдВрдЧрдХрд╛рдИ рдЭреВ, рд╡реЗрдВрдЯрд┐рдВрдЧ рд▓реВ, рдП рджреНрд╡рд╛рд░рд╛ рд▓рд╛рди рдбреЙрд╕ рд╕реИрдВрдЯреЛрд╕ рдХреЛрд╕реНрдЯрд╛, рдорд░рд┐рдпрдо рдлрд╝рдЬрд╝рд▓-рдЬрд╝рд░рдВрдбреА, рдЯреЙрдо рд╕рд░реНрдХреВ, рд╕рд╛рд▓ рдХреИрдВрдбрд┐рдбреЛ, рдЕрд▓реЗрдХреНрдЬреЗрдВрдбрд░ рд░рд╛рдЗрд╡реНрд╕ред
1. **[Falcon](https://huggingface.co/docs/transformers/model_doc/falcon)** (from Technology Innovation Institute) by Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme.
1. **[FastSpeech2Conformer](https://huggingface.co/docs/transformers/model_doc/fastspeech2_conformer)** (ESPnet and Microsoft Research рд╕реЗ) Pengcheng Guo, Florian Boyer, Xuankai Chang, Tomoki Hayashi, Yosuke Higuchi, Hirofumi Inaguma, Naoyuki Kamo, Chenda Li, Daniel Garcia-Romero, Jiatong Shi, Jing Shi, Shinji Watanabe, Kun Wei, Wangyou Zhang, and Yuekai Zhang. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [Recent Developments On Espnet Toolkit Boosted By Conformer](https://arxiv.org/abs/2010.13956) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5)** (from Google AI) released in the repository [google-research/t5x](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints) by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei
1. **[FLAN-UL2](https://huggingface.co/docs/transformers/model_doc/flan-ul2)** (from Google AI) released in the repository [google-research/t5x](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-ul2-checkpoints) by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei
1. **[FlauBERT](https://huggingface.co/docs/transformers/model_doc/flaubert)** (CNRS рд╕реЗ) рд╕рд╛рде рд╡рд╛рд▓рд╛ рдкреЗрдкрд░ [FlauBERT: Unsupervised Language Model Pre-training for French](https://arxiv.org/abs/1912.05372) Hang Le, Lo├пc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, рдмреЗрдВрдЬрд╛рдорд┐рди рд▓реЗрдХреЛрдЙрдЯреЗрдХреНрд╕, рдЕрд▓реЗрдХреНрдЬреЗрдВрдбреНрд░реЗ рдЕрд▓реНрд▓рд╛рдЙрдЬрд╝реЗрди, рдмреЗрдиреЛрдЗрдЯ рдХреНрд░реИрдмреЗ, рд▓реЙрд░реЗрдВрдЯ рдмреЗрд╕реЗрд╕рд┐рдпрд░, рдбрд┐рдбрд┐рдПрд░ рд╢реНрд╡рд╛рдм рджреНрд╡рд╛рд░рд╛ред
1. **[FLAVA](https://huggingface.co/docs/transformers/model_doc/flava)** [FLAVA: A Foundational Language And Vision Alignment Model](https://arxiv.org/abs/2112.04482) рд╕рд╛рде рд╡рд╛рд▓рд╛ рдкреЗрдкрд░ рдЕрдордирдкреНрд░реАрдд рд╕рд┐рдВрд╣, рд░реЛрдВрдЧрд╣рд╛рдВрдЧ рд╣реВ, рд╡реЗрджрд╛рдиреБрдЬ рдЧреЛрд╕реНрд╡рд╛рдореА, рдЧреБрдЗрд▓реНрдпреВрдо рдХреБрдПрд░реЙрди, рд╡реЛрдЬреНрд╢рд┐рдПрдХ рдЧрд╛рд▓реБрдмрд╛, рдорд╛рд░реНрдХрд╕ рд░реЛрд╣рд░рдмреИрдХ, рдФрд░ рдбреМрд╡реЗ рдХреАрд▓рд╛ рджреНрд╡рд╛рд░рд╛ред
1. **[FNet](https://huggingface.co/docs/transformers/model_doc/fnet)** (рдЧреВрдЧрд▓ рд░рд┐рд╕рд░реНрдЪ рд╕реЗ) рд╕рд╛рде рд╡рд╛рд▓рд╛ рдкреЗрдкрд░ [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) рдЬреЗрдореНрд╕ рд▓реА-рдереЙрд░реНрдк, рдЬреЛрд╢реБрдЖ рдЖрдЗрдВрд╕реНрд▓реА, рдЗрд▓реНрдпрд╛ рдПрдХрд╕реНрдЯреАрди, рд╕реИрдВрдЯрд┐рдпрд╛рдЧреЛ рдУрдВрдЯрд╛рдирди рджреНрд╡рд╛рд░рд╛ред
1. **[FocalNet](https://huggingface.co/docs/transformers/model_doc/focalnet)** (Microsoft Research рд╕реЗ) Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [Focal Modulation Networks](https://arxiv.org/abs/2203.11926) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[Funnel Transformer](https://huggingface.co/docs/transformers/model_doc/funnel)** (рд╕реАрдПрдордпреВ/рдЧреВрдЧрд▓ рдмреНрд░реЗрди рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдХрд╛рдЧрдЬ [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https://arxiv.org/abs/2006.03236) рдЬрд┐рд╣рд╛рдВрдЧ рджрд╛рдИ, рдЧреБрдУрдХреБрди рд▓рд╛рдИ, рдпрд┐рдорд┐рдВрдЧ рдпрд╛рдВрдЧ, рдХреНрд╡реЛрдХ рд╡реА. рд▓реЗ рджреНрд╡рд╛рд░рд╛ рд░рд┐рд╣рд╛рдИред
1. **[Fuyu](https://huggingface.co/docs/transformers/model_doc/fuyu)** (ADEPT рд╕реЗ) рд░реЛрд╣рди рдмрд╛рд╡рд┐рд╢реА, рдПрд░рд┐рдЪ рдПрд▓рд╕реЗрди, рдХрд░реНрдЯрд┐рд╕ рд╣реЙрдереЛрд░реНрди, рдореИрдХреНрд╕рд╡реЗрд▓ рдиреА, рдСрдЧрд╕реНрдЯрд╕ рдУрдбреЗрдирд╛, рдЕрд░реБрд╢реА рд╕реЛрдорд╛рдиреА, рд╕рд╛рдЧрдирд╛рдХ рддрд╛рд╕рд┐рд░рд▓рд╛рд░  [blog post](https://www.adept.ai/blog/fuyu-8b)
1. **[Gemma](https://huggingface.co/docs/transformers/model_doc/gemma)** (Google рд╕реЗ) the Gemma Google team. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [Gemma: Open Models Based on Gemini Technology and Research](https://blog.google/technology/developers/gemma-open-models/) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[GIT](https://huggingface.co/docs/transformers/model_doc/git)** (from Microsoft Research) released with the paper [GIT: A Generative Image-to-text Transformer for Vision and Language](https://arxiv.org/abs/2205.14100) by Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, Lijuan Wang.
1. **[GLPN](https://huggingface.co/docs/transformers/model_doc/glpn)** (KAIST рд╕реЗ) рд╕рд╛рде рд╡рд╛рд▓рд╛ рдкреЗрдкрд░ [Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth](https://arxiv.org/abs/2201.07436) рдбреЛрдпреЛрди рдХрд┐рдо, рд╡реВрдВрдЧрд╣реНрдпреБрди рдЧрд╛, рдкреНрдпреБрдВрдЧрд╡рд╛рди рдЖрд╣, рдбреЛрдВрдЧрдЧреНрдпреВ рдЬреВ, рд╕реЗрд╣рд╡рд╛рди рдЪреБрди, рдЬреБрдирдореЛ рдХрд┐рдо рджреНрд╡рд╛рд░рд╛ред
1. **[GPT](https://huggingface.co/docs/transformers/model_doc/openai-gpt)** (OpenAI рд╕реЗ) рд╕рд╛рде рдореЗрдВ рджрд┐рдпрд╛ рдЧрдпрд╛ рдкреЗрдкрд░ [Improving Language Understanding by Generative Pre-Training](https://openai.com/research/language-unsupervised/) рдПрд▓реЗрдХ рд░реИрдбрдлреЛрд░реНрдб, рдХрд╛рд░реНрддрд┐рдХ рдирд░рд╕рд┐рдореНрд╣рди, рдЯрд┐рдо рд╕рд╛рд▓рд┐рдордиреНрд╕ рдФрд░ рдЗрд▓реНрдпрд╛ рд╕реБрддреНрд╕реНрдХреЗрд╡рд░ рджреНрд╡рд╛рд░рд╛ред
1. **[GPT Neo](https://huggingface.co/docs/transformers/model_doc/gpt_neo)** (EleutherAI рд╕реЗ) рд░рд┐рдкреЙрдЬрд┐рдЯрд░реА рдХреЗ рд╕рд╛рде [EleutherAI/gpt-neo](https://github.com/EleutherAI/gpt-neo) рд░рд┐рд▓реАрдЬред рд╕рд┐рдб рдмреНрд▓реИрдХ, рд╕реНрдЯреЗрд▓рд╛ рдмрд┐рдбрд░рдореИрди, рд▓рд┐рдпреЛ рдЧрд╛рдУ, рдлрд┐рд▓ рд╡рд╛рдВрдЧ рдФрд░ рдХреЙрдирд░ рд▓реЗрд╣реА рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[GPT NeoX](https://huggingface.co/docs/transformers/model_doc/gpt_neox)** (EleutherAI рд╕реЗ) рдкреЗрдкрд░ рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛ [GPT-NeoX-20B: An Open-Source Autoregressive Language Model](https://arxiv.org/abs/2204.06745) рд╕рд┐рдб рдмреНрд▓реИрдХ, рд╕реНрдЯреЗрд▓рд╛ рдмрд┐рдбрд░рдореИрди, рдПрд░рд┐рдХ рд╣реИрд▓рд╛рд╣рди, рдХреНрд╡реЗрдВрдЯрд┐рди рдПрдВрдереЛрдиреА, рд▓рд┐рдпреЛ рдЧрд╛рдУ, рд▓реЙрд░реЗрдВрд╕ рдЧреЛрд▓реНрдбрд┐рдВрдЧ, рд╣реЛрд░реЗрд╕ рд╣реЗ, рдХреЙрдирд░ рд▓реЗрд╣реА, рдХрд╛рдЗрд▓ рдореИрдХрдбреЛрдиреЗрд▓, рдЬреЗрд╕рди рдлрд╛рдВрдЧ, рдорд╛рдЗрдХрд▓ рдкрд╛рдЗрд▓рд░, рдпреВрдПрд╕рд╡реАрдПрд╕рдПрди рд╕рд╛рдИ рдкреНрд░рд╢рд╛рдВрдд рджреНрд╡рд╛рд░рд╛ , рд╢рд┐рд╡рд╛рдВрд╢реБ рдкреБрд░реЛрд╣рд┐рдд, рд▓рд╛рд░рд┐рдпрд╛ рд░реЗрдиреЙрд▓реНрдбреНрд╕, рдЬреЛрдирд╛рдерди рдЯреЛ, рдмреЗрди рд╡рд╛рдВрдЧ, рд╕реИрдореБрдЕрд▓ рд╡реЗрдирдмреИрдХ
1. **[GPT NeoX Japanese](https://huggingface.co/docs/transformers/model_doc/gpt_neox_japanese)** (рдЕрдмреЗрдЬрд╛ рдХреЗ рдЬрд░рд┐рдП) рд╢рд┐рдиреНрдпрд╛ рдУрдЯрд╛рдиреА, рддрд╛рдХрд╛рдпреЛрд╢реА рдордХрд╛рдмреЗ, рдЕрдиреБрдЬ рдЕрд░реЛрдбрд╝рд╛, рдХреНрдпреЛ рд╣рдЯреЛрд░реА рджреНрд╡рд╛рд░рд╛ред
1. **[GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2)** (рдУрдкрдирдПрдЖрдИ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [Language Models are Unsupervised Multitask Learners](https://openai.com/research/better-language-models/) рдПрд▓реЗрдХ рд░реИрдбрдлреЛрд░реНрдб, рдЬреЗрдлрд░реА рд╡реВ, рд░реЗрд╡рди рдЪрд╛рдЗрд▓реНрдб, рдбреЗрд╡рд┐рдб рд▓реБрдЖрди, рдбрд╛рд░рд┐рдпреЛ рдПрдореЛрдбреА рджреНрд╡рд╛рд░рд╛ рдФрд░ рдЗрд▓реНрдпрд╛ рд╕реБрддреНрд╕рдХреЗрд╡рд░ рдиреЗ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ред
1. **[GPT-J](https://huggingface.co/docs/transformers/model_doc/gptj)** (EleutherAI рд╕реЗ) рд╕рд╛рде рд╡рд╛рд▓рд╛ рдкреЗрдкрд░ [kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax/) рдмреЗрди рд╡рд╛рдВрдЧ рдФрд░ рдЕрд░рди рдХреЛрдорд╛рддреНрд╕реБрдЬрд╛рдХреА рджреНрд╡рд╛рд░рд╛ред
1. **[GPT-Sw3](https://huggingface.co/docs/transformers/model_doc/gpt-sw3)** (from AI-Sweden) released with the paper [Lessons Learned from GPT-SW3: Building the First Large-Scale Generative Language Model for Swedish](http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.376.pdf) by Ariel Ekgren, Amaru Cuba Gyllensten, Evangelia Gogoulou, Alice Heiman, Severine Verlinden, Joey ├Цhman, Fredrik Carlsson, Magnus Sahlgren.
1. **[GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode)** (BigCode рд╕реЗ) Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo Garc├нa del R├нo, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [SantaCoder: don't reach for the stars!](https://arxiv.org/abs/2301.03988) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[GPTSAN-japanese](https://huggingface.co/docs/transformers/model_doc/gptsan-japanese)** released in the repository [tanreinama/GPTSAN](https://github.com/tanreinama/GPTSAN/blob/main/report/model.md) by Toshiyuki Sakamoto(tanreinama).
1. **[Graphormer](https://huggingface.co/docs/transformers/model_doc/graphormer)** (from Microsoft) released with the paper [Do Transformers Really Perform Bad for Graph Representation?](https://arxiv.org/abs/2106.05234) by Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, Tie-Yan Liu.
1. **[Grounding DINO](https://huggingface.co/docs/transformers/model_doc/grounding-dino)** (Institute for AI, Tsinghua-Bosch Joint Center for ML, Tsinghua University, IDEA Research and others рд╕реЗ) Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection](https://arxiv.org/abs/2303.05499) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[GroupViT](https://huggingface.co/docs/transformers/model_doc/groupvit)** (UCSD, NVIDIA рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдХрд╛рдЧрдЬ [GroupViT: Semantic Segmentation Emerges from Text Supervision](https://arxiv.org/abs/2202.11094) рдЬрд┐рдпрд╛рд░реБрдИ рдЬреВ, рд╢рд╛рд▓рд┐рдиреА рдбреА рдореЗрд▓реЛ, рд╕рд┐рдлрд╝реА рд▓рд┐рдпреВ, рд╡реЛрдирдорд┐рди рдмрд╛рдпрди, рдереЙрдорд╕ рдмреНрд░реЗрдЙрдПрд▓, рдЬрд╛рди рдХреМрдЯреНрдЬрд╝, рдЬрд╝рд┐рдпрд╛рдУрд▓реЛрдВрдЧ рд╡рд╛рдВрдЧ рджреНрд╡рд╛рд░рд╛ред
1. **[HerBERT](https://huggingface.co/docs/transformers/model_doc/herbert)** (Allegro.pl, AGH University of Science and Technology рд╕реЗ) Piotr Rybak, Robert Mroczkowski, Janusz Tracz, Ireneusz Gawlik. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [KLEJ: Comprehensive Benchmark for Polish Language Understanding](https://www.aclweb.org/anthology/2020.acl-main.111.pdf) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[Hubert](https://huggingface.co/docs/transformers/model_doc/hubert)** (рдлреЗрд╕рдмреБрдХ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units](https://arxiv.org/abs/2106.07447) рд╡реЗрдИ-рдирд┐рдВрдЧ рд╕реВ, рдмреЗрдВрдЬрд╛рдорд┐рди рдмреЛрд▓реНрдЯреЗ, рдпрд╛рдУ-рд╣рдВрдЧ рд╣реНрдпреВрдмрд░реНрдЯ рддреНрд╕рд╛рдИ, рдХреБрд╢рд╛рд▓ рд▓рдЦреЛрдЯрд┐рдпрд╛, рд░реБрд╕реНрд▓рд╛рди рд╕рд╛рд▓рд╛рдЦреБрддрджреАрдиреЛрд╡, рдЕрдмреНрджреЗрд▓рд░рд╣рдорд╛рди рдореЛрд╣рдореНрдордж рджреНрд╡рд╛рд░рд╛ред
1. **[I-BERT](https://huggingface.co/docs/transformers/model_doc/ibert)** (рдмрд░реНрдХрд▓реЗ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдХрд╛рдЧрдЬ [I-BERT: Integer-only BERT Quantization](https://arxiv.org/abs/2101.01321) рд╕реЗрд╣реВрди рдХрд┐рдо, рдЕрдореАрд░ рдШреЛрд▓рдореА, рдЬрд╝реЗрд╡реЗрдИ рдпрд╛рдУ, рдорд╛рдЗрдХрд▓ рдбрдмреНрд▓реНрдпреВ рдорд╣реЛрдиреА, рдХрд░реНрдЯ рдХреЗрдЯрдЬрд╝рд░ рджреНрд╡рд╛рд░рд╛ред
1. **[IDEFICS](https://huggingface.co/docs/transformers/model_doc/idefics)** (from HuggingFace) released with the paper [OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents](https://huggingface.co/papers/2306.16527) by Hugo Lauren├зon, Lucile Saulnier, L├йo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, Victor Sanh.
1. **[Idefics2](https://huggingface.co/docs/transformers/model_doc/idefics2)** (Hugging Face рд╕реЗ) L├йo Tronchon, Hugo Laurencon, Victor Sanh. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [IDEFICS2](https://huggingface.co/blog/idefics2) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[ImageGPT](https://huggingface.co/docs/transformers/model_doc/imagegpt)** (from OpenAI) released with the paper [Generative Pretraining from Pixels](https://openai.com/blog/image-gpt/) by Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya Sutskever.
1. **[Informer](https://huggingface.co/docs/transformers/model_doc/informer)** (from Beihang University, UC Berkeley, Rutgers University, SEDD Company) released with the paper [Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting](https://arxiv.org/abs/2012.07436) by Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.
1. **[InstructBLIP](https://huggingface.co/docs/transformers/model_doc/instructblip)** (Salesforce рд╕реЗ) Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/abs/2305.06500) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[Jamba](https://huggingface.co/docs/transformers/model_doc/jamba)** (from AI21 Labs Ltd.) released with the paper [Jamba: A Hybrid Transformer-Mamba Language Model](https://arxiv.org/abs/2403.19887) by Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri Abend, Raz Alon, Tomer Asida, Amir Bergman, Roman Glozman, Michael Gokhman, Avshalom Manevich, Nir Ratner, Noam Rozen, Erez Shwartz, Mor Zusman, Yoav Shoham.
1. **[Jukebox](https://huggingface.co/docs/transformers/model_doc/jukebox)** (from OpenAI) released with the paper [Jukebox: A Generative Model for Music](https://arxiv.org/pdf/2005.00341.pdf) by Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever.
1. **[KOSMOS-2](https://huggingface.co/docs/transformers/model_doc/kosmos-2)** (from Microsoft Research Asia) released with the paper [Kosmos-2: Grounding Multimodal Large Language Models to the World](https://arxiv.org/abs/2306.14824) by Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei.
1. **[LayoutLM](https://huggingface.co/docs/transformers/model_doc/layoutlm)** (from Microsoft Research Asia) released with the paper [LayoutLM: Pre-training of Text and Layout for Document Image Understanding](https://arxiv.org/abs/1912.13318) by Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou.
1. **[LayoutLMv2](https://huggingface.co/docs/transformers/model_doc/layoutlmv2)** (from Microsoft Research Asia) released with the paper [LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://arxiv.org/abs/2012.14740) by Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou.
1. **[LayoutLMv3](https://huggingface.co/docs/transformers/model_doc/layoutlmv3)** (рдорд╛рдЗрдХреНрд░реЛрд╕реЙрдлреНрдЯ рд░рд┐рд╕рд░реНрдЪ рдПрд╢рд┐рдпрд╛ рд╕реЗ) рд╕рд╛рде рджреЗрдиреЗ рд╡рд╛рд▓рд╛ рдкреЗрдкрд░ [LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking](https://arxiv.org/abs/2204.08387) рдпреБрдкрди рд╣реБрдЖрдВрдЧ, рдЯреЗрдВрдЧрдЪрд╛рдУ рд▓рд╡, рд▓реЗрдИ рдХреБрдИ, рдпреБрдЯреЛрдВрдЧ рд▓реВ, рдлреБрд░реБ рд╡реЗрдИ рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[LayoutXLM](https://huggingface.co/docs/transformers/model_doc/layoutxlm)** (from Microsoft Research Asia) released with the paper [LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https://arxiv.org/abs/2104.08836) by Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Furu Wei.
1. **[LED](https://huggingface.co/docs/transformers/model_doc/led)** (from AllenAI) released with the paper [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan.
1. **[LeViT](https://huggingface.co/docs/transformers/model_doc/levit)** (рдореЗрдЯрд╛ AI рд╕реЗ) рд╕рд╛рде рд╡рд╛рд▓рд╛ рдкреЗрдкрд░ [LeViT: A Vision Transformer in ConvNet's Clothing for Faster Inference](https://arxiv.org/abs/2104.01136) рдмреЗрди рдЧреНрд░рд╛рд╣рдо, рдЕрд▓рд╛рдПрд▓реНрдбрд┐рди рдПрд▓-рдиреМрдмреА, рд╣реНрдпреВрдЧреЛ рдЯреМрд╡рд░рди, рдкрд┐рдпрд░реЗ рд╕реНрдЯреЙрдХ, рдЖрд░реНрдордВрдб рдЬреМрд▓рд┐рди, рд╣рд░реНрд╡реЗ рдЬреЗрдЧреМ, рдореИрдерд┐рдЬ рдбреВрдЬрд╝ рджреНрд╡рд╛рд░рд╛ред
1. **[LiLT](https://huggingface.co/docs/transformers/model_doc/lilt)** (рджрдХреНрд╖рд┐рдг рдЪреАрди рдкреНрд░реМрджреНрдпреЛрдЧрд┐рдХреА рд╡рд┐рд╢реНрд╡рд╡рд┐рджреНрдпрд╛рд▓рдп рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдХрд╛рдЧрдЬ [LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding](https://arxiv.org/abs/2202.13669) рдЬрд┐рдпрд╛рдкреЗрдВрдЧ рд╡рд╛рдВрдЧ, рд▓рд┐рдпрд╛рдирд╡реЗрди рдЬрд┐рди, рдХрд╛рдИ рдбрд┐рдВрдЧ рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[LLaMA](https://huggingface.co/docs/transformers/model_doc/llama)** (The FAIR team of Meta AI рд╕реЗ) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth├йe Lacroix, Baptiste Rozi├иre, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[Llama2](https://huggingface.co/docs/transformers/model_doc/llama2)** (The FAIR team of Meta AI рд╕реЗ) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushka rMishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing EllenTan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom.. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [Llama2: Open Foundation and Fine-Tuned Chat Models](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[LLaVa](https://huggingface.co/docs/transformers/model_doc/llava)** (Microsoft Research & University of Wisconsin-Madison рд╕реЗ) Haotian Liu, Chunyuan Li, Yuheng Li and Yong Jae Lee. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[LLaVA-NeXT](https://huggingface.co/docs/transformers/model_doc/llava_next)** (Microsoft Research & University of Wisconsin-Madison рд╕реЗ) Haotian Liu, Chunyuan Li, Yuheng Li and Yong Jae Lee. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [Improved Baselines with Visual Instruction Tuning](https://arxiv.org/abs/2310.03744) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[Longformer](https://huggingface.co/docs/transformers/model_doc/longformer)** (from AllenAI) released with the paper [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan.
1. **[LongT5](https://huggingface.co/docs/transformers/model_doc/longt5)** (рдореИрдВрдбреА рдЧреБрдУ, рдЬреЛрд╢реБрдЖ рдЖрдЗрдВрд╕реНрд▓реА, рдбреЗрд╡рд┐рдб рдпреВрдерд╕, рд╕реИрдВрдЯрд┐рдпрд╛рдЧреЛ рдУрдВрдЯрд╛рдирди, рдЬрд┐рдпрд╛рдирдореЛ рдирд┐, рдпреВрдВ-рд╣реБрдЖрди рд╕реБрдВрдЧ, рдпрд┐рдирдлреЗрдИ рдпрд╛рдВрдЧ рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[LUKE](https://huggingface.co/docs/transformers/model_doc/luke)** (рд╕реНрдЯреВрдбрд┐рдпреЛ рдФрд╕рд┐рдпрд╛ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention](https://arxiv.org/abs/2010.01057) Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto рджреНрд╡рд╛рд░рд╛ред
1. **[LXMERT](https://huggingface.co/docs/transformers/model_doc/lxmert)** (UNC рдЪреИрдкрд▓ рд╣рд┐рд▓ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [LXMERT: Learning Cross-Modality Encoder Representations from Transformers for Open-Domain Question Answering](https://arxiv.org/abs/1908.07490) рд╣рд╛рдУ рдЯреИрди рдФрд░ рдореЛрд╣рд┐рдд рдмрдВрд╕рд▓ рджреНрд╡рд╛рд░рд╛ред
1. **[M-CTC-T](https://huggingface.co/docs/transformers/model_doc/mctct)** (from Facebook) released with the paper [Pseudo-Labeling For Massively Multilingual Speech Recognition](https://arxiv.org/abs/2111.00161) by Loren Lugosch, Tatiana Likhomanenko, Gabriel Synnaeve, and Ronan Collobert.
1. **[M2M100](https://huggingface.co/docs/transformers/model_doc/m2m_100)** (рдлреЗрд╕рдмреБрдХ рд╕реЗ) рд╕рд╛рде рджреЗрдиреЗ рд╡рд╛рд▓рд╛ рдкреЗрдкрд░ [Beyond English-Centric Multilingual Machine Translation](https://arxiv.org/abs/2010.11125) рдПрдВрдЬреЗрд▓рд╛ рдлреИрди, рд╢реНрд░реБрддрд┐ рднреЛрд╕рд▓реЗ, рд╣реЛрд▓реНрдЧрд░ рд╢реНрд╡реЗрдиреНрдХ, рдЭреА рдорд╛, рдЕрд╣рдордж рдЕрд▓-рдХрд┐рд╢реНрдХреА, рд╕рд┐рджреНрдзрд╛рд░реНрде рдЧреЛрдпрд▓, рдордирджреАрдк рдмреИрдиреЗрд╕, рдУрдиреВрд░ рд╕реЗрд▓реЗрдмреА, рдЧреБрдЗрд▓реНрд▓рд╛рдо рд╡реЗрдиреНрдЬреЗрдХ, рд╡рд┐рд╢реНрд░рд╡ рдЪреМрдзрд░реА, рдирдорди рдЧреЛрдпрд▓, рдЯреЙрдо рдмрд░реНрдЪ, рд╡рд┐рдЯрд╛рд▓реА рд▓рд┐рдкрдЪрд┐рдВрд╕реНрдХреА, рд╕рд░реНрдЧреЗрдИ рдПрдбреБрдиреЛрд╡, рдПрдбреМрд░реНрдб рджреНрд╡рд╛рд░рд╛ рдЧреНрд░реЗрд╡, рдорд╛рдЗрдХрд▓ рдФрд▓реА, рдЖрд░реНрдордВрдб рдЬреМрд▓рд┐рди рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[MADLAD-400](https://huggingface.co/docs/transformers/model_doc/madlad-400)** (from Google) released with the paper [MADLAD-400: A Multilingual And Document-Level Large Audited Dataset](https://arxiv.org/abs/2309.04662) by Sneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, Christopher A. Choquette-Choo, Katherine Lee, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, Orhan Firat.
1. **[Mamba](https://huggingface.co/docs/transformers/model_doc/mamba)** (Albert Gu and Tri Dao рд╕реЗ) Albert Gu and Tri Dao. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[MarianMT](https://huggingface.co/docs/transformers/model_doc/marian)** J├╢rg рджреНрд╡рд╛рд░рд╛ [OPUS](http://opus.nlpl.eu/) рдбреЗрдЯрд╛ рд╕реЗ рдкреНрд░рд╢рд┐рдХреНрд╖рд┐рдд рдорд╢реАрдиреА рдЕрдиреБрд╡рд╛рдж рдореЙрдбрд▓ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ рдЯрд╛рдЗрдбреЗрдореИрди рджреНрд╡рд╛рд░рд╛ред [рдореИрд░рд┐рдпрди рдлреНрд░реЗрдорд╡рд░реНрдХ](https://marian-nmt.github.io/) рдорд╛рдЗрдХреНрд░реЛрд╕реЙрдлреНрдЯ рдЯреНрд░рд╛рдВрд╕рд▓реЗрдЯрд░ рдЯреАрдо рджреНрд╡рд╛рд░рд╛ рд╡рд┐рдХрд╕рд┐рддред
1. **[MarkupLM](https://huggingface.co/docs/transformers/model_doc/markuplm)** (рдорд╛рдЗрдХреНрд░реЛрд╕реЙрдлреНрдЯ рд░рд┐рд╕рд░реНрдЪ рдПрд╢рд┐рдпрд╛ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding](https://arxiv.org/abs/2110.08518) рдЬреБрдирд▓реЙрдиреНрдЧ рд▓реА, рдпрд┐рд╣реЗрдВрдЧ рдЬреВ, рд▓реЗрдИ рдХреБрдИ, рдлреБрд░реБ рджреНрд╡рд╛рд░рд╛ рд╡реА рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[Mask2Former](https://huggingface.co/docs/transformers/model_doc/mask2former)** (FAIR and UIUC рд╕реЗ) Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, Rohit Girdhar. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [Masked-attention Mask Transformer for Universal Image Segmentation](https://arxiv.org/abs/2112.01527) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[MaskFormer](https://huggingface.co/docs/transformers/model_doc/maskformer)** (рдореЗрдЯрд╛ рдФрд░ UIUC рд╕реЗ) рдкреЗрдкрд░ рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛ [Per-Pixel Classification is Not All You Need for Semantic Segmentation](https://arxiv.org/abs/2107.06278) рдмреЛрд╡реЗрди рдЪреЗрдВрдЧ, рдЕрд▓реЗрдХреНрдЬреЗрдВрдбрд░ рдЬреА. рд╢реНрд╡рд┐рдВрдЧ, рдЕрд▓реЗрдХреНрдЬреЗрдВрдбрд░ рдХрд┐рд░рд┐рд▓реЛрд╡ рджреНрд╡рд╛рд░рд╛ >>>>>> рд░рд┐рдмреЗрд╕ рдареАрдХ рдХрд░реЗрдВ
1. **[MatCha](https://huggingface.co/docs/transformers/model_doc/matcha)** (Google AI рд╕реЗ) Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Yasemin Altun, Nigel Collier, Julian Martin Eisenschlos. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering](https://arxiv.org/abs/2212.09662) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[mBART](https://huggingface.co/docs/transformers/model_doc/mbart)** (рдлреЗрд╕рдмреБрдХ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [Multilingual Denoising Pre-training for Neural Machine Translation](https://arxiv.org/abs/2001.08210) рдпрд┐рдирд╣рд╛рди рд▓рд┐рдпреВ, рдЬрд┐рдпрд╛рддрд╛рдУ рдЧреБ, рдирдорди рдЧреЛрдпрд▓, рдЬрд┐рдпрд╛рди рд▓реА, рд╕рд░реНрдЧреЗрдИ рдПрдбреБрдиреЛрд╡, рдорд╛рд░реНрдЬрди рдЧрд╝рдЬрд╝рд╡рд┐рдирд┐рдиреЗрдЬрд╛рдж, рдорд╛рдЗрдХ рд▓реБрдИрд╕, рд▓реНрдпреВрдХ рдЬрд╝реЗрдЯрд▓рдореЙрдпрд░ рджреНрд╡рд╛рд░рд╛ред
1. **[mBART-50](https://huggingface.co/docs/transformers/model_doc/mbart)** (рдлреЗрд╕рдмреБрдХ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https://arxiv.org/abs/2008.00401) рдпреБрдХрд┐рдВрдЧ рдЯреИрдВрдЧ, рдЪрд╛рдЙ рдЯреНрд░рд╛рди, рдЬрд┐рдпрд╛рди рд▓реА, рдкреЗрдВрдЧ-рдЬреЗрди рдЪреЗрди, рдирдорди рдЧреЛрдпрд▓, рд╡рд┐рд╢реНрд░рд╡ рдЪреМрдзрд░реА, рдЬрд┐рдпрд╛рддрд╛рдУ рдЧреБ, рдПрдВрдЬреЗрд▓рд╛ рдлреИрди рджреНрд╡рд╛рд░рд╛ред
1. **[MEGA](https://huggingface.co/docs/transformers/model_doc/mega)** (Facebook рд╕реЗ) Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [Mega: Moving Average Equipped Gated Attention](https://arxiv.org/abs/2209.10655) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[Megatron-BERT](https://huggingface.co/docs/transformers/model_doc/megatron-bert)** (NVIDIA рд╕реЗ) рдХрд╛рдЧрдЬ рдХреЗ рд╕рд╛рде [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) рдореЛрд╣рдореНрдордж рд╢реЛрдПрдмреА, рдореЛрд╕реНрдЯреЛрдлрд╛ рдкрдЯрд╡рд╛рд░реА, рд░рд╛рдЙрд▓ рдкреБрд░реА, рдкреИрдЯреНрд░рд┐рдХ рд▓реЗрдЧреНрд░реЗрд╕реНрд▓реЗ, рдЬреЗрд░реЗрдб рдХреИрд╕реНрдкрд░ рдФрд░ рдмреНрд░рд╛рдпрди рдХреИрдЯрд╛рдирдЬрд╝рд╛рд░реЛ рджреНрд╡рд╛рд░рд╛ред
1. **[Megatron-GPT2](https://huggingface.co/docs/transformers/model_doc/megatron_gpt2)** (NVIDIA рд╕реЗ) рд╕рд╛рде рд╡рд╛рд▓рд╛ рдкреЗрдкрд░ [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) рдореЛрд╣рдореНрдордж рд╢реЛрдПрдмреА, рдореЛрд╕реНрдЯреЛрдлрд╛ рдкрдЯрд╡рд╛рд░реА, рд░рд╛рдЙрд▓ рдкреБрд░реА, рдкреИрдЯреНрд░рд┐рдХ рд▓реЗрдЧреНрд░реЗрд╕реНрд▓реЗ, рдЬреЗрд░реЗрдб рдХреИрд╕реНрдкрд░ рдФрд░ рдмреНрд░рд╛рдпрди рдХреИрдЯрд╛рдирдЬрд╝рд╛рд░реЛ рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[MGP-STR](https://huggingface.co/docs/transformers/model_doc/mgp-str)** (Alibaba Research рд╕реЗ) Peng Wang, Cheng Da, and Cong Yao. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [Multi-Granularity Prediction for Scene Text Recognition](https://arxiv.org/abs/2209.03592) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[Mistral](https://huggingface.co/docs/transformers/model_doc/mistral)** (from Mistral AI) by The Mistral AI team: Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, L├йlio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth├йe Lacroix, William El Sayed..
1. **[Mixtral](https://huggingface.co/docs/transformers/model_doc/mixtral)** (from Mistral AI) by The [Mistral AI](https://mistral.ai) team: Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, L├йlio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth├йe Lacroix, William El Sayed.
1. **[mLUKE](https://huggingface.co/docs/transformers/model_doc/mluke)** (рдлреНрд░реЙрдо Studio Ousia) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models](https://arxiv.org/abs/2110.08151) рд░рдпреЛрдХрди рд░реА, рдЗрдХреБрдпрд╛ рдпрд╛рдорд╛рдбрд╛, рдФрд░ рдпреЛрд╢рд┐рдорд╛рд╕рд╛ рддреНрд╕реБрд░реЛрдХрд╛ рджреНрд╡рд╛рд░рд╛ред
1. **[MMS](https://huggingface.co/docs/transformers/model_doc/mms)** (Facebook рд╕реЗ) Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, Alexei Baevski, Yossi Adi, Xiaohui Zhang, Wei-Ning Hsu, Alexis Conneau, Michael Auli. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [Scaling Speech Technology to 1,000+ Languages](https://arxiv.org/abs/2305.13516) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[MobileBERT](https://huggingface.co/docs/transformers/model_doc/mobilebert)** (рд╕реАрдПрдордпреВ/рдЧреВрдЧрд▓ рдмреНрд░реЗрди рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдХрд╛рдЧрдЬ [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://arxiv.org/abs/2004.02984) Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, рдФрд░ Denny Zhou рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[MobileNetV1](https://huggingface.co/docs/transformers/model_doc/mobilenet_v1)** (from Google Inc.) released with the paper [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861) by Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam.
1. **[MobileNetV2](https://huggingface.co/docs/transformers/model_doc/mobilenet_v2)** (from Google Inc.) released with the paper [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381) by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen.
1. **[MobileViT](https://huggingface.co/docs/transformers/model_doc/mobilevit)** (Apple рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдХрд╛рдЧрдЬ [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178) рд╕рдЪрд┐рди рдореЗрд╣рддрд╛ рдФрд░ рдореЛрд╣рдореНрдордж рд░рд╕реНрддрдЧрд░реА рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[MobileViTV2](https://huggingface.co/docs/transformers/model_doc/mobilevitv2)** (Apple рд╕реЗ) Sachin Mehta and Mohammad Rastegari. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [Separable Self-attention for Mobile Vision Transformers](https://arxiv.org/abs/2206.02680) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[MPNet](https://huggingface.co/docs/transformers/model_doc/mpnet)** (from Microsoft Research) released with the paper [MPNet: Masked and Permuted Pre-training for Language Understanding](https://arxiv.org/abs/2004.09297) by Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu.
1. **[MPT](https://huggingface.co/docs/transformers/model_doc/mpt)** (MosaiML рд╕реЗ) the MosaicML NLP Team. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [llm-foundry](https://github.com/mosaicml/llm-foundry/) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[MRA](https://huggingface.co/docs/transformers/model_doc/mra)** (the University of Wisconsin - Madison рд╕реЗ) Zhanpeng Zeng, Sourav Pal, Jeffery Kline, Glenn M Fung, Vikas Singh. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [Multi Resolution Analysis (MRA) for Approximate Self-Attention](https://arxiv.org/abs/2207.10284) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[MT5](https://huggingface.co/docs/transformers/model_doc/mt5)** (Google AI рд╕реЗ) рд╕рд╛рде рд╡рд╛рд▓рд╛ рдкреЗрдкрд░ [mT5: A massively multilingual pre-trained text-to-text transformer](https://arxiv.org/abs/2010.11934) рд▓рд┐рдВрдЯрд┐рдВрдЧ рдЬрд╝реВ, рдиреЛрдЖ рдХреЙрдиреНрд╕рдЯреЗрдВрдЯ, рдПрдбрдо рд░реЙрдмрд░реНрдЯреНрд╕, рдорд┐рд╣рд┐рд░ рдХрд╛рд▓реЗ, рд░рд╛рдореА рдЕрд▓-рд░рдлреВ, рдЖрджрд┐рддреНрдп рд╕рд┐рджреНрдзрд╛рдВрдд, рдЖрджрд┐рддреНрдп рдмрд░реБрдЖ, рдХреЙрд▓рд┐рди рд░реИрдлреЗрд▓ рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[MusicGen](https://huggingface.co/docs/transformers/model_doc/musicgen)** (from Meta) released with the paper [Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284) by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi and Alexandre D├йfossez.
1. **[MusicGen Melody](https://huggingface.co/docs/transformers/model_doc/musicgen_melody)** (from Meta) released with the paper [Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284) by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi and Alexandre D├йfossez.
1. **[MVP](https://huggingface.co/docs/transformers/model_doc/mvp)** (from RUC AI Box) released with the paper [MVP: Multi-task Supervised Pre-training for Natural Language Generation](https://arxiv.org/abs/2206.12131) by Tianyi Tang, Junyi Li, Wayne Xin Zhao and Ji-Rong Wen.
1. **[NAT](https://huggingface.co/docs/transformers/model_doc/nat)** (from SHI Labs) released with the paper [Neighborhood Attention Transformer](https://arxiv.org/abs/2204.07143) by Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi.
1. **[Nezha](https://huggingface.co/docs/transformers/model_doc/nezha)** (рд╣реБрдЖрд╡реЗрдИ рдиреВрд╣ рдХреЗ рдЖрд░реНрдХ рд▓реИрдм рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдХрд╛рдЧрдЬрд╝ [NEZHA: Neural Contextualized Representation for Chinese Language Understanding](https://arxiv.org/abs/1909.00204) рдЬреБрдиреНрдХрд┐рдЙ рд╡реЗрдИ, рдЬрд╝рд┐рдпрд╛рдУрдЬрд╝реЗ рд░реЗрди, рдЬрд╝рд┐рдЖрдУрдЧреБрдЖрдВрдЧ рд▓реА, рд╡реЗрдирдпреЛрдВрдЧ рд╣реБрдЖрдВрдЧ, рдпреА рд▓рд┐рдпрд╛рдУ, рдпрд╛рд╢реЗрдВрдЧ рд╡рд╛рдВрдЧ, рдЬрд┐рдпрд╛рд╢реВ рд▓рд┐рди, рд╢рд┐рди рдЬрд┐рдпрд╛рдВрдЧ, рдЬрд┐рдУ рдЪреЗрди рдФрд░ рдХреБрди рд▓рд┐рдпреВ рджреНрд╡рд╛рд░рд╛ред
1. **[NLLB](https://huggingface.co/docs/transformers/model_doc/nllb)** (рдлреНрд░реЙрдо рдореЗрдЯрд╛) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [No Language Left Behind: Scaling Human-Centered Machine Translation](https://arxiv.org/abs/2207.04672) рдПрдирдПрд▓рдПрд▓рдмреА рдЯреАрдо рджреНрд╡рд╛рд░рд╛ рдкреНрд░рдХрд╛рд╢рд┐рддред
1. **[NLLB-MOE](https://huggingface.co/docs/transformers/model_doc/nllb-moe)** (Meta рд╕реЗ) the NLLB team. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [No Language Left Behind: Scaling Human-Centered Machine Translation](https://arxiv.org/abs/2207.04672) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[Nougat](https://huggingface.co/docs/transformers/model_doc/nougat)** (Meta AI рд╕реЗ) Lukas Blecher, Guillem Cucurull, Thomas Scialom, Robert Stojnic. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [Nougat: Neural Optical Understanding for Academic Documents](https://arxiv.org/abs/2308.13418) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[Nystr├╢mformer](https://huggingface.co/docs/transformers/model_doc/nystromformer)** (рд╡рд┐рд╕реНрдХреЙрдиреНрд╕рд┐рди рд╡рд┐рд╢реНрд╡рд╡рд┐рджреНрдпрд╛рд▓рдп - рдореИрдбрд┐рд╕рди рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдХрд╛рдЧрдЬ [Nystr├╢mformer: A Nystr├╢m-Based Algorithm for Approximating Self-Attention](https://arxiv.org/abs/2102.03902) рдпреБрдирдпрд╛рдВрдЧ рдЬрд╝рд┐рдУрдВрдЧ, рдЭрд╛рдирдкреЗрдВрдЧ рдЬрд╝реЗрдВрдЧ, рд░реБрджреНрд░рд╕рд┐рд╕ рдЪрдХреНрд░рд╡рд░реНрддреА, рдорд┐рдВрдЧрдХреНрд╕рд┐рдВрдЧ рдЯреИрди, рдЧреНрд▓реЗрди рдлрдВрдЧ, рдпрд┐рди рд▓реА, рд╡рд┐рдХрд╛рд╕ рд╕рд┐рдВрд╣ рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[OLMo](https://huggingface.co/docs/transformers/model_doc/olmo)** (AI2 рд╕реЗ) Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, Hannaneh Hajishirzi. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [OLMo: Accelerating the Science of Language Models](https://arxiv.org/abs/2402.00838) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[OneFormer](https://huggingface.co/docs/transformers/model_doc/oneformer)** (SHI Labs рд╕реЗ) рдкреЗрдкрд░ [OneFormer: One Transformer to Rule Universal Image Segmentation](https://arxiv.org/abs/2211.06220) рдЬрд┐рддреЗрд╢ рдЬреИрди, рдЬрд┐рдЖрдЪреЗрди рд▓реА, рдорд╛рдВрдЧрдЯрд┐рдХ рдЪрд┐рдЙ, рдЕрд▓реА рд╣рд╕рдиреА, рдирд┐рдХрд┐рддрд╛ рдУрд░рд▓реЛрд╡, рд╣рдореНрдлреНрд░реА рд╢рд┐ рдХреЗ рджреНрд╡рд╛рд░рд╛ рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛ рд╣реИред
1. **[OpenLlama](https://huggingface.co/docs/transformers/model_doc/open-llama)** (from [s-JoL](https://huggingface.co/s-JoL)) released on GitHub (now removed).
1. **[OPT](https://huggingface.co/docs/transformers/master/model_doc/opt)** (from Meta AI) released with the paper [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068) by Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen et al.
1. **[OWL-ViT](https://huggingface.co/docs/transformers/model_doc/owlvit)** (Google AI рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдХрд╛рдЧрдЬ [Simple Open-Vocabulary Object Detection with Vision Transformers](https://arxiv.org/abs/2205.06230) рдореИрдерд┐рдпрд╛рд╕ рдорд┐рдВрдбрд░рд░, рдПрд▓реЗрдХреНрд╕реА рдЧреНрд░рд┐рдЯреНрд╕реЗрдВрдХреЛ, рдСрд╕реНрдЯрд┐рди рд╕реНрдЯреЛрди, рдореИрдХреНрд╕рд┐рдо рдиреНрдпреВрдореИрди, рдбрд┐рд░реНрдХ рд╡реАрд╕реЗрдирдмреЛрд░реНрди, рдПрд▓реЗрдХреНрд╕реА рдбреЛрд╕реЛрд╡рд┐рддреНрд╕реНрдХреА, рдЕрд░рд╡рд┐рдВрдж рдорд╣реЗрдВрджреНрд░рди, рдЕрдиреБрд░рд╛рдЧ рдЕрд░реНрдирдм, рдореБрд╕реНрддрдлрд╛ рджреЗрд╣рдШрд╛рдиреА, рдЬрд╝реБрдУрд░рди рд╢реЗрди, рдЬрд┐рдУ рд╡рд╛рдВрдЧ, рдЬрд╝рд┐рдпрд╛рдУрд╣реБрдЖ рдЭрд╛рдИ, рдереЙрдорд╕ рдХрд┐рдлрд╝, рдФрд░ рдиреАрд▓ рд╣реЙрд▓реНрд╕рдмреА рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[OWLv2](https://huggingface.co/docs/transformers/model_doc/owlv2)** (Google AI рд╕реЗ) Matthias Minderer, Alexey Gritsenko, Neil Houlsby. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [Scaling Open-Vocabulary Object Detection](https://arxiv.org/abs/2306.09683) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[PatchTSMixer](https://huggingface.co/docs/transformers/model_doc/patchtsmixer)** ( IBM Research рд╕реЗ) Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, Jayant Kalagnanam. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2306.09364.pdf) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[PatchTST](https://huggingface.co/docs/transformers/model_doc/patchtst)** (IBM рд╕реЗ) Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/abs/2211.14730) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[Pegasus](https://huggingface.co/docs/transformers/model_doc/pegasus)** (from Google) released with the paper [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/abs/1912.08777) by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu.
1. **[PEGASUS-X](https://huggingface.co/docs/transformers/model_doc/pegasus_x)** (Google рдХреА рдУрд░ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рджрд┐рдпрд╛ рдЧрдпрд╛ рдкреЗрдкрд░ [Investigating Efficiently Extending Transformers for Long Input Summarization](https://arxiv.org/abs/2208.04347) рдЬреЗрд╕рди рдлрд╛рдВрдЧ, рдпрд╛рдУ рдЭрд╛рдУ, рдкреАрдЯрд░ рдЬреЗ рд▓рд┐рдпреВ рджреНрд╡рд╛рд░рд╛ред
1. **[Perceiver IO](https://huggingface.co/docs/transformers/model_doc/perceiver)** (рджреАрдкрдорд╛рдЗрдВрдб рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [Perceiver IO: A General Architecture for Structured Inputs & Outputs](https://arxiv.org/abs/2107.14795) рдПрдВрдбреНрд░рдпреВ рдЬреЗрдЧрд▓, рд╕реЗрдмреЗрд╕реНрдЯрд┐рдпрди рдмреЛрд░рдЧреНрдпреВрдб, рдЬреАрди-рдмреИрдкреНрдЯрд┐рд╕реНрдЯ рдЕрд▓рд╛рдпрд░рд╛рдХ, рдХрд╛рд░реНрд▓ рдбреЛрд░реНрд╢, рдХреИрдЯрд▓рд┐рди рдЗрдУрдиреЗрд╕реНрдХреБ, рдбреЗрд╡рд┐рдб рджреНрд╡рд╛рд░рд╛ рдбрд┐рдВрдЧ, рд╕реНрдХрдВрдж рдХреЛрдкреНрдкреБрд▓рд╛, рдбреИрдирд┐рдпрд▓ рдЬрд╝реЛрд░рд╛рди, рдПрдВрдбреНрд░рдпреВ рдмреНрд░реЙрдХ, рдЗрд╡рд╛рди рд╢реЗрд▓рд╣реИрдорд░, рдУрд▓рд┐рд╡рд┐рдпрд░ рд╣реЗрдирд╛рдл, рдореИрдереНрдпреВ рдПрдоред рдмреЛрдЯреНрд╡рд┐рдирд┐рдХ, рдПрдВрдбреНрд░рдпреВ рдЬрд╝рд┐рд╕рд░рдореИрди, рдУрд░рд┐рдУрд▓ рд╡рд┐рдирд┐рдпрд▓реНрд╕, рдЬреЛрдЖрдУ рдХреИрд░реЗрд░рд╛ рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[Persimmon](https://huggingface.co/docs/transformers/model_doc/persimmon)** (ADEPT рд╕реЗ) Erich Elsen, Augustus Odena, Maxwell Nye, Sa─Яnak Ta┼Я─▒rlar, Tri Dao, Curtis Hawthorne, Deepak Moparthi, Arushi Somani. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [blog post](https://www.adept.ai/blog/persimmon-8b) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[Phi](https://huggingface.co/docs/transformers/model_doc/phi)** (from Microsoft) released with the papers - [Textbooks Are All You Need](https://arxiv.org/abs/2306.11644) by Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C├йsar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, S├йbastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee and Yuanzhi Li, [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463) by Yuanzhi Li, S├йbastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar and Yin Tat Lee.
1. **[PhoBERT](https://huggingface.co/docs/transformers/model_doc/phobert)** (VinAI Research рд╕реЗ) рдХрд╛рдЧрдЬ рдХреЗ рд╕рд╛рде [PhoBERT: Pre-trained language models for Vietnamese](https://www.aclweb.org/anthology/2020.findings-emnlp.92/) рдбреИрдЯ рдХреНрд╡реЛрдХ рдЧреБрдпреЗрди рдФрд░ рдЕрдиреНрд╣ рддреБрдЖрди рдЧреБрдпреЗрди рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[Pix2Struct](https://huggingface.co/docs/transformers/model_doc/pix2struct)** (Google рд╕реЗ) Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding](https://arxiv.org/abs/2210.03347) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[PLBart](https://huggingface.co/docs/transformers/model_doc/plbart)** (UCLA NLP рд╕реЗ) рд╕рд╛рде рд╡рд╛рд▓рд╛ рдкреЗрдкрд░ [Unified Pre-training for Program Understanding and Generation](https://arxiv.org/abs/2103.06333) рд╡рд╕реА рдЙрджреНрджреАрди рдЕрд╣рдордж, рд╕реИрдХрдд рдЪрдХреНрд░рд╡рд░реНрддреА, рдмреИрд╢рд╛рдЦреА рд░реЗ, рдХрд╛рдИ-рд╡реЗрдИ рдЪрд╛рдВрдЧ рджреНрд╡рд╛рд░рд╛ред
1. **[PoolFormer](https://huggingface.co/docs/transformers/model_doc/poolformer)** (from Sea AI Labs) released with the paper [MetaFormer is Actually What You Need for Vision](https://arxiv.org/abs/2111.11418) by Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng.
1. **[Pop2Piano](https://huggingface.co/docs/transformers/model_doc/pop2piano)** released with the paper [Pop2Piano : Pop Audio-based Piano Cover Generation](https://arxiv.org/abs/2211.00895) by Jongho Choi, Kyogu Lee.
1. **[ProphetNet](https://huggingface.co/docs/transformers/model_doc/prophetnet)** (рдорд╛рдЗрдХреНрд░реЛрд╕реЙрдлреНрдЯ рд░рд┐рд╕рд░реНрдЪ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https://arxiv.org/abs/2001.04063) рдпреВ рдпрд╛рди, рд╡реАрдЬрд╝реЗрди рдХреНрдпреВрдИ, рдпреЗрдпреБрди рдЧреЛрдВрдЧ, рджрдпрд╛рд╣реЗрдВрдЧ рд▓рд┐рдпреВ, рдирд╛рди рдбреБрдЖрди, рдЬрд┐рдЙрд╢реЗрдВрдЧ рдЪреЗрди, рд░реБрдУрдлрд╝реЗрдИ рдЭрд╛рдВрдЧ рдФрд░ рдорд┐рдВрдЧ рдЭреЛрдЙ рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[PVT](https://huggingface.co/docs/transformers/model_doc/pvt)** (Nanjing University, The University of Hong Kong etc. рд╕реЗ) Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions](https://arxiv.org/pdf/2102.12122.pdf) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[PVTv2](https://huggingface.co/docs/transformers/model_doc/pvt_v2)** (Shanghai AI Laboratory, Nanjing University, The University of Hong Kong etc. рд╕реЗ) Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [PVT v2: Improved Baselines with Pyramid Vision Transformer](https://arxiv.org/abs/2106.13797) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[QDQBert](https://huggingface.co/docs/transformers/model_doc/qdqbert)** (NVIDIA рд╕реЗ) рд╕рд╛рде рд╡рд╛рд▓рд╛ рдкреЗрдкрд░ [Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation](https://arxiv.org/abs/2004.09602) рд╣рд╛рдУ рд╡реВ, рдкреИрдЯреНрд░рд┐рдХ рдЬреБрдб, рдЬрд┐рдЖрдУрдЬреА рдЭрд╛рдВрдЧ, рдорд┐рдЦрд╛рдЗрд▓ рдЗрд╕реЗрд╡ рдФрд░ рдкреЙрд▓рд┐рдпрд╕ рдорд╛рдЗрдХреЗрд╡рд┐рд╕рд┐рдпрд╕ рджреНрд╡рд╛рд░рд╛ред
1. **[Qwen2](https://huggingface.co/docs/transformers/model_doc/qwen2)** (the Qwen team, Alibaba Group рд╕реЗ) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou and Tianhang Zhu. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [Qwen Technical Report](https://arxiv.org/abs/2309.16609) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[Qwen2MoE](https://huggingface.co/docs/transformers/model_doc/qwen2_moe)** (the Qwen team, Alibaba Group рд╕реЗ) Bo Zheng, Dayiheng Liu, Rui Men, Junyang Lin, Zhou San, Bowen Yu, An Yang, Mingfeng Xue, Fei Huang, Binyuan Hui, Mei Li, Tianyu Liu, Xingzhang Ren, Xuancheng Ren, Kexin Yang, Chang Zhou, Jingren Zhou. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [blog post](https://qwenlm.github.io/blog/qwen-moe/) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[RAG](https://huggingface.co/docs/transformers/model_doc/rag)** (рдлреЗрд╕рдмреБрдХ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдХрд╛рдЧрдЬ [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) рдкреИрдЯреНрд░рд┐рдХ рд▓реБрдИрд╕, рдПрдерди рдкреЗрд░реЗрдЬрд╝, рдЕрд▓реЗрдХреНрдЬреЗрдВрдбреНрд░рд╛ рдкрд┐рдХреНрдЯрд╕, рдлреИрдмрд┐рдпреЛ рдкреЗрдЯреНрд░реЛрдиреА, рд╡реНрд▓рд╛рджрд┐рдореАрд░ рдХрд╛рд░рдкреБрдЦрд┐рди, рдирдорди рдЧреЛрдпрд▓, рд╣реЗрдирд░рд┐рдХ рдХреБрдЯрд▓рд░, рдорд╛рдЗрдХ рд▓реБрдИрд╕, рд╡реЗрди-рддрд╛рдЙ рдпрд┐рд╣, рдЯрд┐рдо рд░реЙрдХрдЯрд╛рд╢реЗрд▓, рд╕реЗрдмрд╕реНрдЯрд┐рдпрди рд░рд┐рдбреЗрд▓, рдбреМрд╡реЗ рдХреАрд▓рд╛ рджреНрд╡рд╛рд░рд╛ред
1. **[REALM](https://huggingface.co/docs/transformers/model_doc/realm.html)** (Google рдЕрдиреБрд╕рдВрдзрд╛рди рд╕реЗ) рдХреЗрд▓реНрд╡рд┐рди рдЧреБ, рдХреЗрдВрдЯрди рд▓реА, рдЬрд╝реЛрд░рд╛ рддреБрдВрдЧ, рдкрд╛рдиреБрдкреЛрдВрдЧ рдкрд╕реБрдкрдд рдФрд░ рдорд┐рдВрдЧ-рд╡реЗрдИ рдЪрд╛рдВрдЧ рджреНрд╡рд╛рд░рд╛ рд╕рд╛рде рдореЗрдВ рджрд┐рдпрд╛ рдЧрдпрд╛ рдкреЗрдкрд░ [REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909)ред
1. **[RecurrentGemma](https://huggingface.co/docs/transformers/model_doc/recurrent-gemma)** (Google рд╕реЗ) the Griffin, RLHF and Gemma Teams. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [RecurrentGemma: Moving Past Transformers for Efficient Open Language Models](https://storage.googleapis.com/deepmind-media/gemma/recurrentgemma-report.pdf) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[Reformer](https://huggingface.co/docs/transformers/model_doc/reformer)** (from Google Research) released with the paper [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451) by Nikita Kitaev, ┼Бukasz Kaiser, Anselm Levskaya.
1. **[RegNet](https://huggingface.co/docs/transformers/model_doc/regnet)** (META рд░рд┐рд╕рд░реНрдЪ рд╕реЗ) [Designing Network Design Space](https://arxiv.org/abs/2003.13678) рдкреЗрдкрд░ рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛ рдПрдмреНрд╕/2003.13678) рдЗрд▓рд┐рдЬрд╛ рд░рд╛рдбреЛрд╕рд╛рд╡реЛрд╡рд┐рдХ, рд░рд╛рдЬ рдкреНрд░рддреАрдХ рдХреЛрд╕рд╛рд░рд╛рдЬреВ, рд░реЙрд╕ рдЧрд┐рд░реНрд╢рд┐рдХ, рдХреИрдорд┐рдВрдЧ рд╣реА, рдкрд┐рдУрдЯрд░ рдбреЙрд▓рд░ рджреНрд╡рд╛рд░рд╛ред
1. **[RemBERT](https://huggingface.co/docs/transformers/model_doc/rembert)** (рдЧреВрдЧрд▓ рд░рд┐рд╕рд░реНрдЪ рд╕реЗ) рд╕рд╛рде рд╡рд╛рд▓рд╛ рдкреЗрдкрд░ [Rethinking embedding coupling in pre-trained language models](https://arxiv.org/abs/2010.12821) рд╣реНрдпреБрдВрдЧ рд╡реЛрди рдЪреБрдВрдЧ, рдерд┐рдмреЙрд▓реНрдЯ рдлрд╝реЗрд╡рд░реА, рд╣реЗрдирд░реА рддреНрд╕рд╛рдИ, рдПрдо. рдЬреЙрдирд╕рди, рд╕реЗрдмреЗрд╕реНрдЯрд┐рдпрди рд░реБрдбрд░ рджреНрд╡рд╛рд░рд╛ред
1. **[ResNet](https://huggingface.co/docs/transformers/model_doc/resnet)** (рдорд╛рдЗрдХреНрд░реЛрд╕реЙрдлреНрдЯ рд░рд┐рд╕рд░реНрдЪ рд╕реЗ) [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) рдХреИрдорд┐рдВрдЧ рд╣реЗ, рдЬрд┐рдпрд╛рдВрдЧреНрдпреБ рдЭрд╛рдВрдЧ, рд╢рд╛рдУрдХрд┐рдВрдЧ рд░реЗрди, рдЬрд┐рдпрд╛рди рд╕рди рджреНрд╡рд╛рд░рд╛ред
1. **[RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)** (рдлреЗрд╕рдмреБрдХ рд╕реЗ), рд╕рд╛рде рдореЗрдВ рдХрд╛рдЧрдЬ [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) рдпрд┐рдирд╣рд╛рди рд▓рд┐рдпреВ, рдорд╛рдпрд▓ рдУрдЯ, рдирдорди рдЧреЛрдпрд▓, рдЬрд┐рдВрдЧрдлреЗрдИ рдбреВ, рдордВрджрд╛рд░ рдЬреЛрд╢реА, рдбреИрдирдХреА рдЪреЗрди, рдУрдорд░ рд▓реЗрд╡реА, рдорд╛рдЗрдХ рд▓реБрдИрд╕, рд▓реНрдпреВрдХ рдЬрд╝реЗрдЯрд▓рдореЙрдпрд░, рд╡реЗрд╕реЗрд▓рд┐рди рд╕реНрдЯреЛрдпрд╛рдиреЛрд╡ рджреНрд╡рд╛рд░рд╛ред
1. **[RoBERTa-PreLayerNorm](https://huggingface.co/docs/transformers/model_doc/roberta-prelayernorm)** (from Facebook) released with the paper [fairseq: A Fast, Extensible Toolkit for Sequence Modeling](https://arxiv.org/abs/1904.01038) by Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli.
1. **[RoCBert](https://huggingface.co/docs/transformers/model_doc/roc_bert)** (from WeChatAI) released with the paper [RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining](https://aclanthology.org/2022.acl-long.65.pdf) by HuiSu, WeiweiShi, XiaoyuShen, XiaoZhou, TuoJi, JiaruiFang, JieZhou.
1. **[RoFormer](https://huggingface.co/docs/transformers/model_doc/roformer)** (рдЭреБрдИрдИ рдЯреЗрдХреНрдиреЛрд▓реЙрдЬреА рд╕реЗ), рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864) рдЬрд┐рдпрд╛рдирд▓рд┐рди рд╕реБ рдФрд░ рдпреВ рд▓реВ рдФрд░ рд╢реЗрдВрдЧрдлреЗрдВрдЧ рдкреИрди рдФрд░ рдмреЛ рд╡реЗрди рдФрд░ рдпреБрдирдлреЗрдВрдЧ рд▓рд┐рдпреВ рджреНрд╡рд╛рд░рд╛ рдкреНрд░рдХрд╛рд╢рд┐рддред
1. **[RWKV](https://huggingface.co/docs/transformers/model_doc/rwkv)** (Bo Peng рд╕реЗ) Bo Peng. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [this repo](https://github.com/BlinkDL/RWKV-LM) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[SeamlessM4T](https://huggingface.co/docs/transformers/model_doc/seamless_m4t)** (from Meta AI) released with the paper [SeamlessM4T тАФ Massively Multilingual & Multimodal Machine Translation](https://dl.fbaipublicfiles.com/seamless/seamless_m4t_paper.pdf) by the Seamless Communication team.
1. **[SeamlessM4Tv2](https://huggingface.co/docs/transformers/model_doc/seamless_m4t_v2)** (from Meta AI) released with the paper [Seamless: Multilingual Expressive and Streaming Speech Translation](https://ai.meta.com/research/publications/seamless-multilingual-expressive-and-streaming-speech-translation/) by the Seamless Communication team.
1. **[SegFormer](https://huggingface.co/docs/transformers/model_doc/segformer)** (from NVIDIA) released with the paper [SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://arxiv.org/abs/2105.15203) by Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo.
1. **[SegGPT](https://huggingface.co/docs/transformers/model_doc/seggpt)** (Beijing Academy of Artificial Intelligence (BAAI рд╕реЗ) Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, Tiejun Huang. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [SegGPT: Segmenting Everything In Context](https://arxiv.org/abs/2304.03284) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[Segment Anything](https://huggingface.co/docs/transformers/model_doc/sam)** (Meta AI рд╕реЗ) Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar, Ross Girshick. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [Segment Anything](https://arxiv.org/pdf/2304.02643v1.pdf) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[SEW](https://huggingface.co/docs/transformers/model_doc/sew)** (ASAPP рд╕реЗ) рд╕рд╛рде рджреЗрдиреЗ рд╡рд╛рд▓рд╛ рдкреЗрдкрд░ [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https://arxiv.org/abs/2109.06870) рдлреЗрд▓рд┐рдХреНрд╕ рд╡реВ, рдХреНрд╡рд╛рдВрдЧрдпреБрди рдХрд┐рдо, рдЬрд┐рдВрдЧ рдкреИрди, рдХреНрдпреВ рд╣рд╛рди, рдХрд┐рд▓рд┐рдпрди рдХреНрдпреВ. рд╡реЗрдирдмрд░реНрдЧрд░, рдпреЛрд╡ рдЖрд░реНрдЯрдЬрд╝реА рджреНрд╡рд╛рд░рд╛ред
1. **[SEW-D](https://huggingface.co/docs/transformers/model_doc/sew_d)** (ASAPP рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https://arxiv.org/abs/2109.06870) рдлреЗрд▓рд┐рдХреНрд╕ рд╡реВ, рдХреНрд╡рд╛рдВрдЧрдпреБрди рдХрд┐рдо, рдЬрд┐рдВрдЧ рдкреИрди, рдХреНрдпреВ рд╣рд╛рди, рдХрд┐рд▓рд┐рдпрди рдХреНрдпреВ. рд╡реЗрдирдмрд░реНрдЧрд░, рдпреЛрдЖрд╡ рдЖрд░реНрдЯрдЬрд╝реА рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[SigLIP](https://huggingface.co/docs/transformers/model_doc/siglip)** (Google AI рд╕реЗ) Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [Sigmoid Loss for Language Image Pre-Training](https://arxiv.org/abs/2303.15343) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[SpeechT5](https://huggingface.co/docs/transformers/model_doc/speecht5)** (from Microsoft Research) released with the paper [SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing](https://arxiv.org/abs/2110.07205) by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.
1. **[SpeechToTextTransformer](https://huggingface.co/docs/transformers/model_doc/speech_to_text)** (рдлреЗрд╕рдмреБрдХ рд╕реЗ), рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [fairseq S2T: Fast Speech-to-Text Modeling with fairseq](https://arxiv.org/abs/2010.05171) рдЪрд╛рдВрдЧрд╣рд╛рди рд╡рд╛рдВрдЧ, рдпреВрдВ рддрд╛рдВрдЧ, рдЬреБрддрд╛рдИ рдорд╛, рдРрдиреА рд╡реВ, рджрд┐рдорд┐рддреНрд░реЛ рдУрдЦреЛрдирдХреЛ, рдЬреБрдЖрди рдкрд┐рдиреЛ рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛уАВ
1. **[SpeechToTextTransformer2](https://huggingface.co/docs/transformers/model_doc/speech_to_text_2)** (рдлреЗрд╕рдмреБрдХ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [Large-Scale Self- and Semi-Supervised Learning for Speech Translation](https://arxiv.org/abs/2104.06678) рдЪрд╛рдВрдЧрд╣рд╛рди рд╡рд╛рдВрдЧ, рдРрдиреА рд╡реВ, рдЬреБрдЖрди рдкрд┐рдиреЛ, рдПрд▓реЗрдХреНрд╕реА рдмреЗрд╡рд╕реНрдХреА, рдорд╛рдЗрдХрд▓ рдФрд▓реА, рдПрд▓реЗрдХреНрд╕рд┐рд╕ рджреНрд╡рд╛рд░рд╛ Conneau рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[Splinter](https://huggingface.co/docs/transformers/model_doc/splinter)** (рддреЗрд▓ рдЕрд╡реАрд╡ рдпреВрдирд┐рд╡рд░реНрд╕рд┐рдЯреА рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [Few-Shot Question Answering by Pretraining Span Selection](https://arxiv.org/abs/2101.00438) рдУрд░рд┐ рд░рд╛рдо, рдпреБрд╡рд▓ рдХрд░реНрд╕реНрдЯрди, рдЬреЛрдирд╛рдерди рдмреЗрд░реЗрдВрдЯ, рдЕрдореАрд░ рдЧреНрд▓реЛрдмрд░реНрд╕рди, рдУрдорд░ рд▓реЗрд╡реА рджреНрд╡рд╛рд░рд╛ред
1. **[SqueezeBERT](https://huggingface.co/docs/transformers/model_doc/squeezebert)** (рдмрд░реНрдХрд▓реЗ рд╕реЗ) рдХрд╛рдЧрдЬ рдХреЗ рд╕рд╛рде [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) рдлреЙрд░реЗрд╕реНрдЯ рдПрди. рдЗрдирдбреЛрд▓рд╛, рдЕрд▓реНрдмрд░реНрдЯ рдИ. рд╢реЙ, рд░рд╡рд┐ рдХреГрд╖реНрдгрд╛, рдФрд░ рдХрд░реНрдЯ рдбрдмреНрд▓реНрдпреВ. рдХреЗрдЯрдЬрд╝рд░ рджреНрд╡рд╛рд░рд╛ред
1. **[StableLm](https://huggingface.co/docs/transformers/model_doc/stablelm)** (from Stability AI) released with the paper [StableLM 3B 4E1T (Technical Report)](https://stability.wandb.io/stability-llm/stable-lm/reports/StableLM-3B-4E1T--VmlldzoyMjU4?accessToken=u3zujipenkx5g7rtcj9qojjgxpconyjktjkli2po09nffrffdhhchq045vp0wyfo) by  Jonathan Tow, Marco Bellagente, Dakota Mahan, Carlos Riquelme Ruiz, Duy Phung, Maksym Zhuravinskyi, Nathan Cooper, Nikhil Pinnaparaju, Reshinth Adithyan, and James Baicoianu.
1. **[Starcoder2](https://huggingface.co/docs/transformers/model_doc/starcoder2)** (from BigCode team) released with the paper [StarCoder 2 and The Stack v2: The Next Generation](https://arxiv.org/abs/2402.19173) by Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krau├Я, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Mu├▒oz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries.
1. **[SuperPoint](https://huggingface.co/docs/transformers/model_doc/superpoint)** (from MagicLeap) released with the paper [SuperPoint: Self-Supervised Interest Point Detection and Description](https://arxiv.org/abs/1712.07629) by Daniel DeTone, Tomasz Malisiewicz and Andrew Rabinovich.
1. **[SwiftFormer](https://huggingface.co/docs/transformers/model_doc/swiftformer)** (MBZUAI рд╕реЗ) Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications](https://arxiv.org/abs/2303.15446) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[Swin Transformer](https://huggingface.co/docs/transformers/model_doc/swin)** (рдорд╛рдЗрдХреНрд░реЛрд╕реЙрдлреНрдЯ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдХрд╛рдЧрдЬ [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) рдЬрд╝реА рд▓рд┐рдпреВ, рдпреБрдЯреЛрдВрдЧ рд▓рд┐рди, рдпреВ рдХрд╛рдУ, рд╣рд╛рди рд╣реВ, рдпрд┐рдХреНрд╕реБрдЖрди рд╡реЗрдИ, рдЭреЗрдВрдЧ рдЭрд╛рдВрдЧ, рд╕реНрдЯреАрдлрди рд▓рд┐рди, рдмреИрдирд┐рдВрдЧ рдЧреБрдУ рджреНрд╡рд╛рд░рд╛ред
1. **[Swin Transformer V2](https://huggingface.co/docs/transformers/model_doc/swinv2)** (Microsoft рд╕реЗ) рд╕рд╛рде рд╡рд╛рд▓рд╛ рдкреЗрдкрд░ [Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883) рдЬрд╝реА рд▓рд┐рдпреВ, рд╣рд╛рди рд╣реВ, рдпреБрдЯреЛрдВрдЧ рд▓рд┐рди, рдЬрд╝реБрд▓рд┐рдЖрдВрдЧ рдпрд╛рдУ, рдЬрд╝реЗрдВрдбрд╛ рдЬрд╝реА, рдпрд┐рдХреНрд╕реБрдЖрди рд╡реЗрдИ, рдЬрд┐рдпрд╛ рдирд┐рдВрдЧ, рдпреВ рдХрд╛рдУ, рдЭреЗрдВрдЧ рдЭрд╛рдВрдЧ, рд▓реА рдбреЛрдВрдЧ, рдлреБрд░реБ рд╡реЗрдИ, рдмреИрдирд┐рдВрдЧ рдЧреБрдУ рджреНрд╡рд╛рд░рд╛ред
1. **[Swin2SR](https://huggingface.co/docs/transformers/model_doc/swin2sr)** (from University of W├╝rzburg) released with the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345) by Marcos V. Conde, Ui-Jin Choi, Maxime Burchi, Radu Timofte.
1. **[SwitchTransformers](https://huggingface.co/docs/transformers/model_doc/switch_transformers)** (from Google) released with the paper [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961) by William Fedus, Barret Zoph, Noam Shazeer.
1. **[T5](https://huggingface.co/docs/transformers/model_doc/t5)** (цЭешЗк Google AI)рдХреЙрд▓рд┐рди рд░реИрдлреЗрд▓ рдФрд░ рдиреЛрдо рд╢рдЬрд╝реАрд░ рдФрд░ рдПрдбрдо рд░реЙрдмрд░реНрдЯреНрд╕ рдФрд░ рдХреИрдерд░реАрди рд▓реА рдФрд░ рд╢рд░рдг рдирд╛рд░рдВрдЧ рдФрд░ рдорд╛рдЗрдХрд▓ рдордЯреЗрдирд╛ рджреНрд╡рд╛рд░рд╛ рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) рдФрд░ рдпрд╛рдВрдХреА рдЭреЛрдЙ рдФрд░ рд╡реЗрдИ рд▓реА рдФрд░ рдкреАрдЯрд░ рдЬреЗ рд▓рд┐рдпреВред
1. **[T5v1.1](https://huggingface.co/docs/transformers/model_doc/t5v1.1)** (Google AI рд╕реЗ) рд╕рд╛рде рд╡рд╛рд▓рд╛ рдкреЗрдкрд░ [google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511) рдХреЙрд▓рд┐рди рд░реИрдлреЗрд▓ рдФрд░ рдиреЛрдо рд╢рдЬрд╝реАрд░ рдФрд░ рдПрдбрдо рд░реЙрдмрд░реНрдЯреНрд╕ рдФрд░ рдХреИрдерд░реАрди рд▓реА рдФрд░ рд╢рд░рдг рдирд╛рд░рдВрдЧ рджреНрд╡рд╛рд░рд╛ рдФрд░ рдорд╛рдЗрдХрд▓ рдордЯреЗрдирд╛ рдФрд░ рдпрд╛рдВрдХреА рдЭреЛрдЙ рдФрд░ рд╡реЗрдИ рд▓реА рдФрд░ рдкреАрдЯрд░ рдЬреЗ рд▓рд┐рдпреВред
1. **[Table Transformer](https://huggingface.co/docs/transformers/model_doc/table-transformer)** (рдорд╛рдЗрдХреНрд░реЛрд╕реЙрдлреНрдЯ рд░рд┐рд╕рд░реНрдЪ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents](https://arxiv.org/abs/2110.00061) рдмреНрд░реИрдВрдбрди рд╕реНрдореЙрдХ, рд░реЛрд╣рд┐рдд рдкреЗрд╕рд╛рд▓рд╛, рд░реЙрдмрд┐рди рдЕрдмреНрд░рд╛рд╣рдо рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[TAPAS](https://huggingface.co/docs/transformers/model_doc/tapas)** (Google AI рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдХрд╛рдЧрдЬ [TAPAS: Weakly Supervised Table Parsing via Pre-training](https://arxiv.org/abs/2004.02349) рдЬреЛрдирд╛рдерди рд╣рд░реНрдЬрд╝рд┐рдЧ, рдкрд╛рд╡реЗрд▓ рдХреНрд░рд┐рдЬрд╝рд┐рд╕реНрддреЛрдлрд╝ рдиреЛрд╡рд╛рдХ, рдереЙрдорд╕ рдореБрд▓рд░, рдлреНрд░рд╛рдВрд╕реЗрд╕реНрдХреЛ рдкрд┐рдХрд┐рдиреНрдиреЛ рдФрд░ рдЬреВрд▓рд┐рдпрди рдорд╛рд░реНрдЯрд┐рди рдИрд╕реЗрдиреНрдЪреНрд▓реЛрд╕ рджреНрд╡рд╛рд░рд╛ред
1. **[TAPEX](https://huggingface.co/docs/transformers/model_doc/tapex)** (рдорд╛рдЗрдХреНрд░реЛрд╕реЙрдлреНрдЯ рд░рд┐рд╕рд░реНрдЪ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [TAPEX: Table Pre-training via Learning a Neural SQL Executor](https://arxiv.org/abs/2107.07653) рдХрд┐рдпрд╛рди рд▓рд┐рдпреВ, рдмреЗрдИ рдЪреЗрди, рдЬрд┐рдпрд╛рдХреА рдЧреБрдУ, рдореЛрд░реНрдЯреЗрдЬрд╝рд╛ рдЬрд╝рд┐рдпрд╛рджреА, рдЬрд╝реЗрдХреА рд▓рд┐рди, рд╡реАрдЬрд╝реВ рдЪреЗрди, рдЬрд┐рдпрд╛рди-рдЧреБрдЖрдВрдЧ рд▓реВ рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[Time Series Transformer](https://huggingface.co/docs/transformers/model_doc/time_series_transformer)** (from HuggingFace).
1. **[TimeSformer](https://huggingface.co/docs/transformers/model_doc/timesformer)** (from Facebook) released with the paper [Is Space-Time Attention All You Need for Video Understanding?](https://arxiv.org/abs/2102.05095) by Gedas Bertasius, Heng Wang, Lorenzo Torresani.
1. **[Trajectory Transformer](https://huggingface.co/docs/transformers/model_doc/trajectory_transformers)** (from the University of California at Berkeley) released with the paper [Offline Reinforcement Learning as One Big Sequence Modeling Problem](https://arxiv.org/abs/2106.02039) by Michael Janner, Qiyang Li, Sergey Levine
1. **[Transformer-XL](https://huggingface.co/docs/transformers/model_doc/transfo-xl)** (Google/CMU рдХреА рдУрд░ рд╕реЗ) рдХрд╛рдЧрдЬ рдХреЗ рд╕рд╛рде [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860) рдХреНрд╡реЛрдХреЛрдХ рд╡реА. рд▓реЗ, рд░реБрд╕реНрд▓реИрди рд╕рд▓рд╛рдЦреБрддрджреА
1. **[TrOCR](https://huggingface.co/docs/transformers/model_doc/trocr)** (from Microsoft) released with the paper [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282) by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei.
1. **[TVLT](https://huggingface.co/docs/transformers/model_doc/tvlt)** (from UNC Chapel Hill) released with the paper [TVLT: Textless Vision-Language Transformer](https://arxiv.org/abs/2209.14156) by Zineng Tang, Jaemin Cho, Yixin Nie, Mohit Bansal.
1. **[TVP](https://huggingface.co/docs/transformers/model_doc/tvp)** (from Intel) released with the paper [Text-Visual Prompting for Efficient 2D Temporal Video Grounding](https://arxiv.org/abs/2303.04995) by Yimeng Zhang, Xin Chen, Jinghan Jia, Sijia Liu, Ke Ding.
1. **[UDOP](https://huggingface.co/docs/transformers/model_doc/udop)** (Microsoft Research рд╕реЗ) Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Chenguang Zhu, Michael Zeng, Cha Zhang, Mohit Bansal. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [Unifying Vision, Text, and Layout for Universal Document Processing](https://arxiv.org/abs/2212.02623) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[UL2](https://huggingface.co/docs/transformers/model_doc/ul2)** (from Google Research) released with the paper [Unifying Language Learning Paradigms](https://arxiv.org/abs/2205.05131v1) by Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler
1. **[UMT5](https://huggingface.co/docs/transformers/model_doc/umt5)** (Google Research рд╕реЗ) Hyung Won Chung, Xavier Garcia, Adam Roberts, Yi Tay, Orhan Firat, Sharan Narang, Noah Constant. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [UniMax: Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining](https://openreview.net/forum?id=kXwdL1cWOAi) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[UniSpeech](https://huggingface.co/docs/transformers/model_doc/unispeech)** (рдорд╛рдЗрдХреНрд░реЛрд╕реЙрдлреНрдЯ рд░рд┐рд╕рд░реНрдЪ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рджрд┐рдпрд╛ рдЧрдпрд╛ рдкреЗрдкрд░ [UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data](https://arxiv.org/abs/2101.07597) рдЪреЗрдВрдЧрдИ рд╡рд╛рдВрдЧ, рдпреВ рд╡реВ, рдпрд╛рдУ рдХрд┐рдпрд╛рди, рдХреЗрдирд┐рдЪреА рдХреБрдорд╛рддрд╛рдиреА, рд╢реБрдЬреА рд▓рд┐рдпреВ, рдлреБрд░реБ рд╡реЗрдИ, рдорд╛рдЗрдХрд▓ рдЬрд╝реЗрдВрдЧ, рдЬрд╝реБрдПрджреЛрдВрдЧ рд╣реБрдЖрдВрдЧ рджреНрд╡рд╛рд░рд╛ред
1. **[UniSpeechSat](https://huggingface.co/docs/transformers/model_doc/unispeech-sat)** (рдорд╛рдЗрдХреНрд░реЛрд╕реЙрдлреНрдЯ рд░рд┐рд╕рд░реНрдЪ рд╕реЗ) рдХрд╛рдЧрдЬ рдХреЗ рд╕рд╛рде [UNISPEECH-SAT: UNIVERSAL SPEECH REPRESENTATION LEARNING WITH SPEAKER AWARE PRE-TRAINING](https://arxiv.org/abs/2110.05752) рд╕рд╛рдирдпреБрдЖрди рдЪреЗрди, рдпреВ рд╡реВ, рдЪреЗрдВрдЧреНрдпреА рд╡рд╛рдВрдЧ, рдЭреЗрдВрдЧрдпрд╛рдВрдЧ рдЪреЗрди, рдЭреВрдУ рдЪреЗрди, рд╢реБрдЬреА рд▓рд┐рдпреВ, рдЬрд┐рдпрд╛рди рд╡реВ, рдпрд╛рдУ рдХрд┐рдпрд╛рди, рдлреБрд░реБ рд╡реЗрдИ, рдЬрд┐рдиреНрдпреБ рд▓реА, рдЬрд┐рдпрд╛рдВрдЧрдЬрд╝рд╛рди рдпреВ рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[UnivNet](https://huggingface.co/docs/transformers/model_doc/univnet)** (from Kakao Corporation) released with the paper [UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generation](https://arxiv.org/abs/2106.07889) by Won Jang, Dan Lim, Jaesam Yoon, Bongwan Kim, and Juntae Kim.
1. **[UPerNet](https://huggingface.co/docs/transformers/model_doc/upernet)** (from Peking University) released with the paper [Unified Perceptual Parsing for Scene Understanding](https://arxiv.org/abs/1807.10221) by Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, Jian Sun.
1. **[VAN](https://huggingface.co/docs/transformers/model_doc/van)** (рд╕рд┐рдВрдШреБрдЖ рдпреВрдирд┐рд╡рд░реНрд╕рд┐рдЯреА рдФрд░ рдирдирдХрд╛рдИ рдпреВрдирд┐рд╡рд░реНрд╕рд┐рдЯреА рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [Visual Attention Network](https://arxiv.org/abs/2202.09741) рдореЗрдВрдЧ-рд╣рд╛рдУ рдЧреБрдУ, рдЪреЗрдВрдЧ-рдЬрд╝реЗ рд▓реВ, рдЭреЗрдВрдЧ-рдирд┐рдВрдЧ рд▓рд┐рдпреВ, рдорд┐рдВрдЧ-рдорд┐рдВрдЧ рдЪреЗрдВрдЧ, рд╢рд┐-рдорд┐рди рд╣реВ рджреНрд╡рд╛рд░рд╛ред
1. **[VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae)** (рдорд▓реНрдЯреАрдореАрдбрд┐рдпрд╛ рдХрдореНрдкреНрдпреВрдЯрд┐рдВрдЧ рдЧреНрд░реБрдк, рдирд╛рдирдЬрд┐рдВрдЧ рдпреВрдирд┐рд╡рд░реНрд╕рд┐рдЯреА рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training](https://arxiv.org/abs/2203.12602) рдЬрд╝рд╛рди рдЯреЛрдВрдЧ, рдпрд┐рдмрд┐рдВрдЧ рд╕реЙрдиреНрдЧ, рдЬреБрдП рджреНрд╡рд╛рд░рд╛ рд╡рд╛рдВрдЧ, рд▓рд┐рдорд┐рди рд╡рд╛рдВрдЧ рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[ViLT](https://huggingface.co/docs/transformers/model_doc/vilt)** (NAVER AI Lab/Kakao Enterprise/Kakao Brain рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдХрд╛рдЧрдЬ [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://arxiv.org/abs/2102.03334) рд╡реЛрдирдЬреЗ рдХрд┐рдо, рдмреЛрдХреНрдпреВрдВрдЧ рд╕реЛрди, рдЗрд▓реНрдбреВ рдХрд┐рдо рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[VipLlava](https://huggingface.co/docs/transformers/model_doc/vipllava)** (University of WisconsinтАУMadison рд╕реЗ) Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Dennis Park, Yong Jae Lee. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [Making Large Multimodal Models Understand Arbitrary Visual Prompts](https://arxiv.org/abs/2312.00784) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit)** (рдЧреВрдЧрд▓ рдПрдЖрдИ рд╕реЗ) рдХрд╛рдЧрдЬ рдХреЗ рд╕рд╛рде [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) рдПрд▓реЗрдХреНрд╕реА рдбреЛрд╕реЛрд╡рд┐рддреНрд╕реНрдХреА, рд▓реБрдХрд╛рд╕ рдмреЗрдпрд░, рдЕрд▓реЗрдХреНрдЬреЗрдВрдбрд░ рдХреЛрд▓реЗрд╕рдирд┐рдХреЛрд╡, рдбрд┐рд░реНрдХ рд╡реАрд╕реЗрдирдмреЛрд░реНрди, рд╢рд┐рдпрд╛рдУрд╣реБрдЖ рдЭрд╛рдИ, рдереЙрдорд╕ рдЕрдирдЯрд░рдерд┐рдирд░, рдореБрд╕реНрддрдлрд╛ рджреЗрд╣рдШрд╛рдиреА, рдореИрдерд┐рдпрд╛рд╕ рдорд┐рдВрдбрд░рд░, рдЬреЙрд░реНрдЬ рд╣реЗрдЧреЛрд▓реНрдб, рд╕рд┐рд▓реНрд╡реЗрди рдЧреЗрд▓реА, рдЬреИрдХрдм рдЙрд╕реНрдЬрд╝рдХреЛрд░реЗрдЗрдЯ рджреНрд╡рд╛рд░рд╛ рд╣реЙрд▓реНрд╕рдмреА рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[VisualBERT](https://huggingface.co/docs/transformers/model_doc/visual_bert)** (UCLA NLP рд╕реЗ) рд╕рд╛рде рд╡рд╛рд▓рд╛ рдкреЗрдкрд░ [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557) рд▓рд┐рдпреБрдирд┐рдпрди рд╣реЗрд░реЛрд▓реНрдб рд▓реА, рдорд╛рд░реНрдХ рдпрд╛рддреНрд╕реНрдХрд░, рджрд╛ рдпрд┐рди, рдЪреЛ-рдЬреБрдИ рд╣рд╕реАрд╣, рдХрд╛рдИ-рд╡реЗрдИ рдЪрд╛рдВрдЧ рджреНрд╡рд╛рд░рд╛ред
1. **[ViT Hybrid](https://huggingface.co/docs/transformers/model_doc/vit_hybrid)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.
1. **[VitDet](https://huggingface.co/docs/transformers/model_doc/vitdet)** (Meta AI рд╕реЗ) Yanghao Li, Hanzi Mao, Ross Girshick, Kaiming He. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [Exploring Plain Vision Transformer Backbones for Object Detection](https://arxiv.org/abs/2203.16527) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[ViTMAE](https://huggingface.co/docs/transformers/model_doc/vit_mae)** (рдореЗрдЯрд╛ рдПрдЖрдИ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдХрд╛рдЧрдЬ [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377) рдХреИрдорд┐рдВрдЧ рд╣реЗ, рдЬрд╝рд┐рдиреЗрд▓реА рдЪреЗрди, рд╕реЗрдирд┐рдВрдЧ рдЬрд╝реА, рдпрд╛рдВрдЧрд╣реЛ рд▓реА, рдкрд┐рдУрдЯреНрд░ рдбреЙрд▓рд░, рд░реЙрд╕ рдЧрд┐рд░реНрд╢рд┐рдХ рджреНрд╡рд╛рд░рд╛ред
1. **[ViTMatte](https://huggingface.co/docs/transformers/model_doc/vitmatte)** (HUST-VL рд╕реЗ) Jingfeng Yao, Xinggang Wang, Shusheng Yang, Baoyuan Wang. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [ViTMatte: Boosting Image Matting with Pretrained Plain Vision Transformers](https://arxiv.org/abs/2305.15272) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[ViTMSN](https://huggingface.co/docs/transformers/model_doc/vit_msn)** (рдореЗрдЯрд╛ рдПрдЖрдИ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдХрд╛рдЧрдЬ [Masked Siamese Networks for Label-Efficient Learning](https://arxiv.org/abs/2204.07141) рдорд╣рдореВрдж рдЕрд╕рд░рд╛рди, рдордерд┐рд▓реНрдбреЗ рдХреИрд░рди, рдИрд╢рд╛рди рдорд┐рд╢реНрд░рд╛, рдкрд┐рдпреЛрдЯреНрд░ рдмреЛрдЬрд╛рдиреЛрд╡рд╕реНрдХреА, рдлреНрд▓реЛрд░рд┐рдпрди рдмреЛрд░реНрдбреЗрд╕, рдкрд╛рд╕реНрдХрд▓ рд╡рд┐рдВрд╕реЗрдВрдЯ, рдЖрд░реНрдордВрдб рдЬреМрд▓рд┐рди, рдорд╛рдЗрдХрд▓ рд░рдмреНрдмрдд, рдирд┐рдХреЛрд▓рд╕ рдмрд▓реНрд▓рд╛рд╕ рджреНрд╡рд╛рд░рд╛ред
1. **[VITS](https://huggingface.co/docs/transformers/model_doc/vits)** (Kakao Enterprise рд╕реЗ) Jaehyeon Kim, Jungil Kong, Juhee Son. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech](https://arxiv.org/abs/2106.06103) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[ViViT](https://huggingface.co/docs/transformers/model_doc/vivit)** (from Google Research) released with the paper [ViViT: A Video Vision Transformer](https://arxiv.org/abs/2103.15691) by Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu─Нi─З, Cordelia Schmid.
1. **[Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2)** (рдлреЗрд╕рдмреБрдХ рдПрдЖрдИ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) рдПрд▓реЗрдХреНрд╕реА рдмреЗрд╡рд╕реНрдХреА, рд╣реЗрдирд░реА рдЭреЛрдЙ, рдЕрдмреНрджреЗрд▓рд░рд╣рдорд╛рди рдореЛрд╣рдореНрдордж, рдорд╛рдЗрдХрд▓ рдФрд▓реА рджреНрд╡рд╛рд░рд╛ред
1. **[Wav2Vec2-BERT](https://huggingface.co/docs/transformers/model_doc/wav2vec2-bert)** (from Meta AI) released with the paper [Seamless: Multilingual Expressive and Streaming Speech Translation](https://ai.meta.com/research/publications/seamless-multilingual-expressive-and-streaming-speech-translation/) by the Seamless Communication team.
1. **[Wav2Vec2-Conformer](https://huggingface.co/docs/transformers/model_doc/wav2vec2-conformer)** (Facebook AI рд╕реЗ) рд╕рд╛рде рд╡рд╛рд▓рд╛ рдкреЗрдкрд░ [FAIRSEQ S2T: Fast Speech-to-Text Modeling with FAIRSEQ](https://arxiv.org/abs/2010.05171) рдЪрд╛рдВрдЧрд╣рд╛рди рд╡рд╛рдВрдЧ, рдпреВрдВ рддрд╛рдВрдЧ, рдЬреБрддрд╛рдИ рдорд╛, рдРрдиреА рд╡реВ, рд╕рд░рд╡реНрдпрд╛ рдкреЛрдкреБрд░реА, рджрд┐рдорд┐рддреНрд░реЛ рдУрдЦреЛрдирдХреЛ, рдЬреБрдЖрди рдкрд┐рдиреЛ рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[Wav2Vec2Phoneme](https://huggingface.co/docs/transformers/model_doc/wav2vec2_phoneme)** (Facebook AI рд╕реЗ) рд╕рд╛рде рд╡рд╛рд▓рд╛ рдкреЗрдкрд░ [Simple and Effective Zero-shot Cross-lingual Phoneme Recognition](https://arxiv.org/abs/2109.11680) рдХрд┐рдпрд╛рдирдЯреЛрдВрдЧ рдЬреВ, рдПрд▓реЗрдХреНрд╕реА рдмрд╛рдПрд╡реНрд╕реНрдХреА, рдорд╛рдЗрдХрд▓ рдФрд▓реА рджреНрд╡рд╛рд░рд╛ред
1. **[WavLM](https://huggingface.co/docs/transformers/model_doc/wavlm)** (рдорд╛рдЗрдХреНрд░реЛрд╕реЙрдлреНрдЯ рд░рд┐рд╕рд░реНрдЪ рд╕реЗ) рдкреЗрдкрд░ рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛ [WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing](https://arxiv.org/abs/2110.13900) рд╕рд╛рдирдпреБрдЖрди рдЪреЗрди, рдЪреЗрдВрдЧрдпреА рд╡рд╛рдВрдЧ, рдЭреЗрдВрдЧрдпрд╛рдВрдЧ рдЪреЗрди, рдпреВ рд╡реВ, рд╢реБрдЬреА рд▓рд┐рдпреВ, рдЬрд╝реБрдУ рдЪреЗрди, рдЬрд┐рдиреНрдпреБ рд▓реА, рдирд╛рдУрдпреБрдХреА рдХрд╛рдВрдбрд╛, рддрд╛рдХреБрдпрд╛ рдпреЛрд╢рд┐рдпреЛрдХрд╛, рдЬрд╝рд┐рдУрдВрдЧ рдЬрд┐рдУ, рдЬрд┐рдпрд╛рди рд╡реВ, рд▓реЙрдиреНрдЧ рдЭреЛрдЙ, рд╢реБрдУ рд░реЗрди, рдпрд╛рдирдорд┐рди рдХрд┐рдпрд╛рди, рдпрд╛рдУ рдХрд┐рдпрд╛рди, рдЬрд┐рдпрд╛рди рд╡реВ, рдорд╛рдЗрдХрд▓ рдЬрд╝реЗрдВрдЧ, рдлреБрд░реБ рд╡реЗрдИред
1. **[Whisper](https://huggingface.co/docs/transformers/model_doc/whisper)** (OpenAI рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдХрд╛рдЧрдЬ [Robust Speech Recognition via Large-Scale Weak Supervision](https://cdn.openai.com/papers/whisper.pdf) рдПрд▓реЗрдХ рд░реИрдбрдлреЛрд░реНрдб, рдЬреЛрдВрдЧ рд╡реВрдХ рдХрд┐рдо, рддрд╛рдУ рдЬреВ, рдЧреНрд░реЗрдЧ рдмреНрд░реЙрдХрдореИрди, рдХреНрд░рд┐рд╕реНрдЯреАрди рдореИрдХрд▓реАрд╡реЗ, рдЗрд▓реНрдпрд╛ рд╕реБрддреНрд╕реНрдХреЗрд╡рд░ рджреНрд╡рд╛рд░рд╛ред
1. **[X-CLIP](https://huggingface.co/docs/transformers/model_doc/xclip)** (рдорд╛рдЗрдХреНрд░реЛрд╕реЙрдлреНрдЯ рд░рд┐рд╕рд░реНрдЪ рд╕реЗ) рдХрд╛рдЧрдЬ рдХреЗ рд╕рд╛рде [Expanding Language-Image Pretrained Models for General Video Recognition](https://arxiv.org/abs/2208.02816) рдмреЛрд▓рд┐рди рдиреА, рд╣реЛрдЙрд╡реЗрди рдкреЗрдВрдЧ, рдорд┐рдВрдЧрд╛рдУ рдЪреЗрди, рд╕реЛрдВрдЧрдпрд╛рдВрдЧ рдЭрд╛рдВрдЧ, рдЧрд╛рдУрдлреЗрдВрдЧ рдореЗрдВрдЧ, рдЬрд┐рдпрд╛рдирд▓реЛрдВрдЧ рдлреВ, рд╢рд┐рдорд┐рдВрдЧ рдЬрд┐рдпрд╛рдВрдЧ, рд╣реИрдмрд┐рди рд▓рд┐рдВрдЧ рджреНрд╡рд╛рд░рд╛ред
1. **[X-MOD](https://huggingface.co/docs/transformers/model_doc/xmod)** (Meta AI рд╕реЗ) Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, Mikel Artetxe. рджреНрд╡рд╛рд░рд╛рдЕрдиреБрд╕рдВрдзрд╛рди рдкрддреНрд░ [Lifting the Curse of Multilinguality by Pre-training Modular Transformers](http://dx.doi.org/10.18653/v1/2022.naacl-main.255) рдХреЗ рд╕рд╛рде рдЬрд╛рд░реА рдХрд┐рдпрд╛ рдЧрдпрд╛
1. **[XGLM](https://huggingface.co/docs/transformers/model_doc/xglm)** (From Facebook AI) released with the paper [Few-shot Learning with Multilingual Language Models](https://arxiv.org/abs/2112.10668) by Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li.
1. **[XLM](https://huggingface.co/docs/transformers/model_doc/xlm)** (рдлреЗрд╕рдмреБрдХ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291) рдЧрд┐рд▓рд╛рдЙрдо рд▓реИрдореНрдкрд▓ рдФрд░ рдПрд▓реЗрдХреНрд╕рд┐рд╕ рдХреЛрдиреЛ рджреНрд╡рд╛рд░рд╛ред
1. **[XLM-ProphetNet](https://huggingface.co/docs/transformers/model_doc/xlm-prophetnet)** (рдорд╛рдЗрдХреНрд░реЛрд╕реЙрдлреНрдЯ рд░рд┐рд╕рд░реНрдЪ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдХрд╛рдЧрдЬ [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https://arxiv.org/abs/2001.04063) рдпреВ рдпрд╛рди, рд╡реАрдЬрд╝реЗрди рдХреНрдпреВрдИ, рдпреЗрдпреБрди рдЧреЛрдВрдЧ, рджрдпрд╛рд╣реЗрдВрдЧ рд▓рд┐рдпреВ, рдирд╛рди рдбреБрдЖрди, рдЬрд┐рдЙрд╢реЗрдВрдЧ рдЪреЗрди, рд░реБрдУрдлрд╝реЗрдИ рдЭрд╛рдВрдЧ рдФрд░ рдорд┐рдВрдЧ рдЭреЛрдЙ рджреНрд╡рд╛рд░рд╛ред
1. **[XLM-RoBERTa](https://huggingface.co/docs/transformers/model_doc/xlm-roberta)** (рдлреЗрд╕рдмреБрдХ рдПрдЖрдИ рд╕реЗ), рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) рдПрд▓реЗрдХреНрд╕рд┐рд╕ рдХреЛрдиреНрдпреВ*, рдХрд╛рд░реНрддрд┐рдХреЗрдп рдЦрдВрдбреЗрд▓рд╡рд╛рд▓*, рдирдорди рдЧреЛрдпрд▓, рд╡рд┐рд╢реНрд░рд╡ рдЪреМрдзрд░реА, рдЧрд┐рд▓рд╛рдЙрдо рд╡реЗрдирдЬрд╝реЗрдХ, рдлреНрд░рд╛рдВрд╕рд┐рд╕реНрдХреЛ рдЧреБрдЬрд╝рдореИрди рджреНрд╡рд╛рд░рд╛ , рдПрдбреМрд░реНрдб рдЧреНрд░реЗрд╡, рдорд╛рдпрд▓ рдУрдЯ, рд▓реНрдпреВрдХ рдЬрд╝реЗрдЯрд▓рдореЙрдпрд░ рдФрд░ рд╡реЗрд╕реЗрд▓рд┐рди рд╕реНрдЯреЛрдпрд╛рдиреЛрд╡ рджреНрд╡рд╛рд░рд╛ред
1. **[XLM-RoBERTa-XL](https://huggingface.co/docs/transformers/model_doc/xlm-roberta-xl)** (Facebook AI рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдХрд╛рдЧрдЬ [Larger-Scale Transformers for Multilingual Masked Language Modeling](https://arxiv.org/abs/2105.00572) рдирдорди рдЧреЛрдпрд▓, рдЬрд┐рдВрдЧрдлреЗрдИ рдбреВ, рдорд╛рдпрд▓ рдУрдЯ, рдЧрд┐рд░рд┐ рдЕрдирдВрддрд░рд╛рдорди, рдПрд▓реЗрдХреНрд╕рд┐рд╕ рдХреЛрдиреЛ рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[XLM-V](https://huggingface.co/docs/transformers/model_doc/xlm-v)** (from Meta AI) released with the paper [XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models](https://arxiv.org/abs/2301.10472) by Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad, Luke Zettlemoyer, Madian Khabsa.
1. **[XLNet](https://huggingface.co/docs/transformers/model_doc/xlnet)** (Google/CMU рд╕реЗ) рд╕рд╛рде рд╡рд╛рд▓рд╛ рдкреЗрдкрд░ [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) рдЬрд╝реАрд▓рд┐рди рдпрд╛рдВрдЧ*, рдЬрд╝рд┐рд╣рд╛рдВрдЧ рджрд╛рдИ*, рдпрд┐рдорд┐рдВрдЧ рдпрд╛рдВрдЧ, рдЬреИрдо рдХрд╛рд░реНрдмреЛрдиреЗрд▓, рд░реБрд╕реНрд▓рд╛рди рд╕рд▓рд╛рдЦреБрддрджреАрдиреЛрд╡, рдХреНрд╡реЛрдХ рд╡реА. рд▓реЗ рджреНрд╡рд╛рд░рд╛ред
1. **[XLS-R](https://huggingface.co/docs/transformers/model_doc/xls_r)** (Facebook AI рд╕реЗ) рд╕рд╛рде рд╡рд╛рд▓рд╛ рдкреЗрдкрд░ [XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale](https://arxiv.org/abs/2111.09296) рдЕрд░реБрдг рдмрд╛рдмреВ, рдЪрд╛рдВрдЧрд╣рд╛рди рд╡рд╛рдВрдЧ, рдПрдВрдбреНрд░реЛрд╕ рддрдЬрдВрджреНрд░рд╛, рдХреБрд╢рд╛рд▓ рд▓рдЦреЛрдЯрд┐рдпрд╛, рдХрд┐рдпрд╛рдирдЯреЛрдВрдЧ рдЬреВ, рдирдорди рдЧреЛрдпрд▓, рдХреГрддрд┐рдХрд╛ рд╕рд┐рдВрд╣, рдкреИрдЯреНрд░рд┐рдХ рд╡реЙрди рдкреНрд▓реИрдЯрди, рдпрд╛рдерд╛рд░реНрде рд╕рд░рд╛рдл, рдЬреБрдЖрди рдкрд┐рдиреЛ, рдПрд▓реЗрдХреНрд╕реА рдмреЗрд╡рд╕реНрдХреА, рдПрд▓реЗрдХреНрд╕рд┐рд╕ рдХреЛрдиреНрдпреВ, рдорд╛рдЗрдХрд▓ рдФрд▓реА рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[XLSR-Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/xlsr_wav2vec2)** (рдлреЗрд╕рдмреБрдХ рдПрдЖрдИ рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [Unsupervised Cross-Lingual Representation Learning For Speech Recognition](https://arxiv.org/abs/2006.13979) рдПрд▓реЗрдХреНрд╕рд┐рд╕ рдХреЛрдиреНрдпреВ, рдПрд▓реЗрдХреНрд╕реА рдмреЗрд╡рд╕реНрдХреА, рд░реЛрдирди рдХреЛрд▓реЛрдмрд░реНрдЯ, рдЕрдмреНрджреЗрд▓рд░рд╣рдорд╛рди рдореЛрд╣рдореНрдордж, рдорд╛рдЗрдХрд▓ рдФрд▓реА рджреНрд╡рд╛рд░рд╛ред
1. **[YOLOS](https://huggingface.co/docs/transformers/model_doc/yolos)** (рд╣реБрдЖрдЭреЛрдВрдЧ рдпреВрдирд┐рд╡рд░реНрд╕рд┐рдЯреА рдСрдл рд╕рд╛рдЗрдВрд╕ рдПрдВрдб рдЯреЗрдХреНрдиреЛрд▓реЙрдЬреА рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection](https://arxiv.org/abs/2106.00666) рдпреБрдХреНрд╕рд┐рди рдлреЗрдВрдЧ, рдмреЗрдирдЪреЗрдВрдЧ рд▓рд┐рдпрд╛рдУ, рдЬрд┐рдВрдЧрдЧреИрдВрдЧ рд╡рд╛рдВрдЧ, рдЬреЗрдорд┐рди рдлреЗрдВрдЧ, рдЬрд┐рдпрд╛рдВрдЧ рдХреНрдпреВрдИ, рд░реБрдИ рд╡реВ, рдЬрд┐рдпрд╛рдирд╡реЗрдИ рдиреАрдпреВ, рд╡реЗрдиреНрдпреВ рд▓рд┐рдпреВ рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. **[YOSO](https://huggingface.co/docs/transformers/model_doc/yoso)** (рд╡рд┐рд╕реНрдХреЙрдиреНрд╕рд┐рди рд╡рд┐рд╢реНрд╡рд╡рд┐рджреНрдпрд╛рд▓рдп - рдореИрдбрд┐рд╕рди рд╕реЗ) рд╕рд╛рде рдореЗрдВ рдкреЗрдкрд░ [рдпреВ рдУрдирд▓реА рд╕реИрдВрдкрд▓ (рд▓рдЧрднрдЧ) рдЬрд╝рд╛рдирдкреЗрдВрдЧ рдЬрд╝реЗрдВрдЧ, рдпреБрдирдпрд╛рдВрдЧ рдЬрд╝рд┐рдУрдВрдЧ рджреНрд╡рд╛рд░рд╛ , рд╕рддреНрдп рдПрди. рд░рд╡рд┐, рд╢реИрд▓реЗрд╢ рдЖрдЪрд╛рд░реНрдп, рдЧреНрд▓реЗрди рдлрдВрдЧ, рд╡рд┐рдХрд╛рд╕ рд╕рд┐рдВрд╣ рджреНрд╡рд╛рд░рд╛ рдкреЛрд╕реНрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛ред
1. рдПрдХ рдирдП рдореЙрдбрд▓ рдореЗрдВ рдпреЛрдЧрджрд╛рди рджреЗрдирд╛ рдЪрд╛рд╣рддреЗ рд╣реИрдВ? рдирдП рдореЙрдбрд▓ рдЬреЛрдбрд╝рдиреЗ рдореЗрдВ рдЖрдкрдХрд╛ рдорд╛рд░реНрдЧрджрд░реНрд╢рди рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рд╣рдорд╛рд░реЗ рдкрд╛рд╕ рдПрдХ **рд╡рд┐рд╕реНрддреГрдд рдорд╛рд░реНрдЧрджрд░реНрд╢рд┐рдХрд╛ рдФрд░ рдЯреЗрдореНрдкреНрд▓реЗрдЯ** рд╣реИред рдЖрдк рдЙрдиреНрд╣реЗрдВ [`рдЯреЗрдореНрдкрд▓реЗрдЯреНрд╕`](./templates) рдирд┐рд░реНрджреЗрд╢рд┐рдХрд╛ рдореЗрдВ рдкрд╛ рд╕рдХрддреЗ рд╣реИрдВред рдкреАрдЖрд░ рд╢реБрд░реВ рдХрд░рдиреЗ рд╕реЗ рдкрд╣рд▓реЗ [рдпреЛрдЧрджрд╛рди рджрд┐рд╢рд╛рдирд┐рд░реНрджреЗрд╢](./CONTRIBUTING.md) рджреЗрдЦрдирд╛ рдФрд░ рдЕрдиреБрд░рдХреНрд╖рдХреЛрдВ рд╕реЗ рд╕рдВрдкрд░реНрдХ рдХрд░рдирд╛ рдпрд╛ рдкреНрд░рддрд┐рдХреНрд░рд┐рдпрд╛ рдкреНрд░рд╛рдкреНрдд рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдПрдХ рдирдпрд╛ рдореБрджреНрджрд╛ рдЦреЛрд▓рдирд╛ рдпрд╛рдж рд░рдЦреЗрдВред

рдпрд╣ рдЬрд╛рдВрдЪрдиреЗ рдХреЗ рд▓рд┐рдП рдХрд┐ рдХреНрдпрд╛ рдХрд┐рд╕реА рдореЙрдбрд▓ рдореЗрдВ рдкрд╣рд▓реЗ рд╕реЗ рд╣реА Flax, PyTorch рдпрд╛ TensorFlow рдХрд╛ рдХрд╛рд░реНрдпрд╛рдиреНрд╡рдпрди рд╣реИ, рдпрд╛ рдпрджрд┐ рдЙрд╕рдХреЗ рдкрд╛рд╕ Tokenizers рд▓рд╛рдЗрдмреНрд░реЗрд░реА рдореЗрдВ рд╕рдВрдмрдВрдзрд┐рдд рдЯреЛрдХрди рд╣реИ, рддреЛ [рдпрд╣ рддрд╛рд▓рд┐рдХрд╛](https://huggingface.co/docs/transformers/index#supported) рджреЗрдЦреЗрдВред -рдлреНрд░реЗрдорд╡рд░реНрдХ)ред

рдЗрди рдХрд╛рд░реНрдпрд╛рдиреНрд╡рдпрдиреЛрдВ рдХрд╛ рдкрд░реАрдХреНрд╖рдг рдХрдИ рдбреЗрдЯрд╛рд╕реЗрдЯ рдкрд░ рдХрд┐рдпрд╛ рдЧрдпрд╛ рд╣реИ (рджреЗрдЦреЗрдВ рдХреЗрд╕ рд╕реНрдХреНрд░рд┐рдкреНрдЯ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВ) рдФрд░ рд╡реИрдирд┐рд▓рд╛ рдХрд╛рд░реНрдпрд╛рдиреНрд╡рдпрди рдХреЗ рд▓рд┐рдП рддреБрд▓рдирд╛рддреНрдордХ рд░реВрдк рд╕реЗ рдкреНрд░рджрд░реНрд╢рди рдХрд░рдирд╛ рдЪрд╛рд╣рд┐рдПред рдЖрдк рдЙрдкрдпреЛрдЧ рдХреЗ рдорд╛рдорд▓реЗ рдХреЗ рджрд╕реНрддрд╛рд╡реЗрдЬрд╝ [рдЗрд╕ рдЕрдиреБрднрд╛рдЧ](https://huggingface.co/docs/transformers/examples) рдореЗрдВ рд╡реНрдпрд╡рд╣рд╛рд░ рдХрд╛ рд╡рд┐рд╡рд░рдг рдкрдврд╝ рд╕рдХрддреЗ рд╣реИрдВред


## рдЕрдзрд┐рдХ рд╕рдордЭреЗрдВ

|рдЕрдзреНрдпрд╛рдп | рд╡рд┐рд╡рд░рдг |
|-|-|
| [рджрд╕реНрддрд╛рд╡реЗрдЬрд╝реАрдХрд░рдг](https://huggingface.co/transformers/) | рдкреВрд░рд╛ рдПрдкреАрдЖрдИ рджрд╕реНрддрд╛рд╡реЗрдЬрд╝реАрдХрд░рдг рдФрд░ рдЯреНрдпреВрдЯреЛрд░рд┐рдпрд▓ |
| [рдХрд╛рд░реНрдп рд╕рд╛рд░рд╛рдВрд╢](https://huggingface.co/docs/transformers/task_summary) | рдЯреНрд░рд╛рдВрд╕рдлреЙрд░реНрдорд░ рд╕рдорд░реНрдерд┐рдд рдХрд╛рд░реНрдп |
| [рдкреНрд░реАрдкреНрд░реЛрд╕реЗрд╕рд┐рдВрдЧ рдЯреНрдпреВрдЯреЛрд░рд┐рдпрд▓](https://huggingface.co/docs/transformers/preprocessing) | рдореЙрдбрд▓ рдХреЗ рд▓рд┐рдП рдбреЗрдЯрд╛ рддреИрдпрд╛рд░ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП `рдЯреЛрдХрдирд╛рдЗрдЬрд╝рд░` рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рдирд╛ |
| [рдкреНрд░рд╢рд┐рдХреНрд╖рдг рдФрд░ рдлрд╛рдЗрди-рдЯреНрдпреВрдирд┐рдВрдЧ](https://huggingface.co/docs/transformers/training) | PyTorch/TensorFlow рдХреЗ рдЯреНрд░реЗрдирд┐рдВрдЧ рд▓реВрдк рдпрд╛ `рдЯреНрд░реЗрдирд░` API рдореЗрдВ рдЯреНрд░рд╛рдВрд╕рдлреЙрд░реНрдорд░ рджреНрд╡рд╛рд░рд╛ рджрд┐рдП рдЧрдП рдореЙрдбрд▓ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВ |
| [рдХреНрд╡рд┐рдХ рд╕реНрдЯрд╛рд░реНрдЯ: рдЯреНрд╡реАрдХрд┐рдВрдЧ рдПрдВрдб рдпреВрдЬрд╝ рдХреЗрд╕ рд╕реНрдХреНрд░рд┐рдкреНрдЯреНрд╕](https://github.com/huggingface/transformers/tree/main/examples) | рд╡рд┐рднрд┐рдиреНрди рдХрд╛рд░реНрдпреЛрдВ рдХреЗ рд▓рд┐рдП рдХреЗрд╕ рд╕реНрдХреНрд░рд┐рдкреНрдЯ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВ |
| [рдореЙрдбрд▓ рд╕рд╛рдЭрд╛ рдХрд░рдирд╛ рдФрд░ рдЕрдкрд▓реЛрдб рдХрд░рдирд╛](https://huggingface.co/docs/transformers/model_sharing) | рд╕рдореБрджрд╛рдп рдХреЗ рд╕рд╛рде рдЕрдкрдиреЗ рдлрд╛рдЗрди рдЯреВрдирдб рдореЙрдбрд▓ рдЕрдкрд▓реЛрдб рдФрд░ рд╕рд╛рдЭрд╛ рдХрд░реЗрдВ |
| [рдорд╛рдЗрдЧреНрд░реЗрд╢рди](https://huggingface.co/docs/transformers/migration) | `рдкрд╛рдЗрдЯреЛрд░рдЪ-рдЯреНрд░рд╛рдВрд╕рдлреЙрд░реНрдорд░реНрд╕` рдпрд╛ `рдкрд╛рдЗрдЯреЛрд░рдЪ-рдкреНрд░реАрдЯреНрд░реЗрдирдб-рдмрд░реНрдЯ` рд╕реЗ рдЯреНрд░рд╛рдВрд╕рдлреЙрд░реНрдорд░ рдореЗрдВ рдорд╛рдЗрдЧреНрд░реЗрдЯ рдХрд░рдирд╛ |

## рдЙрджреНрдзрд░рдг

рд╣рдордиреЗ рдЖрдзрд┐рдХрд╛рд░рд┐рдХ рддреМрд░ рдкрд░ рдЗрд╕ рд▓рд╛рдЗрдмреНрд░реЗрд░реА рдХрд╛ [рдкреЗрдкрд░](https://www.aclweb.org/anthology/2020.emnlp-demos.6/) рдкреНрд░рдХрд╛рд╢рд┐рдд рдХрд┐рдпрд╛ рд╣реИ, рдЕрдЧрд░ рдЖрдк рдЯреНрд░рд╛рдиреНрд╕рдлрд╝реЙрд░реНрдорд░реНрд╕ рд▓рд╛рдЗрдмреНрд░реЗрд░реА рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рддреЗ рд╣реИрдВ, рддреЛ рдХреГрдкрдпрд╛ рдЙрджреНрдзреГрдд рдХрд░реЗрдВ:
```bibtex
@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R├йmi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}
```
